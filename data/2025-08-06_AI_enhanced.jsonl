{"id": "2508.02679", "pdf": "https://arxiv.org/pdf/2508.02679.pdf", "abs": "https://arxiv.org/abs/2508.02679", "title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "categories": ["cs.HC"], "comment": null, "summary": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health.", "AI": {"tldr": "Proposes a novel LLM agent-based simulation framework to model student activities and mental health using sensing data.", "motivation": "Students' mental well-being is crucial for academic success and influenced by various activities. The study aims to utilize advanced simulation frameworks to deepen the understanding of this relationship.", "method": "Developed a simulation framework with LLM agents initialized with personality questionnaires, guided by smartphone sensing data to predict behaviors and self-report mental health data through EMAs.", "result": "The simulation explores different scenarios not present in the original dataset, investigates peer influence, and allows for intervention studies to enhance student well-being.", "conclusion": "The LLM-driven modeling framework provides insights into behavioral changes that can support student mental health, demonstrating the utility of integrating sensing data with LLMs.", "key_contributions": ["Introduction of a LLM agent-based simulation framework for student mental health analysis", "Use of smartphone sensing data for dynamic mental state management", "Capability to conduct intervention studies and peer interaction simulations"], "limitations": "The framework's accuracy is dependent on the quality of the sensing data and personality questionnaires.", "keywords": ["mental health", "LLM agents", "student life", "sensing data", "behavioral modeling"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.02680", "pdf": "https://arxiv.org/pdf/2508.02680.pdf", "abs": "https://arxiv.org/abs/2508.02680", "title": "AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI", "authors": ["Pragya Singh", "Ankush Gupta", "Mohan Kumar", "Pushpendra Singh"], "categories": ["cs.HC", "cs.AI"], "comment": "To be published in IMWUT, September 2025", "summary": "Emotional and mental well-being are vital components of quality of life, and\nwith the rise of smart devices like smartphones, wearables, and artificial\nintelligence (AI), new opportunities for monitoring emotions in everyday\nsettings have emerged. However, for AI algorithms to be effective, they require\nhigh-quality data and accurate annotations. As the focus shifts towards\ncollecting emotion data in real-world environments to capture more authentic\nemotional experiences, the process of gathering emotion annotations has become\nincreasingly complex. This work explores the challenges of everyday emotion\ndata collection from the perspectives of key stakeholders. We collected 75\nsurvey responses, performed 32 interviews with the public, and 3 focus group\ndiscussions (FGDs) with 12 mental health professionals. The insights gained\nfrom a total of 119 stakeholders informed the development of our framework,\nAnnoSense, designed to support everyday emotion data collection for AI. This\nframework was then evaluated by 25 emotion AI experts for its clarity,\nusefulness, and adaptability. Lastly, we discuss the potential next steps and\nimplications of AnnoSense for future research in emotion AI, highlighting its\npotential to enhance the collection and analysis of emotion data in real-world\ncontexts.", "AI": {"tldr": "This paper presents AnnoSense, a framework supporting everyday emotion data collection for AI, informed by insights from 119 stakeholders, including the public and mental health professionals.", "motivation": "To address the challenges of collecting high-quality emotion data in real-world environments as AI algorithms require accurate annotations to be effective.", "method": "The study employed surveys, interviews, and focus group discussions with a diverse group of 119 stakeholders to gather insights on emotion data collection challenges.", "result": "The evaluation of AnnoSense by 25 emotion AI experts indicated its clarity, usefulness, and adaptability for enhancing emotion data collection.", "conclusion": "The framework has potential implications for improving emotion AI research and data analysis in real-world settings.", "key_contributions": ["Development of the AnnoSense framework for emotion data collection.", "Insights from a diverse range of stakeholders including mental health professionals.", "Evaluation of the framework by emotion AI experts."], "limitations": "", "keywords": ["emotion data", "AI", "well-being", "AnnoSense", "collecting emotions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.02817", "pdf": "https://arxiv.org/pdf/2508.02817.pdf", "abs": "https://arxiv.org/abs/2508.02817", "title": "Real-World Receptivity to Adaptive Mental Health Interventions: Findings from an In-the-Wild Study", "authors": ["Nilesh Kumar Sahu", "Aditya Sneh", "Snehil Gupta", "Haroon R Lone"], "categories": ["cs.HC", "cs.AI", "cs.CY", "eess.SP"], "comment": null, "summary": "The rise of mobile health (mHealth) technologies has enabled real-time\nmonitoring and intervention for mental health conditions using passively sensed\nsmartphone data. Building on these capabilities, Just-in-Time Adaptive\nInterventions (JITAIs) seek to deliver personalized support at opportune\nmoments, adapting to users' evolving contexts and needs. Although prior\nresearch has examined how context affects user responses to generic\nnotifications and general mHealth messages, relatively little work has explored\nits influence on engagement with actual mental health interventions.\nFurthermore, while much of the existing research has focused on detecting when\nusers might benefit from an intervention, less attention has been paid to\nunderstanding receptivity, i.e., users' willingness and ability to engage with\nand act upon the intervention.\n  In this study, we investigate user receptivity through two components:\nacceptance(acknowledging or engaging with a prompt) and feasibility (ability to\nact given situational constraints). We conducted a two-week in-the-wild study\nwith 70 students using a custom Android app, LogMe, which collected passive\nsensor data and active context reports to prompt mental health interventions.\nThe adaptive intervention module was built using Thompson Sampling, a\nreinforcement learning algorithm. We address four research questions relating\nsmartphone features and self-reported contexts to acceptance and feasibility,\nand examine whether an adaptive reinforcement learning approach can optimize\nintervention delivery by maximizing a combined receptivity reward. Our results\nshow that several types of passively sensed data significantly influenced user\nreceptivity to interventions. Our findings contribute insights into the design\nof context-aware, adaptive interventions that are not only timely but also\nactionable in real-world settings.", "AI": {"tldr": "This study explores user receptivity to mobile health interventions, focusing on acceptance and feasibility using passive smartphone data and an adaptive reinforcement learning approach.", "motivation": "To understand how context influences engagement with mental health interventions and improve the effectiveness of Just-in-Time Adaptive Interventions (JITAIs).", "method": "A two-week in-the-wild study with 70 students using a custom Android app, LogMe, to collect passive sensor data and active context reports for prompting mental health interventions. An adaptive module based on Thompson Sampling was used for intervention delivery.", "result": "Several types of passively sensed data significantly influenced user receptivity to mental health interventions, revealing insights into improving intervention design.", "conclusion": "Understanding user receptivity can lead to the development of more effective and actionable context-aware interventions in real-world settings.", "key_contributions": ["Investigated user receptivity components: acceptance and feasibility.", "Utilized passive smartphone data to enhance mental health interventions.", "Applied a reinforcement learning approach to optimize intervention delivery."], "limitations": "", "keywords": ["mobile health", "mental health interventions", "Just-in-Time Adaptive Interventions", "user receptivity", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02823", "pdf": "https://arxiv.org/pdf/2508.02823.pdf", "abs": "https://arxiv.org/abs/2508.02823", "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification", "authors": ["Wenshuo Zhang", "Leixian Shen", "Shuchang Xu", "Jindu Wang", "Jian Zhao", "Huamin Qu", "Linping Yuan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted in UIST 2025", "summary": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency.", "AI": {"tldr": "This paper addresses the misalignment between user intent and code generated by conversational LLMs by proposing a new interaction paradigm called direct intent-task matching.", "motivation": "To solve the problem of users with limited programming experience facing frustration due to misalignment between their intents and the code generated by LLMs.", "method": "The authors propose a new interaction paradigm that externalizes and enables direct manipulation of LLM understanding, implemented as NeuroSync, utilizing a knowledge distillation pipeline.", "result": "NeuroSync enhances intent-task alignment, reduces cognitive effort, and boosts coding efficiency, demonstrated through technical experiments and a user study with 12 participants.", "conclusion": "The proposed method effectively aligns users' intents with coding tasks, improving the overall usability of LLMs in coding scenarios.", "key_contributions": ["Introduction of a novel human-LLM interaction paradigm", "Implementation of NeuroSync to visualize and edit LLM understanding", "Demonstration of enhanced coding efficiency through user study"], "limitations": "", "keywords": ["Human-Computer Interaction", "Conversational LLMs", "User Intent"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02808", "pdf": "https://arxiv.org/pdf/2508.02808.pdf", "abs": "https://arxiv.org/abs/2508.02808", "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.", "AI": {"tldr": "This paper presents ICARE, an interpretable evaluation framework for assessing the clinical quality of automatically generated radiology reports using language model agents that generate dynamic clinical questions.", "motivation": "The need for reliable clinical evaluation of automated radiology reports due to the limitations of existing metrics that lack interpretability.", "method": "ICARE utilizes large language model agents to create clinically relevant questions based on either ground-truth or generated reports, allowing for an interpretable evaluation of report quality through multiple-choice question answering.", "result": "ICARE shows a significant alignment with expert clinician judgments compared to traditional metrics, demonstrating its effectiveness in assessing report quality.", "conclusion": "The framework provides a transparent and interpretable assessment of radiology reports, highlighting its relevance in improving automated report generation systems.", "key_contributions": ["Introduction of ICARE as an interpretable evaluation framework.", "Use of dynamic MCQA to assess clinical precision and recall of generated reports.", "Demonstrated alignment with expert judgment through clinician studies."], "limitations": "The framework's effectiveness may be contingent on the quality of the generated questions and the agents' capabilities.", "keywords": ["radiology report generation", "evaluation framework", "ICARE", "clinical decision-making", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02868", "pdf": "https://arxiv.org/pdf/2508.02868.pdf", "abs": "https://arxiv.org/abs/2508.02868", "title": "Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators", "authors": ["Kaixuan Wang", "Loraine Clarke", "Carl-Cyril J Dreue", "Guancheng Zhou", "Jason T. Jacques"], "categories": ["cs.HC", "cs.CY"], "comment": "22 pages", "summary": "Online communities serve as essential support channels for People Who Use\nDrugs (PWUD), providing access to peer support and harm reduction information.\nThe moderation of these communities involves consequential decisions affecting\nmember safety, yet existing sociotechnical systems provide insufficient support\nfor moderators. Through interviews with experienced moderators from PWUD forums\non Reddit, we analyse the unique nature of this work. We argue that this work\nconstitutes a distinct form of public health intervention characterised by\nthree moderation challenges: the need for specialised, expert risk assessment;\ntime-critical crisis response; and the navigation of a structural conflict\nbetween platform policies and community safety goals. We demonstrate how\ncurrent moderation systems are insufficient in supporting PWUD communities. For\nexample, policies minimising platforms' legal exposure to illicit activities\ncan inadvertently push moderators to implement restrictive rules to protect\ncommunity's existence, which can limit such a vulnerable group's ability to\nshare potentially life-saving resources online. We conclude by identifying two\nnecessary shifts in sociotechnical design to support moderators' work: first,\nmoving to automated tools that support human sensemaking in contexts with\ncompeting interests; and second, shifting from systems that require moderators\nto perform low-level rule programming to those that enable high-level,\nexample-based instruction. Further, we highlight how the design of\nsociotechnical systems in online spaces could impact harm reduction efforts\naimed at improving health outcomes for PWUD communities.", "AI": {"tldr": "This paper analyzes the challenges faced by moderators in online communities for People Who Use Drugs, proposing improvements to sociotechnical systems that support their work.", "motivation": "Moderation in online communities for People Who Use Drugs (PWUD) is critical for ensuring member safety and providing support, yet current systems inadequately assist moderators in addressing unique challenges.", "method": "Interviews with experienced moderators from PWUD forums on Reddit were conducted to understand their experiences and the challenges in moderation.", "result": "The study identifies three main challenges in moderation: specialized risk assessment, time-critical crisis response, and conflicts between platform policies and community safety goals. Current moderation systems fail to adequately support these needs.", "conclusion": "The paper calls for the design of sociotechnical systems that automate support for human decision-making and reduce the necessity for moderators to engage in low-level programming, to improve health outcomes for PWUD communities.", "key_contributions": ["Identification of unique moderation challenges in PWUD forums", "Critique of current moderation systems and policies", "Proposals for improved sociotechnical design to support moderators"], "limitations": "", "keywords": ["Online communities", "Moderation", "People Who Use Drugs", "Public health", "Sociotechnical systems"], "importance_score": 8, "read_time_minutes": 22}}
{"id": "2508.02853", "pdf": "https://arxiv.org/pdf/2508.02853.pdf", "abs": "https://arxiv.org/abs/2508.02853", "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives", "authors": ["Yinuo Xu", "Veronica Derricks", "Allison Earl", "David Jurgens"], "categories": ["cs.CL"], "comment": "28 pages, 17 figures", "summary": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives.", "AI": {"tldr": "The paper presents a model, DEM-MoE, that addresses annotator disagreement in subjective NLP tasks by considering demographic factors, improving data representation through real and synthetic annotations.", "motivation": "Modeling annotator disagreement in subjective NLP tasks has been a challenge, and understanding demographic influences can enhance model performance and data representation.", "method": "The DEM-MoE model routes inputs to expert subnetworks based on annotator demographics and uses LLM-generated synthetic annotations for data enrichment and imputation.", "result": "The model performs competitively across demographic groups, showing better results in datasets with high annotator disagreement and providing scalable methods for data imputation.", "conclusion": "The contributions improve the representation of diverse perspectives in NLP tasks, making it more effective in handling demographic variations.", "key_contributions": ["Introduction of DEM-MoE model for handling demographic influences in NLP", "Use of LLM-generated synthetic annotations for addressing sparse demographic coverage", "Evaluation of data blending strategies tailored to dataset structures."], "limitations": "The effectiveness of synthetic annotation alignment with human annotations varies, and approaches may depend on specific dataset characteristics.", "keywords": ["annotator disagreement", "demographic-aware modeling", "synthetic annotations", "NLP", "data imputation"], "importance_score": 8, "read_time_minutes": 28}}
{"id": "2508.02958", "pdf": "https://arxiv.org/pdf/2508.02958.pdf", "abs": "https://arxiv.org/abs/2508.02958", "title": "VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People", "authors": ["Daniel Killough", "Justin Feng", "Zheng Xue \"ZX\" Ching", "Daniel Wang", "Rithvik Dyava", "Yapeng Tian", "Yuhang Zhao"], "categories": ["cs.HC"], "comment": "17 pages, 10 figures, 2 tables, LaTeX; To be published in ACM's 2025\n  Symposium on User Interface Software and Technology (UIST 2025)", "summary": "Virtual Reality (VR) is inaccessible to blind people. While research has\ninvestigated many techniques to enhance VR accessibility, they require\nadditional developer effort to integrate. As such, most mainstream VR apps\nremain inaccessible as the industry de-prioritizes accessibility. We present\nVRSight, an end-to-end system that recognizes VR scenes post hoc through a set\nof AI models (e.g., object detection, depth estimation, LLM-based atmosphere\ninterpretation) and generates tone-based, spatial audio feedback, empowering\nblind users to interact in VR without developer intervention. To enable virtual\nelement detection, we further contribute DISCOVR, a VR dataset consisting of 30\nvirtual object classes from 17 social VR apps, substituting real-world datasets\nthat remain not applicable to VR contexts. Nine participants used VRSight to\nexplore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in\nfacilitating social tasks like avatar awareness and available seat\nidentification.", "AI": {"tldr": "VRSight enhances VR accessibility for blind users through AI-driven audio feedback and a dedicated VR dataset.", "motivation": "Most VR applications remain inaccessible to blind users due to the lack of integrated accessibility features and industry neglect.", "method": "VRSight uses AI models for object detection, depth estimation, and atmosphere interpretation to generate spatial audio feedback in VR, along with the DISCOVR dataset featuring 30 virtual object classes from 17 social VR apps.", "result": "Nine participants effectively used VRSight to explore a VR app, demonstrating improved social task performance such as avatar awareness and seat identification.", "conclusion": "VRSight enables blind users to interact with VR environments without requiring changes from app developers, addressing a significant gap in VR accessibility.", "key_contributions": ["Introduction of VRSight for VR accessibility", "Development of DISCOVR dataset for virtual object detection", "Effective implementation of spatial audio feedback for blind users."], "limitations": "The study involves a limited participant sample and focuses on a specific VR app, which may not generalize to all VR environments.", "keywords": ["Virtual Reality", "Accessibility", "AI", "Blind Users", "Spatial Audio"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.02872", "pdf": "https://arxiv.org/pdf/2508.02872.pdf", "abs": "https://arxiv.org/abs/2508.02872", "title": "Highlight & Summarize: RAG without the jailbreaks", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.", "AI": {"tldr": "The paper introduces Highlight & Summarize (H&S), a novel design pattern for RAG systems that prevents jailbreaking and model hijacking in LLMs by not revealing the user's question to the model.", "motivation": "Preventing jailbreaking and model hijacking of LLMs is critical, as malicious prompts can generate undesirable content or misdirect the model's functionality.", "method": "The H&S design pattern splits the RAG pipeline into two components: a highlighter that extracts relevant passages from retrieved documents based on the user's question, and a summarizer that condenses these passages into an answer, all while keeping the user's question hidden from the LLM.", "result": "H&S responses, particularly when using an LLM-based highlighter, often outperform those from standard RAG pipelines in terms of correctness, relevance, and response quality.", "conclusion": "The H&S pattern offers a robust approach to enhance the security of LLMs against malicious inputs while maintaining high-quality outputs.", "key_contributions": ["Introduction of the H&S design pattern for RAG systems", "Demonstrated superiority of H&S responses over standard RAG", "Evaluation of various instantiations of the H&S framework"], "limitations": "The evaluation is limited to specific configurations of the H&S approach; further studies may be needed to generalize findings across different contexts.", "keywords": ["Large Language Models", "jailbreaking prevention", "retrieval-augmented generation", "natural language processing", "summarization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03014", "pdf": "https://arxiv.org/pdf/2508.03014.pdf", "abs": "https://arxiv.org/abs/2508.03014", "title": "Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers", "authors": ["Jingyan Wang", "Yang Zhao", "Haotian Mao", "Xubo Yang"], "categories": ["cs.HC", "I.2, H.5"], "comment": "29 pages, 5 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, and their integration with\nExtended Reality (XR) is poised to transform how users interact with immersive\nenvironments. This survey provides a comprehensive review of recent\ndevelopments at the intersection of LLMs and XR, offering a structured\norganization of research along both technical and application dimensions. We\npropose a taxonomy of LLM-enhanced XR systems centered on key technical\nparadigms -- such as interactive agent control, XR development toolkits, and\ngenerative scene synthesis -- and discuss how these paradigms enable novel\ncapabilities in XR. In parallel, we examine how LLM-driven techniques support\npractical XR applications across diverse domains, including immersive\neducation, clinical healthcare, and industrial manufacturing. By connecting\nthese technical paradigms with application frontiers, our survey highlights\ncurrent trends, delineates design considerations, and identifies open\nchallenges in building LLM-augmented XR systems. This work provides insights\nthat can guide researchers and practitioners in advancing the state of the art\nin intelligent XR experiences.", "AI": {"tldr": "This survey reviews the intersection of Large Language Models (LLMs) and Extended Reality (XR), proposing a taxonomy and discussing applications in various domains.", "motivation": "To understand how the integration of LLMs with XR can transform user interactions in immersive environments and guide research and practical applications in this field.", "method": "A comprehensive survey structured along technical and application dimensions, proposing a taxonomy of LLM-enhanced XR systems and discussing both technical paradigms and practical applications.", "result": "The survey identifies key technical paradigms such as interactive agent control, XR development toolkits, and generative scene synthesis and demonstrates their applications in immersive education, clinical healthcare, and industrial manufacturing.", "conclusion": "The work highlights current trends, design considerations, and open challenges in the development of LLM-augmented XR systems, providing insights for future research and applications.", "key_contributions": ["Proposes a taxonomy of LLM-enhanced XR systems.", "Connects technical paradigms with practical application areas.", "Identifies open challenges in LLM and XR integration."], "limitations": "The survey primarily focuses on existing literature and may not cover all emerging trends or technologies in LLM and XR.", "keywords": ["Large Language Models", "Extended Reality", "immersive education", "clinical healthcare", "industrial manufacturing"], "importance_score": 9, "read_time_minutes": 60}}
{"id": "2508.02885", "pdf": "https://arxiv.org/pdf/2508.02885.pdf", "abs": "https://arxiv.org/abs/2508.02885", "title": "Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages", "authors": ["Elliot Murphy", "Rohan Venkatesh", "Edward Khokhlovich", "Andrey Vyshedskiy"], "categories": ["cs.CL"], "comment": null, "summary": "In the modern language sciences, the core computational operation of syntax,\n'Merge', is defined as an operation that combines two linguistic units (e.g.,\n'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).\nThis can then be further combined with additional linguistic units based on\nthis categorial information, respecting non-associativity such that abstract\ngrouping is respected. Some linguists have embraced the view that Merge is an\nelementary, indivisible operation that emerged in a single evolutionary step.\nFrom a neurocognitive standpoint, different mental objects constructed by Merge\nmay be supported by distinct mechanisms: (1) simple command constructions\n(e.g., \"eat apples\"); (2) the merging of adjectives and nouns (\"red boat\"); and\n(3) the merging of nouns with spatial prepositions (\"laptop behind the sofa\").\nHere, we systematically investigate participants' comprehension of sentences\nwith increasing levels of syntactic complexity. Clustering analyses revealed\nbehavioral evidence for three distinct structural types, which we discuss as\npotentially emerging at different developmental stages and subject to selective\nimpairment. While a Merge-based syntax may still have emerged suddenly in\nevolutionary time, responsible for the structured symbolic turn our species\ntook, different cognitive mechanisms seem to underwrite the processing of\nvarious types of Merge-based objects.", "AI": {"tldr": "This paper investigates the cognitive processes involved in understanding syntactic structures formed by the 'Merge' operation in language.", "motivation": "To explore how different levels of syntactic complexity influence sentence comprehension and to identify distinct cognitive mechanisms responsible for processing Merge-based constructs.", "method": "The study involved systematic investigation and clustering analyses of participants' comprehension of sentences with various syntactic complexities.", "result": "Behavioral evidence indicated three distinct structural types of syntactic complexity, which might develop at different stages and could face selective impairments.", "conclusion": "Different cognitive mechanisms support the processing of Merge-based linguistic structures, although the overall merge operation may have emerged suddenly in evolutionary history.", "key_contributions": ["Identifies three distinct types of Merge-based structures in sentence comprehension", "Provides insight into cognitive mechanisms underpinning syntactical processing", "Suggests developmental stages in the understanding of syntactic complexity"], "limitations": "", "keywords": ["syntax", "Merge operation", "cognitive mechanisms", "sentence comprehension", "linguistic structures"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2508.03061", "pdf": "https://arxiv.org/pdf/2508.03061.pdf", "abs": "https://arxiv.org/abs/2508.03061", "title": "Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling", "authors": ["Shuchang Xu"], "categories": ["cs.HC"], "comment": null, "summary": "Empowering blind and low vision (BLV) users to explore visual media improves\ncontent comprehension, strengthens user agency, and fulfills diverse\ninformation needs. However, most existing tools separate exploration from the\nmain narration, which disrupts the narrative flow, increases cognitive load,\nand limits deep engagement with visual media. To address these challenges, my\nPhD research introduces the paradigm of AI-powered interactive storytelling,\nwhich leverages AI to generate interactive narratives, enabling BLV users to\nexplore visual media within a coherent storytelling experience. I have\noperationalized this paradigm through three techniques: (1) Hierarchical\nNarrative, which supports photo-collection exploration at different levels of\ndetail; (2) Parallel Narrative, which provides seamless access to time-synced\nvideo comments; and (3) Branching Narrative, which enables immersive navigation\nof 360{\\deg} videos. Together, these techniques demonstrate that AI-powered\ninteractive storytelling can effectively balance user agency with narrative\ncoherence across diverse media formats. My future work will advance this\nparadigm by enabling more personalized and expressive storytelling experiences\nfor BLV audiences.", "AI": {"tldr": "This research introduces AI-powered interactive storytelling to enhance the experience of blind and low vision users in exploring visual media.", "motivation": "To improve content comprehension, strengthen user agency, and fulfill diverse information needs for blind and low vision users while addressing existing limitations in visual media exploration tools.", "method": "The research operationalizes AI-powered interactive storytelling through three techniques: Hierarchical Narrative, Parallel Narrative, and Branching Narrative, facilitating varied exploration of media formats.", "result": "The techniques demonstrate the efficacy of AI-powered interactive storytelling in balancing user agency with narrative coherence across different types of media.", "conclusion": "Future work aims to develop more personalized and expressive storytelling experiences for blind and low vision audiences.", "key_contributions": ["Introduction of AI-powered interactive storytelling paradigm for BLV users", "Development of three unique narrative techniques: Hierarchical, Parallel, and Branching Narratives", "Demonstration of improved engagement and accessibility in visual media for BLV users"], "limitations": "", "keywords": ["AI", "interactive storytelling", "blind users", "visual media", "narrative techniques"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02886", "pdf": "https://arxiv.org/pdf/2508.02886.pdf", "abs": "https://arxiv.org/abs/2508.02886", "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models", "authors": ["Wenjie Luo", "Ruocheng Li", "Shanshan Zhu", "Julian Perry"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning.", "AI": {"tldr": "The Coherent Multimodal Reasoning Framework (CMRF) enhances large vision-language models' common sense reasoning through iterative, self-evaluating mechanisms, achieving state-of-the-art performance.", "motivation": "Existing large language and vision-language models struggle with complex multi-step reasoning tasks, often relying on superficial associations rather than deep inference.", "method": "CMRF employs three key modules: a Reasoning Decomposition Unit to break down problems, a Contextual Inference Engine for inference, and a Coherence Assessment Module for evaluating consistency, supported by an Adaptive Iterative Refinement strategy.", "result": "CMRF achieves an average accuracy of 69.4% on benchmarks like VCR, A-OKVQA, and DailyLife-MRC, surpassing existing open-source models by 2.4 percentage points, particularly excelling in complex reasoning.", "conclusion": "CMRF represents a significant improvement in LVLM reasoning capabilities, verified by extensive studies and evaluations that underline the importance of its modular approach.", "key_contributions": ["Introduction of the Coherent Multimodal Reasoning Framework (CMRF) for LVLMs", "Development of a novel Multimodal Daily Activity Reasoning (MDAR) dataset", "Demonstration of enhanced performance in complex, multi-step reasoning tasks through iterative refinement."], "limitations": "", "keywords": ["multimodal reasoning", "large language models", "vision-language models", "iterative refinement", "common sense reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03182", "pdf": "https://arxiv.org/pdf/2508.03182.pdf", "abs": "https://arxiv.org/abs/2508.03182", "title": "StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation", "authors": ["Sangho Suh", "Michael Lai", "Kevin Pu", "Steven P. Dow", "Tovi Grossman"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Design processes involve exploration, iteration, and movement across\ninterconnected stages such as persona creation, problem framing, solution\nideation, and prototyping. However, time and resource constraints often hinder\ndesigners from exploring broadly, collecting feedback, and revisiting earlier\nassumptions-making it difficult to uphold core design principles in practice.\nTo better understand these challenges, we conducted a formative study with 15\nparticipants-comprised of UX practitioners, students, and instructors. Based on\nthe findings, we developed StoryEnsemble, a tool that integrates AI into a\nnode-link interface and leverages forward and backward propagation to support\ndynamic exploration and iteration across the design process. A user study with\n10 participants showed that StoryEnsemble enables rapid, multi-directional\niteration and flexible navigation across design stages. This work advances our\nunderstanding of how AI can foster more iterative design practices by\nintroducing novel interactions that make exploration and iteration more fluid,\naccessible, and engaging.", "AI": {"tldr": "This paper presents StoryEnsemble, an AI-integrated design tool that facilitates iterative design processes through a node-link interface, allowing for dynamic exploration and navigation across various design stages.", "motivation": "To address the challenges faced by designers due to time and resource constraints, which limit exploration and feedback collection in the design process.", "method": "A formative study with 15 design practitioners to understand the challenges in design processes, followed by the development and user testing of StoryEnsemble with 10 participants.", "result": "User study results showed that StoryEnsemble supports rapid, multi-directional iteration and allows flexible navigation across design stages, enhancing the design process.", "conclusion": "The introduction of StoryEnsemble advances the understanding of AI's role in improving iterative design practices by enabling more fluid and engaging exploration.", "key_contributions": ["Introduction of the StoryEnsemble tool that leverages AI for design iteration", "Empirical insights from user studies regarding AI's impact on design processes", "Demonstrated novel interactions for design exploration and iteration"], "limitations": "", "keywords": ["Design processes", "Human-Computer Interaction", "AI integration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02901", "pdf": "https://arxiv.org/pdf/2508.02901.pdf", "abs": "https://arxiv.org/abs/2508.02901", "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations", "authors": ["Osama Khalid", "Sanvesh Srivastava", "Padmini Srinivasan"], "categories": ["cs.CL"], "comment": null, "summary": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%.", "AI": {"tldr": "This paper explores the use of sensorial language in communication, proposing a new model (SLIM-LLMs) that leverages reduced-rank regression for stylistic prediction while maintaining performance with fewer parameters.", "motivation": "The study aims to investigate the role of sensorial language in communication and its relationship with traditional stylistic features, enhancing understanding of language models.", "method": "The authors employ a Reduced-Rank Ridge Regression (R4) approach to analyze the effectiveness of low-dimensional latent representations of LIWC features in predicting sensorial language.", "result": "The findings show that SLIM-LLMs, which utilize low-rank LIWC features, can effectively match the performance of full-scale models while reducing the number of parameters by up to 80%.", "conclusion": "By introducing Stylometrically Lean Interpretable Models (SLIM-LLMs), this research highlights a new way to model language that balances complexity and interpretability in natural language processing.", "key_contributions": ["Introduction of SLIM-LLMs for sensorial language modeling", "Demonstration of effective stylistic prediction with reduced features", "Evaluation across multiple genres showing performance retention with fewer parameters"], "limitations": "", "keywords": ["sensorial language", "stylistic features", "SLIM-LLMs", "Reduced-Rank Ridge Regression", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03216", "pdf": "https://arxiv.org/pdf/2508.03216.pdf", "abs": "https://arxiv.org/abs/2508.03216", "title": "Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse", "authors": ["Hikari Yanagawa", "Yuichi Hiroi", "Satomi Tokida", "Yuji Hatada", "Takefumi Hiraki"], "categories": ["cs.HC", "cs.AI"], "comment": "11 pages + supplement 3 pages. To appear in IEEE ISMAR 2025", "summary": "While commercial metaverse platforms offer diverse user-generated content,\nthey lack effective navigation assistance that can dynamically adapt to users'\ninterests and intentions. Although previous research has investigated on-demand\nagents in controlled environments, implementation in commercial settings with\ndiverse world configurations and platform constraints remains challenging.\n  We present Navigation Pixie, an on-demand navigation agent employing a\nloosely coupled architecture that integrates structured spatial metadata with\nLLM-based natural language processing while minimizing platform dependencies,\nwhich enables experiments on the extensive user base of commercial metaverse\nplatforms. Our cross-platform experiments on commercial metaverse platform\nCluster with 99 PC client and 94 VR-HMD participants demonstrated that\nNavigation Pixie significantly increased dwell time and free exploration\ncompared to fixed-route and no-agent conditions across both platforms.\nSubjective evaluations revealed consistent on-demand preferences in PC\nenvironments versus context-dependent social perception advantages in VR-HMD.\nThis research contributes to advancing VR interaction design through\nconversational spatial navigation agents, establishes cross-platform evaluation\nmethodologies revealing environment-dependent effectiveness, and demonstrates\nempirical experimentation frameworks for commercial metaverse platforms.", "AI": {"tldr": "Navigation Pixie is an on-demand navigation agent for metaverse platforms that enhances user exploration through adaptive navigation assistance.", "motivation": "Commercial metaverse platforms lack effective navigation tools that adapt to user interests and intentions in dynamic environments.", "method": "Implemented Navigation Pixie using a loosely coupled architecture to integrate spatial metadata with LLM-based NLP, tested in cross-platform settings.", "result": "Significant increases in user dwell time and exploration were observed compared to static navigation conditions across PC and VR platforms.", "conclusion": "Navigation Pixie shows promise in improving VR interaction design and offers frameworks for empirical experimentation in metaverse environments.", "key_contributions": ["Integration of spatial metadata with LLM-based NLP for navigation", "Cross-platform evaluation methodologies for navigation agents", "Empirical experimentation frameworks tailored for commercial metaverse platforms"], "limitations": "Dependent on platform configurations and user context for effectiveness.", "keywords": ["Navigation", "Metaverse", "User Experience", "LLM", "Virtual Reality"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.02931", "pdf": "https://arxiv.org/pdf/2508.02931.pdf", "abs": "https://arxiv.org/abs/2508.02931", "title": "Can LLMs Generate High-Quality Task-Specific Conversations?", "authors": ["Shengqi Li", "Amarnath Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.", "AI": {"tldr": "This paper presents a framework for precisely controlling the quality of conversations generated by large language models through a set of nine parameters.", "motivation": "To address challenges in conversation generation such as coherence, knowledge progression, and consistency in dialogue.", "method": "The framework explores nine key parameters across six dimensions affecting dialogue properties, tested on state-of-the-art large language models.", "result": "Experiments showed that parameter-based control significantly influenced conversation properties in generated dialogues.", "conclusion": "The framework offers a standardized approach for conversation quality control, relevant for fields like education and customer service, with future work planned for parameter enhancement and evaluation datasets.", "key_contributions": ["Introduction of a parameterization framework for controlling dialogue quality", "Demonstration of statistically significant improvements in conversation properties", "Potential applications in various sectors like education and therapy"], "limitations": "", "keywords": ["conversation quality", "large language models", "dialogue properties", "parameter control", "HCI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.03281", "pdf": "https://arxiv.org/pdf/2508.03281.pdf", "abs": "https://arxiv.org/abs/2508.03281", "title": "Quo-Vadis Multi-Agent Automotive Research? Insights from a Participatory Workshop and Questionnaire", "authors": ["Pavlo Bazilinskyy", "Francesco Walker", "Debargha Dey", "Tram Thi Minh Tran", "Hyungchai Park", "Hyochang Kim", "Hyunmin Kang", "Patrick Ebel"], "categories": ["cs.HC"], "comment": null, "summary": "The transition to mixed-traffic environments that involve automated vehicles,\nmanually operated vehicles, and vulnerable road users presents new challenges\nfor human-centered automotive research. Despite this, most studies in the\ndomain focus on single-agent interactions. This paper reports on a\nparticipatory workshop (N = 15) and a questionnaire (N = 19) conducted during\nthe AutomotiveUI '24 conference to explore the state of multi-agent automotive\nresearch. The participants discussed methodological challenges and\nopportunities in real-world settings, simulations, and computational modeling.\nKey findings reveal that while the value of multi-agent approaches is widely\nrecognized, practical and technical barriers hinder their implementation. The\nstudy highlights the need for interdisciplinary methods, better tools, and\nsimulation environments that support scalable, realistic, and ethically\ninformed multi-agent research.", "AI": {"tldr": "The paper discusses the challenges and opportunities in multi-agent automotive research through a participatory workshop and questionnaire.", "motivation": "To address the transition to mixed-traffic environments with automated and manually operated vehicles, emphasizing the lack of research in multi-agent interactions despite its recognition as valuable.", "method": "Conducted a participatory workshop with 15 participants and a questionnaire with 19 participants during the AutomotiveUI '24 conference.", "result": "Identified barriers to implementing multi-agent approaches, with a recognition of their value in automotive research.", "conclusion": "Calls for interdisciplinary methods, better tools, and simulation environments to advance multi-agent research in automotive settings.", "key_contributions": ["Exploration of multi-agent automotive research through participatory engagement.", "Identification of practical and technical barriers in implementing multi-agent approaches.", "Emphasis on the need for scalable and ethically informed simulation environments."], "limitations": "", "keywords": ["multi-agent systems", "automotive research", "human-centered design", "interdisciplinary methods", "simulation environments"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02997", "pdf": "https://arxiv.org/pdf/2508.02997.pdf", "abs": "https://arxiv.org/abs/2508.02997", "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.", "AI": {"tldr": "The paper presents a novel method for detecting adversarial and jailbreak prompts in Large Language Models (LLMs) using Contextual Co-occurrence Matrices, achieving a notable F1 score of 0.83 with minimal labeled data.", "motivation": "The paper addresses the vulnerability of large language models to attacks, such as jailbreaks that lead to harmful outputs, and the need for effective detection methods.", "method": "A novel approach utilizing the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for detecting adversarial prompts.", "result": "The proposed method demonstrates an F1 score of 0.83 using only 0.5% of labeled prompts, representing a 96.6% improvement over existing baselines. It is also significantly faster, with speedups ranging from 2.3 to 128.4 times compared to baseline models.", "conclusion": "The study provides an effective means for detecting harmful prompts in LLMs, showcasing efficacy in data-scarce environments and contributing to the field by making the implementation publicly available.", "key_contributions": ["Development of a novel detection method using Contextual Co-occurrence Matrices", "Achieving an F1 score of 0.83 with minimal labeled data", "Public availability of the implementation for reproducibility"], "limitations": "", "keywords": ["Large Language Models", "adversarial prompts", "Contextual Co-occurrence Matrix", "detection methods", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.03293", "pdf": "https://arxiv.org/pdf/2508.03293.pdf", "abs": "https://arxiv.org/abs/2508.03293", "title": "Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach", "authors": ["Duc-An Nguyen", "Clara Colombatto", "Steve Fleming", "Ingmar Posner", "Nick Hawes", "Raunak Bhattacharyya"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Joint human-AI inference holds immense potential to improve outcomes in\nhuman-supervised robot missions. Current day missions are generally in the\nAI-assisted setting, where the human operator makes the final inference based\non the AI recommendation. However, due to failures in human judgement on when\nto accept or reject the AI recommendation, complementarity is rarely achieved.\nWe investigate joint human-AI inference where the inference made with higher\nconfidence is selected. Through a user study with N=100 participants on a\nrepresentative simulated robot teleoperation task, specifically studying the\ninference of robots' control delays we show that: a) Joint inference accuracy\nis higher and its extent is regulated by the confidence calibration of the AI\nagent, and b) Humans change their inferences based on AI recommendations and\nthe extent and direction of this change is also regulated by the confidence\ncalibration of the AI agent. Interestingly, our results show that pairing\npoorly-calibrated AI-DSS with humans hurts performance instead of helping the\nteam, reiterating the need for AI-based decision support systems with good\nmetacognitive sensitivity. To the best of our knowledge, our study presents the\nfirst application of a maximum-confidence-based heuristic for joint human-AI\ninference within a simulated robot teleoperation task.", "AI": {"tldr": "This study explores joint human-AI inference in robot teleoperation, showing that the accuracy of decisions improves when selecting the inference with higher confidence, highlighting the critical role of AI confidence calibration.", "motivation": "To improve outcomes in human-supervised robot missions by investigating joint human-AI inference mechanisms, particularly in relation to confidence calibration of AI recommendations.", "method": "A user study was conducted with 100 participants engaging in a simulated robot teleoperation task, focusing on the inference of robots' control delays under varying AI confidence levels.", "result": "The results indicated that joint inference accuracy improved with better AI confidence calibration, and human inferences were influenced by AI recommendations, also affected by AI's confidence sensitivity.", "conclusion": "Well-calibrated AI decision support systems are crucial for enhancing performance in joint inference tasks, as poorly-calibrated systems can negatively impact outcomes.", "key_contributions": ["Introduction of a maximum-confidence-based heuristic for joint human-AI inference", "Demonstration of the importance of AI confidence calibration on decision-making", "Evidence that poor calibration of AI can detrimentally affect team performance."], "limitations": "", "keywords": ["human-AI inference", "robot teleoperation", "confidence calibration", "AI decision support systems", "user study"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037.pdf", "abs": "https://arxiv.org/abs/2508.03037", "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "This study analyzes English-language discourse on AI-generated art from 2013 to 2025, revealing misalignment between artist concerns and media narratives, and advocates for greater transparency and engagement with artists.", "motivation": "The voices of artists in the conversation about AI-generated art are often marginalized, prompting a need to analyze discourse around this topic to understand their perspectives better.", "method": "A twelve-year analysis of 439 curated excerpts from various sources (opinion articles, news reports, blogs, legal filings, transcripts) to identify thematic clusters concerning AI-generated art.", "result": "Identified five stable thematic clusters reflecting artist concerns and how technical jargon can serve as a gatekeeping mechanism, sidelining urgent issues for artists.", "conclusion": "The findings suggest a need for transparent engagement with artist perspectives in discussions about AI's impact on creative labor and provide a methodology for future research.", "key_contributions": ["BERTopic-based methodology for discourse analysis", "Identification of thematic clusters regarding AI-generated art", "Call for greater transparency in artist engagement"], "limitations": "", "keywords": ["AI-generated art", "Discourse analysis", "Artist perspectives", "Creative labor", "Transparency"], "importance_score": 6, "read_time_minutes": 18}}
{"id": "2508.03355", "pdf": "https://arxiv.org/pdf/2508.03355.pdf", "abs": "https://arxiv.org/abs/2508.03355", "title": "Remini: Leveraging Chatbot-Mediated Mutual Reminiscence for Promoting Positive Affect and Feeling of Connectedness among Loved Ones", "authors": ["Zhuoqun Jiang", "ShunYi Yeo", "Wei Xuan Donovan Seow", "Simon Perrault"], "categories": ["cs.HC"], "comment": "Camera-ready submission for PACM HCI, CSCW 2025", "summary": "Mutual reminiscence, defined as revisiting shared positive memories through\nreciprocal self-disclosure, strengthens emotional bonds, enhances well-being,\nand deepens intimacy. However, most technology-mediated reminiscence tools\nemphasize individual reflection or one-way storytelling, which overlooks the\ndynamic, interactive dialogue essential for meaningful mutual reminiscence. To\naddress this limitation, we introduce Remini, a chatbot designed to support\nreciprocal self-disclosure between close partners such as couples, friends, or\nfamily members. Grounded in the Social Functions of Autobiographical Memory\n(SFAM) framework, Remini uses conversational AI to guide emotionally rich\nexchanges through five narrative phases: rapport building, memory narration,\nelaboration, reflection, and summary. In a mixed-method, both between- and\nwithin- subjects study (N = 48, 24 dyads), we compare Remini to a baseline\nchatbot that offers minimal memory-trigger prompts. Our findings show that\nstructured guidance from Remini significantly improves positive affect, feeling\nof connection, and engagement. It also fosters more detailed narrative\nco-construction and greater reciprocal self-disclosure. Participant feedback\nhighlights the practical value, perceived benefits, and design considerations\nof chatbot-mediated reminiscence. We contribute empirically grounded design\nimplications for conversational agents that strengthen human connection through\nmutual reminiscence.", "AI": {"tldr": "The paper introduces Remini, a chatbot that facilitates mutual reminiscence among close partners, improving emotional bonds and well-being through guided interactions.", "motivation": "Current reminiscence tools focus on individual reflections, neglecting the importance of mutual, interactive dialogue for meaningful connections.", "method": "A mixed-method study with 48 participants (24 dyads) comparing Remini to a baseline chatbot featuring minimal prompts, assessing emotional outcomes and engagement.", "result": "Remini significantly enhances positive affect, feelings of connection, and engagement, leading to detailed narrative co-construction and increased reciprocal self-disclosure.", "conclusion": "The chatbot demonstrates the potential to strengthen human connections through structured reminiscence prompts, providing valuable insights for designing conversational agents.", "key_contributions": ["Introduction of Remini chatbot for mutual reminiscence", "Empirical evidence of enhanced emotional outcomes", "Design implications for conversational agents in HCI"], "limitations": "", "keywords": ["chatbot", "mutual reminiscence", "social functions of autobiographical memory", "emotional bonding", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03098", "pdf": "https://arxiv.org/pdf/2508.03098.pdf", "abs": "https://arxiv.org/abs/2508.03098", "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation", "authors": ["Haoran Wang", "Xiongxiao Xu", "Baixiang Huang", "Kai Shu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.", "AI": {"tldr": "This paper presents Privacy-Aware Decoding (PAD), a defense mechanism to protect sensitive data in Retrieval-Augmented Generation systems by adding calibrated noise during response generation.", "motivation": "To address privacy risks associated with the extraction of private information in Retrieval-Augmented Generation systems, especially when using large language models.", "method": "The paper proposes Privacy-Aware Decoding (PAD), which injects calibrated Gaussian noise into token logits with mechanisms for confidence-based screening, efficient sensitivity estimation, and context-aware noise calibration.", "result": "PAD significantly reduces private information leakage while maintaining response utility, outperforming prior defenses and functioning without the need for retraining or extensive preprocessing.", "conclusion": "PAD represents a scalable solution for enhancing privacy in sensitive applications of RAG by improving decoding strategies.", "key_contributions": ["Introduction of Privacy-Aware Decoding (PAD) as a lightweight defense for RAG systems.", "Integration of Renyi Differential Privacy to track cumulative privacy loss during generation.", "Demonstration of improved privacy protection without the need for model retraining."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Privacy-Aware Decoding", "Large Language Models", "Differential Privacy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03430", "pdf": "https://arxiv.org/pdf/2508.03430.pdf", "abs": "https://arxiv.org/abs/2508.03430", "title": "The Science Fiction Science Method", "authors": ["Iyad Rahwan", "Azim Shariff", "Jean-François Bonnefon"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Predicting the social and behavioral impact of future technologies, before\nthey are achieved, would allow us to guide their development and regulation\nbefore these impacts get entrenched. Traditionally, this prediction has relied\non qualitative, narrative methods. Here we describe a method which uses\nexperimental methods to simulate future technologies, and collect quantitative\nmeasures of the attitudes and behaviors of participants assigned to controlled\nvariations of the future. We call this method 'science fiction science'. We\nsuggest that the reason why this method has not been fully embraced yet,\ndespite its potential benefits, is that experimental scientists may be\nreluctant to engage in work facing such serious validity threats as science\nfiction science. To address these threats, we consider possible constraints on\nthe kind of technology that science fiction science may study, as well as the\nunconventional, immersive methods that science fiction science may require. We\nseek to provide perspective on the reasons why this method has been\nmarginalized for so long, what benefits it would bring if it could be built on\nstrong yet unusual methods, and how we can normalize these methods to help the\ndiverse community of science fiction scientists to engage in a virtuous cycle\nof validity improvement.", "AI": {"tldr": "This paper proposes 'science fiction science', a method using experimental simulation of future technologies to quantitatively assess their social and behavioral impacts, while addressing validity concerns.", "motivation": "To predict the social and behavioral impact of future technologies before their actualization, guiding their development and regulation.", "method": "The authors introduce experimental methods that simulate future technologies and collect quantitative measures from participants under controlled scenarios.", "result": "The proposed method provides a framework to quantitatively explore the impacts of technologies that are not yet realized, which has been largely neglected in favor of qualitative approaches.", "conclusion": "Normalizing the use of unconventional methods in science fiction science could enhance societal engagement with emerging technologies and improve validity in predictions.", "key_contributions": ["Introduction of 'science fiction science', a novel experimental approach.", "Addressing validity threats associated with predicting future technology impacts.", "Discussion of immersive methods for engaging diverse audiences in technology simulation."], "limitations": "The method may not be applicable to all technology types and could encounter skepticism within the scientific community.", "keywords": ["science fiction", "experimental methods", "future technologies", "behavioral impact", "validity threats"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.03110", "pdf": "https://arxiv.org/pdf/2508.03110.pdf", "abs": "https://arxiv.org/abs/2508.03110", "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.", "AI": {"tldr": "The paper introduces TPARAG, a novel method to enhance the security of RAG systems against token-level attacks.", "motivation": "To address security vulnerabilities in retrieval-augmented generation (RAG) systems, which can be manipulated by malicious content retrieved from external databases.", "method": "TPARAG utilizes a lightweight white-box LLM to create and optimize malicious passages at the token level, addressing both white-box and black-box RAG scenarios.", "result": "Extensive experiments demonstrate that TPARAG outperforms existing methods in both retrieval and end-to-end attack effectiveness, revealing significant vulnerabilities in RAG systems.", "conclusion": "The findings highlight critical weaknesses in RAG pipelines and suggest avenues for enhancing their security and robustness.", "key_contributions": ["Introduction of TPARAG for token-level attacks on RAG", "Demonstrated superiority over previous attack methods", "Insights into RAG system vulnerabilities"], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "token-level attack"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03547", "pdf": "https://arxiv.org/pdf/2508.03547.pdf", "abs": "https://arxiv.org/abs/2508.03547", "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs and Vision Models", "authors": ["Ada Yi Zhao", "Aditya Gunturu", "Ellen Yi-Luen Do", "Ryo Suzuki"], "categories": ["cs.HC"], "comment": "To appear at UIST 2025", "summary": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows.", "AI": {"tldr": "Guided Reality is an AR system that enhances LLM-generated instructions with visual guidance integrated into the user's physical environment.", "motivation": "Current LLM-based AR guidance lacks effective visual augmentations, which are necessary for better user understanding of physical tasks.", "method": "The system uses LLMs and vision models to generate instructions, identify visual guidance types, extract spatial information, and embed visual help in real-world contexts.", "result": "User studies showed that Guided Reality effectively supports task execution by providing contextually embedded visual guidance based on step-by-step instructions.", "conclusion": "Guided Reality can potentially enhance training workflows and improve user interactions with physical tasks in augmented reality settings.", "key_contributions": ["Automated generation of visual guidance from LLMs", "Identification strategy for visual guidance based on task steps", "Embedding of guidance in physical space for real-world task execution."], "limitations": "Limited user study size (N=16); further research needed to explore broader applications.", "keywords": ["Augmented Reality", "Large Language Models", "Visual Guidance", "Human-Computer Interaction", "Task Execution"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.03112", "pdf": "https://arxiv.org/pdf/2508.03112.pdf", "abs": "https://arxiv.org/abs/2508.03112", "title": "Cross-lingual Opinions and Emotions Mining in Comparable Documents", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "categories": ["cs.CL", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Comparable texts are topic-aligned documents in multiple languages that are\nnot direct translations. They are valuable for understanding how a topic is\ndiscussed across languages. This research studies differences in sentiments and\nemotions across English-Arabic comparable documents. First, texts are annotated\nwith sentiment and emotion labels. We apply a cross-lingual method to label\ndocuments with opinion classes (subjective/objective), avoiding reliance on\nmachine translation. To annotate with emotions (anger, disgust, fear, joy,\nsadness, surprise), we manually translate the English WordNet-Affect (WNA)\nlexicon into Arabic, creating bilingual emotion lexicons used to label the\ncomparable corpora. We then apply a statistical measure to assess the agreement\nof sentiments and emotions in each source-target document pair. This comparison\nis especially relevant when the documents originate from different sources. To\nour knowledge, this aspect has not been explored in prior literature. Our study\nincludes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera\n(JSC). Results show that sentiment and emotion annotations align when articles\ncome from the same news agency and diverge when they come from different ones.\nThe proposed method is language-independent and generalizable to other language\npairs.", "AI": {"tldr": "This research analyzes sentiment and emotion differences in English-Arabic comparable documents, revealing alignment in sentiments from the same news sources and divergence from different ones.", "motivation": "To understand how sentiment and emotions are discussed across languages using comparable documents, rather than through direct translations.", "method": "The study annotates documents with sentiment and emotion labels, utilizing a cross-lingual method without machine translation and employing bilingual emotion lexicons created from English WordNet-Affect for Arabic.", "result": "Sentiment and emotion annotations align for articles from the same news agency (like Euronews, BBC, and Al-Jazeera) but diverge for articles from different agencies, highlighting important cross-linguistic sentiment patterns.", "conclusion": "The proposed method is language-independent and can be generalized to other language pairs, providing insights into multilingual sentiment analysis.", "key_contributions": ["Development of bilingual emotion lexicons from WordNet-Affect", "Introduction of a cross-lingual method for sentiment labeling without machine translation", "Novel insights into sentiment and emotion alignment based on news agency sources."], "limitations": "The study focuses solely on English-Arabic document pairs and may not fully represent other language pairs or domains.", "keywords": ["cross-lingual sentiment analysis", "emotion labeling", "comparable documents", "bilingual lexicons", "multilingual communication"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.03630", "pdf": "https://arxiv.org/pdf/2508.03630.pdf", "abs": "https://arxiv.org/abs/2508.03630", "title": "SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides", "authors": ["Zhuohao Jerry Zhang", "Ruiqi Chen", "Mingyuan Zhong", "Jacob O. Wobbrock"], "categories": ["cs.HC"], "comment": "UIST 2025", "summary": "Automated evaluation of specific graphic designs like presentation slides is\nan open problem. We present SlideAudit, a dataset for automated slide\nevaluation. We collaborated with design experts to develop a thorough taxonomy\nof slide design flaws. Our dataset comprises 2400 slides collected and\nsynthesized from multiple sources, including a subset intentionally modified\nwith specific design problems. We then fully annotated them using our taxonomy\nthrough strictly trained crowdsourcing from Prolific. To evaluate whether AI is\ncapable of identifying design flaws, we compared multiple large language models\nunder different prompting strategies, and with an existing design critique\npipeline. We show that AI models struggle to accurately identify slide design\nflaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting\ntechniques leveraging our taxonomy achieved the highest performance. We further\nconducted a remediation study to assess AI's potential for improving slides.\nAmong 82.0% of slides that showed significant improvement, 87.8% of them were\nimproved more with our taxonomy, further demonstrating its utility.", "AI": {"tldr": "The paper introduces SlideAudit, a dataset aimed at the automated evaluation of presentation slides, revealing the challenges AI faces in identifying design flaws.", "motivation": "Automated evaluation of graphic designs, specifically presentation slides, remains an open problem, necessitating robust datasets and methodologies for assessment.", "method": "Created SlideAudit, a dataset of 2400 slides annotated for design flaws using a carefully developed taxonomy and trained crowdsourcing; tested various large language models on their ability to identify these flaws.", "result": "AI models showed limited accuracy with F1 scores between 0.331 and 0.655; however, improved prompting strategies using the taxonomy yielded better results, and a remediation study indicated high potential for improving slide quality.", "conclusion": "The introduction of SlideAudit and its taxonomy highlights AI's limitations in design evaluation but suggests strategies for improvement; the dataset serves as a valuable resource for future research in design automation.", "key_contributions": ["Introduction of SlideAudit dataset for slide evaluation", "Development of a comprehensive taxonomy for slide design flaws", "Demonstrated effectiveness of taxonomy in AI remediation studies"], "limitations": "AI models still struggle significantly with accurately identifying flaws despite the dataset and taxonomy; additional methods may be required to enhance performance further.", "keywords": ["slide evaluation", "AI design critique", "taxonomy", "crowdsourcing", "presentation design"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.03137", "pdf": "https://arxiv.org/pdf/2508.03137.pdf", "abs": "https://arxiv.org/abs/2508.03137", "title": "Long Story Generation via Knowledge Graph and Literary Theory", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.", "AI": {"tldr": "This paper presents a multi-agent Story Generator using large language models to enhance long text generation by mitigating theme drift and improving narrative coherence.", "motivation": "To address issues of theme drift and incoherent plots in long text generation based on outlines, which have been prevalent in previous methodologies.", "method": "The proposed method incorporates a memory storage model, consisting of long-term and short-term memory, along with a story theme obstacle framework that uses literary narratology theory to enhance storytelling. It also includes a multi-agent interaction stage for writer-reader simulation to refine the story based on feedback.", "result": "Evaluations show that the multi-agent Story Generator produces higher-quality long stories compared to existing methods.", "conclusion": "The introduction of agent interaction and memory models significantly improves the narrative quality and coherence in long story generation tasks.", "key_contributions": ["Multi-agent structure using LLMs for story generation", "Introduction of a memory storage model to avoid theme drift", "Development of a story theme obstacle framework to enhance narrative engagement"], "limitations": "", "keywords": ["long text generation", "story generation", "large language models", "memory models", "multi-agent systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03651", "pdf": "https://arxiv.org/pdf/2508.03651.pdf", "abs": "https://arxiv.org/abs/2508.03651", "title": "Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired", "authors": ["Ruei-Che Chang", "Rosiana Natalie", "Wenqian Xu", "Jovan Zheng Feng Yap", "Anhong Guo"], "categories": ["cs.HC", "cs.AI"], "comment": "ACM ASSETS 2025", "summary": "Recent advancements in large multimodal models have provided blind or\nvisually impaired (BVI) individuals with new capabilities to interpret and\nengage with the real world through interactive systems that utilize live video\nfeeds. However, the potential benefits and challenges of such capabilities to\nsupport diverse real-world assistive tasks remain unclear. In this paper, we\npresent findings from an exploratory study with eight BVI participants.\nParticipants used ChatGPT's Advanced Voice with Video, a state-of-the-art live\nvideo AI released in late 2024, in various real-world scenarios, from locating\nobjects to recognizing visual landmarks, across unfamiliar indoor and outdoor\nenvironments. Our findings indicate that current live video AI effectively\nprovides guidance and answers for static visual scenes but falls short in\ndelivering essential live descriptions required in dynamic situations. Despite\ninaccuracies in spatial and distance information, participants leveraged the\nprovided visual information to supplement their mobility strategies. Although\nthe system was perceived as human-like due to high-quality voice interactions,\nassumptions about users' visual abilities, hallucinations, generic responses,\nand a tendency towards sycophancy led to confusion, distrust, and potential\nrisks for BVI users. Based on the results, we discuss implications for\nassistive video AI agents, including incorporating additional sensing\ncapabilities for real-world use, determining appropriate intervention timing\nbeyond turn-taking interactions, and addressing ecological and safety concerns.", "AI": {"tldr": "Exploratory study on live video AI for blind or visually impaired individuals highlighting benefits and challenges in real-world applications.", "motivation": "To investigate the potential of live video AI in assisting blind or visually impaired individuals in navigating and interacting with their environments.", "method": "An exploratory study with eight blind or visually impaired participants using ChatGPT's Advanced Voice with Video in various real-world scenarios.", "result": "The AI provided useful guidance for static scenes but struggled with dynamic situations due to inaccuracies in spatial information and assumptions about user capabilities.", "conclusion": "The study reveals both the potential and limitations of live video AI, suggesting enhancements like better sensing capabilities and addressing user trust issues.", "key_contributions": ["Insights into BVI users' interaction with live video AI", "Identification of challenges in dynamic environments", "Recommendations for improving assistive AI design"], "limitations": "Participants had varying degrees of experiences with technology, which could influence results and perceptions.", "keywords": ["Blind Individuals", "Assistive Technology", "Live Video AI", "Human-Computer Interaction", "User Experience"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03140", "pdf": "https://arxiv.org/pdf/2508.03140.pdf", "abs": "https://arxiv.org/abs/2508.03140", "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior", "authors": ["Junyao Yang", "Jianwei Wang", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 7 figures", "summary": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability.", "AI": {"tldr": "The paper presents RCP-Merging, a novel framework for merging Long Chain-of-Thought (CoT) models with domain-specific LLMs, maintaining reasoning performance while enhancing domain task outcomes.", "motivation": "To create a dual-capability model that combines long CoT reasoning and domain-specific knowledge without high computational costs, while overcoming challenges in current merging methods that degrade reasoning abilities.", "method": "Introducing RCP-Merging, which utilizes a reasoning capability indicator to prioritize the preservation of core long CoT model weights while selectively merging domain-specific weights.", "result": "RCP-Merging improved domain task performance by 9.5% and 9.2% on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance domains without degrading the long CoT reasoning capability.", "conclusion": "RCP-Merging effectively merges reasoning models with domain-specific LLMs, achieving better performance in specific tasks while safeguarding the intrinsic reasoning abilities of the models.", "key_contributions": ["Introduction of RCP-Merging framework", "Enhancement of performance in domain-specific tasks", "Maintaining reasoning capabilities during model merging"], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought", "model merging", "domain-specific knowledge", "reasoning capability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03673", "pdf": "https://arxiv.org/pdf/2508.03673.pdf", "abs": "https://arxiv.org/abs/2508.03673", "title": "Classifying Epistemic Relationships in Human-AI Interaction: An Exploratory Approach", "authors": ["Shengnan Yang", "Rongqian Ma"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As AI systems become integral to knowledge-intensive work, questions arise\nnot only about their functionality but also their epistemic roles in human-AI\ninteraction. While HCI research has proposed various AI role typologies, it\noften overlooks how AI reshapes users' roles as knowledge contributors. This\nstudy examines how users form epistemic relationships with AI-how they assess,\ntrust, and collaborate with it in research and teaching contexts. Based on 31\ninterviews with academics across disciplines, we developed a five-part codebook\nand identified five relationship types: Instrumental Reliance, Contingent\nDelegation, Co-agency Collaboration, Authority Displacement, and Epistemic\nAbstention. These reflect variations in trust, assessment modes, tasks, and\nhuman epistemic status. Our findings show that epistemic roles are dynamic and\ncontext-dependent. We argue for shifting beyond static metaphors of AI toward a\nmore nuanced framework that captures how humans and AI co-construct knowledge,\nenriching HCI's understanding of the relational and normative dimensions of AI\nuse.", "AI": {"tldr": "This study explores how users interact with AI in knowledge-intensive work, focusing on their epistemic roles and relationships with AI.", "motivation": "To examine how AI reshapes users' roles as knowledge contributors in research and teaching.", "method": "Conducted 31 interviews with academics and developed a five-part codebook to categorize user-AI relationships.", "result": "Identified five relationship types: Instrumental Reliance, Contingent Delegation, Co-agency Collaboration, Authority Displacement, and Epistemic Abstention, highlighting the dynamic nature of epistemic roles.", "conclusion": "Epistemic roles in human-AI interaction are context-dependent and require a nuanced framework to understand co-construction of knowledge.", "key_contributions": ["Development of a five-part codebook categorizing AI-user relationships", "Identification of five distinct relationship types", "Proposing a framework for understanding the dynamic and contextual nature of epistemic roles in HCI."], "limitations": "", "keywords": ["Human-AI interaction", "Epistemic roles", "Trust and collaboration", "Knowledge-intensive work", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03178", "pdf": "https://arxiv.org/pdf/2508.03178.pdf", "abs": "https://arxiv.org/abs/2508.03178", "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following", "authors": ["Chenyang Wang", "Liang Wen", "Shousheng Jia", "Xiangzheng Zhang", "Liang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 10 figures, 7 tables", "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.", "AI": {"tldr": "The paper proposes a framework to improve instruction adherence in LLMs by addressing lazy reasoning, using a method that involves complex constraints and a novel supervised fine-tuning strategy to enhance reasoning abilities.", "motivation": "To enhance the instruction adherence of LLMs, which remains inconsistent especially for complex directives due to lazy reasoning.", "method": "The authors generate complex instructions, filter for valid prompts, and employ rejection sampling to create a high-quality dataset. They then apply an entropy-preserving supervised fine-tuning (Entropy-SFT) and a reinforcement learning method (TEA-RL) to improve reasoning mechanisms.", "result": "The approach led to significant performance improvements on instruction-following benchmarks, with the Light-IF-32B model outperforming larger and closed-source models.", "conclusion": "The proposed framework successfully enhances reasoning abilities in LLMs, facilitating better adherence to complex instructions and demonstrating the efficacy of their methodology.", "key_contributions": ["Introduction of a comprehensive framework for improving LLM reasoning", "Implementation of novel fine-tuning and reinforcement learning strategies", "Demonstrated performance improvements across model scales in instruction adherence tasks."], "limitations": "", "keywords": ["Large Language Models", "Instruction Adherence", "Reinforcement Learning", "Fine-Tuning", "Reasoning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.03181", "pdf": "https://arxiv.org/pdf/2508.03181.pdf", "abs": "https://arxiv.org/abs/2508.03181", "title": "Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification", "authors": ["Lukas Pätz", "Moritz Beyer", "Jannik Späth", "Lasse Bohlen", "Patrick Zschech", "Mathias Kraus", "Julian Rosenberger"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at 20th International Conference on Wirtschaftsinformatik\n  (WI25); September 2025, M\\\"unster, Germany", "summary": "This study investigates political discourse in the German parliament, the\nBundestag, by analyzing approximately 28,000 parliamentary speeches from the\nlast five years. Two machine learning models for topic and sentiment\nclassification were developed and trained on a manually labeled dataset. The\nmodels showed strong classification performance, achieving an area under the\nreceiver operating characteristic curve (AUROC) of 0.94 for topic\nclassification (average across topics) and 0.89 for sentiment classification.\nBoth models were applied to assess topic trends and sentiment distributions\nacross political parties and over time. The analysis reveals remarkable\nrelationships between parties and their role in parliament. In particular, a\nchange in style can be observed for parties moving from government to\nopposition. While ideological positions matter, governing responsibilities also\nshape discourse. The analysis directly addresses key questions about the\nevolution of topics, sentiment dynamics, and party-specific discourse\nstrategies in the Bundestag.", "AI": {"tldr": "This study analyzes 28,000 speeches from the German Bundestag using machine learning to classify topics and sentiments, revealing significant party dynamics and discourse changes.", "motivation": "To understand political discourse dynamics in the German Bundestag and how party roles influence speech style and sentiment over time.", "method": "Two machine learning models were developed for topic and sentiment classification, trained on a manually labeled dataset of speeches, achieving high classification performance.", "result": "The models achieved an AUROC of 0.94 for topic classification and 0.89 for sentiment classification, uncovering trends and relationships in political discourse.", "conclusion": "The analysis shows that shifts in party roles between government and opposition influence discourse styles, and that both ideological beliefs and governing responsibilities play significant roles.", "key_contributions": ["Development of machine learning models for classifying parliamentary speeches", "Insights into political discourse dynamics over time", "Analysis of party-specific discourse strategies"], "limitations": "", "keywords": ["political discourse", "machine learning", "Bundestag", "topic classification", "sentiment analysis"], "importance_score": 3, "read_time_minutes": 7}}
{"id": "2508.03199", "pdf": "https://arxiv.org/pdf/2508.03199.pdf", "abs": "https://arxiv.org/abs/2508.03199", "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "categories": ["cs.CL"], "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems.", "AI": {"tldr": "This paper investigates the influence of grammatical gender on visual representation in Text-to-Image models across multiple languages, establishing that language structure significantly shapes AI image outputs.", "motivation": "To explore how grammatical gender affects visual representation in T2I models, an area previously overlooked in bias research.", "method": "A cross-linguistic benchmark was created using 800 prompts in five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral languages (English, Chinese), generating 28,800 images across three T2I models.", "result": "Grammatical gender significantly impacts image generation, with masculine markers leading to 73% male representation on average, and feminine markers resulting in 38% female representation, showcasing marked differences compared to English.", "conclusion": "The study concludes that language structure influences AI-generated visuals, introducing new considerations for bias and fairness in multilingual AI systems.", "key_contributions": ["Introduces a cross-linguistic benchmark for analyzing T2I models based on grammatical gender.", "Demonstrates the significant impact of grammatical gender on representation in generated images.", "Establishes language structure as a crucial factor in understanding bias in AI-generated outputs."], "limitations": "", "keywords": ["Text-to-Image", "grammatical gender", "multilingual AI", "bias in AI", "visual representation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037.pdf", "abs": "https://arxiv.org/abs/2508.03037", "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "This study analyzes a twelve-year discourse on AI-generated art, highlighting artists' concerns about consent and transparency, and revealing a misalignment with media narratives.", "motivation": "To address the urgent concerns of artists affected by generative AI regarding consent, transparency, and creative labor amid their marginalized voices.", "method": "The study employs a reproducible methodology analyzing 439 curated excerpts from diverse media sources over twelve years, identifying five thematic clusters.", "result": "The analysis uncovers a misalignment between artists' perceptions and prevailing media narratives, revealing that technical jargon can act as a gatekeeping mechanism.", "conclusion": "The study advocates for deeper engagement with artist perspectives and provides a methodology for future research in the AI-creative landscape.", "key_contributions": ["Identification of thematic clusters in AI-generated art discourse", "Highlighting the misalignment between artists' concerns and media narratives", "Providing a reproducible methodology for future studies"], "limitations": "", "keywords": ["AI-generated art", "artist perspectives", "creative labor", "media narratives", "discourse analysis"], "importance_score": 4, "read_time_minutes": 18}}
{"id": "2508.03204", "pdf": "https://arxiv.org/pdf/2508.03204.pdf", "abs": "https://arxiv.org/abs/2508.03204", "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP", "authors": ["Abhirup Sinha", "Pritilata Saha", "Tithi Saha"], "categories": ["cs.CL"], "comment": "To be published in the Proceedings of Die Studierendenkonferenz\n  Informatik (SKILL) 2024", "summary": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.", "AI": {"tldr": "The paper examines approaches to anonymizing private information in data used for training large language models, which pose risks of exposing sensitive information.", "motivation": "To address the privacy risks associated with large language models that learn from data containing private information, especially in light of regulatory frameworks like GDPR.", "method": "The report reviews various pre-processing techniques for masking or pseudonymizing private information in textual data used for domain-agnostic NLP tasks.", "result": "The paper identifies and discusses multiple approaches to anonymization, emphasizing the importance of these techniques in protecting data privacy.", "conclusion": "While complete anonymization is challenging, existing methods can significantly reduce the risks of exposing private information in NLP tasks.", "key_contributions": ["Identification of pre-processing approaches for anonymization in NLP.", "Analysis of the effectiveness of different masking techniques.", "Discussion on the implications of language models on data privacy."], "limitations": "The paper acknowledges that while various methods exist, complete anonymization is not always achievable.", "keywords": ["data privacy", "anonymization", "NLP", "language models", "GDPR"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03211", "pdf": "https://arxiv.org/pdf/2508.03211.pdf", "abs": "https://arxiv.org/abs/2508.03211", "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "authors": ["Pablo J. Diego-Simón", "Emmanuel Chemla", "Jean-Rémi King", "Yair Lakretz"], "categories": ["cs.CL"], "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance.", "AI": {"tldr": "This study evaluates structural probes in large language models, highlighting biases in their performance related to word proximity and linguistic properties.", "motivation": "To clarify the effects of structural and statistical factors on syntactic representations in large language models, and to improve evaluation methods for structural probes.", "method": "Conducts an in-depth analysis using three controlled benchmarks to evaluate the performance of structural probes", "result": "Findings show that structural probes are biased by word proximity, struggle with deep syntactic structures, and are unaffected by word predictability.", "conclusion": "The study reveals significant challenges faced by structural probes and proposes controlled stimuli for better benchmarking.", "key_contributions": ["Identifies bias in structural probes due to word proximity", "Highlights poor representation of deep syntactic structures by probes", "Demonstrates lack of influence from word predictability on probe performance"], "limitations": "", "keywords": ["Structural Probes", "Large Language Models", "Syntax", "Linguistic Properties", "Benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03240", "pdf": "https://arxiv.org/pdf/2508.03240.pdf", "abs": "https://arxiv.org/abs/2508.03240", "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting", "authors": ["Mutaz Ayesh", "Nicolás Gutiérrez-Rolón", "Fernando Alva-Manchego"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results.", "AI": {"tldr": "The CardiffNLP team's work on the CLEARS shared task utilized an LLM-prompting approach with prompt variations for Spanish text adaptation.", "motivation": "To contribute to the CLEARS shared task on Spanish text adaptation and achieve competitive results using LLM technology.", "method": "The team used an LLM-prompting approach, experimenting with different prompt variations, initially with LLaMA-3.2 and then switching to Gemma-3 for final submissions.", "result": "The team achieved third place in Subtask 1 and second place in Subtask 2 of the Spanish text adaptation task.", "conclusion": "The paper presents the successful application of LLM prompting and provides insights into various prompt strategies that led to the team's performance.", "key_contributions": ["LLM-prompting methodology for Spanish text adaptation", "Comparative analysis of prompt variations", "Documentation of experimental results and performance in shared tasks"], "limitations": "", "keywords": ["Spanish text adaptation", "LLM prompting", "IberLEF 2025"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03247", "pdf": "https://arxiv.org/pdf/2508.03247.pdf", "abs": "https://arxiv.org/abs/2508.03247", "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs", "authors": ["Shintaro Sakai", "Jisun An", "Migyeong Kang", "Haewoon Kwak"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications.", "AI": {"tldr": "The paper tests how Large Language Models (LLMs) respond to cultural variations in mental health symptoms and finds that they struggle to replicate cultural patterns, especially when prompted in English.", "motivation": "To investigate whether LLMs reflect cultural differences in symptom reporting observed in clinical psychology, specifically between Western and Eastern cultural contexts in mental health.", "method": "LLMs were prompted using Western or Eastern personas and responses were analyzed to see if cultural symptom patterns were replicated, especially in different languages.", "result": "LLMs did not replicate the expected cultural symptom patterns when prompted in English, but performance improved when prompted in Eastern languages like Chinese, Japanese, and Hindi.", "conclusion": "Current general-purpose LLMs lack the necessary cultural sensitivity for safe mental health applications, highlighting the need for more culture-aware capabilities.", "key_contributions": ["Demonstrates the failure of LLMs to replicate cultural symptom reporting patterns in mental health.", "Shows improvement in LLM responses when using Eastern languages.", "Identifies limitations in LLMs' sensitivity to cultural nuances."], "limitations": "The study is limited to general-purpose LLMs and may not address variations in specialized or fine-tuned models.", "keywords": ["Large Language Models", "Cultural differences", "Mental health applications", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03250", "pdf": "https://arxiv.org/pdf/2508.03250.pdf", "abs": "https://arxiv.org/abs/2508.03250", "title": "RooseBERT: A New Deal For Political Language Modelling", "authors": ["Deborah Dore", "Elena Cabrio", "Serena Villata"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing amount of political debates and politics-related discussions\ncalls for the definition of novel computational methods to automatically\nanalyse such content with the final goal of lightening up political\ndeliberation to citizens. However, the specificity of the political language\nand the argumentative form of these debates (employing hidden communication\nstrategies and leveraging implicit arguments) make this task very challenging,\neven for current general-purpose pre-trained Language Models. To address this\nissue, we introduce a novel pre-trained Language Model for political discourse\nlanguage called RooseBERT. Pre-training a language model on a specialised\ndomain presents different technical and linguistic challenges, requiring\nextensive computational resources and large-scale data. RooseBERT has been\ntrained on large political debate and speech corpora (8K debates, each composed\nof several sub-debates on different topics) in English. To evaluate its\nperformances, we fine-tuned it on four downstream tasks related to political\ndebate analysis, i.e., named entity recognition, sentiment analysis, argument\ncomponent detection and classification, and argument relation prediction and\nclassification. Our results demonstrate significant improvements over\ngeneral-purpose Language Models on these four tasks, highlighting how\ndomain-specific pre-training enhances performance in political debate analysis.\nWe release the RooseBERT language model for the research community.", "AI": {"tldr": "Introduction of RooseBERT, a pre-trained Language Model for political discourse that improves upon general models in analyzing political debates.", "motivation": "To develop computational methods that aid in the automatic analysis of political debates and discussions, enhancing understanding and engagement of citizens.", "method": "RooseBERT was pre-trained on a large corpus of political debates and speeches and fine-tuned on tasks like named entity recognition, sentiment analysis, and argument detection.", "result": "The fine-tuning demonstrated significant improvements over general-purpose models in four key tasks of political debate analysis.", "conclusion": "The introduction of RooseBERT shows that domain-specific pre-training can enhance performance in understanding political discourse, making it available for further research.", "key_contributions": ["Development of RooseBERT for political discourse analysis", "Demonstrated performance improvements over general-purpose models", "Release of the model for public research use"], "limitations": "", "keywords": ["RooseBERT", "political discourse", "language model", "debate analysis", "machine learning"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.03259", "pdf": "https://arxiv.org/pdf/2508.03259.pdf", "abs": "https://arxiv.org/abs/2508.03259", "title": "Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition", "authors": ["Duzhen Zhang", "Chenxing Li", "Jiahua Dong", "Qi Liu", "Dong Yu"], "categories": ["cs.CL"], "comment": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "Continual Named Entity Recognition (CNER) is an evolving field that focuses\non sequentially updating an existing model to incorporate new entity types.\nPrevious CNER methods primarily utilize Knowledge Distillation (KD) to preserve\nprior knowledge and overcome catastrophic forgetting, strictly ensuring that\nthe representations of old and new models remain consistent. Consequently, they\noften impart the model with excessive stability (i.e., retention of old\nknowledge) but limited plasticity (i.e., acquisition of new knowledge). To\naddress this issue, we propose a Stability-Plasticity Trade-off (SPT) method\nfor CNER that balances these aspects from both representation and weight\nperspectives. From the representation perspective, we introduce a pooling\noperation into the original KD, permitting a level of plasticity by\nconsolidating representation dimensions. From the weight perspective, we\ndynamically merge the weights of old and new models, strengthening old\nknowledge while maintaining new knowledge. During this fusion, we implement a\nweight-guided selective mechanism to prioritize significant weights. Moreover,\nwe develop a confidence-based pseudo-labeling approach for the current\nnon-entity type, which predicts entity types using the old model to handle the\nsemantic shift of the non-entity type, a challenge specific to CNER that has\nlargely been ignored by previous methods. Extensive experiments across ten CNER\nsettings on three benchmark datasets demonstrate that our SPT method surpasses\nprevious CNER approaches, highlighting its effectiveness in achieving a\nsuitable stability-plasticity trade-off.", "AI": {"tldr": "A Stability-Plasticity Trade-off method for Continual Named Entity Recognition enhances model adaptability by balancing retention of old knowledge and acquisition of new entity types, demonstrating superior performance against previous methods.", "motivation": "To address the excessive stability and limited plasticity in existing Continual Named Entity Recognition methods that focus on Knowledge Distillation.", "method": "The proposed SPT method introduces a pooling operation for representation flexibility and dynamically merges weights from old and new models, complemented by a confidence-based pseudo-labeling approach.", "result": "Experiments across ten CNER settings on three benchmark datasets show that the SPT method outperforms previous CNER approaches, achieving an effective stability-plasticity trade-off.", "conclusion": "The SPT method effectively improves the adaptability of CNER systems in managing new entity types while retaining essential knowledge from old models.", "key_contributions": ["Introduction of pooling operation in Knowledge Distillation for representation flexibility", "Dynamic merging of weights from old and new models", "Confidence-based pseudo-labeling technique for handling non-entity type semantic shifts."], "limitations": "", "keywords": ["Continual Named Entity Recognition", "Knowledge Distillation", "Stability-Plasticity Trade-off", "Pseudo-labeling", "Machine Learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.03262", "pdf": "https://arxiv.org/pdf/2508.03262.pdf", "abs": "https://arxiv.org/abs/2508.03262", "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?", "authors": ["Junhyuk Choi", "Hyeonchu Park", "Haemin Lee", "Hyebeen Shin", "Hyun Joung Jin", "Bugeun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science.", "AI": {"tldr": "This study evaluates the ability of LLMs to predict economic decision-making using data from real human personas in PWYW pricing experiments.", "motivation": "To address the limitation of prior studies relying on fictional personas by using actual human data to evaluate LLMs' predictive capabilities in economic contexts.", "method": "The study compares three state-of-the-art multimodal LLMs using persona information from 522 Korean participants in various cultural consumption scenarios.", "result": "LLMs struggle with precise individual-level predictions but exhibit reasonable group-level behaviors. Common prompting techniques show little improvement over naive methods.", "conclusion": "The findings provide a comprehensive evaluation of LLMs' capabilities in simulating economic behavior, offering guidance for persona-based simulations in computational social science.", "key_contributions": ["Evaluation of LLMs using real human personas instead of fictional ones", "Comparison of multimodal LLMs' predictive abilities in economic decisions", "Insights on prompting techniques for improving LLM predictions"], "limitations": "LLMs do not achieve accurate individual-level predictions; the effectiveness of prompting methods was not significantly enhanced.", "keywords": ["Large Language Models", "economic decision-making", "PWYW pricing", "persona-based simulation", "computational social science"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2404.17730", "pdf": "https://arxiv.org/pdf/2404.17730.pdf", "abs": "https://arxiv.org/abs/2404.17730", "title": "Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults", "authors": ["Lara J. Martin", "Malathy Nagalakshmi"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "High-tech Augmentative and Alternative Communication (AAC) has been rapidly\nadvancing in recent years due to the increased use of large language models\n(LLMs) like ChatGPT, but many of these techniques are integrated without the\ninclusion of the users' perspectives. Autistic adults have been particularly\nneglected in the design of AAC tools. We conducted in-depth interviews with 12\nautistic adults to find the pain points of current AAC and determine what\ntechnological advances they might find helpful. We found 8 different categories\nof themes from our interviews: input flexibility, output flexibility, selecting\nor adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling\nblocks for continued use, and control of communication. In this paper, we go\nthrough these categories in depth -- comparing each to prior work -- and then\nhighlight novel findings to suggest possible research directions.", "AI": {"tldr": "This paper explores the perspectives of autistic adults on Augmentative and Alternative Communication (AAC) tools, identifying key pain points and potential advancements.", "motivation": "The rapid advancement of high-tech AAC tools powered by large language models like ChatGPT has largely overlooked the inclusion of end-user perspectives, particularly those of autistic adults.", "method": "In-depth interviews were conducted with 12 autistic adults to uncover their experiences and challenges with current AAC tools and to explore desired technological improvements.", "result": "The research identified 8 categories of themes from user interviews, including input flexibility, output flexibility, adaptation of AAC tools, contextual usage, benefits, adult access issues, obstacles for continued use, and user control over communication.", "conclusion": "The findings provide a comprehensive overview of the gaps in AAC design for autistic adults and suggest new avenues for future research to enhance user-centered design in this domain.", "key_contributions": ["Identification of 8 critical themes affecting AAC tools for autistic adults.", "Comparison of user perspectives with existing literature on AAC.", "Recommendations for future research directions to improve AAC technology."], "limitations": "The study is limited by a small sample size of only 12 participants, which may not represent the diversity of all autistic individuals.", "keywords": ["Augmentative and Alternative Communication", "Autistic adults", "large language models", "user-centered design", "technology"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03275", "pdf": "https://arxiv.org/pdf/2508.03275.pdf", "abs": "https://arxiv.org/abs/2508.03275", "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning", "authors": ["Jiahao Zhao"], "categories": ["cs.CL"], "comment": "15 pages, 4 figures, 1 table", "summary": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms.", "AI": {"tldr": "Introducing LECTOR, an adaptive scheduling algorithm leveraging large language models to enhance spaced repetition for test-oriented learning, achieving improved success rates in language examinations.", "motivation": "Existing spaced repetition algorithms struggle with semantic interference and personalization, necessitating a new approach for effective learning.", "method": "LECTOR utilizes large language models for semantic analysis and incorporates personalized learning profiles to address semantic confusion in vocabulary acquisition.", "result": "LECTOR achieved a 90.2% success rate in language examinations, outperforming the best baseline (SSP-MMC) at 88.4%, with notable improvements in managing semantically similar concepts.", "conclusion": "LECTOR presents a significant advancement in intelligent tutoring systems and adaptive learning platforms, addressing key challenges in learning and memory retention.", "key_contributions": ["Introduction of LECTOR algorithm for adaptive spaced repetition", "Utilization of LLM-powered semantic similarity for learning", "Demonstrated improved performance in test-oriented learning scenarios."], "limitations": "", "keywords": ["spaced repetition", "large language models", "adaptive learning", "semantic similarity", "intelligent tutoring systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.04286", "pdf": "https://arxiv.org/pdf/2410.04286.pdf", "abs": "https://arxiv.org/abs/2410.04286", "title": "Embracing Transparency: A Study of Open Science Practices Among Early Career HCI Researchers", "authors": ["Tatiana Chakravorti", "Sanjana Gautam", "Sarah M. Rajtmajer"], "categories": ["cs.HC"], "comment": null, "summary": "Many fields of science, including Human-Computer Interaction (HCI), have\nheightened introspection in the wake of concerns around reproducibility and\nreplicability of published findings. Notably, in recent years the HCI community\nhas worked to implement policy changes and mainstream open science practices.\nOur work investigates early-career HCI researchers' perceptions of open science\nand engagement with best practices through 18 semi-structured interviews. Our\nfindings highlight key barriers to the widespread adoption of data and\nmaterials sharing, and preregistration, namely: lack of clear incentives;\ncultural resistance; limited training; time constraints; concerns about\nintellectual property; and data privacy issues. We observe that small changes\nat major conferences like CHI could meaningfully impact community norms. We\noffer recommendations to address these barriers and to promote transparency and\nopenness in HCI. While these findings provide valuable and interesting insights\nabout the open science practices by early career HCI researchers, their\napplicability is limited to the USA only. The interview study relies on\nself-reported data; therefore, it can be subject to biases like recall bias.\nFuture studies will include the scope to expand HCI researchers from different\nlevels of experience and different countries, allowing for more justifiable\nexamples.", "AI": {"tldr": "This study explores early-career HCI researchers' perceptions of open science and identifies barriers to its adoption through interviews.", "motivation": "To investigate perceptions of open science among early-career researchers in HCI and understand the barriers to adopting best practices.", "method": "Conducted 18 semi-structured interviews with early-career HCI researchers.", "result": "Identified key barriers to data sharing and preregistration including lack of incentives, cultural resistance, limited training, time constraints, intellectual property concerns, and privacy issues.", "conclusion": "Recommendations are made to promote transparency in HCI, suggesting that small changes at major conferences could impact community norms.", "key_contributions": ["Identified specific barriers to open science in HCI.", "Provided recommendations for promoting open science practices.", "Highlighted the need for cultural change in the HCI community regarding transparency."], "limitations": "Findings are limited to the USA and based on self-reported data, susceptible to biases.", "keywords": ["open science", "HCI", "research barriers", "transparency", "early-career researchers"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03276", "pdf": "https://arxiv.org/pdf/2508.03276.pdf", "abs": "https://arxiv.org/abs/2508.03276", "title": "Do language models accommodate their users? A study of linguistic convergence", "authors": ["Terra Blevins", "Susanne Schmalwieser", "Benjamin Roth"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different.", "AI": {"tldr": "This paper investigates whether large language models (LLMs) exhibit linguistic convergence to human language patterns in dialogue, finding significant overfitting to the style of the conversation.", "motivation": "To explore the extent to which LLMs adapt to the linguistic patterns of human users, focusing on the concept of linguistic convergence which is fundamental to human communication.", "method": "The study systematically compares model completions from sixteen language models across three dialogue corpora, analyzing various stylometric features of the responses.", "result": "Models show strong convergence to the conversation's style, with significant overfitting compared to human responses. Instruction-tuned and larger models generally converge less than pretrained models.", "conclusion": "The findings suggest that while LLMs exhibit patterns of convergence, these are distinct from human linguistic behaviors, indicating different underlying mechanisms for language use.", "key_contributions": ["Systematic comparison of LLMs' dialogue completions to human responses.", "Demonstration of significant overfitting in LLM stylistic adaptation.", "Identification of variations in convergence patterns based on model training types."], "limitations": "The study focuses on specific dialogue corpora and stylometric features, which may limit the generalizability of the findings.", "keywords": ["language models", "linguistic convergence", "human communication", "dialogue analysis", "stylometric features"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.04543", "pdf": "https://arxiv.org/pdf/2501.04543.pdf", "abs": "https://arxiv.org/abs/2501.04543", "title": "The Impostor is Among Us: Can Large Language Models Capture the Complexity of Human Personas?", "authors": ["Christopher Lazik", "Christopher Katins", "Charlotte Kauter", "Jonas Jakob", "Caroline Jay", "Lars Grunske", "Thomas Kosch"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) created new opportunities for generating\npersonas, expected to streamline and accelerate the human-centered design\nprocess. Yet, AI-generated personas may not accurately represent actual user\nexperiences, as they can miss contextual and emotional insights critical to\nunderstanding real users' needs and behaviors. This introduces a potential\nthreat to quality, especially for novices. This paper examines the differences\nin how users perceive personas created by LLMs compared to those crafted by\nhumans regarding their credibility for design. We gathered ten human-crafted\npersonas developed by HCI experts according to relevant attributes established\nin related work. Then, we systematically generated ten personas with an LLM and\ncompared them with human-crafted ones in a survey. The results showed that\nparticipants differentiated between human-created and AI-generated personas,\nwith the latter perceived as more informative and consistent. However,\nparticipants noted that the AI-generated personas tended to follow stereotypes,\nhighlighting the need for a greater emphasis on diversity when utilizing LLMs\nfor persona creation.", "AI": {"tldr": "This paper explores the perception of AI-generated personas versus human-crafted ones in HCI design, revealing that AI personas are viewed as more informative but may perpetuate stereotypes.", "motivation": "To investigate the credibility and effectiveness of personas generated by LLMs compared to human-crafted personas in design processes.", "method": "A survey comparing ten human-crafted personas by HCI experts with ten personas generated by an LLM, gathering participants' perceptions on their effectiveness for design.", "result": "Participants distinguished between human-created and AI-generated personas, finding the latter more informative yet prone to stereotypes, underlining the importance of diversity in LLM-generated content.", "conclusion": "While LLM-generated personas can enhance design with informative content, caution is warranted to address issues of stereotyping and lack of diversity.", "key_contributions": ["Examination of user perceptions of AI-generated versus human-crafted personas in HCI.", "Highlighting the potential risks of stereotyping in AI-generated personas.", "Providing insights on the need for diversity in AI-generated personas."], "limitations": "Limited to a survey format and a small sample of personas, which may not encompass all user perspectives.", "keywords": ["Large Language Models", "personas", "HCI", "user experience", "AI-generated content"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03292", "pdf": "https://arxiv.org/pdf/2508.03292.pdf", "abs": "https://arxiv.org/abs/2508.03292", "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes", "authors": ["Shahed Masoudian", "Gustavo Escobedo", "Hannah Strauss", "Markus Schedl"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.", "AI": {"tldr": "This paper investigates gender bias in Large Language Models (LLMs) through narrative generation using psychological stereotypes, revealing how conditioning influences bias in generated content.", "motivation": "To address growing concerns about the amplification of gender biases in LLM outputs, particularly through more implicit biases that manifest in longer content.", "method": "The authors created a novel dataset, StereoBias-Stories, which contains short stories conditioned on various psychological stereotypes. They analyzed how the gender representation in stories changes based on these stereotypes during narrative generation tasks.", "result": "The study found that LLMs exhibit substantial bias towards male characters when unconditioned, but conditioning on attributes can mitigate this bias. Additionally, combining multiple attributes from gender stereotypes can either amplify or alleviate biases depending on the attributes' gender connotations.", "conclusion": "Insights gained from this research underline the necessity of a psychology-informed approach to evaluating and mitigating biases in LLMs, especially as these models grow in size and capability.", "key_contributions": ["Introduction of the StereoBias-Stories dataset for evaluating gender bias in LLMs.", "Demonstration that conditioning on psychological attributes can change bias in narrative generation.", "Establishment of a correlation between model size and alignment with psychological ground-truth in bias.", "Identification of how the combination of stereotypes can either intensify or reduce biases."], "limitations": "The study primarily focuses on narrative generation and may not generalize to all LLM tasks; further exploration in different contexts is needed.", "keywords": ["Large Language Models", "gender bias", "narrative generation", "psychological stereotypes", "StereoBias-Stories"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13308", "pdf": "https://arxiv.org/pdf/2501.13308.pdf", "abs": "https://arxiv.org/abs/2501.13308", "title": "\"It was Mentally Painful to Try and Stop\": Design Opportunities for Just-in-Time Interventions for People with Obsessive-Compulsive Disorder in the Real World", "authors": ["Ru Wang", "Kexin Zhang", "Yuqing Wang", "Keri Brown", "Yuhang Zhao"], "categories": ["cs.HC", "H.5.0"], "comment": null, "summary": "Obsessive-compulsive disorder (OCD) is a mental health condition that\nsignificantly impacts people's quality of life. While evidence-based therapies\nsuch as exposure and response prevention (ERP) can be effective, managing OCD\nsymptoms in everyday life -- an essential part of treatment and independent\nliving -- remains challenging due to fear confrontation and lack of appropriate\nsupport. To better understand the challenges and needs in OCD self-management,\nwe conducted interviews with 10 participants with diverse OCD conditions and\nseven therapists specializing in OCD treatment. Through these interviews, we\nexplored the characteristics of participants' triggers and how they shaped\ntheir compulsions, and uncovered key coping strategies across different stages\nof OCD episodes. Our findings highlight critical gaps between OCD\nself-management needs and currently available support. Building on these\ninsights, we propose design opportunities for just-in-time self-management\ntechnologies for OCD, including personalized symptom tracking, just-in-time\ninterventions, and support for OCD-specific privacy and social needs -- through\ntechnology and beyond.", "AI": {"tldr": "The paper explores self-management challenges in OCD and proposes design opportunities for technology to support personalized interventions.", "motivation": "To understand the challenges and needs in OCD self-management due to fear and lack of support.", "method": "Interviews with 10 OCD participants and 7 therapists to gather insights on triggers, compulsions, and coping strategies.", "result": "Identified critical gaps between OCD self-management needs and available support, leading to the proposal of design opportunities for technology.", "conclusion": "Advocates for just-in-time self-management technologies, including symptom tracking and interventions tailored to OCD-related needs.", "key_contributions": ["Identifies key coping strategies for OCD self-management.", "Highlights gaps between user needs and existing support systems.", "Proposes design opportunities for just-in-time technologies."], "limitations": "", "keywords": ["OCD", "self-management", "technology", "user needs", "mental health"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03294", "pdf": "https://arxiv.org/pdf/2508.03294.pdf", "abs": "https://arxiv.org/abs/2508.03294", "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty", "authors": ["Leonidas Zotos", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea", "Malvina Nissim", "Hedderik van Rijn"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 2 figures, accepted at the 2nd International Workshop on AI\n  in Society, Education and Educational Research (AISEER)", "summary": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment.", "AI": {"tldr": "This paper explores the effectiveness of Large Language Models (LLMs) in estimating exam question difficulty compared to professors.", "motivation": "To improve the accuracy of estimating the difficulty of exam questions for better educational assessments.", "method": "We compared LLM-based approaches with professors' estimations on True/False questions in Neural Networks and Machine Learning, evaluating performance through supervised learning techniques.", "result": "Professors struggled to effectively distinguish between easy and difficult questions, while LLMs, particularly Gemini 2.5, demonstrated superior performance; utilizing LLM uncertainty in a supervised learning framework yielded even better results with minimal training data.", "conclusion": "Supervised learning with LLM uncertainty can enhance professors' ability to estimate exam question difficulty, leading to improved assessment quality.", "key_contributions": ["Demonstrated LLM superiority over human estimators for exam difficulty", "Introduced LLM uncertainty in supervised learning to enhance predictions", "Provided a new perspective on integrating AI in educational assessment."], "limitations": "The study was limited to True/False questions and a small dataset of 42 training samples.", "keywords": ["Large Language Models", "exam difficulty estimation", "supervised learning", "Neural Networks", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.14327", "pdf": "https://arxiv.org/pdf/2501.14327.pdf", "abs": "https://arxiv.org/abs/2501.14327", "title": "Characterizing Visual Intents for People with Low Vision through Eye Tracking", "authors": ["Ru Wang", "Ruijia Chen", "Anqiao Erica Cai", "Zhiyuan Li", "Sanbrita Mondal", "Yuhang Zhao"], "categories": ["cs.HC", "H.5.0"], "comment": null, "summary": "Accessing visual information is crucial yet challenging for people with low\nvision due to visual conditions like low visual acuity and limited visual\nfields. However, unlike blind people, low vision people have and prefer using\ntheir functional vision in daily tasks. Gaze patterns thus become an important\nindicator to uncover their visual challenges and intents, inspiring more\nadaptive visual support. We seek to deeply understand low vision users' gaze\nbehaviors in different image-viewing tasks, characterizing typical visual\nintents and the unique gaze patterns exhibited by people with different low\nvision conditions. We conducted a retrospective think-aloud study using eye\ntracking with 20 low vision participants and 20 sighted controls. Participants\ncompleted various image-viewing tasks and watched the playback of their gaze\ntrajectories to reflect on their visual experiences. Based on the study, we\nderived a visual intent taxonomy with five visual intents characterized by\nparticipants' gaze behaviors. We demonstrated the difference between low vision\nand sighted participants' gaze behaviors and how visual ability affected low\nvision participants' gaze patterns across visual intents. Our findings\nunderscore the importance of combining visual ability information, visual\ncontext, and eye tracking data in visual intent recognition, setting up a\nfoundation for intent-aware assistive technologies for low vision people.", "AI": {"tldr": "The paper explores gaze behaviors of low vision users during image-viewing tasks to inform adaptive visual support technologies.", "motivation": "To understand the unique gaze patterns of people with low vision and improve visual support technologies based on their functional vision.", "method": "A retrospective think-aloud study using eye tracking with 20 low vision participants and 20 sighted controls was conducted. Participants performed image-viewing tasks and reflected on their gaze trajectories.", "result": "A visual intent taxonomy with five visual intents was derived, highlighting differences in gaze behaviors between low vision and sighted participants, influenced by visual ability.", "conclusion": "Combining visual ability data, visual context, and eye tracking is crucial for developing intent-aware assistive technologies for low vision users.", "key_contributions": ["Identification of unique gaze patterns in low vision users", "Development of a visual intent taxonomy", "Insight into the impact of visual ability on gaze behaviors"], "limitations": "", "keywords": ["low vision", "gaze patterns", "assistive technologies", "eye tracking", "visual intent"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03296", "pdf": "https://arxiv.org/pdf/2508.03296.pdf", "abs": "https://arxiv.org/abs/2508.03296", "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling", "authors": ["Anqi Li", "Wenwei Jin", "Jintao Tong", "Pengda Qin", "Weijia Li", "Guo Lu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social platforms have revolutionized information sharing, but also\naccelerated the dissemination of harmful and policy-violating content. To\nensure safety and compliance at scale, moderation systems must go beyond\nefficiency and offer accuracy and interpretability. However, current approaches\nlargely rely on noisy, label-driven learning, lacking alignment with moderation\nrules and producing opaque decisions that hinder human review. Therefore, we\npropose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that\nintroduces a new policy-aligned decision paradigm. The term \"Hierarchical\"\nreflects two key aspects of our system design: (1) a hierarchical moderation\npipeline, where a lightweight binary model first filters safe content and a\nstronger model handles fine-grained risk classification; and (2) a hierarchical\ntaxonomy in the second stage, where the model performs path-based\nclassification over a hierarchical taxonomy ranging from coarse to fine-grained\nlevels. To ensure alignment with evolving moderation policies, Hi-Guard\ndirectly incorporates rule definitions into the model prompt. To further\nenhance structured prediction and reasoning, we introduce a multi-level\nsoft-margin reward and optimize with Group Relative Policy Optimization (GRPO),\npenalizing semantically adjacent misclassifications and improving explanation\nquality. Extensive experiments and real-world deployment demonstrate that\nHi-Guard achieves superior classification accuracy, generalization, and\ninterpretability, paving the way toward scalable, transparent, and trustworthy\ncontent safety systems. Code is available at:\nhttps://github.com/lianqi1008/Hi-Guard.", "AI": {"tldr": "Hi-Guard is a multimodal moderation framework designed to improve content safety on social platforms by enhancing accuracy, interpretability, and alignment with moderation policies.", "motivation": "The rise of harmful and policy-violating content on social platforms necessitates improved moderation systems that are accurate, interpretable, and scalable.", "method": "Hi-Guard employs a hierarchical moderation pipeline with a lightweight binary filter and a stronger model for risk classification, utilizing a hierarchical taxonomy for classifications and incorporating rule definitions directly into the model.", "result": "Hi-Guard demonstrates superior classification accuracy, generalization, and interpretability in extensive experiments and real-world deployments, thereby enhancing content moderation effectiveness.", "conclusion": "The Hi-Guard framework paves the way for more transparent and trustworthy content moderation systems that align with evolving policies.", "key_contributions": ["Introduction of Hierarchical Guard (Hi-Guard) for improved moderation accuracy", "Hierarchical taxonomy for path-based classification", "Use of multi-level soft-margin reward with Group Relative Policy Optimization (GRPO) for enhanced reasoning"], "limitations": "", "keywords": ["social platforms", "content moderation", "multimodal framework", "classification accuracy", "interpretability"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.13920", "pdf": "https://arxiv.org/pdf/2502.13920.pdf", "abs": "https://arxiv.org/abs/2502.13920", "title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health", "authors": ["Xingbo Wang", "Janessa Griffith", "Daniel A. Adler", "Joey Castillo", "Tanzeem Choudhury", "Fei Wang"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2025). Code is available at https://github.com/xingbow/sleephealthLLM", "summary": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots.", "AI": {"tldr": "HealthGuru is a large language model-powered chatbot designed to improve sleep health through adaptive and personalized recommendations based on user data from wearables and contextual insights.", "motivation": "To address the difficulty individuals face in translating sleep tracking data into actionable health improvements.", "method": "HealthGuru integrates data from wearable devices, uses contextual information and a multi-armed bandit model to suggest personalized sleep-enhancing activities while supporting behavior change through natural conversations.", "result": "The deployment study showed HealthGuru improved sleep duration and activity scores, provided higher quality responses, and increased user motivation for behavior change compared to a baseline chatbot.", "conclusion": "HealthGuru demonstrates the potential of leveraging LLMs in health chatbot applications, highlighting the importance of personalization and user engagement.", "key_contributions": ["Introduction of HealthGuru, a novel chatbot for sleep health improvement", "Integration of contextual multi-armed bandit model for personalized recommendations", "Insights from an in-the-wild study on user engagement and behavior change"], "limitations": "Challenges in personalization and user engagement remain for health chatbots.", "keywords": ["sleep health", "large language model", "adaptation", "behavior change", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03333", "pdf": "https://arxiv.org/pdf/2508.03333.pdf", "abs": "https://arxiv.org/abs/2508.03333", "title": "CTTS: Collective Test-Time Scaling", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Tao Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM.", "AI": {"tldr": "This paper explores Collective Test-Time Scaling (CTTS) for enhancing LLMs' effectiveness without extra training, introducing a novel framework CTTS-MM that leverages multi-agent and multi-reward model collaboration to improve inference performance.", "motivation": "To enhance the effectiveness of large language models (LLMs) at test time without additional training, moving beyond single-agent approaches.", "method": "The paper investigates three paradigms for CTTS: 1) single agent with multiple reward models (SA-MR), 2) multiple agents with single reward model (MA-SR), and 3) multiple agents with multiple reward models (MA-MR). It proposes CTTS-MM framework combining multi-agent collaboration (Agent Collaboration Search) and multi-reward-model collaboration (Mixture of Reward Models).", "result": "Extensive experiments show that the MA-MR paradigm consistently achieves the best performance across seven mainstream benchmarks, with the CTTS-MM framework outperforming existing methods.", "conclusion": "Collective strategies in test-time scaling significantly enhance the performance of LLMs, and the proposed CTTS-MM framework demonstrates superior effectiveness.", "key_contributions": ["Introduction of Collective Test-Time Scaling (CTTS) as a new paradigm.", "Development of CTTS-MM which integrates multi-agent and multi-reward model collaborations.", "Demonstration of superior results through extensive tests on various benchmarks."], "limitations": "", "keywords": ["Collective Test-Time Scaling", "Multi-Agent Systems", "Large Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03358", "pdf": "https://arxiv.org/pdf/2508.03358.pdf", "abs": "https://arxiv.org/abs/2508.03358", "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature", "authors": ["Tiago G Canário", "Catarina Duarte", "Flávio L. Pinheiro", "João L. M. Pereira"], "categories": ["cs.CL", "cs.IR"], "comment": "24 pages, 5 Figures, 4 Tables", "summary": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2", "AI": {"tldr": "The paper presents Taggus, a novel NLP pipeline for extracting character social networks from Portuguese literary fiction, significantly outperforming existing tools.", "motivation": "There is a need for optimized methods to identify characters and their interactions in fiction, particularly for underrepresented languages like Portuguese, where existing methods struggle due to insufficient training data.", "method": "Taggus combines Part-of-speech (POS) tagging with heuristics to extract character networks, focusing on named entity recognition and interaction detection.", "result": "The Taggus pipeline achieves an F1-Score of 94.1% for character identification and 75.9% for interaction detection, outperforming traditional tools by 50.7% and 22.3% respectively.", "conclusion": "Taggus provides a robust solution for character interaction extraction in Portuguese literature and is publicly available to facilitate further research in this area.", "key_contributions": ["Introduction of the Taggus pipeline for character social networks", "Significant performance improvement over existing NER tools", "Public availability of the pipeline for further research"], "limitations": "The study acknowledges limitations regarding the size and scope of testing samples.", "keywords": ["Natural Language Processing", "Named Entity Recognition", "Social Networks", "Portuguese Literature", "Character Interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.03363", "pdf": "https://arxiv.org/pdf/2508.03363.pdf", "abs": "https://arxiv.org/abs/2508.03363", "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.", "AI": {"tldr": "This paper proposes JointThinking, a novel in-context learning paradigm for reasoning large language models that enhances reasoning accuracy by generating answers in parallel using two different reasoning modes.", "motivation": "To explore and improve in-context learning capabilities of reasoning large language models, which have been largely underutilized despite their potential for structured and multi-step reasoning.", "method": "JointThinking prompts the model to generate two answers in Thinking and Nothinking modes in parallel. A follow-up Thinking round occurs if the responses are inconsistent, minimizing latency since this scenario is infrequent.", "result": "JointThinking significantly outperforms few-shot chain-of-thought and majority voting techniques, showing improved answer robustness and comparable performance to state-of-the-art methods on in-distribution benchmarks while excelling on out-of-distribution tasks.", "conclusion": "The study indicates that structural thinking diversity enhances reasoning accuracy and suggests future research directions in in-context learning for reasoning large language models.", "key_contributions": ["JointThinking paradigm for in-context learning in RLLMs", "Demonstrated improved reasoning accuracy through structured reasoning modes", "Analysis of model performance scaling with increased model size."], "limitations": "Current limitations include the need for further exploration of in-context learning strategies and the inconsistencies in multi-answer generation.", "keywords": ["in-context learning", "reasoning large language models", "Thinking mode", "Nothinking mode", "JointThinking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.03399", "pdf": "https://arxiv.org/pdf/2508.03399.pdf", "abs": "https://arxiv.org/abs/2508.03399", "title": "ReDSM5: A Reddit Dataset for DSM-5 Depression Detection", "authors": ["Eliseo Bao", "Anxo Pérez", "Javier Parapar"], "categories": ["cs.CL"], "comment": "Accepted as a resource paper at CIKM 2025", "summary": "Depression is a pervasive mental health condition that affects hundreds of\nmillions of individuals worldwide, yet many cases remain undiagnosed due to\nbarriers in traditional clinical access and pervasive stigma. Social media\nplatforms, and Reddit in particular, offer rich, user-generated narratives that\ncan reveal early signs of depressive symptomatology. However, existing\ncomputational approaches often label entire posts simply as depressed or not\ndepressed, without linking language to specific criteria from the DSM-5, the\nstandard clinical framework for diagnosing depression. This limits both\nclinical relevance and interpretability. To address this gap, we introduce\nReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each\nexhaustively annotated at the sentence level by a licensed psychologist for the\nnine DSM-5 depression symptoms. For each label, the annotator also provides a\nconcise clinical rationale grounded in DSM-5 methodology. We conduct an\nexploratory analysis of the collection, examining lexical, syntactic, and\nemotional patterns that characterize symptom expression in social media\nnarratives. Compared to prior resources, ReDSM5 uniquely combines\nsymptom-specific supervision with expert explanations, facilitating the\ndevelopment of models that not only detect depression but also generate\nhuman-interpretable reasoning. We establish baseline benchmarks for both\nmulti-label symptom classification and explanation generation, providing\nreference results for future research on detection and interpretability.", "AI": {"tldr": "This paper introduces ReDSM5, a novel annotated Reddit corpus for recognizing and interpreting depressive symptoms using DSM-5 criteria.", "motivation": "To address undiagnosed depression by utilizing social media narratives and linking them to DSM-5 diagnostic criteria.", "method": "The authors annotated a dataset of 1484 Reddit posts at the sentence level for the nine DSM-5 depression symptoms, providing clinical rationales for each annotation.", "result": "The study conducted an exploratory analysis of symptom expression, establishing benchmarks for multi-label symptom classification and explanation generation.", "conclusion": "ReDSM5 enables the creation of models that detect depression while providing interpretable reasoning, enhancing clinical relevance in symptom detection.", "key_contributions": ["Introduction of ReDSM5 corpus with expert annotations for DSM-5 depression symptoms", "Combination of symptom-specific supervision with clinical rationales", "Establishment of benchmarks for symptom classification and explanation generation."], "limitations": "The study's scope is limited to data from Reddit and may not generalize to other platforms or cultures.", "keywords": ["depression", "social media", "machine learning", "DSM-5", "interpretability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03420", "pdf": "https://arxiv.org/pdf/2508.03420.pdf", "abs": "https://arxiv.org/abs/2508.03420", "title": "Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations", "authors": ["Bing Wang", "Ximing Li", "Yiming Wang", "Changchun Li", "Jiaxu Cui", "Renchu Guan", "Bo Yang"], "categories": ["cs.CL", "cs.SI"], "comment": "Accepted by CIKM 2025. 11 pages, 4 figures. Code:\n  https://github.com/wangbing1416/MISDER", "summary": "The proliferation of misinformation across diverse social media platforms has\ndrawn significant attention from both academic and industrial communities due\nto its detrimental effects. Accordingly, automatically distinguishing\nmisinformation, dubbed as Misinformation Detection (MD), has become an\nincreasingly active research topic. The mainstream methods formulate MD as a\nstatic learning paradigm, which learns the mapping between the content, links,\nand propagation of news articles and the corresponding manual veracity labels.\nHowever, the static assumption is often violated, since in real-world\nscenarios, the veracity of news articles may vacillate within the dynamically\nevolving social environment. To tackle this problem, we propose a novel\nframework, namely Misinformation detection with Dynamic Environmental\nRepresentations (MISDER). The basic idea of MISDER lies in learning a social\nenvironmental representation for each period and employing a temporal model to\npredict the representation for future periods. In this work, we specify the\ntemporal model as the LSTM model, continuous dynamics equation, and pre-trained\ndynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,\nMISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,\nwe compare it to various MD baselines across 2 prevalent datasets, and the\nexperimental results can indicate the effectiveness of our proposed model.", "AI": {"tldr": "A novel framework, MISDER, is proposed for misinformation detection by learning dynamic environmental representations to adapt to changing news veracity.", "motivation": "The need to address the spread of misinformation across social media, which has harmful effects, necessitates effective distinguishing methods that account for dynamic veracity changes in news articles.", "method": "The proposed MISDER framework utilizes a temporal model with three variants: MISDER-LSTM, MISDER-ODE, and MISDER-PT, to learn environmental representations for predicting future veracity.", "result": "MISDER outperforms various misinformation detection baselines across two prevalent datasets, demonstrating its effectiveness in adapting to changes in the social environment.", "conclusion": "The effectiveness of the MISDER framework suggests a promising direction for future research in misinformation detection through adaptive modeling.", "key_contributions": ["Introduction of dynamic environmental representations for misinformation detection.", "Development of three model variants (LSTM, continuous dynamics, pre-trained dynamics) tailored for temporal changes.", "Empirical validation on two datasets demonstrating improved performance over existing models."], "limitations": "", "keywords": ["misinformation detection", "dynamic representation", "temporal model", "LSTM", "social media"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.03440", "pdf": "https://arxiv.org/pdf/2508.03440.pdf", "abs": "https://arxiv.org/abs/2508.03440", "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "authors": ["Junhong Wu", "Jinliang Lu", "Zixuan Ren", "Ganqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.", "AI": {"tldr": "This paper investigates the 'Soft Thinking' capabilities of LLMs and identifies limitations in their reasoning paths, proposing randomness strategies to enhance performance.", "motivation": "To explore and enhance the expressive capabilities of large language models in reasoning tasks by utilizing soft, abstract tokens.", "method": "The paper employs a series of probing techniques to examine the internal behaviors of various LLMs and tests different sampling strategies, including Dirichlet resampling and the Gumbel-Softmax trick, to introduce randomness in the reasoning process.", "result": "Findings reveal that LLMs often rely on a dominant component of the soft inputs, which limits their reasoning exploration. However, introducing randomness through sampling strategies significantly improves their performance across reasoning tasks.", "conclusion": "Incorporating randomness into the Soft Thinking framework can alleviate the limitations of conventional approaches, unlocking better reasoning performance for LLMs.", "key_contributions": ["Identification of limitations in LLMs' Soft Thinking capabilities", "Introduction of randomness in soft token generation", "Demonstration of improved performance using Gumbel-Softmax trick"], "limitations": "Focuses primarily on LLMs and may not generalize to other model types or applications.", "keywords": ["Large Language Models", "Soft Thinking", "Randomness", "Gumbel-Softmax", "Reasoning Benchmarks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03453", "pdf": "https://arxiv.org/pdf/2508.03453.pdf", "abs": "https://arxiv.org/abs/2508.03453", "title": "Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings", "authors": ["Rita González-Márquez", "Philipp Berens", "Dmitry Kobak"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text embeddings, i.e. vector representations of entire texts, play an\nimportant role in many NLP applications, such as retrieval-augmented\ngeneration, sentiment analysis, clustering, or visualizing collections of texts\nfor data exploration. Currently, top-performing embedding models are derived\nfrom pre-trained language models via extensive supervised fine-tuning using\ncurated text pairs. This contrasts with computer vision, where self-supervised\ntraining based on data augmentations has demonstrated remarkable success. Here\nwe systematically compare the two most well-known augmentation strategies for\npositive pair generation in contrastive learning of text embeddings. We assess\nembedding quality on MTEB and additional in-domain evaluations and show that\ncropping augmentation strongly outperforms the dropout-based approach. We find\nthat on out-of-domain data, the quality of resulting embeddings is below the\nsupervised SOTA models, but for in-domain data, self-supervised fine-tuning\nproduces high-quality text embeddings after very short fine-tuning, sometimes\nonly marginally below the supervised SOTA. Finally, we show that representation\nquality increases towards the last transformer layers, which undergo the\nlargest change during fine-tuning; and that fine-tuning only those last layers\nis sufficient to reach similar embedding quality.", "AI": {"tldr": "This paper compares two augmentation strategies for positive pair generation in contrastive learning of text embeddings, showing that cropping augmentation outperforms the dropout-based approach, particularly for in-domain data.", "motivation": "To enhance the quality of text embeddings used in various NLP applications by comparing self-supervised training strategies with established supervised methods.", "method": "Systematic comparison of cropping and dropout-based augmentation strategies for generating positive pairs in contrastive learning, evaluated on MTEB and in-domain datasets.", "result": "Cropping augmentation significantly outperforms dropout-based methods for in-domain evaluations; self-supervised fine-tuning yields high-quality text embeddings, approaching supervised state-of-the-art after limited tuning.", "conclusion": "The study establishes that focusing fine-tuning on the last layers of a transformer model can efficiently produce high-quality embeddings, supporting the case for self-supervised learning in NLP.", "key_contributions": ["Comparison of augmentation strategies for text embeddings in NLP", "Findings on the effectiveness of self-supervised fine-tuning", "Insights on layer-specific fine-tuning benefits."], "limitations": "Quality of embeddings from self-supervised methods is still below state-of-the-art on out-of-domain data.", "keywords": ["text embeddings", "contrastive learning", "self-supervised learning", "NLP", "data augmentation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.03475", "pdf": "https://arxiv.org/pdf/2508.03475.pdf", "abs": "https://arxiv.org/abs/2508.03475", "title": "fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval", "authors": ["Pranshu Rastogi"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables. Code available at\n  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders", "summary": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\nRetrieval is approached as a Learning-to-Rank task using a bi-encoder model\nfine-tuned from a pre-trained transformer optimized for sentence similarity.\nTraining used both the source languages and their English translations for\nmultilingual retrieval and only English translations for cross-lingual\nretrieval. Using lightweight models with fewer than 500M parameters and\ntraining on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual\nand 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.", "AI": {"tldr": "This paper presents the approach and results of SemEval-2025 Task 7 focused on multilingual and crosslingual fact-checked claim retrieval via a bi-encoder model.", "motivation": "The task aims to improve multilingual and crosslingual retrieval processes by refining the Learning-to-Rank methodology while utilizing multilingual data effectively.", "method": "A bi-encoder model fine-tuned from a pre-trained transformer was employed, trained on both source languages and their English translations for multilingual retrieval, and only English for cross-lingual retrieval.", "result": "The proposed method achieved 92% Success@10 in multilingual retrieval and 80% Success@10 in crosslingual retrieval, ranking 5th and 10th, respectively, in their respective tracks.", "conclusion": "The study demonstrates the efficacy of lightweight bi-encoder models in multilingual and crosslingual fact-checked claim retrieval, showcasing competitive performance with fewer parameters.", "key_contributions": ["Introduction of a bi-encoder model optimized for fact-checked claim retrieval", "Successful implementation of multilingual and crosslingual retrieval strategies", "Demonstration of high performance with lightweight models under 500M parameters"], "limitations": "The effectiveness of the approach may vary with different languages and facts not covered in the training data.", "keywords": ["multilingual retrieval", "crosslingual retrieval", "bi-encoder model", "Learning-to-Rank", "fact-checked claims"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03489", "pdf": "https://arxiv.org/pdf/2508.03489.pdf", "abs": "https://arxiv.org/abs/2508.03489", "title": "CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation", "authors": ["Kaiwen Zhao", "Bharathan Balaji", "Stephen Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Product sustainability reports provide valuable insights into the\nenvironmental impacts of a product and are often distributed in PDF format.\nThese reports often include a combination of tables and text, which complicates\ntheir analysis. The lack of standardization and the variability in reporting\nformats further exacerbate the difficulty of extracting and interpreting\nrelevant information from large volumes of documents. In this paper, we tackle\nthe challenge of answering questions related to carbon footprints within\nsustainability reports available in PDF format. Unlike previous approaches, our\nfocus is on addressing the difficulties posed by the unstructured and\ninconsistent nature of text extracted from PDF parsing. To facilitate this\nanalysis, we introduce CarbonPDF-QA, an open-source dataset containing\nquestion-answer pairs for 1735 product report documents, along with\nhuman-annotated answers. Our analysis shows that GPT-4o struggles to answer\nquestions with data inconsistencies. To address this limitation, we propose\nCarbonPDF, an LLM-based technique specifically designed to answer carbon\nfootprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama\n3 with our training data. Our results show that our technique outperforms\ncurrent state-of-the-art techniques, including question-answering (QA) systems\nfinetuned on table and text data.", "AI": {"tldr": "This paper presents CarbonPDF-QA, a dataset for asking questions about carbon footprints in sustainability reports and introduces CarbonPDF, an LLM-based approach to improve question answering from PDF documents.", "motivation": "The analysis of product sustainability reports in PDF format is hindered by unstructured text and inconsistent reporting formats, making it difficult to extract and interpret carbon footprint information.", "method": "The authors created CarbonPDF-QA, an open-source dataset comprising question-answer pairs from 1735 product reports, and developed CarbonPDF by fine-tuning Llama 3 to improve question-answering capabilities.", "result": "CarbonPDF outperforms existing state-of-the-art QA systems, evidencing better handling of inconsistencies in data from sustainability reports.", "conclusion": "The introduction of CarbonPDF provides a promising solution to enhance question answering on carbon footprints, making it easier to analyze sustainability reports effectively.", "key_contributions": ["Creation of the CarbonPDF-QA dataset for sustainability reports", "Introduction of CarbonPDF, an LLM-based approach for improved QA", "Demonstration of superior performance compared to existing QA systems"], "limitations": "The study primarily focuses on carbon footprint questions; other areas of sustainability reporting may not be addressed.", "keywords": ["sustainability reports", "carbon footprint", "LLM-based technique", "PDF analysis", "question-answering systems"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.03520", "pdf": "https://arxiv.org/pdf/2508.03520.pdf", "abs": "https://arxiv.org/abs/2508.03520", "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Aneesh Krishna", "Shafin Rahman", "Tom Gedeon"], "categories": ["cs.CL", "cs.LG"], "comment": "Code available at https://github.com/hasan-rakibul/UPLME", "summary": "Supervised learning for empathy regression is challenged by noisy\nself-reported empathy scores. While many algorithms have been proposed for\nlearning with noisy labels in textual classification problems, the regression\ncounterpart is relatively under-explored. We propose UPLME, an\nuncertainty-aware probabilistic language modelling framework to capture label\nnoise in the regression setting of empathy detection. UPLME includes a\nprobabilistic language model that predicts both empathy score and\nheteroscedastic uncertainty and is trained using Bayesian concepts with\nvariational model ensembling. We further introduce two novel loss components:\none penalises degenerate Uncertainty Quantification (UQ), and another enforces\nthe similarity between the input pairs on which we predict empathy. UPLME\nprovides state-of-the-art performance (Pearson Correlation Coefficient:\n$0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the\nperformance reported in the literature in two public benchmarks, having label\nnoise. Through synthetic label noise injection, we show that UPLME is effective\nin separating noisy and clean samples based on the predicted uncertainty. UPLME\nfurther outperform (Calibration error: $0.571\\rightarrow0.376$) a recent\nvariational model ensembling-based UQ method designed for regression problems.", "AI": {"tldr": "This paper introduces UPLME, a probabilistic language modeling framework for empathy score regression that accounts for label noise.", "motivation": "Supervised learning for empathy regression struggles with noisy self-reported scores, necessitating a better approach to handle label noise.", "method": "UPLME employs a probabilistic language model to predict empathy scores and uncertainty, trained using Bayesian techniques and variational model ensembling, with additional components to manage uncertainty penalties and input similarity.", "result": "UPLME achieves state-of-the-art performance on public benchmarks with noisy labels, improving Pearson correlation and calibration error compared to previous methods.", "conclusion": "UPLME effectively distinguishes noisy from clean samples by leveraging predicted uncertainty, demonstrating its efficacy in regression tasks with label noise.", "key_contributions": ["Introduction of UPLME framework for empathy regression", "New loss components for managing uncertainty and input similarity", "Demonstrated effectiveness on benchmarks with label noise"], "limitations": "", "keywords": ["empathy detection", "label noise", "probabilistic language model", "Bayesian techniques", "uncertainty quantification"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03523", "pdf": "https://arxiv.org/pdf/2508.03523.pdf", "abs": "https://arxiv.org/abs/2508.03523", "title": "FilBench: Can LLMs Understand and Generate Filipino?", "authors": ["Lester James V. Miranda", "Elyanah Aco", "Conner Manuel", "Jan Christian Blaise Cruz", "Joseph Marvin Imperial"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development.", "AI": {"tldr": "Introduction of FilBench, a benchmark for evaluating LLMs in Filipino and Southeast Asian languages, revealing specific performance challenges and the need for language-specific evaluation.", "motivation": "To address the lack of knowledge regarding the capabilities of LLMs in Filipino languages and to promote inclusivity in NLP research for the Philippines.", "method": "The authors introduce FilBench, a curated benchmark that assesses 27 state-of-the-art LLMs on tasks such as Cultural Knowledge, Reading Comprehension, and Generation in Filipino, Tagalog, and Cebuano.", "result": "Evaluation results show that many LLMs struggle with reading comprehension and translation in Filipino, with the best model scoring only 72.23% on FilBench, highlighting significant performance gaps.", "conclusion": "The study underscores the importance of creating language-specific benchmarks like FilBench to enhance NLP research and LLM development for Philippine languages.", "key_contributions": ["Introduction of FilBench for Filipino language evaluation", "Analysis of 27 LLMs performance on Filipino-centric tasks", "Highlighting underperformance of existing models for Southeast Asian languages"], "limitations": "Primarily focused on Filipino languages, not generalizing results to other languages or applications.", "keywords": ["FilBench", "Filipino NLP", "Language Benchmarking", "LLM Performance", "Southeast Asian Languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03529", "pdf": "https://arxiv.org/pdf/2508.03529.pdf", "abs": "https://arxiv.org/abs/2508.03529", "title": "Marito: Structuring and Building Open Multilingual Terminologies for South African NLP", "authors": ["Vukosi Marivate", "Isheanesu Dzingirai", "Fiskani Banda", "Richard Lastrucci", "Thapelo Sindane", "Keabetswe Madumo", "Kayode Olaleye", "Abiodun Modupe", "Unarine Netshifhefhe", "Herkulaas Combrink", "Mohlatlego Nakeng", "Matome Ledwaba"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The critical lack of structured terminological data for South Africa's\nofficial languages hampers progress in multilingual NLP, despite the existence\nof numerous government and academic terminology lists. These valuable assets\nremain fragmented and locked in non-machine-readable formats, rendering them\nunusable for computational research and development. \\emph{Marito} addresses\nthis challenge by systematically aggregating, cleaning, and standardising these\nscattered resources into open, interoperable datasets. We introduce the\nfoundational \\emph{Marito} dataset, released under the equitable,\nAfrica-centered NOODL framework. To demonstrate its immediate utility, we\nintegrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.\nExperiments show substantial improvements in the accuracy and domain-specific\nconsistency of English-to-Tshivenda machine translation for large language\nmodels. \\emph{Marito} provides a scalable foundation for developing robust and\nequitable NLP technologies, ensuring South Africa's rich linguistic diversity\nis represented in the digital age.", "AI": {"tldr": "The paper introduces the Marito dataset, which aggregates and standardizes terminological data for South Africa's official languages, enhancing multilingual NLP applications.", "motivation": "To address the critical lack of structured terminological data for South Africa's languages that impedes multilingual NLP progress.", "method": "The authors systematically aggregate, clean, and standardize fragmented terminology resources into open, interoperable datasets and integrate this into a RAG pipeline.", "result": "Experiments show significant improvements in English-to-Tshivenda machine translation accuracy and domain-specific consistency using the Marito dataset.", "conclusion": "The Marito dataset provides a scalable foundation for developing equitable NLP technologies, promoting representation of South Africa's linguistic diversity.", "key_contributions": ["Introduction of the Marito dataset under the NOODL framework", "Demonstration of improved machine translation performance with the dataset", "Promotion of data equity in NLP for South Africa's languages"], "limitations": "", "keywords": ["multilingual NLP", "terminology dataset", "machine translation", "RAG", "South Africa"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03533", "pdf": "https://arxiv.org/pdf/2508.03533.pdf", "abs": "https://arxiv.org/abs/2508.03533", "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models", "authors": ["Xiaoming Hou", "Jiquan Zhang", "Zibin Lin", "DaCheng Tao", "Shengli Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation.", "AI": {"tldr": "The paper introduces EmbedGrad, a framework that refines text prompt embeddings using gradient-based techniques to enhance task adaptation in AI models without adding complexity or sacrificing interpretability.", "motivation": "Adapting pretrained foundation models for diverse tasks is a challenge due to limitations in current discrete and continuous optimization techniques.", "method": "EmbedGrad uses gradient-based refinement of text prompt embeddings guided by labeled examples during training and integrates optimized embeddings with user queries during inference.", "result": "EmbedGrad demonstrated significant accuracy improvements in various tasks, notably increasing mathematical reasoning accuracy from 14.74% to 58.96%, with consistent gains across model scales and tasks.", "conclusion": "The framework establishes embedding refinement as a new effective paradigm for enhancing task adaptation of AI models.", "key_contributions": ["Introduction of EmbedGrad for optimizing text prompt embeddings", "Decoupling training from deployment for improved interpretability and efficiency", "Demonstrated effectiveness across multiple AI model scales and tasks"], "limitations": "", "keywords": ["Prompt engineering", "Text embeddings", "Gradient-based refinement"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03550", "pdf": "https://arxiv.org/pdf/2508.03550.pdf", "abs": "https://arxiv.org/abs/2508.03550", "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations", "authors": ["Peng Lai", "Jianjie Zheng", "Sijie Cheng", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using large language models, a paradigm known as\n\"LLMas-a-judge.\" However, improving its alignment with human preferences\nwithout complex prompts or fine-tuning remains challenging. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and taskrelevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a lightweight and\nefficient framework for enhancing LLM-as-a-Judge alignment with human scoring,\nvia internal representations. LAGER produces fine-grained judgment scores by\naggregating cross-layer scoretoken logits and computing the expected score from\na softmax-based distribution, with the LLM backbone kept frozen. LAGER fully\nleverages the complementary information across different layers, overcoming the\nlimitations of relying solely on the final layer. We evaluate our method on the\nstandard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman\ncorrelation, and find that LAGER achieves improvements of up to 7.5% over the\nbest baseline across these benchmarks. Without reasoning steps, LAGER matches\nor outperforms reasoning-based methods. Experiments on downstream applications,\nsuch as data selection and emotional understanding, further show the\neffectiveness of our method.", "AI": {"tldr": "LAGER is a framework to enhance LLM alignment with human preferences using cross-layer representations, outperforming existing methods in evaluation tasks.", "motivation": "To improve the alignment of LLM evaluations with human preferences without complex prompts or fine-tuning.", "method": "LAGER aggregates cross-layer score-token logits to produce fine-grained judgment scores while keeping the LLM backbone frozen.", "result": "LAGER achieves improvements of up to 7.5% over baseline models in alignment benchmarks and matches or outperforms reasoning-based methods.", "conclusion": "LAGER demonstrates effectiveness in enhancing LLM evaluations and shows promise in downstream applications like data selection and emotional understanding.", "key_contributions": ["Proposes a lightweight and efficient framework (LAGER) for LLM alignment using internal representations.", "Demonstrates improvement in evaluation scores through cross-layer representation aggregation.", "Validates effectiveness on standard alignment benchmarks and downstream applications."], "limitations": "", "keywords": ["LLM-as-a-judge", "human alignment", "cross-layer representations", "evaluation benchmarks", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.03571", "pdf": "https://arxiv.org/pdf/2508.03571.pdf", "abs": "https://arxiv.org/abs/2508.03571", "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation", "authors": ["Iing Muttakhiroh", "Thomas Fevens"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often suffer from performance degradation when\nfaced with domain shifts, primarily due to catastrophic forgetting. In this\nwork, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),\na novel continual learning framework that integrates dynamic knowledge graphs\nwith instruction tuning. By leveraging retrieved domain-specific knowledge as\nguidance during training, KILO enhances both adaptability to new domains and\nretention of previously acquired knowledge. We pretrain our model on\nWikiText-103 and evaluate sequential adaptation across four diverse target\ndomains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that\nKILO consistently outperforms strong baselines, including continual\nfine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward\ntransfer, F1 score, retention rate, and training efficiency. These results\nhighlight the effectiveness of combining structured knowledge retrieval and\ninstruction prompting to overcome domain shift challenges in continual learning\nscenarios.", "AI": {"tldr": "KILO is a continual learning framework that combines dynamic knowledge graphs and instruction tuning to improve domain adaptability and knowledge retention in Large Language Models.", "motivation": "To address performance degradation in Large Language Models (LLMs) during domain shifts due to catastrophic forgetting.", "method": "KILO integrates dynamic knowledge graphs with instruction tuning, leveraging domain-specific knowledge during training.", "result": "KILO outperforms strong baselines in backward transfer, forward transfer, F1 score, retention rate, and training efficiency across four diverse domains: BioASQ, SciQ, TweetEval, and MIND.", "conclusion": "The combination of structured knowledge retrieval and instruction prompting effectively addresses domain shift challenges in continual learning.", "key_contributions": ["Introduction of KILO framework for continual learning in LLMs", "Combination of knowledge graphs with instruction tuning", "Demonstration of superior performance across multiple domains"], "limitations": "", "keywords": ["Continual Learning", "Large Language Models", "Knowledge Graphs", "Instruction Tuning", "Domain Adaptation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.03644", "pdf": "https://arxiv.org/pdf/2508.03644.pdf", "abs": "https://arxiv.org/abs/2508.03644", "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?", "authors": ["Wenxuan Shen", "Mingjia Wang", "Yaochen Wang", "Dongping Chen", "Junjie Yang", "Yao Wan", "Weiwei Lin"], "categories": ["cs.CL", "cs.CV", "cs.IR"], "comment": "In submission. Project website: https://double-bench.github.io/", "summary": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.", "AI": {"tldr": "Introduction of Double-Bench, a comprehensive evaluation system for retrieval-augmented generation (RAG) systems focusing on multilingual and multimodal document understanding.", "motivation": "To address the shortcomings of existing benchmarks that fail to accurately reflect the challenges faced in real-world document RAG systems.", "method": "Double-Bench features 3,276 documents and 5,168 queries across multiple languages and document types, assessed through extensive human verification.", "result": "Experiments highlight the diminishing gap between text and visual embeddings and unveil a tendency in RAG frameworks to overestimate their confidence when providing answers without evidence support.", "conclusion": "Double-Bench aims to serve as a foundational resource for future studies in document RAG, with plans for annual updates and corpus retrieval.", "key_contributions": ["Introduction of a multilingual and multimodal evaluation system", "Comprehensiveness ensured through human-verified queries and evidence", "Revealing the over-confidence issue in current document RAG frameworks."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Multimodal Large Language Models", "Document Evaluation"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.03654", "pdf": "https://arxiv.org/pdf/2508.03654.pdf", "abs": "https://arxiv.org/abs/2508.03654", "title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?", "authors": ["Xinyu Wang", "Yue Zhang", "Liqiang Jing"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by CIKM 2025", "summary": "Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA.", "AI": {"tldr": "This paper explores the application of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis, proposing a framework that enhances sarcasm detection and explanation.", "motivation": "The research addresses the challenges of sarcasm detection in sentiment analysis, particularly using multimodal approaches with LVLMs, which have been underexplored.", "method": "The authors conduct comprehensive experiments on Multimodal Sarcasm Detection and Explanation, identifying limitations such as poor visual understanding and lack of conceptual knowledge, and propose a training-free framework that combines object extraction and external knowledge.", "result": "The experimental results demonstrate the effectiveness of the proposed framework in improving sarcasm interpretation and explanation across multiple models.", "conclusion": "The study shows that integrating object extraction and conceptual knowledge can significantly enhance the performance of LVLMs in detecting and explaining sarcasm in multimodal settings.", "key_contributions": ["Evaluation of LVLMs in Multimodal Sarcasm Analysis tasks", "Identification of limitations in existing LVLM methodologies", "Proposal of a training-free framework for improved sarcasm interpretation"], "limitations": "Key limitations identified include insufficient visual understanding and a lack of conceptual knowledge in existing models.", "keywords": ["sarcasm detection", "multimodal analysis", "Large Visual Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.03668", "pdf": "https://arxiv.org/pdf/2508.03668.pdf", "abs": "https://arxiv.org/abs/2508.03668", "title": "CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction", "authors": ["Zixuan Li", "Binzong Geng", "Jing Xiong", "Yong He", "Yuxuan Hu", "Jian Chen", "Dingwei Chen", "Xiyu Chang", "Liang Zhang", "Linjian Mo", "Chengming Li", "Chuan Yuan", "Zhenan Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Click-Through Rate (CTR) prediction, a core task in recommendation systems,\nestimates user click likelihood using historical behavioral data. Modeling user\nbehavior sequences as text to leverage Language Models (LMs) for this task has\ngained traction, owing to LMs' strong semantic understanding and contextual\nmodeling capabilities. However, a critical structural gap exists: user behavior\nsequences consist of discrete actions connected by semantically empty\nseparators, differing fundamentally from the coherent natural language in LM\npre-training. This mismatch causes semantic fragmentation, where LM attention\nscatters across irrelevant tokens instead of focusing on meaningful behavior\nboundaries and inter-behavior relationships, degrading prediction performance.\nTo address this, we propose $\\textit{CTR-Sink}$, a novel framework introducing\nbehavior-level attention sinks tailored for recommendation scenarios. Inspired\nby attention sink theory, it constructs attention focus sinks and dynamically\nregulates attention aggregation via external information. Specifically, we\ninsert sink tokens between consecutive behaviors, incorporating\nrecommendation-specific signals such as temporal distance to serve as stable\nattention sinks. To enhance generality, we design a two-stage training strategy\nthat explicitly guides LM attention toward sink tokens and a attention sink\nmechanism that amplifies inter-sink dependencies to better capture behavioral\ncorrelations. Experiments on one industrial dataset and two open-source\ndatasets (MovieLens, Kuairec), alongside visualization results, validate the\nmethod's effectiveness across scenarios.", "AI": {"tldr": "This paper presents CTR-Sink, a novel framework for Click-Through Rate prediction that addresses the issue of semantic fragmentation in user behavior sequences by introducing behavior-level attention sinks.", "motivation": "The motivation behind this work is the identified structural gap in using Language Models for Click-Through Rate prediction, specifically the mismatch between user behavior sequences and natural language coherence.", "method": "The method involves inserting sink tokens between consecutive user behaviors and applying a two-stage training strategy to regulate attention aggregation and enhance the capturing of behavioral correlations.", "result": "Experiments demonstrate that CTR-Sink improves prediction performance on various datasets, effectively addressing the issue of semantic fragmentation.", "conclusion": "The proposed CTR-Sink framework significantly enhances Click-Through Rate prediction by leveraging behavior-level attention sinks and demonstrating its effectiveness in diverse scenarios.", "key_contributions": ["Introduction of behavior-level attention sinks for recommendation scenarios", "Development of a two-stage training strategy to guide attention focus", "Incorporation of temporal distance signals for attention regulation"], "limitations": "", "keywords": ["Click-Through Rate", "recommendation systems", "Language Models", "attention mechanisms", "user behavior sequences"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03677", "pdf": "https://arxiv.org/pdf/2508.03677.pdf", "abs": "https://arxiv.org/abs/2508.03677", "title": "FairLangProc: A Python package for fairness in NLP", "authors": ["Arturo Pérez-Peralta", "Sandra Benítez-Peña", "Rosa E. Lillo"], "categories": ["cs.CL", "stat.ML", "68T50", "I.2.7"], "comment": "40 pages, 4 figures, 3 tables", "summary": "The rise in usage of Large Language Models to near ubiquitousness in recent\nyears has risen societal concern about their applications in decision-making\ncontexts, such as organizational justice or healthcare. This, in turn, poses\nquestions about the fairness of these models in critical settings, which leads\nto the developement of different procedures to address bias in Natural Language\nProcessing. Although many datasets, metrics and algorithms have been proposed\nto measure and mitigate harmful prejudice in Natural Language Processing, their\nimplementation is diverse and far from centralized. As a response, this paper\npresents FairLangProc, a comprehensive Python package providing a common\nimplementation of some of the more recent advances in fairness in Natural\nLanguage Processing providing an interface compatible with the famous Hugging\nFace transformers library, aiming to encourage the widespread use and\ndemocratization of bias mitigation techniques. The implementation can be found\non https://github.com/arturo-perez-peralta/FairLangProc.", "AI": {"tldr": "The paper introduces FairLangProc, a Python package designed to standardize and implement recent advances in fairness in Natural Language Processing (NLP), particularly in mitigating bias in decision-making contexts.", "motivation": "To address societal concerns regarding the fairness of Large Language Models in decision-making contexts such as healthcare and organizational justice.", "method": "Development of a comprehensive Python package that provides a unified implementation of recent fairness techniques in NLP, compatible with Hugging Face transformers.", "result": "FairLangProc is introduced to encourage the adoption and democratization of bias mitigation techniques in NLP, with centralized methods over diverse existing implementations.", "conclusion": "The package aims to facilitate the integration of fairness practices in NLP applications, promoting equitable outcomes in critical decision-making scenarios.", "key_contributions": ["Introduction of FairLangProc for bias mitigation in NLP", "Compatibility with Hugging Face transformers", "Centralized implementation of fairness techniques"], "limitations": "", "keywords": ["fairness", "natural language processing", "bias mitigation", "large language models", "machine learning"], "importance_score": 9, "read_time_minutes": 40}}
{"id": "2508.03678", "pdf": "https://arxiv.org/pdf/2508.03678.pdf", "abs": "https://arxiv.org/abs/2508.03678", "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation", "authors": ["Yangtian Zi", "Harshitha Menon", "Arjun Guha"], "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": null, "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.", "AI": {"tldr": "This paper introduces PartialOrderEval, a method to augment code generation benchmarks by varying prompt detail to analyze how prompt specificity affects the performance of Large Language Models on specialized tasks.", "motivation": "To investigate the reasons behind the varying performance of LLMs on general versus specialized code generation benchmarks, focusing on the impact of prompt detail.", "method": "Developing PartialOrderEval, a framework that systematically varies prompt specificity from minimal to maximally detailed and measuring its effect on pass rates in HumanEval and ParEval benchmarks.", "result": "Experiments demonstrate that LLM performance, specifically pass@1 scores, varies significantly depending on prompt detail, highlighting the importance of explicit instructions in prompts.", "conclusion": "Improving prompt detail, specifically through explicit I/O specifications and stepwise breakdowns, can enhance LLM performance on coding tasks, indicating a need for tailored prompting strategies.", "key_contributions": ["Introduction of PartialOrderEval for evaluating prompts", "Demonstrated sensitivity of LLM performance to prompt specificity", "Identified key factors in prompt detail that improve model performance"], "limitations": "", "keywords": ["Large Language Models", "code generation", "prompt engineering", "benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.03686", "pdf": "https://arxiv.org/pdf/2508.03686.pdf", "abs": "https://arxiv.org/abs/2508.03686", "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward", "authors": ["Shudong Liu", "Hongwei Liu", "Junnan Liu", "Linchen Xiao", "Songyang Gao", "Chengqi Lyu", "Yuzhe Gu", "Wenwei Zhang", "Derek F. Wong", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report; 31 Pages", "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.", "AI": {"tldr": "The paper presents CompassVerifier, a robust model for answer verification in large language models (LLMs), addressing existing limitations in evaluation frameworks.", "motivation": "To improve the verification process for evaluating LLM outputs against standard answers and to enhance LLM optimization.", "method": "Development of CompassVerifier, a lightweight verifier model, and introduction of the VerifierBench benchmark for systematic evaluation of verification capabilities across LLMs.", "result": "CompassVerifier demonstrates high accuracy and robustness in verifying multi-domain tasks including math and reasoning, effectively handling diverse answer types and identifying invalid responses.", "conclusion": "CompassVerifier and VerifierBench are expected to advance answer verification, evaluation methods, and reinforcement learning in LLM research.", "key_contributions": ["Introduction of CompassVerifier for answer verification", "Development of VerifierBench benchmark", "Demonstration of multi-domain competency in answer verification"], "limitations": "Current verifier development is in early stages and may not yet handle all complex edge cases reducibly.", "keywords": ["answer verification", "large language models", "machine learning", "benchmark", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02823", "pdf": "https://arxiv.org/pdf/2508.02823.pdf", "abs": "https://arxiv.org/abs/2508.02823", "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification", "authors": ["Wenshuo Zhang", "Leixian Shen", "Shuchang Xu", "Jindu Wang", "Jian Zhao", "Huamin Qu", "Linping Yuan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted in UIST 2025", "summary": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency.", "AI": {"tldr": "This paper addresses misalignment between user intent and generated code in conversational LLMs by proposing a new interaction paradigm, direct intent-task matching, implemented in the NeuroSync system.", "motivation": "The work investigates the issue of misalignment between user intent and generated code in conversational LLMs, caused by bidirectional ambiguity in expressing nonlinear intents through linear prompts.", "method": "The paper proposes a new human-LLM interaction paradigm called direct intent-task matching, which externalizes LLM understanding and allows users to manipulate coding tasks and their relationships before code generation, implemented in a system named NeuroSync.", "result": "User study results indicate that NeuroSync enhances intent-task alignment, reduces cognitive effort, and improves coding efficiency.", "conclusion": "NeuroSync provides an effective tool for users to align their intents with coding tasks, thereby addressing the challenges faced in conversational LLM interactions.", "key_contributions": ["Introduces direct intent-task matching for human-LLM interaction", "Develops NeuroSync to visualize and manipulate LLM understanding", "Demonstrates improved alignment and coding efficiency through user studies"], "limitations": "", "keywords": ["Human-Computer Interaction", "Conversational LLMs", "Intent-Task Alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2111.05671", "pdf": "https://arxiv.org/pdf/2111.05671.pdf", "abs": "https://arxiv.org/abs/2111.05671", "title": "Pre-trained Transformer-Based Approach for Arabic Question Answering : A Comparative Study", "authors": ["Kholoud Alsubhi", "Amani Jamal", "Areej Alhothali"], "categories": ["cs.CL"], "comment": "Rewrite the paper", "summary": "Question answering(QA) is one of the most challenging yet widely investigated\nproblems in Natural Language Processing (NLP). Question-answering (QA) systems\ntry to produce answers for given questions. These answers can be generated from\nunstructured or structured text. Hence, QA is considered an important research\narea that can be used in evaluating text understanding systems. A large volume\nof QA studies was devoted to the English language, investigating the most\nadvanced techniques and achieving state-of-the-art results. However, research\nefforts in the Arabic question-answering progress at a considerably slower pace\ndue to the scarcity of research efforts in Arabic QA and the lack of large\nbenchmark datasets. Recently many pre-trained language models provided high\nperformance in many Arabic NLP problems. In this work, we evaluate the\nstate-of-the-art pre-trained transformers models for Arabic QA using four\nreading comprehension datasets which are Arabic-SQuAD, ARCD, AQAD, and\nTyDiQA-GoldP datasets. We fine-tuned and compared the performance of the\nAraBERTv2-base model, AraBERTv0.2-large model, and AraELECTRA model. In the\nlast, we provide an analysis to understand and interpret the low-performance\nresults obtained by some models.", "AI": {"tldr": "This paper evaluates various pre-trained transformer models for Arabic question answering (QA) using multiple datasets to address the slower pace of QA research in Arabic compared to English.", "motivation": "To address the challenges and slow progress in Arabic question answering research due to a lack of resources and datasets, as well as to leverage recent advancements in pre-trained language models.", "method": "The authors fine-tune and compare the performance of three pre-trained models: AraBERTv2-base, AraBERTv0.2-large, and AraELECTRA on four Arabic reading comprehension datasets.", "result": "The paper details the performance metrics of the models across the datasets, highlighting the effectiveness of the pre-trained models in Arabic QA.", "conclusion": "Despite advancements in pre-trained language models, some models still exhibit low performance in certain contexts, indicating the need for further investigation and understanding of these limitations.", "key_contributions": ["Evaluation of state-of-the-art pre-trained transformer models for Arabic QA.", "Comparison across multiple Arabic reading comprehension datasets.", "Analysis of low-performance results to provide insights into model behavior."], "limitations": "The study is limited by the scarcity of Arabic QA datasets and the variability in model performance across different contexts.", "keywords": ["question answering", "Arabic NLP", "pre-trained models", "transformers", "reading comprehension"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2409.16647", "pdf": "https://arxiv.org/pdf/2409.16647.pdf", "abs": "https://arxiv.org/abs/2409.16647", "title": "Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data", "authors": ["Kota Dohi", "Aoi Ito", "Harsh Purohit", "Tomoya Nishida", "Takashi Endo", "Yohei Kawaguchi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Due to scarcity of time-series data annotated with descriptive texts,\ntraining a model to generate descriptive texts for time-series data is\nchallenging. In this study, we propose a method to systematically generate\ndomain-independent descriptive texts from time-series data. We identify two\ndistinct approaches for creating pairs of time-series data and descriptive\ntexts: the forward approach and the backward approach. By implementing the\nnovel backward approach, we create the Temporal Automated Captions for\nObservations (TACO) dataset. Experimental results demonstrate that a\ncontrastive learning based model trained using the TACO dataset is capable of\ngenerating descriptive texts for time-series data in novel domains.", "AI": {"tldr": "This study proposes a method for generating descriptive texts from time-series data using a novel dataset called TACO.", "motivation": "The scarcity of annotated time-series data makes training models for generating descriptive texts challenging.", "method": "The study introduces two approaches: a forward approach and a backward approach. The backward approach is used to create the TACO dataset.", "result": "A contrastive learning-based model trained on the TACO dataset demonstrated the ability to generate descriptive texts for time-series data across various domains.", "conclusion": "The proposed method successfully enables the generation of domain-independent descriptive texts from time-series data.", "key_contributions": ["Introduction of the TACO dataset", "Development of a novel backward approach for text generation", "Demonstration of effective contrastive learning for time-series descriptions"], "limitations": "", "keywords": ["time-series data", "descriptive text generation", "TACO dataset", "contrastive learning", "domain-independent"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2412.08920", "pdf": "https://arxiv.org/pdf/2412.08920.pdf", "abs": "https://arxiv.org/abs/2412.08920", "title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning", "authors": ["Pusen Dong", "Tianchen Zhu", "Yue Qiu", "Haoyi Zhou", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NeurIPS 2024", "summary": "Safe reinforcement learning (RL) requires the agent to finish a given task\nwhile obeying specific constraints. Giving constraints in natural language form\nhas great potential for practical scenarios due to its flexible transfer\ncapability and accessibility. Previous safe RL methods with natural language\nconstraints typically need to design cost functions manually for each\nconstraint, which requires domain expertise and lacks flexibility. In this\npaper, we harness the dual role of text in this task, using it not only to\nprovide constraint but also as a training signal. We introduce the\nTrajectory-level Textual Constraints Translator (TTCT) to replace the manually\ndesigned cost function. Our empirical results demonstrate that TTCT effectively\ncomprehends textual constraint and trajectory, and the policies trained by TTCT\ncan achieve a lower violation rate than the standard cost function. Extra\nstudies are conducted to demonstrate that the TTCT has zero-shot transfer\ncapability to adapt to constraint-shift environments.", "AI": {"tldr": "Introducing a novel method to handle safe reinforcement learning using natural language constraints without manual cost function design.", "motivation": "To enhance the flexibility and accessibility of safe reinforcement learning by using natural language constraints instead of manual cost function design.", "method": "The Trajectory-level Textual Constraints Translator (TTCT) is introduced, which uses natural language both as a constraint and a training signal.", "result": "TTCT improves comprehension of constraints and trajectories, resulting in policies with a lower violation rate compared to traditional cost functions, and demonstrates zero-shot transfer capability to constraint-shift environments.", "conclusion": "TTCT shows promise in making safe RL more adaptable and efficient by utilizing natural language constraints effectively.", "key_contributions": ["TTCT replaces manually designed cost functions with a natural language constraint translator.", "Improvements in policy compliance with constraints, leading to lower violation rates.", "Demonstration of zero-shot transfer capability to adapt to changing constraints."], "limitations": "", "keywords": ["safe reinforcement learning", "natural language constraints", "TTCT"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.14854", "pdf": "https://arxiv.org/pdf/2502.14854.pdf", "abs": "https://arxiv.org/abs/2502.14854", "title": "CLIPPER: Compression enables long-context synthetic data generation", "authors": ["Chau Minh Pham", "Yapei Chang", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA).", "AI": {"tldr": "CLIPPER is a compression-based approach to generate high-quality synthetic data for narrative claim verification by summarizing books before claim generation.", "motivation": "Generating high-quality synthetic data for long-context reasoning tasks, specifically narrative claim verification, is challenging.", "method": "CLIPPER compresses books into chapter outlines and summaries before generating claims and reasoning, enhancing the quality of the outputs compared to direct claim generation.", "result": "CLIPPER successfully created a dataset with 19K synthetic claims and improved a model's accuracy on narrative claim verification from 28% to 76%, achieving state-of-the-art results for sub-10B models.", "conclusion": "Using CLIPPER improves the validity and complexity of claims for narrative tasks and enhances chain-of-thought reasoning capabilities.", "key_contributions": ["Introduction of CLIPPER for synthetic data generation in narrative claim verification.", "Creation of a 19K dataset of synthetic claims paired with text and reasoning.", "Achieving a significant accuracy improvement on narrative claim verification tasks."], "limitations": "", "keywords": ["synthetic data", "narrative claim verification", "compression-based generation", "chain-of-thought reasoning", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.05347", "pdf": "https://arxiv.org/pdf/2503.05347.pdf", "abs": "https://arxiv.org/abs/2503.05347", "title": "GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for Radiology Report Evaluation", "authors": ["Zhenxuan Zhang", "Kinhei Lee", "Peiyuan Jing", "Weihang Deng", "Huichi Zhou", "Zihao Jin", "Jiahao Huang", "Zhifan Gao", "Dominic C Marshall", "Yingying Fang", "Guang Yang"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Automatic medical report generation has the potential to support clinical\ndiagnosis, reduce the workload of radiologists, and demonstrate potential for\nenhancing diagnostic consistency. However, current evaluation metrics often\nfail to reflect the clinical reliability of generated reports. Early\noverlap-based methods focus on textual matches between predicted and\nground-truth entities but miss fine-grained clinical details (e.g., anatomical\nlocation, severity). Some diagnostic metrics are limited by fixed vocabularies\nor templates, reducing their ability to capture diverse clinical expressions.\nLLM-based approaches further lack interpretable reasoning steps, making it hard\nto assess or trust their behavior in safety-critical settings. These\nlimitations hinder the comprehensive assessment of the reliability of generated\nreports and pose risks in their selection for clinical use. Therefore, we\npropose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper,\nwhich conducts both objective quantification and subjective evaluation through\na large language model-based multi-agent workflow. Our GEMA-Score parses\nstructured reports and employs stable calculations through interactive\nexchanges of information among agents to assess disease diagnosis, location,\nseverity, and uncertainty. Additionally, an LLM-based scoring agent evaluates\ncompleteness, readability, and clinical terminology while providing explanatory\nfeedback. Extensive experiments validate that GEMA-Score achieves the highest\ncorrelation with human expert evaluations on a public dataset, demonstrating\nits effectiveness in clinical scoring (Kendall coefficient = $0.69$ for ReXVal\ndataset and Kendall coefficient = $0.45$ for RadEvalX dataset). The anonymous\nproject demo is available at: https://github.com/Zhenxuan-Zhang/GEMA_score.", "AI": {"tldr": "This paper introduces GEMA-Score, a new evaluation metric for automatic medical report generation that focuses on evaluating the clinical reliability of generated reports.", "motivation": "Current evaluation metrics for automatic medical report generation fail to adequately capture clinical reliability, leading to potential risks in clinical use.", "method": "The GEMA-Score uses a multi-agent workflow with a large language model to perform both objective quantification and subjective evaluation of medical reports.", "result": "GEMA-Score shows a high correlation with human expert evaluations, achieving a Kendall coefficient of 0.69 for the ReXVal dataset and 0.45 for the RadEvalX dataset.", "conclusion": "GEMA-Score is effective in assessing the clinical reliability of automated medical reports and provides explanatory feedback for further improvement.", "key_contributions": ["Introduction of GEMA-Score for evaluating clinical reports", "Utilization of multi-agent workflow for comprehensive evaluation", "Demonstrated high correlation with human expert evaluations"], "limitations": "", "keywords": ["medical report generation", "evaluation metrics", "clinical reliability", "multi-agent workflow", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.11881", "pdf": "https://arxiv.org/pdf/2503.11881.pdf", "abs": "https://arxiv.org/abs/2503.11881", "title": "GPT is Devastated and LLaMA is Content: Emotion Representation Alignment in LLMs for Keyword-based Generation", "authors": ["Shadab Choudhury", "Asha Kumar", "Lara J. Martin"], "categories": ["cs.CL"], "comment": null, "summary": "In controlled text generation using large language models (LLMs), gaps arise\nbetween the language model's interpretation of concepts and people's\nexpectations. We introduce the human evaluation task of Representation\nAlignment for measuring this gap. We selected four emotion representations:\nWords, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and\nNumeric forms, and Emojis and evaluate them in the context of keyword-guided\nsentence generation using both GPT-4 and LLaMA-3. In addition to Representation\nAlignment, we also measure people's judgments of the accuracy and realism of\nthe generated sentences. While representations like VAD break emotions into\neasy-to-compute components, our findings show that people agree more with how\nLLMs generate when conditioned on English words (e.g., ``angry'') rather than\nVAD scales. This difference is especially visible when comparing Numeric VAD to\nwords. Furthermore, we found that the perception of how much a generated\nsentence conveys an emotion is dependent on both the representation type and\nwhich emotion it is.", "AI": {"tldr": "This paper introduces Representation Alignment, a human evaluation task for measuring the gap between LLM interpretations of concepts and human expectations on emotion representation in text generation.", "motivation": "Understanding and measuring the gap between LLM interpretations of concepts and human expectations in text generation, particularly with emotions.", "method": "Human evaluation of four emotion representations (Words, VAD in Lexical and Numeric forms, Emojis) during keyword-guided sentence generation with GPT-4 and LLaMA-3.", "result": "People found that LLMs generate more agreeable outputs when using words over VAD representations, particularly Numeric VAD, indicating a dependency of emotion perception on representation type.", "conclusion": "The findings suggest that using English words leads to better alignment with human interpretations of emotions in LLM-generated sentences over other representation forms.", "key_contributions": ["Introduction of the Representation Alignment task", "Empirical findings showing the preference for words over VAD scales in emotion representation", "Insights into how different emotion representations impact perception in generated sentences."], "limitations": "Limited to emotion representations and specific language models used; further research needed on broader contexts.", "keywords": ["Representation Alignment", "Emotion Representation", "Large Language Models", "Human Evaluation", "Text Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.13505", "pdf": "https://arxiv.org/pdf/2503.13505.pdf", "abs": "https://arxiv.org/abs/2503.13505", "title": "Ensemble Learning for Large Language Models in Text and Code Generation: A Survey", "authors": ["Mari Ashiga", "Wei Jie", "Fan Wu", "Vardan Voskanyan", "Fateme Dinmohammadi", "Paul Brookes", "Jingzhi Gong", "Zheng Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review by IEEE TAI", "summary": "Generative Pretrained Transformers (GPTs) are foundational Large Language\nModels (LLMs) for text generation. However, individual LLMs often produce\ninconsistent outputs and exhibit biases, limiting their representation of\ndiverse language patterns. The closed-source nature of many powerful LLMs\nfurther restricts industry applications due to data privacy concerns. Inspired\nby successes in text generation, LLM ensemble techniques are now increasingly\nexplored for code generation. This article reviews these emerging ensemble\napproaches to enhance understanding, encourage further research, and promote\npractical implementation in both text and code generation. We categorize LLM\nensembles into seven main methods - weight merging, knowledge fusion,\nmixture-of-experts, reward ensemble, output ensemble, routing, and cascading -\nanalyzing capabilities of those approaches. Our findings highlight key benefits\nsuch as improved diversity representation, enhanced output quality, and greater\napplication flexibility. These insights aid model selection for real-world\ntasks and crucially, lay groundwork for extending ensemble strategies to\nmultimodal LLMs.", "AI": {"tldr": "This article reviews ensemble techniques for Generative Pretrained Transformers (GPTs) to improve text and code generation outputs.", "motivation": "The need to address inconsistencies and biases in individual LLMs and the challenges posed by the closed-source nature of powerful LLMs.", "method": "The paper categorizes LLM ensemble techniques into seven methods: weight merging, knowledge fusion, mixture-of-experts, reward ensemble, output ensemble, routing, and cascading, analyzing their capabilities.", "result": "The review highlights benefits such as improved diversity representation, enhanced output quality, and greater application flexibility.", "conclusion": "The insights from this paper assist in model selection for real-world tasks and set the stage for the application of ensemble strategies in multimodal LLMs.", "key_contributions": ["Categorization of ensemble techniques for LLMs", "Analysis of capabilities and benefits of LLM ensembles", "Foundation for extending ensemble approaches to multimodal models"], "limitations": "", "keywords": ["Generative Pretrained Transformers", "LLM ensemble techniques", "code generation", "text generation", "multimodal LLMs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.06219", "pdf": "https://arxiv.org/pdf/2504.06219.pdf", "abs": "https://arxiv.org/abs/2504.06219", "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs", "authors": ["Dongyang Fan", "Vinko Sabolčec", "Matin Ansaripour", "Ayush Kumar Tarun", "Martin Jaggi", "Antoine Bosselut", "Imanol Schlag"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025 Camera Ready version", "summary": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions. Our website is available at\nhttps://data-compliance.github.io/.", "AI": {"tldr": "This study analyzes the impact of web crawling opt-outs on the performance of large language models (LLMs).", "motivation": "As copyright holders increasingly opt out of web crawling, understanding how these restrictions affect LLM performance is crucial.", "method": "We define and measure the data compliance gap (DCG) in pretraining models from scratch and in continual pretraining from compliant models, using experiments on 1.5B parameter models.", "result": "The results show that compliance with web data opt-outs does not significantly impact general knowledge acquisition, but declines are observed in specialized domains like biomedical research without access to major publishers.", "conclusion": "General-purpose LLMs may be trained effectively with open data, but specialized domains benefit from high-quality copyrighted sources; this informs AI training practices and policy.", "key_contributions": ["Introduces the concept of data compliance gap (DCG) in LLM training.", "Demonstrates the negligible impact of compliance on general knowledge acquisition.", "Highlights performance decline in specialized domains due to lack of access to copyrighted sources."], "limitations": "The analysis is based on models as of January 2025, and further research is needed to confirm ongoing trends.", "keywords": ["Large Language Models", "Data Compliance", "Web Crawling", "Machine Learning", "Biomedical Research"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.07724", "pdf": "https://arxiv.org/pdf/2504.07724.pdf", "abs": "https://arxiv.org/abs/2504.07724", "title": "The Multi-Round Diagnostic RAG Framework for Emulating Clinical Reasoning", "authors": ["Penglei Sun", "Yixiang Chen", "Xiang Li", "Xiaowen Chu"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a trend. Among these, retrieval-augmented generation\n(RAG) has garnered attention due to rapid deployment and privacy protection.\nHowever, the challenge hinder the practical deployment of RAG for medical\ndiagnosis: the semantic gap between colloquial patient descriptions and the\nprofessional terminology within medical knowledge bases. We try to address the\nchallenge from the data perspective and the method perspective. First, to\naddress the semantic gap in existing knowledge bases, we construct\nDiagnosGraph, a generalist knowledge graph covering both modern medicine and\nTraditional Chinese Medicine. It contains 876 common diseases with the graph of\n7,997 nodes and 37,201 triples. To bridge the gap between colloquial patient\nnarratives and academic medical knowledge, DiagnosGraph also introduces $1,908$\nmedical record by formalizing the patient chief complaint and proposing a\nmedical diagnosis. Second, we introduce the Multi-Round Diagnostic RAG\n(MRD-RAG) framework. It utilizes a multi-round dialogue to refine diagnostic\npossibilities, emulating the clinical reasoning of a physician. Experiments\nconducted on four medical benchmarks, with evaluations by human physicians,\ndemonstrate that MRD-RAG enhances the diagnostic performance of LLMs,\nhighlighting its potential to make automated diagnosis more accurate and\nhuman-aligned.", "AI": {"tldr": "The paper presents DiagnosGraph, a knowledge graph and the Multi-Round Diagnostic Retrieval-Augmented Generation (MRD-RAG) framework to improve automated medical diagnosis using large language models.", "motivation": "The growing trend in rapidly deploying medical large language models (LLMs) faces challenges due to the semantic gap between patient descriptions and professional medical terminology.", "method": "The authors constructed DiagnosGraph, a knowledge graph that bridges colloquial and professional medical language, and developed the MRD-RAG framework that employs a multi-round dialogue process for refined diagnostics.", "result": "Experiments showed that MRD-RAG significantly improves the diagnostic performance of LLMs, evaluated positively by human physicians.", "conclusion": "The findings suggest that utilizing knowledge graphs and multi-round dialogues can enhance automated medical diagnosis, making it more accurate and aligned with human reasoning.", "key_contributions": ["Construction of DiagnosGraph, covering both modern and Traditional Chinese Medicine.", "Development of MRD-RAG framework for multi-round dialogue in diagnostics.", "Demonstration of enhanced diagnostic performance through experimental evaluations."], "limitations": "", "keywords": ["medical diagnosis", "large language models", "knowledge graph", "retrieval-augmented generation", "multi-round dialogue"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2504.12326", "pdf": "https://arxiv.org/pdf/2504.12326.pdf", "abs": "https://arxiv.org/abs/2504.12326", "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis", "authors": ["Shahriar Noroozizadeh", "Jeremy C. Weiss"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration.", "AI": {"tldr": "Pipeline to phenotype, extract, and annotate time-localized findings in clinical case reports using large language models, creating an open-access corpus for Sepsis-3.", "motivation": "To leverage clinical case reports for richer, temporally accurate data for model training, addressing limitations of existing structured data streams.", "method": "Construct a pipeline to extract and annotate clinical findings from case reports, generating a textual time series corpus and validating against expert annotations.", "result": "Achieved high recovery rates of clinical findings with strong temporal ordering as validated against physician-expert annotations.", "conclusion": "The study demonstrates the capability of LLMs for time-localizing clinical findings, highlights limitations for temporal reconstruction, and suggests improvements through multimodal integration.", "key_contributions": ["Development of a pipeline for time-localization in clinical text", "Creation of an open-access textual time series corpus for Sepsis-3", "Validation of LLM capabilities against expert annotation standards"], "limitations": "LLMs show limitations in temporal reconstruction that need addressing with multimodal approaches.", "keywords": ["Clinical case reports", "Large language models", "Temporal localization", "Sepsis-3", "Textual time series"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.13134", "pdf": "https://arxiv.org/pdf/2504.13134.pdf", "abs": "https://arxiv.org/abs/2504.13134", "title": "Energy-Based Reward Models for Robust Language Model Alignment", "authors": ["Anamika Lochab", "Ruqi Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Accepted by COLM 2025", "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "AI": {"tldr": "This paper introduces the Energy-Based Reward Model (EBRM), a framework that enhances the robustness and generalization of reward models used for aligning LLMs with human preferences.", "motivation": "The paper addresses challenges in aligning Large Language Models (LLMs) with complex human preferences and the difficulty in generalizing to unseen data.", "method": "EBRM employs conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization to explicitly model reward distribution and capture uncertainty in human preferences without requiring retraining.", "result": "Empirical evaluations show EBRM achieves significant improvements in robustness and generalization, with up to a 5.97% enhancement in safety-critical alignment tasks and improved alignment quality in reinforcement learning experiments.", "conclusion": "EBRM is a scalable and effective enhancement for existing reward models and alignment pipelines, demonstrating efficiency and adaptability across various models and tasks.", "key_contributions": ["Introduces a lightweight post-hoc refinement framework for reward models.", "Enhances RM robustness and generalization without the need for retraining.", "Achieves empirical improvements in alignment tasks through innovative training techniques."], "limitations": "", "keywords": ["Energy-Based Reward Model", "human preferences", "alignment", "Large Language Models", "robustness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.13834", "pdf": "https://arxiv.org/pdf/2504.13834.pdf", "abs": "https://arxiv.org/abs/2504.13834", "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific knowledge is growing rapidly, making it difficult to track\nprogress and high-level conceptual links across broad disciplines. While tools\nlike citation networks and search engines help retrieve related papers, they\nlack the abstraction needed to capture the needed to represent the density and\nstructure of activity across subfields.\n  We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific\nliterature into a high-quality hierarchical structure that spans multiple\nlevels of abstraction -- from broad domains to specific studies. Such a\nrepresentation can provide insights into which fields are well-explored and\nwhich are under-explored. To achieve this goal, we develop a hybrid approach\nthat combines efficient embedding-based clustering with LLM-based prompting,\nstriking a balance between scalability and semantic precision. Compared to\nLLM-heavy methods like iterative tree construction, our approach achieves\nsuperior quality-speed trade-offs. Our hierarchies capture different dimensions\nof research contributions, reflecting the interdisciplinary and multifaceted\nnature of modern science. We evaluate its utility by measuring how effectively\nan LLM-based agent can navigate the hierarchy to locate target papers. Results\nshow that our method improves interpretability and offers an alternative\npathway for exploring scientific literature beyond traditional search methods.\nCode, data and demo are available:\nhttps://github.com/JHU-CLSP/science-hierarchography", "AI": {"tldr": "Introducing SCIENCE HIERARCHOGRAPHY for organizing scientific literature hierarchically, blending embedding-based clustering with LLM prompting for improved research navigation.", "motivation": "The rapid growth of scientific knowledge makes it challenging to track progress across disciplines, necessitating a method to represent the density and structure of activity in science.", "method": "A hybrid approach combining embedding-based clustering with LLM-based prompting to create a hierarchical structure of scientific literature.", "result": "The method enhances interpretability and allows a LLM-based agent to better navigate the hierarchical structure, improving research paper location efficiency.", "conclusion": "The proposed hierarchies provide insights into explored and under-explored fields and serve as an alternative to traditional literature search approaches.", "key_contributions": ["Development of SCIENCE HIERARCHOGRAPHY for organizing literature hierarchically.", "Fusion of embedding-based clustering with LLM prompting.", "Improved interpretability and navigation of scientific literature."], "limitations": "", "keywords": ["scientific literature", "hierarchical structure", "embedding-based clustering", "LLM prompting", "research navigation"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2404.17730", "pdf": "https://arxiv.org/pdf/2404.17730.pdf", "abs": "https://arxiv.org/abs/2404.17730", "title": "Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults", "authors": ["Lara J. Martin", "Malathy Nagalakshmi"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "High-tech Augmentative and Alternative Communication (AAC) has been rapidly\nadvancing in recent years due to the increased use of large language models\n(LLMs) like ChatGPT, but many of these techniques are integrated without the\ninclusion of the users' perspectives. Autistic adults have been particularly\nneglected in the design of AAC tools. We conducted in-depth interviews with 12\nautistic adults to find the pain points of current AAC and determine what\ntechnological advances they might find helpful. We found 8 different categories\nof themes from our interviews: input flexibility, output flexibility, selecting\nor adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling\nblocks for continued use, and control of communication. In this paper, we go\nthrough these categories in depth -- comparing each to prior work -- and then\nhighlight novel findings to suggest possible research directions.", "AI": {"tldr": "Investigation of AAC tools for autistic adults, highlighting user perspectives and identifying key themes for improvement.", "motivation": "To address the neglect of autistic adults in the design of AAC tools, particularly with the advent of LLMs.", "method": "In-depth interviews with 12 autistic adults to identify pain points and technological needs.", "result": "Identified 8 themes: input/output flexibility, AAC adaptation, usage contexts, benefits, adult access, barriers to usage, and communication control.", "conclusion": "The findings suggest new directions for research in designing more effective AAC tools by focusing on user perspectives.", "key_contributions": ["Identified specific needs of autistic adults in AAC design", "Categorized pain points for existing AAC technology", "Proposed new research directions based on user feedback"], "limitations": "", "keywords": ["Augmentative Communication", "Autistic Adults", "User-Centered Design", "Large Language Models", "Assistive Technology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.13920", "pdf": "https://arxiv.org/pdf/2502.13920.pdf", "abs": "https://arxiv.org/abs/2502.13920", "title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health", "authors": ["Xingbo Wang", "Janessa Griffith", "Daniel A. Adler", "Joey Castillo", "Tanzeem Choudhury", "Fei Wang"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2025). Code is available at https://github.com/xingbow/sleephealthLLM", "summary": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots.", "AI": {"tldr": "HealthGuru is a chatbot enhanced by a large language model aimed at improving sleep health through personalized, data-driven recommendations and behavior change support.", "motivation": "Many sleep-tracking devices fail to provide actionable and personalized insights for sleep improvement; HealthGuru aims to bridge this gap.", "method": "HealthGuru employs a multi-agent framework that integrates data from wearable devices and context-aware inputs, using a contextual multi-armed bandit model to offer tailored recommendations and support dialogue.", "result": "The eight-week study demonstrated that HealthGuru improved sleep duration, activity metrics, and user motivation compared to a baseline chatbot.", "conclusion": "The findings indicate the effectiveness of HealthGuru in promoting better sleep through personalized recommendations and underscore the need for improved personalization and engagement strategies in health chatbots.", "key_contributions": ["Introduction of a novel LLM-powered chatbot for sleep improvement", "Integration of contextual data and behavior change techniques", "Demonstrated effectiveness through an in-the-wild study."], "limitations": "Challenges in personalization and user engagement identified during the study.", "keywords": ["Sleep health", "Large language models", "Chatbots", "Health informatics", "Behavior change"], "importance_score": 9, "read_time_minutes": 10}}
