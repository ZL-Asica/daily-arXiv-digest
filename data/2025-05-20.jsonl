{"id": "2505.11666", "pdf": "https://arxiv.org/pdf/2505.11666.pdf", "abs": "https://arxiv.org/abs/2505.11666", "title": "DesignFromX: Empowering Consumer-Driven Design Space Exploration through Feature Composition of Referenced Products", "authors": ["Runlin Duan", "Chenfei Zhu", "Yuzhao Chen", "Yichen Hu", "Jingyu Shi", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Industrial products are designed to satisfy the needs of consumers. The rise\nof generative artificial intelligence (GenAI) enables consumers to easily\nmodify a product by prompting a generative model, opening up opportunities to\nincorporate consumers in exploring the product design space. However, consumers\noften struggle to articulate their preferred product features due to their\nunfamiliarity with terminology and their limited understanding of the structure\nof product features. We present DesignFromX, a system that empowers\nconsumer-driven design space exploration by helping consumers to design a\nproduct based on their preferences. Leveraging an effective GenAI-based\nframework, the system allows users to easily identify design features from\nproduct images and compose those features to generate conceptual images and 3D\nmodels of a new product. A user study with 24 participants demonstrates that\nDesignFromX lowers the barriers and frustration for consumer-driven design\nspace explorations by enhancing both engagement and enjoyment for the\nparticipants."}
{"id": "2505.11684", "pdf": "https://arxiv.org/pdf/2505.11684.pdf", "abs": "https://arxiv.org/abs/2505.11684", "title": "Designing for Constructive Civic Communication: A Framework for Human-AI Collaboration in Community Engagement Processes", "authors": ["Cassandra Overney"], "categories": ["cs.HC"], "comment": "10 pages, 2 figures, report", "summary": "Community engagement processes form a critical foundation of democratic\ngovernance, yet frequently struggle with resource constraints, sensemaking\nchallenges, and barriers to inclusive participation. These processes rely on\nconstructive communication between public leaders and community organizations\ncharacterized by understanding, trust, respect, legitimacy, and agency. As\nartificial intelligence (AI) technologies become increasingly integrated into\ncivic contexts, they offer promising capabilities to streamline\nresource-intensive workflows, reveal new insights in community feedback,\ntranslate complex information into accessible formats, and facilitate\nreflection across social divides. However, these same systems risk undermining\ndemocratic processes through accuracy issues, transparency gaps, bias\namplification, and threats to human agency. In this paper, we examine how\nhuman-AI collaboration might address these risks and transform civic\ncommunication dynamics by identifying key communication pathways and proposing\ndesign considerations that maintain a high level of control over\ndecision-making for both public leaders and communities while leveraging\ncomputer automation. By thoughtfully integrating AI to amplify human connection\nand understanding while safeguarding agency, community engagement processes can\nutilize AI to promote more constructive communication in democratic governance."}
{"id": "2505.11715", "pdf": "https://arxiv.org/pdf/2505.11715.pdf", "abs": "https://arxiv.org/abs/2505.11715", "title": "ConflictLens: LLM-Based Conflict Resolution Training in Romantic Relationship", "authors": ["Jiwon Chun", "Gefei Zhang", "Meng Xia"], "categories": ["cs.HC"], "comment": null, "summary": "Romantic conflicts are often rooted in deep psychological factors such as\ncoping styles, emotional responses, and communication habits. Existing systems\ntend to address surface-level behaviors or isolated events, offering limited\nsupport for understanding the underlying dynamics. We present ConflictLens, an\ninteractive system that leverages psychological theory and large language\nmodels (LLMs) to help individuals analyze and reflect on the deeper mechanisms\nbehind their conflicts. The system provides multi-level strategy\nrecommendations and guided dialogue exercises, including annotation, rewriting,\nand continuation tasks. A case study demonstrates how ConflictLens supports\nemotional insight, improves relational understanding, and fosters more\nconstructive communication. This work offers a novel approach to supporting\nself-awareness and growth in romantic relationships."}
{"id": "2505.11784", "pdf": "https://arxiv.org/pdf/2505.11784.pdf", "abs": "https://arxiv.org/abs/2505.11784", "title": "Utilizing Provenance as an Attribute for Visual Data Analysis: A Design Probe with ProvenanceLens", "authors": ["Arpit Narechania", "Shunan Guo", "Eunyee Koh", "Alex Endert", "Jane Hoffswell"], "categories": ["cs.HC"], "comment": "14 pages, 6 figures, 1 table, accepted in IEEE TVCG 2025", "summary": "Analytic provenance can be visually encoded to help users track their ongoing\nanalysis trajectories, recall past interactions, and inform new analytic\ndirections. Despite its significance, provenance is often hardwired into\nanalytics systems, affording limited user control and opportunities for\nself-reflection. We thus propose modeling provenance as an attribute that is\navailable to users during analysis. We demonstrate this concept by modeling two\nprovenance attributes that track the recency and frequency of user interactions\nwith data. We integrate these attributes into a visual data analysis system\nprototype, ProvenanceLens, wherein users can visualize their interaction\nrecency and frequency by mapping them to encoding channels (e.g., color, size)\nor applying data transformations (e.g., filter, sort). Using ProvenanceLens as\na design probe, we conduct an exploratory study with sixteen users to\ninvestigate how these provenance-tracking affordances are utilized for both\ndecision-making and self-reflection. We find that users can accurately and\nconfidently answer questions about their analysis, and we show that mismatches\nbetween the user's mental model and the provenance encodings can be surprising,\nthereby prompting useful self-reflection. We also report on the user strategies\nsurrounding these affordances, and reflect on their intuitiveness and\neffectiveness in representing provenance."}
{"id": "2505.11533", "pdf": "https://arxiv.org/pdf/2505.11533.pdf", "abs": "https://arxiv.org/abs/2505.11533", "title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "categories": ["cs.CL"], "comment": null, "summary": "In the tourism domain, Large Language Models (LLMs) often struggle to mine\nimplicit user intentions from tourists' ambiguous inquiries and lack the\ncapacity to proactively guide users toward clarifying their needs. A critical\nbottleneck is the scarcity of high-quality training datasets that facilitate\nproactive questioning and implicit intention mining. While recent advances\nleverage LLM-driven data synthesis to generate such datasets and transfer\nspecialized knowledge to downstream models, existing approaches suffer from\nseveral shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed\ndistributions of detail levels in initial inquiries, (3) contextual redundancy\nin the implicit intention mining module, and (4) lack of explicit thinking\nabout tourists' emotions and intention values. Therefore, we propose SynPT (A\nData Synthesis Method Driven by LLMs for Proactive Mining of Implicit User\nIntentions in the Tourism), which constructs an LLM-driven user agent and\nassistant agent to simulate dialogues based on seed data collected from Chinese\ntourism websites. This approach addresses the aforementioned limitations and\ngenerates SynPT-Dialog, a training dataset containing explicit reasoning. The\ndataset is utilized to fine-tune a general LLM, enabling it to proactively mine\nimplicit user intentions. Experimental evaluations, conducted from both human\nand LLM perspectives, demonstrate the superiority of SynPT compared to existing\nmethods. Furthermore, we analyze key hyperparameters and present case studies\nto illustrate the practical applicability of our method, including discussions\non its adaptability to English-language scenarios. All code and data are\npublicly available."}
{"id": "2505.11888", "pdf": "https://arxiv.org/pdf/2505.11888.pdf", "abs": "https://arxiv.org/abs/2505.11888", "title": "AR Secretary Agent: Real-time Memory Augmentation via LLM-powered Augmented Reality Glasses", "authors": ["Raphaël A. El Haddad", "Zeyu Wang", "Yeonsu Shin", "Ranyi Liu", "Yuntao Wang", "Chun Yu"], "categories": ["cs.HC"], "comment": null, "summary": "Interacting with a significant number of individuals on a daily basis is\ncommonplace for many professionals, which can lead to challenges in recalling\nspecific details: Who is this person? What did we talk about last time? The\nadvant of augmented reality (AR) glasses, equipped with visual and auditory\ndata capture capabilities, presents a solution. In our work, we implemented an\nAR Secretary Agent with advanced Large Language Models (LLMs) and Computer\nVision technologies. This system could discreetly provide real-time information\nto the wearer, identifying who they are conversing with and summarizing\nprevious discussions. To verify AR Secretary, we conducted a user study with 13\nparticipants and showed that our technique can efficiently help users to\nmemorize events by up to 20\\% memory enhancement on our study."}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550.pdf", "abs": "https://arxiv.org/abs/2505.11550", "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627."}
{"id": "2505.11996", "pdf": "https://arxiv.org/pdf/2505.11996.pdf", "abs": "https://arxiv.org/abs/2505.11996", "title": "To Recommend or Not to Recommend: Designing and Evaluating AI-Enabled Decision Support for Time-Critical Medical Events", "authors": ["Angela Mastrianni", "Mary Suhyun Kim", "Travis M. Sullivan", "Genevieve Jayne Sippel", "Randall S. Burd", "Krzysztof Z. Gajos", "Aleksandra Sarcevic"], "categories": ["cs.HC"], "comment": null, "summary": "AI-enabled decision-support systems aim to help medical providers rapidly\nmake decisions with limited information during medical emergencies. A critical\nchallenge in developing these systems is supporting providers in interpreting\nthe system output to make optimal treatment decisions. In this study, we\ndesigned and evaluated an AI-enabled decision-support system to aid providers\nin treating patients with traumatic injuries. We first conducted user research\nwith physicians to identify and design information types and AI outputs for a\ndecision-support display. We then conducted an online experiment with 35\nmedical providers from six health systems to evaluate two human-AI interaction\nstrategies: (1) AI information synthesis and (2) AI information and\nrecommendations. We found that providers were more likely to make correct\ndecisions when AI information and recommendations were provided compared to\nreceiving no AI support. We also identified two socio-technical barriers to\nproviding AI recommendations during time-critical medical events: (1) an\naccuracy-time trade-off in providing recommendations and (2) polarizing\nperceptions of recommendations between providers. We discuss three implications\nfor developing AI-enabled decision support used in time-critical events,\ncontributing to the limited research on human-AI interaction in this context."}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556.pdf", "abs": "https://arxiv.org/abs/2505.11556", "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction."}
{"id": "2505.12064", "pdf": "https://arxiv.org/pdf/2505.12064.pdf", "abs": "https://arxiv.org/abs/2505.12064", "title": "From Data to Actionable Understanding: A Learner-Centered Framework for Dynamic Learning Analytics", "authors": ["Madjid Sadallah"], "categories": ["cs.HC"], "comment": null, "summary": "Learning Analytics Dashboards (LADs) often fall short of their potential to\nempower learners, frequently prioritizing data visualization over the cognitive\nprocesses crucial for translating data into actionable learning strategies.\nThis represents a significant gap in the field: while much research has focused\non data collection and presentation, there is a lack of comprehensive models\nfor how LADs can actively support learners' sensemaking and self-regulation.\nThis paper introduces the Adaptive Understanding Framework (AUF), a novel\nconceptual model for learner-centered LAD design. The AUF seeks to address this\nlimitation by integrating a multi-dimensional model of situational awareness,\ndynamic sensemaking strategies, adaptive mechanisms, and metacognitive support.\nThis transforms LADs into dynamic learning partners that actively scaffold\nlearners' sensemaking. Unlike existing frameworks that tend to treat these\naspects in isolation, the AUF emphasizes their dynamic and intertwined\nrelationships, creating a personalized and adaptive learning ecosystem that\nresponds to individual needs and evolving understanding. The paper details the\nAUF's core principles, key components, and suggests a research agenda for\nfuture empirical validation. By fostering a deeper, more actionable\nunderstanding of learning data, AUF-inspired LADs have the potential to promote\nmore effective, equitable, and engaging learning experiences."}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604.pdf", "abs": "https://arxiv.org/abs/2505.11604", "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Jaehyeok Jang", "Jagul Choo"], "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "Existing research on large language models (LLMs) for PowerPoint\npredominantly focuses on slide generation, overlooking the common yet tedious\ntask of editing existing slides. We introduce Talk-to-Your-Slides, an\nLLM-powered agent that directly edits slides within active PowerPoint sessions\nthrough COM communication. Our system employs a two-level approach: (1)\nhigh-level processing where an LLM agent interprets instructions and formulates\nediting plans, and (2) low-level execution where Python scripts directly\nmanipulate PowerPoint objects. Unlike previous methods relying on predefined\noperations, our approach enables more flexible and contextually-aware editing.\nTo facilitate evaluation, we present TSBench, a human-annotated dataset of 379\ndiverse editing instructions with corresponding slide variations. Experimental\nresults demonstrate that Talk-to-Your-Slides significantly outperforms baseline\nmethods in execution success rate, instruction fidelity, and editing\nefficiency. Our code and benchmark are available at\nhttps://anonymous.4open.science/r/talk-to-your-slides/"}
{"id": "2505.12080", "pdf": "https://arxiv.org/pdf/2505.12080.pdf", "abs": "https://arxiv.org/abs/2505.12080", "title": "TrainBo: An Interactive Robot-assisted Scenario Training System for Older Adults with Dementia", "authors": ["Kwong Chiu Fung", "Wai Ho Mow"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Dementia is an overall decline in memory and cognitive skills severe enough\nto reduce an elders ability to perform everyday activities. There is an\nincreasing need for accessible technologies for cognitive training to slow down\nthe cognitive decline. With the ability to provide instant feedback and\nassistance, social robotic systems have been proven effective in enhancing\nlearning abilities across various age groups. This study focuses on the design\nof an interactive robot-assisted scenario training system TrainBo with\nself-determination theory, derives design requirements through formative and\nformal studies and the system usability is also be evaluated. A pilot test is\nconducted on seven older adults with dementia in an elderly care center in Hong\nKong for four weeks. Our finding shows that older adults with dementia have an\nimprovement in behavioural engagement, emotional engagement, and intrinsic\nmotivation after using Trainbo. These findings can provide valuable insights\ninto the development of more captivating interactive robots for extensive\ntraining purposes."}
{"id": "2505.11613", "pdf": "https://arxiv.org/pdf/2505.11613.pdf", "abs": "https://arxiv.org/abs/2505.11613", "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "authors": ["Xiaomin Li", "Mingye Gao", "Yuexing Hao", "Taoran Li", "Guangya Wan", "Zihan Wang", "Yijun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings."}
{"id": "2505.12101", "pdf": "https://arxiv.org/pdf/2505.12101.pdf", "abs": "https://arxiv.org/abs/2505.12101", "title": "Designing Scaffolded Interfaces for Enhanced Learning and Performance in Professional Software", "authors": ["Yimeng Liu", "Misha Sra"], "categories": ["cs.HC"], "comment": null, "summary": "Professional software offers immense power but also presents significant\nlearning challenges. Its complex interfaces, as well as insufficient built-in\nstructured guidance and unfamiliar terminology, often make newcomers struggle\nwith task completion. To address these challenges, we introduce ScaffoldUI, a\nmethod for scaffolded interface design to reduce interface complexity, provide\nstructured guidance, and enhance software learnability. The scaffolded\ninterface presents task-relevant tools, progressively discloses tool\ncomplexity, and organizes tools based on domain concepts, aiming to assist task\nperformance and software learning. To evaluate the feasibility of our interface\ndesign method, we present a technical pipeline for scaffolded interface\nimplementation in professional 3D software, i.e., Blender, and conduct user\nstudies with beginners (N=32) and experts (N=8). Study results demonstrate that\nour scaffolded interfaces significantly reduce perceived task load caused by\ninterface complexity, support task performance through structured guidance, and\naugment learning by clearly connecting concepts and tools within the taskflow\ncontext. Based on a discussion of the user study findings, we offer insights\nfor future research on designing scaffolded interfaces to support instruction,\nproductivity, creativity, and cross-software workflows."}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615.pdf", "abs": "https://arxiv.org/abs/2505.11615", "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior."}
{"id": "2505.12114", "pdf": "https://arxiv.org/pdf/2505.12114.pdf", "abs": "https://arxiv.org/abs/2505.12114", "title": "Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals", "authors": ["Dena F. Mujtaba", "Nihar R. Mahapatra"], "categories": ["cs.HC", "cs.CV", "eess.IV"], "comment": null, "summary": "AI-enhanced personality assessments are increasingly shaping hiring\ndecisions, using affective computing to predict traits from the Big Five\n(OCEAN) model. However, integrating AI into these assessments raises ethical\nconcerns, especially around bias amplification rooted in training data. These\nbiases can lead to discriminatory outcomes based on protected attributes like\ngender, ethnicity, and age. To address this, we introduce a\ncounterfactual-based framework to systematically evaluate and quantify bias in\nAI-driven personality assessments. Our approach employs generative adversarial\nnetworks (GANs) to generate counterfactual representations of job applicants by\naltering protected attributes, enabling fairness analysis without access to the\nunderlying model. Unlike traditional bias assessments that focus on unimodal or\nstatic data, our method supports multimodal evaluation-spanning visual, audio,\nand textual features. This comprehensive approach is particularly important in\nhigh-stakes applications like hiring, where third-party vendors often provide\nAI systems as black boxes. Applied to a state-of-the-art personality prediction\nmodel, our method reveals significant disparities across demographic groups. We\nalso validate our framework using a protected attribute classifier to confirm\nthe effectiveness of our counterfactual generation. This work provides a\nscalable tool for fairness auditing of commercial AI hiring platforms,\nespecially in black-box settings where training data and model internals are\ninaccessible. Our results highlight the importance of counterfactual approaches\nin improving ethical transparency in affective computing."}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626.pdf", "abs": "https://arxiv.org/abs/2505.11626", "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "categories": ["cs.CL"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications."}
{"id": "2505.12516", "pdf": "https://arxiv.org/pdf/2505.12516.pdf", "abs": "https://arxiv.org/abs/2505.12516", "title": "Towards Immersive Mixed Reality Street Play: Understanding Collocated Bodily Play with See-through Head-Mounted Displays in Public Spaces", "authors": ["Botao Amber Hu", "Rem Rungu Lin", "Yilan Elan Tao", "Samuli Laato", "Yue Li"], "categories": ["cs.HC", "cs.CY"], "comment": "Submitted to CSCW 2025", "summary": "We're witnessing an upcoming paradigm shift as Mixed Reality (MR) See-through\nHead-Mounted Displays (HMDs) become ubiquitous, with use shifting from\ncontrolled, private settings to spontaneous, public ones. While location-based\npervasive mobile games like Pok\\'emon GO have seen success, the embodied\ninteraction of MR HMDs is moving us from phone-based screen-touching gameplay\nto MR HMD-enabled collocated bodily play. Major tech companies are continuously\nreleasing visionary videos where urban streets transform into vast mixed\nreality playgrounds-imagine Harry Potter-style wizard duels on city streets.\nHowever, few researchers have conducted real-world, in-the-wild studies of such\nImmersive Mixed Reality Street Play (IMRSP) in public spaces in anticipation of\na near future with prevalent MR HMDs. Through empirical studies on a series of\nresearch-through-design game probes called Multiplayer Omnipresent Fighting\nArena (MOFA), we gain initial understanding of this under-explored area by\nidentifying the social implications, challenges, and opportunities of this new\nparadigm."}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628.pdf", "abs": "https://arxiv.org/abs/2505.11628", "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques."}
{"id": "2505.12666", "pdf": "https://arxiv.org/pdf/2505.12666.pdf", "abs": "https://arxiv.org/abs/2505.12666", "title": "Adapting to LLMs: How Insiders and Outsiders Reshape Scientific Knowledge Production", "authors": ["Huimin Xu", "Houjiang Liu", "Yan Leng", "Ying Ding"], "categories": ["cs.HC"], "comment": null, "summary": "CSCW has long examined how emerging technologies reshape the ways researchers\ncollaborate and produce knowledge, with scientific knowledge production as a\ncentral area of focus. As AI becomes increasingly integrated into scientific\nresearch, understanding how researchers adapt to it reveals timely\nopportunities for CSCW research -- particularly in supporting new forms of\ncollaboration, knowledge practices, and infrastructure in AI-driven science.\n  This study quantifies LLM impacts on scientific knowledge production based on\nan evaluation workflow that combines an insider-outsider perspective with a\nknowledge production framework. Our findings reveal how LLMs catalyze both\ninnovation and reorganization in scientific communities, offering insights into\nthe broader transformation of knowledge production in the age of generative AI\nand sheds light on new research opportunities in CSCW."}
{"id": "2505.11643", "pdf": "https://arxiv.org/pdf/2505.11643.pdf", "abs": "https://arxiv.org/abs/2505.11643", "title": "Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2", "authors": ["Xiang Fu"], "categories": ["cs.CL"], "comment": null, "summary": "We demonstrate that a developmentally ordered curriculum markedly improves\nreasoning transparency and sample-efficiency in small language models (SLMs).\nConcretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage\nsyllabus that ascends from lexical matching to multi-step symbolic inference\nand then evaluate it without any task-specific fine-tuning. Cognivolve reaches\ntarget accuracy in half the optimization steps of a single-phase baseline,\nactivates an order-of-magnitude more gradient-salient reasoning heads, and\nshifts those heads toward deeper layers, yielding higher-entropy attention that\nbalances local and long-range context. The same curriculum applied out of order\nor with optimizer resets fails to reproduce these gains, confirming that\nprogression--not extra compute--drives the effect. We also identify open\nchallenges: final-answer success still lags a conventional run by about 30%,\nand our saliency probe under-detects verbal-knowledge heads in the hardest\nstage, suggesting directions for mixed-stage fine-tuning and probe expansion."}
{"id": "2505.12780", "pdf": "https://arxiv.org/pdf/2505.12780.pdf", "abs": "https://arxiv.org/abs/2505.12780", "title": "Beyond Individual UX: Defining Group Experience(GX) as a New Paradigm for Group-centered AI", "authors": ["Soohwan Lee", "Seoyeong Hwang", "Kyungho Lee"], "categories": ["cs.HC"], "comment": "Accepted at DIS'25 Companion (Provocations)", "summary": "Recent advancements in HCI and AI have predominantly centered on individual\nuser experiences, often neglecting the emergent dynamics of group interactions.\nThis provocation introduces Group Experience(GX) to capture the collective\nperceptual, emotional, and cognitive dimensions that arise when individuals\ninteract in cohesive groups. We challenge the conventional Human-centered AI\nparadigm and propose Group-centered AI(GCAI) as a framework that actively\nmediates group dynamics, amplifies diverse voices, and fosters ethical\ncollective decision-making. Drawing on social psychology, organizational\nbehavior, and group dynamics, we outline a group-centered design approach that\nbalances individual autonomy with collective interests while developing novel\nevaluative metrics. Our analysis emphasizes rethinking traditional\nmethodologies that focus solely on individual outcomes and advocates for\ninnovative strategies to capture group collaboration. We call on researchers to\nbridge the gap between micro-level experiences and macro-level impacts,\nultimately enriching and transforming collaborative human interactions."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665.pdf", "abs": "https://arxiv.org/abs/2505.11665", "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.13046", "pdf": "https://arxiv.org/pdf/2505.13046.pdf", "abs": "https://arxiv.org/abs/2505.13046", "title": "StudyAlign: A Software System for Conducting Web-Based User Studies with Functional Interactive Prototypes", "authors": ["Florian Lehmann", "Daniel Buschek"], "categories": ["cs.HC", "H.5.2; I.2.7"], "comment": "EICS 2025", "summary": "Interactive systems are commonly prototyped as web applications. This\napproach enables studies with functional prototypes on a large scale. However,\nsetting up these studies can be complex due to implementing experiment\nprocedures, integrating questionnaires, and data logging. To enable such user\nstudies, we developed the software system StudyAlign which offers: 1) a\nfrontend for participants, 2) an admin panel to manage studies, 3) the\npossibility to integrate questionnaires, 4) a JavaScript library to integrate\ndata logging into prototypes, and 5) a backend server for persisting log data,\nand serving logical functions via an API to the different parts of the system.\nWith our system, researchers can set up web-based experiments and focus on the\ndesign and development of interactions and prototypes. Furthermore, our\nsystematic approach facilitates the replication of studies and reduces the\nrequired effort to execute web-based user studies. We conclude with reflections\non using StudyAlign for conducting HCI studies online."}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679.pdf", "abs": "https://arxiv.org/abs/2505.11679", "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction."}
{"id": "2505.13218", "pdf": "https://arxiv.org/pdf/2505.13218.pdf", "abs": "https://arxiv.org/abs/2505.13218", "title": "Human Response to Decision Support in Face Matching: The Influence of Task Difficulty and Machine Accuracy", "authors": ["Marina Estévez-Almenzar", "Ricardo Baeza-Yates", "Carlos Castillo"], "categories": ["cs.HC"], "comment": null, "summary": "Decision support systems enhanced by Artificial Intelligence (AI) are\nincreasingly being used in high-stakes scenarios where errors or biased\noutcomes can have significant consequences. In this work, we explore the\nconditions under which AI-based decision support systems affect the decision\naccuracy of humans involved in face matching tasks. Previous work suggests that\nthis largely depends on various factors, such as the specific nature of the\ntask and how users perceive the quality of the decision support, among others.\nHence, we conduct extensive experiments to examine how both task difficulty and\nthe precision of the system influence human outcomes. Our results show a strong\ninfluence of task difficulty, which not only makes humans less precise but also\nless capable of determining whether the decision support system is yielding\naccurate suggestions or not. This has implications for the design of decision\nsupport systems, and calls for a careful examination of the context in which\nthey are deployed and on how they are perceived by users."}
{"id": "2505.11683", "pdf": "https://arxiv.org/pdf/2505.11683.pdf", "abs": "https://arxiv.org/abs/2505.11683", "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation", "authors": ["Susanna Rücker", "Alan Akbik"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Entity disambiguation (ED) is the task of linking mentions in text to\ncorresponding entries in a knowledge base. Dual Encoders address this by\nembedding mentions and label candidates in a shared embedding space and\napplying a similarity metric to predict the correct label. In this work, we\nfocus on evaluating key design decisions for Dual Encoder-based ED, such as its\nloss function, similarity metric, label verbalization format, and negative\nsampling strategy. We present the resulting model VerbalizED, a document-level\nDual Encoder model that includes contextual label verbalizations and efficient\nhard negative sampling. Additionally, we explore an iterative prediction\nvariant that aims to improve the disambiguation of challenging data points.\nComprehensive experiments on AIDA-Yago validate the effectiveness of our\napproach, offering insights into impactful design choices that result in a new\nState-of-the-Art system on the ZELDA benchmark."}
{"id": "2505.13381", "pdf": "https://arxiv.org/pdf/2505.13381.pdf", "abs": "https://arxiv.org/abs/2505.13381", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms."}
{"id": "2505.11690", "pdf": "https://arxiv.org/pdf/2505.11690.pdf", "abs": "https://arxiv.org/abs/2505.11690", "title": "Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions", "authors": ["Sukairaj Hafiz Imam", "Babangida Sani", "Dawit Ketema Gete", "Bedru Yimam Ahamed", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Seid Muhie Yimam", "Muhammad Yahuza Bello", "Shamsuddeen Hassan Muhammad"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) technologies have transformed\nhuman-computer interaction; however, low-resource languages in Africa remain\nsignificantly underrepresented in both research and practical applications.\nThis study investigates the major challenges hindering the development of ASR\nsystems for these languages, which include data scarcity, linguistic\ncomplexity, limited computational resources, acoustic variability, and ethical\nconcerns surrounding bias and privacy. The primary goal is to critically\nanalyze these barriers and identify practical, inclusive strategies to advance\nASR technologies within the African context. Recent advances and case studies\nemphasize promising strategies such as community-driven data collection,\nself-supervised and multilingual learning, lightweight model architectures, and\ntechniques that prioritize privacy. Evidence from pilot projects involving\nvarious African languages showcases the feasibility and impact of customized\nsolutions, which encompass morpheme-based modeling and domain-specific ASR\napplications in sectors like healthcare and education. The findings highlight\nthe importance of interdisciplinary collaboration and sustained investment to\ntackle the distinct linguistic and infrastructural challenges faced by the\ncontinent. This study offers a progressive roadmap for creating ethical,\nefficient, and inclusive ASR systems that not only safeguard linguistic\ndiversity but also improve digital accessibility and promote socioeconomic\nparticipation for speakers of African languages."}
{"id": "2505.11579", "pdf": "https://arxiv.org/pdf/2505.11579.pdf", "abs": "https://arxiv.org/abs/2505.11579", "title": "Toward Adaptive Categories: Dimensional Governance for Agentic AI", "authors": ["Zeynep Engin", "David Hand"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "comment": "12 pages core text, 14 pages including references, 2 figures", "summary": "As AI systems evolve from static tools to dynamic agents, traditional\ncategorical governance frameworks -- based on fixed risk tiers, levels of\nautonomy, or human oversight models -- are increasingly insufficient on their\nown. Systems built on foundation models, self-supervised learning, and\nmulti-agent architectures increasingly blur the boundaries that categories were\ndesigned to police. In this Perspective, we make the case for dimensional\ngovernance: a framework that tracks how decision authority, process autonomy,\nand accountability (the 3As) distribute dynamically across human-AI\nrelationships. A critical advantage of this approach is its ability to\nexplicitly monitor system movement toward and across key governance thresholds,\nenabling preemptive adjustments before risks materialize. This dimensional\napproach provides the necessary foundation for more adaptive categorization,\nenabling thresholds and classifications that can evolve with emerging\ncapabilities. While categories remain essential for decision-making, building\nthem upon dimensional foundations allows for context-specific adaptability and\nstakeholder-responsive governance that static approaches cannot achieve. We\noutline key dimensions, critical trust thresholds, and practical examples\nillustrating where rigid categorical frameworks fail -- and where a dimensional\nmindset could offer a more resilient and future-proof path forward for both\ngovernance and innovation at the frontier of artificial intelligence."}
{"id": "2505.11693", "pdf": "https://arxiv.org/pdf/2505.11693.pdf", "abs": "https://arxiv.org/abs/2505.11693", "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-Jyrä", "Carlos Gómez-Rodríguez"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Original submission; camera-ready coming soon", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks."}
{"id": "2505.11612", "pdf": "https://arxiv.org/pdf/2505.11612.pdf", "abs": "https://arxiv.org/abs/2505.11612", "title": "Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors", "authors": ["Hung Nguyen", "Alireza Rahimi", "Veronica Whitford", "Hélène Fournier", "Irina Kondratova", "René Richard", "Hung Cao"], "categories": ["cs.AI", "cs.HC"], "comment": "41 pages", "summary": "Psychiatric disorders affect millions globally, yet their diagnosis faces\nsignificant challenges in clinical practice due to subjective assessments and\naccessibility concerns, leading to potential delays in treatment. To help\naddress this issue, we present Heart2Mind, a human-centered contestable\npsychiatric disorder diagnosis system using wearable electrocardiogram (ECG)\nmonitors. Our approach leverages cardiac biomarkers, particularly heart rate\nvariability (HRV) and R-R intervals (RRI) time series, as objective indicators\nof autonomic dysfunction in psychiatric conditions. The system comprises three\nkey components: (1) a Cardiac Monitoring Interface (CMI) for real-time data\nacquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency\nTransformer (MSTFT) that processes RRI time series through integrated\ntime-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI)\ncombining Self-Adversarial Explanations (SAEs) with contestable Large Language\nModels (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using\nleave-one-out cross-validation, outperforming state-of-the-art methods. SAEs\nsuccessfully detect inconsistencies in model predictions by comparing\nattention-based and gradient-based explanations, while LLMs enable clinicians\nto validate correct predictions and contest erroneous ones. This work\ndemonstrates the feasibility of combining wearable technology with Explainable\nArtificial Intelligence (XAI) and contestable LLMs to create a transparent,\ncontestable system for psychiatric diagnosis that maintains clinical oversight\nwhile leveraging advanced AI capabilities. Our implementation is publicly\navailable at: https://github.com/Analytics-Everywhere-Lab/heart2mind."}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726.pdf", "abs": "https://arxiv.org/abs/2505.11726", "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues."}
{"id": "2505.11687", "pdf": "https://arxiv.org/pdf/2505.11687.pdf", "abs": "https://arxiv.org/abs/2505.11687", "title": "Second SIGIR Workshop on Simulations for Information Access (Sim4IA 2025)", "authors": ["Philipp Schaer", "Christin Katharina Kreutz", "Krisztian Balog", "Timo Breuer", "Andreas Konstantin Kruff"], "categories": ["cs.IR", "cs.AI", "cs.HC"], "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25), July 13--18,\n  2025, Padua, Italy", "summary": "Simulations in information access (IA) have recently gained interest, as\nshown by various tutorials and workshops around that topic. Simulations can be\nkey contributors to central IA research and evaluation questions, especially\naround interactive settings when real users are unavailable, or their\nparticipation is impossible due to ethical reasons. In addition, simulations in\nIA can help contribute to a better understanding of users, reduce complexity of\nevaluation experiments, and improve reproducibility. Building on recent\ndevelopments in methods and toolkits, the second iteration of our Sim4IA\nworkshop aims to again bring together researchers and practitioners to form an\ninteractive and engaging forum for discussions on the future perspectives of\nthe field. An additional aim is to plan an upcoming TREC/CLEF campaign."}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733.pdf", "abs": "https://arxiv.org/abs/2505.11733", "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."}
{"id": "2505.11808", "pdf": "https://arxiv.org/pdf/2505.11808.pdf", "abs": "https://arxiv.org/abs/2505.11808", "title": "Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control", "authors": ["Shangqun Yu", "Hochul Hwang", "Trung M. Dang", "Joydeep Biswas", "Nicholas A. Giudice", "Sunghoon Ivan Lee", "Donghyun Kim"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "A quadruped robot is a promising system that can offer assistance comparable\nto that of dog guides due to its similar form factor. However, various\nchallenges remain in making these robots a reliable option for blind and\nlow-vision (BLV) individuals. Among these challenges, noise and jerky motion\nduring walking are critical drawbacks of existing quadruped robots. While these\nissues have largely been overlooked in guide dog robot research, our interviews\nwith guide dog handlers and trainers revealed that acoustic and physical\ndisturbances can be particularly disruptive for BLV individuals, who rely\nheavily on environmental sounds for navigation. To address these issues, we\ndeveloped a novel walking controller for slow stepping and smooth foot\nswing/contact while maintaining human walking speed, as well as robust and\nstable balance control. The controller integrates with a perception system to\nfacilitate locomotion over non-flat terrains, such as stairs. Our controller\nwas extensively tested on the Unitree Go1 robot and, when compared with other\ncontrol methods, demonstrated significant noise reduction -- half of the\ndefault locomotion controller. In this study, we adopt a mixed-methods approach\nto evaluate its usability with BLV individuals. In our indoor walking\nexperiments, participants compared our controller to the robot's default\ncontroller. Results demonstrated superior acceptance of our controller,\nhighlighting its potential to improve the user experience of guide dog robots.\nVideo demonstration (best viewed with audio) available at:\nhttps://youtu.be/8-pz_8Hqe6s."}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739.pdf", "abs": "https://arxiv.org/abs/2505.11739", "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183.pdf", "abs": "https://arxiv.org/abs/2505.12183", "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "authors": ["Manari Hirose", "Masato Uchida"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746.pdf", "abs": "https://arxiv.org/abs/2505.11746", "title": "Token Masking Improves Transformer-Based Text Classification", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349.pdf", "abs": "https://arxiv.org/abs/2505.12349", "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "authors": ["Axel Abels", "Tom Lenaerts"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.11754", "pdf": "https://arxiv.org/pdf/2505.11754.pdf", "abs": "https://arxiv.org/abs/2505.11754", "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "authors": ["Wenyu Huang", "Pavlos Vougiouklis", "Mirella Lapata", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408.pdf", "abs": "https://arxiv.org/abs/2505.12408", "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764.pdf", "abs": "https://arxiv.org/abs/2505.11764", "title": "Towards Universal Semantics With Large Language Models", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.12525", "pdf": "https://arxiv.org/pdf/2505.12525.pdf", "abs": "https://arxiv.org/abs/2505.12525", "title": "Development of a non-wearable support robot capable of reproducing natural standing-up movements", "authors": ["Atsuya Kusui", "Susumu Hirai", "Asuka Takai"], "categories": ["q-bio.NC", "cs.HC", "cs.RO"], "comment": null, "summary": "To reproduce natural standing-up motion, recent studies have emphasized the\nimportance of coordination between the assisting robot and the human. However,\nmany non-wearable assistive devices have struggled to replicate natural motion\ntrajectories. While wearable devices offer better coordination with the human\nbody, they present challenges in completely isolating mechanical and electrical\nhazards. To address this, we developed a novel standing-assist robot that\nintegrates features of both wearable and non-wearable systems, aiming to\nachieve high coordination while maintaining safety. The device employs a\nfour-link mechanism aligned with the human joint structure, designed to\nreproduce the S-shaped trajectory of the hip and the arc trajectory of the knee\nduring natural standing-up motion. Subject-specific trajectory data were\nobtained using a gyroscope, and the link lengths were determined to drive the\nseat along the optimal path. A feedforward speed control using a stepping motor\nwas implemented, and the reproducibility of the trajectory was evaluated based\non the geometric constraints of the mechanism. A load-bearing experiment with\nweights fixed to the seat was conducted to assess the trajectory accuracy under\ndifferent conditions. Results showed that the reproduction errors for the hip\nand knee trajectories remained within approximately 4 percent of the seat's\ntotal displacement, demonstrating high fidelity to the target paths. In\naddition, durability testing, thermal safety evaluation, and risk assessment\nconfirmed the reliability and safety of the system for indoor use. These\nfindings suggest that the proposed design offers a promising approach for\ndeveloping assistive technologies that adapt to individual physical\ncharacteristics, with potential applications in elderly care and\nrehabilitation."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807.pdf", "abs": "https://arxiv.org/abs/2505.11807", "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718.pdf", "abs": "https://arxiv.org/abs/2505.12718", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content."}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810.pdf", "abs": "https://arxiv.org/abs/2505.11810", "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many language information processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to Classical Chinese information processing\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727.pdf", "abs": "https://arxiv.org/abs/2505.12727", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma."}
{"id": "2505.11811", "pdf": "https://arxiv.org/pdf/2505.11811.pdf", "abs": "https://arxiv.org/abs/2505.11811", "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering", "authors": ["Taolin Zhang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang", "Xiaofeng He"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 main track", "summary": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios."}
{"id": "2505.12734", "pdf": "https://arxiv.org/pdf/2505.12734.pdf", "abs": "https://arxiv.org/abs/2505.12734", "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "comment": "14 pages, 5 figures", "summary": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences."}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820.pdf", "abs": "https://arxiv.org/abs/2505.11820", "title": "Chain-of-Model Learning for Language Model", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."}
{"id": "2505.12981", "pdf": "https://arxiv.org/pdf/2505.12981.pdf", "abs": "https://arxiv.org/abs/2505.12981", "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": null, "summary": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827.pdf", "abs": "https://arxiv.org/abs/2505.11827", "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010.pdf", "abs": "https://arxiv.org/abs/2505.13010", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.11829", "pdf": "https://arxiv.org/pdf/2505.11829.pdf", "abs": "https://arxiv.org/abs/2505.11829", "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks", "authors": ["Chenlu Wang", "Weimin Lyu", "Ritwik Banerjee"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground."}
{"id": "2505.13044", "pdf": "https://arxiv.org/pdf/2505.13044.pdf", "abs": "https://arxiv.org/abs/2505.13044", "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents", "authors": ["Rebecca Westhäußer", "Frederik Berenz", "Wolfgang Minker", "Sebastian Zepf"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835.pdf", "abs": "https://arxiv.org/abs/2505.11835", "title": "Multilingual Collaborative Defense for Large Language Models", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.13054", "pdf": "https://arxiv.org/pdf/2505.13054.pdf", "abs": "https://arxiv.org/abs/2505.13054", "title": "Disentangling Coordiante Frames for Task Specific Motion Retargeting in Teleoperation using Shared Control and VR Controllers", "authors": ["Max Grobbel", "Daniel Flögel", "Philipp Rigoll", "Sören Hohmann"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "8 pages, 4 figures, conference", "summary": "Task performance in terms of task completion time in teleoperation is still\nfar behind compared to humans conducting tasks directly. One large identified\nimpact on this is the human capability to perform transformations and\nalignments, which is directly influenced by the point of view and the motion\nretargeting strategy. In modern teleoperation systems, motion retargeting is\nusually implemented through a one time calibration or switching modes. Complex\ntasks, like concatenated screwing, might be difficult, because the operator has\nto align (e.g. mirror) rotational and translational input commands. Recent\nresearch has shown, that the separation of translation and rotation leads to\nincreased task performance. This work proposes a formal motion retargeting\nmethod, which separates translational and rotational input commands. This\nmethod is then included in a optimal control based trajectory planner and shown\nto work on a UR5e manipulator."}
{"id": "2505.11855", "pdf": "https://arxiv.org/pdf/2505.11855.pdf", "abs": "https://arxiv.org/abs/2505.11855", "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research", "authors": ["Guijin Son", "Jiwoo Hong", "Honglu Fan", "Heejeong Nam", "Hyunwoo Ko", "Seungwon Lim", "Jinyeop Song", "Jinha Choi", "Gonçalo Paulo", "Youngjae Yu", "Stella Biderman"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227.pdf", "abs": "https://arxiv.org/abs/2505.13227", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876.pdf", "abs": "https://arxiv.org/abs/2505.11876", "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts."}
{"id": "2505.13246", "pdf": "https://arxiv.org/pdf/2505.13246.pdf", "abs": "https://arxiv.org/abs/2505.13246", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging."}
{"id": "2505.11887", "pdf": "https://arxiv.org/pdf/2505.11887.pdf", "abs": "https://arxiv.org/abs/2505.11887", "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "authors": ["Xiechi Zhang", "Zetian Ouyang", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Ya Zhang", "Yanfeng Wang", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments."}
{"id": "2306.11766", "pdf": "https://arxiv.org/pdf/2306.11766.pdf", "abs": "https://arxiv.org/abs/2306.11766", "title": "Agreeing and Disagreeing in Collaborative Knowledge Graph Construction: An Analysis of Wikidata", "authors": ["Elisavet Koutsiana", "Tushita Yadav", "Nitisha Jain", "Albert Meroño-Peñuela", "Elena Simperl"], "categories": ["cs.HC"], "comment": "The paper has been accepted at the Journal of Web Semantics", "summary": "In this work, we study disagreements in discussions around Wikidata, an\nonline knowledge community that builds the data backend of Wikipedia.\nDiscussions are essential in collaborative work as they can increase\ncontributor performance and encourage the emergence of shared norms and\npractices. While disagreements can play a productive role in discussions, they\ncan also lead to conflicts and controversies, which impact contributor'\nwell-being and their motivation to engage. We want to understand if and when\nsuch phenomena arise in Wikidata, using a mix of quantitative and qualitative\nanalyses to identify the types of topics people disagree about, the most common\npatterns of interaction, and roles people play when arguing for or against an\nissue. We find that decisions to create Wikidata properties are much faster\nthan those to delete properties and that more than half of controversial\ndiscussions do not lead to consensus. Our analysis suggests that Wikidata is an\ninclusive community, considering different opinions when making decisions, and\nthat conflict and vandalism are rare in discussions. At the same time, while\none-fourth of the editors participating in controversial discussions contribute\nlegitimate and insightful opinions about Wikidata's emerging issues, they\nrespond with one or two posts and do not remain engaged in the discussions to\nreach consensus. Our work contributes to the analysis of collaborative KG\nconstruction with insights about communication and decision-making in projects,\nas well as with methodological directions and open datasets. We hope our\nfindings will help managers and designers support community decision-making and\nimprove discussion tools and practices."}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891.pdf", "abs": "https://arxiv.org/abs/2505.11891", "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2308.10260", "pdf": "https://arxiv.org/pdf/2308.10260.pdf", "abs": "https://arxiv.org/abs/2308.10260", "title": "Hand Dominance and Congruence for Wrist-worn Haptics using Custom Voice-Coil Actuation", "authors": ["Ayoade Adeyemi", "Umit Sen", "Samet Mert Ercan", "Mine Sarac"], "categories": ["cs.HC", "cs.RO"], "comment": "6 pages, 7 figures", "summary": "During virtual interactions, rendering haptic feedback on a remote location\n(like the wrist) instead of the fingertips freeing users' hands from mechanical\ndevices. This allows for real interactions while still providing information\nregarding the mechanical properties of virtual objects. In this paper, we\npresent CoWrHap -- a novel wrist-worn haptic device with custom-made voice coil\nactuation to render force feedback. Then, we investigate the impact of asking\nparticipants to use their dominant or non-dominant hand for virtual\ninteractions and the best mapping between the active hand and the wrist\nreceiving the haptic feedback, which can be defined as hand-wrist congruence\nthrough a user experiment based on a stiffness discrimination task. Our results\nshow that participants performed the tasks (i) better with non-congruent\nmapping but reported better experiences with congruent mapping, and (ii) with\nno statistical difference in terms of hand dominance but reported better user\nexperience and enjoyment using their dominant hands. This study indicates that\nparticipants can perceive mechanical properties via haptic feedback provided\nthrough CoWrHap."}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893.pdf", "abs": "https://arxiv.org/abs/2505.11893", "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness."}
{"id": "2309.12555", "pdf": "https://arxiv.org/pdf/2309.12555.pdf", "abs": "https://arxiv.org/abs/2309.12555", "title": "PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent", "authors": ["Donghoon Shin", "Gary Hsieh", "Young-Ho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "17 pages including reference. Accepted to ACM CUI 2025", "summary": "Creating personalized and actionable exercise plans often requires iteration\nwith experts, which can be costly and inaccessible to many individuals. This\nwork explores the capabilities of Large Language Models (LLMs) in addressing\nthese challenges. We present PlanFitting, an LLM-driven conversational agent\nthat assists users in creating and refining personalized weekly exercise plans.\nBy engaging users in free-form conversations, PlanFitting helps elicit users'\ngoals, availabilities, and potential obstacles, and enables individuals to\ngenerate personalized exercise plans aligned with established exercise\nguidelines. Our study -- involving a user study, intrinsic evaluation, and\nexpert evaluation -- demonstrated PlanFitting's ability to guide users to\ncreate tailored, actionable, and evidence-based plans. We discuss future design\nopportunities for LLM-driven conversational agents to create plans that better\ncomply with exercise principles and accommodate personal constraints."}
{"id": "2505.11900", "pdf": "https://arxiv.org/pdf/2505.11900.pdf", "abs": "https://arxiv.org/abs/2505.11900", "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data", "authors": ["Philipp Christmann", "Gerhard Weikum"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds."}
{"id": "2310.09617", "pdf": "https://arxiv.org/pdf/2310.09617.pdf", "abs": "https://arxiv.org/abs/2310.09617", "title": "How Good is ChatGPT in Giving Advice on Your Visualization Design?", "authors": ["Nam Wook Kim", "Yongsu Ahn", "Grace Myers", "Benjamin Bach"], "categories": ["cs.HC"], "comment": "34 pages, 4 figures", "summary": "Data visualization creators often lack formal training, resulting in a\nknowledge gap in design practice. Large language models such as ChatGPT, with\ntheir vast internet-scale training data, offer transformative potential to\naddress this gap. In this study, we used both qualitative and quantitative\nmethods to investigate how well ChatGPT can address visualization design\nquestions. First, we quantitatively compared the ChatGPT-generated responses\nwith anonymous online Human replies to data visualization questions on the\nVisGuides user forum. Next, we conducted a qualitative user study examining the\nreactions and attitudes of practitioners toward ChatGPT as a visualization\ndesign assistant. Participants were asked to bring their visualizations and\ndesign questions and received feedback from both Human experts and ChatGPT in\nrandomized order. Our findings from both studies underscore ChatGPT's\nstrengths, particularly its ability to rapidly generate diverse design options,\nwhile also highlighting areas for improvement, such as nuanced contextual\nunderstanding and fluid interaction dynamics beyond the chat interface. Drawing\non these insights, we discuss design considerations for future LLM-based design\nfeedback systems."}
{"id": "2505.11908", "pdf": "https://arxiv.org/pdf/2505.11908.pdf", "abs": "https://arxiv.org/abs/2505.11908", "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration", "authors": ["Zhangyu Wang", "Siyuan Gao", "Rong Zhou", "Hao Wang", "Li Ning"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude."}
{"id": "2405.07475", "pdf": "https://arxiv.org/pdf/2405.07475.pdf", "abs": "https://arxiv.org/abs/2405.07475", "title": "Design Opportunities for Explainable AI Paraphrasing Tools: A User Study with Non-native English Speakers", "authors": ["Yewon Kim", "Thanh-Long V. Le", "Donghwi Kim", "Mina Lee", "Sung-Ju Lee"], "categories": ["cs.HC"], "comment": "Published as a conference paper at DIS 2025", "summary": "We investigate how non-native English speakers (NNESs) interact with diverse\ninformation aids to assess and select AI-generated paraphrases. We develop\nParaScope, an AI paraphrasing assistant that integrates diverse information\naids, such as back-translation, explanations, and usage examples, and logs user\ninteraction data. Our in-lab study with 22 NNESs reveals that user preferences\nfor information aids vary by language proficiency, with workflows progressing\nfrom global to more detailed information. While back-translation was the most\nfrequently used aid, it was not a decisive factor in suggestion acceptance;\nusers combined multiple information aids to make informed decisions. Our\nfindings demonstrate the potential of explainable AI paraphrasing tools to\nenhance NNESs' confidence, autonomy, and writing efficiency, while also\nemphasizing the importance of thoughtful design to prevent information\noverload. Based on these findings, we offer design implications for explainable\nAI paraphrasing tools that support NNESs in making informed decisions when\nusing AI writing systems."}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922.pdf", "abs": "https://arxiv.org/abs/2505.11922", "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency."}
{"id": "2407.07732", "pdf": "https://arxiv.org/pdf/2407.07732.pdf", "abs": "https://arxiv.org/abs/2407.07732", "title": "Text2VP: Generative AI for Visual Programming and Parametric Modeling", "authors": ["Guangxi Feng", "Wei Yan"], "categories": ["cs.HC", "cs.AI"], "comment": "Demonstration Video:\n  https://www.youtube.com/playlist?list=PLUOmOLuLSaDWss2En2buixBxvTPy-lDvA", "summary": "The integration of generative artificial intelligence (AI) into architectural\ndesign has advanced significantly, enabling the generation of text, images, and\n3D models. However, prior AI applications lack support for text-to-parametric\nmodels, essential for generating and optimizing diverse parametric design\noptions. This study introduces Text-to-Visual Programming (Text2VP) GPT, a\nnovel generative AI derived from GPT-4.1, designed to automate graph-based\nvisual programming workflows, parameters, and their interconnections. Text2VP\nleverages detailed documentation, specific instructions, and example-driven\nfew-shot learning to reflect user intentions accurately and facilitate\ninteractive parameter adjustments. Testing demonstrates Text2VP's capability in\ngenerating functional parametric models, although higher complexity models\npresent increased error rates. This research highlights generative AI's\npotential in visual programming and parametric modeling, laying groundwork for\nfuture improvements to manage complex modeling tasks. Ultimately, Text2VP aims\nto enable designers to easily create and modify parametric models without\nextensive training in specialized platforms like Grasshopper."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924.pdf", "abs": "https://arxiv.org/abs/2505.11924", "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2408.02803", "pdf": "https://arxiv.org/pdf/2408.02803.pdf", "abs": "https://arxiv.org/abs/2408.02803", "title": "SiCo: An Interactive Size-Controllable Virtual Try-On Approach for Informed Decision-Making", "authors": ["Sherry X. Chen", "Alex Christopher Lim", "Yimeng Liu", "Pradeep Sen", "Misha Sra"], "categories": ["cs.HC", "cs.CV", "H.5.2; I.4.9"], "comment": null, "summary": "Virtual try-on (VTO) applications aim to replicate the in-store shopping\nexperience and enhance online shopping by enabling users to interact with\ngarments. However, many existing tools adopt a one-size-fits-all approach when\nvisualizing clothing items. This approach limits user interaction with\ngarments, particularly regarding size and fit adjustments, and fails to provide\ndirect insights for size recommendations. As a result, these limitations\ncontribute to high return rates in online shopping. To address this, we\nintroduce SiCo, a new online VTO system that allows users to upload images of\nthemselves and interact with garments by visualizing how different sizes would\nfit their bodies. Our user study demonstrates that our approach significantly\nimproves users' ability to assess how outfits will appear on their bodies and\nincreases their confidence in selecting clothing sizes that align with their\npreferences. Based on our evaluation, we believe that SiCo has the potential to\nreduce return rates and transform the online clothing shopping experience."}
{"id": "2505.11932", "pdf": "https://arxiv.org/pdf/2505.11932.pdf", "abs": "https://arxiv.org/abs/2505.11932", "title": "Neuro-Symbolic Query Compiler", "authors": ["Yuyao Zhang", "Zhicheng Dou", "Xiaoxi Li", "Jiajie Jin", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler", "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries."}
{"id": "2410.03032", "pdf": "https://arxiv.org/pdf/2410.03032.pdf", "abs": "https://arxiv.org/abs/2410.03032", "title": "CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing", "authors": ["Xiaohan Ding", "Kaike Ping", "Uma Sushmitha Gunturi", "Buse Carik", "Sophia Stil", "Lance T Wilhelm", "Taufiq Daryanto", "James Hawdon", "Sang Won Lee", "Eugenia H Rho"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Online hate speech has become increasingly prevalent on social media, causing\nharm to individuals and society. While automated content moderation has\nreceived considerable attention, user-driven counterspeech remains a less\nexplored yet promising approach. However, many people face difficulties in\ncrafting effective responses. We introduce CounterQuill, a human-AI\ncollaborative system that helps everyday users with writing empathetic\ncounterspeech, not by generating automatic replies, but by educating them\nthrough reflection and response. CounterQuill follows a three-stage workflow\ngrounded in computational thinking: (1) a learning session to build\nunderstanding of hate speech and counterspeech, (2) a brainstorming session to\nidentify harmful patterns and ideate counterspeech ideas, and (3) a co-writing\nsession that helps users refine their counter responses while preserving\npersonal voice. Through a user study \\r{ho}(N=20), we found that CounterQuill\nhelped participants develop the skills to brainstorm and draft counterspeech\nwith increased confidence and control throughout the process. Our findings\nhighlight how AI systems can scaffold complex communication tasks through\nstructured, human-centered workflows that educate users on how to recognize,\nreflect on, and respond to online hate speech."}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935.pdf", "abs": "https://arxiv.org/abs/2505.11935", "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit."}
{"id": "2501.02084", "pdf": "https://arxiv.org/pdf/2501.02084.pdf", "abs": "https://arxiv.org/abs/2501.02084", "title": "Simulated prosthetic vision confirms checkerboard as an effective raster pattern for epiretinal implants", "authors": ["Justin M. Kasowski", "Apurv Varshney", "Roksana Sadeghi", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial scheduling of electrode activation (\"rastering\") is essential for\nsafely operating high-density retinal implants, yet its perceptual consequences\nremain poorly understood. This study systematically evaluates the impact of\nraster patterns, or spatial arrangements of sequential electrode activation, on\nperformance and perceived difficulty in simulated prosthetic vision (SPV). By\naddressing this gap, we aimed to identify patterns that optimize functional\nvision in retinal implants. Sighted participants completed letter recognition\nand motion discrimination tasks under four raster patterns (horizontal,\nvertical, checkerboard, and random) using an immersive SPV system. The\nsimulations emulated epiretinal implant perception and employed\npsychophysically validated models of electrode activation, phosphene\nappearance, nonlinear spatial summation, and temporal dynamics, ensuring\nrealistic representation of prosthetic vision. Performance accuracy and\nself-reported difficulty were analyzed to assess the effects of raster\npatterning. The checkerboard pattern consistently outperformed other raster\npatterns, yielding significantly higher accuracy and lower difficulty ratings\nacross both tasks. The horizontal and vertical patterns introduced biases\naligned with apparent motion artifacts, while the checkerboard minimized such\neffects. Random patterns resulted in the lowest performance, underscoring the\nimportance of structured activation. Notably, checkerboard matched performance\nin the \"No Raster\" condition, despite conforming to groupwise safety\nconstraints. This is the first quantitative, task-based evaluation of raster\npatterns in SPV. Checkerboard-style scheduling enhances perceptual clarity\nwithout increasing computational load, offering a low-overhead, clinically\nrelevant strategy for improving usability in next-generation retinal\nprostheses."}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958.pdf", "abs": "https://arxiv.org/abs/2505.11958", "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems."}
{"id": "2503.07586", "pdf": "https://arxiv.org/pdf/2503.07586.pdf", "abs": "https://arxiv.org/abs/2503.07586", "title": "Design for Hope: Cultivating Deliberate Hope in the Face of Complex Societal Challenges", "authors": ["JaeWon Kim", "Jiaying \"Lizzy\" Liu", "Lindsay Popowski", "Cassidy Pyle", "Sowmya Somanath", "Hua Shen", "Casey Fiesler", "Gillian R. Hayes", "Alexis Hiniker", "Wendy Ju", "Florian \"Floyd\" Mueller", "Ahmer Arif", "Yasmine Kotturi"], "categories": ["cs.HC"], "comment": null, "summary": "Design has the potential to cultivate hope in the face of complex societal\nchallenges, especially those central to CSCW research. These challenges are\noften addressed through efforts aimed at harm reduction and prevention --\nessential but sometimes limiting approaches that can unintentionally narrow our\ncollective sense of what is possible. This one-day, in-person workshop builds\non the Positech Workshop at CSCW 2024 (https://positech-cscw-2024.github.io/)\nby offering practical ways to move beyond reactive problem-solving toward\nbuilding capacity for proactive goal setting and generating pathways forward.\nWe explore how collaborative and reflective design methodologies can help\nresearch communities navigate uncertainty, expand possibilities, and foster\nmeaningful change. By connecting design thinking with hope theory, which frames\nhope as the interplay of \"goal-directed,\" \"pathways,\" and \"agentic\" thinking,\nwe will examine how researchers might chart new directions in the face of\ncomplexity and constraint. Through hands-on activities including problem\nreframing, building a shared taxonomy of design methods that align with hope\ntheory, and reflecting on what it means to sustain hopeful research\ntrajectories, participants will develop strategies to embed a deliberately\nhopeful approach into their research."}
{"id": "2505.11959", "pdf": "https://arxiv.org/pdf/2505.11959.pdf", "abs": "https://arxiv.org/abs/2505.11959", "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "categories": ["cs.CL"], "comment": null, "summary": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech."}
{"id": "2504.07285", "pdf": "https://arxiv.org/pdf/2504.07285.pdf", "abs": "https://arxiv.org/abs/2504.07285", "title": "A Scalable Approach to Clustering Embedding Projections", "authors": ["Donghao Ren", "Fred Hohman", "Dominik Moritz"], "categories": ["cs.HC", "cs.LG"], "comment": "Code: https://github.com/apple/embedding-atlas", "summary": "Interactive visualization of embedding projections is a useful technique for\nunderstanding data and evaluating machine learning models. Labeling data within\nthese visualizations is critical for interpretation, as labels provide an\noverview of the projection and guide user navigation. However, most methods for\nproducing labels require clustering the points, which can be computationally\nexpensive as the number of points grows. In this paper, we describe an\nefficient clustering approach using kernel density estimation in the projected\n2D space instead of points. This algorithm can produce high-quality cluster\nregions from a 2D density map in a few hundred milliseconds, orders of\nmagnitude faster than current approaches. We contribute the design of the\nalgorithm, benchmarks, and applications that demonstrate the utility of the\nalgorithm, including labeling and summarization."}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965.pdf", "abs": "https://arxiv.org/abs/2505.11965", "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "authors": ["Xu Liu", "Guanyi Chen"], "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task."}
{"id": "2504.09438", "pdf": "https://arxiv.org/pdf/2504.09438.pdf", "abs": "https://arxiv.org/abs/2504.09438", "title": "Cartographers in Cubicles: How Training and Preferences of Mapmakers Interplay with Structures and Norms in Not-for-Profit Organizations", "authors": ["Arpit Narechania", "Alex Endert", "Clio Andris"], "categories": ["cs.HC"], "comment": "24 pages, 4 figures, 2 tables; to appear in ACM CSCW 2025", "summary": "Choropleth maps are a common and effective way to visualize geographic\nthematic data. Although cartographers have established many principles about\nmap design, data binning and color usage, less is known about how mapmakers\nmake individual decisions in practice. We interview 16 cartographers and\ngeographic information systems (GIS) experts from 13 government organizations,\nNGOs, and federal agencies about their choropleth mapmaking decisions and\nworkflows. We categorize our findings and report on how mapmakers follow\ncartographic guidelines and personal rules of thumb, collaborate with other\nstakeholders within and outside their organization, and how organizational\nstructures and norms are tied to decision-making during data preparation, data\nanalysis, data binning, map styling, and map post-processing. We find several\npoints of variation as well as regularity across mapmakers and organizations\nand present takeaways to inform cartographic education and practice, including\nbroader implications and opportunities for CSCW, HCI, and information\nvisualization researchers and practitioners."}
{"id": "2505.11969", "pdf": "https://arxiv.org/pdf/2505.11969.pdf", "abs": "https://arxiv.org/abs/2505.11969", "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786."}
{"id": "2505.01000", "pdf": "https://arxiv.org/pdf/2505.01000.pdf", "abs": "https://arxiv.org/abs/2505.01000", "title": "Togedule: Scheduling Meetings with Large Language Models and Adaptive Representations of Group Availability", "authors": ["Jaeyoon Song", "Zahra Ashktorab", "Thomas W. Malone"], "categories": ["cs.HC"], "comment": "This paper has been accepted at CSCW 2025", "summary": "Scheduling is a perennial-and often challenging-problem for many groups.\nExisting tools are mostly static, showing an identical set of choices to\neveryone, regardless of the current status of attendees' inputs and\npreferences. In this paper, we propose Togedule, an adaptive scheduling tool\nthat uses large language models to dynamically adjust the pool of choices and\ntheir presentation format. With the initial prototype, we conducted a formative\nstudy (N=10) and identified the potential benefits and risks of such an\nadaptive scheduling tool. Then, after enhancing the system, we conducted two\ncontrolled experiments, one each for attendees and organizers (total N=66). For\neach experiment, we compared scheduling with verbal messages, shared calendars,\nor Togedule. Results show that Togedule significantly reduces the cognitive\nload of attendees indicating their availability and improves the speed and\nquality of the decisions made by organizers."}
{"id": "2505.11995", "pdf": "https://arxiv.org/pdf/2505.11995.pdf", "abs": "https://arxiv.org/abs/2505.11995", "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": "SIGIR 2025", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains."}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831.pdf", "abs": "https://arxiv.org/abs/2505.10831", "title": "Creating General User Models from Computer Use", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture user knowledge and preferences. GUMs can infer that a user is preparing\nfor a wedding they're attending from messages with a friend. Or recognize that\na user is struggling with a collaborator's feedback on a draft by observing\nmultiple stalled edits and a switch to reading related work. GUMs introduce an\narchitecture that infers new propositions about a user from multimodal\nobservations, retrieves related propositions for context, and continuously\nrevises existing propositions. To illustrate the breadth of applications that\nGUMs enable, we demonstrate how they augment chat-based assistants with\ncontext, manage OS notifications to selectively surface important information,\nand enable interactive agents that adapt to preferences across apps. We also\ninstantiate proactive assistants (GUMBOs) that discover and execute useful\nsuggestions on a user's behalf using their GUM. In our evaluations, we find\nthat GUMs make calibrated and accurate inferences about users, and that\nassistants built on GUMs proactively identify and perform actions that users\nwouldn't think to request explicitly. Altogether, GUMs introduce methods that\nleverage multimodal models to understand unstructured context, enabling\nlong-standing visions of HCI and entirely new interactive systems that\nanticipate user needs."}
{"id": "2505.12028", "pdf": "https://arxiv.org/pdf/2505.12028.pdf", "abs": "https://arxiv.org/abs/2505.12028", "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method", "authors": ["Yupei Ren", "Xinyi Zhou", "Ning Zhang", "Shangqing Zhao", "Man Lan", "Xiaopeng Bai"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025; 13 pages, 3 figures", "summary": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis."}
{"id": "2310.10378", "pdf": "https://arxiv.org/pdf/2310.10378.pdf", "abs": "https://arxiv.org/abs/2310.10378", "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models", "authors": ["Jirui Qi", "Raquel Fernández", "Arianna Bisazza"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "EMNLP2023 Outstanding Paper (Multilinguality and Linguistic Diversity\n  Track). All code and data are released at\n  https://github.com/Betswish/Cross-Lingual-Consistency", "summary": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score."}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043.pdf", "abs": "https://arxiv.org/abs/2505.12043", "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Although LLMs perform well in general tasks, domain-specific applications\nsuffer from hallucinations and accuracy limitations. CPT approaches encounter\ntwo key issues: (1) domain-biased data degrades general language skills, and\n(2) improper corpus-mixture ratios limit effective adaptation. To address\nthese, we propose a novel framework, Mixture of Losses (MoL), which decouples\noptimization objectives for domain-specific and general corpora. Specifically,\ncross-entropy (CE) loss is applied to domain data to ensure knowledge\nacquisition, while Kullback-Leibler (KL) divergence aligns general-corpus\ntraining with the base model's foundational capabilities. This dual-loss\narchitecture preserves universal skills while enhancing domain expertise,\navoiding catastrophic forgetting. Empirically, we validate that a 1:1\ndomain-to-general corpus ratio optimally balances training and overfitting\nwithout the need for extensive tuning or resource-intensive experiments.\nFurthermore, our experiments demonstrate significant performance gains compared\nto traditional CPT approaches, which often suffer from degradation in general\nlanguage capabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach."}
{"id": "2407.13195", "pdf": "https://arxiv.org/pdf/2407.13195.pdf", "abs": "https://arxiv.org/abs/2407.13195", "title": "Scalable Exploration via Ensemble++", "authors": ["Yingru Li", "Jiawei Xu", "Baoxiang Wang", "Zhi-Quan Luo"], "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.IT", "math.IT", "stat.ML"], "comment": "53 pages", "summary": "Thompson Sampling is a principled method for balancing exploration and\nexploitation, but its real-world adoption faces computational challenges in\nlarge-scale or non-conjugate settings. While ensemble-based approaches offer\npartial remedies, they typically require prohibitively large ensemble sizes. We\npropose Ensemble++, a scalable exploration framework using a novel\nshared-factor ensemble architecture with random linear combinations. For linear\nbandits, we provide theoretical guarantees showing that Ensemble++ achieves\nregret comparable to exact Thompson Sampling with only $\\Theta(d \\log T)$\nensemble sizes--significantly outperforming prior methods. Crucially, this\nefficiency holds across both compact and finite action sets with either\ntime-invariant or time-varying contexts without configuration changes. We\nextend this theoretical foundation to nonlinear rewards by replacing fixed\nfeatures with learnable neural representations while preserving the same\nincremental update principle, effectively bridging theory and practice for\nreal-world tasks. Comprehensive experiments across linear, quadratic, neural,\nand GPT-based contextual bandits validate our theoretical findings and\ndemonstrate Ensemble++'s superior regret-computation tradeoff versus\nstate-of-the-art methods."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050.pdf", "abs": "https://arxiv.org/abs/2505.12050", "title": "ABoN: Adaptive Best-of-N Alignment", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2407.15010", "pdf": "https://arxiv.org/pdf/2407.15010.pdf", "abs": "https://arxiv.org/abs/2407.15010", "title": "ChatISA: A Prompt-Engineered, In-House Multi-Modal Generative AI Chatbot for Information Systems Education", "authors": ["Fadel M. Megahed", "Ying-Ju Chen", "Joshua A. Ferris", "Cameron Resatar", "Kaitlyn Ross", "Younghwa Lee", "L. Allison Jones-Farmer"], "categories": ["cs.CY", "cs.HC", "cs.LG"], "comment": "22 pages", "summary": "As generative AI ('GenAI') continues to evolve, educators face the challenge\nof preparing students for a future where AI-assisted work is integral to\nprofessional success. This paper introduces ChatISA, an in-house, multi-model\nAI chatbot designed to support students and faculty in an Information Systems\nand Analytics (ISA) department. ChatISA comprises four primary modules: Coding\nCompanion, Project Coach, Exam Ally, and Interview Mentor, each tailored to\nenhance different aspects of the educational experience. Through iterative\ndevelopment, student feedback, and leveraging open-source frameworks, we\ncreated a robust tool that addresses coding inquiries, project management, exam\npreparation, and interview readiness. The implementation of ChatISA provided\nvaluable insights and highlighted key challenges. Our findings demonstrate the\nbenefits of ChatISA for ISA education while underscoring the need for adaptive\npedagogy and proactive engagement with AI tools to fully harness their\neducational potential. To support broader adoption and innovation, all code for\nChatISA is made publicly available on GitHub, enabling other institutions to\ncustomize and integrate similar AI-driven educational tools within their\ncurricula."}
{"id": "2505.12054", "pdf": "https://arxiv.org/pdf/2505.12054.pdf", "abs": "https://arxiv.org/abs/2505.12054", "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs", "authors": ["Matúš Pikuliak"], "categories": ["cs.CL"], "comment": null, "summary": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring."}
{"id": "2410.22076", "pdf": "https://arxiv.org/pdf/2410.22076.pdf", "abs": "https://arxiv.org/abs/2410.22076", "title": "USpeech: Ultrasound-Enhanced Speech with Minimal Human Effort via Cross-Modal Synthesis", "authors": ["Luca Jiang-Tao Yu", "Running Zhao", "Sijie Ji", "Edith C. H. Ngai", "Chenshu Wu"], "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": "Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (ACM IMWUT/UbiComp 2025)", "summary": "Speech enhancement is crucial for ubiquitous human-computer interaction.\nRecently, ultrasound-based acoustic sensing has emerged as an attractive choice\nfor speech enhancement because of its superior ubiquity and performance.\nHowever, due to inevitable interference from unexpected and unintended sources\nduring audio-ultrasound data acquisition, existing solutions rely heavily on\nhuman effort for data collection and processing. This leads to significant data\nscarcity that limits the full potential of ultrasound-based speech enhancement.\nTo address this, we propose USpeech, a cross-modal ultrasound synthesis\nframework for speech enhancement with minimal human effort. At its core is a\ntwo-stage framework that establishes the correspondence between visual and\nultrasonic modalities by leveraging audio as a bridge. This approach overcomes\nchallenges from the lack of paired video-ultrasound datasets and the inherent\nheterogeneity between video and ultrasound data. Our framework incorporates\ncontrastive video-audio pre-training to project modalities into a shared\nsemantic space and employs an audio-ultrasound encoder-decoder for ultrasound\nsynthesis. We then present a speech enhancement network that enhances speech in\nthe time-frequency domain and recovers the clean speech waveform via a neural\nvocoder. Comprehensive experiments show USpeech achieves remarkable performance\nusing synthetic ultrasound data comparable to physical data, outperforming\nstate-of-the-art ultrasound-based speech enhancement baselines. USpeech is\nopen-sourced at https://github.com/aiot-lab/USpeech/."}
{"id": "2505.12060", "pdf": "https://arxiv.org/pdf/2505.12060.pdf", "abs": "https://arxiv.org/abs/2505.12060", "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement", "authors": ["Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": "Acccepted by ACL 2025 Findings, 21 pages, 9 figures, 14 tables", "summary": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE."}
{"id": "2505.12071", "pdf": "https://arxiv.org/pdf/2505.12071.pdf", "abs": "https://arxiv.org/abs/2505.12071", "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach", "authors": ["Harald Baayen", "Kristian Berg", "Maziyah Mohamed"], "categories": ["cs.CL"], "comment": "35 pages, 11 figures", "summary": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words."}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075.pdf", "abs": "https://arxiv.org/abs/2505.12075", "title": "Do different prompting methods yield a common task representation in language models?", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082.pdf", "abs": "https://arxiv.org/abs/2505.12082", "title": "Model Merging in Pre-training of Large Language Models", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090.pdf", "abs": "https://arxiv.org/abs/2505.12090", "title": "Personalized Author Obfuscation with Large Language Models", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue."}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100.pdf", "abs": "https://arxiv.org/abs/2505.12100", "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jordão"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116.pdf", "abs": "https://arxiv.org/abs/2505.12116", "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety."}
{"id": "2505.12158", "pdf": "https://arxiv.org/pdf/2505.12158.pdf", "abs": "https://arxiv.org/abs/2505.12158", "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions", "authors": ["Elisa Bassignana", "Amanda Cercas Curry", "Dirk Hovy"], "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups."}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160.pdf", "abs": "https://arxiv.org/abs/2505.12160", "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances."}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182.pdf", "abs": "https://arxiv.org/abs/2505.12182", "title": "Truth Neurons", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183.pdf", "abs": "https://arxiv.org/abs/2505.12183", "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "authors": ["Manari Hirose", "Masato Uchida"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196.pdf", "abs": "https://arxiv.org/abs/2505.12196", "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used."}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201.pdf", "abs": "https://arxiv.org/abs/2505.12201", "title": "How Reliable is Multilingual LLM-as-a-Judge?", "authors": ["Xiyan Fu", "Wei Liu"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications."}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212.pdf", "abs": "https://arxiv.org/abs/2505.12212", "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "authors": ["Shaobo Wang", "Ziming Wang", "Xiangqi Jin", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup."}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215.pdf", "abs": "https://arxiv.org/abs/2505.12215", "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin."}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216.pdf", "abs": "https://arxiv.org/abs/2505.12216", "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "authors": ["Rongguang Ye", "Ming Tang"], "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines."}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218.pdf", "abs": "https://arxiv.org/abs/2505.12218", "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236.pdf", "abs": "https://arxiv.org/abs/2505.12236", "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238.pdf", "abs": "https://arxiv.org/abs/2505.12238", "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs."}
{"id": "2505.12244", "pdf": "https://arxiv.org/pdf/2505.12244.pdf", "abs": "https://arxiv.org/abs/2505.12244", "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "authors": ["Haojin Wang", "Zining Zhu", "Freda Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers."}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250.pdf", "abs": "https://arxiv.org/abs/2505.12250", "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B"}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259.pdf", "abs": "https://arxiv.org/abs/2505.12259", "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance."}
{"id": "2505.12265", "pdf": "https://arxiv.org/pdf/2505.12265.pdf", "abs": "https://arxiv.org/abs/2505.12265", "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation", "authors": ["Chengwei Qin", "Wenxuan Zhou", "Karthik Abinav Sankararaman", "Nanshu Wang", "Tengyu Xu", "Alexander Radovic", "Eryk Helenowski", "Arya Talebzadeh", "Aditya Tayade", "Sinong Wang", "Shafiq Joty", "Han Fang", "Hao Ma"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact."}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268.pdf", "abs": "https://arxiv.org/abs/2505.12268", "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "authors": ["Pratim Chowdhary"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits."}
{"id": "2505.12273", "pdf": "https://arxiv.org/pdf/2505.12273.pdf", "abs": "https://arxiv.org/abs/2505.12273", "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark", "authors": ["Md. Atiqur Rahman", "Sabrina Islam", "Mushfiqul Haque Omi"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage."}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287.pdf", "abs": "https://arxiv.org/abs/2505.12287", "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299.pdf", "abs": "https://arxiv.org/abs/2505.12299", "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.12300", "pdf": "https://arxiv.org/pdf/2505.12300.pdf", "abs": "https://arxiv.org/abs/2505.12300", "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets."}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306.pdf", "abs": "https://arxiv.org/abs/2505.12306", "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%."}
{"id": "2505.12313", "pdf": "https://arxiv.org/pdf/2505.12313.pdf", "abs": "https://arxiv.org/abs/2505.12313", "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost."}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328.pdf", "abs": "https://arxiv.org/abs/2505.12328", "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123."}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345.pdf", "abs": "https://arxiv.org/abs/2505.12345", "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349.pdf", "abs": "https://arxiv.org/abs/2505.12349", "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "authors": ["Axel Abels", "Tom Lenaerts"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368.pdf", "abs": "https://arxiv.org/abs/2505.12368", "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381.pdf", "abs": "https://arxiv.org/abs/2505.12381", "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392.pdf", "abs": "https://arxiv.org/abs/2505.12392", "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398.pdf", "abs": "https://arxiv.org/abs/2505.12398", "title": "Traversal Verification for Speculative Tree Decoding", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405.pdf", "abs": "https://arxiv.org/abs/2505.12405", "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415.pdf", "abs": "https://arxiv.org/abs/2505.12415", "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423.pdf", "abs": "https://arxiv.org/abs/2505.12423", "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC."}
{"id": "2505.12439", "pdf": "https://arxiv.org/pdf/2505.12439.pdf", "abs": "https://arxiv.org/abs/2505.12439", "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games", "authors": ["Jinming Zhang", "Yunfei Long"], "categories": ["cs.CL"], "comment": null, "summary": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments."}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452.pdf", "abs": "https://arxiv.org/abs/2505.12452", "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized."}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454.pdf", "abs": "https://arxiv.org/abs/2505.12454", "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods."}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474.pdf", "abs": "https://arxiv.org/abs/2505.12474", "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task."}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476.pdf", "abs": "https://arxiv.org/abs/2505.12476", "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively."}
{"id": "2505.12495", "pdf": "https://arxiv.org/pdf/2505.12495.pdf", "abs": "https://arxiv.org/abs/2505.12495", "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation", "authors": ["Nikita Tatarinov", "Vidhyakshaya Kannan", "Haricharana Srinivasa", "Arnav Raj", "Harpreet Singh Anand", "Varun Singh", "Aditya Luthra", "Ravij Lade", "Agam Shah", "Sudheer Chava"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations."}
{"id": "2505.12507", "pdf": "https://arxiv.org/pdf/2505.12507.pdf", "abs": "https://arxiv.org/abs/2505.12507", "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "authors": ["Xu Zheng", "Zhuomin Chen", "Esteban Schafir", "Sipeng Chen", "Hojat Allah Salehi", "Haifeng Chen", "Farhad Shirani", "Wei Cheng", "Dongsheng Luo"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT."}
{"id": "2505.12511", "pdf": "https://arxiv.org/pdf/2505.12511.pdf", "abs": "https://arxiv.org/abs/2505.12511", "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design", "authors": ["Yanting Li", "Jiyue Jiang", "Zikang Wang", "Ziqian Lin", "Dongchen He", "Yuheng Shan", "Yanruisheng Shao", "Jiayi Li", "Xiangyu Shi", "Jiuming Wang", "Yanyu Chen", "Yimin Fan", "Han Li", "Yu Li"], "categories": ["cs.CL"], "comment": null, "summary": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities."}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531.pdf", "abs": "https://arxiv.org/abs/2505.12531", "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "authors": ["Navid Madani", "Rohini Srihari"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI."}
{"id": "2505.12533", "pdf": "https://arxiv.org/pdf/2505.12533.pdf", "abs": "https://arxiv.org/abs/2505.12533", "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "authors": ["Varvara Arzt", "Allan Hanbury", "Michael Wiegand", "Gábor Recski", "Terra Blevins"], "categories": ["cs.CL"], "comment": null, "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability."}
{"id": "2505.12543", "pdf": "https://arxiv.org/pdf/2505.12543.pdf", "abs": "https://arxiv.org/abs/2505.12543", "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "authors": ["Md Mehrab Tanjim", "Yeonjun In", "Xiang Chen", "Victor S. Bursztyn", "Ryan A. Rossi", "Sungchul Kim", "Guang-Jie Ren", "Vaishnavi Muppala", "Shun Jiang", "Yongsung Kim", "Chanyoung Park"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems."}
{"id": "2505.12545", "pdf": "https://arxiv.org/pdf/2505.12545.pdf", "abs": "https://arxiv.org/abs/2505.12545", "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models", "authors": ["Yang Zhao", "Pu Wang", "Yibo Zhao", "Hongru Du", "Hao", "Yang"], "categories": ["cs.CL"], "comment": "Last revised 13 Feb 2025. Under review in Nature portfolio", "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes."}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546.pdf", "abs": "https://arxiv.org/abs/2505.12546", "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side."}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560.pdf", "abs": "https://arxiv.org/abs/2505.12560", "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "authors": ["Hiram Ring"], "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large automatically tagged parallel dataset which has been\ndeveloped to partially address this issue. The taggedPBC contains more than\n1,800 sentences of pos-tagged parallel text data from over 1,500 languages,\nrepresenting 133 language families and 111 isolates, dwarfing previously\navailable resources. The accuracy of tags in this dataset is shown to correlate\nwell with both existing SOTA taggers for high-resource languages (SpaCy,\nTrankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).\nAdditionally, a novel measure derived from this dataset, the N1 ratio,\ncorrelates with expert determinations of word order in three typological\ndatabases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier\ntrained on this feature can accurately identify basic word order for languages\nnot in those databases. While much work is still needed to expand and develop\nthis dataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub."}
{"id": "2505.12568", "pdf": "https://arxiv.org/pdf/2505.12568.pdf", "abs": "https://arxiv.org/abs/2505.12568", "title": "Enriching Patent Claim Generation with European Patent Dataset", "authors": ["Lekang Jiang", "Chengzu Li", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "18 pages, 13 tables, 4 figures", "summary": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch."}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572.pdf", "abs": "https://arxiv.org/abs/2505.12572", "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "authors": ["Hanwen Shen", "Ting Ying"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels."}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584.pdf", "abs": "https://arxiv.org/abs/2505.12584", "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations."}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587.pdf", "abs": "https://arxiv.org/abs/2505.12587", "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages."}
{"id": "2505.12592", "pdf": "https://arxiv.org/pdf/2505.12592.pdf", "abs": "https://arxiv.org/abs/2505.12592", "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "categories": ["cs.CL"], "comment": null, "summary": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts."}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594.pdf", "abs": "https://arxiv.org/abs/2505.12594", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."}
{"id": "2505.12616", "pdf": "https://arxiv.org/pdf/2505.12616.pdf", "abs": "https://arxiv.org/abs/2505.12616", "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "authors": ["Shujauddin Syed", "Ted Pedersen"], "categories": ["cs.CL", "68T50"], "comment": "SemEval-2025", "summary": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios."}
{"id": "2505.12621", "pdf": "https://arxiv.org/pdf/2505.12621.pdf", "abs": "https://arxiv.org/abs/2505.12621", "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "authors": ["João Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "categories": ["cs.CL", "cs.IR"], "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "summary": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box."}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625.pdf", "abs": "https://arxiv.org/abs/2505.12625", "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment."}
{"id": "2505.12636", "pdf": "https://arxiv.org/pdf/2505.12636.pdf", "abs": "https://arxiv.org/abs/2505.12636", "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654.pdf", "abs": "https://arxiv.org/abs/2505.12654", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662.pdf", "abs": "https://arxiv.org/abs/2505.12662", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716.pdf", "abs": "https://arxiv.org/abs/2505.12716", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717.pdf", "abs": "https://arxiv.org/abs/2505.12717", "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks."}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718.pdf", "abs": "https://arxiv.org/abs/2505.12718", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content."}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723.pdf", "abs": "https://arxiv.org/abs/2505.12723", "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727.pdf", "abs": "https://arxiv.org/abs/2505.12727", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma."}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768.pdf", "abs": "https://arxiv.org/abs/2505.12768", "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set."}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781.pdf", "abs": "https://arxiv.org/abs/2505.12781", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792.pdf", "abs": "https://arxiv.org/abs/2505.12792", "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies."}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808.pdf", "abs": "https://arxiv.org/abs/2505.12808", "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena."}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814.pdf", "abs": "https://arxiv.org/abs/2505.12814", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821.pdf", "abs": "https://arxiv.org/abs/2505.12821", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec."}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831.pdf", "abs": "https://arxiv.org/abs/2505.12831", "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835.pdf", "abs": "https://arxiv.org/abs/2505.12835", "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837.pdf", "abs": "https://arxiv.org/abs/2505.12837", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications."}
{"id": "2505.12859", "pdf": "https://arxiv.org/pdf/2505.12859.pdf", "abs": "https://arxiv.org/abs/2505.12859", "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "categories": ["cs.CL"], "comment": "To be presented a ACL 2025, Main, Long paper", "summary": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864.pdf", "abs": "https://arxiv.org/abs/2505.12864", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.12888", "pdf": "https://arxiv.org/pdf/2505.12888.pdf", "abs": "https://arxiv.org/abs/2505.12888", "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines."}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896.pdf", "abs": "https://arxiv.org/abs/2505.12896", "title": "On the Thinking-Language Modeling Gap in Large Language Models", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks."}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920.pdf", "abs": "https://arxiv.org/abs/2505.12920", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication."}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929.pdf", "abs": "https://arxiv.org/abs/2505.12929", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942.pdf", "abs": "https://arxiv.org/abs/2505.12942", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.12949", "pdf": "https://arxiv.org/pdf/2505.12949.pdf", "abs": "https://arxiv.org/abs/2505.12949", "title": "Neural Morphological Tagging for Nguni Languages", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "categories": ["cs.CL"], "comment": null, "summary": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages."}
{"id": "2505.12950", "pdf": "https://arxiv.org/pdf/2505.12950.pdf", "abs": "https://arxiv.org/abs/2505.12950", "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "categories": ["cs.CL"], "comment": "14 pages, 9 figures", "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE."}
{"id": "2505.12964", "pdf": "https://arxiv.org/pdf/2505.12964.pdf", "abs": "https://arxiv.org/abs/2505.12964", "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master."}
{"id": "2505.12969", "pdf": "https://arxiv.org/pdf/2505.12969.pdf", "abs": "https://arxiv.org/abs/2505.12969", "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other."}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970.pdf", "abs": "https://arxiv.org/abs/2505.12970", "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "authors": ["Robin Jegan", "Andreas Henrich"], "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801"}
{"id": "2505.12973", "pdf": "https://arxiv.org/pdf/2505.12973.pdf", "abs": "https://arxiv.org/abs/2505.12973", "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "categories": ["cs.CL"], "comment": "8 main body pages, total 25 pages, 15 figures", "summary": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems."}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983.pdf", "abs": "https://arxiv.org/abs/2505.12983", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch."}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996.pdf", "abs": "https://arxiv.org/abs/2505.12996", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance."}
{"id": "2505.13004", "pdf": "https://arxiv.org/pdf/2505.13004.pdf", "abs": "https://arxiv.org/abs/2505.13004", "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x."}
{"id": "2505.13006", "pdf": "https://arxiv.org/pdf/2505.13006.pdf", "abs": "https://arxiv.org/abs/2505.13006", "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias Käfer"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 industry track", "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010.pdf", "abs": "https://arxiv.org/abs/2505.13010", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034.pdf", "abs": "https://arxiv.org/abs/2505.13034", "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "authors": ["Márton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models."}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036.pdf", "abs": "https://arxiv.org/abs/2505.13036", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike Züfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation."}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053.pdf", "abs": "https://arxiv.org/abs/2505.13053", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general."}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069.pdf", "abs": "https://arxiv.org/abs/2505.13069", "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenolé Quellec"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077.pdf", "abs": "https://arxiv.org/abs/2505.13077", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089.pdf", "abs": "https://arxiv.org/abs/2505.13089", "title": "Systematic Generalization in Language Models Scales with Information Entropy", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "Étienne Simon"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization."}
{"id": "2505.13090", "pdf": "https://arxiv.org/pdf/2505.13090.pdf", "abs": "https://arxiv.org/abs/2505.13090", "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "authors": ["David Stap", "Christof Monz"], "categories": ["cs.CL"], "comment": null, "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115.pdf", "abs": "https://arxiv.org/abs/2505.13115", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136.pdf", "abs": "https://arxiv.org/abs/2505.13136", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141.pdf", "abs": "https://arxiv.org/abs/2505.13141", "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English."}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147.pdf", "abs": "https://arxiv.org/abs/2505.13147", "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP."}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156.pdf", "abs": "https://arxiv.org/abs/2505.13156", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157.pdf", "abs": "https://arxiv.org/abs/2505.13157", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171.pdf", "abs": "https://arxiv.org/abs/2505.13171", "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173.pdf", "abs": "https://arxiv.org/abs/2505.13173", "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176.pdf", "abs": "https://arxiv.org/abs/2505.13176", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.13181", "pdf": "https://arxiv.org/pdf/2505.13181.pdf", "abs": "https://arxiv.org/abs/2505.13181", "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models."}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204.pdf", "abs": "https://arxiv.org/abs/2505.13204", "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210.pdf", "abs": "https://arxiv.org/abs/2505.13210", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."}
{"id": "2505.13220", "pdf": "https://arxiv.org/pdf/2505.13220.pdf", "abs": "https://arxiv.org/abs/2505.13220", "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design."}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244.pdf", "abs": "https://arxiv.org/abs/2505.13244", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."}
{"id": "2505.13251", "pdf": "https://arxiv.org/pdf/2505.13251.pdf", "abs": "https://arxiv.org/abs/2505.13251", "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "authors": ["Sidney Wong"], "categories": ["cs.CL"], "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "summary": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide."}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252.pdf", "abs": "https://arxiv.org/abs/2505.13252", "title": "Natural Language Planning via Coding and Inference Scaling", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization."}
{"id": "2505.13254", "pdf": "https://arxiv.org/pdf/2505.13254.pdf", "abs": "https://arxiv.org/abs/2505.13254", "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257.pdf", "abs": "https://arxiv.org/abs/2505.13257", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258.pdf", "abs": "https://arxiv.org/abs/2505.13258", "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released."}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259.pdf", "abs": "https://arxiv.org/abs/2505.13259", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268.pdf", "abs": "https://arxiv.org/abs/2505.13268", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271.pdf", "abs": "https://arxiv.org/abs/2505.13271", "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql."}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282.pdf", "abs": "https://arxiv.org/abs/2505.13282", "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods."}
{"id": "2505.13302", "pdf": "https://arxiv.org/pdf/2505.13302.pdf", "abs": "https://arxiv.org/abs/2505.13302", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm"}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307.pdf", "abs": "https://arxiv.org/abs/2505.13307", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312.pdf", "abs": "https://arxiv.org/abs/2505.13312", "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility."}
{"id": "2505.13328", "pdf": "https://arxiv.org/pdf/2505.13328.pdf", "abs": "https://arxiv.org/abs/2505.13328", "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338.pdf", "abs": "https://arxiv.org/abs/2505.13338", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346.pdf", "abs": "https://arxiv.org/abs/2505.13346", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.13348", "pdf": "https://arxiv.org/pdf/2505.13348.pdf", "abs": "https://arxiv.org/abs/2505.13348", "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks."}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353.pdf", "abs": "https://arxiv.org/abs/2505.13353", "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "authors": ["Adam Štorek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."}
{"id": "2505.13360", "pdf": "https://arxiv.org/pdf/2505.13360.pdf", "abs": "https://arxiv.org/abs/2505.13360", "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian Kästner", "Tongshuang Wu"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379.pdf", "abs": "https://arxiv.org/abs/2505.13379", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388.pdf", "abs": "https://arxiv.org/abs/2505.13388", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403.pdf", "abs": "https://arxiv.org/abs/2505.13403", "title": "MR. Judge: Multimodal Reasoner as a Judge", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%."}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404.pdf", "abs": "https://arxiv.org/abs/2505.13404", "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary"}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417.pdf", "abs": "https://arxiv.org/abs/2505.13417", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418.pdf", "abs": "https://arxiv.org/abs/2505.13418", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."}
{"id": "2505.13434", "pdf": "https://arxiv.org/pdf/2505.13434.pdf", "abs": "https://arxiv.org/abs/2505.13434", "title": "SMOTExT: SMOTE meets Large Language Models", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "categories": ["cs.CL"], "comment": null, "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444.pdf", "abs": "https://arxiv.org/abs/2505.13444", "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448.pdf", "abs": "https://arxiv.org/abs/2505.13448", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE."}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545.pdf", "abs": "https://arxiv.org/abs/2505.11545", "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io."}
{"id": "2505.11572", "pdf": "https://arxiv.org/pdf/2505.11572.pdf", "abs": "https://arxiv.org/abs/2505.11572", "title": "ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems", "authors": ["Anand Rai", "Satyam Rahangdale", "Utkarsh Anand", "Animesh Mukherjee"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Paper accepted at INTERSPEECH 2025", "summary": "Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday\napplications, yet significant disparities in performance across diverse\ndemographic groups persist. In this work, we introduce the ASR-FAIRBENCH\nleaderboard which is designed to assess both the accuracy and equity of ASR\nmodels in real-time. Leveraging the Meta's Fair-Speech dataset, which captures\ndiverse demographic characteristics, we employ a mixed-effects Poisson\nregression model to derive an overall fairness score. This score is integrated\nwith traditional metrics like Word Error Rate (WER) to compute the Fairness\nAdjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our\napproach reveals significant performance disparities in SOTA ASR models across\ndemographic groups and offers a benchmark to drive the development of more\ninclusive ASR technologies."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595.pdf", "abs": "https://arxiv.org/abs/2505.11595", "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611.pdf", "abs": "https://arxiv.org/abs/2505.11611", "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes."}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614.pdf", "abs": "https://arxiv.org/abs/2505.11614", "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717.pdf", "abs": "https://arxiv.org/abs/2505.11717", "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731.pdf", "abs": "https://arxiv.org/abs/2505.11731", "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737.pdf", "abs": "https://arxiv.org/abs/2505.11737", "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756.pdf", "abs": "https://arxiv.org/abs/2505.11756", "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770.pdf", "abs": "https://arxiv.org/abs/2505.11770", "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812.pdf", "abs": "https://arxiv.org/abs/2505.11812", "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842.pdf", "abs": "https://arxiv.org/abs/2505.11842", "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861.pdf", "abs": "https://arxiv.org/abs/2505.11861", "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines."}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875.pdf", "abs": "https://arxiv.org/abs/2505.11875", "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training."}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979.pdf", "abs": "https://arxiv.org/abs/2505.11979", "title": "Introduction to Analytical Software Engineering Design Paradigm", "authors": ["Tarik Houichime", "Younes El Amrani"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics."}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039.pdf", "abs": "https://arxiv.org/abs/2505.12039", "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research."}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058.pdf", "abs": "https://arxiv.org/abs/2505.12058", "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "authors": ["Vincent Koc"], "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065.pdf", "abs": "https://arxiv.org/abs/2505.12065", "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135.pdf", "abs": "https://arxiv.org/abs/2505.12135", "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Takáč", "Salem Lahlou"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$)."}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185.pdf", "abs": "https://arxiv.org/abs/2505.12185", "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189.pdf", "abs": "https://arxiv.org/abs/2505.12189", "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "André Freitas"], "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225.pdf", "abs": "https://arxiv.org/abs/2505.12225", "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260.pdf", "abs": "https://arxiv.org/abs/2505.12260", "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269.pdf", "abs": "https://arxiv.org/abs/2505.12269", "title": "Vague Knowledge: Evidence from Analyst Reports", "authors": ["Kerry Xiao", "Amy Zang"], "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284.pdf", "abs": "https://arxiv.org/abs/2505.12284", "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301.pdf", "abs": "https://arxiv.org/abs/2505.12301", "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307.pdf", "abs": "https://arxiv.org/abs/2505.12307", "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312.pdf", "abs": "https://arxiv.org/abs/2505.12312", "title": "Visuospatial Cognitive Assistant", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363.pdf", "abs": "https://arxiv.org/abs/2505.12363", "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371.pdf", "abs": "https://arxiv.org/abs/2505.12371", "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442.pdf", "abs": "https://arxiv.org/abs/2505.12442", "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457.pdf", "abs": "https://arxiv.org/abs/2505.12457", "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565.pdf", "abs": "https://arxiv.org/abs/2505.12565", "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629.pdf", "abs": "https://arxiv.org/abs/2505.12629", "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632.pdf", "abs": "https://arxiv.org/abs/2505.12632", "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680.pdf", "abs": "https://arxiv.org/abs/2505.12680", "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692.pdf", "abs": "https://arxiv.org/abs/2505.12692", "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763.pdf", "abs": "https://arxiv.org/abs/2505.12763", "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842.pdf", "abs": "https://arxiv.org/abs/2505.12842", "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871.pdf", "abs": "https://arxiv.org/abs/2505.12871", "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886.pdf", "abs": "https://arxiv.org/abs/2505.12886", "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891.pdf", "abs": "https://arxiv.org/abs/2505.12891", "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900.pdf", "abs": "https://arxiv.org/abs/2505.12900", "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938.pdf", "abs": "https://arxiv.org/abs/2505.12938", "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992.pdf", "abs": "https://arxiv.org/abs/2505.12992", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028.pdf", "abs": "https://arxiv.org/abs/2505.13028", "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "authors": ["Sayon Palit", "Daniel Woods"], "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.13032", "pdf": "https://arxiv.org/pdf/2505.13032.pdf", "abs": "https://arxiv.org/abs/2505.13032", "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "summary": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area."}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098.pdf", "abs": "https://arxiv.org/abs/2505.13098", "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109.pdf", "abs": "https://arxiv.org/abs/2505.13109", "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126.pdf", "abs": "https://arxiv.org/abs/2505.13126", "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208.pdf", "abs": "https://arxiv.org/abs/2505.13208", "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227.pdf", "abs": "https://arxiv.org/abs/2505.13227", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.13237", "pdf": "https://arxiv.org/pdf/2505.13237.pdf", "abs": "https://arxiv.org/abs/2505.13237", "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308.pdf", "abs": "https://arxiv.org/abs/2505.13308", "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380.pdf", "abs": "https://arxiv.org/abs/2505.13380", "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526"}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393.pdf", "abs": "https://arxiv.org/abs/2505.13393", "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "authors": ["Christopher K. Frantz"], "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool."}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398.pdf", "abs": "https://arxiv.org/abs/2505.13398", "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization."}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408.pdf", "abs": "https://arxiv.org/abs/2505.13408", "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430.pdf", "abs": "https://arxiv.org/abs/2505.13430", "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438.pdf", "abs": "https://arxiv.org/abs/2505.13438", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445.pdf", "abs": "https://arxiv.org/abs/2505.13445", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}
{"id": "2305.00948", "pdf": "https://arxiv.org/pdf/2305.00948.pdf", "abs": "https://arxiv.org/abs/2305.00948", "title": "Large Linguistic Models: Investigating LLMs' metalinguistic abilities", "authors": ["Gašper Beguš", "Maksymilian Dąbkowski", "Ryan Rhodes"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) has recently improved to the\npoint where models can perform well on many language tasks. We show here\nthat--for the first time--the models can also generate valid metalinguistic\nanalyses of language data. We outline a research program where the behavioral\ninterpretability of LLMs on these tasks is tested via prompting. LLMs are\ntrained primarily on text--as such, evaluating their metalinguistic abilities\nimproves our understanding of their general capabilities and sheds new light on\ntheoretical models in linguistics. We show that OpenAI's (2024) o1 vastly\noutperforms other models on tasks involving drawing syntactic trees and\nphonological generalization. We speculate that OpenAI o1's unique advantage\nover other models may result from the model's chain-of-thought mechanism, which\nmimics the structure of human reasoning used in complex cognitive tasks, such\nas linguistic analysis."}
{"id": "2305.13673", "pdf": "https://arxiv.org/pdf/2305.13673.pdf", "abs": "https://arxiv.org/abs/2305.13673", "title": "Physics of Language Models: Part 1, Learning Hierarchical Language Structures", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "V2 polishes writing and adds Appendix G; V3 polishes writing and\n  changes the title; V4 improves writing and adds Appendix H (more uniform\n  attention results)", "summary": "Transformer-based language models are effective but complex, and\nunderstanding their inner workings and reasoning mechanisms is a significant\nchallenge. Previous research has primarily explored how these models handle\nsimple tasks like name copying or selection, and we extend this by\ninvestigating how these models perform recursive language structure reasoning\ndefined by context-free grammars (CFGs). We introduce a family of synthetic\nCFGs that produce hierarchical rules, capable of generating lengthy sentences\n(e.g., hundreds of tokens) that are locally ambiguous and require dynamic\nprogramming to parse. Despite this complexity, we demonstrate that generative\nmodels like GPT can accurately learn and reason over CFG-defined hierarchies\nand generate sentences based on it. We explore the model's internals, revealing\nthat its hidden states precisely capture the structure of CFGs, and its\nattention patterns resemble the information passing in a dynamic programming\nalgorithm.\n  This paper also presents several corollaries, including showing why absolute\npositional embeddings is inferior to relative and rotary embeddings; uniform\nattention alone is surprisingly effective (motivating our follow-up work on\nCanon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep\nstructure reasoning on CFGs compared to autoregressive models (e.g., GPT); and\ninjecting structural or syntactic noise into pretraining data markedly improves\nrobustness to corrupted language prompts."}
{"id": "2310.10378", "pdf": "https://arxiv.org/pdf/2310.10378.pdf", "abs": "https://arxiv.org/abs/2310.10378", "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models", "authors": ["Jirui Qi", "Raquel Fernández", "Arianna Bisazza"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "EMNLP2023 Outstanding Paper (Multilinguality and Linguistic Diversity\n  Track). All code and data are released at\n  https://github.com/Betswish/Cross-Lingual-Consistency", "summary": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score."}
{"id": "2310.18290", "pdf": "https://arxiv.org/pdf/2310.18290.pdf", "abs": "https://arxiv.org/abs/2310.18290", "title": "Automatically generating Riddles aiding Concept Attainment", "authors": ["Niharika Sri Parasa", "Chaitali Diwan", "Srinath Srinivasa"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "One of the primary challenges in online learning environments, is to retain\nlearner engagement. Several different instructional strategies are proposed\nboth in online and offline environments to enhance learner engagement. The\nConcept Attainment Model is one such instructional strategy that focuses on\nlearners acquiring a deeper understanding of a concept rather than just its\ndictionary definition. This is done by searching and listing the properties\nused to distinguish examples from non-examples of various concepts. Our work\nattempts to apply the Concept Attainment Model to build conceptual riddles, to\ndeploy over online learning environments. The approach involves creating\nfactual triples from learning resources, classifying them based on their\nuniqueness to a concept into `Topic Markers' and `Common', followed by\ngenerating riddles based on the Concept Attainment Model's format and capturing\nall possible solutions to those riddles. The results obtained from the human\nevaluation of riddles prove encouraging."}
{"id": "2402.01172", "pdf": "https://arxiv.org/pdf/2402.01172.pdf", "abs": "https://arxiv.org/abs/2402.01172", "title": "Streaming Sequence Transduction through Dynamic Compression", "authors": ["Weiting Tan", "Yunmo Chen", "Tongfei Chen", "Guanghui Qin", "Haoran Xu", "Heidi C. Zhang", "Benjamin Van Durme", "Philipp Koehn"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "We introduce STAR (Stream Transduction with Anchor Representations), a novel\nTransformer-based model designed for efficient sequence-to-sequence\ntransduction over streams. STAR dynamically segments input streams to create\ncompressed anchor representations, achieving nearly lossless compression (12x)\nin Automatic Speech Recognition (ASR) and outperforming existing methods.\nMoreover, STAR demonstrates superior segmentation and latency-quality\ntrade-offs in simultaneous speech-to-text tasks, optimizing latency, memory\nfootprint, and quality."}
{"id": "2402.10528", "pdf": "https://arxiv.org/pdf/2402.10528.pdf", "abs": "https://arxiv.org/abs/2402.10528", "title": "Can We Verify Step by Step for Incorrect Answer Detection?", "authors": ["Xin Xu", "Shizhe Diao", "Can Yang", "Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to IJCAI 2025", "summary": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy."}
{"id": "2402.12692", "pdf": "https://arxiv.org/pdf/2402.12692.pdf", "abs": "https://arxiv.org/abs/2402.12692", "title": "FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning", "authors": ["Xiao Li", "Bolin Zhu", "Kaiwen Shi", "Sichen Liu", "Yin Zhu", "Yiwei Liu", "Gong Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "The application of formulas (e.g., physics formulas) is a fundamental ability\nof humans when solving numerical reasoning problems. Existing numerical\nreasoning datasets seldom explicitly indicate the formulas employed in\nreasoning, as their questions rely on implicit commonsense mathematical\nknowledge. In contrast, in this paper, we introduce FormulaReasoning, a new\ndataset specifically designed for formula-based numerical reasoning. Each of\nthe 4,751 questions in our dataset requires numerical calculation with external\nphysics formulas, making it a more challenging benchmark for evaluating large\nlanguage models (LLMs). We offer normalized fine-grained annotations for the\nquestions, available in English and Chinese, including formula structures,\nparameter names, symbols, numerical values, and units, derived from extensive\nmanual effort with LLM assistance for guaranteed quality. We also provide a\nconsolidated formula database to serve as an external knowledge base\naccompanying the dataset. We employ FormulaReasoning to evaluate LLMs with 7B\nto over 100B parameters, and explore retrieval-augmented generation with the\nformula database. Our evaluation also covers supervised methods that break down\nthe reasoning process into formula generation, parameter extraction, and\nnumerical calculation, as well as direct preference optimization methods based\non derived preference data."}
{"id": "2402.12819", "pdf": "https://arxiv.org/pdf/2402.12819.pdf", "abs": "https://arxiv.org/abs/2402.12819", "title": "Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance", "authors": ["Branislav Pecher", "Ivan Srba", "Maria Bielikova"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When solving NLP tasks with limited labelled data, researchers typically\neither use a general large language model without further update, or use a\nsmall number of labelled samples to tune a specialised smaller model. In this\nwork, we answer an important question -- how many labelled samples are required\nfor the specialised small models to outperform general large models, while\ntaking the performance variance into consideration. By observing the behaviour\nof fine-tuning, instruction-tuning, prompting and in-context learning on 8\nlanguage models, we identify such performance break-even points across 8\nrepresentative text classification tasks of varying characteristics. We show\nthat the specialised models often need only few samples (on average $100$) to\nbe on par or better than the general ones. At the same time, the number of\nrequired labels strongly depends on the dataset or task characteristics, with\nfine-tuning on binary datasets requiring significantly more samples. When\nperformance variance is taken into consideration, the number of required labels\nincreases on average by $100 - 200\\%$. Finally, larger models do not\nconsistently lead to better performance and lower variance, with 4-bit\nquantisation having negligible impact."}
{"id": "2404.17874", "pdf": "https://arxiv.org/pdf/2404.17874.pdf", "abs": "https://arxiv.org/abs/2404.17874", "title": "From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets", "authors": ["Manuel Tonneau", "Diyi Liu", "Samuel Fraiberger", "Ralph Schroeder", "Scott A. Hale", "Paul Röttger"], "categories": ["cs.CL"], "comment": "Accepted at WOAH (NAACL 2024). Please cite the ACL Anthology version:\n  https://aclanthology.org/2024.woah-1.23/", "summary": "Perceptions of hate can vary greatly across cultural contexts. Hate speech\n(HS) datasets, however, have traditionally been developed by language. This\nhides potential cultural biases, as one language may be spoken in different\ncountries home to different cultures. In this work, we evaluate cultural bias\nin HS datasets by leveraging two interrelated cultural proxies: language and\ngeography. We conduct a systematic survey of HS datasets in eight languages and\nconfirm past findings on their English-language bias, but also show that this\nbias has been steadily decreasing in the past few years. For three\ngeographically-widespread languages -- English, Arabic and Spanish -- we then\nleverage geographical metadata from tweets to approximate geo-cultural contexts\nby pairing language and country information. We find that HS datasets for these\nlanguages exhibit a strong geo-cultural bias, largely overrepresenting a\nhandful of countries (e.g., US and UK for English) relative to their prominence\nin both the broader social media population and the general population speaking\nthese languages. Based on these findings, we formulate recommendations for the\ncreation of future HS datasets."}
{"id": "2405.15525", "pdf": "https://arxiv.org/pdf/2405.15525.pdf", "abs": "https://arxiv.org/abs/2405.15525", "title": "Sparse Matrix in Large Language Model Fine-tuning", "authors": ["Haoze He", "Juncheng Billy Li", "Xuan Jiang", "Heather Miller"], "categories": ["cs.CL"], "comment": "14 pages", "summary": "LoRA and its variants have become popular parameter-efficient fine-tuning\n(PEFT) methods due to their ability to avoid excessive computational costs.\nHowever, an accuracy gap often exists between PEFT methods and full fine-tuning\n(FT), and this gap has yet to be systematically studied. In this work, we\nintroduce a method for selecting sparse sub-matrices that aim to minimize the\nperformance gap between PEFT vs. full fine-tuning (FT) while also reducing both\nfine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT)\nmethod begins by identifying the most significant sub-matrices in the gradient\nupdate, updating only these blocks during the fine-tuning process. In our\nexperiments, we demonstrate that SMT consistently surpasses other PEFT baseline\n(e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA\nacross a broad spectrum of tasks, while reducing the GPU memory footprint by\n67% compared to FT. We also examine how the performance of LoRA and DoRA tends\nto plateau and decline as the number of trainable parameters increases, in\ncontrast, our SMT method does not suffer from such issue."}
{"id": "2405.20947", "pdf": "https://arxiv.org/pdf/2405.20947.pdf", "abs": "https://arxiv.org/abs/2405.20947", "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models", "authors": ["Justin Cui", "Wei-Lin Chiang", "Ion Stoica", "Cho-Jui Hsieh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!", "summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models."}
{"id": "2406.10785", "pdf": "https://arxiv.org/pdf/2406.10785.pdf", "abs": "https://arxiv.org/abs/2406.10785", "title": "ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation", "authors": ["Yurun Song", "Junchen Zhao", "Ian G. Harris", "Sangeetha Abdu Jyothi"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "In this paper, we introduce \\textbf{Share}d \\textbf{Lo}w \\textbf{R}ank\n\\textbf{A}daptation (ShareLoRA), a Large Language Model (LLM) fine-tuning\ntechnique that balances parameter efficiency, adaptability, and robustness\nwithout compromising performance. By strategically sharing the low-rank weight\nmatrices across different layers, ShareLoRA achieves 44\\% to 96\\% reduction in\ntrainable parameters compared to standard LoRA, alongside a substantial\ndecrease in memory overhead. This efficiency gain scales with model size,\nmaking ShareLoRA particularly advantageous for resource-constrained\nenvironments. Importantly, ShareLoRA not only maintains model performance but\nalso exhibits robustness in both classification and generation tasks across\ndiverse models, including RoBERTa, GPT-2, and LLaMA series (1, 2, and 3). It\nconsistently outperforms LoRA in zero-shot, few-shot, and continual fine-tuning\nscenarios, achieving up to 1.2\\% average accuracy improvement, and enhanced\ngeneralization across domains. In continual learning settings, ShareLoRA\nachieves 1.2\\% higher accuracy on GSM8K, 0.6\\% on HumanEval, and 0.5\\% on both\nMMLU and MMLU-Pro. Our results demonstrate that ShareLoRA supports high-quality\nfine-tuning while offering strong generalization and continual adaptation\nacross various model scales and diverse tasks."}
{"id": "2406.16330", "pdf": "https://arxiv.org/pdf/2406.16330.pdf", "abs": "https://arxiv.org/abs/2406.16330", "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "authors": ["Deyuan Liu", "Zhanyue Qin", "Hairu Wang", "Zhao Yang", "Zecheng Wang", "Fangying Rong", "Qingbin Liu", "Yanchao Hao", "Xi Chen", "Cunhang Fan", "Zhao Lv", "Zhiying Tu", "Dianhui Chu", "Bo Li", "Dianbo Sui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) excel in many domains, their complexity\nand scale challenge deployment in resource-limited environments. Current\ncompression techniques, such as parameter pruning, often fail to effectively\nutilize the knowledge from pruned parameters. To address these challenges, we\npropose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA),\na novel approach that uses manifold learning and the Normalized Pairwise\nInformation Bottleneck (NPIB) measure to merge similar layers, reducing model\nsize while preserving essential performance. We evaluate MKA on multiple\nbenchmark datasets and various LLMs. Our findings show that MKA not only\npreserves model performance but also achieves substantial compression ratios,\noutperforming traditional pruning methods. Moreover, when coupled with\nquantization, MKA delivers even greater compression. Specifically, on the MMLU\ndataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75%\nwith a minimal performance decrease of only 2.82\\%. The proposed MKA method\noffers a resource-efficient and performance-preserving model compression\ntechnique for LLMs."}
{"id": "2406.17513", "pdf": "https://arxiv.org/pdf/2406.17513.pdf", "abs": "https://arxiv.org/abs/2406.17513", "title": "Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Lei Shi", "Andreas Bulling"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2024 Workshop on Mechanistic Interpretability version:\n  https://openreview.net/forum?id=yEwEVoH9Be", "summary": "Despite growing interest in Theory of Mind (ToM) tasks for evaluating\nlanguage models (LMs), little is known about how LMs internally represent\nmental states of self and others. Understanding these internal mechanisms is\ncritical - not only to move beyond surface-level performance, but also for\nmodel alignment and safety, where subtle misattributions of mental states may\ngo undetected in generated outputs. In this work, we present the first\nsystematic investigation of belief representations in LMs by probing models\nacross different scales, training regimens, and prompts - using control tasks\nto rule out confounds. Our experiments provide evidence that both model size\nand fine-tuning substantially improve LMs' internal representations of others'\nbeliefs, which are structured - not mere by-products of spurious correlations -\nyet brittle to prompt variations. Crucially, we show that these representations\ncan be strengthened: targeted edits to model activations can correct wrong ToM\ninferences."}
{"id": "2407.00248", "pdf": "https://arxiv.org/pdf/2407.00248.pdf", "abs": "https://arxiv.org/abs/2407.00248", "title": "DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising", "authors": ["Zhenhao Li", "Huichi Zhou", "Marek Rei", "Lucia Specia"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Pretrained language models have significantly advanced performance across\nvarious natural language processing tasks. However, adversarial attacks\ncontinue to pose a critical challenge to systems built using these models, as\nthey can be exploited with carefully crafted adversarial texts. Inspired by the\nability of diffusion models to predict and reduce noise in computer vision, we\npropose a novel and flexible adversarial defense method for language\nclassification tasks, DiffuseDef, which incorporates a diffusion layer as a\ndenoiser between the encoder and the classifier. The diffusion layer is trained\non top of the existing classifier, ensuring seamless integration with any model\nin a plug-and-play manner. During inference, the adversarial hidden state is\nfirst combined with sampled noise, then denoised iteratively and finally\nensembled to produce a robust text representation. By integrating adversarial\ntraining, denoising, and ensembling techniques, we show that DiffuseDef\nimproves over existing adversarial defense methods and achieves\nstate-of-the-art performance against common black-box and white-box adversarial\nattacks."}
{"id": "2407.01976", "pdf": "https://arxiv.org/pdf/2407.01976.pdf", "abs": "https://arxiv.org/abs/2407.01976", "title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding", "authors": ["Jinghui Lu", "Haiyang Yu", "Yanjie Wang", "Yongjie Ye", "Jingqun Tang", "Ziwei Yang", "Binghong Wu", "Qi Liu", "Hao Feng", "Han Wang", "Hao Liu", "Can Huang"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "Accept to ACL2025 Findings", "summary": "Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. LayTextLLM\nprojects each bounding box to a single embedding and interleaves it with text,\nefficiently avoiding long sequence issues while leveraging autoregressive\ntraits of LLMs. LayTextLLM not only streamlines the interaction of layout and\ntextual data but also shows enhanced performance in KIE and VQA. Comprehensive\nbenchmark evaluations reveal significant improvements of LayTextLLM, with a\n15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA\nOCR-based LLMs. All resources are available at\nhttps://github.com/LayTextLLM/LayTextLLM."}
{"id": "2407.18525", "pdf": "https://arxiv.org/pdf/2407.18525.pdf", "abs": "https://arxiv.org/abs/2407.18525", "title": "ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks", "authors": ["Yinghao Zhu", "Junyi Gao", "Zixiang Wang", "Weibin Liao", "Xiaochen Zheng", "Lifang Liang", "Miguel O. Bernabeu", "Yasha Wang", "Lequan Yu", "Chengwei Pan", "Ewen M. Harrison", "Liantao Ma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in medicine. However,\ntheir utility in non-generative clinical prediction, often presumed inferior to\nspecialized models, remains under-evaluated, leading to ongoing debate within\nthe field and potential for misuse, misunderstanding, or over-reliance due to a\nlack of systematic benchmarking. Our ClinicRealm study addresses this by\nbenchmarking 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods\non unstructured clinical notes and structured Electronic Health Records (EHR).\nKey findings reveal a significant shift: for clinical note predictions, leading\nLLMs (e.g., DeepSeek R1/V3, GPT o3-mini-high) in zero-shot settings now\ndecisively outperform finetuned BERT models. On structured EHRs, while\nspecialized models excel with ample data, advanced LLMs (e.g., GPT-4o, DeepSeek\nR1/V3) show potent zero-shot capabilities, often surpassing conventional models\nin data-scarce settings. Notably, leading open-source LLMs can match or exceed\nproprietary counterparts. These results establish modern LLMs as powerful\nnon-generative clinical prediction tools, particularly with unstructured text\nand offering data-efficient structured data options, thus necessitating a\nre-evaluation of model selection strategies. This research should serve as an\nimportant insight for medical informaticists, AI developers, and clinical\nresearchers, potentially prompting a reassessment of current assumptions and\ninspiring new approaches to LLM application in predictive healthcare."}
{"id": "2408.05517", "pdf": "https://arxiv.org/pdf/2408.05517.pdf", "abs": "https://arxiv.org/abs/2408.05517", "title": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning", "authors": ["Yuze Zhao", "Jintao Huang", "Jinghan Hu", "Xingjun Wang", "Yunlin Mao", "Daoze Zhang", "Hong Zhang", "Zeyinzi Jiang", "Zhikai Wu", "Baole Ai", "Ang Wang", "Wenmeng Zhou", "Yingda Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the most\ncomprehensive support for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%."}
{"id": "2408.08696", "pdf": "https://arxiv.org/pdf/2408.08696.pdf", "abs": "https://arxiv.org/abs/2408.08696", "title": "Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling", "authors": ["Xianzhen Luo", "Yixuan Wang", "Qingfu Zhu", "Zhiming Zhang", "Xuanyu Zhang", "Qing Yang", "Dongliang Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL2025. Code is\n  [here](https://github.com/Luowaterbi/TokenRecycling). Token Recycling has\n  already merged into [SpecBench](https://github.com/hemingkx/Spec-Bench)", "summary": "Massive parameters of LLMs have made inference latency a fundamental\nbottleneck. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm. Some methods rely on additional\narchitectures to guess draft tokens, which need extra training before use.\nAlternatively, retrieval-based training-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nIt stores candidate tokens in an adjacency matrix and employs a\nbreadth-first-search (BFS)-like algorithm to construct a draft tree, which is\nthen validated through tree attention. New candidate tokens from the decoding\nprocess are then used to update the matrix. Token Recycling requires\n\\textless2MB of additional storage and achieves approximately 2x speedup across\nall sizes of LLMs. It significantly outperforms existing train-free methods by\n30\\% and even a widely recognized training method by 25\\%."}
{"id": "2408.08780", "pdf": "https://arxiv.org/pdf/2408.08780.pdf", "abs": "https://arxiv.org/abs/2408.08780", "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions", "authors": ["Chenming Tang", "Zhixiang Wang", "Hao Sun", "Yunfang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL performance. But to our surprise, LLMs might not care what the\ndescriptions actually say, and the performance gain is primarily caused by the\nensemble format, since it could lead to improvement even with random\ndescriptive nouns. We further apply this new ensemble framework on a range of\ncommonsense, math, logical reasoning and hallucination tasks with three LLMs\nand achieve promising results, suggesting again that designing a proper prompt\nformat would be much more effective and efficient than paying effort into\nspecific descriptions. Our code will be publicly available once this paper is\npublished."}
{"id": "2408.12249", "pdf": "https://arxiv.org/pdf/2408.12249.pdf", "abs": "https://arxiv.org/abs/2408.12249", "title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction", "authors": ["Aishik Nagar", "Viktor Schlegel", "Thanh-Tung Nguyen", "Hao Li", "Yuping Wu", "Kuluhan Binici", "Stefan Winkler"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages", "summary": "Large Language Models (LLMs) are increasingly adopted for applications in\nhealthcare, reaching the performance of domain experts on tasks such as\nquestion answering and document summarisation. Despite their success on these\ntasks, it is unclear how well LLMs perform on tasks that are traditionally\npursued in the biomedical domain, such as structured information extraction. To\nbridge this gap, in this paper, we systematically benchmark LLM performance in\nMedical Classification and Named Entity Recognition (NER) tasks. We aim to\ndisentangle the contribution of different factors to the performance,\nparticularly the impact of LLMs' task knowledge and reasoning capabilities,\ntheir (parametric) domain knowledge, and addition of external knowledge. To\nthis end, we evaluate various open LLMs - including BioMistral and Llama-2\nmodels - on a diverse set of biomedical datasets, using standard prompting,\nChain of-Thought (CoT) and Self Consistency based reasoning as well as\nRetrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter\nintuitively, our results reveal that standard prompting consistently\noutperforms more complex techniques across both tasks, laying bare the\nlimitations in the current application of CoT, self-consistency and RAG in the\nbiomedical domain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are\nnot easily portable to biomedical tasks where precise structured outputs are\nrequired. This highlights the need for more effective integration of external\nknowledge and reasoning mechanisms in LLMs to enhance their performance in\nreal-world biomedical applications."}
{"id": "2409.01893", "pdf": "https://arxiv.org/pdf/2409.01893.pdf", "abs": "https://arxiv.org/abs/2409.01893", "title": "What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices", "authors": ["Zhi Chen", "Qiguang Chen", "Libo Qin", "Qipeng Guo", "Haijun Lv", "Yicheng Zou", "Wanxiang Che", "Hang Yan", "Kai Chen", "Dahua Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Camera Ready. Code is available at:\n  https://github.com/WowCZ/LongMIT", "summary": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT."}
{"id": "2409.07170", "pdf": "https://arxiv.org/pdf/2409.07170.pdf", "abs": "https://arxiv.org/abs/2409.07170", "title": "Learning Efficient Recursive Numeral Systems via Reinforcement Learning", "authors": ["Andrea Silvi", "Jonathan Thomas", "Emil Carlsson", "Devdatt Dubhashi", "Moa Johansson"], "categories": ["cs.CL"], "comment": "Accepted to CogSci 2025", "summary": "It has previously been shown that by using reinforcement learning (RL),\nagents can derive simple approximate and exact-restricted numeral systems that\nare similar to human ones (Carlsson, 2021). However, it is a major challenge to\nshow how more complex recursive numeral systems, similar to for example\nEnglish, could arise via a simple learning mechanism such as RL. Here, we\nintroduce an approach towards deriving a mechanistic explanation of the\nemergence of efficient recursive number systems. We consider pairs of agents\nlearning how to communicate about numerical quantities through a meta-grammar\nthat can be gradually modified throughout the interactions. Utilising a\nslightly modified version of the meta-grammar of Hurford (1975), we demonstrate\nthat our RL agents, shaped by the pressures for efficient communication, can\neffectively modify their lexicon towards Pareto-optimal configurations which\nare comparable to those observed within human numeral systems in terms of their\nefficiency."}
{"id": "2409.11055", "pdf": "https://arxiv.org/pdf/2409.11055.pdf", "abs": "https://arxiv.org/abs/2409.11055", "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure", "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in Coding and STEM tasks, though it occasionally reports\nimprovements in reasoning."}
{"id": "2409.20120", "pdf": "https://arxiv.org/pdf/2409.20120.pdf", "abs": "https://arxiv.org/abs/2409.20120", "title": "PACE: Abstractions for Communicating Efficiently", "authors": ["Jonathan D. Thomas", "Andrea Silvi", "Devdatt Dubhashi", "Moa Johansson"], "categories": ["cs.CL"], "comment": "Accepted to CogSci 2025 for presentation", "summary": "A central but unresolved aspect of problem-solving in AI is the capability to\nintroduce and use abstractions, something humans excel at. Work in cognitive\nscience has demonstrated that humans tend towards higher levels of abstraction\nwhen engaged in collaborative task-oriented communication, enabling gradually\nshorter and more information-efficient utterances. Several computational\nmethods have attempted to replicate this phenomenon, but all make unrealistic\nsimplifying assumptions about how abstractions are introduced and learned. Our\nmethod, Procedural Abstractions for Communicating Efficiently (PACE), overcomes\nthese limitations through a neuro-symbolic approach. On the symbolic side, we\ndraw on work from library learning for proposing abstractions. We combine this\nwith neural methods for communication and reinforcement learning, via a novel\nuse of bandit algorithms for controlling the exploration and exploitation\ntrade-off in introducing new abstractions. PACE exhibits similar tendencies to\nhumans on a collaborative construction task from the cognitive science\nliterature, where one agent (the architect) instructs the other (the builder)\nto reconstruct a scene of block-buildings. PACE results in the emergence of an\nefficient language as a by-product of collaborative communication. Beyond\nproviding mechanistic insights into human communication, our work serves as a\nfirst step to providing conversational agents with the ability for human-like\ncommunicative abstractions."}
{"id": "2410.00168", "pdf": "https://arxiv.org/pdf/2410.00168.pdf", "abs": "https://arxiv.org/abs/2410.00168", "title": "SSR: Alignment-Aware Modality Connector for Speech Language Models", "authors": ["Weiting Tan", "Hirofumi Inaguma", "Ning Dong", "Paden Tomasello", "Xutai Ma"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Fusing speech into pre-trained language model (SpeechLM) usually suffers from\ninefficient encoding of long-form speech and catastrophic forgetting of\npre-trained text modality. We propose SSR-Connector (Segmented Speech\nRepresentation Connector) for better modality fusion. Leveraging speech-text\nalignments, our approach segments and compresses speech features to match the\ngranularity of text embeddings. Additionally, we introduce a two-stage training\npipeline that includes the distillation and fine-tuning phases to mitigate\ncatastrophic forgetting. SSR-Connector outperforms existing mechanism for\nspeech-text modality fusion, consistently achieving better speech understanding\n(e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving\npre-trained text ability."}
{"id": "2410.02707", "pdf": "https://arxiv.org/pdf/2410.02707.pdf", "abs": "https://arxiv.org/abs/2410.02707", "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations", "authors": ["Hadas Orgad", "Michael Toker", "Zorik Gekhman", "Roi Reichart", "Idan Szpektor", "Hadas Kotek", "Yonatan Belinkov"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."}
{"id": "2410.06577", "pdf": "https://arxiv.org/pdf/2410.06577.pdf", "abs": "https://arxiv.org/abs/2410.06577", "title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions", "authors": ["Zhihao He", "Hang Yu", "Zi Gong", "Shizhan Liu", "Jianguo Li", "Weiyao Lin"], "categories": ["cs.CL"], "comment": "Accepted by ICLR 2025. Camera-ready Version", "summary": "Recent advancements in Transformer-based large language models (LLMs) have\nset new standards in natural language processing. However, the classical\nsoftmax attention incurs significant computational costs, leading to a $O(T)$\ncomplexity for per-token generation, where $T$ represents the context length.\nThis work explores reducing LLMs' complexity while maintaining performance by\nintroducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an\ninnovative data-dependent tempered selection (DDTS) mechanism within a linear\nattention-based, purely recurrent framework, achieving significant accuracy\nwhile drastically reducing the memory usage typically associated with recurrent\nmodels. This method exemplifies semantic compression by maintaining essential\ninput information with fixed-size hidden states. Building on this, Rodimus$+$\ncombines Rodimus with the innovative Sliding Window Shared-Key Attention\n(SW-SKA) in a hybrid approach, effectively leveraging the complementary\nsemantic, token, and head compression techniques. Our experiments demonstrate\nthat Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior\ndownstream performance against models trained on more tokens, including\nQwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the\naccuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are\nopen-sourced at https://github.com/codefuse-ai/rodimus."}
{"id": "2410.07076", "pdf": "https://arxiv.org/pdf/2410.07076.pdf", "abs": "https://arxiv.org/abs/2410.07076", "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Tong Xie", "Yuqiang Li", "Wanli Ouyang", "Soujanya Poria", "Erik Cambria", "Dongzhan Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Scientific discovery plays a pivotal role in advancing human society, and\nrecent progress in large language models (LLMs) suggests their potential to\naccelerate this process. However, it remains unclear whether LLMs can\nautonomously generate novel and valid hypotheses in chemistry. In this work, we\ninvestigate whether LLMs can discover high-quality chemistry hypotheses given\nonly a research background-comprising a question and/or a survey-without\nrestriction on the domain of the question. We begin with the observation that\nhypothesis discovery is a seemingly intractable task. To address this, we\npropose a formal mathematical decomposition grounded in a fundamental\nassumption: that most chemistry hypotheses can be composed from a research\nbackground and a set of inspirations. This decomposition leads to three\npractical subtasks-retrieving inspirations, composing hypotheses with\ninspirations, and ranking hypotheses - which together constitute a sufficient\nset of subtasks for the overall scientific discovery task. We further develop\nan agentic LLM framework, MOOSE-Chem, that is a direct implementation of this\nmathematical decomposition. To evaluate this framework, we construct a\nbenchmark of 51 high-impact chemistry papers published and online after January\n2024, each manually annotated by PhD chemists with background, inspirations,\nand hypothesis. The framework is able to rediscover many hypotheses with high\nsimilarity to the groundtruth, successfully capturing the core\ninnovations-while ensuring no data contamination since it uses an LLM with\nknowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high\naccuracy on inspiration retrieval, a task with inherently out-of-distribution\nnature, we propose a bold assumption: that LLMs may already encode latent\nscientific knowledge associations not yet recognized by humans."}
{"id": "2411.01533", "pdf": "https://arxiv.org/pdf/2411.01533.pdf", "abs": "https://arxiv.org/abs/2411.01533", "title": "Enhancing LLM Evaluations: The Garbling Trick", "authors": ["William F. Bradley"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures", "summary": "As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models. We propose a general method to transform existing LLM\nevaluations into a series of progressively more difficult tasks. These enhanced\nevaluations emphasize reasoning capabilities and can reveal relative\nperformance differences that are not apparent in the original assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative abilities\nof these models, particularly highlighting the differences between base LLMs\nand more recent \"reasoning\" models."}
{"id": "2411.11266", "pdf": "https://arxiv.org/pdf/2411.11266.pdf", "abs": "https://arxiv.org/abs/2411.11266", "title": "VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs", "authors": ["Keer Lu", "Keshi Zhao", "Zhuoran Zhang", "Zheng Liang", "Da Pan", "Shusen Zhang", "Xin Wu", "Guosheng Dong", "Bin Cui", "Tengjiao Wang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT\nand Claude series, LLMs have the potential to achieve remarkable proficiency\nacross a wide range of domains, including law, medicine, finance, science,\ncode, etc., all within a single model. These capabilities are further augmented\nduring the Supervised Fine-Tuning (SFT) phase. Despite their potential,\nexisting work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce **VersaTune**, a\nnovel data composition framework designed for enhancing LLMs' overall\nmulti-domain capabilities during training. We begin with detecting the\ndistribution of domain-specific knowledge within the base model, followed by\nthe training data composition that aligns with the model's existing knowledge\ndistribution. During the subsequent training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results indicate that VersaTune is effective in multi-domain\nfostering, with an improvement of 35.21\\% in the overall multi-ability\nperformances compared to uniform domain weights. Furthermore, we find that\nQwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,\nClaude3.5-Sonnet and DeepSeek-V3 by 0.86\\%, 4.76\\% and 4.60\\%. Additionally, in\nscenarios where flexible expansion of a specific domain is required, VersaTune\nreduces the performance degradation in other domains by 38.77\\%, while\npreserving the training efficacy of the target domain."}
{"id": "2411.16707", "pdf": "https://arxiv.org/pdf/2411.16707.pdf", "abs": "https://arxiv.org/abs/2411.16707", "title": "Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework", "authors": ["Mengshuo Jia", "Zeyu Cui", "Gabriela Hug"], "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": "16 pages", "summary": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond."}
{"id": "2411.19477", "pdf": "https://arxiv.org/pdf/2411.19477.pdf", "abs": "https://arxiv.org/abs/2411.19477", "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models", "authors": ["Yanxi Chen", "Xuchen Pan", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms."}
{"id": "2412.02466", "pdf": "https://arxiv.org/pdf/2412.02466.pdf", "abs": "https://arxiv.org/abs/2412.02466", "title": "Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths", "authors": ["Mohammed Q. Shormani"], "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "comment": "18 pages, 3 figures", "summary": "This study sets out to answer one major question: Can ChatGPT capture\nswearing nuances? It presents an empirical study on the ability of ChatGPT to\ntranslate Arabic oath expressions into English. 30 Arabic oath expressions were\ncollected from the literature. These 30 oaths were first translated via ChatGPT\nand then analyzed and compared to the human translation in terms of types of\ngaps left unfulfilled by ChatGPT. Specifically, the gaps involved are:\nreligious gap, cultural gap, both religious and cultural gaps, no gap, using\nnon-oath particles, redundancy and noncapturing of Arabic script diacritics. It\nconcludes that ChatGPT translation of oaths is still much unsatisfactory,\nunveiling the need of further developments of ChatGPT, and the inclusion of\nArabic data on which ChatGPT should be trained including oath expressions, oath\nnuances, rituals, and practices."}
{"id": "2412.11500", "pdf": "https://arxiv.org/pdf/2412.11500.pdf", "abs": "https://arxiv.org/abs/2412.11500", "title": "Intention Knowledge Graph Construction for User Intention Relation Modeling", "authors": ["Jiaxin Bai", "Zhaobo Wang", "Junfei Cheng", "Dan Yu", "Zerui Huang", "Weiqi Wang", "Xin Liu", "Chen Luo", "Yanming Zhu", "Bo Li", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding user intentions is challenging for online platforms. Recent\nwork on intention knowledge graphs addresses this but often lacks focus on\nconnecting intentions, which is crucial for modeling user behavior and\npredicting future actions. This paper introduces a framework to automatically\ngenerate an intention knowledge graph, capturing connections between user\nintentions. Using the Amazon m2 dataset, we construct an intention graph with\n351 million edges, demonstrating high plausibility and acceptance. Our model\neffectively predicts new session intentions and enhances product\nrecommendations, outperforming previous state-of-the-art methods and showcasing\nthe approach's practical utility."}
{"id": "2412.13377", "pdf": "https://arxiv.org/pdf/2412.13377.pdf", "abs": "https://arxiv.org/abs/2412.13377", "title": "DateLogicQA: Benchmarking Temporal Biases in Large Language Models", "authors": ["Gagan Bhatia", "MingZe Tang", "Cristina Mahanta", "Madiha Kazi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately."}
{"id": "2412.14872", "pdf": "https://arxiv.org/pdf/2412.14872.pdf", "abs": "https://arxiv.org/abs/2412.14872", "title": "Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set", "authors": ["Lecheng Wang", "Xianjie Shi", "Ge Li", "Jia Li", "Xuanming Zhang", "Yihong Dong", "Wenpin Jiao", "Hong Mei"], "categories": ["cs.CL"], "comment": "20 pages, 3 figures", "summary": "Auto-regressive language models (LMs) have been widely used to generate data\nin data-scarce domains to train new LMs, compensating for the scarcity of\nreal-world data. Previous work experimentally found that LMs collapse when\ntrained on recursively generated data. This paper presents a theoretical proof:\nonce a corpus (such as a subset of the World Wide Web) begins to incorporate\ngenerated data and no new real-world data is added to the corpus, then no\nmatter how small the amount of data each LM generates and contributes to the\ncorpus, LM collapse is inevitable after sufficient time. This finding suggests\nthat attempts to mitigate collapse by limiting the quantity of synthetic data\nin the corpus are fundamentally insufficient. Instead, avoiding collapse hinges\non ensuring the quality of synthetic data."}
{"id": "2412.17061", "pdf": "https://arxiv.org/pdf/2412.17061.pdf", "abs": "https://arxiv.org/abs/2412.17061", "title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration", "authors": ["Hai Ye", "Mingbao Lin", "Hwee Tou Ng", "Shuicheng Yan"], "categories": ["cs.CL"], "comment": "In submission", "summary": "Scaling laws for inference compute in multi-agent systems remain\nunder-explored compared to single-agent scenarios. This work aims to bridge\nthis gap by investigating the problem of data synthesis through multi-agent\nsampling, where synthetic responses are generated by sampling from multiple\ndistinct language models. Effective model coordination is crucial for\nsuccessful multi-agent collaboration. Unlike previous approaches that rely on\nfixed workflows, we treat model coordination as a multi-step decision-making\nprocess, optimizing generation structures dynamically for each input question.\nWe introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow\nevolves iteratively during the sequential sampling process. To achieve this, we\nleverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide\nreal-time feedback and accelerate exploration. Our experiments on alignment,\nmachine translation, and mathematical reasoning demonstrate that multi-agent\nsampling significantly outperforms single-agent sampling as inference compute\nscales. TOA is the most compute-efficient approach, achieving SOTA performance\non WMT and a 72.2\\% LC win rate on AlpacaEval. Moreover, fine-tuning with our\nsynthesized alignment data surpasses strong preference learning methods on\nchallenging benchmarks such as Arena-Hard and AlpacaEval."}
{"id": "2501.02506", "pdf": "https://arxiv.org/pdf/2501.02506.pdf", "abs": "https://arxiv.org/abs/2501.02506", "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use", "authors": ["Junjie Ye", "Zhengyin Du", "Xuesong Yao", "Weijian Lin", "Yufei Xu", "Zehui Chen", "Zaiyuan Wang", "Sining Zhu", "Zhiheng Xi", "Siyu Yuan", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Jiecao Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."}
{"id": "2501.11269", "pdf": "https://arxiv.org/pdf/2501.11269.pdf", "abs": "https://arxiv.org/abs/2501.11269", "title": "Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios", "authors": ["Zhongtian Hu", "Yiwen Cui", "Ronghan Li", "Meng Zhao", "Lifang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Current multilingual large language models(MLLMs) still focus on simple\nquestion-answering formats, often overlooking more complex dialogue scenarios.\nIn other words, their capabilities of multilingual large models have yet to be\nvalidated in dialogue tasks with intricate structures. We therefore ask, Q1:\nHow well do LLMs generalize to more complex dialog scenarios? Q2: Can\nsupervised fine-tuning on a high-quality parallel benchmark restore this\nability? Q3: Does the \"multilingual complementarity\" effect survive in the\nsetting? To answer these questions, we introduce XMP, a high-quality parallel\nMultilingual dataset sourced from Multi-party Podcast dialogues, which is the\nfirst parallel dataset focusing on multi-party dialogue scenarios. Most samples\nin the dataset feature three or more participants, discussing a wide range of\ntopics. Through extensive experiments, we find that, R1: MLLMs fail to\ngeneralize to multi-party setting, R2 Fine-tuning on XMP improves only\nmarginally, with the 70B model achieving at most a 1% absolute gain over its 8B\ncounterpart; R3: Mixing languages during SFT is usually detrimental, with any\nbenefits being marginal and limited to isolated cases in the 70B model."}
{"id": "2501.11292", "pdf": "https://arxiv.org/pdf/2501.11292.pdf", "abs": "https://arxiv.org/abs/2501.11292", "title": "Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning", "authors": ["Zhongtian Hu", "Qi He", "Ronghan Li", "Meng Zhao", "Lifang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-party dialogues, common in collaborative scenarios like brainstorming\nsessions and negotiations, pose significant challenges due to their complexity\nand diverse speaker roles. Current methods often use graph neural networks to\nmodel dialogue context, capturing structural dynamics but heavily relying on\nannotated graph structures and overlooking individual speaking styles. To\naddress these challenges, we propose CMR, a Contrastive learning-based\nMulti-party dialogue Response generation framework. CMR employs a two-stage\nself-supervised contrastive learning framework. First, it captures global\ndifferences in speaking styles across individuals. Then, it focuses on\nintra-conversation comparisons to identify thematic transitions and\ncontextually relevant facts. To the best of our knowledge, this is the first\napproach that applies contrastive learning in multi-party dialogue generation.\nExperimental results demonstrate that CMR not only significantly outperforms\nstate-of-the-art models, but also generalizes well to large pre-trained\nlanguage models, effectively enhancing their capability in handling multi-party\nconversations."}
{"id": "2501.11496", "pdf": "https://arxiv.org/pdf/2501.11496.pdf", "abs": "https://arxiv.org/abs/2501.11496", "title": "Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges", "authors": ["Vincent Koc"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 91F20", "I.2.7; I.2.6; J.5"], "comment": "9 pages, 3 figures, 2 tables, submitted for IEEE publication.\n  Pre-print updated as part of review process", "summary": "The global crisis of language endangerment meets a technological turning\npoint as Generative AI (GenAI) and Large Language Models (LLMs) unlock new\nfrontiers in automating corpus creation, transcription, translation, and\ntutoring. However, this promise is imperiled by fragmented practices and the\ncritical lack of a methodology to navigate the fraught balance between LLM\ncapabilities and the profound risks of data scarcity, cultural\nmisappropriation, and ethical missteps. This paper introduces a novel\nanalytical framework that systematically evaluates GenAI applications against\nlanguage-specific needs, embedding community governance and ethical safeguards\nas foundational pillars. We demonstrate its efficacy through the Te Reo M\\=aori\nrevitalization, where it illuminates successes, such as community-led Automatic\nSpeech Recognition achieving 92% accuracy, while critically surfacing\npersistent challenges in data sovereignty and model bias for digital archives\nand educational tools. Our findings underscore that GenAI can indeed\nrevolutionize language preservation, but only when interventions are rigorously\nanchored in community-centric data stewardship, continuous evaluation, and\ntransparent risk management. Ultimately, this framework provides an\nindispensable toolkit for researchers, language communities, and policymakers,\naiming to catalyze the ethical and high-impact deployment of LLMs to safeguard\nthe world's linguistic heritage."}
{"id": "2501.12162", "pdf": "https://arxiv.org/pdf/2501.12162.pdf", "abs": "https://arxiv.org/abs/2501.12162", "title": "AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding", "authors": ["Zikun Li", "Zhuofu Chen", "Remi Delacourt", "Gabriele Oliaro", "Zeyu Wang", "Qinghan Chen", "Shuhuai Lin", "April Yang", "Zhihao Zhang", "Zhuoming Chen", "Sean Lai", "Xinhao Cheng", "Xupeng Miao", "Zhihao Jia"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Modern large language model (LLM) applications exhibit diverse service-level\nobjectives (SLOs), from low-latency requirements in interactive coding\nassistants to more relaxed constraints in data wrangling tasks. Existing LLM\nserving systems, which rely on uniform batching and scheduling strategies,\noften fail to meet these heterogeneous SLOs concurrently. We present AdaServe,\nthe first LLM serving system designed to support efficient multi-SLO serving\nthrough SLO-customized speculative decoding. AdaServe formulates multi-SLO\nserving as a constrained optimization problem and introduces a hardware-aware\nalgorithm that constructs a speculation tree tailored to each request's latency\ntarget. It features a speculate-select-verify pipeline that enables\nfine-grained control over decoding speed while maximizing system throughput.\nAdaServe further adapts to workload variation by dynamically adjusting\nspeculation parameters. Evaluations across diverse workloads show that AdaServe\nreduces SLO violations by up to 4.3$\\times$ and improves goodput by up to\n1.9$\\times$ compared to the best performing baselines, highlighting its\neffectiveness in multi-SLO serving."}
{"id": "2501.15175", "pdf": "https://arxiv.org/pdf/2501.15175.pdf", "abs": "https://arxiv.org/abs/2501.15175", "title": "Option-ID Based Elimination For Multiple Choice Questions", "authors": ["Zhenhao Zhu", "Bulou Liu", "Qingyao Ai", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies."}
{"id": "2501.17047", "pdf": "https://arxiv.org/pdf/2501.17047.pdf", "abs": "https://arxiv.org/abs/2501.17047", "title": "How Linguistics Learned to Stop Worrying and Love the Language Models", "authors": ["Richard Futrell", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Language models can produce fluent, grammatical text. Nonetheless, some\nmaintain that language models don't really learn language and also that, even\nif they did, that would not be informative for the study of human learning and\nprocessing. On the other side, there have been claims that the success of LMs\nobviates the need for studying linguistic theory and structure. We argue that\nboth extremes are wrong. LMs can contribute to fundamental questions about\nlinguistic structure, language processing, and learning. They force us to\nrethink arguments and ways of thinking that have been foundational in\nlinguistics. While they do not replace linguistic structure and theory, they\nserve as model systems and working proofs of concept for gradient, usage-based\napproaches to language. We offer an optimistic take on the relationship between\nlanguage models and linguistics."}
{"id": "2501.18280", "pdf": "https://arxiv.org/pdf/2501.18280.pdf", "abs": "https://arxiv.org/abs/2501.18280", "title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models", "authors": ["Haoyu Liang", "Youran Sun", "Yunfeng Cai", "Jun Zhu", "Bo Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "The security issue of large language models (LLMs) has gained wide attention\nrecently, with various defense mechanisms developed to prevent harmful output,\namong which safeguards based on text embedding models serve as a fundamental\ndefense. Through testing, we discover that the output distribution of text\nembedding models is severely biased with a large mean. Inspired by this\nobservation, we propose novel, efficient methods to search for **universal\nmagic words** that attack text embedding models. Universal magic words as\nsuffixes can shift the embedding of any text towards the bias direction, thus\nmanipulating the similarity of any text pair and misleading safeguards.\nAttackers can jailbreak the safeguards by appending magic words to user prompts\nand requiring LLMs to end answers with magic words. Experiments show that magic\nword attacks significantly degrade safeguard performance on JailbreakBench,\ncause real-world chatbots to produce harmful outputs in full-pipeline attacks,\nand generalize across input/output texts, models, and languages. To eradicate\nthis security risk, we also propose defense methods against such attacks, which\ncan correct the bias of text embeddings and improve downstream performance in a\ntrain-free manner."}
{"id": "2502.00791", "pdf": "https://arxiv.org/pdf/2502.00791.pdf", "abs": "https://arxiv.org/abs/2502.00791", "title": "Vision-centric Token Compression in Large Language Model", "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Xiangbo Shu", "Jinhui Tang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Real-world applications are stretching context windows to hundreds of\nthousand of tokens while Large Language Models (LLMs) swell from billions to\ntrillions of parameters. This dual expansion send compute and memory costs\nskyrocketing, making token compression indispensable. We introduce Vision\nCentric Token Compression (Vist), a slow-fast compression framework that\nmirrors human reading: the fast path renders distant tokens into images,\nletting a frozen, lightweight vision encoder skim the low-salience context; the\nslow path feeds the proximal window into the LLM for fine-grained reasoning. A\nProbability-Informed Visual Enhancement (PVE) objective masks high-frequency\ntokens during training, steering the Resampler to concentrate on semantically\nrich regions-just as skilled reader gloss over function words. On eleven\nin-context learning benchmarks, Vist achieves the same accuracy with 2.3 times\nfewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers\nremarkable results, outperforming the strongest text encoder-based compression\nmethod CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI,\nand CLIN, setting a new standard for token efficiency in LLMs. The source code\nwill be released."}
{"id": "2502.01179", "pdf": "https://arxiv.org/pdf/2502.01179.pdf", "abs": "https://arxiv.org/abs/2502.01179", "title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning", "authors": ["Wen Lai", "Alexander Fraser", "Ivan Titov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025. The code is released at\n  https://github.com/wenlai-lavine/jola", "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. These methods, due to their extremely small parameter counts,\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola."}
{"id": "2502.01637", "pdf": "https://arxiv.org/pdf/2502.01637.pdf", "abs": "https://arxiv.org/abs/2502.01637", "title": "Scaling Embedding Layers in Language Models", "authors": ["Da Yu", "Edith Cohen", "Badih Ghazi", "Yangsibo Huang", "Pritish Kamath", "Ravi Kumar", "Daogao Liu", "Chiyuan Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "Added downstream evaluation results and improved the overall clarity\n  and writing", "summary": "We propose SCONE ($S$calable, $C$ontextualized, $O$ffloaded, $N$-gram\n$E$mbedding), a new method for extending input embedding layers to enhance\nlanguage model performance. To avoid increased decoding costs, SCONE retains\nthe original vocabulary while introducing embeddings for a set of frequent\n$n$-grams. These embeddings provide contextualized representation for each\ninput token and are learned with a separate model during training. After\ntraining, embeddings are precomputed and stored in off-accelerator memory;\nduring inference, querying them has minimal impact on latency due to the low\ncomplexity of embedding lookups. SCONE enables two new scaling strategies:\nincreasing the number of $n$-gram embeddings and scaling the model used to\nlearn them, both while maintaining fixed accelerator usage during inference (in\nterms of FLOPS and memory). We show that scaling both aspects enables a model\nwith 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline\nacross diverse corpora, while using only about half the FLOPS and accelerator\nmemory during inference."}
{"id": "2502.02444", "pdf": "https://arxiv.org/pdf/2502.02444.pdf", "abs": "https://arxiv.org/abs/2502.02444", "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models", "authors": ["Haoran Ye", "Tianze Zhang", "Yuhang Xie", "Liyuan Zhang", "Yuanyi Ren", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2024 Main", "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."}
{"id": "2502.04235", "pdf": "https://arxiv.org/pdf/2502.04235.pdf", "abs": "https://arxiv.org/abs/2502.04235", "title": "Reformulation for Pretraining Data Augmentation", "authors": ["Xintong Hao", "Ruijie Zhu", "Ge Zhang", "Ke Shen", "Chenggang Li"], "categories": ["cs.CL"], "comment": "Dataset released\n  https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu", "summary": "Despite the impressive capabilities of large language models across various\ntasks, their continued scaling is severely hampered not only by data scarcity\nbut also by the performance degradation associated with excessive data\nrepetition during training. To overcome this critical bottleneck, we propose\nthe Massive Genre-Audience(MGA) reformulation method, a lightweight and\nscalable data augmentation technique inspired by synthetic data methodologies.\nMGA systematically reformulates existing corpora into diverse,\ncontextually-rich variations to mitigate the negative effects of repetition,\nand we introduce this approach along with the resulting 770 billion token\nMGACorpus in this work. We experimentally validate its core benefit by\ndemonstrating superior performance against data repetition and upsampling in\nscaling scenarios (up to 13B parameters). Furthermore, comprehensive analysis\ninvestigates the role of prompt engineering in generation quality and reveals\nnuances in evaluating model capabilities using standard loss metrics. Our work\nshows that MGA provides a reliable pathway to substantially augment training\ndatasets, effectively alleviating repetition bottlenecks and enabling more\nefficient scaling of large language models."}
{"id": "2502.05567", "pdf": "https://arxiv.org/pdf/2502.05567.pdf", "abs": "https://arxiv.org/abs/2502.05567", "title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data", "authors": ["Xiaoyang Liu", "Kangjie Bao", "Jiashuo Zhang", "Yunqi Liu", "Yuntian Liu", "Yu Chen", "Yang Jiao", "Tao Luo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoformalization, the automatic translation of mathematical content from\nnatural language into machine-verifiable formal languages, has seen significant\nprogress driven by advances in large language models (LLMs). Nonetheless, a\nprimary barrier to further improvements is the limited availability of parallel\ncorpora that map informal mathematical text to its formal counterpart. To\naddress this limitation, we propose ATLAS (Autoformalizing Theorems through\nLifting, Augmentation, and Synthesis of Data), a novel data generation\nframework designed to produce large-scale, high-quality parallel corpora of\ntheorem statements. Distinct from prior approaches, ATLAS begins with a concept\nrepository, accelerates the improvement of student model through expert\niteration combined with knowledge distillation, and introduces two novel\naugmentation strategies that exploit the structural characteristics of formal\nlanguages. With the proposed ATLAS running for 10 iterations, we construct an\nundergraduate-level dataset comprising 117k theorem statements and develop\nATLAS Translator, which demonstrates statistically significant improvements\nover both the HERALD Translator and the Kimina-Autoformalizer across all\nbenchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The\ndatasets, model, and code will be released to the public soon."}
{"id": "2502.05605", "pdf": "https://arxiv.org/pdf/2502.05605.pdf", "abs": "https://arxiv.org/abs/2502.05605", "title": "Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization", "authors": ["Yongcheng Zeng", "Xinyu Cui", "Xuanfa Jin", "Guoqing Liu", "Zexu Sun", "Dong Li", "Ning Yang", "Jianye Hao", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable general\nperformance, enabling smaller models to achieve capabilities comparable to\ntheir larger counterparts remains a critical challenge. For humans, iterative\nrefinement of problem analysis and responses is a common strategy to enhance\nanswer quality. However, we observe that existing LLMs exhibit limited ability\nto refine their outputs for quality improvement. In this paper, we first\ninvestigate mechanisms to unlock and progressively enhance self-refinement\nability in smaller models within an iterative preference optimization\nframework, aiming to bridge the performance gap with larger models. To this\nend, we propose EVOLVE, a novel post-training and inference framework that\niteratively integrates preference training with self-refinement-driven data\ncollection. During training, EVOLVE strengthens the model's direct\nquestion-answering ability while simultaneously unlocking its self-refinement\npotential. At inference, the framework leverages this capability to generate\nprogressively refined responses, which are filtered to construct datasets for\nsubsequent rounds of preference training. Experiments demonstrate EVOLVE's\nexceptional performance: when applied to Llama-3.1-8B base model and under the\nself-refinement setting, it surpasses state-of-the-art models including\nLlama-3.1-405B-Instruct and GPT-4o, achieving a 62.3% length-controlled win\nrate and 63.3% raw win rate on AlpacaEval 2, along with a 50.3% win rate on\nArena-Hard. Furthermore, EVOLVE consistently enhances performance on\nmathematical reasoning tasks like GSM8K and MATH."}
{"id": "2502.06207", "pdf": "https://arxiv.org/pdf/2502.06207.pdf", "abs": "https://arxiv.org/abs/2502.06207", "title": "Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement", "authors": ["Junyu Lu", "Kai Ma", "Kaichun Wang", "Kelaiti Xiao", "Roy Ka-Wei Lee", "Bo Xu", "Liang Yang", "Hongfei Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, accepted at the ACL 2025", "summary": "Large Language Models (LLMs) have become essential for offensive language\ndetection, yet their ability to handle annotation disagreement remains\nunderexplored. Disagreement samples, which arise from subjective\ninterpretations, pose a unique challenge due to their ambiguous nature.\nUnderstanding how LLMs process these cases, particularly their confidence\nlevels, can offer insight into their alignment with human annotators. This\nstudy systematically evaluates the performance of multiple LLMs in detecting\noffensive language at varying levels of annotation agreement. We analyze binary\nclassification accuracy, examine the relationship between model confidence and\nhuman disagreement, and explore how disagreement samples influence model\ndecision-making during few-shot learning and instruction fine-tuning. Our\nfindings reveal that LLMs struggle with low-agreement samples, often exhibiting\noverconfidence in these ambiguous cases. However, utilizing disagreement\nsamples in training improves both detection accuracy and model alignment with\nhuman judgment. These insights provide a foundation for enhancing LLM-based\noffensive language detection in real-world moderation tasks."}
{"id": "2502.06659", "pdf": "https://arxiv.org/pdf/2502.06659.pdf", "abs": "https://arxiv.org/abs/2502.06659", "title": "Who Taught You That? Tracing Teachers in Model Distillation", "authors": ["Somin Wadhwa", "Chantal Shaib", "Silvio Amir", "Byron C. Wallace"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers."}
{"id": "2502.09120", "pdf": "https://arxiv.org/pdf/2502.09120.pdf", "abs": "https://arxiv.org/abs/2502.09120", "title": "Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues", "authors": ["Ye-eun Cho", "Yunho Maeng"], "categories": ["cs.CL"], "comment": "11 pages, 4 figures, 7 tables", "summary": "This study investigates whether vision-language models (VLMs) can perform\npragmatic inference, focusing on ignorance implicatures, utterances that imply\nthe speaker's lack of precise knowledge. To test this, we systematically\nmanipulated contextual cues: the visually depicted situation (visual cue) and\nQUD-based linguistic prompts (linguistic cue). When only visual cues were\nprovided, three state-of-the-art VLMs (GPT-4o, Gemini 1.5 Pro, and Claude 3.5\nsonnet) produced interpretations largely based on the lexical meaning of the\nmodified numerals. When linguistic cues were added to enhance contextual\ninformativeness, Claude exhibited more human-like inference by integrating both\ntypes of contextual cues. In contrast, GPT and Gemini favored precise, literal\ninterpretations. Although the influence of contextual cues increased, they\ntreated each contextual cue independently and aligned them with semantic\nfeatures rather than engaging in context-driven reasoning. These findings\nsuggest that although the models differ in how they handle contextual cues,\nClaude's ability to combine multiple cues may signal emerging pragmatic\ncompetence in multimodal models."}
{"id": "2502.10996", "pdf": "https://arxiv.org/pdf/2502.10996.pdf", "abs": "https://arxiv.org/abs/2502.10996", "title": "RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation", "authors": ["Pengcheng Jiang", "Lang Cao", "Ruike Zhu", "Minhao Jiang", "Yunyi Zhang", "Jimeng Sun", "Jiawei Han"], "categories": ["cs.CL"], "comment": "under review", "summary": "Large language models (LLMs) have achieved impressive performance on\nknowledge-intensive tasks, yet they often struggle with multi-step reasoning\ndue to the unstructured nature of retrieved context. While retrieval-augmented\ngeneration (RAG) methods provide external information, the lack of explicit\norganization among retrieved passages limits their effectiveness, leading to\nbrittle reasoning pathways. Recent interpretability studies highlighting the\nimportance of structured intermediate reasoning further align with this\nperspective. We propose Retrieval-And-Structuring (RAS), a framework that\ndynamically constructs query-specific knowledge graphs through iterative\nretrieval and structured knowledge building. RAS interleaves targeted retrieval\nplanning with incremental graph construction, enabling models to assemble and\nreason over evolving knowledge structures tailored to each query. On seven\nknowledge-intensive benchmarks, RAS consistently outperforms strong baselines,\nachieving up to 6.4% and 7.0% gains with open-source and proprietary LLMs,\nrespectively. Our results demonstrate that dynamic, query-specific knowledge\nstructuring offers a robust path to improving reasoning accuracy and robustness\nin language model generation. Our data and code can be found at\nhttps://github.com/pat-jj/RAS."}
{"id": "2502.11114", "pdf": "https://arxiv.org/pdf/2502.11114.pdf", "abs": "https://arxiv.org/abs/2502.11114", "title": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation", "authors": ["Alon Eirew", "Kfir Bar", "Ido Dagan"], "categories": ["cs.CL"], "comment": null, "summary": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, where event pairs are classified in isolation, leading\nto computational inefficiency and a lack of global consistency in the resulting\ntemporal graph. In this work, we propose a novel zero-shot method for TRE that\ngenerates a document's complete temporal graph in a single step, followed by\ntemporal constraint optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\noutperforms existing zero-shot approaches and offers a competitive alternative\nto supervised TRE models."}
{"id": "2502.11150", "pdf": "https://arxiv.org/pdf/2502.11150.pdf", "abs": "https://arxiv.org/abs/2502.11150", "title": "Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures", "authors": ["Keren Gruteke Klein", "Shachar Frenkel", "Omer Shubi", "Yevgeni Berzak"], "categories": ["cs.CL"], "comment": null, "summary": "Automated text readability prediction is widely used in many real-world\nscenarios. Over the past century, such measures have primarily been developed\nand evaluated on reading comprehension outcomes and on human annotations of\ntext readability levels. In this work, we propose an alternative, eye\ntracking-based cognitive framework which directly taps into a key aspect of\nreadability: reading ease. We use this framework for evaluating a broad range\nof prominent readability measures, including two systems widely used in\neducation, by quantifying their ability to account for reading facilitation\neffects in text simplification, as well as text reading ease more broadly. Our\nanalyses suggest that existing readability measures are poor predictors of\nreading facilitation and reading ease, outperformed by word properties commonly\nused in psycholinguistics, and in particular by surprisal."}
{"id": "2502.11177", "pdf": "https://arxiv.org/pdf/2502.11177.pdf", "abs": "https://arxiv.org/abs/2502.11177", "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild", "authors": ["Wanli Yang", "Fei Sun", "Jiajun Tan", "Xinyu Ma", "Qi Cao", "Dawei Yin", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Despite near-perfect results in artificial evaluations, the effectiveness of\nmodel editing in real-world applications remains unexplored. To bridge this\ngap, we propose to study model editing in question answering (QA) by\nestablishing a rigorous evaluation practice to assess the effectiveness of\nediting methods in correcting LLMs' errors. It consists of QAEdit, a new\nbenchmark derived from popular QA datasets, and a standardized evaluation\nframework. Our single editing experiments indicate that current editing methods\nperform substantially worse than previously reported (38.5% vs. ~96%). Through\nmodule analysis and controlled experiments, we demonstrate that this\nperformance decline stems from issues in evaluation practices of prior editing\nresearch. One key issue is the inappropriate use of teacher forcing in testing\nprevents error propagation by feeding ground truth tokens (inaccessible in\nreal-world scenarios) as input. Furthermore, we simulate real-world deployment\nby sequential editing, revealing that current approaches fail drastically with\nonly 1000 edits. Our analysis provides a fundamental reexamination of both the\nreal-world applicability of existing model editing methods and their evaluation\npractices, and establishes a rigorous evaluation framework with key insights to\nadvance reliable and practical model editing research."}
{"id": "2502.11380", "pdf": "https://arxiv.org/pdf/2502.11380.pdf", "abs": "https://arxiv.org/abs/2502.11380", "title": "From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs", "authors": ["Zhu Liu", "Ying Liu", "KangYang Luo", "Cunliang Kong", "Maosong Sun"], "categories": ["cs.CL"], "comment": "Paper under review", "summary": "Lexico-semantic networks represent words as nodes and their semantic\nrelatedness as edges. While such networks are traditionally constructed using\nembeddings from encoder-based models or static vectors, embeddings from\ndecoder-only large language models (LLMs) remain underexplored. Unlike encoder\nmodels, LLMs are trained with a next-token prediction objective, which does not\ndirectly encode the meaning of the current token. In this paper, we construct\nlexico-semantic networks from the input embeddings of LLMs with varying\nparameter scales and conduct a comparative analysis of their global and local\nstructures. Our results show that these networks exhibit small-world\nproperties, characterized by high clustering and short path lengths. Moreover,\nlarger LLMs yield more intricate networks with less small-world effects and\nlonger paths, reflecting richer semantic structures and relations. We further\nvalidate our approach through analyses of common conceptual pairs, structured\nlexical relations derived from WordNet, and a cross-lingual semantic network\nfor qualitative words."}
{"id": "2502.11525", "pdf": "https://arxiv.org/pdf/2502.11525.pdf", "abs": "https://arxiv.org/abs/2502.11525", "title": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs", "authors": ["Yi Hu", "Shijia Kang", "Haotong Yang", "Haotian Xu", "Muhan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Length generalization, the ability to solve problems longer than those seen\nduring training, remains a critical challenge for large language models (LLMs).\nPrevious work modifies positional encodings (PEs) and data formats to improve\nlength generalization on specific symbolic tasks such as addition and sorting.\nHowever, these approaches are fundamentally limited to special tasks, often\ndegrading general language performance. Furthermore, they are typically\nevaluated on small transformers trained from scratch on single tasks and can\ncause performance drop when applied during post-training stage of practical\nLLMs with general capabilities. Hu et al., (2024) proposed Rule-Following\nFine-Tuning (RFFT) to improve length generalization in the post-training stage\nof LLMs. Despite its compatibility with practical models and strong\nperformance, RFFT is proposed for single tasks too, requiring re-training for\neach individual task with extensive examples. In this paper, we study length\ngeneralization in multi-task settings and propose Meta Rule-Following\nFine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length\ngeneralization. As our first contribution, we construct a large length\ngeneralization dataset containing 86 tasks spanning code execution, number\nprocessing, symbolic and logical reasoning tasks, beyond the common addition or\nmultiplication tasks. Secondly, we show that cross-task length generalization\nis possible with Meta-RFFT. After training on a large number of tasks and\ninstances, the models achieve remarkable length generalization ability on\nunseen tasks with minimal fine-tuning or one-shot prompting. For example, after\nfine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30\ndigit addition, significantly outperforming the state-of-the-art reasoning\nmodels (DeepSeek-R1-671B: 72%), despite never seeing this task during\nRF-pretraining."}
{"id": "2502.11571", "pdf": "https://arxiv.org/pdf/2502.11571.pdf", "abs": "https://arxiv.org/abs/2502.11571", "title": "FaMTEB: Massive Text Embedding Benchmark in Persian Language", "authors": ["Erfan Zinvandi", "Morteza Alikhani", "Mehran Sarmadi", "Zahra Pourbahman", "Sepehr Arvin", "Reza Kazemi", "Arash Amini"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard."}
{"id": "2502.12202", "pdf": "https://arxiv.org/pdf/2502.12202.pdf", "abs": "https://arxiv.org/abs/2502.12202", "title": "To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models", "authors": ["Zihao Zhu", "Hongbao Zhang", "Ruotong Wang", "Ke Xu", "Siwei Lyu", "Baoyuan Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "39 pages, 13 tables, 14 figures", "summary": "Large Reasoning Models (LRMs) are designed to solve complex tasks by\ngenerating explicit reasoning traces before producing final answers. However,\nwe reveal a critical vulnerability in LRMs -- termed Unthinking Vulnerability\n-- wherein the thinking process can be bypassed by manipulating special\ndelimiter tokens. It is empirically demonstrated to be widespread across\nmainstream LRMs, posing both a significant risk and potential utility,\ndepending on how it is exploited. In this paper, we systematically investigate\nthis vulnerability from both malicious and beneficial perspectives. On the\nmalicious side, we introduce Breaking of Thought (BoT), a novel attack that\nenables adversaries to bypass the thinking process of LRMs, thereby\ncompromising their reliability and availability. We present two variants of\nBoT: a training-based version that injects backdoor during the fine-tuning\nstage, and a training-free version based on adversarial attack during the\ninference stage. As a potential defense, we propose thinking recovery alignment\nto partially mitigate the vulnerability. On the beneficial side, we introduce\nMonitoring of Thought (MoT), a plug-and-play framework that allows model owners\nto enhance efficiency and safety. It is implemented by leveraging the same\nvulnerability to dynamically terminate redundant or risky reasoning through\nexternal monitoring. Extensive experiments show that BoT poses a significant\nthreat to reasoning reliability, while MoT provides a practical solution for\npreventing overthinking and jailbreaking. Our findings expose an inherent flaw\nin current LRM architectures and underscore the need for more robust reasoning\nsystems in the future."}
{"id": "2502.12464", "pdf": "https://arxiv.org/pdf/2502.12464.pdf", "abs": "https://arxiv.org/abs/2502.12464", "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models", "authors": ["Seanie Lee", "Dong Bok Lee", "Dominik Wagner", "Minki Kang", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Deploying large language models (LLMs) in real-world applications requires\nrobust safety guard models to detect and block harmful user prompts. While\nlarge safety guard models achieve strong performance, their computational cost\nis substantial. To mitigate this, smaller distilled models are used, but they\noften underperform on \"hard\" examples where the larger model provides accurate\npredictions. We observe that many inputs can be reliably handled by the smaller\nmodel, while only a small fraction require the larger model's capacity.\nMotivated by this, we propose SafeRoute, a binary router that distinguishes\nhard examples from easy ones. Our method selectively applies the larger safety\nguard model to the data that the router considers hard, improving efficiency\nwhile maintaining accuracy compared to solely using the larger safety guard\nmodel. Experimental results on multiple benchmark datasets demonstrate that our\nadaptive model selection significantly enhances the trade-off between\ncomputational cost and safety performance, outperforming relevant baselines."}
{"id": "2502.13691", "pdf": "https://arxiv.org/pdf/2502.13691.pdf", "abs": "https://arxiv.org/abs/2502.13691", "title": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora", "authors": ["Tristan Karch", "Luca Engel", "Philippe Schwaller", "Frédéric Kaplan"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing five strategically selected datasets: EPFL PhD manuscripts, a private\ncollection of Venetian historical records, two sets of Wikipedia articles on\nrelated topics, and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts."}
{"id": "2502.14285", "pdf": "https://arxiv.org/pdf/2502.14285.pdf", "abs": "https://arxiv.org/abs/2502.14285", "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach", "authors": ["Yurong Wu", "Fangwen Mu", "Qiuhong Zhang", "Jinjing Zhao", "Xinrun Xu", "Lingrui Mei", "Yang Wu", "Lin Shi", "Junjie Wang", "Zhiming Ding", "Yiwei Wang"], "categories": ["cs.CL"], "comment": "14 pages,8 figures,4 tables", "summary": "Prompt trading has emerged as a significant intellectual property concern in\nrecent years, where vendors entice users by showcasing sample images before\nselling prompt templates that can generate similar images. This work\ninvestigates a critical security vulnerability: attackers can steal prompt\ntemplates using only a limited number of sample images. To investigate this\nthreat, we introduce Prism, a prompt-stealing benchmark consisting of 50\ntemplates and 450 images, organized into Easy and Hard difficulty levels. To\nidentify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a\nnovel template stealing method that operates without model fine-tuning by\nleveraging differential evolution algorithms. The system first initializes\npopulation sets using multimodal large language models (MLLMs) based on\npredefined patterns, then iteratively generates enhanced offspring through\nMLLMs. During evolution, EvoStealer identifies common features across offspring\nto derive generalized templates. Our comprehensive evaluation conducted across\nopen-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini)\ndemonstrates that EvoStealer's stolen templates can reproduce images highly\nsimilar to originals and effectively generalize to other subjects,\nsignificantly outperforming baseline methods with an average improvement of\nover 10%. Moreover, our cost analysis reveals that EvoStealer achieves template\nstealing with negligible computational expenses. Our code and dataset are\navailable at https://github.com/whitepagewu/evostealer."}
{"id": "2502.15654", "pdf": "https://arxiv.org/pdf/2502.15654.pdf", "abs": "https://arxiv.org/abs/2502.15654", "title": "Machine-generated text detection prevents language model collapse", "authors": ["George Drayson", "Emine Yilmaz", "Vasileios Lampos"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, converge to a low\nvariance output distribution, and ultimately yield a declining performance. In\nthis study, we investigate the impact of decoding strategy on model collapse,\nanalysing the text characteristics at each model generation, the similarity to\nhuman references, and the resulting model performance. Using the decoding\nstrategies that lead to the most significant degradation, we evaluate model\ncollapse in more realistic scenarios where the origin of the data (human or\nsynthetic) is unknown. We train a machine-generated text detector and propose\nan importance sampling approach to alleviate model collapse. Our method is\nvalidated on two LLM variants (GPT-2 and SmolLM2), across a range of model\nsizes (124M to 1.7B), on the open-ended text generation task. We demonstrate\nthat it can not only prevent model collapse but also improve performance when\nsufficient human-authored samples are present. Source code:\ngithub.com/GeorgeDrayson/model_collapse."}
{"id": "2502.21309", "pdf": "https://arxiv.org/pdf/2502.21309.pdf", "abs": "https://arxiv.org/abs/2502.21309", "title": "FANformer: Improving Large Language Models Through Effective Periodicity Modeling", "authors": ["Yihong Dong", "Ge Li", "Xue Jiang", "Yongding Tao", "Kechi Zhang", "Hao Zhu", "Huanyu Liu", "Jiazheng Ding", "Jia Li", "Jinliang Deng", "Hong Mei"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which adapts Fourier Analysis Network (FAN) into\nattention mechanism to achieve efficient periodicity modeling, by modifying the\nfeature projection process of attention mechanism. Extensive experimental\nresults on language modeling show that FANformer consistently outperforms\nTransformer when scaling up model size and training tokens, underscoring its\nsuperior learning efficiency. Our pretrained FANformer-1B exhibits marked\nimprovements on downstream tasks compared to open-source LLMs with similar\nmodel parameters or training tokens. Moreover, we reveal that FANformer\nexhibits superior ability to learn and apply rules for reasoning compared to\nTransformer. The results position FANformer as an effective and promising\narchitecture for advancing LLMs."}
{"id": "2503.03008", "pdf": "https://arxiv.org/pdf/2503.03008.pdf", "abs": "https://arxiv.org/abs/2503.03008", "title": "MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings", "authors": ["Andrea Gurioli", "Federico Pennino", "João Monteiro", "Maurizio Gabbrielli"], "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "comment": null, "summary": "Deploying language models often requires navigating accuracy vs. performance\ntrade-offs to meet latency constraints while preserving utility. Traditional\nmodel distillation reduces size but incurs substantial costs through training\nseparate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter\nmulti-exit encoder for code retrieval and classification that employs a novel\nSelf-Distillation mechanism. This approach significantly enhances lower-layer\nrepresentations, enabling flexible deployment of different model portions with\nfavorable performance trade-offs. Our architecture improves text-to-code and\ncode-to-code search by targeting specific encoder layers as exit heads, where\nhigher layers guide earlier ones during training-improving intermediate\nrepresentations at minimal additional cost. We further enhance MoSE with a\nrepository-level contextual loss that maximizes training context window\nutilization. Additionally, we release a new dataset created through code\ntranslation that extends text-to-code benchmarks with cross-language\ncode-to-code pairs. Evaluations demonstrate the effectiveness of\nSelf-Distillation as a principled approach to trading inference cost for\naccuracy across various code understanding tasks."}
{"id": "2503.03261", "pdf": "https://arxiv.org/pdf/2503.03261.pdf", "abs": "https://arxiv.org/abs/2503.03261", "title": "Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions", "authors": ["Yichong Zhao", "Susumu Goto"], "categories": ["cs.CL"], "comment": null, "summary": "Multiple previous studies have reported suboptimal performance of LLMs in\nbiomedical text mining. By analyzing failure patterns in these evaluations, we\nidentified three primary challenges for LLMs in biomedical corpora: (1) LLMs\nfail to learn implicit dataset-specific nuances from supervised data, (2) The\ncommon formatting requirements of discriminative tasks limit the reasoning\ncapabilities of LLMs particularly for LLMs that lack test-time compute, and (3)\nLLMs struggle to adhere to annotation guidelines and match exact schemas, which\nhinders their ability to understand detailed annotation requirements which is\nessential in biomedical annotation workflow. We experimented with prompt\nengineering techniques targeted to the above issues, and developed a pipeline\nthat dynamically extracts instructions from annotation guidelines. Our results\nshow that frontier LLMs can approach or surpass the performance of SOTA\nBERT-based models with minimal reliance on manually annotated data and without\nfine-tuning. Furthermore, we performed model distillation on a closed-source\nLLM, demonstrating that a BERT model trained exclusively on synthetic data\nannotated by LLMs can also achieve a practical performance. Based on these\nfindings, we explored the feasibility of partially replacing manual annotation\nwith LLMs in production scenarios for biomedical text mining."}
{"id": "2503.06218", "pdf": "https://arxiv.org/pdf/2503.06218.pdf", "abs": "https://arxiv.org/abs/2503.06218", "title": "SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios", "authors": ["Weidong Zhan", "Yue Wang", "Nan Hu", "Liming Xiao", "Jingyuan Ma", "Yuhang Qin", "Zheng Li", "Yixin Yang", "Sirui Deng", "Jinkun Ding", "Wenhan Ma", "Rui Li", "Weilin Luo", "Qun Liu", "Zhifang Sui"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, long-chain reasoning remains a key challenge for large language\nmodels (LLMs) because natural texts lack sufficient explicit reasoning data.\nHowever, existing benchmarks suffer from limitations such as narrow coverage,\nshort reasoning paths, or high construction costs. We introduce SCoRE\n(Scenario-based Commonsense Reasoning Evaluation), a benchmark that synthesizes\nmulti-hop questions from scenario schemas of entities, relations, and logical\nrules to assess long-chain commonsense reasoning. SCoRE contains 100k bilingual\n(Chinese-English) multiple-choice questions whose reasoning chains span 2-11\nhops and are grouped into various difficulty levels. Each question is\naccompanied by fine-grained knowledge labels, explicit reasoning chains, and\ndifficulty levels for diagnostic evaluation. Evaluation results on cutting-edge\nLLMs such as o3-mini and Deepseek R1 shows that even the best model attains\nonly 69.78% accuracy on SCoRE (even only 47.91% on the hard set), with errors\noften stemming from rare knowledge, logical inconsistency, and\nover-interpretation of simple questions. SCoRE offers a scalable, extensible\nframework for evaluating and diagnosing the long-chain commonsense reasoning\nabilities of LLMs and guiding future advances in model design and training."}
{"id": "2503.09543", "pdf": "https://arxiv.org/pdf/2503.09543.pdf", "abs": "https://arxiv.org/abs/2503.09543", "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs", "authors": ["Oskar van der Wal", "Pietro Lesci", "Max Muller-Eberstein", "Naomi Saphra", "Hailey Schoelkopf", "Willem Zuidema", "Stella Biderman"], "categories": ["cs.CL", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "The stability of language model pre-training and its effects on downstream\nperformance are still understudied. Prior work shows that the training process\ncan yield significantly different results in response to slight variations in\ninitial conditions, e.g., the random seed. Crucially, the research community\nstill lacks sufficient resources and tools to systematically investigate\npre-training stability, particularly for decoder-only language models. We\nintroduce the PolyPythias, a set of 45 new training runs for the Pythia model\nsuite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting\nin about 7k new checkpoints that we release. Using these new 45 training runs,\nin addition to the 5 already available, we study the effects of different\ninitial conditions determined by the seed -- i.e., parameters' initialisation\nand data order -- on (i) downstream performance, (ii) learned linguistic\nrepresentations, and (iii) emergence of training phases. In addition to common\nscaling behaviours, our analyses generally reveal highly consistent training\ndynamics across both model sizes and initial conditions. Further, the new seeds\nfor each model allow us to identify outlier training runs and delineate their\ncharacteristics. Our findings show the potential of using these methods to\npredict training stability."}
{"id": "2503.09674", "pdf": "https://arxiv.org/pdf/2503.09674.pdf", "abs": "https://arxiv.org/abs/2503.09674", "title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation", "authors": ["Jonathan Zheng", "Sauvik Das", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, preprint", "summary": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average."}
{"id": "2503.10669", "pdf": "https://arxiv.org/pdf/2503.10669.pdf", "abs": "https://arxiv.org/abs/2503.10669", "title": "UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality", "authors": ["Zelei Cheng", "Xin-Qiang Cai", "Yuting Tang", "Pushi Zhang", "Boming Yang", "Masashi Sugiyama", "Xinyu Xing"], "categories": ["cs.CL", "cs.AI"], "comment": "Language Modeling, Machine Learning for NLP, Distributional\n  Pareto-Optimal", "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs) with human values. However, existing\napproaches struggle to capture the multi-dimensional, distributional nuances of\nhuman preferences. Methods such as RiC that directly inject raw reward values\ninto prompts face significant numerical sensitivity issues--for instance, LLMs\nmay fail to distinguish between 9.11 and 9.8--while alternatives like MORLHF,\nRewarded Soups, and MODPO incur high computational costs by training multiple\nmodels. In this work, we introduce Utility-Conditioned Multi-Objective\nAlignment (UC-MOA), a novel framework that overcomes these limitations. Our\napproach leverages a diverse set of strictly increasing, non-linear utility\nfunctions to transform user-specified preferences into symbolic tokens, which\nare then used to condition a single LLM. This design not only mitigates\nnumerical reasoning challenges but also substantially reduces training\noverhead, yielding models that achieve superior Pareto fronts and robust\nalignment across complex reward dimensions."}
{"id": "2503.12908", "pdf": "https://arxiv.org/pdf/2503.12908.pdf", "abs": "https://arxiv.org/abs/2503.12908", "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models", "authors": ["Xinyan Jiang", "Hang Ye", "Yongxin Zhu", "Xiaoying Zheng", "Zikang Chen", "Jun Gong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 findings", "summary": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks."}
{"id": "2503.14411", "pdf": "https://arxiv.org/pdf/2503.14411.pdf", "abs": "https://arxiv.org/abs/2503.14411", "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models", "authors": ["Siwei Zhang", "Yun Xiong", "Yateng Tang", "Xi Chen", "Zian Jia", "Zehao Gu", "Jiarong Xu", "Jiawei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Submit to NeurIPS2025", "summary": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{CROSS}, a flexible\nframework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is\ndesigned by decomposing the TTAG modeling process into two phases: (i) temporal\nsemantics extraction; and (ii) semantic-structural information unification. The\nkey idea is to advance the large language models (LLMs) to dynamically extract\nthe temporal semantics in text space and then generate cohesive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the CROSS framework, which empowers LLMs to offer the\ntemporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experiments show that CROSS achieves state-of-the-art results on four\npublic datasets and one industrial dataset, with 24.7% absolute MRR gain on\naverage in temporal link prediction and 3.7% AUC gain in node classification of\nindustrial application."}
{"id": "2503.21380", "pdf": "https://arxiv.org/pdf/2503.21380.pdf", "abs": "https://arxiv.org/abs/2503.21380", "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models", "authors": ["Haoxiang Sun", "Yingqian Min", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "Technical Report on Slow Thinking with LLMs: Evaluation Benchmark", "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5\nPro Exp demonstrating notably limited accuracy on the hard subset. Furthermore,\nthe benchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the benchmark,\nevaluation code, detailed results and a data visualization tool at\nhttps://github.com/RUCAIBox/OlymMATH."}
{"id": "2503.21729", "pdf": "https://arxiv.org/pdf/2503.21729.pdf", "abs": "https://arxiv.org/abs/2503.21729", "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation", "authors": ["Zhicheng Lee", "Shulin Cao", "Jinxin Liu", "Jiajie Zhang", "Weichuan Liu", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG)."}
{"id": "2503.21805", "pdf": "https://arxiv.org/pdf/2503.21805.pdf", "abs": "https://arxiv.org/abs/2503.21805", "title": "ImF: Implicit Fingerprint for Large Language Models", "authors": ["Wu jiaxuan", "Peng Wanli", "Fu hang", "Xue Yiming", "Wen juan"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures", "summary": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing fingerprinting techniques\ntypically embed identifiable patterns with weak semantic coherence, resulting\nin fingerprints that significantly differ from the natural question-answering\n(QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of\nthe embedded fingerprints and makes them vulnerable to adversarial attacks. In\nthis paper, we first demonstrate the critical vulnerability of existing\nfingerprint embedding methods by introducing a novel adversarial attack named\nGeneration Revision Intervention (GRI) attack. GRI attack exploits the semantic\nfragility of current fingerprinting methods, effectively erasing fingerprints\nby disrupting their weakly correlated semantic structures. Our empirical\nevaluation highlights that traditional fingerprinting approaches are\nsignificantly compromised by the GRI attack, revealing severe limitations in\ntheir robustness under realistic adversarial conditions. To advance the\nstate-of-the-art in model fingerprinting, we propose a novel model fingerprint\nparadigm called Implicit Fingerprints (ImF). ImF leverages steganography\ntechniques to subtly embed ownership information within natural texts,\nsubsequently using Chain-of-Thought (CoT) prompting to construct semantically\ncoherent and contextually natural QA pairs. This design ensures that\nfingerprints seamlessly integrate with the standard model behavior, remaining\nindistinguishable from regular outputs and substantially reducing the risk of\naccidental triggering and targeted removal. We conduct a comprehensive\nevaluation of ImF on 15 diverse LLMs, spanning different architectures and\nvarying scales."}
{"id": "2503.22388", "pdf": "https://arxiv.org/pdf/2503.22388.pdf", "abs": "https://arxiv.org/abs/2503.22388", "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future. DSDBench is publicly available at\ngithub.com/KevinCL16/DSDBench."}
{"id": "2503.23513", "pdf": "https://arxiv.org/pdf/2503.23513.pdf", "abs": "https://arxiv.org/abs/2503.23513", "title": "RARE: Retrieval-Augmented Reasoning Modeling", "authors": ["Zhengren Wang", "Jiayang Yu", "Dongsheng Ma", "Zhe Chen", "Yu Wang", "Zhiyu Li", "Feiyu Xiong", "Yanfeng Wang", "Weinan E", "Linpeng Tang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": "Repo: https://github.com/Open-DataFlow/RARE", "summary": "Domain-specific intelligence demands specialized knowledge and sophisticated\nreasoning for problem-solving, posing significant challenges for large language\nmodels (LLMs) that struggle with knowledge hallucination and inadequate\nreasoning capabilities under constrained parameter budgets. Inspired by Bloom's\nTaxonomy in educational theory, we propose Retrieval-Augmented Reasoning\nModeling (RARE), a novel paradigm that decouples knowledge storage from\nreasoning optimization. RARE externalizes domain knowledge to retrievable\nsources and internalizes domain-specific reasoning patterns during training.\nSpecifically, by injecting retrieved knowledge into training prompts with\nmasked losses, RARE transforms learning objectives from rote memorization to\ncontextualized reasoning. It enables models to bypass parameter-intensive\nmemorization and prioritize the development of higher-order cognitive\nprocesses. Extensive experiments demonstrate that lightweight RARE-trained\nmodels (e.g., Llama-3.1-8B) could achieve state-of-the-art performance,\nsurpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\\%\naccuracy. RARE establishes a paradigm shift where maintainable external\nknowledge bases synergize with compact, reasoning-optimized models,\ncollectively driving more scalable domain-specific intelligence."}
{"id": "2504.04050", "pdf": "https://arxiv.org/pdf/2504.04050.pdf", "abs": "https://arxiv.org/abs/2504.04050", "title": "FISH-Tuning: Enhancing PEFT Methods with Fisher Information", "authors": ["Kang Xue", "Ming Dong", "Xinhui Tu", "Tingting He"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth in the parameter size of Large Language Models (LLMs) has\nspurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods to\nmitigate the substantial computational costs of fine-tuning. Among these,\nFisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT\ntechnique that identifies a critical subset of pre-trained parameters using\napproximate Fisher information. While addition-based and\nreparameterization-based PEFT methods like LoRA and Adapter already fine-tune\nonly a small number of parameters, the newly introduced parameters within these\nmethods themselves present an opportunity for further optimization. Selectively\nfine-tuning only the most impactful among these new parameters could further\nreduce resource consumption while maintaining, or even improving, fine-tuning\neffectiveness. In this paper, we propose \\textbf{FISH-Tuning}, a novel approach\nthat incorporates FISH Mask into such PEFT methods, including LoRA, Adapter,\nand their variants. By leveraging Fisher information to identify and update\nonly the most significant parameters within these added or reparameterized\ncomponents, FISH-Tuning aims to achieve superior performance without increasing\ntraining time or inference latency compared to the vanilla PEFT methods.\nExperimental results across various datasets and pre-trained models demonstrate\nthat FISH-Tuning consistently outperforms the vanilla PEFT methods when using\nthe same proportion of trainable parameters. Code is available at\nhttps://anonymous.4open.science/r/FISH-Tuning-6F7C."}
{"id": "2504.05831", "pdf": "https://arxiv.org/pdf/2504.05831.pdf", "abs": "https://arxiv.org/abs/2504.05831", "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "authors": ["Mingye Zhu", "Yi Liu", "Zheren Fu", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL"], "comment": null, "summary": "Preference alignment methods are increasingly critical for steering large\nlanguage models (LLMs) to generate outputs consistent with human values. While\nrecent approaches often rely on synthetic data generated by LLMs for\nscalability and cost-efficiency reasons, this reliance can introduce\ndistribution shifts that undermine the nuanced representation of human\npreferences needed for desirable outputs. In this paper, we propose a novel\ndistribution-aware optimization framework that improves preference alignment\ndespite such shifts. Our approach first leverages well-learned classifiers to\nassign a calibration value to each training sample, quantifying its alignment\nwith the target human-preferred distribution. These values are then\nincorporated into a robust optimization objective that minimizes the worst-case\nloss over regions of the data space most relevant to human preferences. By\nexplicitly focusing optimization on the target distribution, our approach\nmitigates the impact of distributional mismatch and improves the generation of\nresponses that better reflect intended values."}
{"id": "2504.07433", "pdf": "https://arxiv.org/pdf/2504.07433.pdf", "abs": "https://arxiv.org/abs/2504.07433", "title": "LSR-MCTS: Alleviating Long Range Dependency in Code Generation", "authors": ["Tingwei Lu", "Yangning Li", "Liyuan Wang", "Binghuai Lin", "Jiwei Tang", "Qingsong Lv", "Wanshi Xu", "Hai-Tao Zheng", "Yinghui Li", "Xin Su", "Zifei Shan"], "categories": ["cs.CL"], "comment": null, "summary": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches."}
{"id": "2504.08300", "pdf": "https://arxiv.org/pdf/2504.08300.pdf", "abs": "https://arxiv.org/abs/2504.08300", "title": "Large Language Models Could Be Rote Learners", "authors": ["Yuyang Xu", "Renjun Hu", "Haochao Ying", "Jian Wu", "Xing Shi", "Wei Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework\nreformulating MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average)."}
{"id": "2504.10198", "pdf": "https://arxiv.org/pdf/2504.10198.pdf", "abs": "https://arxiv.org/abs/2504.10198", "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation", "authors": ["Hanghui Guo", "Jia Zhu", "Shimin Di", "Weijie Shi", "Zhangze Chen", "Jiajie Xu"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main", "summary": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work."}
{"id": "2504.12052", "pdf": "https://arxiv.org/pdf/2504.12052.pdf", "abs": "https://arxiv.org/abs/2504.12052", "title": "Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events", "authors": ["François Haguinet", "Jeffery L Painter", "Gregory E Powell", "Andrea Callegaro", "Andrew Bate"], "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": "32 pages, 7 figures, 5 supplementary figures", "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwith a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from clinically similar\nMedDRA Preferred Terms (PTs) to the target PT. This continuous similarity-based\nborrowing overcomes limitations of rigid hierarchical grouping in current\ndisproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evaluate our approach -- termed IC SSM -- against traditional\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term level (IC HLGT). A reference set (PVLens), derived from\nFDA product label update, enabled prospective evaluation of method performance\nin identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated higher sensitivity (1332/2337=0.570,\nYouden's J=0.246) than traditional IC (Se=0.501, J=0.250) and IC HLGT\n(Se=0.556, J=0.225), consistently identifying more true positives and doing so\non average 5 months sooner than traditional IC. Despite a marginally lower\naggregate F1-score and Youden's index, IC SSM showed higher performance in\nearly post-marketing periods or when the detection threshold was raised,\nproviding more stable and relevant alerts than IC HLGT and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods, with\npotential for validation across other datasets and exploration of additional\nsimilarity metrics and Bayesian strategies using case-level data."}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534.pdf", "abs": "https://arxiv.org/abs/2504.13534", "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Dan Feng", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning boosts large language models' (LLMs)\nperformance on complex tasks but faces two key limitations: a lack of\nreliability when solely relying on LLM-generated reasoning chains and\ninterference from natural language reasoning steps with the models' inference\nprocess, also known as the inference logic of LLMs. To address these issues, we\npropose CoT-RAG, a novel reasoning framework with three key designs: (i)\nKnowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate\nreasoning chain generation of LLMs, thereby enhancing reasoning credibility;\n(ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented\ngeneration (RAG) into knowledge graphs to retrieve relevant sub-cases and\nsub-descriptions, providing LLMs with learnable information; (iii)\nPseudo-Program Prompting Execution, which promotes greater logical rigor by\nguiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine\npublic datasets spanning three reasoning tasks reveal significant accuracy\ngains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore,\ntests on four domain-specific datasets demonstrate exceptional accuracy and\nefficient execution, underscoring its practical applicability and scalability."}
{"id": "2504.14321", "pdf": "https://arxiv.org/pdf/2504.14321.pdf", "abs": "https://arxiv.org/abs/2504.14321", "title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach", "authors": ["Xingyu Li", "Chen Gong", "Guohong Fu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources. To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues."}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669.pdf", "abs": "https://arxiv.org/abs/2504.14669", "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "categories": ["cs.CL"], "comment": "11 pages, 4 figures, accepted by ACL 2025 as findings", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss."}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895.pdf", "abs": "https://arxiv.org/abs/2504.15895", "title": "Dynamic Early Exit in Reasoning Models", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Qiaowei Li", "Zheng Lin", "Li Cao", "Weiping Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 15 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and\nLiveCodeBench) show that the proposed method is consistently effective on 11\ncutting-edge reasoning LLMs of varying series and sizes, reducing the length of\nCoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3%\nto 5.0%."}
{"id": "2504.16074", "pdf": "https://arxiv.org/pdf/2504.16074.pdf", "abs": "https://arxiv.org/abs/2504.16074", "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Jiaming Ji", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Yaodong Yang", "Muhan Zhang", "Hua Xing Zhu"], "categories": ["cs.CL"], "comment": "34 pages ,12 figures, 7 tables, latest update in 2025/05/18", "summary": "Current benchmarks for evaluating the reasoning capabilities of Large\nLanguage Models (LLMs) face significant limitations: task oversimplification,\ndata contamination, and flawed evaluation items. These deficiencies necessitate\nmore rigorous assessment methods. To address these limitations, we introduce\nPHYBench, a benchmark of 500 original physics problems ranging from high school\nto Physics Olympiad difficulty. PHYBench addresses data contamination through\noriginal content and employs a systematic curation pipeline to eliminate flawed\nitems. Evaluations show that PHYBench activates more tokens and provides\nstronger differentiation between reasoning models compared to other baselines\nlike AIME 2024, OlympiadBench and GPQA. Even the best-performing model, Gemini\n2.5 Pro, achieves only 36.9% accuracy compared to human experts' 61.9%. To\nfurther enhance evaluation precision, we introduce the Expression Edit Distance\n(EED) Score for mathematical expression assessment, which improves sample\nefficiency by 204% over binary scoring. Moreover, PHYBench effectively elicits\nmulti-step and multi-condition reasoning, providing a platform for examining\nmodels' reasoning robustness, preferences, and deficiencies. The benchmark\nresults and dataset are publicly available at https://www.phybench.cn/."}
{"id": "2504.16918", "pdf": "https://arxiv.org/pdf/2504.16918.pdf", "abs": "https://arxiv.org/abs/2504.16918", "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "authors": ["Raghav Thind", "Youran Sun", "Ling Liang", "Haizhao Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimization plays a vital role in scientific research and practical\napplications. However, formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce OptimAI,\na framework for solving Optimization problems described in natural language by\nleveraging LLM-powered AI agents, and achieve superior performance over current\nstate-of-the-art methods. Our framework is built upon the following key roles:\n(1) a formulator that translates natural language problem descriptions into\nprecise mathematical formulations; (2) a planner that constructs a high-level\nsolution strategy prior to execution; and (3) a coder and a code critic capable\nof interacting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, and our experiments confirm that combining diverse models leads\nto performance gains. Our approach attains 88.1% accuracy on the NLP4LP dataset\nand 82.3% on the Optibench dataset, reducing error rates by 58% and 52%,\nrespectively, over prior best results."}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192.pdf", "abs": "https://arxiv.org/abs/2504.17192", "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, particularly from the authors of those\npapers, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins. Code is available at:\nhttps://github.com/going-doer/Paper2Code."}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162.pdf", "abs": "https://arxiv.org/abs/2504.19162", "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project webpage: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, SPC can\nguide the test-time search of diverse LLMs and significantly improve their\nmathematical reasoning performance on MATH500 and AIME2024, surpassing those\nguided by state-of-the-art process reward models."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627.pdf", "abs": "https://arxiv.org/abs/2504.19627", "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157.pdf", "abs": "https://arxiv.org/abs/2504.20157", "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Suin Kim", "Dongyeop Kang"], "categories": ["cs.CL"], "comment": "Code and data: https://github.com/minnesotanlp/mpo", "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nfrom essay writing to mathematical reasoning, without requiring specialized\nreward designs. Beyond standard RLAIF, MPO's meta-learning formulation is\nreadily extensible to higher-level alignment frameworks. Overall, this method\naddresses theoretical and practical challenges in reward-based RL alignment for\nLLMs, paving the way for more robust and adaptable alignment strategies. The\ncode and data can be accessed at: https://github.com/minnesotanlp/mpo"}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734.pdf", "abs": "https://arxiv.org/abs/2504.20734", "title": "UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single aggregated corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over various modality-specific and unified\nbaselines."}
{"id": "2504.20771", "pdf": "https://arxiv.org/pdf/2504.20771.pdf", "abs": "https://arxiv.org/abs/2504.20771", "title": "Computational Reasoning of Large Language Models", "authors": ["Haitao Wu", "Zongbo Han", "Joey Tianyi Zhou", "Huaxi Huang", "Changqing Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), multidimensional evaluation has become increasingly critical.\nHowever, current evaluations are often domain-specific and overly complex,\nlimiting their effectiveness as cross-domain proxies for core capabilities. To\naddress these limitations and enable a unified and simple evaluation framework,\nan ideal proxy task should target a basic capability that generalizes across\ntasks and is independent of domain-specific knowledge. Turing machine provides\na powerful theoretical lens by reducing complex processes to basic,\ndomain-agnostic computational operations. This perspective offers a principled\nframework for evaluating basic computational abilities essential to a wide\nrange of tasks. Motivated by this abstraction, we introduce \\textbf{Turing\nMachine Bench}, a benchmark designed to assess the ability of LLMs to\n\\textbf{strictly follow rules} and \\textbf{accurately manage internal states}\nfor multi-step, referred to as \\textbf{computational reasoning}. TMBench\nincorporates four key features: self-contained and knowledge-agnostic\nreasoning, a minimalistic multi-step structure, controllable difficulty, and a\nsolid theoretical foundation based on Turing machine. Empirical results\ndemonstrate that TMBench serves as an effective proxy for evaluating\ncomputational reasoning on representative LLMs. It produces clear step-wise\naccuracy curves, revealing LLMs' ability to execute multi-step reasoning\nprocesses. By analyzing performance trends across TMBench and established\nreasoning benchmarks, we find strong correlations with real-world tasks,\nbridging real-task evaluation with basic ability assessment. These findings\nsuggest that TMBench holds potential as a cross-domain dimension for evaluating\nreasoning in LLMs. Code and data are available at\n\\href{https://github.com/HaitaoWuTJU/Turing-Machine-Bench}{Repo}."}
{"id": "2505.00570", "pdf": "https://arxiv.org/pdf/2505.00570.pdf", "abs": "https://arxiv.org/abs/2505.00570", "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension", "authors": ["Jushi Kai", "Boyi Zeng", "Yixuan Wang", "Haoli Bai", "Ziwei He", "Bo Jiang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method."}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979.pdf", "abs": "https://arxiv.org/abs/2505.00979", "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability."}
{"id": "2505.02387", "pdf": "https://arxiv.org/pdf/2505.02387.pdf", "abs": "https://arxiv.org/abs/2505.02387", "title": "RM-R1: Reward Modeling as Reasoning", "authors": ["Xiusi Chen", "Gaotang Li", "Ziqi Wang", "Bowen Jin", "Cheng Qian", "Yu Wang", "Hongru Wang", "Yu Zhang", "Denghui Zhang", "Tong Zhang", "Hanghang Tong", "Heng Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures", "summary": "Reward modeling is essential for aligning large language models with human\npreferences through reinforcement learning from human feedback. To provide\naccurate reward signals, a reward model (RM) should stimulate deep thinking and\nconduct interpretable reasoning before assigning a score or a judgment.\nInspired by recent advances of long chain-of-thought on reasoning-intensive\ntasks, we hypothesize and validate that integrating reasoning capabilities into\nreward modeling significantly enhances RMs interpretability and performance. To\nthis end, we introduce a new class of generative reward models - Reasoning\nReward Models (ReasRMs) - which formulate reward modeling as a reasoning task.\nWe propose a reasoning-oriented training pipeline and train a family of\nReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -\nself-generating sample-level chat rubrics or math/code solutions, and\nevaluating candidate responses against them. The training of RM-R1 consists of\ntwo key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. Empirically, our models achieve\nstate-of-the-art performance across three reward model benchmarks on average,\noutperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and\nproprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we\nperform thorough empirical analyses to understand the key ingredients of\nsuccessful ReasRM training. To facilitate future research, we release six\nREASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1."}
{"id": "2505.04671", "pdf": "https://arxiv.org/pdf/2505.04671.pdf", "abs": "https://arxiv.org/abs/2505.04671", "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards", "authors": ["Yuxin Zhang", "Meihao Fan", "Ju Fan", "Mingyang Yi", "Yuyu Luo", "Jian Tan", "Guoliang Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nperformance on the Text-to-SQL task by leveraging their powerful reasoning\ncapabilities. To enhance accuracy during the reasoning process, external\nProcess Reward Models (PRMs) can be introduced during training and inference to\nprovide fine-grained supervision. However, if misused, PRMs may distort the\nreasoning trajectory and lead to suboptimal or incorrect SQL generation. To\naddress this challenge, we propose Reward-SQL, a framework that systematically\nexplores how to incorporate PRMs into the Text-to-SQL reasoning process\neffectively. Our approach follows a \"cold start, then PRM supervision\"\nparadigm. Specifically, we first train the model to decompose SQL queries into\nstructured stepwise reasoning chains using common table expressions\n(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.\nThen, we investigate four strategies for integrating PRMs, and find that\ncombining PRM as an online training signal (e.g.,GRPO) with PRM-guided\ninference (e.g., best-of-N sampling) yields the best results. Empirically, on\nthe BIRD benchmark, Reward-SQL enables models supervised by PRM (7B) to achieve\na 13.1% performance gain across various guidance strategies. Notably, our\nGRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9%\naccuracy on the BIRD development set, outperforming all baseline methods under\nthe same model size. These results demonstrate the effectiveness of Reward-SQL\nin leveraging reward-based supervision for Text-to-SQL reasoning."}
{"id": "2505.05327", "pdf": "https://arxiv.org/pdf/2505.05327.pdf", "abs": "https://arxiv.org/abs/2505.05327", "title": "RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection", "authors": ["Yixin Yang", "Qingxiu Dong", "Linli Yao", "Fangwei Zhu", "Zhifang Sui"], "categories": ["cs.CL"], "comment": null, "summary": "Data selection for instruction tuning is crucial for improving the\nperformance of large language models (LLMs) while reducing training costs. In\nthis paper, we propose Refined Contribution Measurement with In-Context\nLearning (RICo), a novel gradient-free method that quantifies the fine-grained\ncontribution of individual samples to both task-level and global-level model\nperformance. RICo enables more accurate identification of high-contribution\ndata, leading to better instruction tuning. We further introduce a lightweight\nselection paradigm trained on RICo scores, enabling scalable data selection\nwith a strictly linear inference complexity. Extensive experiments on three\nLLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the\neffectiveness of RICo. Remarkably, on LLaMA3.1-8B, models trained on 15% of\nRICo-selected data outperform full datasets by 5.42% points and exceed the best\nperformance of widely used selection methods by 2.06% points. We further\nanalyze high-contribution samples selected by RICo, which show both diverse\ntasks and appropriate difficulty levels, rather than just the hardest ones."}
{"id": "2505.07610", "pdf": "https://arxiv.org/pdf/2505.07610.pdf", "abs": "https://arxiv.org/abs/2505.07610", "title": "Concept-Level Explainability for Auditing & Steering LLM Responses", "authors": ["Kenza Amara", "Rita Sevastjanova", "Mennatallah El-Assady"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 7 figures, Submission to Neurips 2025", "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior."}
{"id": "2505.07890", "pdf": "https://arxiv.org/pdf/2505.07890.pdf", "abs": "https://arxiv.org/abs/2505.07890", "title": "TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks", "authors": ["Kutay Ertürk", "Furkan Altınışık", "İrem Sarıaltın", "Ömer Nezih Gerek"], "categories": ["cs.CL", "eess.IV"], "comment": null, "summary": "This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information. Our approach revisits sign language recognition\nas sequence-to-sequence translation, inspired by the linguistic nature of sign\nlanguages and the success of transformers in natural language processing. Since\nTSLFormer uses the self-attention mechanism, it effectively captures temporal\nco-occurrence within gesture sequences and highlights meaningful motion\npatterns as words unfold. Evaluated on the AUTSL dataset with over 36,000\nsamples and 227 different words, TSLFormer achieves competitive performance\nwith minimal computational cost. These results show that joint-based input is\nsufficient for enabling real-time, mobile, and assistive communication systems\nfor hearing-impaired individuals."}
{"id": "2505.08392", "pdf": "https://arxiv.org/pdf/2505.08392.pdf", "abs": "https://arxiv.org/abs/2505.08392", "title": "Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping", "authors": ["Ren Zhuang", "Ben Wang", "Shuifa Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models leverage Chain-of-Thought (CoT) prompting for complex\ntasks, but their reasoning traces are often excessively verbose and\ninefficient, leading to significant computational costs and latency. Current\nCoT compression techniques typically rely on generic importance metrics and\nstatic compression rates, which may inadvertently remove functionally critical\ntokens or fail to adapt to varying reasoning complexity. To overcome these\nlimitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic\nCoT compression via supervised fine-tuning. This approach introduces two\nsynergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric\naccurately identifying functionally relevant tokens by measuring the gradient\ninfluence of their intermediate representations on the final answer loss, and\n(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the\ncompression rate based on runtime model uncertainty while ensuring local\ncoherence through an adaptive N-token constraint. To our knowledge, this is the\nfirst work unifying a goal-oriented, gradient-based importance metric with\ndynamic, uncertainty-aware skipping for CoT compression. Trained on compressed\nMATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization\nacross diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It\nachieves substantial efficiency gains - reducing CoT token counts by over 45%\non average and delivering 1.6-2.0 times inference speedups - while maintaining\nhigh reasoning accuracy. Notably, it significantly outperforms existing\nbaselines by preserving accuracy even at high effective compression rates,\nadvancing the state of the art in the CoT reasoning efficiency-accuracy\ntrade-off."}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527.pdf", "abs": "https://arxiv.org/abs/2505.10527", "title": "WorldPM: Scaling Human Preference Modeling", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations."}
{"id": "2505.10719", "pdf": "https://arxiv.org/pdf/2505.10719.pdf", "abs": "https://arxiv.org/abs/2505.10719", "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models", "authors": ["Tomás Vergara-Browne", "Álvaro Soto"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Motivated by the surge of large language models, there has been a push to\nformally characterize the symbolic abilities intrinsic to the transformer\narchitecture. A programming language, called RASP, has been proposed, which can\nbe directly compiled into transformer weights to implement these algorithms.\nHowever, the tasks that can be implemented in RASP are often uncommon to learn\nfrom natural unsupervised data, showing a mismatch between theoretical\ncapabilities of the transformer architecture, and the practical learnability of\nthese capabilities from unsupervised data. We propose tracr-injection, a method\nthat allows us to distill algorithms written in RASP directly into a\npre-trained language model. We showcase our method by injecting 3 different\nalgorithms into a language model. We show how our method creates an\ninterpretable subspace within the model's residual stream, which can be decoded\ninto the variables present in the code of the RASP algorithm. Additionally, we\nfound that the proposed method can improve out-of-distribution performance\ncompared to our baseline, indicating that indeed a more symbolic mechanism is\ntaking place in the inner workings of the model. We release the code used to\nrun our experiments."}
{"id": "2505.10792", "pdf": "https://arxiv.org/pdf/2505.10792.pdf", "abs": "https://arxiv.org/abs/2505.10792", "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "authors": ["Zhan Peng Lee", "Andre Lin", "Calvin Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\nimprove factuality in large language models (LLMs) by grounding their outputs\nin retrieved documents. However, ensuring perfect retrieval of relevant\ninformation remains challenging, and when irrelevant content is passed\ndownstream to an LLM, it can lead to hallucinations. In this work, we propose\nFinetune-RAG, a simple and effective fine-tuning approach that features the\nfirst-of-its-kind RAG training dataset constructed to mimic real-world\nimperfections. Experimental results show that Finetune-RAG improves factual\naccuracy by 21.2% over the base model. We also propose Bench-RAG, an\nLLM-as-a-judge evaluation pipeline that stress tests models under realistic\nimperfect retrieval scenarios. Our codebase and dataset are fully open sourced\nfor community use."}
{"id": "2505.11031", "pdf": "https://arxiv.org/pdf/2505.11031.pdf", "abs": "https://arxiv.org/abs/2505.11031", "title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning", "authors": ["Xiao Zhang", "Huiyuan Lai", "Qianru Meng", "Johan Bos"], "categories": ["cs.CL"], "comment": "Paper submitted to NeurIPS 2025 dataset and benchmark track", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing tasks, yet their ability to process\nstructured symbolic knowledge remains underexplored. To address this gap, we\npropose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the\nfirst comprehensive benchmark designed to systematically evaluate LLMs'\nproficiency in handling ontologies -- formal, symbolic representations of\ndomain knowledge through concepts, relationships, and instances. Based on the\nproposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n58,981 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing proficiency in understanding\nontological knowledge but substantial weaknesses in reasoning and learning\ntasks. These findings highlight fundamental limitations in LLMs' capability to\nprocess symbolic knowledge and establish OntoURL as a critical benchmark for\nadvancing the integration of LLMs with formal knowledge representations."}
{"id": "2309.12555", "pdf": "https://arxiv.org/pdf/2309.12555.pdf", "abs": "https://arxiv.org/abs/2309.12555", "title": "PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent", "authors": ["Donghoon Shin", "Gary Hsieh", "Young-Ho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "17 pages including reference. Accepted to ACM CUI 2025", "summary": "Creating personalized and actionable exercise plans often requires iteration\nwith experts, which can be costly and inaccessible to many individuals. This\nwork explores the capabilities of Large Language Models (LLMs) in addressing\nthese challenges. We present PlanFitting, an LLM-driven conversational agent\nthat assists users in creating and refining personalized weekly exercise plans.\nBy engaging users in free-form conversations, PlanFitting helps elicit users'\ngoals, availabilities, and potential obstacles, and enables individuals to\ngenerate personalized exercise plans aligned with established exercise\nguidelines. Our study -- involving a user study, intrinsic evaluation, and\nexpert evaluation -- demonstrated PlanFitting's ability to guide users to\ncreate tailored, actionable, and evidence-based plans. We discuss future design\nopportunities for LLM-driven conversational agents to create plans that better\ncomply with exercise principles and accommodate personal constraints."}
{"id": "2402.01591", "pdf": "https://arxiv.org/pdf/2402.01591.pdf", "abs": "https://arxiv.org/abs/2402.01591", "title": "BAT: Learning to Reason about Spatial Sounds with Large Language Models", "authors": ["Zhisheng Zheng", "Puyuan Peng", "Ziyang Ma", "Xie Chen", "Eunsol Choi", "David Harwath"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted to ICML 2024. Our demo, dataset, code and model weights are\n  available at: https://zhishengzheng.com/bat", "summary": "Spatial sound reasoning is a fundamental human skill, enabling us to navigate\nand interpret our surroundings based on sound. In this paper we present BAT,\nwhich combines the spatial sound perception ability of a binaural acoustic\nscene analysis model with the natural language reasoning capabilities of a\nlarge language model (LLM) to replicate this innate ability. To address the\nlack of existing datasets of in-the-wild spatial sounds, we synthesized a\nbinaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed\nSpatialSoundQA, a spatial sound-based question-answering dataset, offering a\nrange of QA tasks that train BAT in various aspects of spatial sound perception\nand reasoning. The acoustic front end encoder of BAT is a novel spatial audio\nencoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by\nitself achieves strong performance across sound event detection, spatial\nlocalization, and distance estimation. By integrating Spatial-AST with LLaMA-2\n7B model, BAT transcends standard Sound Event Localization and Detection (SELD)\ntasks, enabling the model to reason about the relationships between the sounds\nin its environment. Our experiments demonstrate BAT's superior performance on\nboth spatial sound perception and reasoning, showcasing the immense potential\nof LLMs in navigating and interpreting complex spatial audio environments."}
{"id": "2403.08946", "pdf": "https://arxiv.org/pdf/2403.08946.pdf", "abs": "https://arxiv.org/abs/2403.08946", "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era", "authors": ["Xuansheng Wu", "Haiyan Zhao", "Yaochen Zhu", "Yucheng Shi", "Fan Yang", "Lijie Hu", "Tianming Liu", "Xiaoming Zhai", "Wenlin Yao", "Jundong Li", "Mengnan Du", "Ninghao Liu"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": "43 pages, 6 figures, including the latest works published in\n  2024-2025", "summary": "Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended toward explaining Large Language Models (LLMs). This extension calls\nfor a significant transformation in the XAI methodologies for two reasons.\nFirst, many existing XAI methods cannot be directly applied to LLMs due to\ntheir complexity and advanced capabilities. Second, as LLMs are increasingly\ndeployed in diverse applications, the role of XAI shifts from merely opening\nthe ``black box'' to actively enhancing the productivity and applicability of\nLLMs in real-world settings. Meanwhile, the conversation and generation\nabilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we\nintroduce Usable XAI in the context of LLMs by analyzing (1) how XAI can\nexplain and improve LLM-based AI systems and (2) how XAI techniques can be\nimproved by using LLMs. We introduce 10 strategies, introducing the key\ntechniques for each and discussing their associated challenges. We also provide\ncase studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM."}
{"id": "2403.15309", "pdf": "https://arxiv.org/pdf/2403.15309.pdf", "abs": "https://arxiv.org/abs/2403.15309", "title": "Controlled Training Data Generation with Diffusion Models", "authors": ["Teresa Yeo", "Andrei Atanov", "Harold Benoit", "Aleksandr Alekseev", "Ruchira Ray", "Pooya Esmaeil Akhoondi", "Amir Zamir"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project page at https://adversarial-prompts.epfl.ch/", "summary": "We present a method to control a text-to-image generative model to produce\ntraining data useful for supervised learning. Unlike previous works that employ\nan open-loop approach and pre-define prompts to generate new data using either\na language model or human expertise, we develop an automated closed-loop system\nwhich involves two feedback mechanisms. The first mechanism uses feedback from\na given supervised model and finds adversarial prompts that result in image\ngenerations that maximize the model loss. While these adversarial prompts\nresult in diverse data informed by the model, they are not informed of the\ntarget distribution, which can be inefficient. Therefore, we introduce the\nsecond feedback mechanism that guides the generation process towards a certain\ntarget distribution. We call the method combining these two mechanisms Guided\nAdversarial Prompts. We perform our evaluations on different tasks, datasets\nand architectures, with different types of distribution shifts (spuriously\ncorrelated data, unseen domains) and demonstrate the efficiency of the proposed\nfeedback mechanisms compared to open-loop approaches."}
{"id": "2405.20015", "pdf": "https://arxiv.org/pdf/2405.20015.pdf", "abs": "https://arxiv.org/abs/2405.20015", "title": "Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak", "authors": ["Zhenxing Niu", "Yuyao Sun", "Haoxuan Ji", "Zheng Lin", "Haichang Gao", "Xinbo Gao", "Gang Hua", "Rong Jin"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper focuses on jailbreaking attacks against large language models\n(LLMs), eliciting them to generate objectionable content in response to harmful\nuser queries. Unlike previous LLM-jailbreak methods that directly orient to\nLLMs, our approach begins by constructing a multimodal large language model\n(MLLM) built upon the target LLM. Subsequently, we perform an efficient MLLM\njailbreak and obtain a jailbreaking embedding. Finally, we convert the\nembedding into a textual jailbreaking suffix to carry out the jailbreak of\ntarget LLM. Compared to the direct LLM-jailbreak methods, our indirect\njailbreaking approach is more efficient, as MLLMs are more vulnerable to\njailbreak than pure LLM. Additionally, to improve the attack success rate of\njailbreak, we propose an image-text semantic matching scheme to identify a\nsuitable initial input. Extensive experiments demonstrate that our approach\nsurpasses current state-of-the-art jailbreak methods in terms of both\nefficiency and effectiveness. Moreover, our approach exhibits superior\ncross-class generalization abilities."}
{"id": "2406.09556", "pdf": "https://arxiv.org/pdf/2406.09556.pdf", "abs": "https://arxiv.org/abs/2406.09556", "title": "$S^3$ -- Semantic Signal Separation", "authors": ["Márton Kardos", "Jan Kostkan", "Arnault-Quentin Vermillet", "Kristoffer Nielbo", "Kenneth Enevoldsen", "Roberta Rocca"], "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.7"], "comment": "24 pages, 13 figures (main manuscript has 9 pages and 7 figures); The\n  paper has been adjusted according to reviewers' feedback", "summary": "Topic models are useful tools for discovering latent semantic structures in\nlarge textual corpora. Recent efforts have been oriented at incorporating\ncontextual representations in topic modeling and have been shown to outperform\nclassical topic models. These approaches are typically slow, volatile, and\nrequire heavy preprocessing for optimal results. We present Semantic Signal\nSeparation ($S^3$), a theory-driven topic modeling approach in neural embedding\nspaces. $S^3$ conceptualizes topics as independent axes of semantic space and\nuncovers these by decomposing contextualized document embeddings using\nIndependent Component Analysis. Our approach provides diverse and highly\ncoherent topics, requires no preprocessing, and is demonstrated to be the\nfastest contextual topic model, being, on average, 4.5x faster than the\nrunner-up BERTopic. We offer an implementation of $S^3$, and all contextual\nbaselines, in the Turftopic Python package."}
{"id": "2406.10281", "pdf": "https://arxiv.org/pdf/2406.10281.pdf", "abs": "https://arxiv.org/abs/2406.10281", "title": "Watermarking Language Models with Error Correcting Codes", "authors": ["Patrick Chao", "Yan Sun", "Edgar Dobriban", "Hamed Hassani"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating $p$-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art."}
{"id": "2406.10504", "pdf": "https://arxiv.org/pdf/2406.10504.pdf", "abs": "https://arxiv.org/abs/2406.10504", "title": "Task Facet Learning: A Structured Approach to Prompt Optimization", "authors": ["Gurusha Juneja", "Gautam Jajoo", "Nagarajan Natarajan", "Hua Li", "Jian Jiao", "Amit Sharma"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Appearing in ACL Findings 2025", "summary": "Given a task in the form of a basic description and its training examples,\nprompt optimization is the problem of synthesizing the given information into a\ntext prompt for a large language model. Humans solve this problem by also\nconsidering the different facets that define a task (e.g., counter-examples,\nexplanations, analogies) and including them in the prompt. However, it is\nunclear whether existing algorithmic approaches, based on iteratively editing a\ngiven prompt or automatically selecting a few in-context examples, can cover\nthe multiple facets required to solve a complex task. In this work, we view\nprompt optimization as that of learning multiple facets of a task from a set of\ntraining examples. We exploit structure in the prompt optimization problem and\nbreak down a prompt into loosely coupled semantic sections. The proposed\nalgorithm, UniPrompt, (1) clusters the input space and uses clustered batches\nso that each batch likely corresponds to a different facet of the task, and (2)\nutilizes a feedback mechanism to propose adding, editing or deleting a section,\nwhich in turn is aggregated over a batch to capture generalizable facets.\nEmpirical evaluation on multiple datasets and a real-world task shows that\nprompts generated using \\shortname{} obtain higher accuracy than human-tuned\nprompts and those from state-of-the-art methods. In particular, our algorithm\ncan generate long, complex prompts that existing methods are unable to\ngenerate. Code for UniPrompt is available at https://aka.ms/uniprompt."}
{"id": "2407.02772", "pdf": "https://arxiv.org/pdf/2407.02772.pdf", "abs": "https://arxiv.org/abs/2407.02772", "title": "Gradient descent with generalized Newton's method", "authors": ["Zhiqi Bu", "Shiyun Xu"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, our method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers."}
{"id": "2407.11062", "pdf": "https://arxiv.org/pdf/2407.11062.pdf", "abs": "https://arxiv.org/abs/2407.11062", "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language Models", "authors": ["Mengzhao Chen", "Wenqi Shao", "Peng Xu", "Jiahao Wang", "Peng Gao", "Kaipeng Zhang", "Ping Luo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025 Main, camera ready version", "summary": "Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT."}
{"id": "2409.02718", "pdf": "https://arxiv.org/pdf/2409.02718.pdf", "abs": "https://arxiv.org/abs/2409.02718", "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation", "authors": ["Zi Liang", "Qingqing Ye", "Yanyun Wang", "Sen Zhang", "Yaxin Xiao", "Ronghua Li", "Jianliang Xu", "Haibo Hu"], "categories": ["cs.CR", "cs.CL"], "comment": "To appear at ACL 25 main conference", "summary": "Model extraction attacks (MEAs) on large language models (LLMs) have received\nincreasing attention in recent research. However, existing attack methods\ntypically adapt the extraction strategies originally developed for deep neural\nnetworks (DNNs). They neglect the underlying inconsistency between the training\ntasks of MEA and LLM alignment, leading to suboptimal attack performance. To\ntackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel\nmodel extraction algorithm specifically designed for LLMs. In particular, LoRD\nemploys a newly defined policy-gradient-style training task that utilizes the\nresponses of victim model as the signal to guide the crafting of preference for\nthe local model. Theoretical analyses demonstrate that I) The convergence\nprocedure of LoRD in model extraction is consistent with the alignment\nprocedure of LLMs, and II) LoRD can reduce query complexity while mitigating\nwatermark protection through our exploration-based stealing. Extensive\nexperiments validate the superiority of our method in extracting various\nstate-of-the-art commercial LLMs. Our code is available at:\nhttps://github.com/liangzid/LoRD-MEA ."}
{"id": "2410.02647", "pdf": "https://arxiv.org/pdf/2410.02647.pdf", "abs": "https://arxiv.org/abs/2410.02647", "title": "Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection", "authors": ["Song Li", "Yang Tan", "Song Ke", "Liang Hong", "Bingxin Zhou"], "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "comment": "20 pages, 17 tables, 6 figures", "summary": "Immunogenicity prediction is a central topic in reverse vaccinology for\nfinding candidate vaccines that can trigger protective immune responses.\nExisting approaches typically rely on highly compressed features and simple\nmodel architectures, leading to limited prediction accuracy and poor\ngeneralizability. To address these challenges, we introduce VenusVaccine, a\nnovel deep learning solution with a dual attention mechanism that integrates\npre-trained latent vector representations of protein sequences and structures.\nWe also compile the most comprehensive immunogenicity dataset to date,\nencompassing over 7000 antigen sequences, structures, and immunogenicity labels\nfrom bacteria, virus, and tumor. Extensive experiments demonstrate that\nVenusVaccine outperforms existing methods across a wide range of evaluation\nmetrics. Furthermore, we establish a post-hoc validation protocol to assess the\npractical significance of deep learning models in tackling vaccine design\nchallenges. Our work provides an effective tool for vaccine design and sets\nvaluable benchmarks for future research. The implementation is at\nhttps://github.com/songleee/VenusVaccine."}
{"id": "2410.09349", "pdf": "https://arxiv.org/pdf/2410.09349.pdf", "abs": "https://arxiv.org/abs/2410.09349", "title": "Inference and Verbalization Functions During In-Context Learning", "authors": ["Junyi Tao", "Xiaoyin Chen", "Nelson F. Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2024 Findings", "summary": "Large language models (LMs) are capable of in-context learning from a few\ndemonstrations (example-label pairs) to solve new tasks during inference.\nDespite the intuitive importance of high-quality demonstrations, previous work\nhas observed that, in some settings, ICL performance is minimally affected by\nirrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with\nirrelevant labels via two sequential processes: an inference function that\nsolves the task, followed by a verbalization function that maps the inferred\nanswer to the label space. Importantly, we hypothesize that the inference\nfunction is invariant to remappings of the label space (e.g., \"true\"/\"false\" to\n\"cat\"/\"dog\"), enabling LMs to share the same inference function across settings\nwith different label words. We empirically validate this hypothesis with\ncontrolled layer-wise interchange intervention experiments. Our findings\nconfirm the hypotheses on multiple datasets and tasks (natural language\ninference, sentiment analysis, and topic classification) and further suggest\nthat the two functions can be localized in specific layers across various\nopen-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and\nLLAMA-3.1-70B."}
{"id": "2410.12010", "pdf": "https://arxiv.org/pdf/2410.12010.pdf", "abs": "https://arxiv.org/abs/2410.12010", "title": "Bias Similarity Across Large Language Models", "authors": ["Hyejun Jeong", "Shiqing Ma", "Amir Houmansadr"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "under review", "summary": "Bias in Large Language Models remains a critical concern as these systems are\nincreasingly deployed in high-stakes applications. Yet most fairness\nevaluations rely on scalar metrics or single-model analysis, overlooking how\nbiases align -- or diverge -- across model families, scales, and tuning\nstrategies. In this work, we reframe bias similarity as a form of functional\nsimilarity and evaluate 24 LLMs from four major families on over one million\nstructured prompts spanning four bias dimensions. Our findings uncover that\nfairness is not strongly determined by model size, architecture, instruction\ntuning, or openness. Instead, bias behaviors are highly context-dependent and\nstructurally persistent, often resistant to current alignment techniques.\nContrary to common assumptions, we find that open-source models frequently\nmatch or outperform proprietary models in both fairness and utility. These\nresults call into question the default reliance on proprietary systems and\nhighlight the need for behaviorally grounded, model-specific audits to better\nunderstand how bias manifests and endures across the LLM landscape."}
{"id": "2410.14971", "pdf": "https://arxiv.org/pdf/2410.14971.pdf", "abs": "https://arxiv.org/abs/2410.14971", "title": "BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation", "authors": ["Jilong Li", "Zhenxi Song", "Jiaqi Wang", "Meishan Zhang", "Honghai Liu", "Min Zhang", "Zhiguo Zhang"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Current EEG/MEG-to-text decoding systems suffer from three key limitations:\n(1) reliance on teacher-forcing methods, which compromises robustness during\ninference, (2) sensitivity to session-specific noise, hindering generalization\nacross subjects, and (3) misalignment between brain signals and linguistic\nrepresentations due to pre-trained language model over-dominance. To overcome\nthese challenges, we propose BrainECHO (Brain signal decoding via\nvEctor-quantized speCtrogram reconstruction for WHisper-enhanced text\ngeneratiOn), a multi-stage framework that employs decoupled representation\nlearning to achieve state-of-the-art performance on both EEG and MEG datasets.\nSpecifically, BrainECHO consists of three stages: (1) Discrete autoencoding,\nwhich transforms continuous Mel spectrograms into a finite set of high-quality\ndiscrete representations for subsequent stages. (2) Frozen alignment, where\nbrain signal embeddings are mapped to corresponding Mel spectrogram embeddings\nin a frozen latent space, effectively filtering session-specific noise through\nvector-quantized reconstruction, yielding a 3.65% improvement in BLEU-4 score.\n(3) Constrained decoding fine-tuning, which leverages the pre-trained Whisper\nmodel for audio-to-text translation, balancing signal adaptation with knowledge\npreservation, and achieving 74%-89% decoding BLEU scores without excessive\nreliance on teacher forcing. BrainECHO demonstrates robustness across sentence,\nsession, and subject-independent conditions, passing Gaussian noise tests and\nshowcasing its potential for enhancing language-based brain-computer\ninterfaces."}
{"id": "2410.16638", "pdf": "https://arxiv.org/pdf/2410.16638.pdf", "abs": "https://arxiv.org/abs/2410.16638", "title": "LLMScan: Causal Scan for LLM Misbehavior Detection", "authors": ["Mengdi Zhang", "Kai Kiat Goh", "Peixin Zhang", "Jun Sun", "Rose Lin Xin", "Hongyu Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks."}
{"id": "2410.17462", "pdf": "https://arxiv.org/pdf/2410.17462.pdf", "abs": "https://arxiv.org/abs/2410.17462", "title": "Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation", "authors": ["Minhua Lin", "Zhengzhang Chen", "Yanchi Liu", "Xujiang Zhao", "Zongyu Wu", "Junxiang Wang", "Xiang Zhang", "Suhang Wang", "Haifeng Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "29 pages, 12 figures, 32 tables", "summary": "Time series data is ubiquitous across various domains, including\nmanufacturing, finance, and healthcare. High-quality annotations are essential\nfor effectively understanding time series and facilitating downstream tasks;\nhowever, obtaining such annotations is challenging, particularly in\nmission-critical domains. In this paper, we propose TESSA, a multi-agent system\ndesigned to automatically generate both general and domain-specific annotations\nfor time series data. TESSA introduces two agents: a general annotation agent\nand a domain-specific annotation agent. The general agent captures common\npatterns and knowledge across multiple source domains, leveraging both\ntime-series-wise and text-wise features to generate general annotations.\nMeanwhile, the domain-specific agent utilizes limited annotations from the\ntarget domain to learn domain-specific terminology and generate targeted\nannotations. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate that TESSA effectively generates high-quality annotations,\noutperforming existing methods."}
{"id": "2411.19939", "pdf": "https://arxiv.org/pdf/2411.19939.pdf", "abs": "https://arxiv.org/abs/2411.19939", "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety", "authors": ["Xuhao Hu", "Dongrui Liu", "Hao Li", "Xuanjing Huang", "Jing Shao"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "comment": "ACL2025 Main", "summary": "Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counterintuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs aligned with\nimage text pairs. To explain such a phenomenon, we discover a Visual Safety\nInformation Leakage (VSIL) problem in existing multimodal safety benchmarks,\ni.e., the potentially risky content in the image has been revealed in the\ntextual query. Thus, MLLMs can easily refuse these sensitive image-text pairs\naccording to textual queries only, leading to unreliable cross-modality safety\nevaluation of MLLMs. We also conduct a further comparison experiment between\ntextual alignment and multimodal alignment to highlight this drawback. To this\nend, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k\nimage-text pairs through an automated data pipeline. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically\ncompare textual and multimodal alignment methods on VLSBench and find that\ntextual alignment is effective enough for multimodal safety scenarios with\nVSIL, while multimodal alignment is preferable for safety scenarios without\nVSIL. Code and data are released under https://github.com/AI45Lab/VLSBench"}
{"id": "2412.05723", "pdf": "https://arxiv.org/pdf/2412.05723.pdf", "abs": "https://arxiv.org/abs/2412.05723", "title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language Models", "authors": ["Haizhou Shi", "Yibin Wang", "Ligong Han", "Huan Zhang", "Hao Wang"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "Pre-print; Accepted (non-archivally) at ICLR'25 Workshop: \"Quantify\n  Uncertainty and Hallucination in Foundation Models: The Next Frontier in\n  Reliable AI\"", "summary": "Estimating the uncertainty of responses from Large Language Models (LLMs)\nremains a critical challenge. While recent Bayesian methods have demonstrated\neffectiveness in quantifying uncertainty through low-rank weight updates, they\ntypically require complex fine-tuning or post-training procedures. In this\npaper, we propose Training-Free Bayesianization (TFB), a simple yet\ntheoretically grounded framework that efficiently transforms trained low-rank\nadapters into Bayesian ones without additional training. TFB systematically\nsearches for the maximally acceptable level of variance in the weight\nposterior, constrained within a family of low-rank isotropic Gaussian\ndistributions. Our theoretical analysis shows that under mild conditions, this\nsearch process is equivalent to KL-regularized variational optimization, a\ngeneralized form of variational inference. Through comprehensive experiments,\nwe show that TFB achieves superior uncertainty estimation and generalization\ncompared to existing methods while eliminating the need for complex\nBayesianization training procedures. Code will be available at\nhttps://github.com/Wang-ML-Lab/bayesian-peft."}
{"id": "2412.06141", "pdf": "https://arxiv.org/pdf/2412.06141.pdf", "abs": "https://arxiv.org/abs/2412.06141", "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization", "authors": ["Kangyu Zhu", "Peng Xia", "Yun Li", "Hongtu Zhu", "Sheng Wang", "Huaxiu Yao"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "The advancement of Large Vision-Language Models (LVLMs) has propelled their\napplication in the medical field. However, Medical LVLMs (Med-LVLMs) encounter\nfactuality challenges due to modality misalignment, where the models prioritize\ntextual knowledge over visual input, leading to hallucinations that contradict\ninformation in medical images. Previous attempts to enhance modality alignment\nin Med-LVLMs through preference optimization have inadequately mitigated\nclinical relevance in preference data, making these samples easily\ndistinguishable and reducing alignment effectiveness. To address this\nchallenge, we propose MMedPO, a novel multimodal medical preference\noptimization approach that considers the clinical relevance of preference\nsamples to enhance Med-LVLM alignment. MMedPO curates multimodal preference\ndata by introducing two types of dispreference: (1) plausible hallucinations\ninjected through target Med-LVLMs or GPT-4o to produce medically inaccurate\nresponses, and (2) lesion region neglect achieved through local lesion-noising,\ndisrupting visual understanding of critical areas. We then calculate clinical\nrelevance for each sample based on scores from multiple Med-LLMs and visual\ntools, and integrate these scores into the preference optimization process as\nweights, enabling effective alignment. Our experiments demonstrate that MMedPO\nsignificantly enhances factual accuracy in Med-LVLMs, achieving substantial\nimprovements over existing preference optimization methods by averaging 14.2%\nand 51.7% across the Med-VQA and report generation tasks. Our code are\navailable in https://github.com/aiming-lab/MMedPO."}
{"id": "2412.10849", "pdf": "https://arxiv.org/pdf/2412.10849.pdf", "abs": "https://arxiv.org/abs/2412.10849", "title": "Superhuman performance of a large language model on the reasoning tasks of a physician", "authors": ["Peter G. Brodeur", "Thomas A. Buckley", "Zahir Kanjee", "Ethan Goh", "Evelyn Bin Ling", "Priyank Jain", "Stephanie Cabral", "Raja-Elie Abdulnour", "Adrian D. Haimovich", "Jason A. Freed", "Andrew Olson", "Daniel J. Morgan", "Jason Hom", "Robert Gallo", "Liam G. McCoy", "Haadi Mombini", "Christopher Lucas", "Misha Fotoohi", "Matthew Gwiazdon", "Daniele Restifo", "Daniel Restrepo", "Eric Horvitz", "Jonathan Chen", "Arjun K. Manrai", "Adam Rodman"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials."}
{"id": "2501.04568", "pdf": "https://arxiv.org/pdf/2501.04568.pdf", "abs": "https://arxiv.org/abs/2501.04568", "title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision", "authors": ["Giorgio Giannone", "Ruoteng Li", "Qianli Feng", "Evgeny Perevodchikov", "Rui Chen", "Aleix Martinez"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Sampling-based Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on manually curated text-image pairs or preference annotation. SVP\nleverages a small set of manually selected images, self-captioning and a\npre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14 %\naverage improvement in captioning tasks, up to 12 % increase in object recall,\nand significantly reduced hallucinations, while maintaining question-answering\ncapabilities. Using SVP, a small VLM achieves hallucination reductions similar\nto a model five times larger, while a VLM with initially poor referring\ncapabilities more than doubles its performance, approaching parity with a model\ntwice its size."}
{"id": "2502.00814", "pdf": "https://arxiv.org/pdf/2502.00814.pdf", "abs": "https://arxiv.org/abs/2502.00814", "title": "Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling", "authors": ["Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Yue Wang", "Li Li", "Wengang Zhou", "Houqiang Li"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has achieved considerable\nsuccess in aligning large language models (LLMs) by modeling human preferences\nwith a learnable reward model and employing a reinforcement learning algorithm\nto maximize the reward model's scores. However, these reward models are\nsusceptible to exploitation through various superficial confounding factors,\nwith length bias emerging as a particularly significant concern. Moreover,\nwhile the pronounced impact of length bias on preference modeling suggests that\nLLMs possess an inherent sensitivity to length perception, our preliminary\ninvestigations reveal that fine-tuned LLMs consistently struggle to adhere to\nexplicit length instructions. To address these two limitations, we propose a\nnovel framework wherein the reward model explicitly differentiates between\nhuman semantic preferences and response length requirements. Specifically, we\nintroduce a $\\textbf{R}$esponse-$\\textbf{c}$onditioned\n$\\textbf{B}$radley-$\\textbf{T}$erry (Rc-BT) model that enhances the model's\ncapability in length bias mitigating and length instruction following, through\ntraining on our augmented dataset. Furthermore, we propose the Rc-RM and Rc-DPO\nalgorithm to leverage the Rc-BT model for reward modeling and direct policy\noptimization (DPO) of LLMs, simultaneously mitigating length bias and promoting\nadherence to length instructions. Extensive experiments across various\nfoundational models and datasets demonstrate the effectiveness and\ngeneralizability of our approach."}
{"id": "2502.01481", "pdf": "https://arxiv.org/pdf/2502.01481.pdf", "abs": "https://arxiv.org/abs/2502.01481", "title": "Explaining Context Length Scaling and Bounds for Language Models", "authors": ["Jingzhe Shi", "Qinwei Ma", "Hongyi Liu", "Hang Zhao", "Jeng-Neng Hwang", "Lei Li"], "categories": ["cs.LG", "cs.CL"], "comment": "30 pages, 13 figures, 2 tables", "summary": "Long Context Language Models have drawn great attention in the past few\nyears. There has been work discussing the impact of long context on Language\nModel performance: some find that long irrelevant context could harm\nperformance, while some experimentally summarize loss reduction by relevant\nlong context as Scaling Laws. This calls for a more thorough understanding on\nhow long context impacts Language Modeling. In this work, we (1) propose a\nclean and effective theoretical framework for explaining the impact of context\nlength on Language Modeling, from an Intrinsic Space perspective; and (2)\nconduct experiments on natural language and synthetic data, validating our\nproposed theoretical assumptions and deductions. Our theoretical framework can\nprovide practical insights such as establishing that training dataset size\ndictates an optimal context length and bounds context length scaling for\ncertain cases. We hope our work may inspire new long context Language Models,\nas well as future work studying Physics for Language Models. Code for our\nexperiments is available at:\nhttps://github.com/JingzheShi/NLPCtlScalingAndBounds."}
{"id": "2502.02790", "pdf": "https://arxiv.org/pdf/2502.02790.pdf", "abs": "https://arxiv.org/abs/2502.02790", "title": "Leveraging the true depth of LLMs", "authors": ["Ramón Calvo González", "Daniele Paliotta", "Matteo Pagliardini", "Martin Jaggi", "François Fleuret"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities at the cost\nof high compute requirements. Recent studies have demonstrated that\nintermediate layers in LLMs can be removed or reordered without substantial\naccuracy loss; however, this insight has not yet been exploited to improve\ninference efficiency. Leveraging observed layer independence, we propose a\nnovel method that groups consecutive layers into pairs evaluated in parallel,\neffectively restructuring the computational graph to enhance parallelism.\nWithout requiring retraining or fine-tuning, this approach achieves an\ninference throughput improvement of 1.05x-1.20x on standard benchmarks,\nretaining 95\\%-99\\% of the original model accuracy. Empirical results\ndemonstrate the practicality of this method in significantly reducing inference\ncost for large-scale LLM deployment. Additionally, we demonstrate that modest\nperformance degradation can be substantially mitigated through lightweight\nfine-tuning, further enhancing the method's applicability."}
{"id": "2502.09620", "pdf": "https://arxiv.org/pdf/2502.09620.pdf", "abs": "https://arxiv.org/abs/2502.09620", "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs", "authors": ["Yiwen Tang", "Zoey Guo", "Zhuhao Wang", "Ray Zhang", "Qizhi Chen", "Junli Liu", "Delin Qu", "Zhigang Wang", "Dong Wang", "Xuelong Li", "Bin Zhao"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "The code is released at https://github.com/Ivan-Tang-3D/ENEL", "summary": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\nalleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM layers to focus on\nthe local details of the point clouds. To the end, we present the first\nEncoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art\nmodel, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL"}
{"id": "2502.11799", "pdf": "https://arxiv.org/pdf/2502.11799.pdf", "abs": "https://arxiv.org/abs/2502.11799", "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning", "authors": ["Peiying Yu", "Guoxin Chen", "Jingjing Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025 Main", "summary": "Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate."}
{"id": "2502.15359", "pdf": "https://arxiv.org/pdf/2502.15359.pdf", "abs": "https://arxiv.org/abs/2502.15359", "title": "ARS: Automatic Routing Solver with Large Language Models", "authors": ["Kai Li", "Fei Liu", "Zhenkun Wang", "Xialiang Tong", "Xiongwei Han", "Mingxuan Yuan", "Qingfu Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": "Authorship is under discussion; arXiv release will follow\n  finalization", "summary": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks."}
{"id": "2502.16565", "pdf": "https://arxiv.org/pdf/2502.16565.pdf", "abs": "https://arxiv.org/abs/2502.16565", "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems", "authors": ["Zengqing Wu", "Takayuki Ito"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "Source codes are available at\n  https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS", "summary": "Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making."}
{"id": "2502.20034", "pdf": "https://arxiv.org/pdf/2502.20034.pdf", "abs": "https://arxiv.org/abs/2502.20034", "title": "Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore", "authors": ["Hongseok Oh", "Wonseok Hwang"], "categories": ["cs.CV", "cs.CL"], "comment": "4 pages", "summary": "Recently, Large Vision-Language Models (LVLMs) show remarkable performance\nacross various domains. However, these models suffer from object hallucination.\nThis study revisits the previous claim that the primary cause of such\nhallucination lies in the limited representational capacity of the vision\nencoder. Our analysis reveals that the capacity of the vision encoder itself is\nalready adequate for detecting object hallucination. Based on this insight, we\npropose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective\nevaluation metric that enhances object-level granularity by incorporating text\nembeddings at the noun level. Evaluations on the OHD-Caps benchmark show that\nF-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a\nlarge margin of 39.6\\% without additional training. We further demonstrate that\nF-CLIPScore-based data filtering reduces object hallucination in LVLMs (4.9\\%\nin POPE)."}
{"id": "2502.20854", "pdf": "https://arxiv.org/pdf/2502.20854.pdf", "abs": "https://arxiv.org/abs/2502.20854", "title": "A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation", "authors": ["Xujie Yuan", "Yongxu Liu", "Shimin Di", "Shiwen Wu", "Libin Zheng", "Rui Meng", "Lei Chen", "Xiaofang Zhou", "Jian Yin"], "categories": ["cs.AI", "cs.CL"], "comment": "9 pages, 2 figures, 19 tables", "summary": "The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 9 datasets in diverse domains and scenarios, analyzing the\nimpact of 9 KG-RAG configurations in combination with 17 LLMs, and combining\nMetacognition with KG-RAG as a pilot attempt. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components."}
{"id": "2503.01711", "pdf": "https://arxiv.org/pdf/2503.01711.pdf", "abs": "https://arxiv.org/abs/2503.01711", "title": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment", "authors": ["Weicong Qin", "Yi Xu", "Weijie Yu", "Chenglei Shen", "Ming He", "Jianping Fan", "Xiao Zhang", "Jun Xu"], "categories": ["cs.IR", "cs.CL"], "comment": "accepted to ACL 2025 main conference", "summary": "Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks."}
{"id": "2503.13139", "pdf": "https://arxiv.org/pdf/2503.13139.pdf", "abs": "https://arxiv.org/abs/2503.13139", "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding", "authors": ["Weiyu Guo", "Ziyang Chen", "Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Jinhui Ye", "Ying Sun", "Hui Xiong"], "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "comment": "32 pages, under review", "summary": "Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to \"finding a needle in a haystack.\"\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable."}
{"id": "2503.18225", "pdf": "https://arxiv.org/pdf/2503.18225.pdf", "abs": "https://arxiv.org/abs/2503.18225", "title": "DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation", "authors": ["Massimo Bini", "Leander Girrbach", "Zeynep Akata"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA."}
{"id": "2503.24260", "pdf": "https://arxiv.org/pdf/2503.24260.pdf", "abs": "https://arxiv.org/abs/2503.24260", "title": "MaintainCoder: Maintainable Code Generation Under Dynamic Requirements", "authors": ["Zhengren Wang", "Rui Ling", "Chufan Wang", "Yongan Yu", "Sizhe Wang", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "categories": ["cs.SE", "cs.CL"], "comment": "https://github.com/IAAR-Shanghai/MaintainCoder", "summary": "Modern code generation has made significant strides in functional correctness\nand execution efficiency. However, these systems often overlook a critical\ndimension in real-world software development: \\textit{maintainability}. To\nhandle dynamic requirements with minimal rework, we propose\n\\textbf{MaintainCoder} as a pioneering solution. It integrates the Waterfall\nmodel, design patterns, and multi-agent collaboration to systematically enhance\ncohesion, reduce coupling, achieving clear responsibility boundaries and better\nmaintainability. We also introduce \\textbf{MaintainBench}, a benchmark\ncomprising requirement changes and novel dynamic metrics on maintenance\nefforts. Experiments demonstrate that existing code generation methods struggle\nto meet maintainability standards when requirements evolve. In contrast,\nMaintainCoder improves dynamic maintainability metrics by more than 60\\% with\neven higher correctness of initial codes. Furthermore, while static metrics\nfail to accurately reflect maintainability and even contradict each other, our\nproposed dynamic metrics exhibit high consistency. Our work not only provides\nthe foundation for maintainable code generation, but also highlights the need\nfor more realistic and comprehensive code generation research."}
{"id": "2503.24370", "pdf": "https://arxiv.org/pdf/2503.24370.pdf", "abs": "https://arxiv.org/abs/2503.24370", "title": "Effectively Controlling Reasoning Models through Thinking Intervention", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "G. Edward Suh", "Prateek Mittal"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We find that the\nThinking Intervention paradigm enhances the capabilities of reasoning models\nacross a wide range of tasks, including instruction following on IFEval,\ninstruction hierarchy on SEP, and safety alignment on XSTest and SorryBench.\nOur results demonstrate that Thinking Intervention significantly outperforms\nbaseline prompting approaches, achieving up to 6.7% accuracy gains in\ninstruction-following scenarios, 15.4% improvements in reasoning about\ninstruction hierarchies, and a 40.0% increase in refusal rates for unsafe\nprompts using open-source DeepSeek R1 models. Overall, our work opens a\npromising new research avenue for controlling reasoning LLMs."}
{"id": "2504.04453", "pdf": "https://arxiv.org/pdf/2504.04453.pdf", "abs": "https://arxiv.org/abs/2504.04453", "title": "Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation", "authors": ["Mohammad Amaan Sayeed", "Engin Tekin", "Maryam Nadeem", "Nancy A. ElNaker", "Aahan Singh", "Natalia Vassilieva", "Boulbaba Ben Amor"], "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Unlocking the next generation of biotechnology and therapeutic innovation\ndemands overcoming the inherent complexity and resource-intensity of\nconventional protein engineering methods. Recent GenAI-powered computational\ntechniques often rely on the availability of the target protein's 3D structures\nand specific binding sites to generate high-affinity binders, constraints\nexhibited by models such as AlphaProteo and RFdiffusion. In this work, we\nexplore the use of Protein Language Models (pLMs) for high-affinity binder\ngeneration. We introduce Prot42, a novel family of Protein Language Models\n(pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing\ndeep evolutionary, structural, and functional insights through an advanced\nauto-regressive, decoder-only architecture inspired by breakthroughs in natural\nlanguage processing, Prot42 dramatically expands the capabilities of\ncomputational protein design based on language only. Remarkably, our models\nhandle sequences up to 8,192 amino acids, significantly surpassing standard\nlimitations and enabling precise modeling of large proteins and complex\nmulti-domain sequences. Demonstrating powerful practical applications, Prot42\nexcels in generating high-affinity protein binders and sequence-specific\nDNA-binding proteins. Our innovative models are publicly available, offering\nthe scientific community an efficient and precise computational toolkit for\nrapid protein engineering."}
{"id": "2504.14107", "pdf": "https://arxiv.org/pdf/2504.14107.pdf", "abs": "https://arxiv.org/abs/2504.14107", "title": "Signatures of human-like processing in Transformer forward passes", "authors": ["Jennifer Hu", "Michael A. Lepori", "Michael Franke"], "categories": ["cs.AI", "cs.CL"], "comment": "under review", "summary": "Modern AI models are increasingly being used as theoretical tools to study\nhuman cognition. One dominant approach is to evaluate whether human-derived\nmeasures are predicted by a model's output: that is, the end-product of a\nforward pass. However, recent advances in mechanistic interpretability have\nbegun to reveal the internal processes that give rise to model outputs, raising\nthe question of whether models might use human-like processing strategies.\nHere, we investigate the relationship between real-time processing in humans\nand layer-time dynamics of computation in Transformers, testing 20 open-source\nmodels in 6 domains. We first explore whether forward passes show mechanistic\nsignatures of competitor interference, taking high-level inspiration from\ncognitive theories. We find that models indeed appear to initially favor a\ncompeting incorrect answer in the cases where we would expect decision conflict\nin humans. We then systematically test whether forward-pass dynamics predict\nsignatures of processing in humans, above and beyond properties of the model's\noutput probability distribution. We find that dynamic measures improve\nprediction of human processing measures relative to static final-layer\nmeasures. Moreover, across our experiments, larger models do not always show\nmore human-like processing patterns. Our work suggests a new way of using AI\nmodels to study human cognition: not just as a black box mapping stimuli to\nresponses, but potentially also as explicit processing models."}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858.pdf", "abs": "https://arxiv.org/abs/2504.14858", "title": "AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a widely adopted paradigm for\nenabling knowledge-grounded large language models (LLMs). However, standard RAG\npipelines often fail to ensure that model reasoning remains consistent with the\nevidence retrieved, leading to factual inconsistencies or unsupported\nconclusions. In this work, we reinterpret RAG as Retrieval-Augmented Reasoning\nand identify a central but underexplored problem: \\textit{Reasoning\nMisalignment}-the divergence between an LLM's internal reasoning trajectory and\nthe evidential constraints provided by retrieval. To address this issue, we\npropose \\textsc{AlignRAG}, a novel iterative framework grounded in\nCritique-Driven Alignment (CDA). At the heart of \\textsc{AlignRAG} lies a\n\\textit{contrastive critique synthesis} mechanism that generates\nretrieval-sensitive critiques while mitigating self-bias. This mechanism trains\na dedicated retrieval-augmented \\textit{Critic Language Model (CLM)} using\nlabeled critiques that distinguish between evidence-aligned and misaligned\nreasoning. Alignment signals for supervision are obtained through\nself-supervised or externally guided labeling strategies. The resulting CLM is\nexplicitly optimized for evidence sensitivity, enabling it to detect and revise\nreasoning errors during inference without relying solely on self-generated\nfeedback. Empirical evaluations show that our 8B-parameter CLM improves\nperformance over the Self-Refine baseline by 12.1\\% on out-of-domain tasks and\noutperforms a standard 72B-parameter CLM by 2.2\\%, while remaining compatible\nwith existing RAG architectures as a plug-and-play module. Overall, AlignRAG\noffers a principled solution for aligning model reasoning with retrieved\nevidence, substantially improving the factual reliability and robustness of RAG\nsystems."}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585.pdf", "abs": "https://arxiv.org/abs/2504.15585", "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Yu Wang", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Wenjie Qu", "Yue Liu", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Zhaoxin Fan", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Yingxin Lai", "Zitong Yu", "Xinfeng Li", "Yifan Jiang", "Yanhui Li", "Xinyu Deng", "Junlin Wu", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Qiufeng Wang", "Wenxuan Wang", "Dongrui Liu", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Heng Chang", "Tianlin Li", "Yi Yu", "Chenghao Li", "Jiawei Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Jiaheng Zhang", "Tianwei Zhang", "Xingjun Ma", "Jindong Gu", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Lingjuan Lyu", "Yuval Elovici", "Bhavya Kailkhura", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."}
{"id": "2504.16828", "pdf": "https://arxiv.org/pdf/2504.16828.pdf", "abs": "https://arxiv.org/abs/2504.16828", "title": "Process Reward Models That Think", "authors": ["Muhammad Khalifa", "Rishabh Agarwal", "Lajanugen Logeswaran", "Jaekyeom Kim", "Hao Peng", "Moontae Lee", "Honglak Lee", "Lu Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm."}
{"id": "2505.00234", "pdf": "https://arxiv.org/pdf/2505.00234.pdf", "abs": "https://arxiv.org/abs/2505.00234", "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Improving Large Language Model (LLM) agents for sequential decision-making\ntasks typically requires extensive task-specific knowledge engineering--custom\nprompts, curated examples, and specialized observation/action spaces. We\ninvestigate a different approach where agents automatically improve by learning\nfrom their own successful experiences without human intervention. Our method\nconstructs and refines a database of self-generated trajectories that serve as\nin-context examples for future tasks. Even naive accumulation of successful\ntrajectories yields substantial performance gains across three diverse\nbenchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL\n(75% to 79%). These improvements exceed those achieved by upgrading from\ngpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts\nper task. We further enhance this approach with two innovations: database-level\ncuration using population-based training to propagate high-performing example\ncollections, and exemplar-level curation that selectively retains trajectories\nbased on their empirical utility as in-context examples. With these\nenhancements, our method achieves 93% success on ALFWorld--surpassing\napproaches that use more powerful LLMs and hand-crafted components. Our\ntrajectory bootstrapping technique demonstrates that agents can autonomously\nimprove through experience, offering a scalable alternative to labor-intensive\nknowledge engineering."}
{"id": "2505.04364", "pdf": "https://arxiv.org/pdf/2505.04364.pdf", "abs": "https://arxiv.org/abs/2505.04364", "title": "Benchmarking LLMs' Swarm intelligence", "authors": ["Kai Ruan", "Mowen Huang", "Ji-Rong Wen", "Hao Sun"], "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict swarm-like constraints-limited local perception and\ncommunication-remains largely unexplored. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination when agents\noperate with incomplete spatio-temporal information. To bridge this gap, we\nintroduce SwarmBench, a novel benchmark designed to systematically evaluate the\nswarm intelligence capabilities of LLMs acting as decentralized agents.\nSwarmBench features five foundational MAS coordination tasks (Pursuit,\nSynchronization, Foraging, Flocking, Transport) within a configurable 2D grid\nenvironment, forcing agents to rely solely on local sensory input ($k\\times k$\nview) and local communication. We propose metrics for coordination\neffectiveness and analyze emergent group dynamics. Zero-shot evaluations of\nleading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent\nperformance variations. While some rudimentary coordination is observed, our\nresults indicate that current LLMs significantly struggle with robust\nlong-range planning and adaptive strategy formation under the uncertainty\ninherent in these decentralized scenarios. Assessing LLMs under such swarm-like\nconstraints is crucial for understanding their utility in future decentralized\nintelligent systems. We release SwarmBench as an open, extensible toolkit-built\non a customizable physical system-providing environments, prompts, evaluation\nscripts, and comprehensive datasets. This aims to foster reproducible research\ninto LLM-based MAS coordination and the theoretical underpinnings of emergent\ncollective behavior under severe informational decentralization. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."}
{"id": "2505.08638", "pdf": "https://arxiv.org/pdf/2505.08638.pdf", "abs": "https://arxiv.org/abs/2505.08638", "title": "TRAIL: Trace Reasoning and Agentic Issue Localization", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Jitin Krishnan", "Anand Kannappan", "Rebecca Qian"], "categories": ["cs.AI", "cs.CL"], "comment": "Dataset: https://huggingface.co/datasets/PatronusAI/TRAIL", "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."}
{"id": "2505.09436", "pdf": "https://arxiv.org/pdf/2505.09436.pdf", "abs": "https://arxiv.org/abs/2505.09436", "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "authors": ["Raghav Garg", "Kapil Sharma", "Karan Gupta"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques."}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117.pdf", "abs": "https://arxiv.org/abs/2505.10117", "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments."}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465.pdf", "abs": "https://arxiv.org/abs/2505.10465", "title": "Superposition Yields Robust Neural Scaling", "authors": ["Yizhou Liu", "Ziming Liu", "Jeff Gore"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures, with corrections", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters."}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526.pdf", "abs": "https://arxiv.org/abs/2505.10526", "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pages, 4 figures, 3 tables. Supplementary: 1 page", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs."}
{"id": "2505.10597", "pdf": "https://arxiv.org/pdf/2505.10597.pdf", "abs": "https://arxiv.org/abs/2505.10597", "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment", "authors": ["Jiazheng Zhang", "Wenqing Jing", "Zizhuo Zhang", "Zhiheng Xi", "Shihan Dou", "Rongxiang Weng", "Jiahuan Li", "Jingang Wang", "Mingxu Chai", "Shibo Hong", "Tao Gui", "Qi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human values. However, noisy preferences in human feedback can lead\nto reward misgeneralization - a phenomenon where reward models learn spurious\ncorrelations or overfit to noisy preferences, which poses important challenges\nto the generalization of RMs. This paper systematically analyzes the\ncharacteristics of preference pairs and aims to identify how noisy preferences\ndiffer from human-aligned preferences in reward modeling. Our analysis reveals\nthat noisy preferences are difficult for RMs to fit, as they cause sharp\ntraining fluctuations and irregular gradient updates. These distinctive\ndynamics suggest the feasibility of identifying and excluding such noisy\npreferences. Empirical studies demonstrate that policy LLM optimized with a\nreward model trained on the full preference dataset, which includes substantial\nnoise, performs worse than the one trained on a subset of exclusively high\nquality preferences. To address this challenge, we propose an online\nCollaborative Reward Modeling (CRM) framework to achieve robust preference\nlearning through peer review and curriculum learning. In particular, CRM\nmaintains two RMs that collaboratively filter potential noisy preferences by\npeer-reviewing each other's data selections. Curriculum learning synchronizes\nthe capabilities of two models, mitigating excessive disparities to promote the\nutility of peer review. Extensive experiments demonstrate that CRM\nsignificantly enhances RM generalization, with up to 9.94 points improvement on\nRewardBench under an extreme 40\\% noise. Moreover, CRM can seamlessly extend to\nimplicit-reward alignment methods, offering a robust and versatile alignment\nstrategy."}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831.pdf", "abs": "https://arxiv.org/abs/2505.10831", "title": "Creating General User Models from Computer Use", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture user knowledge and preferences. GUMs can infer that a user is preparing\nfor a wedding they're attending from messages with a friend. Or recognize that\na user is struggling with a collaborator's feedback on a draft by observing\nmultiple stalled edits and a switch to reading related work. GUMs introduce an\narchitecture that infers new propositions about a user from multimodal\nobservations, retrieves related propositions for context, and continuously\nrevises existing propositions. To illustrate the breadth of applications that\nGUMs enable, we demonstrate how they augment chat-based assistants with\ncontext, manage OS notifications to selectively surface important information,\nand enable interactive agents that adapt to preferences across apps. We also\ninstantiate proactive assistants (GUMBOs) that discover and execute useful\nsuggestions on a user's behalf using their GUM. In our evaluations, we find\nthat GUMs make calibrated and accurate inferences about users, and that\nassistants built on GUMs proactively identify and perform actions that users\nwouldn't think to request explicitly. Altogether, GUMs introduce methods that\nleverage multimodal models to understand unstructured context, enabling\nlong-standing visions of HCI and entirely new interactive systems that\nanticipate user needs."}
{"id": "2505.10872", "pdf": "https://arxiv.org/pdf/2505.10872.pdf", "abs": "https://arxiv.org/abs/2505.10872", "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?", "authors": ["Chenxi Jiang", "Chuhao Zhou", "Jianfei Yang"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children."}
{"id": "2505.11365", "pdf": "https://arxiv.org/pdf/2505.11365.pdf", "abs": "https://arxiv.org/abs/2505.11365", "title": "Phare: A Safety Probe for Large Language Models", "authors": ["Pierre Le Jeune", "Benoît Malézieux", "Weixuan Xiao", "Matteo Dora"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems."}
