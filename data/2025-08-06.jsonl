{"id": "2508.02679", "pdf": "https://arxiv.org/pdf/2508.02679.pdf", "abs": "https://arxiv.org/abs/2508.02679", "title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "categories": ["cs.HC"], "comment": null, "summary": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health."}
{"id": "2508.02680", "pdf": "https://arxiv.org/pdf/2508.02680.pdf", "abs": "https://arxiv.org/abs/2508.02680", "title": "AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI", "authors": ["Pragya Singh", "Ankush Gupta", "Mohan Kumar", "Pushpendra Singh"], "categories": ["cs.HC", "cs.AI"], "comment": "To be published in IMWUT, September 2025", "summary": "Emotional and mental well-being are vital components of quality of life, and\nwith the rise of smart devices like smartphones, wearables, and artificial\nintelligence (AI), new opportunities for monitoring emotions in everyday\nsettings have emerged. However, for AI algorithms to be effective, they require\nhigh-quality data and accurate annotations. As the focus shifts towards\ncollecting emotion data in real-world environments to capture more authentic\nemotional experiences, the process of gathering emotion annotations has become\nincreasingly complex. This work explores the challenges of everyday emotion\ndata collection from the perspectives of key stakeholders. We collected 75\nsurvey responses, performed 32 interviews with the public, and 3 focus group\ndiscussions (FGDs) with 12 mental health professionals. The insights gained\nfrom a total of 119 stakeholders informed the development of our framework,\nAnnoSense, designed to support everyday emotion data collection for AI. This\nframework was then evaluated by 25 emotion AI experts for its clarity,\nusefulness, and adaptability. Lastly, we discuss the potential next steps and\nimplications of AnnoSense for future research in emotion AI, highlighting its\npotential to enhance the collection and analysis of emotion data in real-world\ncontexts."}
{"id": "2508.02817", "pdf": "https://arxiv.org/pdf/2508.02817.pdf", "abs": "https://arxiv.org/abs/2508.02817", "title": "Real-World Receptivity to Adaptive Mental Health Interventions: Findings from an In-the-Wild Study", "authors": ["Nilesh Kumar Sahu", "Aditya Sneh", "Snehil Gupta", "Haroon R Lone"], "categories": ["cs.HC", "cs.AI", "cs.CY", "eess.SP"], "comment": null, "summary": "The rise of mobile health (mHealth) technologies has enabled real-time\nmonitoring and intervention for mental health conditions using passively sensed\nsmartphone data. Building on these capabilities, Just-in-Time Adaptive\nInterventions (JITAIs) seek to deliver personalized support at opportune\nmoments, adapting to users' evolving contexts and needs. Although prior\nresearch has examined how context affects user responses to generic\nnotifications and general mHealth messages, relatively little work has explored\nits influence on engagement with actual mental health interventions.\nFurthermore, while much of the existing research has focused on detecting when\nusers might benefit from an intervention, less attention has been paid to\nunderstanding receptivity, i.e., users' willingness and ability to engage with\nand act upon the intervention.\n  In this study, we investigate user receptivity through two components:\nacceptance(acknowledging or engaging with a prompt) and feasibility (ability to\nact given situational constraints). We conducted a two-week in-the-wild study\nwith 70 students using a custom Android app, LogMe, which collected passive\nsensor data and active context reports to prompt mental health interventions.\nThe adaptive intervention module was built using Thompson Sampling, a\nreinforcement learning algorithm. We address four research questions relating\nsmartphone features and self-reported contexts to acceptance and feasibility,\nand examine whether an adaptive reinforcement learning approach can optimize\nintervention delivery by maximizing a combined receptivity reward. Our results\nshow that several types of passively sensed data significantly influenced user\nreceptivity to interventions. Our findings contribute insights into the design\nof context-aware, adaptive interventions that are not only timely but also\nactionable in real-world settings."}
{"id": "2508.02823", "pdf": "https://arxiv.org/pdf/2508.02823.pdf", "abs": "https://arxiv.org/abs/2508.02823", "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification", "authors": ["Wenshuo Zhang", "Leixian Shen", "Shuchang Xu", "Jindu Wang", "Jian Zhao", "Huamin Qu", "Linping Yuan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted in UIST 2025", "summary": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency."}
{"id": "2508.02808", "pdf": "https://arxiv.org/pdf/2508.02808.pdf", "abs": "https://arxiv.org/abs/2508.02808", "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns."}
{"id": "2508.02868", "pdf": "https://arxiv.org/pdf/2508.02868.pdf", "abs": "https://arxiv.org/abs/2508.02868", "title": "Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators", "authors": ["Kaixuan Wang", "Loraine Clarke", "Carl-Cyril J Dreue", "Guancheng Zhou", "Jason T. Jacques"], "categories": ["cs.HC", "cs.CY"], "comment": "22 pages", "summary": "Online communities serve as essential support channels for People Who Use\nDrugs (PWUD), providing access to peer support and harm reduction information.\nThe moderation of these communities involves consequential decisions affecting\nmember safety, yet existing sociotechnical systems provide insufficient support\nfor moderators. Through interviews with experienced moderators from PWUD forums\non Reddit, we analyse the unique nature of this work. We argue that this work\nconstitutes a distinct form of public health intervention characterised by\nthree moderation challenges: the need for specialised, expert risk assessment;\ntime-critical crisis response; and the navigation of a structural conflict\nbetween platform policies and community safety goals. We demonstrate how\ncurrent moderation systems are insufficient in supporting PWUD communities. For\nexample, policies minimising platforms' legal exposure to illicit activities\ncan inadvertently push moderators to implement restrictive rules to protect\ncommunity's existence, which can limit such a vulnerable group's ability to\nshare potentially life-saving resources online. We conclude by identifying two\nnecessary shifts in sociotechnical design to support moderators' work: first,\nmoving to automated tools that support human sensemaking in contexts with\ncompeting interests; and second, shifting from systems that require moderators\nto perform low-level rule programming to those that enable high-level,\nexample-based instruction. Further, we highlight how the design of\nsociotechnical systems in online spaces could impact harm reduction efforts\naimed at improving health outcomes for PWUD communities."}
{"id": "2508.02853", "pdf": "https://arxiv.org/pdf/2508.02853.pdf", "abs": "https://arxiv.org/abs/2508.02853", "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives", "authors": ["Yinuo Xu", "Veronica Derricks", "Allison Earl", "David Jurgens"], "categories": ["cs.CL"], "comment": "28 pages, 17 figures", "summary": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives."}
{"id": "2508.02958", "pdf": "https://arxiv.org/pdf/2508.02958.pdf", "abs": "https://arxiv.org/abs/2508.02958", "title": "VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People", "authors": ["Daniel Killough", "Justin Feng", "Zheng Xue \"ZX\" Ching", "Daniel Wang", "Rithvik Dyava", "Yapeng Tian", "Yuhang Zhao"], "categories": ["cs.HC"], "comment": "17 pages, 10 figures, 2 tables, LaTeX; To be published in ACM's 2025\n  Symposium on User Interface Software and Technology (UIST 2025)", "summary": "Virtual Reality (VR) is inaccessible to blind people. While research has\ninvestigated many techniques to enhance VR accessibility, they require\nadditional developer effort to integrate. As such, most mainstream VR apps\nremain inaccessible as the industry de-prioritizes accessibility. We present\nVRSight, an end-to-end system that recognizes VR scenes post hoc through a set\nof AI models (e.g., object detection, depth estimation, LLM-based atmosphere\ninterpretation) and generates tone-based, spatial audio feedback, empowering\nblind users to interact in VR without developer intervention. To enable virtual\nelement detection, we further contribute DISCOVR, a VR dataset consisting of 30\nvirtual object classes from 17 social VR apps, substituting real-world datasets\nthat remain not applicable to VR contexts. Nine participants used VRSight to\nexplore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in\nfacilitating social tasks like avatar awareness and available seat\nidentification."}
{"id": "2508.02872", "pdf": "https://arxiv.org/pdf/2508.02872.pdf", "abs": "https://arxiv.org/abs/2508.02872", "title": "Highlight & Summarize: RAG without the jailbreaks", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline."}
{"id": "2508.03014", "pdf": "https://arxiv.org/pdf/2508.03014.pdf", "abs": "https://arxiv.org/abs/2508.03014", "title": "Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers", "authors": ["Jingyan Wang", "Yang Zhao", "Haotian Mao", "Xubo Yang"], "categories": ["cs.HC", "I.2, H.5"], "comment": "29 pages, 5 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, and their integration with\nExtended Reality (XR) is poised to transform how users interact with immersive\nenvironments. This survey provides a comprehensive review of recent\ndevelopments at the intersection of LLMs and XR, offering a structured\norganization of research along both technical and application dimensions. We\npropose a taxonomy of LLM-enhanced XR systems centered on key technical\nparadigms -- such as interactive agent control, XR development toolkits, and\ngenerative scene synthesis -- and discuss how these paradigms enable novel\ncapabilities in XR. In parallel, we examine how LLM-driven techniques support\npractical XR applications across diverse domains, including immersive\neducation, clinical healthcare, and industrial manufacturing. By connecting\nthese technical paradigms with application frontiers, our survey highlights\ncurrent trends, delineates design considerations, and identifies open\nchallenges in building LLM-augmented XR systems. This work provides insights\nthat can guide researchers and practitioners in advancing the state of the art\nin intelligent XR experiences."}
{"id": "2508.02885", "pdf": "https://arxiv.org/pdf/2508.02885.pdf", "abs": "https://arxiv.org/abs/2508.02885", "title": "Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages", "authors": ["Elliot Murphy", "Rohan Venkatesh", "Edward Khokhlovich", "Andrey Vyshedskiy"], "categories": ["cs.CL"], "comment": null, "summary": "In the modern language sciences, the core computational operation of syntax,\n'Merge', is defined as an operation that combines two linguistic units (e.g.,\n'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).\nThis can then be further combined with additional linguistic units based on\nthis categorial information, respecting non-associativity such that abstract\ngrouping is respected. Some linguists have embraced the view that Merge is an\nelementary, indivisible operation that emerged in a single evolutionary step.\nFrom a neurocognitive standpoint, different mental objects constructed by Merge\nmay be supported by distinct mechanisms: (1) simple command constructions\n(e.g., \"eat apples\"); (2) the merging of adjectives and nouns (\"red boat\"); and\n(3) the merging of nouns with spatial prepositions (\"laptop behind the sofa\").\nHere, we systematically investigate participants' comprehension of sentences\nwith increasing levels of syntactic complexity. Clustering analyses revealed\nbehavioral evidence for three distinct structural types, which we discuss as\npotentially emerging at different developmental stages and subject to selective\nimpairment. While a Merge-based syntax may still have emerged suddenly in\nevolutionary time, responsible for the structured symbolic turn our species\ntook, different cognitive mechanisms seem to underwrite the processing of\nvarious types of Merge-based objects."}
{"id": "2508.03061", "pdf": "https://arxiv.org/pdf/2508.03061.pdf", "abs": "https://arxiv.org/abs/2508.03061", "title": "Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling", "authors": ["Shuchang Xu"], "categories": ["cs.HC"], "comment": null, "summary": "Empowering blind and low vision (BLV) users to explore visual media improves\ncontent comprehension, strengthens user agency, and fulfills diverse\ninformation needs. However, most existing tools separate exploration from the\nmain narration, which disrupts the narrative flow, increases cognitive load,\nand limits deep engagement with visual media. To address these challenges, my\nPhD research introduces the paradigm of AI-powered interactive storytelling,\nwhich leverages AI to generate interactive narratives, enabling BLV users to\nexplore visual media within a coherent storytelling experience. I have\noperationalized this paradigm through three techniques: (1) Hierarchical\nNarrative, which supports photo-collection exploration at different levels of\ndetail; (2) Parallel Narrative, which provides seamless access to time-synced\nvideo comments; and (3) Branching Narrative, which enables immersive navigation\nof 360{\\deg} videos. Together, these techniques demonstrate that AI-powered\ninteractive storytelling can effectively balance user agency with narrative\ncoherence across diverse media formats. My future work will advance this\nparadigm by enabling more personalized and expressive storytelling experiences\nfor BLV audiences."}
{"id": "2508.02886", "pdf": "https://arxiv.org/pdf/2508.02886.pdf", "abs": "https://arxiv.org/abs/2508.02886", "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models", "authors": ["Wenjie Luo", "Ruocheng Li", "Shanshan Zhu", "Julian Perry"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning."}
{"id": "2508.03182", "pdf": "https://arxiv.org/pdf/2508.03182.pdf", "abs": "https://arxiv.org/abs/2508.03182", "title": "StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation", "authors": ["Sangho Suh", "Michael Lai", "Kevin Pu", "Steven P. Dow", "Tovi Grossman"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Design processes involve exploration, iteration, and movement across\ninterconnected stages such as persona creation, problem framing, solution\nideation, and prototyping. However, time and resource constraints often hinder\ndesigners from exploring broadly, collecting feedback, and revisiting earlier\nassumptions-making it difficult to uphold core design principles in practice.\nTo better understand these challenges, we conducted a formative study with 15\nparticipants-comprised of UX practitioners, students, and instructors. Based on\nthe findings, we developed StoryEnsemble, a tool that integrates AI into a\nnode-link interface and leverages forward and backward propagation to support\ndynamic exploration and iteration across the design process. A user study with\n10 participants showed that StoryEnsemble enables rapid, multi-directional\niteration and flexible navigation across design stages. This work advances our\nunderstanding of how AI can foster more iterative design practices by\nintroducing novel interactions that make exploration and iteration more fluid,\naccessible, and engaging."}
{"id": "2508.02901", "pdf": "https://arxiv.org/pdf/2508.02901.pdf", "abs": "https://arxiv.org/abs/2508.02901", "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations", "authors": ["Osama Khalid", "Sanvesh Srivastava", "Padmini Srinivasan"], "categories": ["cs.CL"], "comment": null, "summary": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%."}
{"id": "2508.03216", "pdf": "https://arxiv.org/pdf/2508.03216.pdf", "abs": "https://arxiv.org/abs/2508.03216", "title": "Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse", "authors": ["Hikari Yanagawa", "Yuichi Hiroi", "Satomi Tokida", "Yuji Hatada", "Takefumi Hiraki"], "categories": ["cs.HC", "cs.AI"], "comment": "11 pages + supplement 3 pages. To appear in IEEE ISMAR 2025", "summary": "While commercial metaverse platforms offer diverse user-generated content,\nthey lack effective navigation assistance that can dynamically adapt to users'\ninterests and intentions. Although previous research has investigated on-demand\nagents in controlled environments, implementation in commercial settings with\ndiverse world configurations and platform constraints remains challenging.\n  We present Navigation Pixie, an on-demand navigation agent employing a\nloosely coupled architecture that integrates structured spatial metadata with\nLLM-based natural language processing while minimizing platform dependencies,\nwhich enables experiments on the extensive user base of commercial metaverse\nplatforms. Our cross-platform experiments on commercial metaverse platform\nCluster with 99 PC client and 94 VR-HMD participants demonstrated that\nNavigation Pixie significantly increased dwell time and free exploration\ncompared to fixed-route and no-agent conditions across both platforms.\nSubjective evaluations revealed consistent on-demand preferences in PC\nenvironments versus context-dependent social perception advantages in VR-HMD.\nThis research contributes to advancing VR interaction design through\nconversational spatial navigation agents, establishes cross-platform evaluation\nmethodologies revealing environment-dependent effectiveness, and demonstrates\nempirical experimentation frameworks for commercial metaverse platforms."}
{"id": "2508.02931", "pdf": "https://arxiv.org/pdf/2508.02931.pdf", "abs": "https://arxiv.org/abs/2508.02931", "title": "Can LLMs Generate High-Quality Task-Specific Conversations?", "authors": ["Shengqi Li", "Amarnath Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation."}
{"id": "2508.03281", "pdf": "https://arxiv.org/pdf/2508.03281.pdf", "abs": "https://arxiv.org/abs/2508.03281", "title": "Quo-Vadis Multi-Agent Automotive Research? Insights from a Participatory Workshop and Questionnaire", "authors": ["Pavlo Bazilinskyy", "Francesco Walker", "Debargha Dey", "Tram Thi Minh Tran", "Hyungchai Park", "Hyochang Kim", "Hyunmin Kang", "Patrick Ebel"], "categories": ["cs.HC"], "comment": null, "summary": "The transition to mixed-traffic environments that involve automated vehicles,\nmanually operated vehicles, and vulnerable road users presents new challenges\nfor human-centered automotive research. Despite this, most studies in the\ndomain focus on single-agent interactions. This paper reports on a\nparticipatory workshop (N = 15) and a questionnaire (N = 19) conducted during\nthe AutomotiveUI '24 conference to explore the state of multi-agent automotive\nresearch. The participants discussed methodological challenges and\nopportunities in real-world settings, simulations, and computational modeling.\nKey findings reveal that while the value of multi-agent approaches is widely\nrecognized, practical and technical barriers hinder their implementation. The\nstudy highlights the need for interdisciplinary methods, better tools, and\nsimulation environments that support scalable, realistic, and ethically\ninformed multi-agent research."}
{"id": "2508.02997", "pdf": "https://arxiv.org/pdf/2508.02997.pdf", "abs": "https://arxiv.org/abs/2508.02997", "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available."}
{"id": "2508.03293", "pdf": "https://arxiv.org/pdf/2508.03293.pdf", "abs": "https://arxiv.org/abs/2508.03293", "title": "Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach", "authors": ["Duc-An Nguyen", "Clara Colombatto", "Steve Fleming", "Ingmar Posner", "Nick Hawes", "Raunak Bhattacharyya"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Joint human-AI inference holds immense potential to improve outcomes in\nhuman-supervised robot missions. Current day missions are generally in the\nAI-assisted setting, where the human operator makes the final inference based\non the AI recommendation. However, due to failures in human judgement on when\nto accept or reject the AI recommendation, complementarity is rarely achieved.\nWe investigate joint human-AI inference where the inference made with higher\nconfidence is selected. Through a user study with N=100 participants on a\nrepresentative simulated robot teleoperation task, specifically studying the\ninference of robots' control delays we show that: a) Joint inference accuracy\nis higher and its extent is regulated by the confidence calibration of the AI\nagent, and b) Humans change their inferences based on AI recommendations and\nthe extent and direction of this change is also regulated by the confidence\ncalibration of the AI agent. Interestingly, our results show that pairing\npoorly-calibrated AI-DSS with humans hurts performance instead of helping the\nteam, reiterating the need for AI-based decision support systems with good\nmetacognitive sensitivity. To the best of our knowledge, our study presents the\nfirst application of a maximum-confidence-based heuristic for joint human-AI\ninference within a simulated robot teleoperation task."}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037.pdf", "abs": "https://arxiv.org/abs/2508.03037", "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape."}
{"id": "2508.03355", "pdf": "https://arxiv.org/pdf/2508.03355.pdf", "abs": "https://arxiv.org/abs/2508.03355", "title": "Remini: Leveraging Chatbot-Mediated Mutual Reminiscence for Promoting Positive Affect and Feeling of Connectedness among Loved Ones", "authors": ["Zhuoqun Jiang", "ShunYi Yeo", "Wei Xuan Donovan Seow", "Simon Perrault"], "categories": ["cs.HC"], "comment": "Camera-ready submission for PACM HCI, CSCW 2025", "summary": "Mutual reminiscence, defined as revisiting shared positive memories through\nreciprocal self-disclosure, strengthens emotional bonds, enhances well-being,\nand deepens intimacy. However, most technology-mediated reminiscence tools\nemphasize individual reflection or one-way storytelling, which overlooks the\ndynamic, interactive dialogue essential for meaningful mutual reminiscence. To\naddress this limitation, we introduce Remini, a chatbot designed to support\nreciprocal self-disclosure between close partners such as couples, friends, or\nfamily members. Grounded in the Social Functions of Autobiographical Memory\n(SFAM) framework, Remini uses conversational AI to guide emotionally rich\nexchanges through five narrative phases: rapport building, memory narration,\nelaboration, reflection, and summary. In a mixed-method, both between- and\nwithin- subjects study (N = 48, 24 dyads), we compare Remini to a baseline\nchatbot that offers minimal memory-trigger prompts. Our findings show that\nstructured guidance from Remini significantly improves positive affect, feeling\nof connection, and engagement. It also fosters more detailed narrative\nco-construction and greater reciprocal self-disclosure. Participant feedback\nhighlights the practical value, perceived benefits, and design considerations\nof chatbot-mediated reminiscence. We contribute empirically grounded design\nimplications for conversational agents that strengthen human connection through\nmutual reminiscence."}
{"id": "2508.03098", "pdf": "https://arxiv.org/pdf/2508.03098.pdf", "abs": "https://arxiv.org/abs/2508.03098", "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation", "authors": ["Haoran Wang", "Xiongxiao Xu", "Baixiang Huang", "Kai Shu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD."}
{"id": "2508.03430", "pdf": "https://arxiv.org/pdf/2508.03430.pdf", "abs": "https://arxiv.org/abs/2508.03430", "title": "The Science Fiction Science Method", "authors": ["Iyad Rahwan", "Azim Shariff", "Jean-Fran√ßois Bonnefon"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Predicting the social and behavioral impact of future technologies, before\nthey are achieved, would allow us to guide their development and regulation\nbefore these impacts get entrenched. Traditionally, this prediction has relied\non qualitative, narrative methods. Here we describe a method which uses\nexperimental methods to simulate future technologies, and collect quantitative\nmeasures of the attitudes and behaviors of participants assigned to controlled\nvariations of the future. We call this method 'science fiction science'. We\nsuggest that the reason why this method has not been fully embraced yet,\ndespite its potential benefits, is that experimental scientists may be\nreluctant to engage in work facing such serious validity threats as science\nfiction science. To address these threats, we consider possible constraints on\nthe kind of technology that science fiction science may study, as well as the\nunconventional, immersive methods that science fiction science may require. We\nseek to provide perspective on the reasons why this method has been\nmarginalized for so long, what benefits it would bring if it could be built on\nstrong yet unusual methods, and how we can normalize these methods to help the\ndiverse community of science fiction scientists to engage in a virtuous cycle\nof validity improvement."}
{"id": "2508.03110", "pdf": "https://arxiv.org/pdf/2508.03110.pdf", "abs": "https://arxiv.org/abs/2508.03110", "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness."}
{"id": "2508.03547", "pdf": "https://arxiv.org/pdf/2508.03547.pdf", "abs": "https://arxiv.org/abs/2508.03547", "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs and Vision Models", "authors": ["Ada Yi Zhao", "Aditya Gunturu", "Ellen Yi-Luen Do", "Ryo Suzuki"], "categories": ["cs.HC"], "comment": "To appear at UIST 2025", "summary": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows."}
{"id": "2508.03112", "pdf": "https://arxiv.org/pdf/2508.03112.pdf", "abs": "https://arxiv.org/abs/2508.03112", "title": "Cross-lingual Opinions and Emotions Mining in Comparable Documents", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "categories": ["cs.CL", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Comparable texts are topic-aligned documents in multiple languages that are\nnot direct translations. They are valuable for understanding how a topic is\ndiscussed across languages. This research studies differences in sentiments and\nemotions across English-Arabic comparable documents. First, texts are annotated\nwith sentiment and emotion labels. We apply a cross-lingual method to label\ndocuments with opinion classes (subjective/objective), avoiding reliance on\nmachine translation. To annotate with emotions (anger, disgust, fear, joy,\nsadness, surprise), we manually translate the English WordNet-Affect (WNA)\nlexicon into Arabic, creating bilingual emotion lexicons used to label the\ncomparable corpora. We then apply a statistical measure to assess the agreement\nof sentiments and emotions in each source-target document pair. This comparison\nis especially relevant when the documents originate from different sources. To\nour knowledge, this aspect has not been explored in prior literature. Our study\nincludes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera\n(JSC). Results show that sentiment and emotion annotations align when articles\ncome from the same news agency and diverge when they come from different ones.\nThe proposed method is language-independent and generalizable to other language\npairs."}
{"id": "2508.03630", "pdf": "https://arxiv.org/pdf/2508.03630.pdf", "abs": "https://arxiv.org/abs/2508.03630", "title": "SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides", "authors": ["Zhuohao Jerry Zhang", "Ruiqi Chen", "Mingyuan Zhong", "Jacob O. Wobbrock"], "categories": ["cs.HC"], "comment": "UIST 2025", "summary": "Automated evaluation of specific graphic designs like presentation slides is\nan open problem. We present SlideAudit, a dataset for automated slide\nevaluation. We collaborated with design experts to develop a thorough taxonomy\nof slide design flaws. Our dataset comprises 2400 slides collected and\nsynthesized from multiple sources, including a subset intentionally modified\nwith specific design problems. We then fully annotated them using our taxonomy\nthrough strictly trained crowdsourcing from Prolific. To evaluate whether AI is\ncapable of identifying design flaws, we compared multiple large language models\nunder different prompting strategies, and with an existing design critique\npipeline. We show that AI models struggle to accurately identify slide design\nflaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting\ntechniques leveraging our taxonomy achieved the highest performance. We further\nconducted a remediation study to assess AI's potential for improving slides.\nAmong 82.0% of slides that showed significant improvement, 87.8% of them were\nimproved more with our taxonomy, further demonstrating its utility."}
{"id": "2508.03137", "pdf": "https://arxiv.org/pdf/2508.03137.pdf", "abs": "https://arxiv.org/abs/2508.03137", "title": "Long Story Generation via Knowledge Graph and Literary Theory", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories."}
{"id": "2508.03651", "pdf": "https://arxiv.org/pdf/2508.03651.pdf", "abs": "https://arxiv.org/abs/2508.03651", "title": "Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired", "authors": ["Ruei-Che Chang", "Rosiana Natalie", "Wenqian Xu", "Jovan Zheng Feng Yap", "Anhong Guo"], "categories": ["cs.HC", "cs.AI"], "comment": "ACM ASSETS 2025", "summary": "Recent advancements in large multimodal models have provided blind or\nvisually impaired (BVI) individuals with new capabilities to interpret and\nengage with the real world through interactive systems that utilize live video\nfeeds. However, the potential benefits and challenges of such capabilities to\nsupport diverse real-world assistive tasks remain unclear. In this paper, we\npresent findings from an exploratory study with eight BVI participants.\nParticipants used ChatGPT's Advanced Voice with Video, a state-of-the-art live\nvideo AI released in late 2024, in various real-world scenarios, from locating\nobjects to recognizing visual landmarks, across unfamiliar indoor and outdoor\nenvironments. Our findings indicate that current live video AI effectively\nprovides guidance and answers for static visual scenes but falls short in\ndelivering essential live descriptions required in dynamic situations. Despite\ninaccuracies in spatial and distance information, participants leveraged the\nprovided visual information to supplement their mobility strategies. Although\nthe system was perceived as human-like due to high-quality voice interactions,\nassumptions about users' visual abilities, hallucinations, generic responses,\nand a tendency towards sycophancy led to confusion, distrust, and potential\nrisks for BVI users. Based on the results, we discuss implications for\nassistive video AI agents, including incorporating additional sensing\ncapabilities for real-world use, determining appropriate intervention timing\nbeyond turn-taking interactions, and addressing ecological and safety concerns."}
{"id": "2508.03140", "pdf": "https://arxiv.org/pdf/2508.03140.pdf", "abs": "https://arxiv.org/abs/2508.03140", "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior", "authors": ["Junyao Yang", "Jianwei Wang", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 7 figures", "summary": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability."}
{"id": "2508.03673", "pdf": "https://arxiv.org/pdf/2508.03673.pdf", "abs": "https://arxiv.org/abs/2508.03673", "title": "Classifying Epistemic Relationships in Human-AI Interaction: An Exploratory Approach", "authors": ["Shengnan Yang", "Rongqian Ma"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As AI systems become integral to knowledge-intensive work, questions arise\nnot only about their functionality but also their epistemic roles in human-AI\ninteraction. While HCI research has proposed various AI role typologies, it\noften overlooks how AI reshapes users' roles as knowledge contributors. This\nstudy examines how users form epistemic relationships with AI-how they assess,\ntrust, and collaborate with it in research and teaching contexts. Based on 31\ninterviews with academics across disciplines, we developed a five-part codebook\nand identified five relationship types: Instrumental Reliance, Contingent\nDelegation, Co-agency Collaboration, Authority Displacement, and Epistemic\nAbstention. These reflect variations in trust, assessment modes, tasks, and\nhuman epistemic status. Our findings show that epistemic roles are dynamic and\ncontext-dependent. We argue for shifting beyond static metaphors of AI toward a\nmore nuanced framework that captures how humans and AI co-construct knowledge,\nenriching HCI's understanding of the relational and normative dimensions of AI\nuse."}
{"id": "2508.03178", "pdf": "https://arxiv.org/pdf/2508.03178.pdf", "abs": "https://arxiv.org/abs/2508.03178", "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following", "authors": ["Chenyang Wang", "Liang Wen", "Shousheng Jia", "Xiangzheng Zhang", "Liang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 10 figures, 7 tables", "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6."}
{"id": "2508.02733", "pdf": "https://arxiv.org/pdf/2508.02733.pdf", "abs": "https://arxiv.org/abs/2508.02733", "title": "What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus", "authors": ["Rijul Jain", "Shraddha Barke", "Gabriel Ebner", "Md Rakib Hossain Misu", "Shan Lu", "Sarah Fakhoury"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Proof-oriented programming languages (POPLs) empower developers to write code\nalongside formal correctness proofs, providing formal guarantees that the code\nadheres to specified requirements. Despite their powerful capabilities, POPLs\npresent a steep learning curve and have not yet been adopted by the broader\nsoftware community. The lack of understanding about the proof-development\nprocess and how expert proof developers interact with POPLs has hindered the\nadvancement of effective proof engineering and the development of\nproof-synthesis models/tools.\n  In this work, we conduct a user study, involving the collection and analysis\nof fine-grained source code telemetry from eight experts working with two\nlanguages, F* and Verus. Results reveal interesting trends and patterns about\nhow experts reason about proofs and key challenges encountered during the proof\ndevelopment process. We identify three distinct strategies and multiple\ninformal practices that are not captured final code snapshots, yet are\npredictive of task outcomes. We translate these findings into concrete design\nguidance for AI proof assistants: bias toward early specification drafting,\nexplicit sub-goal decomposition, bounded active errors, and disciplined\nverifier interaction. We also present a case study of an F* proof agent\ngrounded in these recommendations, and demonstrate improved performance over\nbaseline LLMs"}
{"id": "2508.03181", "pdf": "https://arxiv.org/pdf/2508.03181.pdf", "abs": "https://arxiv.org/abs/2508.03181", "title": "Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification", "authors": ["Lukas P√§tz", "Moritz Beyer", "Jannik Sp√§th", "Lasse Bohlen", "Patrick Zschech", "Mathias Kraus", "Julian Rosenberger"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at 20th International Conference on Wirtschaftsinformatik\n  (WI25); September 2025, M\\\"unster, Germany", "summary": "This study investigates political discourse in the German parliament, the\nBundestag, by analyzing approximately 28,000 parliamentary speeches from the\nlast five years. Two machine learning models for topic and sentiment\nclassification were developed and trained on a manually labeled dataset. The\nmodels showed strong classification performance, achieving an area under the\nreceiver operating characteristic curve (AUROC) of 0.94 for topic\nclassification (average across topics) and 0.89 for sentiment classification.\nBoth models were applied to assess topic trends and sentiment distributions\nacross political parties and over time. The analysis reveals remarkable\nrelationships between parties and their role in parliament. In particular, a\nchange in style can be observed for parties moving from government to\nopposition. While ideological positions matter, governing responsibilities also\nshape discourse. The analysis directly addresses key questions about the\nevolution of topics, sentiment dynamics, and party-specific discourse\nstrategies in the Bundestag."}
{"id": "2508.02926", "pdf": "https://arxiv.org/pdf/2508.02926.pdf", "abs": "https://arxiv.org/abs/2508.02926", "title": "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics", "authors": ["Arthur Cho"], "categories": ["cs.LG", "cs.AI", "cs.HC", "I.2.6; I.2.7"], "comment": "26 pages, 1 table. Open-source implementation available on PyPI\n  (grandjury package) and GitHub. Dataset available on Hugging Face under\n  CC-BY-4.0 license", "summary": "Generative Machine Learning models have become central to modern systems,\npowering applications in creative writing, summarization, multi-hop reasoning,\nand context-aware dialogue. These models underpin large-scale AI assistants,\nworkflow automation, and autonomous decision-making. In such domains,\nacceptable response is rarely absolute or static, but plural and highly\ncontext-dependent. Yet standard evaluation regimes still rely on static,\nbenchmark-style tests, incentivizing optimization toward leaderboard scores\nrather than alignment with dynamic user needs or evolving realities. GrandJury\nintroduces a formal evaluation protocol combining time-decayed aggregation,\ncomplete traceability, with the support of dynamic, transparent task rubric\nattribution, and multi-rater human judgment. Together, these elements enable\npluralistic, accountable evaluation that captures evolving consensus and\nsurfaces disagreement. We provide an open-source implementation (grandjury PyPI\npackage) and a public collection of Large Language Model (LLM) inference\noutputs to illustrate the need and method. GrandJury provides a new paradigm\nfor AI practitioners when evaluating machine learning outputs without absolute\nground truth."}
{"id": "2508.03199", "pdf": "https://arxiv.org/pdf/2508.03199.pdf", "abs": "https://arxiv.org/abs/2508.03199", "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "categories": ["cs.CL"], "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems."}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037.pdf", "abs": "https://arxiv.org/abs/2508.03037", "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape."}
{"id": "2508.03204", "pdf": "https://arxiv.org/pdf/2508.03204.pdf", "abs": "https://arxiv.org/abs/2508.03204", "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP", "authors": ["Abhirup Sinha", "Pritilata Saha", "Tithi Saha"], "categories": ["cs.CL"], "comment": "To be published in the Proceedings of Die Studierendenkonferenz\n  Informatik (SKILL) 2024", "summary": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks."}
{"id": "2508.03274", "pdf": "https://arxiv.org/pdf/2508.03274.pdf", "abs": "https://arxiv.org/abs/2508.03274", "title": "Investigating the Cognitive Response of Brake Lights in Initiating Braking Action Using EEG", "authors": ["Ramaswamy Palaniappan", "Surej Mouli", "Howard Bowman", "Ian McLoughlin"], "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.IR"], "comment": "arXiv admin note: text overlap with arXiv:2010.10584", "summary": "Half of all road accidents result from either lack of driver attention or\nfrom maintaining insufficient separation between vehicles. Collision from the\nrear, in particular, has been identified as the most common class of accident\nin the UK, and its influencing factors have been widely studied for many years.\nRear-mounted stop lamps, illuminated when braking, are the primary mechanism to\nalert following drivers to the need to reduce speed or brake. This paper\ndevelops a novel brain response approach to measuring subject reaction to\ndifferent brake light designs. A variety of off-the-shelf brake light\nassemblies are tested in a physical simulated driving environment to assess the\ncognitive reaction times of 22 subjects. Eight pairs of LED-based and two pairs\nof incandescent bulb-based brake light assemblies are used and\nelectroencephalogram (EEG) data recorded. Channel Pz is utilised to extract the\nP3 component evoked during the decision making process that occurs in the brain\nwhen a participant decides to lift their foot from the accelerator and depress\nthe brake. EEG analysis shows that both incandescent bulb-based lights are\nstatistically slower to evoke cognitive responses than all tested LED-based\nlights. Between the LED designs, differences are evident, but not statistically\nsignificant, attributed to the significant amount of movement artifact in the\nEEG signal."}
{"id": "2508.03211", "pdf": "https://arxiv.org/pdf/2508.03211.pdf", "abs": "https://arxiv.org/abs/2508.03211", "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "authors": ["Pablo J. Diego-Sim√≥n", "Emmanuel Chemla", "Jean-R√©mi King", "Yair Lakretz"], "categories": ["cs.CL"], "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance."}
{"id": "2508.03410", "pdf": "https://arxiv.org/pdf/2508.03410.pdf", "abs": "https://arxiv.org/abs/2508.03410", "title": "VisAug: Facilitating Speech-Rich Web Video Navigation and Engagement with Auto-Generated Visual Augmentations", "authors": ["Baoquan Zhao", "Xiaofan Ma", "Qianshi Pang", "Ruomei Wang", "Fan Zhou", "Shujin Lin"], "categories": ["cs.MM", "cs.HC"], "comment": null, "summary": "The widespread adoption of digital technology has ushered in a new era of\ndigital transformation across all aspects of our lives. Online learning,\nsocial, and work activities, such as distance education, videoconferencing,\ninterviews, and talks, have led to a dramatic increase in speech-rich video\ncontent. In contrast to other video types, such as surveillance footage, which\ntypically contain abundant visual cues, speech-rich videos convey most of their\nmeaningful information through the audio channel. This poses challenges for\nimproving content consumption using existing visual-based video summarization,\nnavigation, and exploration systems. In this paper, we present VisAug, a novel\ninteractive system designed to enhance speech-rich video navigation and\nengagement by automatically generating informative and expressive visual\naugmentations based on the speech content of videos. Our findings suggest that\nthis system has the potential to significantly enhance the consumption and\nengagement of information in an increasingly video-driven digital landscape."}
{"id": "2508.03240", "pdf": "https://arxiv.org/pdf/2508.03240.pdf", "abs": "https://arxiv.org/abs/2508.03240", "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting", "authors": ["Mutaz Ayesh", "Nicol√°s Guti√©rrez-Rol√≥n", "Fernando Alva-Manchego"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results."}
{"id": "2508.03514", "pdf": "https://arxiv.org/pdf/2508.03514.pdf", "abs": "https://arxiv.org/abs/2508.03514", "title": "Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours", "authors": ["Pavlos Panagiotidis", "Victor Zhi Heung Ngo", "Sean Myatt", "Roma Patel", "Rachel Ramchurn", "Alan Chamberlain", "Ayse Kucukyilmaz"], "categories": ["cs.RO", "cs.HC"], "comment": "The paper is accepted for presentation to International Conference on\n  Social Robotics + AI (https://icsr2025.eu/)", "summary": "In this paper, we propose theatre-in-the-loop, a framework for developing\nexpressive robot behaviours tailored to artistic performance through a\ndirector-guided puppeteering workflow. Leveraging theatrical methods, we use\nnarrative objectives to direct a puppeteer in generating improvised robotic\ngestures that convey specific emotions. These improvisations are captured and\ncurated to build a dataset of reusable movement templates for standalone\nplayback in future autonomous performances. Initial trials demonstrate the\nfeasibility of this approach, illustrating how the workflow enables precise\nsculpting of robotic gestures into coherent emotional arcs while revealing\nchallenges posed by the robot's mechanical constraints. We argue that this\npractice-led framework provides a model for interdisciplinary teams creating\nsocially expressive robot behaviours, contributing to (1) theatre as an\ninteractive training ground for human-robot interaction and (2) co-creation\nmethodologies between humans and machines."}
{"id": "2508.03247", "pdf": "https://arxiv.org/pdf/2508.03247.pdf", "abs": "https://arxiv.org/abs/2508.03247", "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs", "authors": ["Shintaro Sakai", "Jisun An", "Migyeong Kang", "Haewoon Kwak"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications."}
{"id": "2508.03638", "pdf": "https://arxiv.org/pdf/2508.03638.pdf", "abs": "https://arxiv.org/abs/2508.03638", "title": "Design Support for Multitape Turing Machines", "authors": ["Marco T. Moraz√°n", "Oliwia Kempinski", "Andr√©s M. Garced"], "categories": ["cs.FL", "cs.HC", "cs.PL", "cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "Many Formal Languages and Automata Theory courses introduce students to\nTuring machine extensions. One of the most widely-used extensions endows Turing\nmachines with multiple tapes. Although multitape Turing machines are an\nabstraction to simplify Turing machine design, students find them no less\nchallenging. To aid students in understanding these machines, the FSM\nprogramming language provides support for their definition and execution. This,\nhowever, has proven insufficient for many students to understand the\noperational semantics of such machines and to understand why such machines\naccept or reject a word. To address this problem, three visualization tools\nhave been developed. The first is a dynamic visualization tool that simulates\nmachine execution. The second is a static visualization tool that automatically\nrenders a graphic for a multitape Turing machine's transition diagram. The\nthird is a static visualization tool that automatically renders computation\ngraphs for multitape Turing machines. This article presents these tools and\nillustrates how they are used to help students design and implement multitape\nTuring machines. In addition, empirical data is presented that suggests these\ntools are well-received and found useful by students."}
{"id": "2508.03250", "pdf": "https://arxiv.org/pdf/2508.03250.pdf", "abs": "https://arxiv.org/abs/2508.03250", "title": "RooseBERT: A New Deal For Political Language Modelling", "authors": ["Deborah Dore", "Elena Cabrio", "Serena Villata"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing amount of political debates and politics-related discussions\ncalls for the definition of novel computational methods to automatically\nanalyse such content with the final goal of lightening up political\ndeliberation to citizens. However, the specificity of the political language\nand the argumentative form of these debates (employing hidden communication\nstrategies and leveraging implicit arguments) make this task very challenging,\neven for current general-purpose pre-trained Language Models. To address this\nissue, we introduce a novel pre-trained Language Model for political discourse\nlanguage called RooseBERT. Pre-training a language model on a specialised\ndomain presents different technical and linguistic challenges, requiring\nextensive computational resources and large-scale data. RooseBERT has been\ntrained on large political debate and speech corpora (8K debates, each composed\nof several sub-debates on different topics) in English. To evaluate its\nperformances, we fine-tuned it on four downstream tasks related to political\ndebate analysis, i.e., named entity recognition, sentiment analysis, argument\ncomponent detection and classification, and argument relation prediction and\nclassification. Our results demonstrate significant improvements over\ngeneral-purpose Language Models on these four tasks, highlighting how\ndomain-specific pre-training enhances performance in political debate analysis.\nWe release the RooseBERT language model for the research community."}
{"id": "2508.03639", "pdf": "https://arxiv.org/pdf/2508.03639.pdf", "abs": "https://arxiv.org/abs/2508.03639", "title": "A Design Recipe and Recipe-Based Errors for Regular Expressions", "authors": ["Marco T. Moraz√°n", "Shamil Dzhatdoyev", "Josephine Des Rosiers", "Tijana Miniƒá", "Andr√©s M. Garced", "David Anthony K. Fields"], "categories": ["cs.FL", "cs.HC", "cs.PL", "cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "This article presents a novel framework to provide Formal Languages and\nAutomata Theory students design support for the development of regular\nexpressions. This framework includes a design recipe for regular expressions\nand a customized error messaging system. The error messaging system produces\nrecipe-based errors that include the step of the design recipe not successfully\ncompleted. Furthermore, the error messages follow the established practices of\nbeing concise, succinct, jargon-free, and nonprescriptive. In addition, a\nshorthand syntax developed for writing unit tests is described. The in-class\nuse of the design recipe is illustrated, two debugging sessions using the\ndescribed system are discussed, and the implementation of the error messaging\nsystem is briefly sketched."}
{"id": "2508.03259", "pdf": "https://arxiv.org/pdf/2508.03259.pdf", "abs": "https://arxiv.org/abs/2508.03259", "title": "Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition", "authors": ["Duzhen Zhang", "Chenxing Li", "Jiahua Dong", "Qi Liu", "Dong Yu"], "categories": ["cs.CL"], "comment": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "Continual Named Entity Recognition (CNER) is an evolving field that focuses\non sequentially updating an existing model to incorporate new entity types.\nPrevious CNER methods primarily utilize Knowledge Distillation (KD) to preserve\nprior knowledge and overcome catastrophic forgetting, strictly ensuring that\nthe representations of old and new models remain consistent. Consequently, they\noften impart the model with excessive stability (i.e., retention of old\nknowledge) but limited plasticity (i.e., acquisition of new knowledge). To\naddress this issue, we propose a Stability-Plasticity Trade-off (SPT) method\nfor CNER that balances these aspects from both representation and weight\nperspectives. From the representation perspective, we introduce a pooling\noperation into the original KD, permitting a level of plasticity by\nconsolidating representation dimensions. From the weight perspective, we\ndynamically merge the weights of old and new models, strengthening old\nknowledge while maintaining new knowledge. During this fusion, we implement a\nweight-guided selective mechanism to prioritize significant weights. Moreover,\nwe develop a confidence-based pseudo-labeling approach for the current\nnon-entity type, which predicts entity types using the old model to handle the\nsemantic shift of the non-entity type, a challenge specific to CNER that has\nlargely been ignored by previous methods. Extensive experiments across ten CNER\nsettings on three benchmark datasets demonstrate that our SPT method surpasses\nprevious CNER approaches, highlighting its effectiveness in achieving a\nsuitable stability-plasticity trade-off."}
{"id": "2508.03641", "pdf": "https://arxiv.org/pdf/2508.03641.pdf", "abs": "https://arxiv.org/abs/2508.03641", "title": "Visual Execution and Validation of Finite-State Machines and Pushdown Automata", "authors": ["Marco T. Moraz√°n", "David Anthony K. Fields", "Andr√©s M. Garced", "Tijana Miniƒá"], "categories": ["cs.FL", "cs.HC", "cs.PL", "cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "In Formal Languages and Automata Theory courses, students find understanding\nnondeterministic finite-state and pushdown automata difficult. In many cases,\nthis means that it is challenging for them to comprehend the operational\nsemantics of such machines and, as a consequence, determine why a word is\naccepted or rejected. This is not entirely surprising, because students are\nmostly trained to design and implement deterministic programs. Comprehension of\npushdown automata is further complicated, because reasoning about the stack is\nnecessary. A common difficulty students face, for example, is understanding\nthat two different computations on the same word may reach the same state with\ndifferent stack values. To aid student understanding, we present two novel\ndynamic visualization tools for FSM -- a domain-specific programming language\nfor the Automata Theory classroom -- to support the design of such machines.\nThese tools visualize all computations that may be performed, respectively, by\na nondeterministic finite-state machine or by a pushdown automata in a stepwise\nmanner. In addition, these tools aid the machine verification process by\nallowing users to visually validate whether the properties a state represents\nhold when a machine transitions into it."}
{"id": "2508.03262", "pdf": "https://arxiv.org/pdf/2508.03262.pdf", "abs": "https://arxiv.org/abs/2508.03262", "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?", "authors": ["Junhyuk Choi", "Hyeonchu Park", "Haemin Lee", "Hyebeen Shin", "Hyun Joung Jin", "Bugeun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science."}
{"id": "2404.17730", "pdf": "https://arxiv.org/pdf/2404.17730.pdf", "abs": "https://arxiv.org/abs/2404.17730", "title": "Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults", "authors": ["Lara J. Martin", "Malathy Nagalakshmi"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "High-tech Augmentative and Alternative Communication (AAC) has been rapidly\nadvancing in recent years due to the increased use of large language models\n(LLMs) like ChatGPT, but many of these techniques are integrated without the\ninclusion of the users' perspectives. Autistic adults have been particularly\nneglected in the design of AAC tools. We conducted in-depth interviews with 12\nautistic adults to find the pain points of current AAC and determine what\ntechnological advances they might find helpful. We found 8 different categories\nof themes from our interviews: input flexibility, output flexibility, selecting\nor adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling\nblocks for continued use, and control of communication. In this paper, we go\nthrough these categories in depth -- comparing each to prior work -- and then\nhighlight novel findings to suggest possible research directions."}
{"id": "2508.03275", "pdf": "https://arxiv.org/pdf/2508.03275.pdf", "abs": "https://arxiv.org/abs/2508.03275", "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning", "authors": ["Jiahao Zhao"], "categories": ["cs.CL"], "comment": "15 pages, 4 figures, 1 table", "summary": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms."}
{"id": "2410.04286", "pdf": "https://arxiv.org/pdf/2410.04286.pdf", "abs": "https://arxiv.org/abs/2410.04286", "title": "Embracing Transparency: A Study of Open Science Practices Among Early Career HCI Researchers", "authors": ["Tatiana Chakravorti", "Sanjana Gautam", "Sarah M. Rajtmajer"], "categories": ["cs.HC"], "comment": null, "summary": "Many fields of science, including Human-Computer Interaction (HCI), have\nheightened introspection in the wake of concerns around reproducibility and\nreplicability of published findings. Notably, in recent years the HCI community\nhas worked to implement policy changes and mainstream open science practices.\nOur work investigates early-career HCI researchers' perceptions of open science\nand engagement with best practices through 18 semi-structured interviews. Our\nfindings highlight key barriers to the widespread adoption of data and\nmaterials sharing, and preregistration, namely: lack of clear incentives;\ncultural resistance; limited training; time constraints; concerns about\nintellectual property; and data privacy issues. We observe that small changes\nat major conferences like CHI could meaningfully impact community norms. We\noffer recommendations to address these barriers and to promote transparency and\nopenness in HCI. While these findings provide valuable and interesting insights\nabout the open science practices by early career HCI researchers, their\napplicability is limited to the USA only. The interview study relies on\nself-reported data; therefore, it can be subject to biases like recall bias.\nFuture studies will include the scope to expand HCI researchers from different\nlevels of experience and different countries, allowing for more justifiable\nexamples."}
{"id": "2508.03276", "pdf": "https://arxiv.org/pdf/2508.03276.pdf", "abs": "https://arxiv.org/abs/2508.03276", "title": "Do language models accommodate their users? A study of linguistic convergence", "authors": ["Terra Blevins", "Susanne Schmalwieser", "Benjamin Roth"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different."}
{"id": "2501.04543", "pdf": "https://arxiv.org/pdf/2501.04543.pdf", "abs": "https://arxiv.org/abs/2501.04543", "title": "The Impostor is Among Us: Can Large Language Models Capture the Complexity of Human Personas?", "authors": ["Christopher Lazik", "Christopher Katins", "Charlotte Kauter", "Jonas Jakob", "Caroline Jay", "Lars Grunske", "Thomas Kosch"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) created new opportunities for generating\npersonas, expected to streamline and accelerate the human-centered design\nprocess. Yet, AI-generated personas may not accurately represent actual user\nexperiences, as they can miss contextual and emotional insights critical to\nunderstanding real users' needs and behaviors. This introduces a potential\nthreat to quality, especially for novices. This paper examines the differences\nin how users perceive personas created by LLMs compared to those crafted by\nhumans regarding their credibility for design. We gathered ten human-crafted\npersonas developed by HCI experts according to relevant attributes established\nin related work. Then, we systematically generated ten personas with an LLM and\ncompared them with human-crafted ones in a survey. The results showed that\nparticipants differentiated between human-created and AI-generated personas,\nwith the latter perceived as more informative and consistent. However,\nparticipants noted that the AI-generated personas tended to follow stereotypes,\nhighlighting the need for a greater emphasis on diversity when utilizing LLMs\nfor persona creation."}
{"id": "2508.03292", "pdf": "https://arxiv.org/pdf/2508.03292.pdf", "abs": "https://arxiv.org/abs/2508.03292", "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes", "authors": ["Shahed Masoudian", "Gustavo Escobedo", "Hannah Strauss", "Markus Schedl"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs."}
{"id": "2501.13308", "pdf": "https://arxiv.org/pdf/2501.13308.pdf", "abs": "https://arxiv.org/abs/2501.13308", "title": "\"It was Mentally Painful to Try and Stop\": Design Opportunities for Just-in-Time Interventions for People with Obsessive-Compulsive Disorder in the Real World", "authors": ["Ru Wang", "Kexin Zhang", "Yuqing Wang", "Keri Brown", "Yuhang Zhao"], "categories": ["cs.HC", "H.5.0"], "comment": null, "summary": "Obsessive-compulsive disorder (OCD) is a mental health condition that\nsignificantly impacts people's quality of life. While evidence-based therapies\nsuch as exposure and response prevention (ERP) can be effective, managing OCD\nsymptoms in everyday life -- an essential part of treatment and independent\nliving -- remains challenging due to fear confrontation and lack of appropriate\nsupport. To better understand the challenges and needs in OCD self-management,\nwe conducted interviews with 10 participants with diverse OCD conditions and\nseven therapists specializing in OCD treatment. Through these interviews, we\nexplored the characteristics of participants' triggers and how they shaped\ntheir compulsions, and uncovered key coping strategies across different stages\nof OCD episodes. Our findings highlight critical gaps between OCD\nself-management needs and currently available support. Building on these\ninsights, we propose design opportunities for just-in-time self-management\ntechnologies for OCD, including personalized symptom tracking, just-in-time\ninterventions, and support for OCD-specific privacy and social needs -- through\ntechnology and beyond."}
{"id": "2508.03294", "pdf": "https://arxiv.org/pdf/2508.03294.pdf", "abs": "https://arxiv.org/abs/2508.03294", "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty", "authors": ["Leonidas Zotos", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea", "Malvina Nissim", "Hedderik van Rijn"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 2 figures, accepted at the 2nd International Workshop on AI\n  in Society, Education and Educational Research (AISEER)", "summary": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment."}
{"id": "2501.14327", "pdf": "https://arxiv.org/pdf/2501.14327.pdf", "abs": "https://arxiv.org/abs/2501.14327", "title": "Characterizing Visual Intents for People with Low Vision through Eye Tracking", "authors": ["Ru Wang", "Ruijia Chen", "Anqiao Erica Cai", "Zhiyuan Li", "Sanbrita Mondal", "Yuhang Zhao"], "categories": ["cs.HC", "H.5.0"], "comment": null, "summary": "Accessing visual information is crucial yet challenging for people with low\nvision due to visual conditions like low visual acuity and limited visual\nfields. However, unlike blind people, low vision people have and prefer using\ntheir functional vision in daily tasks. Gaze patterns thus become an important\nindicator to uncover their visual challenges and intents, inspiring more\nadaptive visual support. We seek to deeply understand low vision users' gaze\nbehaviors in different image-viewing tasks, characterizing typical visual\nintents and the unique gaze patterns exhibited by people with different low\nvision conditions. We conducted a retrospective think-aloud study using eye\ntracking with 20 low vision participants and 20 sighted controls. Participants\ncompleted various image-viewing tasks and watched the playback of their gaze\ntrajectories to reflect on their visual experiences. Based on the study, we\nderived a visual intent taxonomy with five visual intents characterized by\nparticipants' gaze behaviors. We demonstrated the difference between low vision\nand sighted participants' gaze behaviors and how visual ability affected low\nvision participants' gaze patterns across visual intents. Our findings\nunderscore the importance of combining visual ability information, visual\ncontext, and eye tracking data in visual intent recognition, setting up a\nfoundation for intent-aware assistive technologies for low vision people."}
{"id": "2508.03296", "pdf": "https://arxiv.org/pdf/2508.03296.pdf", "abs": "https://arxiv.org/abs/2508.03296", "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling", "authors": ["Anqi Li", "Wenwei Jin", "Jintao Tong", "Pengda Qin", "Weijia Li", "Guo Lu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social platforms have revolutionized information sharing, but also\naccelerated the dissemination of harmful and policy-violating content. To\nensure safety and compliance at scale, moderation systems must go beyond\nefficiency and offer accuracy and interpretability. However, current approaches\nlargely rely on noisy, label-driven learning, lacking alignment with moderation\nrules and producing opaque decisions that hinder human review. Therefore, we\npropose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that\nintroduces a new policy-aligned decision paradigm. The term \"Hierarchical\"\nreflects two key aspects of our system design: (1) a hierarchical moderation\npipeline, where a lightweight binary model first filters safe content and a\nstronger model handles fine-grained risk classification; and (2) a hierarchical\ntaxonomy in the second stage, where the model performs path-based\nclassification over a hierarchical taxonomy ranging from coarse to fine-grained\nlevels. To ensure alignment with evolving moderation policies, Hi-Guard\ndirectly incorporates rule definitions into the model prompt. To further\nenhance structured prediction and reasoning, we introduce a multi-level\nsoft-margin reward and optimize with Group Relative Policy Optimization (GRPO),\npenalizing semantically adjacent misclassifications and improving explanation\nquality. Extensive experiments and real-world deployment demonstrate that\nHi-Guard achieves superior classification accuracy, generalization, and\ninterpretability, paving the way toward scalable, transparent, and trustworthy\ncontent safety systems. Code is available at:\nhttps://github.com/lianqi1008/Hi-Guard."}
{"id": "2502.13920", "pdf": "https://arxiv.org/pdf/2502.13920.pdf", "abs": "https://arxiv.org/abs/2502.13920", "title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health", "authors": ["Xingbo Wang", "Janessa Griffith", "Daniel A. Adler", "Joey Castillo", "Tanzeem Choudhury", "Fei Wang"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2025). Code is available at https://github.com/xingbow/sleephealthLLM", "summary": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots."}
{"id": "2508.03333", "pdf": "https://arxiv.org/pdf/2508.03333.pdf", "abs": "https://arxiv.org/abs/2508.03333", "title": "CTTS: Collective Test-Time Scaling", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Tao Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM."}
{"id": "2506.14468", "pdf": "https://arxiv.org/pdf/2506.14468.pdf", "abs": "https://arxiv.org/abs/2506.14468", "title": "MERba: Multi-Receptive Field MambaVision for Micro-Expression Recognition", "authors": ["Xinglong Mao", "Shifeng Liu", "Sirui Zhao", "Tong Xu", "Hanchao Wang", "Baozhi Jia", "Enhong Chen"], "categories": ["cs.HC"], "comment": null, "summary": "Micro-expressions (MEs) are brief, involuntary facial movements that reveal\ngenuine emotions, offering valuable insights for psychological assessment and\ncriminal investigations. Despite significant progress in automatic ME\nrecognition (MER), existing methods still struggle to simultaneously capture\nlocalized muscle activations and global facial dependencies, both essential for\ndecoding subtle emotional cues. To address this challenge, we propose MERba, a\nhierarchical multi-receptive field architecture specially designed for MER,\nwhich incorporates a series of Local-Global Feature Integration stages. Within\neach stage, detailed intra-window motion patterns are captured using MERba\nLocal Extractors, which integrate MambaVision Mixers with a tailored asymmetric\nmulti-scanning strategy to enhance local spatial sensitivity. These localized\nfeatures are then aggregated through lightweight self-attention layers that\nexplicitly model inter-window relationships, enabling effective global context\nconstruction. Furthermore, to mitigate the challenge of high inter-class\nsimilarity among negative MEs, we introduce a Dual-Granularity Classification\nModule that decomposes the recognition task into a coarse-to-fine paradigm.\nExtensive experiments on three benchmark datasets demonstrate that MERba\nconsistently outperforms existing methods, with ablation studies confirming the\neffectiveness of each proposed component."}
{"id": "2508.03358", "pdf": "https://arxiv.org/pdf/2508.03358.pdf", "abs": "https://arxiv.org/abs/2508.03358", "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature", "authors": ["Tiago G Can√°rio", "Catarina Duarte", "Fl√°vio L. Pinheiro", "Jo√£o L. M. Pereira"], "categories": ["cs.CL", "cs.IR"], "comment": "24 pages, 5 Figures, 4 Tables", "summary": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2"}
{"id": "2507.14494", "pdf": "https://arxiv.org/pdf/2507.14494.pdf", "abs": "https://arxiv.org/abs/2507.14494", "title": "\"It looks sexy but it's wrong.\" Tensions in creativity and accuracy using genAI for biomedical visualization", "authors": ["Roxanne Ziman", "Shehryar Saharan", "Ga√´l McGill", "Laura Garrison"], "categories": ["cs.HC"], "comment": "11 pages, 3 figures. Accepted to IEEE VIS 2025 Conference", "summary": "We contribute an in-depth analysis of the workflows and tensions arising from\ngenerative AI (genAI) use in biomedical visualization (BioMedVis). Although\ngenAI affords facile production of aesthetic visuals for biological and medical\ncontent, the architecture of these tools fundamentally limits the accuracy and\ntrustworthiness of the depicted information, from imaginary (or fanciful)\nmolecules to alien anatomy. Through 17 interviews with a diverse group of\npractitioners and researchers, we qualitatively analyze the concerns and values\ndriving genAI (dis)use for the visual representation of spatially-oriented\nbiomedical data. We find that BioMedVis experts, both in roles as developers\nand designers, use genAI tools at different stages of their daily workflows and\nhold attitudes ranging from enthusiastic adopters to skeptical avoiders of\ngenAI. In contrasting the current use and perspectives on genAI observed in our\nstudy with predictions towards genAI in the visualization pipeline from prior\nwork, we refocus the discussion of genAI's effects on projects in visualization\nin the here and now with its respective opportunities and pitfalls for future\nvisualization research. At a time when public trust in science is in jeopardy,\nwe are reminded to first do no harm, not just in biomedical visualization but\nin science communication more broadly. Our observations reaffirm the necessity\nof human intervention for empathetic design and assessment of accurate\nscientific visuals."}
{"id": "2508.03363", "pdf": "https://arxiv.org/pdf/2508.03363.pdf", "abs": "https://arxiv.org/abs/2508.03363", "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs."}
{"id": "2507.21462", "pdf": "https://arxiv.org/pdf/2507.21462.pdf", "abs": "https://arxiv.org/abs/2507.21462", "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals", "authors": ["Tingying He", "Maggie McCracken", "Daniel Hajas", "Sarah Creem-Regehr", "Alexander Lex"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether tactile charts support comprehension and learning of\ncomplex visualizations for blind and low-vision (BLV) individuals and\ncontribute four tactile chart designs and an interview study. Visualizations\nare powerful tools for conveying data, yet BLV individuals typically can rely\nonly on assistive technologies -- primarily alternative texts -- to access this\ninformation. Prior research shows the importance of mental models of chart\ntypes for interpreting these descriptions, yet BLV individuals have no means to\nbuild such a mental model based on images of visualizations. Tactile charts\nshow promise to fill this gap in supporting the process of building mental\nmodels. Yet studies on tactile data representations mostly focus on simple\nchart types, and it is unclear whether they are also appropriate for more\ncomplex charts as would be found in scientific publications. Working with two\nBLV researchers, we designed 3D-printed tactile template charts with\nexploration instructions for four advanced chart types: UpSet plots, violin\nplots, clustered heatmaps, and faceted line charts. We then conducted an\ninterview study with 12 BLV participants comparing whether using our tactile\ntemplates improves mental models and understanding of charts and whether this\nunderstanding translates to novel datasets experienced through alt texts.\nThematic analysis shows that tactile models support chart type understanding\nand are the preferred learning method by BLV individuals. We also report\nparticipants' opinions on tactile chart design and their role in BLV education."}
{"id": "2508.03399", "pdf": "https://arxiv.org/pdf/2508.03399.pdf", "abs": "https://arxiv.org/abs/2508.03399", "title": "ReDSM5: A Reddit Dataset for DSM-5 Depression Detection", "authors": ["Eliseo Bao", "Anxo P√©rez", "Javier Parapar"], "categories": ["cs.CL"], "comment": "Accepted as a resource paper at CIKM 2025", "summary": "Depression is a pervasive mental health condition that affects hundreds of\nmillions of individuals worldwide, yet many cases remain undiagnosed due to\nbarriers in traditional clinical access and pervasive stigma. Social media\nplatforms, and Reddit in particular, offer rich, user-generated narratives that\ncan reveal early signs of depressive symptomatology. However, existing\ncomputational approaches often label entire posts simply as depressed or not\ndepressed, without linking language to specific criteria from the DSM-5, the\nstandard clinical framework for diagnosing depression. This limits both\nclinical relevance and interpretability. To address this gap, we introduce\nReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each\nexhaustively annotated at the sentence level by a licensed psychologist for the\nnine DSM-5 depression symptoms. For each label, the annotator also provides a\nconcise clinical rationale grounded in DSM-5 methodology. We conduct an\nexploratory analysis of the collection, examining lexical, syntactic, and\nemotional patterns that characterize symptom expression in social media\nnarratives. Compared to prior resources, ReDSM5 uniquely combines\nsymptom-specific supervision with expert explanations, facilitating the\ndevelopment of models that not only detect depression but also generate\nhuman-interpretable reasoning. We establish baseline benchmarks for both\nmulti-label symptom classification and explanation generation, providing\nreference results for future research on detection and interpretability."}
{"id": "2507.21837", "pdf": "https://arxiv.org/pdf/2507.21837.pdf", "abs": "https://arxiv.org/abs/2507.21837", "title": "VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos", "authors": ["Yotam Sechayk", "Ariel Shamir", "Amy Pavel", "Takeo Igarashi"], "categories": ["cs.HC"], "comment": "ASSETS '25, Denver, CO, USA", "summary": "Instructors often rely on visual actions such as pointing, marking, and\nsketching to convey information in educational presentation videos. These\nsubtle visual cues often lack verbal descriptions, forcing low-vision (LV)\nlearners to search for visual indicators or rely solely on audio, which can\nlead to missed information and increased cognitive load. To address this\nchallenge, we conducted a co-design study with three LV participants and\ndeveloped VeasyGuide, a tool that uses motion detection to identify instructor\nactions and dynamically highlight and magnify them. VeasyGuide produces\nfamiliar visual highlights that convey spatial context and adapt to diverse\nlearners and content through extensive personalization and real-time visual\nfeedback. VeasyGuide reduces visual search effort by clarifying what to look\nfor and where to look. In an evaluation with 8 LV participants, learners\ndemonstrated a significant improvement in detecting instructor actions, with\nfaster response times and significantly reduced cognitive load. A separate\nevaluation with 8 sighted participants showed that VeasyGuide also enhanced\nengagement and attentiveness, suggesting its potential as a universally\nbeneficial tool."}
{"id": "2508.03420", "pdf": "https://arxiv.org/pdf/2508.03420.pdf", "abs": "https://arxiv.org/abs/2508.03420", "title": "Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations", "authors": ["Bing Wang", "Ximing Li", "Yiming Wang", "Changchun Li", "Jiaxu Cui", "Renchu Guan", "Bo Yang"], "categories": ["cs.CL", "cs.SI"], "comment": "Accepted by CIKM 2025. 11 pages, 4 figures. Code:\n  https://github.com/wangbing1416/MISDER", "summary": "The proliferation of misinformation across diverse social media platforms has\ndrawn significant attention from both academic and industrial communities due\nto its detrimental effects. Accordingly, automatically distinguishing\nmisinformation, dubbed as Misinformation Detection (MD), has become an\nincreasingly active research topic. The mainstream methods formulate MD as a\nstatic learning paradigm, which learns the mapping between the content, links,\nand propagation of news articles and the corresponding manual veracity labels.\nHowever, the static assumption is often violated, since in real-world\nscenarios, the veracity of news articles may vacillate within the dynamically\nevolving social environment. To tackle this problem, we propose a novel\nframework, namely Misinformation detection with Dynamic Environmental\nRepresentations (MISDER). The basic idea of MISDER lies in learning a social\nenvironmental representation for each period and employing a temporal model to\npredict the representation for future periods. In this work, we specify the\ntemporal model as the LSTM model, continuous dynamics equation, and pre-trained\ndynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,\nMISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,\nwe compare it to various MD baselines across 2 prevalent datasets, and the\nexperimental results can indicate the effectiveness of our proposed model."}
{"id": "2508.02639", "pdf": "https://arxiv.org/pdf/2508.02639.pdf", "abs": "https://arxiv.org/abs/2508.02639", "title": "Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable", "authors": ["Tingying He", "Jason Dykes", "Petra Isenberg", "Tobias Isenberg"], "categories": ["cs.HC"], "comment": null, "summary": "We present a new comprehensive theory for explaining, exploring, and using\npattern as a visual variable in visualization. Although patterns have long been\nused for data encoding and continue to be valuable today, their conceptual\nfoundations are precarious: the concepts and terminology used across the\nresearch literature and in practice are inconsistent, making it challenging to\nuse patterns effectively and to conduct research to inform their use. To\naddress this problem, we conduct a comprehensive cross-disciplinary literature\nreview that clarifies ambiguities around the use of \"pattern\" and \"texture\". As\na result, we offer a new consistent treatment of pattern as a composite visual\nvariable composed of structured groups of graphic primitives that can serve as\nmarks for encoding data individually and collectively. This new and widely\napplicable formulation opens a sizable design space for the visual variable\npattern, which we formalize as a new system comprising three sets of variables:\nthe spatial arrangement of primitives, the appearance relationships among\nprimitives, and the retinal visual variables that characterize individual\nprimitives. We show how our pattern system relates to existing visualization\ntheory and highlight opportunities for visualization design. We further explore\npatterns based on complex spatial arrangements, demonstrating explanatory power\nand connecting our conceptualization to broader theory on maps and cartography.\nAn author version and additional materials are available on OSF: osf.io/z7ae2."}
{"id": "2508.03440", "pdf": "https://arxiv.org/pdf/2508.03440.pdf", "abs": "https://arxiv.org/abs/2508.03440", "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "authors": ["Junhong Wu", "Jinliang Lu", "Zixuan Ren", "Ganqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks."}
{"id": "2409.11535", "pdf": "https://arxiv.org/pdf/2409.11535.pdf", "abs": "https://arxiv.org/abs/2409.11535", "title": "Balancing Optimality and Diversity: Human-Centered Decision Making through Generative Curation", "authors": ["Michael Lingzhi Li", "Shixiang Zhu"], "categories": ["cs.LG", "cs.HC", "math.OC"], "comment": null, "summary": "Operational decisions in healthcare, logistics, and public policy\nincreasingly involve algorithms that recommend candidate solutions, such as\ntreatment plans, delivery routes, or policy options, while leaving the final\nchoice to human decision-makers. For instance, school districts use algorithms\nto design bus routes, but administrators make the final call given community\nfeedback. In these settings, decision quality depends not on a single\nalgorithmic ``optimum'', but on whether the portfolio of recommendations\ncontains at least one option the human ultimately deems desirable. We propose\ngenerative curation, a framework that optimally generates recommendation sets\nwhen desirability depends on both observable objectives and unobserved\nqualitative considerations. Instead of a fixed solution, generative curation\nlearns a distribution over solutions designed to maximize the expected\ndesirability of the best option within a manageable portfolio. Our analysis\nidentifies a trade-off between quantitative quality and qualitative diversity,\nformalized through a novel diversity metric derived from the reformulated\nobjective. We implement the framework using a generative neural network and a\nsequential optimization method, and show in synthetic and real-world studies\nthat it consistently reduces expected regret compared to existing benchmarks.\nOur framework provides decision-makers with a principled way to design\nalgorithms that complement, rather than replace, human judgment. By generating\nportfolios of diverse yet high-quality options, decision-support tools can\nbetter accommodate unmodeled factors such as stakeholder preferences, political\nfeasibility, or community acceptance. More broadly, the framework enables\norganizations to operationalize human-centered decision-making at scale,\nensuring that algorithmic recommendations remain useful even when objectives\nare incomplete or evolving."}
{"id": "2508.03453", "pdf": "https://arxiv.org/pdf/2508.03453.pdf", "abs": "https://arxiv.org/abs/2508.03453", "title": "Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings", "authors": ["Rita Gonz√°lez-M√°rquez", "Philipp Berens", "Dmitry Kobak"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text embeddings, i.e. vector representations of entire texts, play an\nimportant role in many NLP applications, such as retrieval-augmented\ngeneration, sentiment analysis, clustering, or visualizing collections of texts\nfor data exploration. Currently, top-performing embedding models are derived\nfrom pre-trained language models via extensive supervised fine-tuning using\ncurated text pairs. This contrasts with computer vision, where self-supervised\ntraining based on data augmentations has demonstrated remarkable success. Here\nwe systematically compare the two most well-known augmentation strategies for\npositive pair generation in contrastive learning of text embeddings. We assess\nembedding quality on MTEB and additional in-domain evaluations and show that\ncropping augmentation strongly outperforms the dropout-based approach. We find\nthat on out-of-domain data, the quality of resulting embeddings is below the\nsupervised SOTA models, but for in-domain data, self-supervised fine-tuning\nproduces high-quality text embeddings after very short fine-tuning, sometimes\nonly marginally below the supervised SOTA. Finally, we show that representation\nquality increases towards the last transformer layers, which undergo the\nlargest change during fine-tuning; and that fine-tuning only those last layers\nis sufficient to reach similar embedding quality."}
{"id": "2501.13836", "pdf": "https://arxiv.org/pdf/2501.13836.pdf", "abs": "https://arxiv.org/abs/2501.13836", "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to AIES 2025", "summary": "Most social media users come from the Global South, where harmful content\nusually appears in local languages. Yet, AI-driven moderation systems struggle\nwith low-resource languages spoken in these regions. Through semi-structured\ninterviews with 22 AI experts working on harmful content detection in four\nlow-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi\nArabic (North Africa), and Quechua (South America)--we examine systemic issues\nin building automated moderation tools for these languages. Our findings reveal\nthat beyond data scarcity, socio-political factors such as tech companies'\nmonopoly on user data and lack of investment in moderation for low-profit\nGlobal South markets exacerbate historic inequities. Even if more data were\navailable, the English-centric and data-intensive design of language models and\npreprocessing techniques overlooks the need to design for morphologically\ncomplex, linguistically diverse, and code-mixed languages. We argue these\nlimitations are not just technical gaps caused by \"data scarcity\" but reflect\nstructural inequities, rooted in colonial suppression of non-Western languages.\nWe discuss multi-stakeholder approaches to strengthen local research capacity,\ndemocratize data access, and support language-aware solutions to improve\nautomated moderation for low-resource languages."}
{"id": "2508.03475", "pdf": "https://arxiv.org/pdf/2508.03475.pdf", "abs": "https://arxiv.org/abs/2508.03475", "title": "fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval", "authors": ["Pranshu Rastogi"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables. Code available at\n  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders", "summary": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\nRetrieval is approached as a Learning-to-Rank task using a bi-encoder model\nfine-tuned from a pre-trained transformer optimized for sentence similarity.\nTraining used both the source languages and their English translations for\nmultilingual retrieval and only English translations for cross-lingual\nretrieval. Using lightweight models with fewer than 500M parameters and\ntraining on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual\nand 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks."}
{"id": "2504.16273", "pdf": "https://arxiv.org/pdf/2504.16273.pdf", "abs": "https://arxiv.org/abs/2504.16273", "title": "From Promising Capability to Pervasive Bias: Assessing Large Language Models for Emergency Department Triage", "authors": ["Joseph Lee", "Tianqi Shang", "Jae Young Baik", "Duy Duong-Tran", "Shu Yang", "Lingyao Li", "Li Shen"], "categories": ["cs.AI", "cs.HC"], "comment": "Presented at GenAI4Health Workshop @ AAAI 2025 (non-archival),\n  Preprint of an article submitted for consideration in Pacific Symposium on\n  Biocomputing 2026", "summary": "Large Language Models (LLMs) have shown promise in clinical decision support,\nyet their application to triage remains underexplored. We systematically\ninvestigate the capabilities of LLMs in emergency department triage through two\nkey dimensions: (1) robustness to distribution shifts and missing data, and (2)\ncounterfactual analysis of intersectional biases across sex and race. We assess\nmultiple LLM-based approaches, ranging from continued pre-training to\nin-context learning, as well as machine learning approaches. Our results\nindicate that LLMs exhibit superior robustness, and we investigate the key\nfactors contributing to the promising LLM-based approaches. Furthermore, in\nthis setting, we identify gaps in LLM preferences that emerge in particular\nintersections of sex and race. LLMs generally exhibit sex-based differences,\nbut they are most pronounced in certain racial groups. These findings suggest\nthat LLMs encode demographic preferences that may emerge in specific clinical\ncontexts or particular combinations of characteristics."}
{"id": "2508.03489", "pdf": "https://arxiv.org/pdf/2508.03489.pdf", "abs": "https://arxiv.org/abs/2508.03489", "title": "CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation", "authors": ["Kaiwen Zhao", "Bharathan Balaji", "Stephen Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Product sustainability reports provide valuable insights into the\nenvironmental impacts of a product and are often distributed in PDF format.\nThese reports often include a combination of tables and text, which complicates\ntheir analysis. The lack of standardization and the variability in reporting\nformats further exacerbate the difficulty of extracting and interpreting\nrelevant information from large volumes of documents. In this paper, we tackle\nthe challenge of answering questions related to carbon footprints within\nsustainability reports available in PDF format. Unlike previous approaches, our\nfocus is on addressing the difficulties posed by the unstructured and\ninconsistent nature of text extracted from PDF parsing. To facilitate this\nanalysis, we introduce CarbonPDF-QA, an open-source dataset containing\nquestion-answer pairs for 1735 product report documents, along with\nhuman-annotated answers. Our analysis shows that GPT-4o struggles to answer\nquestions with data inconsistencies. To address this limitation, we propose\nCarbonPDF, an LLM-based technique specifically designed to answer carbon\nfootprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama\n3 with our training data. Our results show that our technique outperforms\ncurrent state-of-the-art techniques, including question-answering (QA) systems\nfinetuned on table and text data."}
{"id": "2507.18905", "pdf": "https://arxiv.org/pdf/2507.18905.pdf", "abs": "https://arxiv.org/abs/2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany L. Brazile", "Natasha Chase", "Dimple Patel Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools."}
{"id": "2508.03520", "pdf": "https://arxiv.org/pdf/2508.03520.pdf", "abs": "https://arxiv.org/abs/2508.03520", "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Aneesh Krishna", "Shafin Rahman", "Tom Gedeon"], "categories": ["cs.CL", "cs.LG"], "comment": "Code available at https://github.com/hasan-rakibul/UPLME", "summary": "Supervised learning for empathy regression is challenged by noisy\nself-reported empathy scores. While many algorithms have been proposed for\nlearning with noisy labels in textual classification problems, the regression\ncounterpart is relatively under-explored. We propose UPLME, an\nuncertainty-aware probabilistic language modelling framework to capture label\nnoise in the regression setting of empathy detection. UPLME includes a\nprobabilistic language model that predicts both empathy score and\nheteroscedastic uncertainty and is trained using Bayesian concepts with\nvariational model ensembling. We further introduce two novel loss components:\none penalises degenerate Uncertainty Quantification (UQ), and another enforces\nthe similarity between the input pairs on which we predict empathy. UPLME\nprovides state-of-the-art performance (Pearson Correlation Coefficient:\n$0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the\nperformance reported in the literature in two public benchmarks, having label\nnoise. Through synthetic label noise injection, we show that UPLME is effective\nin separating noisy and clean samples based on the predicted uncertainty. UPLME\nfurther outperform (Calibration error: $0.571\\rightarrow0.376$) a recent\nvariational model ensembling-based UQ method designed for regression problems."}
{"id": "2508.03523", "pdf": "https://arxiv.org/pdf/2508.03523.pdf", "abs": "https://arxiv.org/abs/2508.03523", "title": "FilBench: Can LLMs Understand and Generate Filipino?", "authors": ["Lester James V. Miranda", "Elyanah Aco", "Conner Manuel", "Jan Christian Blaise Cruz", "Joseph Marvin Imperial"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development."}
{"id": "2508.03529", "pdf": "https://arxiv.org/pdf/2508.03529.pdf", "abs": "https://arxiv.org/abs/2508.03529", "title": "Marito: Structuring and Building Open Multilingual Terminologies for South African NLP", "authors": ["Vukosi Marivate", "Isheanesu Dzingirai", "Fiskani Banda", "Richard Lastrucci", "Thapelo Sindane", "Keabetswe Madumo", "Kayode Olaleye", "Abiodun Modupe", "Unarine Netshifhefhe", "Herkulaas Combrink", "Mohlatlego Nakeng", "Matome Ledwaba"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The critical lack of structured terminological data for South Africa's\nofficial languages hampers progress in multilingual NLP, despite the existence\nof numerous government and academic terminology lists. These valuable assets\nremain fragmented and locked in non-machine-readable formats, rendering them\nunusable for computational research and development. \\emph{Marito} addresses\nthis challenge by systematically aggregating, cleaning, and standardising these\nscattered resources into open, interoperable datasets. We introduce the\nfoundational \\emph{Marito} dataset, released under the equitable,\nAfrica-centered NOODL framework. To demonstrate its immediate utility, we\nintegrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.\nExperiments show substantial improvements in the accuracy and domain-specific\nconsistency of English-to-Tshivenda machine translation for large language\nmodels. \\emph{Marito} provides a scalable foundation for developing robust and\nequitable NLP technologies, ensuring South Africa's rich linguistic diversity\nis represented in the digital age."}
{"id": "2508.03533", "pdf": "https://arxiv.org/pdf/2508.03533.pdf", "abs": "https://arxiv.org/abs/2508.03533", "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models", "authors": ["Xiaoming Hou", "Jiquan Zhang", "Zibin Lin", "DaCheng Tao", "Shengli Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation."}
{"id": "2508.03550", "pdf": "https://arxiv.org/pdf/2508.03550.pdf", "abs": "https://arxiv.org/abs/2508.03550", "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations", "authors": ["Peng Lai", "Jianjie Zheng", "Sijie Cheng", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using large language models, a paradigm known as\n\"LLMas-a-judge.\" However, improving its alignment with human preferences\nwithout complex prompts or fine-tuning remains challenging. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and taskrelevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a lightweight and\nefficient framework for enhancing LLM-as-a-Judge alignment with human scoring,\nvia internal representations. LAGER produces fine-grained judgment scores by\naggregating cross-layer scoretoken logits and computing the expected score from\na softmax-based distribution, with the LLM backbone kept frozen. LAGER fully\nleverages the complementary information across different layers, overcoming the\nlimitations of relying solely on the final layer. We evaluate our method on the\nstandard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman\ncorrelation, and find that LAGER achieves improvements of up to 7.5% over the\nbest baseline across these benchmarks. Without reasoning steps, LAGER matches\nor outperforms reasoning-based methods. Experiments on downstream applications,\nsuch as data selection and emotional understanding, further show the\neffectiveness of our method."}
{"id": "2508.03571", "pdf": "https://arxiv.org/pdf/2508.03571.pdf", "abs": "https://arxiv.org/abs/2508.03571", "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation", "authors": ["Iing Muttakhiroh", "Thomas Fevens"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often suffer from performance degradation when\nfaced with domain shifts, primarily due to catastrophic forgetting. In this\nwork, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),\na novel continual learning framework that integrates dynamic knowledge graphs\nwith instruction tuning. By leveraging retrieved domain-specific knowledge as\nguidance during training, KILO enhances both adaptability to new domains and\nretention of previously acquired knowledge. We pretrain our model on\nWikiText-103 and evaluate sequential adaptation across four diverse target\ndomains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that\nKILO consistently outperforms strong baselines, including continual\nfine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward\ntransfer, F1 score, retention rate, and training efficiency. These results\nhighlight the effectiveness of combining structured knowledge retrieval and\ninstruction prompting to overcome domain shift challenges in continual learning\nscenarios."}
{"id": "2508.03644", "pdf": "https://arxiv.org/pdf/2508.03644.pdf", "abs": "https://arxiv.org/abs/2508.03644", "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?", "authors": ["Wenxuan Shen", "Mingjia Wang", "Yaochen Wang", "Dongping Chen", "Junjie Yang", "Yao Wan", "Weiwei Lin"], "categories": ["cs.CL", "cs.CV", "cs.IR"], "comment": "In submission. Project website: https://double-bench.github.io/", "summary": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis."}
{"id": "2508.03654", "pdf": "https://arxiv.org/pdf/2508.03654.pdf", "abs": "https://arxiv.org/abs/2508.03654", "title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?", "authors": ["Xinyu Wang", "Yue Zhang", "Liqiang Jing"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by CIKM 2025", "summary": "Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA."}
{"id": "2508.03668", "pdf": "https://arxiv.org/pdf/2508.03668.pdf", "abs": "https://arxiv.org/abs/2508.03668", "title": "CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction", "authors": ["Zixuan Li", "Binzong Geng", "Jing Xiong", "Yong He", "Yuxuan Hu", "Jian Chen", "Dingwei Chen", "Xiyu Chang", "Liang Zhang", "Linjian Mo", "Chengming Li", "Chuan Yuan", "Zhenan Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Click-Through Rate (CTR) prediction, a core task in recommendation systems,\nestimates user click likelihood using historical behavioral data. Modeling user\nbehavior sequences as text to leverage Language Models (LMs) for this task has\ngained traction, owing to LMs' strong semantic understanding and contextual\nmodeling capabilities. However, a critical structural gap exists: user behavior\nsequences consist of discrete actions connected by semantically empty\nseparators, differing fundamentally from the coherent natural language in LM\npre-training. This mismatch causes semantic fragmentation, where LM attention\nscatters across irrelevant tokens instead of focusing on meaningful behavior\nboundaries and inter-behavior relationships, degrading prediction performance.\nTo address this, we propose $\\textit{CTR-Sink}$, a novel framework introducing\nbehavior-level attention sinks tailored for recommendation scenarios. Inspired\nby attention sink theory, it constructs attention focus sinks and dynamically\nregulates attention aggregation via external information. Specifically, we\ninsert sink tokens between consecutive behaviors, incorporating\nrecommendation-specific signals such as temporal distance to serve as stable\nattention sinks. To enhance generality, we design a two-stage training strategy\nthat explicitly guides LM attention toward sink tokens and a attention sink\nmechanism that amplifies inter-sink dependencies to better capture behavioral\ncorrelations. Experiments on one industrial dataset and two open-source\ndatasets (MovieLens, Kuairec), alongside visualization results, validate the\nmethod's effectiveness across scenarios."}
{"id": "2508.03677", "pdf": "https://arxiv.org/pdf/2508.03677.pdf", "abs": "https://arxiv.org/abs/2508.03677", "title": "FairLangProc: A Python package for fairness in NLP", "authors": ["Arturo P√©rez-Peralta", "Sandra Ben√≠tez-Pe√±a", "Rosa E. Lillo"], "categories": ["cs.CL", "stat.ML", "68T50", "I.2.7"], "comment": "40 pages, 4 figures, 3 tables", "summary": "The rise in usage of Large Language Models to near ubiquitousness in recent\nyears has risen societal concern about their applications in decision-making\ncontexts, such as organizational justice or healthcare. This, in turn, poses\nquestions about the fairness of these models in critical settings, which leads\nto the developement of different procedures to address bias in Natural Language\nProcessing. Although many datasets, metrics and algorithms have been proposed\nto measure and mitigate harmful prejudice in Natural Language Processing, their\nimplementation is diverse and far from centralized. As a response, this paper\npresents FairLangProc, a comprehensive Python package providing a common\nimplementation of some of the more recent advances in fairness in Natural\nLanguage Processing providing an interface compatible with the famous Hugging\nFace transformers library, aiming to encourage the widespread use and\ndemocratization of bias mitigation techniques. The implementation can be found\non https://github.com/arturo-perez-peralta/FairLangProc."}
{"id": "2508.03678", "pdf": "https://arxiv.org/pdf/2508.03678.pdf", "abs": "https://arxiv.org/abs/2508.03678", "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation", "authors": ["Yangtian Zi", "Harshitha Menon", "Arjun Guha"], "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": null, "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement."}
{"id": "2508.03686", "pdf": "https://arxiv.org/pdf/2508.03686.pdf", "abs": "https://arxiv.org/abs/2508.03686", "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward", "authors": ["Shudong Liu", "Hongwei Liu", "Junnan Liu", "Linchen Xiao", "Songyang Gao", "Chengqi Lyu", "Yuzhe Gu", "Wenwei Zhang", "Derek F. Wong", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report; 31 Pages", "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier."}
{"id": "2507.10593", "pdf": "https://arxiv.org/pdf/2507.10593.pdf", "abs": "https://arxiv.org/abs/2507.10593", "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs", "authors": ["Peng Ding"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) applications are increasingly relying on external\ntools to extend their capabilities beyond text generation. However, current\ntool integration approaches suffer from fragmentation, protocol limitations,\nand implementation complexity, leading to substantial development overhead.\nThis paper presents Toolregistry, a protocol-agnostic tool management library\nthat simplifies tool registration, representation, execution, and lifecycle\nmanagement via a unified interface. Our evaluation demonstrates that\n\\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x\nperformance improvements through concurrent execution, and 100% compatibility\nwith OpenAI function calling standards. Real-world case studies show\nsignificant improvements in development efficiency and code maintainability\nacross diverse integration scenarios. \\toolregistry is open-source and\navailable at https://github.com/Oaklight/ToolRegistry, with comprehensive\ndocumentation at https://toolregistry.readthedocs.io/."}
{"id": "2508.02694", "pdf": "https://arxiv.org/pdf/2508.02694.pdf", "abs": "https://arxiv.org/abs/2508.02694", "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions."}
{"id": "2508.02731", "pdf": "https://arxiv.org/pdf/2508.02731.pdf", "abs": "https://arxiv.org/abs/2508.02731", "title": "Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education", "authors": ["Jean-Francois Chamberland", "Martin C. Carlisle", "Arul Jayaraman", "Krishna R. Narayanan", "Sunay Palsole", "Karan Watson"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating teaching effectiveness at scale remains a persistent challenge for\nlarge universities, particularly within engineering programs that enroll tens\nof thousands of students. Traditional methods, such as manual review of student\nevaluations, are often impractical, leading to overlooked insights and\ninconsistent data use. This article presents a scalable, AI-supported framework\nfor synthesizing qualitative student feedback using large language models. The\nsystem employs hierarchical summarization, anonymization, and exception\nhandling to extract actionable themes from open-ended comments while upholding\nethical safeguards. Visual analytics contextualize numeric scores through\npercentile-based comparisons, historical trends, and instructional load. The\napproach supports meaningful evaluation and aligns with best practices in\nqualitative analysis and educational assessment, incorporating student, peer,\nand self-reflective inputs without automating personnel decisions. We report on\nits successful deployment across a large college of engineering. Preliminary\nvalidation through comparisons with human reviewers, faculty feedback, and\nlongitudinal analysis suggests that LLM-generated summaries can reliably\nsupport formative evaluation and professional development. This work\ndemonstrates how AI systems, when designed with transparency and shared\ngovernance, can promote teaching excellence and continuous improvement at scale\nwithin academic institutions."}
{"id": "2508.02738", "pdf": "https://arxiv.org/pdf/2508.02738.pdf", "abs": "https://arxiv.org/abs/2508.02738", "title": "CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration", "authors": ["Yumeng Shi", "Zhongliang Yang", "DiYang Lu", "Yisi Wang", "Yiting Zhou", "Linna Zhou"], "categories": ["q-fin.ST", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "Corporate credit rating serves as a crucial intermediary service in the\nmarket economy, playing a key role in maintaining economic order. Existing\ncredit rating models rely on financial metrics and deep learning. However, they\noften overlook insights from non-financial data, such as corporate annual\nreports. To address this, this paper introduces a corporate credit rating\nframework that integrates financial data with features extracted from annual\nreports using FinBERT, aiming to fully leverage the potential value of\nunstructured text data. In addition, we have developed a large-scale dataset,\nthe Comprehensive Corporate Rating Dataset (CCRD), which combines both\ntraditional financial data and textual data from annual reports. The\nexperimental results show that the proposed method improves the accuracy of the\nrating predictions by 8-12%, significantly improving the effectiveness and\nreliability of corporate credit ratings."}
{"id": "2508.02823", "pdf": "https://arxiv.org/pdf/2508.02823.pdf", "abs": "https://arxiv.org/abs/2508.02823", "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification", "authors": ["Wenshuo Zhang", "Leixian Shen", "Shuchang Xu", "Jindu Wang", "Jian Zhao", "Huamin Qu", "Linping Yuan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted in UIST 2025", "summary": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency."}
{"id": "2508.02849", "pdf": "https://arxiv.org/pdf/2508.02849.pdf", "abs": "https://arxiv.org/abs/2508.02849", "title": "SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec", "authors": ["Chunyu Qiang", "Haoyu Wang", "Cheng Gong", "Tianrui Wang", "Ruibo Fu", "Tao Wang", "Ruilong Chen", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Speech codecs serve as a crucial bridge in unifying speech and text language\nmodels. Existing codec methods face several challenges in semantic encoding,\nsuch as residual paralinguistic information (e.g., timbre, emotion),\ninsufficient semantic completeness, limited reconstruction capability, and lack\nof support for streaming. To address these challenges, we propose\nSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that\ndisentangles semantic and paralinguistic information in a single-codebook\nspace. To ensure semantic completeness and reconstruction fidelity,\nparalinguistic encoding is introduced to bridge the information gap between\nsemantic and acoustic encoding. A semantic-only efficient quantization method\nbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is\nproposed. This approach alleviates the long-tail distribution problem of tokens\nwhile maintaining high codebook utilization. A semantic disentanglement method\nbased on contrastive learning is proposed, which aligns text and speech in a\njoint multimodal frame-level space, effectively removing paralinguistic\ninformation from semantic encoding. An acoustic-constrained multi-stage\noptimization strategy is proposed to ensure robust and stable convergence.\nFigure~\\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA\n(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.\nThe code and model weights for SecoustiCodec will be open-sourced upon the\ncompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,\ncode, and model weights."}
{"id": "2508.02890", "pdf": "https://arxiv.org/pdf/2508.02890.pdf", "abs": "https://arxiv.org/abs/2508.02890", "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction", "authors": ["Rongxin Jiang", "Robert Long", "Chenghao Gu", "Mingrui Yan"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications."}
{"id": "2508.02917", "pdf": "https://arxiv.org/pdf/2508.02917.pdf", "abs": "https://arxiv.org/abs/2508.02917", "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces", "authors": ["Vebj√∏rn Haug K√•sene", "Pierre Lison"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": "This paper has been accepted to ICNSLP 2025", "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task."}
{"id": "2508.02961", "pdf": "https://arxiv.org/pdf/2508.02961.pdf", "abs": "https://arxiv.org/abs/2508.02961", "title": "Defend LLMs Through Self-Consciousness", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms."}
{"id": "2508.02979", "pdf": "https://arxiv.org/pdf/2508.02979.pdf", "abs": "https://arxiv.org/abs/2508.02979", "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "authors": ["Peng Ding", "Rick Stevens"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development."}
{"id": "2508.02999", "pdf": "https://arxiv.org/pdf/2508.02999.pdf", "abs": "https://arxiv.org/abs/2508.02999", "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "M√≥nica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "categories": ["cs.AI", "cs.CL"], "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs."}
{"id": "2508.03058", "pdf": "https://arxiv.org/pdf/2508.03058.pdf", "abs": "https://arxiv.org/abs/2508.03058", "title": "VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision", "authors": ["Dingwei Zhu", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Guoqiang Zhang", "Jiazheng Zhang", "Junjie Ye", "Mingxu Chai", "Enyu Zhou", "Ming Zhang", "Caishuang Huang", "Yunke Zhang", "Yuran Wang", "Tao Gui"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or\nimperfect reward supervision in real-world settings, which undermines policy\nstability and generalization. Such noise may cause models to lose attention on\nkey words during advantage estimation. While prior work focuses on reward\ndenoising or filtering poor data, it often overlooks the critical role of the\nvalue model in policy optimization. In this work, we show that a strong value\nmodel is essential for mitigating noise by absorbing unstable signals and\nenabling more reliable advantage estimation. We propose VRPO, a value-centric\nframework for robust PPO training under noisy supervision. VRPO combines two\ncore designs: (1) an auxiliary loss guided by entropy and perplexity from a\nfrozen language model, and (2) a variational information bottleneck. These\nmechanisms enhance the value model's ability to filter out noise and capture\nkey words from the context during advantage estimation, transforming it from a\npassive predictor into an active regulator of noise. Experiments on math\nreasoning, science QA, and multi-turn dialogue, under both rule-based and\nmodel-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO\nbaselines. Our findings underscore the often-overlooked importance of the value\nmodel in RLHF and offer a principled and practical approach to robust policy\noptimization in noisy real-world environments."}
{"id": "2508.03092", "pdf": "https://arxiv.org/pdf/2508.03092.pdf", "abs": "https://arxiv.org/abs/2508.03092", "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking."}
{"id": "2508.03164", "pdf": "https://arxiv.org/pdf/2508.03164.pdf", "abs": "https://arxiv.org/abs/2508.03164", "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning", "authors": ["Junyoung Lim", "Jaewoo Ahn", "Gunhee Kim"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICCV 2025 (Highlight)", "summary": "Generating accurate, informative, and hallucination-free captions for charts\nremains challenging for vision language models, primarily due to the lack of\nlarge-scale, high-quality datasets of real-world charts. However, existing\nreal-world chart datasets suffer from the inclusion of extraneous information\nthat cannot be inferred from the chart and failure to sufficiently capture\nstructural elements and key insights. Therefore, we introduce ChartCap, a\nlarge-scale dataset of 565K real-world chart images paired with type-specific,\ndense captions that exclude extraneous information and highlight both\nstructural elements and key insights in detail. To build ChartCap, we design a\nfour-stage pipeline that generates captions using only the discernible data\nfrom the chart and employ a cycle consistency-based human verification, which\naccelerates quality control without sacrificing accuracy. Additionally, we\npropose a novel metric, the Visual Consistency Score, which evaluates caption\nquality by measuring the similarity between the chart regenerated from a\ncaption and the original chart, independent of reference captions. Extensive\nexperiments confirms that models fine-tuned on ChartCap consistently generate\nmore accurate and informative captions with reduced hallucinations, surpassing\nboth open-source and proprietary models and even human-annotated captions."}
{"id": "2508.03280", "pdf": "https://arxiv.org/pdf/2508.03280.pdf", "abs": "https://arxiv.org/abs/2508.03280", "title": "Understanding the Embedding Models on Hyper-relational Knowledge Graph", "authors": ["Yubo Wang", "Shimin Di", "Zhili Wang", "Haoyang Li", "Fei Teng", "Hao Xin", "Lei Chen"], "categories": ["cs.LG", "cs.CL", "cs.SI"], "comment": "Accepted by CIKM 2025", "summary": "Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an\nextension of traditional Knowledge Graphs (KGs) to better represent real-world\nfacts with additional qualifiers. As a result, researchers have attempted to\nadapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing\nextra qualifier processing modules. However, it remains unclear whether the\nsuperior performance of Hyper-relational KGE (HKGE) models arises from their\nbase KGE model or the specially designed extension module. Hence, in this\npaper, we data-wise convert HKGs to KG format using three decomposition methods\nand then evaluate the performance of several classical KGE models on HKGs. Our\nresults show that some KGE models achieve performance comparable to that of\nHKGE models. Upon further analysis, we find that the decomposition methods\nalter the original HKG topology and fail to fully preserve HKG information.\nMoreover, we observe that current HKGE models are either insufficient in\ncapturing the graph's long-range dependency or struggle to integrate\nmain-triple and qualifier information due to the information compression issue.\nTo further justify our findings and offer a potential direction for future HKGE\nresearch, we propose the FormerGNN framework. This framework employs a\nqualifier integrator to preserve the original HKG topology, and a GNN-based\ngraph encoder to capture the graph's long-range dependencies, followed by an\nimproved approach for integrating main-triple and qualifier information to\nmitigate compression issues. Our experimental results demonstrate that\nFormerGNN outperforms existing HKGE models."}
{"id": "2508.03306", "pdf": "https://arxiv.org/pdf/2508.03306.pdf", "abs": "https://arxiv.org/abs/2508.03306", "title": "Reliable Evaluation Protocol for Low-Precision Retrieval", "authors": ["Kisu Yang", "Yoonna Jang", "Hwanseok Jang", "Kenneth Choi", "Isabelle Augenstein", "Heuiseok Lim"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "11 pages, 5 figures, submitted to ARR", "summary": "Lowering the numerical precision of model parameters and computations is\nwidely adopted to improve the efficiency of retrieval systems. However, when\ncomputing relevance scores between the query and documents in low-precision, we\nobserve spurious ties due to the reduced granularity. This introduces high\nvariability in the results based on tie resolution, making the evaluation less\nreliable. To address this, we propose a more robust retrieval evaluation\nprotocol designed to reduce score variation. It consists of: (1) High-Precision\nScoring (HPS), which upcasts the final scoring step to higher precision to\nresolve tied candidates with minimal computational cost; and (2) Tie-aware\nRetrieval Metrics (TRM), which report expected scores, range, and bias to\nquantify order uncertainty of tied candidates. Our experiments test multiple\nmodels with three scoring functions on two retrieval datasets to demonstrate\nthat HPS dramatically reduces tie-induced instability, and TRM accurately\nrecovers expected metric values. This combination enables a more consistent and\nreliable evaluation system for lower-precision retrievals."}
{"id": "2508.03351", "pdf": "https://arxiv.org/pdf/2508.03351.pdf", "abs": "https://arxiv.org/abs/2508.03351", "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation", "authors": ["Yufei Xue", "Yushi Huang", "Jiawei Shao", "Jun Zhang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "13 pages, 5 figures", "summary": "Post-training quantization (PTQ) has emerged as an effective approach for\ncompressing large models and accelerating their inference without retraining.\nWhile PTQ has been extensively studied in the context of large language models\n(LLMs), its applicability to vision-language models (VLMs) remains\nunderexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.},\nlimited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs.\nHowever, existing Hessian-based LLM PTQ methods treat all tokens equally during\nquantization, resulting in severe performance drops when applied to VLMs.\nMotivated by this observation, we propose a novel importance-aware PTQ\nframework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token\nredundancy, VLMQ 1) optimizes an importance-aware objective that yields an\nenhanced Hessian with token-level importance factors, while retaining\ncompatibility with parallelized weight updates, and 2) ensures efficiency and\neffectiveness by computing these factors via a single lightweight block-wise\nbackward pass, guided by a theoretical connection to token-level perturbations.\nExtensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the\nstate-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit\nsettings. For example, it achieves a substantial \\textbf{16.45\\%} improvement\non MME-RealWorld under 2-bit quantization."}
{"id": "2508.03366", "pdf": "https://arxiv.org/pdf/2508.03366.pdf", "abs": "https://arxiv.org/abs/2508.03366", "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "authors": ["Michael K. Chen"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input."}
{"id": "2508.03481", "pdf": "https://arxiv.org/pdf/2508.03481.pdf", "abs": "https://arxiv.org/abs/2508.03481", "title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models", "authors": ["Hyungjin Kim", "Seokho Ahn", "Young-Duk Seo"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at ICCV 2025", "summary": "Personalized generation in T2I diffusion models aims to naturally incorporate\nindividual user preferences into the generation process with minimal user\nintervention. However, existing studies primarily rely on prompt-level modeling\nwith large-scale models, often leading to inaccurate personalization due to the\nlimited input token capacity of T2I diffusion models. To address these\nlimitations, we propose DrUM, a novel method that integrates user profiling\nwith a transformer-based adapter to enable personalized generation through\ncondition-level modeling in the latent space. DrUM demonstrates strong\nperformance on large-scale datasets and seamlessly integrates with open-source\ntext encoders, making it compatible with widely used foundation T2I models\nwithout requiring additional fine-tuning."}
{"id": "2508.03501", "pdf": "https://arxiv.org/pdf/2508.03501.pdf", "abs": "https://arxiv.org/abs/2508.03501", "title": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning", "authors": ["Alexander Golubev", "Maria Trofimova", "Sergei Polezhaev", "Ibragim Badertdinov", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Sergey Abramov", "Andrei Andriushchenko", "Filipp Fisin", "Sergei Skvortsov", "Boris Yangel"], "categories": ["cs.LG", "cs.CL", "cs.SE"], "comment": null, "summary": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models."}
{"id": "2508.03527", "pdf": "https://arxiv.org/pdf/2508.03527.pdf", "abs": "https://arxiv.org/abs/2508.03527", "title": "MoKA: Mixture of Kronecker Adapters", "authors": ["Mohammadreza Sadeghi", "Mahsa Ghazvini Nejad", "MirHamed Jafarzadeh Asl", "Yu Gu", "Yuanhao Yu", "Masoud Asgharian", "Vahid Partovi Nia"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) is essential for reducing the\ncomputational overhead of large language models (LLMs). Low-rank family\nadapters are commonly used to control the parameter size efficiently while\nmaintaining the generative power of LLMs. However, their limited expressiveness\ndue to the rank constraint often restricts their performance on complex tasks.\nWe propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker\nadapters that addresses this limitation by modeling weight updates as a mixture\nof Kronecker products. Our proposed adapter leverages a gating mechanism that\nmeasures the importance of each Kronecker factor, enabling more expressive\nadaptation. Moreover, MoKA enables a rank flexibility that provides a better\ntrade-off between parameter efficiency and accuracy. To ensure hardware\nefficiency, we reformulate Kronecker computations using standard matrix\noperations, allowing seamless deployment on GPU-optimized hardware. We conduct\nextensive experiments on instruction-tuning and commonsense reasoning tasks\nusing low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not\nonly outperforms PEFT baselines, but also reduces the number of trainable\nparameters up to 27x, achieving state-of-the-art trade-offs between performance\nand parameter efficiency."}
{"id": "2508.03553", "pdf": "https://arxiv.org/pdf/2508.03553.pdf", "abs": "https://arxiv.org/abs/2508.03553", "title": "MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation", "authors": ["Wenlong Wu", "Haofen Wang", "Bohan Li", "Peixuan Huang", "Xinzhe Zhao", "Lei Liang"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by ICDE 2025 Research Paper", "summary": "Retrieval Augmented Generation (RAG) has emerged as a promising solution to\naddress hallucination issues in Large Language Models (LLMs). However, the\nintegration of multiple retrieval sources, while potentially more informative,\nintroduces new challenges that can paradoxically exacerbate hallucination\nproblems. These challenges manifest primarily in two aspects: the sparse\ndistribution of multi-source data that hinders the capture of logical\nrelationships and the inherent inconsistencies among different sources that\nlead to information conflicts. To address these challenges, we propose\nMultiRAG, a novel framework designed to mitigate hallucination in multi-source\nretrieval-augmented generation through knowledge-guided approaches. Our\nframework introduces two key innovations: (1) a knowledge construction module\nthat employs multi-source line graphs to efficiently aggregate logical\nrelationships across different knowledge sources, effectively addressing the\nsparse data distribution issue; and (2) a sophisticated retrieval module that\nimplements a multi-level confidence calculation mechanism, performing both\ngraph-level and node-level assessments to identify and eliminate unreliable\ninformation nodes, thereby reducing hallucinations caused by inter-source\ninconsistencies. Extensive experiments on four multi-domain query datasets and\ntwo multi-hop QA datasets demonstrate that MultiRAG significantly enhances the\nreliability and efficiency of knowledge retrieval in complex multi-source\nscenarios. \\textcolor{blue}{Our code is available in\nhttps://github.com/wuwenlong123/MultiRAG."}
{"id": "2508.03555", "pdf": "https://arxiv.org/pdf/2508.03555.pdf", "abs": "https://arxiv.org/abs/2508.03555", "title": "PyLate: Flexible Training and Retrieval for Late Interaction Models", "authors": ["Antoine Chaffin", "Rapha√´l Sourty"], "categories": ["cs.IR", "cs.CL"], "comment": "5 pages", "summary": "Neural ranking has become a cornerstone of modern information retrieval.\nWhile single vector search remains the dominant paradigm, it suffers from the\nshortcoming of compressing all the information into a single vector. This\ncompression leads to notable performance degradation in out-of-domain,\nlong-context, and reasoning-intensive retrieval tasks. Multi-vector approaches\npioneered by ColBERT aim to address these limitations by preserving individual\ntoken embeddings and computing similarity via the MaxSim operator. This\narchitecture has demonstrated superior empirical advantages, including enhanced\nout-of-domain generalization, long-context handling, and performance in complex\nretrieval scenarios. Despite these compelling empirical results and clear\ntheoretical advantages, the practical adoption and public availability of late\ninteraction models remain low compared to their single-vector counterparts,\nprimarily due to a lack of accessible and modular tools for training and\nexperimenting with such models. To bridge this gap, we introduce PyLate, a\nstreamlined library built on top of Sentence Transformers to support\nmulti-vector architectures natively, inheriting its efficient training,\nadvanced logging, and automated model card generation while requiring minimal\ncode changes to code templates users are already familiar with. By offering\nmulti-vector-specific features such as efficient indexes, PyLate aims to\naccelerate research and real-world application of late interaction models,\nthereby unlocking their full potential in modern IR systems. Finally, PyLate\nhas already enabled the development of state-of-the-art models, including\nGTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility\nfor both research and production environments."}
{"id": "2508.03562", "pdf": "https://arxiv.org/pdf/2508.03562.pdf", "abs": "https://arxiv.org/abs/2508.03562", "title": "Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching", "authors": ["Muzhaffar Hazman", "Susan McKeever", "Josephine Griffith"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted for publication at IEEE International Conference on Image\n  Processing Theory, Tools and Applications (IPTA) 2025", "summary": "Internet memes, now a staple of digital communication, play a pivotal role in\nhow users engage within online communities and allow researchers to gain\ninsight into contemporary digital culture. These engaging user-generated\ncontent are characterised by their reuse of visual elements also found in other\nmemes. Matching instances of memes via these shared visual elements, called\nMeme Matching, is the basis of a wealth of meme analysis approaches. However,\nmost existing methods assume that every meme consists of a shared visual\nbackground, called a Template, with some overlaid text, thereby limiting meme\nmatching to comparing the background image alone. Current approaches exclude\nthe many memes that are not template-based and limit the effectiveness of\nautomated meme analysis and would not be effective at linking memes to\ncontemporary web-based meme dictionaries. In this work, we introduce a broader\nformulation of meme matching that extends beyond template matching. We show\nthat conventional similarity measures, including a novel segment-wise\ncomputation of the similarity measures, excel at matching template-based memes\nbut fall short when applied to non-template-based meme formats. However, the\nsegment-wise approach was found to consistently outperform the whole-image\nmeasures on matching non-template-based memes. Finally, we explore a\nprompting-based approach using a pretrained Multimodal Large Language Model for\nmeme matching. Our results highlight that accurately matching memes via shared\nvisual elements, not just background templates, remains an open challenge that\nrequires more sophisticated matching techniques."}
{"id": "2508.03599", "pdf": "https://arxiv.org/pdf/2508.03599.pdf", "abs": "https://arxiv.org/abs/2508.03599", "title": "OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War", "authors": ["Johannes Niu", "Mila Stillman", "Anna Kruspe"], "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "This paper examines the role of Open Source Intelligence (OSINT) on Twitter\nregarding the Russo-Ukrainian war, distinguishing between genuine OSINT and\ndeceptive misinformation efforts, termed \"BULLSHINT.\" Utilizing a dataset\nspanning from January 2022 to July 2023, we analyze nearly 2 million tweets\nfrom approximately 1,040 users involved in discussing real-time military\nengagements, strategic analyses, and misinformation related to the conflict.\nUsing sentiment analysis, partisanship detection, misinformation\nidentification, and Named Entity Recognition (NER), we uncover communicative\npatterns and dissemination strategies within the OSINT community. Significant\nfindings reveal a predominant negative sentiment influenced by war events, a\nnuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the\npotential strategic manipulation of information. Additionally, we apply\ncommunity detection techniques, which are able to identify distinct clusters\npartisanship, topics, and misinformation, highlighting the complex dynamics of\ninformation spread on social media. This research contributes to the\nunderstanding of digital warfare and misinformation dynamics, offering insights\ninto the operationalization of OSINT in geopolitical conflicts."}
{"id": "2508.03663", "pdf": "https://arxiv.org/pdf/2508.03663.pdf", "abs": "https://arxiv.org/abs/2508.03663", "title": "Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation", "authors": ["Deepak Pandita", "Flip Korn", "Chris Welty", "Christopher M. Homan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reproducibility is a cornerstone of scientific validation and of the\nauthority it confers on its results. Reproducibility in machine learning\nevaluations leads to greater trust, confidence, and value. However, the ground\ntruth responses used in machine learning often necessarily come from humans,\namong whom disagreement is prevalent, and surprisingly little research has\nstudied the impact of effectively ignoring disagreement in these responses, as\nis typically the case. One reason for the lack of research is that budgets for\ncollecting human-annotated evaluation data are limited, and obtaining more\nsamples from multiple annotators for each example greatly increases the\nper-item annotation costs. We investigate the trade-off between the number of\nitems ($N$) and the number of responses per item ($K$) needed for reliable\nmachine learning evaluation. We analyze a diverse collection of categorical\ndatasets for which multiple annotations per item exist, and simulated\ndistributions fit to these datasets, to determine the optimal $(N, K)$\nconfiguration, given a fixed budget ($N \\times K$), for collecting evaluation\ndata and reliably comparing the performance of machine learning models. Our\nfindings show, first, that accounting for human disagreement may come with $N\n\\times K$ at no more than 1000 (and often much lower) for every dataset tested\non at least one metric. Moreover, this minimal $N \\times K$ almost always\noccurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and\n$N$ -- or if one even existed -- depends on the evaluation metric, with metrics\nthat are more sensitive to the full distribution of responses performing better\nat higher levels of $K$. Our methods can be used to help ML practitioners get\nmore effective test data by finding the optimal metrics and number of items and\nannotations per item to collect to get the most reliability for their budget."}
{"id": "2111.05671", "pdf": "https://arxiv.org/pdf/2111.05671.pdf", "abs": "https://arxiv.org/abs/2111.05671", "title": "Pre-trained Transformer-Based Approach for Arabic Question Answering : A Comparative Study", "authors": ["Kholoud Alsubhi", "Amani Jamal", "Areej Alhothali"], "categories": ["cs.CL"], "comment": "Rewrite the paper", "summary": "Question answering(QA) is one of the most challenging yet widely investigated\nproblems in Natural Language Processing (NLP). Question-answering (QA) systems\ntry to produce answers for given questions. These answers can be generated from\nunstructured or structured text. Hence, QA is considered an important research\narea that can be used in evaluating text understanding systems. A large volume\nof QA studies was devoted to the English language, investigating the most\nadvanced techniques and achieving state-of-the-art results. However, research\nefforts in the Arabic question-answering progress at a considerably slower pace\ndue to the scarcity of research efforts in Arabic QA and the lack of large\nbenchmark datasets. Recently many pre-trained language models provided high\nperformance in many Arabic NLP problems. In this work, we evaluate the\nstate-of-the-art pre-trained transformers models for Arabic QA using four\nreading comprehension datasets which are Arabic-SQuAD, ARCD, AQAD, and\nTyDiQA-GoldP datasets. We fine-tuned and compared the performance of the\nAraBERTv2-base model, AraBERTv0.2-large model, and AraELECTRA model. In the\nlast, we provide an analysis to understand and interpret the low-performance\nresults obtained by some models."}
{"id": "2408.06787", "pdf": "https://arxiv.org/pdf/2408.06787.pdf", "abs": "https://arxiv.org/abs/2408.06787", "title": "Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions", "authors": ["Bo Xue", "Yi Xu", "Yunchong Song", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). By contrast, Large Language Models (LLMs) encapsulate extensive\nworld knowledge and exhibit powerful context modeling capabilities, making them\npromising for mitigating the limitations of traditional methods. However,\ndirect fine-tuning of LLMs for KGC, though effective, imposes substantial\ncomputational and memory overheads, while utilizing non-fine-tuned LLMs is\nefficient but yields suboptimal performance. In this work, we propose a novel\nframework that synergizes the strengths of LLMs with robust knowledge\nrepresentation to enable effective and efficient KGC. We extract the\ncontext-aware hidden states of knowledge triples from the intermediate layers\nof LLMs, thereby capturing rich semantic and relational nuances. These\nrepresentations are then utilized to train a data-efficient classifier tailored\nspecifically for KGC tasks. To bridge the semantic gaps between LLMs and KGs,\nwe employ subgraph sampling on KGs to generate model-friendly entity\ndescriptions. We further adopt sliced mutual information (SMI) as a principled\nmetric to quantify the task-specific information encoded in these\nrepresentations. Extensive experiments on standard benchmarks validate the\nefficiency and effectiveness of our approach. We achieve a 47\\% relative\nimprovement over previous methods based on non-fine-tuned LLMs and, to our\nknowledge, are the first to achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $26.11\\times$."}
{"id": "2409.16647", "pdf": "https://arxiv.org/pdf/2409.16647.pdf", "abs": "https://arxiv.org/abs/2409.16647", "title": "Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data", "authors": ["Kota Dohi", "Aoi Ito", "Harsh Purohit", "Tomoya Nishida", "Takashi Endo", "Yohei Kawaguchi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Due to scarcity of time-series data annotated with descriptive texts,\ntraining a model to generate descriptive texts for time-series data is\nchallenging. In this study, we propose a method to systematically generate\ndomain-independent descriptive texts from time-series data. We identify two\ndistinct approaches for creating pairs of time-series data and descriptive\ntexts: the forward approach and the backward approach. By implementing the\nnovel backward approach, we create the Temporal Automated Captions for\nObservations (TACO) dataset. Experimental results demonstrate that a\ncontrastive learning based model trained using the TACO dataset is capable of\ngenerating descriptive texts for time-series data in novel domains."}
{"id": "2412.01131", "pdf": "https://arxiv.org/pdf/2412.01131.pdf", "abs": "https://arxiv.org/abs/2412.01131", "title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Simone Teufel", "Takenobu Tokunaga"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, much work has concerned itself with the enigma of what exactly\npretrained language models~(PLMs) learn about different aspects of language,\nand how they learn it. One stream of this type of research investigates the\nknowledge that PLMs have about semantic relations. However, many aspects of\nsemantic relations were left unexplored. Generally, only one relation has been\nconsidered, namely hypernymy. Furthermore, previous work did not measure\nhumans' performance on the same task as that performed by the PLMs. This means\nthat at this point in time, there is only an incomplete view of the extent of\nthese models' semantic relation knowledge. To address this gap, we introduce a\ncomprehensive evaluation framework covering five relations beyond hypernymy,\nnamely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use five\nmetrics (two newly introduced here) for recently untreated aspects of semantic\nrelation knowledge, namely soundness, completeness, symmetry, prototypicality,\nand distinguishability. Using these, we can fairly compare humans and models on\nthe same task. Our extensive experiments involve six PLMs, four masked and two\ncausal language models. The results reveal a significant knowledge gap between\nhumans and models for all semantic relations. In general, causal language\nmodels, despite their wide use, do not always perform significantly better than\nmasked language models. Antonymy is the outlier relation where all models\nperform reasonably well. The evaluation materials can be found at\nhttps://github.com/hancules/ProbeResponses."}
{"id": "2412.08920", "pdf": "https://arxiv.org/pdf/2412.08920.pdf", "abs": "https://arxiv.org/abs/2412.08920", "title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning", "authors": ["Pusen Dong", "Tianchen Zhu", "Yue Qiu", "Haoyi Zhou", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NeurIPS 2024", "summary": "Safe reinforcement learning (RL) requires the agent to finish a given task\nwhile obeying specific constraints. Giving constraints in natural language form\nhas great potential for practical scenarios due to its flexible transfer\ncapability and accessibility. Previous safe RL methods with natural language\nconstraints typically need to design cost functions manually for each\nconstraint, which requires domain expertise and lacks flexibility. In this\npaper, we harness the dual role of text in this task, using it not only to\nprovide constraint but also as a training signal. We introduce the\nTrajectory-level Textual Constraints Translator (TTCT) to replace the manually\ndesigned cost function. Our empirical results demonstrate that TTCT effectively\ncomprehends textual constraint and trajectory, and the policies trained by TTCT\ncan achieve a lower violation rate than the standard cost function. Extra\nstudies are conducted to demonstrate that the TTCT has zero-shot transfer\ncapability to adapt to constraint-shift environments."}
{"id": "2501.13836", "pdf": "https://arxiv.org/pdf/2501.13836.pdf", "abs": "https://arxiv.org/abs/2501.13836", "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to AIES 2025", "summary": "Most social media users come from the Global South, where harmful content\nusually appears in local languages. Yet, AI-driven moderation systems struggle\nwith low-resource languages spoken in these regions. Through semi-structured\ninterviews with 22 AI experts working on harmful content detection in four\nlow-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi\nArabic (North Africa), and Quechua (South America)--we examine systemic issues\nin building automated moderation tools for these languages. Our findings reveal\nthat beyond data scarcity, socio-political factors such as tech companies'\nmonopoly on user data and lack of investment in moderation for low-profit\nGlobal South markets exacerbate historic inequities. Even if more data were\navailable, the English-centric and data-intensive design of language models and\npreprocessing techniques overlooks the need to design for morphologically\ncomplex, linguistically diverse, and code-mixed languages. We argue these\nlimitations are not just technical gaps caused by \"data scarcity\" but reflect\nstructural inequities, rooted in colonial suppression of non-Western languages.\nWe discuss multi-stakeholder approaches to strengthen local research capacity,\ndemocratize data access, and support language-aware solutions to improve\nautomated moderation for low-resource languages."}
{"id": "2501.16154", "pdf": "https://arxiv.org/pdf/2501.16154.pdf", "abs": "https://arxiv.org/abs/2501.16154", "title": "AdaMCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Multilingual Chain-of-Thought", "authors": ["Weihua Zheng", "Xin Huang", "Zhengyuan Liu", "Tarun Kumar Vangani", "Bowei Zou", "Xiyan Tao", "Yuhao Wu", "Ai Ti Aw", "Nancy F. Chen", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. Although these models show strong\nreasoning abilities, their performance varies significantly between languages\ndue to the imbalanced distribution of training data. Existing approaches using\nsample-level translation for extensive multilingual pretraining and\ncross-lingual tuning face scalability challenges and often fail to capture\nnuanced reasoning processes across languages. In this paper, we introduce\nAdaMCOT (Adaptive Multilingual Chain-of-Thought), a framework that enhances\nmultilingual factual reasoning by dynamically routing thought processes in\nintermediary \"thinking languages\" before generating target-language responses.\nAdaMCOT leverages a language-agnostic core and incorporates an adaptive,\nreward-based mechanism for selecting optimal reasoning pathways without\nrequiring additional pretraining. Our comprehensive evaluation across multiple\nbenchmarks demonstrates substantial improvements in both factual reasoning\nquality and cross-lingual consistency, with particularly strong performance\ngains in low-resource language settings. An in-depth analysis of the model's\nhidden states and semantic space further elucidates the underlying mechanism of\nour method. The results suggest that adaptive reasoning paths can effectively\nbridge the performance gap between high and low-resource languages while\nmaintaining cultural and linguistic nuances."}
{"id": "2502.14854", "pdf": "https://arxiv.org/pdf/2502.14854.pdf", "abs": "https://arxiv.org/abs/2502.14854", "title": "CLIPPER: Compression enables long-context synthetic data generation", "authors": ["Chau Minh Pham", "Yapei Chang", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA)."}
{"id": "2503.04856", "pdf": "https://arxiv.org/pdf/2503.04856.pdf", "abs": "https://arxiv.org/abs/2503.04856", "title": "M2S: Multi-turn to Single-turn jailbreak in Red Teaming for LLMs", "authors": ["Junwoo Ha", "Hyunjun Kim", "Sangyoon Yu", "Haon Park", "Ashkan Yousefpour", "Yuna Park", "Suhyun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Track). Camera-ready version", "summary": "We introduce a novel framework for consolidating multi-turn adversarial\n``jailbreak'' prompts into single-turn queries, significantly reducing the\nmanual overhead required for adversarial testing of large language models\n(LLMs). While multi-turn human jailbreaks have been shown to yield high attack\nsuccess rates, they demand considerable human effort and time. Our\nmulti-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize\n-- systematically reformat multi-turn dialogues into structured single-turn\nprompts. Despite removing iterative back-and-forth interactions, these prompts\npreserve and often enhance adversarial potency: in extensive evaluations on the\nMulti-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success\nrates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs.\nRemarkably, the single-turn prompts outperform the original multi-turn attacks\nby as much as 17.5 percentage points while cutting token usage by more than\nhalf on average. Further analysis shows that embedding malicious requests in\nenumerated or code-like structures exploits ``contextual blindness'', bypassing\nboth native guardrails and external input-output filters. By converting\nmulti-turn conversations into concise single-turn prompts, the M2S framework\nprovides a scalable tool for large-scale red teaming and reveals critical\nweaknesses in contemporary LLM defenses."}
{"id": "2503.05347", "pdf": "https://arxiv.org/pdf/2503.05347.pdf", "abs": "https://arxiv.org/abs/2503.05347", "title": "GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for Radiology Report Evaluation", "authors": ["Zhenxuan Zhang", "Kinhei Lee", "Peiyuan Jing", "Weihang Deng", "Huichi Zhou", "Zihao Jin", "Jiahao Huang", "Zhifan Gao", "Dominic C Marshall", "Yingying Fang", "Guang Yang"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Automatic medical report generation has the potential to support clinical\ndiagnosis, reduce the workload of radiologists, and demonstrate potential for\nenhancing diagnostic consistency. However, current evaluation metrics often\nfail to reflect the clinical reliability of generated reports. Early\noverlap-based methods focus on textual matches between predicted and\nground-truth entities but miss fine-grained clinical details (e.g., anatomical\nlocation, severity). Some diagnostic metrics are limited by fixed vocabularies\nor templates, reducing their ability to capture diverse clinical expressions.\nLLM-based approaches further lack interpretable reasoning steps, making it hard\nto assess or trust their behavior in safety-critical settings. These\nlimitations hinder the comprehensive assessment of the reliability of generated\nreports and pose risks in their selection for clinical use. Therefore, we\npropose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper,\nwhich conducts both objective quantification and subjective evaluation through\na large language model-based multi-agent workflow. Our GEMA-Score parses\nstructured reports and employs stable calculations through interactive\nexchanges of information among agents to assess disease diagnosis, location,\nseverity, and uncertainty. Additionally, an LLM-based scoring agent evaluates\ncompleteness, readability, and clinical terminology while providing explanatory\nfeedback. Extensive experiments validate that GEMA-Score achieves the highest\ncorrelation with human expert evaluations on a public dataset, demonstrating\nits effectiveness in clinical scoring (Kendall coefficient = $0.69$ for ReXVal\ndataset and Kendall coefficient = $0.45$ for RadEvalX dataset). The anonymous\nproject demo is available at: https://github.com/Zhenxuan-Zhang/GEMA_score."}
{"id": "2503.11881", "pdf": "https://arxiv.org/pdf/2503.11881.pdf", "abs": "https://arxiv.org/abs/2503.11881", "title": "GPT is Devastated and LLaMA is Content: Emotion Representation Alignment in LLMs for Keyword-based Generation", "authors": ["Shadab Choudhury", "Asha Kumar", "Lara J. Martin"], "categories": ["cs.CL"], "comment": null, "summary": "In controlled text generation using large language models (LLMs), gaps arise\nbetween the language model's interpretation of concepts and people's\nexpectations. We introduce the human evaluation task of Representation\nAlignment for measuring this gap. We selected four emotion representations:\nWords, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and\nNumeric forms, and Emojis and evaluate them in the context of keyword-guided\nsentence generation using both GPT-4 and LLaMA-3. In addition to Representation\nAlignment, we also measure people's judgments of the accuracy and realism of\nthe generated sentences. While representations like VAD break emotions into\neasy-to-compute components, our findings show that people agree more with how\nLLMs generate when conditioned on English words (e.g., ``angry'') rather than\nVAD scales. This difference is especially visible when comparing Numeric VAD to\nwords. Furthermore, we found that the perception of how much a generated\nsentence conveys an emotion is dependent on both the representation type and\nwhich emotion it is."}
{"id": "2503.13505", "pdf": "https://arxiv.org/pdf/2503.13505.pdf", "abs": "https://arxiv.org/abs/2503.13505", "title": "Ensemble Learning for Large Language Models in Text and Code Generation: A Survey", "authors": ["Mari Ashiga", "Wei Jie", "Fan Wu", "Vardan Voskanyan", "Fateme Dinmohammadi", "Paul Brookes", "Jingzhi Gong", "Zheng Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review by IEEE TAI", "summary": "Generative Pretrained Transformers (GPTs) are foundational Large Language\nModels (LLMs) for text generation. However, individual LLMs often produce\ninconsistent outputs and exhibit biases, limiting their representation of\ndiverse language patterns. The closed-source nature of many powerful LLMs\nfurther restricts industry applications due to data privacy concerns. Inspired\nby successes in text generation, LLM ensemble techniques are now increasingly\nexplored for code generation. This article reviews these emerging ensemble\napproaches to enhance understanding, encourage further research, and promote\npractical implementation in both text and code generation. We categorize LLM\nensembles into seven main methods - weight merging, knowledge fusion,\nmixture-of-experts, reward ensemble, output ensemble, routing, and cascading -\nanalyzing capabilities of those approaches. Our findings highlight key benefits\nsuch as improved diversity representation, enhanced output quality, and greater\napplication flexibility. These insights aid model selection for real-world\ntasks and crucially, lay groundwork for extending ensemble strategies to\nmultimodal LLMs."}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756.pdf", "abs": "https://arxiv.org/abs/2503.20756", "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "ACM MM 2025", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit/blob/main/examples/ADSEdit.md."}
{"id": "2504.02732", "pdf": "https://arxiv.org/pdf/2504.02732.pdf", "abs": "https://arxiv.org/abs/2504.02732", "title": "Why do LLMs attend to the first token?", "authors": ["Federico Barbero", "√Ålvaro Arroyo", "Xiangming Gu", "Christos Perivolaropoulos", "Michael Bronstein", "Petar Veliƒçkoviƒá", "Razvan Pascanu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."}
{"id": "2504.06219", "pdf": "https://arxiv.org/pdf/2504.06219.pdf", "abs": "https://arxiv.org/abs/2504.06219", "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs", "authors": ["Dongyang Fan", "Vinko Sabolƒçec", "Matin Ansaripour", "Ayush Kumar Tarun", "Martin Jaggi", "Antoine Bosselut", "Imanol Schlag"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025 Camera Ready version", "summary": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions. Our website is available at\nhttps://data-compliance.github.io/."}
{"id": "2504.07724", "pdf": "https://arxiv.org/pdf/2504.07724.pdf", "abs": "https://arxiv.org/abs/2504.07724", "title": "The Multi-Round Diagnostic RAG Framework for Emulating Clinical Reasoning", "authors": ["Penglei Sun", "Yixiang Chen", "Xiang Li", "Xiaowen Chu"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a trend. Among these, retrieval-augmented generation\n(RAG) has garnered attention due to rapid deployment and privacy protection.\nHowever, the challenge hinder the practical deployment of RAG for medical\ndiagnosis: the semantic gap between colloquial patient descriptions and the\nprofessional terminology within medical knowledge bases. We try to address the\nchallenge from the data perspective and the method perspective. First, to\naddress the semantic gap in existing knowledge bases, we construct\nDiagnosGraph, a generalist knowledge graph covering both modern medicine and\nTraditional Chinese Medicine. It contains 876 common diseases with the graph of\n7,997 nodes and 37,201 triples. To bridge the gap between colloquial patient\nnarratives and academic medical knowledge, DiagnosGraph also introduces $1,908$\nmedical record by formalizing the patient chief complaint and proposing a\nmedical diagnosis. Second, we introduce the Multi-Round Diagnostic RAG\n(MRD-RAG) framework. It utilizes a multi-round dialogue to refine diagnostic\npossibilities, emulating the clinical reasoning of a physician. Experiments\nconducted on four medical benchmarks, with evaluations by human physicians,\ndemonstrate that MRD-RAG enhances the diagnostic performance of LLMs,\nhighlighting its potential to make automated diagnosis more accurate and\nhuman-aligned."}
{"id": "2504.12326", "pdf": "https://arxiv.org/pdf/2504.12326.pdf", "abs": "https://arxiv.org/abs/2504.12326", "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis", "authors": ["Shahriar Noroozizadeh", "Jeremy C. Weiss"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration."}
{"id": "2504.13134", "pdf": "https://arxiv.org/pdf/2504.13134.pdf", "abs": "https://arxiv.org/abs/2504.13134", "title": "Energy-Based Reward Models for Robust Language Model Alignment", "authors": ["Anamika Lochab", "Ruqi Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Accepted by COLM 2025", "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."}
{"id": "2504.13834", "pdf": "https://arxiv.org/pdf/2504.13834.pdf", "abs": "https://arxiv.org/abs/2504.13834", "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific knowledge is growing rapidly, making it difficult to track\nprogress and high-level conceptual links across broad disciplines. While tools\nlike citation networks and search engines help retrieve related papers, they\nlack the abstraction needed to capture the needed to represent the density and\nstructure of activity across subfields.\n  We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific\nliterature into a high-quality hierarchical structure that spans multiple\nlevels of abstraction -- from broad domains to specific studies. Such a\nrepresentation can provide insights into which fields are well-explored and\nwhich are under-explored. To achieve this goal, we develop a hybrid approach\nthat combines efficient embedding-based clustering with LLM-based prompting,\nstriking a balance between scalability and semantic precision. Compared to\nLLM-heavy methods like iterative tree construction, our approach achieves\nsuperior quality-speed trade-offs. Our hierarchies capture different dimensions\nof research contributions, reflecting the interdisciplinary and multifaceted\nnature of modern science. We evaluate its utility by measuring how effectively\nan LLM-based agent can navigate the hierarchy to locate target papers. Results\nshow that our method improves interpretability and offers an alternative\npathway for exploring scientific literature beyond traditional search methods.\nCode, data and demo are available:\nhttps://github.com/JHU-CLSP/science-hierarchography"}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720.pdf", "abs": "https://arxiv.org/abs/2504.17720", "title": "Multilingual Performance Biases of Large Language Models in Education", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vil√©m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\neight languages (Mandarin, Hindi, Arabic, German, Farsi, Telugu, Ukrainian,\nCzech) in addition to English. We find that the performance on these tasks\nsomewhat corresponds to the amount of language represented in training data,\nwith lower-resource languages having poorer task performance. Although the\nmodels perform reasonably well in most languages, the frequent performance drop\nfrom English is significant. Thus, we recommend that practitioners first verify\nthat the LLM works well in the target language for their educational task\nbefore deployment."}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247.pdf", "abs": "https://arxiv.org/abs/2505.18247", "title": "MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) struggles with domain-specific\nenterprise datasets, often isolated behind firewalls and rich in complex,\nspecialized terminology unseen by LLMs during pre-training. Semantic\nvariability across domains like medicine, networking, or law hampers RAG's\ncontext precision, while fine-tuning solutions are costly, slow, and lack\ngeneralization as new data emerges. Achieving zero-shot precision with\nretrievers without fine-tuning still remains a key challenge. We introduce\n'MetaGen Blended RAG', a novel enterprise search approach that enhances\nsemantic retrievers through a metadata generation pipeline and hybrid query\nindexes using dense and sparse vectors. By leveraging key concepts, topics, and\nacronyms, our method creates metadata-enriched semantic indexes and boosted\nhybrid queries, delivering robust, scalable performance without fine-tuning. On\nthe biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks\nand even rivaling fine-tuned models on that dataset, while also excelling on\ndatasets like SQuAD and NQ. This approach redefines enterprise search using a\nnew approach to building semantic retrievers with unmatched generalization\nacross specialized domains."}
{"id": "2506.05070", "pdf": "https://arxiv.org/pdf/2506.05070.pdf", "abs": "https://arxiv.org/abs/2506.05070", "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation", "authors": ["Tianjiao Li", "Mengran Yu", "Chenyu Shi", "Yanjun Zhao", "Xiaojing Liu", "Qiang Zhang", "Qi Zhang", "Xuanjing Huang", "Jiayin Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) possess strong multilingual capabilities, and\ncombining Reinforcement Learning from Human Feedback (RLHF) with translation\ntasks has shown great potential. However, we observe that this paradigm\nperforms unexpectedly poorly when applied to colloquial subtitle translation\ntasks. In this work, we investigate this issue and find that the offline reward\nmodel (RM) gradually diverges from the online LLM due to distributional shift,\nultimately leading to undesirable training outcomes. To address this, we\npropose RIVAL, an adversarial training framework that formulates the process as\na min-max game between the RM and the LLM. RIVAL iteratively updates the both\nmodels, with the RM trained to distinguish strong from weak translations\n(qualitative preference reward), and the LLM trained to enhance its translation\nfor closing this gap. To stabilize training and improve generalizability, we\nalso incorporate quantitative preference reward (e.g., BLEU) into the RM,\nenabling reference-free quality modeling aligned with human evaluation. Through\nextensive experiments, we demonstrate that the proposed adversarial training\nframework significantly improves upon translation baselines."}
{"id": "2506.05305", "pdf": "https://arxiv.org/pdf/2506.05305.pdf", "abs": "https://arxiv.org/abs/2506.05305", "title": "ProRefine: Inference-Time Prompt Refinement with Textual Feedback", "authors": ["Deepak Pandita", "Tharindu Cyril Weerasooriya", "Ankit Parag Shah", "Isabelle Diana May-Xin Ng", "Christopher M. Homan", "Wei Wei"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, play a substantial role in many cutting-edge\ncommercial applications, and continue to fascinate researchers across nearly\nall fields for their potential to accomplish expensive, complex tasks that,\nuntil recently, only humans have been trusted to do. These workflows depend\ncritically on the prompts used to provide the roles models play in such\nworkflows. Poorly designed prompts that fail even slightly to guide individual\nagents can lead to sub-optimal performance that may snowball within a system of\nagents, limiting their reliability and scalability. To address this important\nproblem of inference-time prompt optimization, we introduce ProRefine, an\ninnovative inference-time optimization method that uses an agentic loop of LLMs\nto generate and apply textual feedback. ProRefine dynamically refines prompts\nfor multi-step reasoning tasks without additional training or ground truth\nlabels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine\nsignificantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37\npercentage points. This approach not only boosts accuracy but also allows\nsmaller models to approach the performance of their larger counterparts. This\nhighlights its potential for building more cost-effective and powerful hybrid\nAI systems, thereby democratizing access to high-performing AI."}
{"id": "2506.10960", "pdf": "https://arxiv.org/pdf/2506.10960.pdf", "abs": "https://arxiv.org/abs/2506.10960", "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark", "authors": ["Kangwei Liu", "Siyuan Cheng", "Bozhong Tian", "Xiaozhuan Liang", "Yuyang Yin", "Meng Han", "Ningyu Zhang", "Bryan Hooi", "Xi Chen", "Shumin Deng"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench."}
{"id": "2506.12537", "pdf": "https://arxiv.org/pdf/2506.12537.pdf", "abs": "https://arxiv.org/abs/2506.12537", "title": "What Makes a Good Speech Tokenizer for LLM-Centric Speech Generation? A Systematic Study", "authors": ["Xiaoran Fan", "Zhichao Sun", "Yangfan Gao", "Jingfei Xiong", "Hang Yan", "Yifei Cao", "Jiajun Sun", "Shuo Li", "Zhihao Zhang", "Zhiheng Xi", "Yuhao Zhou", "Senjie Jin", "Changhao Jiang", "Junjie Ye", "Ming Zhang", "Rui Zheng", "Zhenhua Han", "Yunke Zhang", "Demei Yan", "Shaokang Dong", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "Speech-language models (SLMs) offer a promising path toward unifying speech\nand text understanding and generation. However, challenges remain in achieving\neffective cross-modal alignment and high-quality speech generation. In this\nwork, we systematically investigate the role of speech tokenizer designs in\nLLM-centric SLMs, augmented by speech heads and speaker modeling. We compare\ncoupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM\nframework and find that decoupled tokenization significantly improves alignment\nand synthesis quality. To address the information density mismatch between\nspeech and text, we introduce multi-token prediction (MTP) into SLMs, enabling\neach hidden state to decode multiple speech tokens. This leads to up to\n12$\\times$ faster decoding and a substantial drop in word error rate (from 6.07\nto 3.01). Furthermore, we propose a speaker-aware generation paradigm and\nintroduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with\ndiverse speaker identities. Experiments demonstrate that our methods enhance\nboth knowledge understanding and speaker consistency."}
{"id": "2506.19794", "pdf": "https://arxiv.org/pdf/2506.19794.pdf", "abs": "https://arxiv.org/abs/2506.19794", "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study", "authors": ["Yuqi Zhu", "Yi Zhong", "Jintian Zhang", "Ziheng Zhang", "Shuofei Qiao", "Yujie Luo", "Lun Du", "Da Zheng", "Ningyu Zhang", "Huajun Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate model behavior across\nthree core dimensions: data understanding, code generation, and strategic\nplanning. Our analysis reveals three key findings: (1) Strategic planning\nquality serves as the primary determinant of model performance; (2) Interaction\ndesign and task complexity significantly influence reasoning capabilities; (3)\nData quality demonstrates a greater impact than diversity in achieving optimal\nperformance. We leverage these insights to develop a data synthesis\nmethodology, demonstrating significant improvements in open-source LLMs'\nanalytical reasoning capabilities. Code is available at\nhttps://github.com/zjunlp/DataMind."}
{"id": "2507.01903", "pdf": "https://arxiv.org/pdf/2507.01903.pdf", "abs": "https://arxiv.org/abs/2507.01903", "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, Paper list is available at\n  https://github.com/LightChen233/Awesome-AI4Research", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research."}
{"id": "2507.03674", "pdf": "https://arxiv.org/pdf/2507.03674.pdf", "abs": "https://arxiv.org/abs/2507.03674", "title": "STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking", "authors": ["Tek Raj Chhetri", "Yibei Chen", "Puja Trivedi", "Dorota Jarecka", "Saif Haobsh", "Patrick Ray", "Lydia Ng", "Satrajit S. Ghosh"], "categories": ["cs.CL", "cs.AI"], "comment": "-", "summary": "The ability to extract structured information from unstructured sources-such\nas free-text documents and scientific literature-is critical for accelerating\nscientific discovery and knowledge synthesis. Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in various natural language processing\ntasks, including structured information extraction. However, their\neffectiveness often diminishes in specialized, domain-specific contexts that\nrequire nuanced understanding and expert-level domain knowledge. In addition,\nexisting LLM-based approaches frequently exhibit poor transferability across\ntasks and domains, limiting their scalability and adaptability. To address\nthese challenges, we introduce StructSense, a modular, task-agnostic,\nopen-source framework for structured information extraction built on LLMs.\nStructSense is guided by domain-specific symbolic knowledge encoded in\nontologies, enabling it to navigate complex domain content more effectively. It\nfurther incorporates agentic capabilities through self-evaluative judges that\nform a feedback loop for iterative refinement, and includes human-in-the-loop\nmechanisms to ensure quality and validation. We demonstrate that StructSense\ncan overcome both the limitations of domain sensitivity and the lack of\ncross-task generalizability, as shown through its application to diverse\nneuroscience information extraction tasks."}
{"id": "2507.03724", "pdf": "https://arxiv.org/pdf/2507.03724.pdf", "abs": "https://arxiv.org/abs/2507.03724", "title": "MemOS: A Memory OS for AI System", "authors": ["Zhiyu Li", "Shichao Song", "Chenyang Xi", "Hanyu Wang", "Chen Tang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chunyu Li", "Qingchen Yu", "Jihao Zhao", "Yezhaohui Wang", "Peng Liu", "Zehao Lin", "Pengyuan Wang", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhen Tao", "Huayi Lai", "Hao Wu", "Bo Tang", "Zhenren Wang", "Zhaoxin Fan", "Ningyu Zhang", "Linfeng Zhang", "Junchi Yan", "Mingchuan Yang", "Tong Xu", "Wei Xu", "Huajun Chen", "Haofen Wang", "Hongkang Yang", "Wentao Zhang", "Zhi-Qin John Xu", "Siheng Chen", "Feiyu Xiong"], "categories": ["cs.CL"], "comment": "36 pages, 10 figures, 5 tables", "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling."}
{"id": "2507.17234", "pdf": "https://arxiv.org/pdf/2507.17234.pdf", "abs": "https://arxiv.org/abs/2507.17234", "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores."}
{"id": "2507.18905", "pdf": "https://arxiv.org/pdf/2507.18905.pdf", "abs": "https://arxiv.org/abs/2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany L. Brazile", "Natasha Chase", "Dimple Patel Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools."}
{"id": "2507.20252", "pdf": "https://arxiv.org/pdf/2507.20252.pdf", "abs": "https://arxiv.org/abs/2507.20252", "title": "Post-Completion Learning for Language Models", "authors": ["Xiang Fei", "Siqi Wang", "Shu Wei", "Yuxiang Nie", "Wei Shi", "Hao Feng", "Chao Feng", "Can Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency."}
{"id": "2507.21009", "pdf": "https://arxiv.org/pdf/2507.21009.pdf", "abs": "https://arxiv.org/abs/2507.21009", "title": "Memorization in Fine-Tuned Large Language Models", "authors": ["Danil Savine"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."}
{"id": "2507.22676", "pdf": "https://arxiv.org/pdf/2507.22676.pdf", "abs": "https://arxiv.org/abs/2507.22676", "title": "Listening to the Unspoken: Exploring \"365\" Aspects of Multimodal Interview Performance Assessment", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Jialong Hu", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects."}
{"id": "2507.23095", "pdf": "https://arxiv.org/pdf/2507.23095.pdf", "abs": "https://arxiv.org/abs/2507.23095", "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL", "cs.AI"], "comment": "This requires some internal approval before the public release", "summary": "We present SMART-Editor, a framework for compositional layout and content\nediting across structured (posters, websites) and unstructured (natural images)\ndomains. Unlike prior models that perform local edits, SMART-Editor preserves\nglobal coherence through two strategies: Reward-Refine, an inference-time\nrewardguided refinement method, and RewardDPO, a training-time preference\noptimization approach using reward-aligned layout pairs. To evaluate model\nperformance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,\ncascading edit scenarios. SMART-Editor outperforms strong baselines like\nInstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in\nstructured settings and Reward-Refine showing advantages on natural images.\nAutomatic and human evaluations confirm the value of reward-guided planning in\nproducing semantically consistent and visually aligned edits."}
{"id": "2508.00429", "pdf": "https://arxiv.org/pdf/2508.00429.pdf", "abs": "https://arxiv.org/abs/2508.00429", "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning."}
{"id": "2508.01696", "pdf": "https://arxiv.org/pdf/2508.01696.pdf", "abs": "https://arxiv.org/abs/2508.01696", "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bing Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks."}
{"id": "2508.02013", "pdf": "https://arxiv.org/pdf/2508.02013.pdf", "abs": "https://arxiv.org/abs/2508.02013", "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "We request withdrawal of this paper due to an error in the\n  experimental results reported in Table 2 on page 8. Specifically, the results\n  for the Qwen2.5-Omni model are incorrect. We are currently conducting further\n  verification and plan to resubmit with corrected results", "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field."}
{"id": "2508.02087", "pdf": "https://arxiv.org/pdf/2508.02087.pdf", "abs": "https://arxiv.org/abs/2508.02087", "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models", "authors": ["Keyu Wang", "Jin Li", "Shu Yang", "Zhuoran Zhang", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems."}
{"id": "2508.02208", "pdf": "https://arxiv.org/pdf/2508.02208.pdf", "abs": "https://arxiv.org/abs/2508.02208", "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems", "authors": ["Yebo Peng", "Zixiang Liu", "Yaoming Li", "Zhizhuo Yang", "Xinye Xu", "Bowen Ye", "Weijun Yuan", "Zihan Wang", "Tong Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems."}
{"id": "2508.02271", "pdf": "https://arxiv.org/pdf/2508.02271.pdf", "abs": "https://arxiv.org/abs/2508.02271", "title": "Dynaword: From One-shot to Continuously Developed Datasets", "authors": ["Kenneth Enevoldsen", "Kristian N√∏rgaard Jensen", "Jan Kostkan", "Bal√°zs Szab√≥", "M√°rton Kardos", "Kirten Vad", "Johan Heinsen", "Andrea Blasi N√∫√±ez", "Gianluca Barmina", "Jacob Nielsen", "Rasmus Larsen", "Peter Vahlstrup", "Per M√∏ldrup Dalum", "Desmond Elliott", "Lukas Galke", "Peter Schneider-Kamp", "Kristoffer Nielbo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution."}
{"id": "2508.02308", "pdf": "https://arxiv.org/pdf/2508.02308.pdf", "abs": "https://arxiv.org/abs/2508.02308", "title": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training", "authors": ["Sikui Zhang", "Guangze Gao", "Ziyun Gan", "Chunfeng Yuan", "Zefeng Lin", "Houwen Peng", "Bing Li", "Weiming Hu"], "categories": ["cs.CL"], "comment": "13 pages, 9 figures", "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE."}
{"id": "2508.02317", "pdf": "https://arxiv.org/pdf/2508.02317.pdf", "abs": "https://arxiv.org/abs/2508.02317", "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."}
{"id": "2404.17730", "pdf": "https://arxiv.org/pdf/2404.17730.pdf", "abs": "https://arxiv.org/abs/2404.17730", "title": "Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults", "authors": ["Lara J. Martin", "Malathy Nagalakshmi"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "High-tech Augmentative and Alternative Communication (AAC) has been rapidly\nadvancing in recent years due to the increased use of large language models\n(LLMs) like ChatGPT, but many of these techniques are integrated without the\ninclusion of the users' perspectives. Autistic adults have been particularly\nneglected in the design of AAC tools. We conducted in-depth interviews with 12\nautistic adults to find the pain points of current AAC and determine what\ntechnological advances they might find helpful. We found 8 different categories\nof themes from our interviews: input flexibility, output flexibility, selecting\nor adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling\nblocks for continued use, and control of communication. In this paper, we go\nthrough these categories in depth -- comparing each to prior work -- and then\nhighlight novel findings to suggest possible research directions."}
{"id": "2410.14971", "pdf": "https://arxiv.org/pdf/2410.14971.pdf", "abs": "https://arxiv.org/abs/2410.14971", "title": "BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation", "authors": ["Jilong Li", "Zhenxi Song", "Jiaqi Wang", "Meishan Zhang", "Honghai Liu", "Min Zhang", "Zhiguo Zhang"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": "8 pages (excluding references), accepted by Findings of ACL 2025", "summary": "Current EEG/MEG-to-text decoding systems suffer from three key limitations:\n(1) reliance on teacher-forcing methods, which compromises robustness during\ninference, (2) sensitivity to session-specific noise, hindering generalization\nacross subjects, and (3) misalignment between brain signals and linguistic\nrepresentations due to pre-trained language model over-dominance. To overcome\nthese challenges, we propose BrainECHO (Brain signal decoding via\nvEctor-quantized speCtrogram reconstruction for WHisper-enhanced text\ngeneratiOn), a multi-stage framework that employs decoupled representation\nlearning to achieve state-of-the-art performance on both EEG and MEG datasets.\nSpecifically, BrainECHO consists of three stages: (1) Discrete autoencoding,\nwhich transforms continuous Mel spectrograms into a finite set of high-quality\ndiscrete representations for subsequent stages. (2) Frozen alignment, where\nbrain signal embeddings are mapped to corresponding Mel spectrogram embeddings\nin a frozen latent space, effectively filtering session-specific noise through\nvector-quantized reconstruction, yielding a 3.65% improvement in BLEU-4 score.\n(3) Constrained decoding fine-tuning, which leverages the pre-trained Whisper\nmodel for audio-to-text translation, balancing signal adaptation with knowledge\npreservation, and achieving 74%-89% decoding BLEU scores without excessive\nreliance on teacher forcing. BrainECHO demonstrates robustness across sentence,\nsession, and subject-independent conditions, passing Gaussian noise tests and\nshowcasing its potential for enhancing language-based brain-computer\ninterfaces."}
{"id": "2411.19331", "pdf": "https://arxiv.org/pdf/2411.19331.pdf", "abs": "https://arxiv.org/abs/2411.19331", "title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation", "authors": ["Luca Barsellotti", "Lorenzo Bianchi", "Nicola Messina", "Fabio Carrara", "Marcella Cornia", "Lorenzo Baraldi", "Fabrizio Falchi", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form\ntextual concepts without predefined training classes. While existing\nvision-language models such as CLIP can generate segmentation masks by\nleveraging coarse spatial information from Vision Transformers, they face\nchallenges in spatial localization due to their global alignment of image and\ntext features. Conversely, self-supervised visual models like DINO excel in\nfine-grained visual encoding but lack integration with language. To bridge this\ngap, we present Talk2DINO, a novel hybrid approach that combines the spatial\naccuracy of DINOv2 with the language understanding of CLIP. Our approach aligns\nthe textual embeddings of CLIP to the patch-level features of DINOv2 through a\nlearned mapping function without the need to fine-tune the underlying\nbackbones. At training time, we exploit the attention maps of DINOv2 to\nselectively align local visual patches with textual embeddings. We show that\nthe powerful semantic and localization abilities of Talk2DINO can enhance the\nsegmentation process, resulting in more natural and less noisy segmentations,\nand that our approach can also effectively distinguish foreground objects from\nthe background. Experimental results demonstrate that Talk2DINO achieves\nstate-of-the-art performance across several unsupervised OVS benchmarks. Source\ncode and models are publicly available at:\nhttps://lorebianchi98.github.io/Talk2DINO/."}
{"id": "2412.02141", "pdf": "https://arxiv.org/pdf/2412.02141.pdf", "abs": "https://arxiv.org/abs/2412.02141", "title": "WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image", "authors": ["Yuci Liang", "Xinheng Lyu", "Meidan Ding", "Wenting Chen", "Jipeng Zhang", "Yuexiang Ren", "Xiangjian He", "Song Wu", "Sen Yang", "Xiyue Wang", "Xiaohan Xing", "Linlin Shen"], "categories": ["cs.CV", "cs.CL"], "comment": "ICCV 2025, 38 pages, 22 figures, 35 tables", "summary": "Recent advancements in computational pathology have produced patch-level\nMulti-modal Large Language Models (MLLMs), but these models are limited by\ntheir inability to analyze whole slide images (WSIs) comprehensively and their\ntendency to bypass crucial morphological features that pathologists rely on for\ndiagnosis. To address these challenges, we first introduce WSI-Bench, a\nlarge-scale morphology-aware benchmark containing 180k VQA pairs from 9,850\nWSIs across 30 cancer types, designed to evaluate MLLMs' understanding of\nmorphological characteristics crucial for accurate diagnosis. Building upon\nthis benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI\nunderstanding that employs a three-stage training approach: WSI-text alignment,\nfeature space alignment, and task-specific instruction tuning. To better assess\nmodel performance in pathological contexts, we develop two specialized WSI\nmetrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that\nWSI-LLaVA outperforms existing models across all capability dimensions, with a\nsignificant improvement in morphological analysis, establishing a clear\ncorrelation between morphological understanding and diagnostic accuracy."}
{"id": "2502.12591", "pdf": "https://arxiv.org/pdf/2502.12591.pdf", "abs": "https://arxiv.org/abs/2502.12591", "title": "CutPaste&Find: Efficient Multimodal Hallucination Detector with Visual-aid Knowledge Base", "authors": ["Cong-Duy Nguyen", "Xiaobao Wu", "Duc Anh Vu", "Shuai Zhao", "Thong Nguyen", "Anh Tuan Luu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nreasoning capabilities, but they remain susceptible to hallucination,\nparticularly object hallucination where non-existent objects or incorrect\nattributes are fabricated in generated descriptions. Existing detection methods\nachieve strong performance but rely heavily on expensive API calls and\niterative LVLM-based validation, making them impractical for large-scale or\noffline use. To address these limitations, we propose CutPaste\\&Find, a\nlightweight and training-free framework for detecting hallucinations in\nLVLM-generated outputs. Our approach leverages off-the-shelf visual and\nlinguistic modules to perform multi-step verification efficiently without\nrequiring LVLM inference. At the core of our framework is a Visual-aid\nKnowledge Base that encodes rich entity-attribute relationships and associated\nimage representations. We introduce a scaling factor to refine similarity\nscores, mitigating the issue of suboptimal alignment values even for\nground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,\nincluding POPE and R-Bench, demonstrate that CutPaste\\&Find achieves\ncompetitive hallucination detection performance while being significantly more\nefficient and cost-effective than previous methods."}
{"id": "2502.13920", "pdf": "https://arxiv.org/pdf/2502.13920.pdf", "abs": "https://arxiv.org/abs/2502.13920", "title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health", "authors": ["Xingbo Wang", "Janessa Griffith", "Daniel A. Adler", "Joey Castillo", "Tanzeem Choudhury", "Fei Wang"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2025). Code is available at https://github.com/xingbow/sleephealthLLM", "summary": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots."}
{"id": "2503.10408", "pdf": "https://arxiv.org/pdf/2503.10408.pdf", "abs": "https://arxiv.org/abs/2503.10408", "title": "Out-of-Context Relational Reasoning in Large Language Models", "authors": ["Jonathan Shaki", "Emanuele La Malfa", "Michael Wooldridge", "Sarit Kraus"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Binary relations, such as equality, are basic mathematical concepts that\nappear, implicitly or explicitly, in most benchmarks for Large Language Models\n(LLM). A recent trend in the literature is benchmarking LLMs on out-of-context\nlearning, where the data is not presented in the prompt, but only during the\nmodel's training. However, existing works mostly focus on higher-order tasks,\nmaking it hard to interpret success or failure. In this work, we study how well\ncan LLMs reason out-of-context on binary relations by only learning the\nrepresentations of newly introduced tokens. Our experiments focus on equality\n($=$), inequality ($<$), and inclusion ($\\subset$) and the properties they\nsatisfy, such as reflexivity, symmetry, transitivity, and logical complexity\n(e.g., the number of reasoning \"hops\"). We show that LLMs achieve better than\nrandom accuracy, but are still far from perfect, even on relatively simple\nreasoning tasks involving binary relations. We analyse the learned\nrepresentations and show that LLMs encode useful information directly,\narranging the embeddings according to the task."}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037.pdf", "abs": "https://arxiv.org/abs/2504.09037", "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures. Accepted to TMLR, with Survey Certification\n  award", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ..."}
{"id": "2504.10352", "pdf": "https://arxiv.org/pdf/2504.10352.pdf", "abs": "https://arxiv.org/abs/2504.10352", "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "authors": ["Yifan Yang", "Shujie Liu", "Jinyu Li", "Yuxuan Hu", "Haibin Wu", "Hui Wang", "Jianwei Yu", "Lingwei Meng", "Haiyang Sun", "Yanqing Liu", "Yan Lu", "Kai Yu", "Xie Chen"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted in ACMMM 2025", "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://microsoft.com/research/project/vall-e-x/palle."}
{"id": "2504.13146", "pdf": "https://arxiv.org/pdf/2504.13146.pdf", "abs": "https://arxiv.org/abs/2504.13146", "title": "Antidistillation Sampling", "authors": ["Yash Savani", "Asher Trockman", "Zhili Feng", "Avi Schwarzschild", "Alexander Robey", "Marc Finzi", "J. Zico Kolter"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By strategically\nmodifying a model's next-token probability distribution, antidistillation\nsampling poisons reasoning traces, rendering them significantly less effective\nfor distillation while preserving the model's practical utility. For further\ndetails, see https://antidistillation.com."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260.pdf", "abs": "https://arxiv.org/abs/2505.12260", "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based text retrieval retrieves documents\nrelevant to search queries based on vector similarities. Documents are\npre-encoded offline, while queries arrive in real-time, necessitating an\nefficient online query encoder. Although LLMs significantly enhance retrieval\ncapabilities, serving deeply parameterized LLMs slows down query inference\nthroughput and increases demands for online deployment resources. In this\npaper, we propose LightRetriever, a novel LLM-based retriever with extremely\nlightweight query encoders. Our method retains a full-sized LLM for document\nencoding, but reduces the workload of query encoding to no more than an\nembedding lookup. Compared to serving a full LLM on an A800 GPU, our method\nachieves over 1000x speedup in query encoding and over 10x increase in\nend-to-end retrieval throughput. Extensive experiments on large-scale retrieval\nbenchmarks show that LightRetriever generalizes well across diverse tasks,\nmaintaining an average of 95% retrieval performance."}
{"id": "2507.04562", "pdf": "https://arxiv.org/pdf/2507.04562.pdf", "abs": "https://arxiv.org/abs/2507.04562", "title": "Evaluating LLMs on Real-World Forecasting Against Expert Forecasters", "authors": ["Janna Lu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against top forecasters.\nFrontier models achieve Brier scores that ostensibly surpass the human crowd\nbut still significantly underperform a group of experts."}
{"id": "2507.07855", "pdf": "https://arxiv.org/pdf/2507.07855.pdf", "abs": "https://arxiv.org/abs/2507.07855", "title": "Principled Foundations for Preference Optimization", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds."}
{"id": "2507.10532", "pdf": "https://arxiv.org/pdf/2507.10532.pdf", "abs": "https://arxiv.org/abs/2507.10532", "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Huijie Lv", "Ming Zhang", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "33 pages", "summary": "Reasoning in large language models has long been a central research focus,\nand recent studies employing reinforcement learning (RL) have introduced\ndiverse methods that yield substantial performance gains with minimal or even\nno external supervision. Surprisingly, some studies even suggest that random or\nincorrect reward signals can enhance performance. However, these breakthroughs\nare predominantly observed for the mathematically strong Qwen2.5 series on\nbenchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like\nLlama, which warrants a more in-depth investigation. In this work, our\nempirical analysis reveals that pre-training on massive web-scale corpora\nleaves Qwen2.5 susceptible to data contamination in widely used benchmarks.\nConsequently, conclusions derived from contaminated benchmarks on Qwen2.5\nseries may be unreliable. To obtain trustworthy evaluation results, we\nintroduce a generator that creates fully clean arithmetic problems of arbitrary\nlength and difficulty, dubbed RandomCalculation. Using this leakage-free\ndataset, we show that only accurate reward signals yield steady improvements\nthat surpass the base model's performance boundary in mathematical reasoning,\nwhereas random or incorrect rewards do not. Moreover, we conduct more\nfine-grained analyses to elucidate the factors underlying the different\nperformance observed on the MATH-500 and RandomCalculation benchmarks.\nConsequently, we recommend that future studies evaluate models on\nuncontaminated benchmarks and, when feasible, test various model series to\nensure trustworthy conclusions about RL and related methods."}
{"id": "2507.15882", "pdf": "https://arxiv.org/pdf/2507.15882.pdf", "abs": "https://arxiv.org/abs/2507.15882", "title": "Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark", "authors": ["Goeric Huybrechts", "Srikanth Ronanki", "Sai Muralidhar Jayanthi", "Jack Fitzgerald", "Srinivasan Veeravanallur"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The proliferation of multimodal Large Language Models has significantly\nadvanced the ability to analyze and understand complex data inputs from\ndifferent modalities. However, the processing of long documents remains\nunder-explored, largely due to a lack of suitable benchmarks. To address this,\nwe introduce Document Haystack, a comprehensive benchmark designed to evaluate\nthe performance of Vision Language Models (VLMs) on long, visually complex\ndocuments. Document Haystack features documents ranging from 5 to 200 pages and\nstrategically inserts pure text or multimodal text+image \"needles\" at various\ndepths within the documents to challenge VLMs' retrieval capabilities.\nComprising 400 document variants and a total of 8,250 questions, it is\nsupported by an objective, automated evaluation framework. We detail the\nconstruction and characteristics of the Document Haystack dataset, present\nresults from prominent VLMs and discuss potential research avenues in this\narea."}
{"id": "2507.17307", "pdf": "https://arxiv.org/pdf/2507.17307.pdf", "abs": "https://arxiv.org/abs/2507.17307", "title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning", "authors": ["Zhuokun Chen", "Zeren Chen", "Jiahao He", "Mingkui Tan", "Jianfei Cai", "Bohan Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of\nlarge language models by encouraging step-by-step intermediate reasoning during\ninference. While effective, CoT introduces substantial computational overhead\ndue to its reliance on autoregressive decoding over long token sequences.\nExisting acceleration strategies either reduce sequence length through early\nstopping or compressive reward designs, or improve decoding speed via\nspeculative decoding with smaller models. However, speculative decoding suffers\nfrom limited speedup when the agreement between small and large models is low,\nand fails to exploit the potential advantages of small models in producing\nconcise intermediate reasoning. In this paper, we present R-Stitch, a\ntoken-level, confidence-based hybrid decoding framework that accelerates CoT\ninference by switching between a small language model (SLM) and a large\nlanguage model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to\ngenerate tokens by default and delegates to the LLM only when the SLM's\nconfidence falls below a threshold. This design avoids full-sequence rollback\nand selectively invokes the LLM on uncertain steps, preserving both efficiency\nand answer quality. R-Stitch is model-agnostic, training-free, and compatible\nwith standard decoding pipelines. Experiments on math reasoning benchmarks\ndemonstrate that R-Stitch achieves up to 85\\% reduction in inference latency\nwith negligible accuracy drop, highlighting its practical effectiveness in\naccelerating CoT reasoning."}
{"id": "2508.00222", "pdf": "https://arxiv.org/pdf/2508.00222.pdf", "abs": "https://arxiv.org/abs/2508.00222", "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address for distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem."}
{"id": "2508.00282", "pdf": "https://arxiv.org/pdf/2508.00282.pdf", "abs": "https://arxiv.org/abs/2508.00282", "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals. We\nconclude that there is a core gap between the value-driven, embodied nature of\nhuman cognition and the statistical patterns of LLMs, highlighting the\nnecessity of incorporating intrinsic motivation and physical grounding into the\ndesign of more human-aligned agents."}
{"id": "2508.00901", "pdf": "https://arxiv.org/pdf/2508.00901.pdf", "abs": "https://arxiv.org/abs/2508.00901", "title": "Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge", "authors": ["Ruichen Xu", "Kexin Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern large language models excel in knowledge-intensive tasks, yet how\ntransformers acquire (store) knowledge during pre-training and extract\n(retrieve) it during post-fine-tuning inference remains theoretically opaque.\nWhile prior theoretical work has begun to investigate these questions through\nthe analysis of training dynamics, such studies are limited to single-layer,\nattention-only architectures. However, most existing studies suggest that MLPs\nare the most contributing components for storing knowledge in transformer-based\nlanguage models. Meanwhile, our empirical investigations reveal that such\nsimplified models, when trained using standard next-token prediction\nobjectives, may be incapable of acquiring or extracting factual knowledge. To\novercome this limitation, we introduce a tractable one-layer transformer\nframework that crucially incorporates both self-attention and MLP modules. By\ntracking its gradient dynamics, we establish convergence and generalization\nguarantees that illuminate the ability of knowledge acquisition and extraction.\nWe prove that 1) Transformers can achieve near-optimal training loss during\npre-training, signifying effective knowledge acquisition; 2) With a large\nfine-tuning dataset and specific data multiplicity conditions met, transformers\ncan achieve low generalization error when tested on factual knowledge learned\nduring pre-training but not reinforced during the fine-tuning, indicating\nsuccessful knowledge extraction; 3) When the conditions are not satisfied,\ntransformers exhibit high generalization loss, resulting in hallucinations. Our\nanalysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,\nour analysis offers theoretical insights into several pertinent empirical\nphenomena, such as the role of learning rate schedules. Experiments on\nsynthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate\nour results."}
{"id": "2508.01031", "pdf": "https://arxiv.org/pdf/2508.01031.pdf", "abs": "https://arxiv.org/abs/2508.01031", "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent", "authors": ["Jingzhe Ni", "Xiaolong Yin", "Xingyu Lu", "Xintong Li", "Ji Wei", "Ruofeng Tong", "Min Tang", "Peng Du"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation."}
{"id": "2508.01191", "pdf": "https://arxiv.org/pdf/2508.01191.pdf", "abs": "https://arxiv.org/abs/2508.01191", "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "authors": ["Chengshuai Zhao", "Zhen Tan", "Pingchuan Ma", "Dawei Li", "Bohan Jiang", "Yancheng Wang", "Yingzhen Yang", "Huan Liu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning."}
{"id": "2508.01365", "pdf": "https://arxiv.org/pdf/2508.01365.pdf", "abs": "https://arxiv.org/abs/2508.01365", "title": "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models", "authors": ["Zihan Wang", "Rui Zhang", "Hongwei Li", "Wenshu Fan", "Wenbo Jiang", "Qingchuan Zhao", "Guowen Xu"], "categories": ["cs.CR", "cs.CL"], "comment": "Under review", "summary": "Backdoor attacks pose a significant threat to Large Language Models (LLMs),\nwhere adversaries can embed hidden triggers to manipulate LLM's outputs. Most\nexisting defense methods, primarily designed for classification tasks, are\nineffective against the autoregressive nature and vast output space of LLMs,\nthereby suffering from poor performance and high latency. To address these\nlimitations, we investigate the behavioral discrepancies between benign and\nbackdoored LLMs in output space. We identify a critical phenomenon which we\nterm sequence lock: a backdoored model generates the target sequence with\nabnormally high and consistent confidence compared to benign generation.\nBuilding on this insight, we propose ConfGuard, a lightweight and effective\ndetection method that monitors a sliding window of token confidences to\nidentify sequence lock. Extensive experiments demonstrate ConfGuard achieves a\nnear 100\\% true positive rate (TPR) and a negligible false positive rate (FPR)\nin the vast majority of cases. Crucially, the ConfGuard enables real-time\ndetection almost without additional latency, making it a practical backdoor\ndefense for real-world LLM deployments."}
{"id": "2508.02175", "pdf": "https://arxiv.org/pdf/2508.02175.pdf", "abs": "https://arxiv.org/abs/2508.02175", "title": "Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers", "authors": ["Liang Lin", "Miao Yu", "Kaiwen Luo", "Yibo Zhang", "Lilan Peng", "Dexian Wang", "Xuehai Tang", "Yuanhe Zhang", "Xikang Yang", "Zhenhong Zhou", "Kun Wang", "Yang Liu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth."}
