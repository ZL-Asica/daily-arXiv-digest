{"id": "2507.14189", "pdf": "https://arxiv.org/pdf/2507.14189.pdf", "abs": "https://arxiv.org/abs/2507.14189", "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter is a multimodal, customizable writing assistant designed to improve the writing process in specialized domains by utilizing curated offline knowledge.", "motivation": "To address limitations of existing writing assistants in specialized domains such as finance and medicine, particularly issues of deep domain knowledge and content hallucination.", "method": "DeepWriter employs a novel pipeline, including task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection, leveraging a structured corpus for information extraction.", "result": "Experiments demonstrate that DeepWriter generates high-quality, verifiable financial reports that exceed existing baselines in terms of factual accuracy and content quality.", "conclusion": "DeepWriter effectively addresses the challenges faced by existing writing assistants in specialized domains, providing reliable and professional-grade document generation.", "key_contributions": ["Introduction of a customizable multimodal writing assistant", "Development of a novel pipeline for document generation", "Proposal of a hierarchical knowledge representation for improved retrieval efficiency"], "limitations": "", "keywords": ["DeepWriter", "writing assistant", "retrieval-augmented generation", "financial report generation", "multimodal retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.14198", "pdf": "https://arxiv.org/pdf/2507.14198.pdf", "abs": "https://arxiv.org/abs/2507.14198", "title": "Retention analysis of edited knowledge after fine-tuning", "authors": ["Fufang Wen", "Shichang Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "This paper investigates the impact of fine-tuning on the knowledge edited in large language models and proposes methods to improve knowledge retention during this process.", "motivation": "To understand how fine-tuning affects knowledge edited into large language models, especially since model editing methods are crucial for updating LLMs efficiently.", "method": "The authors conducted a systematic investigation comparing different fine-tuning objectives and model editing techniques, analyzing the retention of edited versus intrinsic knowledge.", "result": "Edited knowledge is more likely to be forgotten during fine-tuning compared to knowledge acquired through pre-training. Freezing layers associated with edited content can enhance knowledge retention.", "conclusion": "Evaluating the robustness of edits during downstream fine-tuning is essential for the practical application of model editing techniques; future editing methods should consider this aspect to improve retention.", "key_contributions": ["Detailed analysis of edited knowledge retention during fine-tuning of LLMs.", "Identification of freezing layers as a potential solution for better knowledge retention.", "Highlighting the critical need for robustness evaluation in current model editing approaches."], "limitations": "The study primarily focuses on the interaction between fine-tuning objectives and editing techniques but does not explore other potential factors affecting knowledge retention in LLMs.", "keywords": ["large language models", "model editing", "fine-tuning", "knowledge retention", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.14200", "pdf": "https://arxiv.org/pdf/2507.14200.pdf", "abs": "https://arxiv.org/abs/2507.14200", "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "The paper presents SMACS, a scalable multi-agent system that uses multiple open-source LLMs to outperform closed-source LLMs by optimizing performance through a novel selection and enhancement framework.", "motivation": "To explore the capability of open-source LLMs in competing with closed-source LLMs and to optimize their collaboration for improved performance.", "method": "The SMACS framework employs a Retrieval-based Prior Selection (RPS) to score and select the top-performing LLMs for specific questions, and an Exploration-Exploitation-Driven Posterior Enhancement (EPE) to generate diverse and high-quality responses.", "result": "SMACS, integrating fifteen open-source LLMs, demonstrates significant performance improvements over closed-source counterparts like Claude-3.7-Sonnet and GPT-4.1, showing advancements in multiple task benchmarks.", "conclusion": "The research indicates that open-source LLM collectives can excel in tasks traditionally dominated by closed-source models, thus expanding the potential for open-source applications in AI.", "key_contributions": ["Introduction of SMACS framework for multi-agent collaboration in LLMs", "Introduction of RPS and EPE methods for performance optimization", "Demonstrated significant performance improvement over closed-source LLMs"], "limitations": "", "keywords": ["open-source LLMs", "multi-agent systems", "performance optimization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.14214", "pdf": "https://arxiv.org/pdf/2507.14214.pdf", "abs": "https://arxiv.org/abs/2507.14214", "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer is a neuro-symbolic system that personalizes privacy policy analysis using NLP to extract data usage practices, enabling users to understand complex privacy policies efficiently.", "motivation": "To assist users in understanding the privacy policies of online accounts without the need to read lengthy documents, as most users do not engage with these texts despite claiming to do so.", "method": "PoliAnalyzer employs Natural Language Processing (NLP) to extract formal representations of data usage practices and uses logical inference to compare these with user preferences, producing compliance reports. It utilizes an enriched PolicyIE dataset for evaluation.", "result": "In evaluations, PoliAnalyzer achieved an F1-score of 90-100% for identifying relevant data usage practices and revealed that 95.2% of privacy policy segments do not conflict with user preferences, allowing users to focus on the minority that does.", "conclusion": "PoliAnalyzer effectively reduces cognitive burden and enhances understanding of privacy policies, facilitating individuals to regain control over their data and promoting discussions about data practices.", "key_contributions": ["Introduction of PoliAnalyzer, a neuro-symbolic system for analyzing privacy policies", "High accuracy in compliance analysis using NLP tools", "Identification of common privacy policy practices that violate user expectations"], "limitations": "The study relies on an existing dataset and may have limitations based on the diversity of user profiles and specific policies analyzed.", "keywords": ["Privacy Policy", "Natural Language Processing", "User Compliance", "Data Usage Practices", "Cognitive Burden"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14316", "pdf": "https://arxiv.org/pdf/2507.14316.pdf", "abs": "https://arxiv.org/abs/2507.14316", "title": "Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap", "authors": ["Xianhao Carton Liu", "Difan Jia", "Tongyu Nie", "Evan Suma Rosenberg", "Victoria Interrante", "Chen Zhu-Tian"], "categories": ["cs.HC"], "comment": null, "summary": "In high-stakes, time-critical scenarios-such as emergency evacuation, first\nresponder prioritization, and crisis management -- decision-makers must rapidly\nchoose among spatial targets, such as exits, individuals to assist, or areas to\nsecure. Advances in indoor sensing and artificial intelligence (AI) can support\nthese decisions by visualizing real-time situational data and AI suggestions on\n2D maps. However, mentally mapping this information onto real-world spaces\nimposes significant cognitive load. This load can impair users' ability to\nappropriately judge AI suggestions, leading to inappropriate reliance (e.g.,\naccepting wrong AI suggestions or rejecting correct ones). Embedded\nvisualizations in Augmented Reality (AR), by directly overlaying information\nonto physical environments, may reduce this load and foster more deliberate,\nappropriate reliance on AI. But is this true? In this work, we conducted an\nempirical study (N = 32) comparing AR see-through (embedded visualization) and\n2D Minimap in time-critical, AI-assisted spatial target selection tasks.\nContrary to our expectations, users exhibited greater inappropriate reliance on\nAI in the AR condition. Our analysis further reveals that this is primarily due\nto over-reliance, with factors specific to embedded visualizations, such as\nperceptual challenges, visual proximity illusions, and highly realistic visual\nrepresentations. Nonetheless, embedded visualizations demonstrated notable\nbenefits in spatial reasoning, such as spatial mapping and egocentric spatial\nimagery. We conclude by discussing the empirical insights, deriving design\nimplications, and outlining important directions for future research on\nhuman-AI decision collaboration in AR.", "AI": {"tldr": "This paper investigates the effectiveness of Augmented Reality (AR) in reducing cognitive load during AI-assisted decision-making in emergency scenarios.", "motivation": "High-stakes decision-making in time-critical scenarios often leads to cognitive overload, impacting the judgement of AI suggestions. AR may offer a solution by overlaying information in the real world, but its effectiveness is questioned.", "method": "An empirical study with 32 participants compared AI-assisted decision-making using AR see-through visualizations versus 2D minimaps in spatial target selection tasks.", "result": "Participants using AR exhibited greater inappropriate reliance on AI suggestions, attributed to perceptual challenges and realism in the visual representation, despite AR aiding spatial reasoning skills.", "conclusion": "Embedded visualizations in AR can impair decision-making due to over-reliance on AI but show promise in enhancing spatial reasoning. Further research is necessary to refine the design of human-AI collaboration in AR.", "key_contributions": ["Comparison of AR and 2D maps for AI-assisted decisions", "Insights into cognitive load and AI reliance", "Guidelines for future research on AR in decision-making"], "limitations": "The study was limited to specific emergency scenarios and may not generalize to other contexts.", "keywords": ["Augmented Reality", "AI decision-making", "cognitive load", "spatial reasoning", "human-AI collaboration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.14231", "pdf": "https://arxiv.org/pdf/2507.14231.pdf", "abs": "https://arxiv.org/abs/2507.14231", "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "authors": ["Khalid Hasan", "Jamil Saquer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "This paper investigates NLP models for detecting signs of bipolar disorder in social media text, demonstrating that transformer models, particularly RoBERTa, significantly outperform LSTM models with static embeddings.", "motivation": "Bipolar disorder is often underdiagnosed; hence, leveraging NLP to identify signs through social media could enhance early detection and treatment.", "method": "Evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and LSTMs using both contextualized and static word embeddings on a dataset of Reddit posts.", "result": "RoBERTa achieved the highest performance with an F1 score of ~98%, while LSTMs with static embeddings struggled, scoring near-zero F1.", "conclusion": "Contextual language modeling plays a vital role in effective bipolar disorder detection, and DistilBERT strikes a good balance between efficiency and accuracy.", "key_contributions": ["Comprehensive evaluation of various NLP models for mental health detection", "Demonstrated effectiveness of contextualized embeddings over static ones", "Provided insights on model training efficiency for health informatics applications"], "limitations": "", "keywords": ["Bipolar disorder", "Natural Language Processing", "Transformer models", "Social media", "Early detection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.14384", "pdf": "https://arxiv.org/pdf/2507.14384.pdf", "abs": "https://arxiv.org/abs/2507.14384", "title": "Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions", "authors": ["Angjelin Hila", "Elliott Hauser"], "categories": ["cs.HC", "cs.CL"], "comment": "Extended version of paper accepted for presentation at the ASIS&T\n  Annual Meeting 2025. 38 pages, 12 figures", "summary": "In this study, we investigate the use of large language models (LLMs),\nspecifically ChatGPT, for structured deductive qualitative coding. While most\ncurrent research emphasizes inductive coding applications, we address the\nunderexplored potential of LLMs to perform deductive classification tasks\naligned with established human-coded schemes. Using the Comparative Agendas\nProject (CAP) Master Codebook, we classified U.S. Supreme Court case summaries\ninto 21 major policy domains. We tested four intervention methods: zero-shot,\nfew-shot, definition-based, and a novel Step-by-Step Task Decomposition\nstrategy, across repeated samples. Performance was evaluated using standard\nclassification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's\nalpha), and construct validity was assessed using chi-squared tests and\nCramer's V. Chi-squared and effect size analyses confirmed that intervention\nstrategies significantly influenced classification behavior, with Cramer's V\nvalues ranging from 0.359 to 0.613, indicating moderate to strong shifts in\nclassification patterns. The Step-by-Step Task Decomposition strategy achieved\nthe strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),\nachieving thresholds for substantial agreement. Despite the semantic ambiguity\nwithin case summaries, ChatGPT displayed stable agreement across samples,\nincluding high F1 scores in low-support subclasses. These findings demonstrate\nthat with targeted, custom-tailored interventions, LLMs can achieve reliability\nlevels suitable for integration into rigorous qualitative coding workflows.", "AI": {"tldr": "This study explores the use of large language models like ChatGPT for structured deductive qualitative coding, focusing on how various intervention methods can influence classification performance in specific domains.", "motivation": "To investigate the underexplored potential of LLMs for deductive classification tasks, contrasting with the predominant focus on inductive coding in current research.", "method": "Using the Comparative Agendas Project Master Codebook, case summaries from the U.S. Supreme Court were classified into 21 policy domains via four intervention methods: zero-shot, few-shot, definition-based, and a novel Step-by-Step Task Decomposition strategy. Performance metrics included accuracy, F1-score, Cohen's kappa, and Krippendorff's alpha, with validity assessed using chi-squared tests and effect size analyses.", "result": "The Step-by-Step Task Decomposition strategy yielded the highest performance with an accuracy of 0.775 and kappa of 0.744. Chi-squared analyses showed that the intervention methods significantly influenced classification behavior, suggesting reliable LLM performance in qualitative coding tasks.", "conclusion": "With targeted interventions, LLMs like ChatGPT can achieve reliable classification levels suitable for qualitative coding workflows, despite challenges such as semantic ambiguity in case summaries.", "key_contributions": ["Demonstrated the application of LLMs in deductive qualitative coding tasks.", "Introduced a novel Step-by-Step Task Decomposition strategy for improved classification performance.", "Provided evidence of LLM reliability in qualitative coding through various performance metrics."], "limitations": "The study's focus on a specific dataset may limit generalizability to other qualitative coding contexts, and limitations related to semantic ambiguity within texts were noted.", "keywords": ["Large Language Models", "Deductive Coding", "Qualitative Research", "ChatGPT", "Machine Learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.14238", "pdf": "https://arxiv.org/pdf/2507.14238.pdf", "abs": "https://arxiv.org/abs/2507.14238", "title": "Language Models Change Facts Based on the Way You Talk", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "This paper analyzes how identity markers in users' writing affect the responses of large language models (LLMs) across various applications such as medicine and job salary recommendations, revealing biases based on race, gender, and age.", "motivation": "To understand how LLMs use identity information in real-world decision-making and to assess the biases that may arise in sensitive applications.", "method": "Conducted a comprehensive analysis of LLM responses across five high-stakes applications, examining the influence of identity markers like race, gender, and age.", "result": "Findings reveal significant biases where LLMs treat users differently based on identity, leading to potential harmful outcomes in medical advice, salary recommendations, and political discourse.", "conclusion": "The study highlights the need for thorough evaluations of LLM applications to avoid perpetuating biases and recommends further assessments before deploying LLMs in user-facing roles.", "key_contributions": ["First comprehensive analysis of LLM biases linked to identity markers", "Identification of harmful biases in high-stakes applications", "Development of tools for evaluating LLM decision-making under identity influences"], "limitations": "", "keywords": ["large language models", "bias", "identity markers", "ethical AI", "user-facing applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.14418", "pdf": "https://arxiv.org/pdf/2507.14418.pdf", "abs": "https://arxiv.org/abs/2507.14418", "title": "Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students", "authors": ["Taufiq Daryanto", "Sophia Stil", "Xiaohan Ding", "Daniel Manesh", "Sang Won Lee", "Tim Lee", "Stephanie Lunn", "Sarah Rodriguez", "Chris Brown", "Eugenia Rho"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "One challenge in technical interviews is the think-aloud process, where\ncandidates verbalize their thought processes while solving coding tasks.\nDespite its importance, opportunities for structured practice remain limited.\nConversational AI offers potential assistance, but limited research explores\nuser perceptions of its role in think-aloud practice. To address this gap, we\nconducted a study with 17 participants using an LLM-based technical interview\npractice tool. Participants valued AI's role in simulation, feedback, and\nlearning from generated examples. Key design recommendations include promoting\nsocial presence in conversational AI for technical interview simulation,\nproviding feedback beyond verbal content analysis, and enabling crowdsourced\nthink-aloud examples through human-AI collaboration. Beyond feature design, we\nexamined broader considerations, including intersectional challenges and\npotential strategies to address them, how AI-driven interview preparation could\npromote equitable learning in computing careers, and the need to rethink AI's\nrole in interview practice by suggesting a research direction that integrates\nhuman-AI collaboration.", "AI": {"tldr": "This study explores the role of conversational AI in enhancing the think-aloud process during technical interviews, focusing on user perceptions and design recommendations.", "motivation": "To investigate user perceptions of conversational AI's role in think-aloud practice for technical interviews and to explore design opportunities for improvement.", "method": "A study was conducted with 17 participants using an LLM-based tool for technical interview practice, gathering data on their experiences and preferences.", "result": "Participants appreciated the AI's contribution to simulation, feedback, and learning through example generation, leading to key design insights.", "conclusion": "The research underscores the importance of social presence in conversational AI for interview simulation and proposes a need for equitable learning strategies in AI-driven interview preparation.", "key_contributions": ["Promoting social presence in AI interactions for better simulation", "Providing diverse feedback mechanisms beyond content analysis", "Encouraging crowdsourced examples through collaboration with AI."], "limitations": "The study is limited to 17 participants, which may not fully represent broader user experiences with AI in interview practice.", "keywords": ["human-AI collaboration", "technical interviews", "conversational AI", "think-aloud process", "equitable learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.14239", "pdf": "https://arxiv.org/pdf/2507.14239.pdf", "abs": "https://arxiv.org/abs/2507.14239", "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "The paper introduces CCL-XCoT, a framework to reduce hallucinations in Multilingual Large Language Models by enhancing cross-lingual semantic alignment and incorporating a Chain-of-Thought prompting strategy.", "motivation": "Address hallucinations in Multilingual Large Language Models, particularly for low-resource languages due to training data imbalances.", "method": "A two-stage fine-tuning framework that enhances cross-lingual semantic alignment through curriculum-based contrastive learning and incorporates a Chain-of-Thought prompting strategy during instruction fine-tuning.", "result": "CCL-XCoT reduces hallucination rates by up to 62% and improves factual knowledge transfer across language pairs.", "conclusion": "The proposed framework mitigates hallucinations in low-resource language generation tasks effectively without the need for external retrieval or multi-model ensembles.", "key_contributions": ["Introduction of the CCL-XCoT framework for MLLMs", "Curriculum-based contrastive learning for enhanced semantic alignment", "Implementation of cross-lingual Chain-of-Thought prompting strategy"], "limitations": "", "keywords": ["Multilingual Large Language Models", "hallucinations", "cross-lingual", "contrastive learning", "Chain-of-Thought"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.14482", "pdf": "https://arxiv.org/pdf/2507.14482.pdf", "abs": "https://arxiv.org/abs/2507.14482", "title": "Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies", "authors": ["Qianhe Chen", "Yong Wang", "Yixin Yu", "Xiyuan Zhu", "Xuerou Yu", "Ran Wang"], "categories": ["cs.HC"], "comment": null, "summary": "In-depth analysis of competitive debates is essential for participants to\ndevelop argumentative skills and refine strategies, and further improve their\ndebating performance. However, manual analysis of unstructured and unlabeled\ntextual records of debating is time-consuming and ineffective, as it is\nchallenging to reconstruct contextual semantics and track logical connections\nfrom raw data. To address this, we propose Conch, an interactive visualization\nsystem that systematically analyzes both what is debated and how it is debated.\nIn particular, we propose a novel parallel spiral visualization that compactly\ntraces the multidimensional evolution of clash points and participant\ninteractions throughout debate process. In addition, we leverage large language\nmodels with well-designed prompts to automatically identify critical debate\nelements such as clash points, disagreements, viewpoints, and strategies,\nenabling participants to understand the debate context comprehensively.\nFinally, through two case studies on real-world debates and a\ncarefully-designed user study, we demonstrate Conch's effectiveness and\nusability for competitive debate analysis.", "AI": {"tldr": "Conch is an interactive visualization system that analyzes competitive debates using novel visualization techniques and large language models to enhance participants' understanding and performance.", "motivation": "To improve argumentative skills and debating performance, participants need effective tools to analyze competitive debates rather than relying on manual analysis of textual records.", "method": "Conch employs a novel parallel spiral visualization to represent clash points and participant interactions and utilizes large language models to automatically identify critical debate elements.", "result": "The effectiveness and usability of Conch were demonstrated through case studies on real-world debates and a user study.", "conclusion": "Conch provides a comprehensive understanding of debate context, improving the analysis and strategies of debaters.", "key_contributions": ["Introduction of parallel spiral visualization for debate analysis", "Use of large language models to identify critical debate elements", "Empirical validation of effectiveness through real-world case studies"], "limitations": "", "keywords": ["visualization", "debate analysis", "large language models", "HCI", "interaction design"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240.pdf", "abs": "https://arxiv.org/abs/2507.14240", "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "This paper studies the relationships between large language models (LLMs) and their datasets, analyzing the LLM supply chain through a directed heterogeneous graph.", "motivation": "Understanding the complex interdependencies between models and datasets is crucial for mitigating risks, improving fairness, and ensuring compliance in the deployment of LLMs.", "method": "The authors systematically collected LLM supply chain data to construct a directed heterogeneous graph representing the relationships between models and datasets, containing 397,376 nodes and 453,469 edges.", "result": "The analysis revealed that the LLM supply chain graph is large and sparse, follows a power-law degree distribution, features a densely connected core, and exhibits strong interdependence between models and datasets, with daily updates indicating a dynamic ecosystem.", "conclusion": "Insights from the study can help stakeholders better understand potential risks and improve the overall governance of LLMs and their components.", "key_contributions": ["Development of a systematic method for collecting LLM supply chain data", "Construction of a directed heterogeneous graph modeling model-dataset relationships", "Analysis revealing the dynamic nature and structure of the LLM supply chain"], "limitations": "", "keywords": ["large language models", "supply chain", "datasets", "model-dataset relationships", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.14494", "pdf": "https://arxiv.org/pdf/2507.14494.pdf", "abs": "https://arxiv.org/abs/2507.14494", "title": "\"It looks sexy but it's wrong.\" Tensions in creativity and accuracy using genAI for biomedical visualization", "authors": ["Roxanne Ziman", "Shehryar Saharan", "Gaël McGill", "Laura Garrison"], "categories": ["cs.HC"], "comment": "11 pages, 3 figures. Accepted to IEEE VIS 2025 Conference", "summary": "We contribute an in-depth analysis of the workflows and tensions arising from\ngenerative AI (genAI) use in biomedical visualization (BioMedVis). Although\ngenAI affords facile production of aesthetic visuals for biological and medical\ncontent, the architecture of these tools fundamentally limits the accuracy and\ntrustworthiness of the depicted information, from imaginary (or fanciful)\nmolecules to alien anatomy. Through 17 interviews with a diverse group of\npractitioners and researchers, we qualitatively analyze the concerns and values\ndriving genAI (dis)use for the visual representation of spatially-oriented\nbiomedical data. We find that BioMedVis experts, both in roles as developers\nand designers, use genAI tools at different stages of their daily workflows and\nhold attitudes ranging from enthusiastic adopters to skeptical avoiders of\ngenAI. In contrasting the current use and perspectives on genAI observed in our\nstudy with predictions towards genAI in the visualization pipeline from prior\nwork, our refocus the discussion of genAI's effects on projects in\nvisualization in the here and now with its respective opportunities and\npitfalls for future visualization research. At a time when public trust in\nscience is in jeopardy, we are reminded to first do no harm, not just in\nbiomedical visualization but in science communication more broadly. Our\nobservations reaffirm the necessity of human intervention for empathetic design\nand assessment of accurate scientific visuals.", "AI": {"tldr": "This paper analyzes the use of generative AI in biomedical visualization, assessing its impact on accuracy and trustworthiness through interviews with practitioners, and emphasizes the need for human oversight in design.", "motivation": "To explore the workflows and tensions in biomedical visualization arising from the use of generative AI and its implications for accuracy and trustworthiness in scientific communication.", "method": "The study conducted 17 qualitative interviews with practitioners and researchers in biomedical visualization to gather insights on their experiences and perspectives regarding generative AI tools.", "result": "The findings reveal a spectrum of attitudes towards generative AI among BioMedVis professionals, highlighting issues with accuracy and the importance of human involvement in design processes to ensure trustworthiness of visual representations.", "conclusion": "The paper calls for careful consideration of generative AI's role in biomedical visualization, advocating for human intervention to maintain the integrity of scientific visuals amidst the automation trends.", "key_contributions": ["In-depth analysis of the use of generative AI in biomedical visualization.", "Qualitative insights from diverse practitioners highlighting the tension between innovation and accuracy.", "Recommendations for integrating human oversight in AI-assisted design processes."], "limitations": "The study is limited to interviews and does not include quantitative data on the impact of genAI in biomedical visualization workflows.", "keywords": ["generative AI", "biomedical visualization", "trustworthiness", "human-computer interaction", "science communication"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.14241", "pdf": "https://arxiv.org/pdf/2507.14241.pdf", "abs": "https://arxiv.org/abs/2507.14241", "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Promptomatix is an automatic prompt optimization framework that generates high-quality prompts from natural language task descriptions without manual tuning or domain expertise.", "motivation": "To address the challenges of manual and inconsistent prompt engineering in Large Language Models (LLMs) that limits accessibility for non-experts.", "method": "Promptomatix uses a meta-prompt-based optimizer and a DSPy-powered compiler to analyze user intent, generate synthetic training data, select prompting strategies, and refine prompts with cost-aware objectives.", "result": "Promptomatix was evaluated across 5 task categories, showing competitive or superior performance compared to existing libraries while reducing prompt length and computational overhead.", "conclusion": "The framework enables scalable and efficient prompt optimization, making it accessible and effective for a broader range of users.", "key_contributions": ["Automatic prompt generation from natural language descriptions", "Modular design for future extensions", "Scalable and efficient prompt optimization"], "limitations": "", "keywords": ["Large Language Models", "Prompt Engineering", "Natural Language Processing", "Task Optimization", "Synthetic Data Generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.14527", "pdf": "https://arxiv.org/pdf/2507.14527.pdf", "abs": "https://arxiv.org/abs/2507.14527", "title": "PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration", "authors": ["Runhua Zhang", "Yang Ouyang", "Leixian Shen", "Yuying Tang", "Xiaojuan Ma", "Huamin Qu", "Xian Xu"], "categories": ["cs.HC"], "comment": "Conditionally accepted by UIST'25", "summary": "Researchers frequently need to synthesize their own publications into\ncoherent narratives that demonstrate their scholarly contributions. To suit\ndiverse communication contexts, exploring alternative ways to organize one's\nwork while maintaining coherence is particularly challenging, especially in\ninterdisciplinary fields like HCI where individual researchers' publications\nmay span diverse domains and methodologies. In this paper, we present\nPaperBridge, a human-AI co-exploration system informed by a formative study and\ncontent analysis. PaperBridge assists researchers in exploring diverse\nperspectives for organizing their publications into coherent narratives. At its\ncore is a bi-directional analysis engine powered by large language models,\nsupporting iterative exploration through both top-down user intent (e.g.,\ndetermining organization structure) and bottom-up refinement on narrative\ncomponents (e.g., thematic paper groupings). Our user study (N=12) demonstrated\nPaperBridge's usability and effectiveness in facilitating the exploration of\nalternative research narratives. Our findings also provided empirical insights\ninto how interactive systems can scaffold academic communication tasks.", "AI": {"tldr": "PaperBridge is a human-AI system that helps researchers organize publications into coherent narratives, particularly useful in interdisciplinary fields like HCI.", "motivation": "Researchers need to effectively synthesize and communicate their scholarly contributions, especially in diverse contexts and interdisciplinary fields, making organization challenging.", "method": "A formative study and content analysis informed the development of PaperBridge, which includes a bi-directional analysis engine powered by large language models to support narrative organization and exploration.", "result": "A user study involving 12 participants demonstrated PaperBridge's usability and effectiveness in helping researchers explore alternative narratives for their work.", "conclusion": "The findings provide empirical insights on how interactive systems can assist in academic communication tasks, showcasing the potential of AI in scholarly synthesis.", "key_contributions": ["Development of PaperBridge, a system for narrative organization", "Use of large language models for bi-directional analysis", "Insights into interactive systems facilitating academic communication"], "limitations": "The study involved a small number of participants (N=12), which may limit generalizability.", "keywords": ["Human-Computer Interaction", "large language models", "academic communication", "interdisciplinary research", "narrative organization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.14298", "pdf": "https://arxiv.org/pdf/2507.14298.pdf", "abs": "https://arxiv.org/abs/2507.14298", "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "ChartScope is an LVLM designed for enhanced comprehension of scientific charts across various types, addressing limitations in existing models by introducing a data generation pipeline and a dual-path training strategy.", "motivation": "To improve chart comprehension in LVLMs which previously relied on limited paired data and lacked targeted pre-training for chart-data alignment.", "method": "Introduces an efficient data generation pipeline for synthesizing paired data across diverse chart types and a novel Dual-Path training strategy that enhances understanding and reasoning about the data.", "result": "ChartScope demonstrates significantly improved comprehension of scientific charts across a wide variety of types, as evaluated using the newly established ChartDQA benchmark.", "conclusion": "ChartScope effectively enhances the capability of LVLMs in understanding and reasoning over diverse chart data, representing a step forward in scientific chart comprehension.", "key_contributions": ["Introduction of ChartScope, a tailored LVLM for chart comprehension", "Development of a data generation pipeline for diverse chart types", "Launch of ChartDQA benchmark for comprehensive evaluation of chart understanding"], "limitations": "", "keywords": ["Large Vision Language Models", "chart comprehension", "data generation", "training strategy", "benchmark"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.14537", "pdf": "https://arxiv.org/pdf/2507.14537.pdf", "abs": "https://arxiv.org/abs/2507.14537", "title": "Uncovering the EEG Temporal Representation of Low-dimensional Object Properties", "authors": ["Jiahua Tang", "Song Wang", "Jiachen Zou", "Chen Wei", "Quanying Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Understanding how the human brain encodes and processes external visual\nstimuli has been a fundamental challenge in neuroscience. With advancements in\nartificial intelligence, sophisticated visual decoding architectures have\nachieved remarkable success in fMRI research, enabling more precise and\nfine-grained spatial concept localization. This has provided new tools for\nexploring the spatial representation of concepts in the brain. However, despite\nthe millisecond-scale temporal resolution of EEG, which offers unparalleled\nadvantages in tracking the dynamic evolution of cognitive processes, the\ntemporal dynamics of neural representations based on EEG remain underexplored.\nThis is primarily due to EEG's inherently low signal-to-noise ratio and its\ncomplex spatiotemporal coupling characteristics. To bridge this research gap,\nwe propose a novel approach that integrates advanced neural decoding algorithms\nto systematically investigate how low-dimensional object properties are\ntemporally encoded in EEG signals. We are the first to attempt to identify the\nspecificity and prototypical temporal characteristics of concepts within\ntemporal distributions. Our framework not only enhances the interpretability of\nneural representations but also provides new insights into visual decoding in\nbrain-computer interfaces (BCI).", "AI": {"tldr": "This paper proposes a novel approach to investigate how low-dimensional object properties are temporally encoded in EEG signals, enhancing the interpretability of neural representations and advancing visual decoding in brain-computer interfaces.", "motivation": "To address the gap in understanding the temporal dynamics of neural representations based on EEG, which remains underexplored compared to fMRI despite its advantages in temporal resolution.", "method": "Integrating advanced neural decoding algorithms to analyze temporal encoding of object properties in EEG signals.", "result": "The framework enables identification of specificity and prototypical temporal characteristics of concepts within EEG signals.", "conclusion": "The proposed approach enhances interpretability of neural representations and provides insights for visual decoding in BCIs.", "key_contributions": ["Novel integration of neural decoding algorithms with EEG analysis", "First attempt to identify prototypical temporal characteristics of concepts in EEG", "Insights into visual decoding applicable to brain-computer interfaces"], "limitations": "", "keywords": ["EEG", "visual decoding", "brain-computer interfaces", "neural representations", "temporal dynamics"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2507.14304", "pdf": "https://arxiv.org/pdf/2507.14304.pdf", "abs": "https://arxiv.org/abs/2507.14304", "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "This study investigates LLM-based selective translation to improve alignment of multilingual large language models, specifically for low-resource languages like Hindi, by preserving non-translatable content and exploring its effectiveness compared to traditional translation methods.", "motivation": "To address the performance gap between English and non-English languages in multilingual large language models, especially in low-resource settings where data is scarce.", "method": "The study explores LLM-based selective translation, which involves translating only the translatable parts of a text while preserving non-translatable content and sentence structure. It evaluates this method against traditional translation techniques using data from Hindi and compares outputs from Google Cloud Translation and Llama-3.1-405B.", "result": "The experiments demonstrate that selective translation can effectively improve the multilingual alignment of LLMs, showing promise as a viable alternative to conventional translation approaches.", "conclusion": "Selective translation offers a practical method for enhancing LLM performance in low-resource languages by avoiding the pitfalls of traditional translation methods that fail to retain crucial content structure.", "key_contributions": ["Introduction of selective translation technique for LLMs", "Comparative analysis of translation quality between GCP and Llama-3.1-405B", "Demonstration of effectiveness in aligning multilingual models for low-resource languages"], "limitations": "The study primarily focuses on Hindi and may not generalize to other languages; further experiments needed for broader applicability.", "keywords": ["multilingual LLMs", "selective translation", "low-resource languages", "Hindi", "alignment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14685", "pdf": "https://arxiv.org/pdf/2507.14685.pdf", "abs": "https://arxiv.org/abs/2507.14685", "title": "EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences", "authors": ["Luis Montana", "Jessica Magallanes", "Miguel Juarez", "Suzanne Mason", "Andrew Narracott", "Lindsey van Gemeren", "Steven Wood", "Maria-Cruz Villa-Uriol"], "categories": ["cs.HC"], "comment": "This is the author's version of the article to be published in IEEE\n  Transactions on Visualization and Computer Graphics, and presented at IEEE\n  VIS 2025. 11 pages, 7 figures", "summary": "The rapid growth and availability of event sequence data across domains\nrequires effective analysis and exploration methods to facilitate\ndecision-making. Visual analytics combines computational techniques with\ninteractive visualizations, enabling the identification of patterns, anomalies,\nand attribute interactions. However, existing approaches frequently overlook\nthe interplay between temporal and multivariate attributes. We introduce\nEventBox, a novel data representation and visual encoding approach for\nanalyzing groups of events and their multivariate attributes. We have\nintegrated EventBox into Sequen-C, a visual analytics system for the analysis\nof event sequences. To enable the agile creation of EventBoxes in Sequen-C, we\nhave added user-driven transformations, including alignment, sorting,\nsubstitution and aggregation. To enhance analytical depth, we incorporate\nautomatically generated statistical analyses, providing additional insight into\nthe significance of attribute interactions. We evaluated our approach involving\n21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T\nframework to assess visualization value, user performance metrics completing a\nseries of tasks, and interactive sessions with domain experts. We also present\nthree case studies with real-world healthcare data demonstrating how EventBox\nand its integration into Sequen-C reveal meaningful patterns, anomalies, and\ninsights. These results demonstrate that our work advances visual analytics by\nproviding a flexible solution for exploring temporal and multivariate\nattributes in event sequences.", "AI": {"tldr": "EventBox is a novel visual analytics method for analyzing event sequences, integrated into Sequen-C, enhancing exploration of temporal and multivariate attributes through user-driven transformations and statistical analyses.", "motivation": "The need for effective methods to analyze event sequence data to facilitate decision-making, particularly given the abundant availability of such data.", "method": "Integration of EventBox into Sequen-C, allowing for user-driven transformations like alignment, sorting, substitution, and aggregation, along with automated statistical analyses.", "result": "Evaluation with 21 participants showed that EventBox improves visualization value and user performance in analyzing event sequences, revealing significant patterns in healthcare data.", "conclusion": "EventBox enhances the depth of visual analytics, providing a flexible framework for examining complex interactions in event sequences.", "key_contributions": ["Introduction of a new data representation and visual encoding technique (EventBox).", "User-driven transformations for agile data analysis.", "Application of the ICE-T framework to evaluate visualization effectiveness."], "limitations": "", "keywords": ["Event sequences", "Visual analytics", "Statistics", "Data representation", "Healthcare"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2507.14307", "pdf": "https://arxiv.org/pdf/2507.14307.pdf", "abs": "https://arxiv.org/abs/2507.14307", "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "This study examines LLMs' processing of temporal meaning in narratives, revealing differences from human cognition and developing a framework for assessment.", "motivation": "To explore whether LLMs exhibit human-like cognition in understanding temporal aspects of narratives and to assess their ability to comprehend such narratives.", "method": "The study employs a series of targeted experiments using an Expert-in-the-Loop probing pipeline to evaluate LLMs' semantic and pragmatic processing.", "result": "Findings indicate that LLMs rely heavily on prototypical examples, show inconsistency in aspectual judgments, and struggle with causal reasoning related to aspect, highlighting their limitations in narrative understanding.", "conclusion": "The results suggest that LLMs process narrative aspects differently than humans and lack a robust understanding of narratives, while also proposing a new experimental framework for evaluating LLMs' cognitive abilities.", "key_contributions": ["Identification of LLMs' over-reliance on prototypicality in aspectual judgments.", "Demonstration of inconsistent aspectual reasoning in LLMs.", "Proposal of a standardized framework for assessing LLMs' cognitive capabilities."], "limitations": "The study focuses primarily on LLMs and may not represent broader AI linguistic processing capabilities.", "keywords": ["Large Language Models", "Linguistic Aspect", "Cognition", "Narratives", "Pragmatic Inferences"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14702", "pdf": "https://arxiv.org/pdf/2507.14702.pdf", "abs": "https://arxiv.org/abs/2507.14702", "title": "A Notification Based Nudge for Handling Excessive Smartphone Use", "authors": ["Partha Sarker", "Dipto Dey", "Marium-E-Jannat"], "categories": ["cs.HC", "F.2.2, I.2.7"], "comment": "6 pages, 8 figures", "summary": "Excessive use of smartphones is a worldwide known issue. In this study, we\nproposed a notification-based intervention approach to reduce smartphone\noveruse without making the user feel any annoyance or irritation. Most of the\nwork in this field tried to reduce smartphone overuse by making smartphone use\nmore difficult for the user. In our user study (n = 109), we found that 19.3%\nof the participants are unwilling to use any usage-limiting application because\na) they do not want their smartphone activities to get restricted or b) those\napplications are annoying. Following that, we devised a hypothesis to minimize\nsmartphone usage among undergraduates. Finally, we designed a prototype for\nAndroid, \"App Usage Monitor,\" and conducted a 3-week experiment through which\nwe found proof of concept for our hypothesis. In our prototype, we combined\ntechniques such as nudge and visualization to increase self-awareness among the\nuser by leveraging notifications.", "AI": {"tldr": "This study presents a notification-based intervention to reduce smartphone overuse by increasing user self-awareness without causing annoyance.", "motivation": "Address the global issue of excessive smartphone usage, particularly among undergraduates, without making users feel restricted or annoyed.", "method": "Developed a prototype app, \"App Usage Monitor,\" and conducted a 3-week user study with 109 participants to test the effectiveness of notifications and nudges.", "result": "The intervention showed promising results, indicating that self-awareness can be increased through strategic notifications, leading to reduced smartphone usage.", "conclusion": "The study demonstrates that a non-intrusive approach utilizing notifications can effectively minimize smartphone overuse among users.", "key_contributions": ["Proposed a novel notification-based intervention for smartphone overuse", "Developed and tested a prototype that combines nudges and visualization", "Showcased the importance of self-awareness in reducing smartphone usage"], "limitations": "The study is limited by its small sample size and short duration of the experiment.", "keywords": ["smartphone overuse", "notification-based intervention", "self-awareness", "user study", "nudge techniques"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2507.14314", "pdf": "https://arxiv.org/pdf/2507.14314.pdf", "abs": "https://arxiv.org/abs/2507.14314", "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "authors": ["Marija Anđedelić", "Dominik Šipek", "Laura Majer", "Jan Šnajder"], "categories": ["cs.CL"], "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "This paper presents a dataset and methods for detecting clickbait in Croatian news headlines, comparing fine-tuned models with LLM-based approaches.", "motivation": "To preserve information quality and reader trust in digital media by addressing the prevalence of clickbait headlines.", "method": "Compilation of the CLIC dataset for clickbait detection; fine-tuning the BERTić model; comparison with LLM-based in-context learning methods for Croatian and English prompts.", "result": "Fine-tuned models outperformed LLMs in detecting clickbait, with nearly half of the analyzed Croatian headlines identified as clickbait.", "conclusion": "Fine-tuning yields better clickbait detection results than using general LLMs, emphasizing the need for context-specific approaches.", "key_contributions": ["Introduction of the CLIC dataset for Croatian headlines", "Demonstration of the superiority of fine-tuned models over LLMs", "Analysis of linguistic properties of clickbait."], "limitations": "", "keywords": ["clickbait detection", "BERTić", "LLM", "Croatian news", "information quality"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.14767", "pdf": "https://arxiv.org/pdf/2507.14767.pdf", "abs": "https://arxiv.org/abs/2507.14767", "title": "XplainAct: Visualization for Personalized Intervention Insights", "authors": ["Yanming Zhang", "Krishnakumar Hegde", "Klaus Mueller"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "This paper will be published and presented at IEEE Visualization\n  (VIS) 2025, Vienna, Austria, November 2025", "summary": "Causality helps people reason about and understand complex systems,\nparticularly through what-if analyses that explore how interventions might\nalter outcomes. Although existing methods embrace causal reasoning using\ninterventions and counterfactual analysis, they primarily focus on effects at\nthe population level. These approaches often fall short in systems\ncharacterized by significant heterogeneity, where the impact of an intervention\ncan vary widely across subgroups. To address this challenge, we present\nXplainAct, a visual analytics framework that supports simulating, explaining,\nand reasoning interventions at the individual level within subpopulations. We\ndemonstrate the effectiveness of XplainAct through two case studies:\ninvestigating opioid-related deaths in epidemiology and analyzing voting\ninclinations in the presidential election.", "AI": {"tldr": "This paper introduces XplainAct, a visual analytics framework for simulating and reasoning interventions at the individual level within heterogeneous subpopulations.", "motivation": "Existing causal reasoning methods mainly focus on population-level effects, which are inadequate for systems with significant heterogeneity in intervention outcomes.", "method": "XplainAct framework that enables visual analytics for individual-level intervention simulation and explanation within subpopulations.", "result": "Demonstrated effectiveness through case studies on opioid-related deaths and voting inclinations.", "conclusion": "XplainAct provides enhanced causal reasoning capabilities for complex systems, allowing for better understanding and intervention analysis at the individual level.", "key_contributions": ["Introduction of the XplainAct framework for individual-level intervention exploration", "Application of the framework in diverse case studies", "Focus on addressing heterogeneity in causal outcomes"], "limitations": "", "keywords": ["Causal reasoning", "Visual analytics", "Subpopulations"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.14355", "pdf": "https://arxiv.org/pdf/2507.14355.pdf", "abs": "https://arxiv.org/abs/2507.14355", "title": "Can LLMs Infer Personality from Real World Conversations?", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "categories": ["cs.CL"], "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "This paper evaluates the effectiveness of LLMs in personality assessment using a benchmark of real-world interviews, identifying issues with validity despite strong reliability.", "motivation": "The study aims to assess the capability of large language models for inferring personality traits from open-ended language in a reliable and valid way.", "method": "The authors tested three LLMs (GPT-4.1 Mini, Meta-LLaMA, DeepSeek) using zero-shot prompting for BFI-10 item prediction and various prompting techniques for Big Five trait inference.", "result": "All models demonstrated high test-retest reliability, but had weak correlation with ground-truth personality scores and low interrater agreement, with predictions biased towards higher trait levels.", "conclusion": "The findings reveal significant limitations in LLMs for personality inference, emphasizing the necessity for more evidence-based approaches in psychological applications.", "key_contributions": ["Introduction of a benchmark with real-world semi-structured interviews for personality assessment using LLMs", "Demonstration of the reliability of LLMs in personality trait prediction despite low validity", "Highlighting the need for improved methodologies in LLM-based psychological applications"], "limitations": "Construct validity was limited, with weak correlations to ground truth and low interrater agreement.", "keywords": ["Large Language Models", "Personality Assessment", "Human-Computer Interaction", "Psychometrics", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14769", "pdf": "https://arxiv.org/pdf/2507.14769.pdf", "abs": "https://arxiv.org/abs/2507.14769", "title": "Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs", "authors": ["Ananya Gubbi Mohanbabu", "Yotam Sechayk", "Amy Pavel"], "categories": ["cs.HC"], "comment": "18 pages, 4 figures, 7 tables", "summary": "Modern web interfaces are unnecessarily complex to use as they overwhelm\nusers with excessive text and visuals unrelated to their current goals. This\nproblem particularly impacts screen reader users (SRUs), who navigate content\nsequentially and may spend minutes traversing irrelevant elements before\nreaching desired information compared to vision users (VUs) who visually skim\nin seconds. We present Task Mode, a system that dynamically filters web content\nbased on user-specified goals using large language models to identify and\nprioritize relevant elements while minimizing distractions. Our approach\npreserves page structure while offering multiple viewing modes tailored to\ndifferent access needs. Our user study with 12 participants (6 VUs, 6 SRUs)\ndemonstrates that our approach reduced task completion time for SRUs while\nmaintaining performance for VUs, decreasing the completion time gap between\ngroups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the\nfuture, reporting that Task Mode supported completing tasks with less effort\nand fewer distractions. This work demonstrates how designing new interactions\nsimultaneously for visual and non-visual access can reduce rather than\nreinforce accessibility disparities in future technology created by\nhuman-computer interaction researchers and practitioners.", "AI": {"tldr": "Task Mode is a system that enhances web accessibility for screen reader users by filtering content based on user-defined goals, demonstrated to improve task completion time while supporting visual users.", "motivation": "Web interfaces are overly complex and difficult for screen reader users to navigate, leading to increased task completion times and accessibility disparities.", "method": "Task Mode utilizes large language models to dynamically filter web content according to user goals, preserving page structure and providing various viewing modes for different user needs.", "result": "A user study involving 12 participants showed that Task Mode significantly reduced task completion time for screen reader users while maintaining performance for visual users, lowering the completion time gap from 2x to 1.2x.", "conclusion": "Task Mode demonstrates that new interaction designs can reduce accessibility disparities and improve usability for both visual and non-visual users.", "key_contributions": ["Introduction of Task Mode for dynamic content filtering", "Improvement of task completion times for screen reader users", "Reduction of accessibility gaps in web interactions"], "limitations": "Study limited to a small sample size of 12 participants.", "keywords": ["web accessibility", "human-computer interaction", "large language models"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372.pdf", "abs": "https://arxiv.org/abs/2507.14372", "title": "Text-to-SQL for Enterprise Data Analytics", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "This paper discusses the development of an internal chatbot at LinkedIn that enables product managers and engineers to easily access data insights from a dynamic data lake using a Text-to-SQL approach.", "motivation": "To facilitate self-service data insights for LinkedIn's product managers, engineers, and operations teams, leveraging large language models in an enterprise setting.", "method": "The approach consists of building a knowledge graph to capture metadata and historical queries, developing a Text-to-SQL agent for accurate query generation, and creating an interactive chatbot for diverse user intents.", "result": "The chatbot has 300+ weekly users, with expert reviews indicating that around 53% of its responses are correct or close to correct on the internal benchmark.", "conclusion": "The study highlights important components of knowledge graphs and modeling for enterprise Text-to-SQL solutions, providing a practical path for future developments.", "key_contributions": ["Construction of a knowledge graph for real-time data insights", "Development of a Text-to-SQL agent to correct hallucinations and syntax errors", "Creation of an interactive chatbot for enhanced user engagement"], "limitations": "The performance is limited to the quality of the knowledge graph and the training data; improvements are necessary for broader contexts.", "keywords": ["Text-to-SQL", "knowledge graph", "chatbot", "enterprise solutions", "data insights"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14792", "pdf": "https://arxiv.org/pdf/2507.14792.pdf", "abs": "https://arxiv.org/abs/2507.14792", "title": "SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors", "authors": ["Kaixin Ji", "Danula Hettiachchi", "Falk Scholer", "Flora D. Salim", "Damiano Spina"], "categories": ["cs.HC"], "comment": "Accepted in Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (IMWUT), September 2025", "summary": "Information processing tasks involve complex cognitive mechanisms that are\nshaped by various factors, including individual goals, prior experience, and\nsystem environments. Understanding such behaviors requires a sophisticated and\npersonalized data capture of how one interacts with modern information systems\n(e.g., web search engines). Passive sensors, such as wearables, capturing\nphysiological and behavioral data, have the potential to provide solutions in\nthis context. This paper presents a novel dataset, SenseSeek, designed to\nevaluate the effectiveness of consumer-grade sensors in a complex information\nprocessing scenario: searching via systems (e.g., search engines), one of the\ncommon strategies users employ for information seeking. The SenseSeek dataset\ncomprises data collected from 20 participants, 235 trials of the stimulated\nsearch process, 940 phases of stages in the search process, including the\nrealization of Information Need (IN), Query Formulation (QF), Query Submission\nby Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)\nor Listening (RJ-L). The data includes Electrodermal Activities (EDA),\nElectroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured\nusing consumer-grade sensors. It also contains 258 features extracted from the\nsensor data, the gaze-annotated screen recordings, and task responses. We\nvalidate the usefulness of the dataset by providing baseline analysis on the\nimpacts of different cognitive intents and interaction modalities on the sensor\ndata, and effectiveness of the data in discriminating the search stages. To our\nknowledge, SenseSeek is the first dataset that characterizes the multiple\nstages involved in information seeking with physiological signals collected\nfrom multiple sensors. We hope this dataset can serve as a reference for future\nresearch on information-seeking behaviors.", "AI": {"tldr": "This paper introduces the SenseSeek dataset, which captures physiological and behavioral data from individuals during information searching tasks, aiming to enhance understanding of user interactions with information systems.", "motivation": "To understand complex cognitive behaviors during information processing tasks and improve information-seeking strategies through personalized data capture.", "method": "The SenseSeek dataset was created by collecting data from 20 participants across 235 trials of a stimulated search process, using consumer-grade sensors to capture physiological indicators such as EDA, EEG, and motion data.", "result": "The analysis demonstrates the effectiveness of the dataset in discriminating different cognitive intents and interaction stages during the search process.", "conclusion": "SenseSeek is the first dataset to characterize multiple stages of information seeking with physiological signals, serving as a potential reference for future research in this field.", "key_contributions": ["Introduction of the SenseSeek dataset for information-seeking behaviors", "Characterization of cognitive stages using physiological data", "Baseline analysis of sensor data effectiveness in discriminating search stages"], "limitations": "", "keywords": ["Information processing", "Cognitive mechanisms", "Physiological data", "Information seeking", "SenseSeek dataset"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14374", "pdf": "https://arxiv.org/pdf/2507.14374.pdf", "abs": "https://arxiv.org/abs/2507.14374", "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "This paper presents an error-aware teacher-student framework to enhance Relation Classification in biomedical texts using structured guidance from a large language model, achieving state-of-the-art performance on multiple datasets.", "motivation": "The motivation is to improve Relation Classification in biomedical texts to aid in knowledge graph construction and applications like drug repurposing and clinical decision-making.", "method": "The proposed method involves a teacher model analyzing prediction failures from a baseline student model to classify errors, assign difficulty scores, and generate targeted remediations, followed by training two student models using instruction tuning and curriculum learning based on a curated dataset.", "result": "The framework achieves new state-of-the-art performance on 4 out of 5 Protein-Protein Interaction datasets and the Drug-Drug Interaction dataset, while remaining competitive on the ChemProt dataset.", "conclusion": "The study demonstrates the effectiveness of using structured guidance from a large language model to improve Relation Classification in biomedical texts, yielding significant performance gains.", "key_contributions": ["Introduction of an error-aware teacher-student framework for Relation Classification", "Development of a heterogeneous biomedical knowledge graph from PubMed abstracts", "Achievement of state-of-the-art results on multiple biomedical datasets"], "limitations": "", "keywords": ["Relation Classification", "Biomedical Texts", "Knowledge Graphs", "Curriculum Learning", "Instruction Tuning"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2507.14818", "pdf": "https://arxiv.org/pdf/2507.14818.pdf", "abs": "https://arxiv.org/abs/2507.14818", "title": "Understanding How Visually Impaired Players Socialize in Mobile Games", "authors": ["Zihe Ran", "Xiyu Li", "Qing Xiao", "Yanyun Wang", "Franklin Mingzhe Li", "Zhicong Lu"], "categories": ["cs.HC", "cs.CY"], "comment": "16 pages, 1 table, accepted by ASSETS25", "summary": "Mobile games are becoming a vital medium for social interaction, offering a\nplatform that transcends geographical boundaries. An increasing number of\nvisually impaired individuals are engaging in mobile gaming to connect,\ncollaborate, compete, and build friendships. In China, visually impaired\ncommunities face significant social challenges in offline settings, making\nmobile games a crucial avenue for socialization. However, the design of mobile\ngames and their mapping to real-world environments significantly shape their\nsocial gaming experiences. This study explores how visually impaired players in\nChina navigate socialization and integrate into gaming communities. Through\ninterviews with 30 visually impaired players, we found that while mobile games\nfulfill many of their social needs, technological barriers and insufficient\naccessibility features, and internal community divisions present significant\nchallenges to their participation. This research sheds light on their social\nexperiences and offers insights for designing more inclusive and accessible\nmobile games.", "AI": {"tldr": "The study investigates how visually impaired players in China engage in mobile gaming for socialization and community integration, uncovering challenges they face in accessibility and community support.", "motivation": "Mobile games offer a vital medium for social interaction, especially for visually impaired individuals who encounter challenges in offline social settings.", "method": "Interviews conducted with 30 visually impaired players in China to understand their social gaming experiences and community integration.", "result": "The study reveals that mobile games fulfill many social needs for visually impaired individuals, but challenges such as technological barriers and insufficient accessibility features hinder participation.", "conclusion": "The research highlights the need for more inclusive and accessible mobile game designs to better support the social experiences of visually impaired players.", "key_contributions": ["Insights on social experiences of visually impaired mobile gamers in China", "Identification of technological and community challenges faced by players", "Recommendations for designing more inclusive mobile games"], "limitations": "The study focuses on visually impaired players in China, which may limit the generalizability of the findings to other cultures or regions.", "keywords": ["Mobile Games", "Visually Impaired", "Socialization", "Accessibility", "Inclusive Design"], "importance_score": 7, "read_time_minutes": 16}}
{"id": "2507.14430", "pdf": "https://arxiv.org/pdf/2507.14430.pdf", "abs": "https://arxiv.org/abs/2507.14430", "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "X-Intelligence 3.0 is a high-performance reasoning model developed for the semiconductor display industry, achieving notable performance improvements through domain-specific training and innovative retrieval-augmented generation (RAG) mechanisms.", "motivation": "The effectiveness of large language models in the semiconductor display industry is limited due to a lack of domain-specific training and expertise, necessitating a specialized model.", "method": "X-Intelligence 3.0 utilizes supervised fine-tuning, reinforcement learning, and a domain-specific knowledge base for training, along with an automated evaluation framework for expert-level assessments.", "result": "X-Intelligence 3.0 significantly outperforms existing models like DeepSeek-R1-671B on benchmark datasets despite having a smaller parameter size, showcasing its efficiency and effectiveness.", "conclusion": "The development of a tailored reasoning model, X-Intelligence 3.0, addresses complex reasoning challenges in the semiconductor display industry successfully by leveraging domain-specific knowledge and advanced training methodologies.", "key_contributions": ["Introduction of X-Intelligence 3.0 as a specialized reasoning model for the semiconductor display industry", "Integration of a domain-specific retrieval-augmented generation mechanism to improve performance", "Implementation of an automated evaluation framework to simulate expert-level assessments"], "limitations": "", "keywords": ["large language models", "semiconductor display", "reasoning", "retrieval-augmented generation", "automated evaluation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.14846", "pdf": "https://arxiv.org/pdf/2507.14846.pdf", "abs": "https://arxiv.org/abs/2507.14846", "title": "Progressive Sentences: Combining the Benefits of Word and Sentence Learning", "authors": ["Nuwan Janaka", "Shengdong Zhao", "Ashwin Ram", "Ruoxin Sun", "Sherisse Tan Jing Wen", "Danae Li", "David Hsu"], "categories": ["cs.HC", "cs.CY"], "comment": "12 pages, 4 figures, 4 tables", "summary": "The rapid evolution of lightweight consumer augmented reality (AR) smart\nglasses (a.k.a. optical see-through head-mounted displays) offers novel\nopportunities for learning, particularly through their unique capability to\ndeliver multimodal information in just-in-time, micro-learning scenarios. This\nresearch investigates how such devices can support mobile second-language\nacquisition by presenting progressive sentence structures in multimodal\nformats. In contrast to the commonly used vocabulary (i.e., word) learning\napproach for novice learners, we present a \"progressive presentation\" method\nthat combines both word and sentence learning by sequentially displaying\nsentence components (subject, verb, object) while retaining prior context.\nPilot and formal studies revealed that progressive presentation enhances\nrecall, particularly in mobile scenarios such as walking. Additionally,\nincorporating timed gaps between word presentations further improved learning\neffectiveness under multitasking conditions. Our findings demonstrate the\nutility of progressive presentation and provide usage guidelines for\neducational applications-even during brief, on-the-go learning moments.", "AI": {"tldr": "This research examines the use of lightweight AR smart glasses to enhance second-language acquisition through a progressive presentation method.", "motivation": "To explore how augmented reality can facilitate mobile learning, particularly in language acquisition, by delivering multimodal information.", "method": "The study utilizes a 'progressive presentation' method that sequentially shows sentence components and incorporates timed gaps for improved learning.", "result": "Pilot and formal studies show that progressive presentation enhances recall, especially during mobile activities like walking, with timed gaps further improving effectiveness.", "conclusion": "The research concludes that progressive presentation is beneficial for language learning in mobile contexts and offers guidelines for educational applications.", "key_contributions": ["Demonstrated efficacy of AR in mobile second-language learning", "Introduced the progressive presentation method combining word and sentence structures", "Provided guidelines for educational use of AR technology"], "limitations": "", "keywords": ["augmented reality", "language acquisition", "multimodal learning", "mobile learning", "progressive presentation"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2507.14578", "pdf": "https://arxiv.org/pdf/2507.14578.pdf", "abs": "https://arxiv.org/abs/2507.14578", "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "XL-DURel is a multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification, outperforming previous models on related tasks.", "motivation": "To enhance Word-in-Context classification by using a unified model for both ordinal and binary tasks.", "method": "Finetuning a multilingual Sentence Transformer with various loss functions for regression and ranking tasks, focusing on angular distance in complex space.", "result": "XL-DURel outperforms previous models on both ordinal and binary Word-in-Context tasks.", "conclusion": "A generalized approach for Word-in-Context modeling can improve performance across various task formulations, including binary cases.", "key_contributions": ["Introduction of XL-DURel model", "Demonstration of ordinal WiC treatments benefiting binary tasks", "Novel application of angular distance for ranking objectives"], "limitations": "", "keywords": ["Word-in-Context", "multilingual models", "Sentence Transformer", "ordinal classification", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.14859", "pdf": "https://arxiv.org/pdf/2507.14859.pdf", "abs": "https://arxiv.org/abs/2507.14859", "title": "Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications", "authors": ["Nils Mandischer", "Alexander Atanasyan", "Ulrich Dahmen", "Michael Schluse", "Jürgen Rossmann", "Lars Mikelsons"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "This work was accepted by the IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC), Vienna, Austria, 2025", "summary": "The digital twin of humans is a relatively new concept. While many diverse\ndefinitions, architectures, and applications exist, a clear picture is missing\non what, in fact, makes a human digital twin. Within this context, researchers\nand industrial use-case owners alike are unaware about the market potential of\nthe - at the moment - rather theoretical construct. In this work, we draw a\nholistic vision of the human digital twin, and derive the specification of this\nholistic human digital twin in form of requirements, stakeholders, and users.\nFor each group of users, we define exemplary applications that fall into the\nsix levels of functionality: store, analyze, personalize, predict, control, and\noptimize. The functionality levels facilitate an abstraction of abilities of\nthe human digital twin. From the manifold applications, we discuss three in\ndetail to showcase the feasibility of the abstraction levels and the analysis\nof stakeholders and users. Based on the deep discussion, we derive a\ncomprehensive list of requirements on the holistic human digital twin. These\nconsiderations shall be used as a guideline for research and industries for the\nimplementation of human digital twins, particularly in context of reusability\nin multiple target applications.", "AI": {"tldr": "This paper presents a holistic vision and specification of the human digital twin, detailing its functionalities, requirements, and potential applications.", "motivation": "The existing definitions and applications of human digital twins are unclear, hindering awareness of their market potential.", "method": "The paper defines the specifications of a holistic human digital twin by identifying requirements, stakeholders, and users, and categorizes applications based on six functionality levels: store, analyze, personalize, predict, control, and optimize.", "result": "The study showcases three detailed applications to demonstrate functionality levels and stakeholder analysis, ultimately deriving a comprehensive list of requirements for implementing human digital twins in various applications.", "conclusion": "The findings offer a guideline for researchers and industries to develop and implement human digital twins, emphasizing reusability across multiple target applications.", "key_contributions": ["Introduces a holistic vision of human digital twins.", "Defines six levels of functionality for human digital twins.", "Derives comprehensive requirements for practical implementation."], "limitations": "", "keywords": ["human digital twin", "functionality levels", "requirements engineering"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.14579", "pdf": "https://arxiv.org/pdf/2507.14579.pdf", "abs": "https://arxiv.org/abs/2507.14579", "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "This paper explores the use of the AudiBERT model for detecting collaborative problem solving indicators in dialogue, highlighting its enhancements over traditional BERT, particularly in social-cognitive classifications.", "motivation": "To address the challenge of detecting collaborative problem solving indicators from dialogue using machine learning in AI in Education, and to evaluate the effectiveness of the AudiBERT model.", "method": "The study utilized the AudiBERT model, which integrates multimodal features (speech and acoustic-prosodic audio) and compared its performance to traditional BERT in classifying CPS indicators.", "result": "AudiBERT exhibited statistically significant improvements in classifying sparse classes related to the social-cognitive dimension compared to BERT, though not for the affective dimension. Larger training data correlated with improved recall performance.", "conclusion": "The study advocates for a structured approach to enhance human-AI complementarity in CPS diagnosis, emphasizing model explainability to boost human engagement in the coding process.", "key_contributions": ["Demonstrated significant improvements in CPS indicator classification using AudiBERT over BERT.", "Showed correlation between larger training datasets and improved model performance.", "Outlined a structured method for achieving human-AI complementarity in CPS diagnosis."], "limitations": "The model did not show significant improvements in affective dimension classifications and performance was inconsistent for well-detected indicators using BERT.", "keywords": ["Collaborative Problem Solving", "AudiBERT", "Machine Learning", "Human-AI Complementarity", "Model Explainability"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.14944", "pdf": "https://arxiv.org/pdf/2507.14944.pdf", "abs": "https://arxiv.org/abs/2507.14944", "title": "LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection", "authors": ["Boning Zhao", "Yutong Hu"], "categories": ["cs.HC"], "comment": null, "summary": "Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a\ndual challenge: the need for deep, dynamic expert knowledge injection and\nnuanced value alignment. Prevailing paradigms often address these challenges\nseparately, creating a persistent tension between knowledge and alignment;\nknowledge-focused methods like Retrieval-Augmented Generation (RAG) have\nlimited deep alignment capabilities, while alignment-focused methods like\nReinforcement Learning from Human Feedback (RLHF) struggle with the agile\ninjection of expert wisdom. This paper introduces a new collaborative\nphilosophy, Expert-owned AI behavior design, realized through Architectural\nAlignment-a paradigm that unifies these two goals within a single framework\ncalled the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA\noperates as an intelligent intermediary that guides an LLM's reasoning process\nwithout altering its weights, utilizing a three-tiered structure: a Theoretical\nLayer for core principles, a Practical Layer for exemplary cases, and an\nEvaluative Layer for real-time, value-aligned self-correction. We demonstrate\nthe efficacy of this paradigm through the successful implementation of a\nLEKIA-based psychological support assistant for the special education field.\nOur work presents a path toward more responsible and expert-driven AI,\nempowering domain specialists to directly architect AI behavior and resolve the\ntension between knowledge and alignment.", "AI": {"tldr": "This paper introduces the Layered Expert Knowledge Injection Architecture (LEKIA) as a collaborative approach to integrating deep expert knowledge and value alignment in Large Language Models (LLMs).", "motivation": "To address the dual challenge of injecting expert knowledge and ensuring value alignment in deploying LLMs in high-stakes domains.", "method": "The paper proposes the Layered Expert Knowledge Injection Architecture (LEKIA), which consists of a Theoretical Layer, a Practical Layer, and an Evaluative Layer to guide LLM reasoning without altering weights.", "result": "LEKIA was successfully implemented in a psychological support assistant for the special education field, demonstrating its efficacy in unifying knowledge and alignment.", "conclusion": "The paradigm presents a path toward more responsible and expert-driven AI, enhancing the capability of domain specialists in AI behavior design.", "key_contributions": ["Introduction of a unified framework for knowledge injection and alignment in LLMs.", "Development of the LEKIA architecture with three distinct layers for enhanced AI behavior design.", "Practical implementation in the context of a psychological support assistant to demonstrate effectiveness."], "limitations": "", "keywords": ["Large Language Models", "Expert knowledge", "Value alignment", "AI behavior design", "Layered Expert Knowledge Injection Architecture"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.14584", "pdf": "https://arxiv.org/pdf/2507.14584.pdf", "abs": "https://arxiv.org/abs/2507.14584", "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "This study explores the explainability of BERT-based models in classifying collaborative problem solving (CPS) processes, using SHAP to examine the contributions of tokenized words, revealing discrepancies between classification performance and meaningfulness of explanations.", "motivation": "To enhance explainability and transparency of BERT models in educational applications, improving trust and adoption by end users such as teachers.", "method": "The study employed SHapley Additive exPlanations (SHAP) to analyze the contribution of individual tokenized words in transcription data on the classification decisions made by the BERT model.", "result": "The analysis indicated that high classification performance does not guarantee meaningful explanations; certain frequent tokenized words influenced classifications without semantic relevance, highlighting specific spurious words.", "conclusion": "While model transparency may not directly improve user practice, it emphasizes the need to balance LLM diagnostics with human expertise and calls for further exploration of ensemble models and human-AI collaboration in CPS diagnosis.", "key_contributions": ["Introduced SHAP for explainability in BERT-based CPS classification", "Identified spurious words affecting model classification", "Called for research on human-AI complementarity in CPS diagnostics"], "limitations": "The findings primarily address preliminary insights into explainability without extensive practical applications for end users.", "keywords": ["BERT", "explainability", "collaborative problem solving", "SHAP", "AI in education"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2507.14947", "pdf": "https://arxiv.org/pdf/2507.14947.pdf", "abs": "https://arxiv.org/abs/2507.14947", "title": "Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake", "authors": ["Ivan C. H. Liu", "Chung-En Hao", "Jing Xie"], "categories": ["cs.HC", "nlin.AO"], "comment": "7 pages, 8 figures, submitted to Leonardo", "summary": "Echoes of the Land is an interactive installation that transforms seismic\ndynamics into a multisensory experience through a scientifically grounded\nspring-block model. Simulating earthquake recurrence and self-organized\ncriticality, the work generates real-time sound and light via motion capture\nand concatenative granular synthesis. Each block acts as an agent, producing\nemergent audiovisual cascades that visualize the physics of rupture and\nthreshold behavior. This work exemplifies the amalgamation of scientific\nknowledge and artistic practice, opening new avenues for novel forms of musical\ninstrument and narrative medium, while inviting further investigation into the\nintersection of emergent complexity, aesthetics and interactivity.", "AI": {"tldr": "An interactive installation, Echoes of the Land, transforms seismic dynamics into a multisensory experience using a spring-block model, simulating earthquakes through sound and light.", "motivation": "To explore the intersection of scientific knowledge and artistic practice, creating a new medium for musical and narrative expression.", "method": "The installation employs motion capture and concatenative granular synthesis to simulate earthquake recurrence and self-organized criticality, with each block acting as an agent in the system.", "result": "Emergent audiovisual cascades visualize the physics of rupture and threshold behavior, showcasing a novel way to experience seismic dynamics.", "conclusion": "The work opens new avenues for exploration in emergent complexity, aesthetics, and interactivity, potentially leading to new forms of musical instruments and narratives.", "key_contributions": ["Introduction of a multisensory installation based on seismic dynamics", "Use of real-time sound and light generation in artistic practice", "Exploration of emergent complexity in HCI and audiovisual experiences"], "limitations": "", "keywords": ["interactive installation", "seismic dynamics", "multisensory experience", "sound synthesis", "emergent complexity"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.14590", "pdf": "https://arxiv.org/pdf/2507.14590.pdf", "abs": "https://arxiv.org/abs/2507.14590", "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "authors": ["Łukasz Radliński", "Mateusz Guściora", "Jan Kocoń"], "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "The paper investigates data augmentation techniques for NLP tasks, comparing traditional methods with generative approaches using large language models like ChatGPT.", "motivation": "The paper addresses the challenges of data scarcity and class imbalance in domain-specific machine learning tasks within NLP.", "method": "A series of experiments were conducted, comparing four data augmentation approaches, including paraphrasing and backtranslation, evaluating their effectiveness in generating data and improving classification performance.", "result": "The experiments showed that backtranslation and paraphrasing can produce comparable or better results compared to zero and few-shot generation methods.", "conclusion": "Traditional data augmentation methods, when paired with large language models, can effectively mitigate data scarcity issues and improve performance in NLP tasks.", "key_contributions": ["Systematic exploration of data augmentation techniques for NLP using LLMs.", "Comparison of traditional methods with generative methods for data quality and performance.", "Empirical evidence on the effectiveness of backtranslation and paraphrasing for data augmentation."], "limitations": "The study focuses on specific datasets and may not generalize across all NLP tasks or domains.", "keywords": ["data augmentation", "NLP", "machine learning", "ChatGPT", "backtranslation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.14961", "pdf": "https://arxiv.org/pdf/2507.14961.pdf", "abs": "https://arxiv.org/abs/2507.14961", "title": "Emphasizing Deliberation and Critical Thinking in an AI Hype World", "authors": ["Katja Rogers"], "categories": ["cs.HC"], "comment": "Accepted at CHI 2025 workshop: \"Resisting AI Solutionism: Where Do We\n  Go From Here?\" (https://doi.org/10.1145/3706599.3706732)", "summary": "AI solutionism is accelerated and substantiated by hype and HCI's elevation\nof novelty. Banning or abandoning technology is unlikely to work and probably\nnot beneficial on the whole either -- but slow(er), deliberate use together\nwith conscientious, critical engagement and non-engagement may help us navigate\na post-AI hype world while contributing to a solid knowledge foundation and\nreducing harmful impacts in education and research.", "AI": {"tldr": "The paper discusses the challenges posed by AI solutionism and proposes a cautious and critical approach to technology use in HCI to mitigate harmful impacts.", "motivation": "To address the detrimental effects of rapid AI adoption and hype in human-computer interaction and education.", "method": "The authors advocate for a slower, more deliberate approach to technology implementation while promoting critical engagement with AI.", "result": "A framework for navigating post-AI hype by encouraging researchers and educators to adopt intentional practices towards technology.", "conclusion": "Deliberate and critical engagement with AI technologies can help establish a robust knowledge base and reduce negative consequences in education and research contexts.", "key_contributions": ["Proposes a framework for critical engagement with AI technologies in HCI.", "Highlights the need for a conscientious approach to avoid the pitfalls of AI solutionism.", "Suggests strategies for navigating technology use post-hype."], "limitations": "The approach may not provide immediate solutions for all stakeholders in technology adoption.", "keywords": ["AI solutionism", "Human-Computer Interaction", "critical engagement", "technology use", "education"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.14615", "pdf": "https://arxiv.org/pdf/2507.14615.pdf", "abs": "https://arxiv.org/abs/2507.14615", "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "AI": {"tldr": "This paper presents a methodology and dataset for evaluating Large Language Models in Kenyan primary healthcare, focusing on clinical scenarios and localized evaluation metrics.", "motivation": "To improve healthcare access in low-resource settings like African primary care by effectively using Large Language Models (LLMs).", "method": "A benchmark dataset, the Alama Health QA dataset, was created using retrieval augmented generation (RAG) aligned with Kenya's national clinical guidelines. This included the digitization, chunking, and indexing of guidelines, generating clinical scenarios and questions in English and Swahili, involving co-creation with Kenyan physicians, and establishing evaluation metrics for clinical reasoning and adaptability.", "result": "The dataset includes thousands of regulator-aligned question-answer pairs, revealing performance gaps in LLMs for localized scenarios compared to US benchmarks, highlighting lower accuracy for African medical content.", "conclusion": "The study provides a replicable model for guideline-driven dynamic benchmarking, emphasizing the need for localized evaluations to ensure safe AI deployment in African health systems.", "key_contributions": ["Creation of the Alama Health QA dataset for Kenyan primary care", "Introduction of new evaluation metrics for clinical reasoning and adaptability", "Highlighting performance gaps of LLMs in localized African medical scenarios"], "limitations": "Specificity to Kenyan healthcare may limit generalizability, and initial results indicate significant performance gaps for LLMs in these contexts.", "keywords": ["Large Language Models", "healthcare access", "Kenyan primary care", "retrieval augmented generation", "AI deployment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.15033", "pdf": "https://arxiv.org/pdf/2507.15033.pdf", "abs": "https://arxiv.org/abs/2507.15033", "title": "'A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data", "authors": ["Dhruvee Birla", "Nazia Akhtar"], "categories": ["cs.HC"], "comment": null, "summary": "Social media was one of the most popular forms of communication among young\npeople with digital access during the pandemic. Consequently, crucial debates\nand discussions about the pandemic crisis have also developed on social media\nplatforms, making them a great primary source to study the experiences of\nspecific groups and communities during the pandemic. This study involved\nresearch using LDA topic modeling and sentiment analysis on data obtained from\nthe social media platform Reddit to understand the themes and attitudes in\ncirculation within five subreddits devoted to LGBTQ+ experiences and issues. In\nthe process, we attempt to make sense of the role that Reddit may have played\nin the lives of LGBTQ+ people who were online during the pandemic, and whether\nthis was marked by any continuities or discontinuities from before the pandemic\nperiod.", "AI": {"tldr": "This study analyzes Reddit discussions in LGBTQ+ communities during the pandemic using LDA topic modeling and sentiment analysis to uncover themes and attitudes.", "motivation": "To understand the experiences of specific groups, particularly LGBTQ+ individuals, during the pandemic through social media.", "method": "Utilized LDA topic modeling and sentiment analysis on data from five subreddits focused on LGBTQ+ issues to analyze themes and sentiments.", "result": "Identified key themes and sentiments expressed in LGBTQ+ subreddits during the pandemic and assessed changes compared to pre-pandemic discussions.", "conclusion": "Reddit served as a significant platform for LGBTQ+ individuals to share experiences during the pandemic, revealing both continuities and changes in discourse from previous periods.", "key_contributions": ["Application of LDA topic modeling in social media analysis", "Insights into LGBTQ+ community sentiment during a crisis", "Comparison of discussions before and during the pandemic"], "limitations": "Study limited to data from Reddit and may not represent all LGBTQ+ experiences; potential biases in sentiment analysis.", "keywords": ["LGBTQ+", "social media", "sentiment analysis", "topic modeling", "pandemic"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2507.14640", "pdf": "https://arxiv.org/pdf/2507.14640.pdf", "abs": "https://arxiv.org/abs/2507.14640", "title": "Linear Relational Decoding of Morphology in Language Models", "authors": ["Eric Xia", "Jugal Kalita"], "categories": ["cs.CL"], "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "AI": {"tldr": "A two-part affine approximation model reveals that transformer computations can reproduce final object states effectively for subject-object relations, achieving high fidelity in morphological relations across languages and models.", "motivation": "To explore the interpretability of conceptual relationships in language models and assess the effectiveness of linear transformations on transformer computations.", "method": "A two-part affine approximation is applied to transformer computations, utilizing a linear transformation derived from model derivatives to analyze middle layer representations of subject tokens.", "result": "The model achieves 90% faithfulness on morphological relations and demonstrates similar success across multiple languages and different models.", "conclusion": "Key insights show that certain relationships in language models can be interpreted from latent spaces and are encoded by sparse cross-layer linear transformations.", "key_contributions": ["Demonstrates effectiveness of affine approximation for interpreting transformer computations.", "Achieves high fidelity in modeling morphological relations.", "Extends findings across different languages and models."], "limitations": "", "keywords": ["affine approximation", "transformer models", "morphological relations", "language interpretation", "cross-layer transformations"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.15041", "pdf": "https://arxiv.org/pdf/2507.15041.pdf", "abs": "https://arxiv.org/abs/2507.15041", "title": "Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic", "authors": ["Dhruvee Birla", "Nazia Akhtar"], "categories": ["cs.HC"], "comment": null, "summary": "In India, online news media outlets were an important source of information\nfor people with digital access during the COVID-19 pandemic. In India, where\n\"transgender\" was legally recognised as a category only in 2014, and same-sex\nmarriages are yet to be legalised, it becomes crucial to analyse whether and\nhow they reported the lived realities of vulnerable LGBTQ+ communities during\nthe pandemic. This study analysed articles from online editions of two\nEnglish-language newspaper websites, which differed vastly in their circulation\nfigures-The Times of India and The Indian Express. The results of our study\nsuggest that these newspaper websites published articles surrounding various\naspects of the lives of LGBTQ+ individuals with a greater focus on transgender\ncommunities. However, they lacked quality and depth. Focusing on the period\nspanning March 2020 to August 2021, we analysed articles using sentiment\nanalysis and topic modelling. We also compared our results to the period before\nthe pandemic (January 2019 - December 2019) to understand the shift in topics,\nsentiments, and stances across the two newspaper websites. A manual analysis of\nthe articles indicated that the language used in certain articles by The Times\nof India was transphobic and obsolete. Our study captures the visibility and\nrepresentation of the LGBTQ+ communities in Indian newspaper websites during\nthe pandemic.", "AI": {"tldr": "The study examines the representation of LGBTQ+ communities in Indian online news media during the COVID-19 pandemic, focusing on articles from The Times of India and The Indian Express.", "motivation": "To analyze the reporting on LGBTQ+ communities, particularly transgender individuals, in Indian online news media amidst the COVID-19 pandemic, highlighting issues of visibility and representation.", "method": "The study utilized sentiment analysis and topic modeling on articles published from March 2020 to August 2021, comparing results to the previous period (January 2019 - December 2019).", "result": "The analysis revealed that both newspapers published more articles regarding LGBTQ+ individuals, especially transgender issues, but lacked depth and quality. Certain articles also exhibited transphobic language.", "conclusion": "The study underscores the challenges in quality representation of LGBTQ+ communities in Indian media, indicating a need for improvement in coverage and language.", "key_contributions": ["Empirical analysis of LGBTQ+ representation during the pandemic", "Sentiment analysis and topic modeling methodology applied to news articles", "Comparison of pre-pandemic and pandemic reporting"], "limitations": "The analysis was limited to only two newspapers and may not represent the entirety of online news media in India.", "keywords": ["LGBTQ+", "COVID-19", "sentiment analysis", "topic modeling", "Indian media"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.14649", "pdf": "https://arxiv.org/pdf/2507.14649.pdf", "abs": "https://arxiv.org/abs/2507.14649", "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "AI": {"tldr": "The study introduces Cleanse, a clustering-based semantic consistency method for estimating uncertainty in responses from large language models (LLMs) to address hallucination issues.", "motivation": "To tackle the critical problem of hallucinations in large language models, which can compromise their reliability and safety.", "method": "The proposed Cleanse approach quantifies uncertainty by assessing the intra-cluster consistency of LLM hidden embeddings through a clustering technique.", "result": "Cleanse effectively detects hallucinations in responses by validating its performance on four models (LLaMA-7B, LLaMA-13B, LLaMA2-7B, Mistral-7B) across SQuAD and CoQA benchmarks.", "conclusion": "The method shows promise in improving the identification of inaccurate responses from LLMs, thereby enhancing their reliability.", "key_contributions": ["Introduction of Cleanse for uncertainty estimation in LLMs.", "Use of clustering for assessing semantic consistency in model outputs.", "Validation on multiple LLMs and question-answering benchmarks."], "limitations": "", "keywords": ["large language models", "hallucinations", "uncertainty estimation", "clustering", "semantic consistency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.15049", "pdf": "https://arxiv.org/pdf/2507.15049.pdf", "abs": "https://arxiv.org/abs/2507.15049", "title": "Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence", "authors": ["Andres Navarro", "Carlos de Quinto", "José Alberto Hernández"], "categories": ["cs.HC"], "comment": null, "summary": "Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as\nagile, intelligent nodes capable of advanced analytics and instantaneous\nsituational awareness. This article introduces a budget-friendly quadcopter\nplatform that unites 5G communications, edge-based processing, and AI to tackle\ncore challenges in NTN scenarios. Outfitted with a panoramic camera, robust\nonboard computation, and LLMs, the drone system delivers seamless object\nrecognition, contextual analysis, and immersive operator experiences through\nvirtual reality VR technology. Field evaluations confirm the platform's ability\nto process visual streams with low latency and sustain robust 5G links. Adding\nLLMs further streamlines operations by extracting actionable insights and\nrefining collected data for decision support. Demonstrated use cases, including\nemergency response, infrastructure assessment, and environmental surveillance,\nunderscore the system's adaptability in demanding contexts.", "AI": {"tldr": "The paper presents a budget-friendly quadcopter platform that combines 5G communications, edge-based processing, and AI to enhance Non-Terrestrial Networks with capabilities such as object recognition and contextual analysis, validated through field evaluations.", "motivation": "Address core challenges in Non-Terrestrial Network scenarios using advanced technologies like 5G, edge processing, and AI.", "method": "Introduction of a quadcopter platform equipped with a panoramic camera, onboard computation, and LLMs for various operational scenarios.", "result": "Successful field evaluations demonstrated low-latency processing of visual streams and robust 5G connectivity, showcasing the system's capabilities.", "conclusion": "The quadcopter system is adaptable for use cases such as emergency response, infrastructure assessment, and environmental surveillance.", "key_contributions": ["Development of a budget-friendly quadcopter platform for NTN", "Integration of 5G, edge processing, and LLMs for enhanced situational awareness", "Validation of the platform through practical use cases"], "limitations": "", "keywords": ["Unmanned Aerial Vehicles", "5G communications", "Edge-based processing", "AI", "Non-Terrestrial Networks"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.14664", "pdf": "https://arxiv.org/pdf/2507.14664.pdf", "abs": "https://arxiv.org/abs/2507.14664", "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "AI": {"tldr": "This paper introduces Mangosteen, a high-quality Thai language corpus designed to enhance language model training. It presents a transparent pipeline that includes extensive cleaning and curation strategies aimed at addressing the challenges of existing datasets.", "motivation": "Existing language models suffer from poor quality due to noisy data sources, particularly in Thai. The need for a high-quality, transparent corpus that respects cultural nuances and reliably filters out problematic content is essential for robust language model training.", "method": "The paper outlines the creation of the Mangosteen corpus using a customized Dolma pipeline that combines language identification, quality filtering, and content curation methods adapted specifically for Thai text.", "result": "The Mangosteen corpus consists of 47 billion tokens and successfully reduces the original CommonCrawl dataset while improving the performance of Thai language models, as demonstrated through systematic ablation studies with GPT-2 and subsequent evaluations of the SEA-LION model.", "conclusion": "The Mangosteen corpus and its accompanying pipeline are released as reproducible resources, promoting transparency and enabling further research in Thai and regional language modeling.", "key_contributions": ["Introduction of the Mangosteen corpus with 47 billion tokens.", "Development of a tailored pipeline for Thai text processing that enhances language model training.", "Release of all related code, cleaning manifests, and data, ensuring reproducibility."], "limitations": "", "keywords": ["Thai corpus", "Language model", "Data cleaning", "Natural language processing", "Thai language"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.15072", "pdf": "https://arxiv.org/pdf/2507.15072.pdf", "abs": "https://arxiv.org/abs/2507.15072", "title": "NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments", "authors": ["Maisha Maimuna", "Minhaz Bin Farukee", "Sama Nikanfar", "Mahfuza Siddiqua", "Ayon Roy", "Fillia Makedon"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Industrial warehouses are congested with moving forklifts, shelves and\npersonnel, making robot teleoperation particularly risky and demanding for\nblind and low-vision (BLV) operators. Although accessible teleoperation plays a\nkey role in inclusive workforce participation, systematic research on its use\nin industrial environments is limited, and few existing studies barely address\nmultimodal guidance designed for BLV users. We present a novel multimodal\nguidance simulator that enables BLV users to control a mobile robot through a\nhigh-fidelity warehouse environment while simultaneously receiving synchronized\nvisual, auditory, and haptic feedback. The system combines a navigation mesh\nwith regular re-planning so routes remain accurate avoiding collisions as\nforklifts and human avatars move around the warehouse. Users with low vision\nare guided with a visible path line towards destination; navigational voice\ncues with clockwise directions announce upcoming turns, and finally\nproximity-based haptic feedback notifies the users of static and moving\nobstacles in the path. This real-time, closed-loop system offers a repeatable\ntestbed and algorithmic reference for accessible teleoperation research. The\nsimulator's design principles can be easily adapted to real robots due to the\nalignment of its navigation, speech, and haptic modules with commercial\nhardware, supporting rapid feasibility studies and deployment of inclusive\ntelerobotic tools in actual warehouses.", "AI": {"tldr": "A simulator designed for blind and low-vision operators to teleoperate robots in industrial warehouses using multimodal guidance.", "motivation": "To address the risks and demands of robot teleoperation for blind and low-vision users in industrial settings, where accessibility is crucial for inclusive workforce participation.", "method": "The system utilizes a navigation mesh with dynamic re-planning, providing synchronized visual, auditory, and haptic feedback to guide users in a high-fidelity warehouse environment.", "result": "The simulator successfully enables BLV users to control a mobile robot with enhanced awareness of their surroundings through multimodal feedback, ensuring accurate navigation and obstacle avoidance in real time.", "conclusion": "This closed-loop guidance simulator serves as a repeatable testbed for accessible teleoperation research, with design principles that can be adapted for real robots, promoting the development of inclusive telerobotic tools.", "key_contributions": ["Introduction of a novel multimodal guidance simulator for BLV operators", "Combination of visual, auditory, and haptic feedback for real-time navigation", "Dynamic path replanning for obstacle avoidance in a high-fidelity environment"], "limitations": "", "keywords": ["teleoperation", "blind and low-vision", "multimodal guidance", "warehouse robotics", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.14681", "pdf": "https://arxiv.org/pdf/2507.14681.pdf", "abs": "https://arxiv.org/abs/2507.14681", "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel Gómez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "categories": ["cs.CL"], "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "AI": {"tldr": "This study evaluates the ability of large language models (LLMs) to assign ICPC-2 codes to clinical expressions using a domain-specific semantic search engine.", "motivation": "To assess the capability of LLMs in automating the assignment of medical coding for better healthcare data usage.", "method": "Utilized a dataset of 437 clinical expressions annotated with ICPC-2 codes, prompting 33 LLMs with queries from a semantic search engine and evaluating performance based on F1-score, cost, and format adherence.", "result": "28 LLMs achieved F1-scores greater than 0.8, with top performers such as gpt-4.5-preview exhibiting the best results. Retriever optimization can enhance performance and most models provided valid codes with fewer hallucinations.", "conclusion": "LLMs demonstrate strong potential for automating ICPC-2 coding, but further evaluations are necessary for clinical validation across broader datasets.", "key_contributions": ["Benchmark of LLM performance for medical coding", "Identification of top-performing LLMs in ICPC-2 assignment", "Insights on retriever optimization impacts on performance"], "limitations": "Findings are constrained by dataset scope and setup; further multilingual evaluations needed for comprehensive validation.", "keywords": ["ICPC-2", "large language models", "medical coding", "health informatics", "semantic search"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.15081", "pdf": "https://arxiv.org/pdf/2507.15081.pdf", "abs": "https://arxiv.org/abs/2507.15081", "title": "\"If I were in Space\": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives", "authors": ["Qi Gong", "Ximing Shen", "Ziyou Yin", "Yaning Li", "Ray Lc"], "categories": ["cs.HC"], "comment": null, "summary": "Social isolation can lead to pervasive health issues like anxiety and\nloneliness. Previous work focused on physical interventions like exercise and\nteleconferencing, but overlooked the narrative potential of adaptive\nstrategies. To address this, we designed a collaborative online storytelling\nexperience in social VR, enabling participants in isolation to design an\nimaginary space journey as a metaphor for quarantine, in order to learn about\ntheir isolation adaptation strategies in the process. Eighteen individuals\nparticipated during real quarantine undertaken a virtual role-play experience,\ndesigning their own spaceship rooms and engaging in collaborative activities\nthat revealed creative adaptative strategies. Qualitative analyses of\nparticipant designs, transcripts, and interactions revealed how they coped with\nisolation, and how the engagement unexpectedly influenced their adaptation\nprocess. This study shows how designing playful narrative experiences, rather\nthan solution-driven approaches, can serve as probes to surface how people\nnavigate social isolation.", "AI": {"tldr": "This study explores how collaborative online storytelling in social VR can help individuals cope with isolation by using playful narrative experiences to reveal adaptive strategies.", "motivation": "To address the health issues arising from social isolation, particularly anxiety and loneliness, by examining the narrative potential of adaptive strategies through virtual experiences.", "method": "Eighteen individuals participated in a collaborative online storytelling experience in social VR, engaging in a virtual role-play where they designed imaginary spaceship rooms to represent their coping mechanisms during quarantine.", "result": "Qualitative analyses indicated that participants revealed creative adaptative strategies through their designs and interactions, influencing their adaptation process unexpectedly.", "conclusion": "Designing playful narrative experiences in social VR can serve as effective probes to better understand how individuals navigate social isolation, highlighting the value of creativity over traditional solution-driven approaches.", "key_contributions": ["Introduced a novel approach to address isolation through narrative experiences in social VR.", "Revealed how collaborative storytelling can unveil adaptive strategies for coping with loneliness.", "Demonstrated the unexpected influences of playful narratives on adaptation processes."], "limitations": "The study primarily involved a small sample size and was conducted during specific quarantine conditions, which may limit generalizability.", "keywords": ["Social isolation", "Virtual reality", "Collaborative storytelling", "Adaptive strategies", "Health informatics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.14683", "pdf": "https://arxiv.org/pdf/2507.14683.pdf", "abs": "https://arxiv.org/abs/2507.14683", "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "categories": ["cs.CL"], "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "AI": {"tldr": "Introduction of fully open-source reasoning language models (RLMs) with a focus on mathematical reasoning, enhancing transparency and reproducibility in their development.", "motivation": "To address the lack of transparency and reproducibility in the development of reasoning language models (RLMs), particularly in the domain of mathematical reasoning.", "method": "The models are trained in two stages: SFT on a curated corpus of 719K math-reasoning problems followed by RLVR on 62K challenging problems, using Context-Aware Multi-Stage Policy Optimization for improved training effectiveness.", "result": "MiroMind-M1 series matches or exceeds existing open-source RLM performance, achieving state-of-the-art on AIME24, AIME25, and MATH benchmarks with superior token efficiency.", "conclusion": "The release of the MiroMind-M1 series, along with datasets and training configurations, aims to promote reproducibility and support ongoing research in RLMs.", "key_contributions": ["Introduction of MiroMind-M1 series as open-source RLMs", "Development of Context-Aware Multi-Stage Policy Optimization algorithm", "Comprehensive release of models and datasets for transparency"], "limitations": "", "keywords": ["reasoning language models", "mathematical reasoning", "open-source", "Context-Aware Multi-Stage Policy Optimization", "reproducibility"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15202", "pdf": "https://arxiv.org/pdf/2507.15202.pdf", "abs": "https://arxiv.org/abs/2507.15202", "title": "TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style", "authors": ["Karim Benharrak", "Puyuan Peng", "Amy Pavel"], "categories": ["cs.HC"], "comment": null, "summary": "Millions of people listen to podcasts, audio stories, and lectures, but\nediting speech remains tedious and time-consuming. Creators remove unnecessary\nwords, cut tangential discussions, and even re-record speech to make recordings\nconcise and engaging. Prior work automatically summarized speech by removing\nfull sentences (extraction), but rigid extraction limits expressivity. AI tools\ncan summarize then re-synthesize speech (abstraction), but abstraction strips\nthe speaker's style. We present TalkLess, a system that flexibly combines\nextraction and abstraction to condense speech while preserving its content and\nstyle. To edit speech, TalkLess first generates possible transcript edits,\nselects edits to maximize compression, coverage, and audio quality, then uses a\nspeech editing model to translate transcript edits into audio edits. TalkLess's\ninterface provides creators control over automated edits by separating\nlow-level wording edits (via the compression pane) from major content edits\n(via the outline pane). TalkLess achieves higher coverage and removes more\nspeech errors than a state-of-the-art extractive approach. A comparison study\n(N=12) showed that TalkLess significantly decreased cognitive load and editing\neffort in speech editing. We further demonstrate TalkLess's potential in an\nexploratory study (N=3) where creators edited their own speech.", "AI": {"tldr": "TalkLess is a system that combines extraction and abstraction to automate speech editing, preserving both content and speaker style.", "motivation": "Editing speech recordings is a tedious process for many creators, necessitating a solution that enhances efficiency without sacrificing personal style.", "method": "TalkLess generates possible transcript edits that maximize compression and audio quality, using a speech editing model to convert transcript changes into audio edits, and provides an interface that separates different levels of editing.", "result": "TalkLess outperformed a leading extractive approach in terms of coverage and error reduction, and reduced cognitive load and editing effort through user studies.", "conclusion": "TalkLess represents a significant advancement in speech editing technology by allowing control over automated edits while preserving speaker style.", "key_contributions": ["Combines extraction and abstraction for automated speech editing", "Offers a user-friendly interface with two distinct editing panes", "Demonstrates significant improvements in editing efficiency and effectiveness compared to existing methods"], "limitations": "", "keywords": ["speech editing", "automation", "user interface", "AI", "cognitive load"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.14688", "pdf": "https://arxiv.org/pdf/2507.14688.pdf", "abs": "https://arxiv.org/abs/2507.14688", "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "AI": {"tldr": "This paper reviews the quality and diversity of Arabic post-training datasets for Large Language Models available on Hugging Face, identifying critical gaps and offering recommendations for improvement.", "motivation": "Aligning pre-trained LLMs with human instructions is vital for enhancing their performance, particularly in Arabic contexts. The paper aims to evaluate available datasets to address performance disparities.", "method": "The review categorizes datasets based on LLM capabilities, steerability, alignment, and robustness, evaluating them against criteria like popularity, adoption, documentation quality, licensing, and scientific contribution.", "result": "The analysis revealed significant gaps in task diversity, documentation quality, and community adoption of Arabic post-training datasets.", "conclusion": "Addressing these gaps is crucial for advancing Arabic LLM capabilities and applications. The paper provides recommendations to improve dataset development.", "key_contributions": ["Comprehensive review of Arabic post-training datasets on Hugging Face Hub", "Identification of critical gaps in the current datasets", "Recommendations for future development of post-training datasets"], "limitations": "Limited focus on post-training datasets outside Arabic and lack of experimental validation of findings.", "keywords": ["Large Language Models", "Arabic Datasets", "Human-in-the-loop", "Post-training", "Dataset Evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.15244", "pdf": "https://arxiv.org/pdf/2507.15244.pdf", "abs": "https://arxiv.org/abs/2507.15244", "title": "How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective", "authors": ["Leixian Shen", "Leni Yang", "Haotian Li", "Yun Wang", "Yuyu Luo", "Huamin Qu"], "categories": ["cs.HC"], "comment": null, "summary": "Empirical research in creative design deepens our theoretical understanding\nof design principles and perceptual effects, offering valuable guidance for\ninnovating creation tools. However, how these empirical insights currently\ninfluence the development of creation tools, and how their integration can be\nenhanced in the future, remains insufficiently understood. In this paper, we\naim to unveil the gap through a case study on data videos, a prominent and\nwide-spread medium for effective data storytelling. To achieve the goal, we\nconducted a comprehensive analysis of 46 empirical research papers and 48\ncreation tool papers on data video, complemented by interviews with 11 experts.\nBuilding upon a systematic collection and structured characterization of\nempirical research by their methodologies (e.g., corpus analysis, comparative\nevaluations) and component focus (e.g., visuals, motions, narratives, audio),\nwe conducted a context-aware citation analysis and revealed a taxonomy of\nrecurring patterns in how empirical findings inform tool design across citation\nfunctions (e.g., problem framing, technical reference). Expert interviews\nfurther uncovered researchers' practice patterns in applying empirical findings\n(e.g., adaptation, synthesis, iteration, etc.) and identified key factors\ninfluencing applicability, such as contextual relevance, granularity matching,\nclarity, credibility, and feasibility. Finally, we derive suggestions and\ndiscuss future opportunities to foster closer mutual engagement between\nempirical and tool research, aiming to reinforce the theoretical grounding of\ncreation tools and enhance the practical impact of empirical research.", "AI": {"tldr": "This paper explores the integration of empirical research insights into the development of creation tools for data videos, revealing gaps and proposing improvements for mutual engagement between empirical studies and tool design.", "motivation": "To understand how empirical research influences creation tools and to enhance their integration for better design practice.", "method": "A case study on data videos, involving analysis of 46 empirical research papers and 48 creation tool papers, plus interviews with 11 experts.", "result": "Identification of a taxonomy of patterns showing how empirical findings inform tool design and pratice patterns among researchers.", "conclusion": "The study reveals critical factors for applying empirical insights to tool development and suggests ways to improve the synergy between empirical research and tool design.", "key_contributions": ["Developed a taxonomy of recurring patterns in empirical findings and tool design integration.", "Identified key factors influencing the applicability of empirical research to tool development.", "Provided recommendations for enhancing collaboration between empirical and tool research."], "limitations": "Focuses specifically on data videos, which may limit generalizability to other domains.", "keywords": ["empirical research", "creation tools", "data videos", "tool design", "HCI"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.14693", "pdf": "https://arxiv.org/pdf/2507.14693.pdf", "abs": "https://arxiv.org/abs/2507.14693", "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "This study addresses challenges in suicidal ideation detection by creating a Turkish corpus and an efficient annotation framework while evaluating model performance and annotation reliability.", "motivation": "The study aims to tackle the limited language coverage and unreliable annotation practices in suicidal ideation detection, critical for suicide prevention.", "method": "A novel Turkish suicidal ideation corpus was constructed using social media posts. An annotation framework with three human annotators and two LLMs was introduced, followed by bidirectional evaluation of label reliability and model consistency.", "result": "The study found significant gaps in annotation practices and highlighted the inadequate performance of popular models in zero-shot transfer learning scenarios.", "conclusion": "Emphasis was placed on the need for transparent, rigorous, and language-inclusive approaches in mental health NLP to ensure data and model reliability.", "key_contributions": ["Creation of a Turkish suicidal ideation corpus.", "Introduction of a resource-efficient annotation framework using human annotators and LLMs.", "Evaluation of label reliability and model consistency against existing datasets."], "limitations": "The study primarily focuses on the Turkish language with limited applicability to other languages and datasets.", "keywords": ["suicidal ideation", "natural language processing", "annotation reliability", "language coverage", "mental health"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.15355", "pdf": "https://arxiv.org/pdf/2507.15355.pdf", "abs": "https://arxiv.org/abs/2507.15355", "title": "Efficient Visual Appearance Optimization by Learning from Prior Preferences", "authors": ["Zhipeng Li", "Yi-Chi Liao", "Christian Holz"], "categories": ["cs.HC", "cs.LG"], "comment": "24 pages, UIST'25", "summary": "Adjusting visual parameters such as brightness and contrast is common in our\neveryday experiences. Finding the optimal parameter setting is challenging due\nto the large search space and the lack of an explicit objective function,\nleaving users to rely solely on their implicit preferences. Prior work has\nexplored Preferential Bayesian Optimization (PBO) to address this challenge,\ninvolving users to iteratively select preferred designs from candidate sets.\nHowever, PBO often requires many rounds of preference comparisons, making it\nmore suitable for designers than everyday end-users. We propose Meta-PO, a\nnovel method that integrates PBO with meta-learning to improve sample\nefficiency. Specifically, Meta-PO infers prior users' preferences and stores\nthem as models, which are leveraged to intelligently suggest design candidates\nfor the new users, enabling faster convergence and more personalized results.\nAn experimental evaluation of our method for appearance design tasks on 2D and\n3D content showed that participants achieved satisfactory appearance in 5.86\niterations using Meta-PO when participants shared similar goals with a\npopulation (e.g., tuning for a ``warm'' look) and in 8 iterations even\ngeneralizes across divergent goals (e.g., from ``vintage'', ``warm'', to\n``holiday''). Meta-PO makes personalized visual optimization more applicable to\nend-users through a generalizable, more efficient optimization conditioned on\npreferences, with the potential to scale interface personalization more\nbroadly.", "AI": {"tldr": "Meta-PO integrates Preferential Bayesian Optimization with meta-learning to optimize visual parameter adjustments efficiently for end-users.", "motivation": "Finding optimal visual parameters like brightness and contrast is challenging due to large search spaces and implicit user preferences.", "method": "The proposed Meta-PO method infers prior users' preferences and creates models that suggest design candidates for new users, enhancing sample efficiency and convergence speed.", "result": "Meta-PO achieved satisfactory appearance in 5.86 iterations for users with similar goals and in 8 iterations across divergent goals during experimental evaluations.", "conclusion": "Meta-PO enhances personalized visual optimization, making it more accessible for end-users and allowing broader scaling of interface personalization.", "key_contributions": ["Novel integration of PBO with meta-learning for user preference inference", "Improved sample efficiency in visual optimization tasks", "Generalizability across different user goals for design tasks"], "limitations": "Depends on the availability of prior users' preferences for effective optimization.", "keywords": ["Human-Computer Interaction", "Preferential Bayesian Optimization", "Meta-learning", "User Preferences", "Visual Optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.14741", "pdf": "https://arxiv.org/pdf/2507.14741.pdf", "abs": "https://arxiv.org/abs/2507.14741", "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "categories": ["cs.CL"], "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "AI": {"tldr": "This study analyzes over 80,000 peer reviews to uncover linguistic biases related to author demographics, revealing implications for fairness in academic publishing.", "motivation": "To investigate subtle linguistic biases in the peer review process that may reinforce disparities among authors based on demographics.", "method": "The study employs natural language processing and large-scale statistical modeling to analyze review tone, sentiment, and supportive language across demographic variables, utilizing a dataset of both anonymous and signed peer reviews.", "result": "The analysis reveals significant variations in review language influenced by the gender, race, and institutional affiliation of authors, and highlights how reviewer anonymity affects evaluation language.", "conclusion": "The findings challenge conventional views on the fairness provided by anonymity in peer review and suggest that review policies can significantly impact scientific integrity and career trajectories.", "key_contributions": ["Comprehensive linguistic analysis of peer reviews", "Identification of biases related to author demographics", "Implications for reform in academic publishing"], "limitations": "", "keywords": ["peer review", "language analysis", "bias", "academic publishing", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.15433", "pdf": "https://arxiv.org/pdf/2507.15433.pdf", "abs": "https://arxiv.org/abs/2507.15433", "title": "Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools", "authors": ["Lou Schwartz", "Mohammad Ghoniem", "Valérie Maquil", "Adrien Coppens", "Johannes Hermen"], "categories": ["cs.HC"], "comment": "Publication URL:\n  https://www.thinkmind.org/library/Soft/Soft_v18_n12_2025/soft_v18_n12_2025_5.html", "summary": "Wall-Sized Displays have spatial characteristics that are difficult to\naddress during user interface design. The design at scale 1:1 could be part of\nthe solution. In this paper, we present the results of two user studies and one\ntechnology review, exploring the usability of popular, desktop-optimized\nprototyping tools, for designing at scale on Wall-Sized Displays. We considered\ntwo wall-sized display setups, and three different interaction methods: touch,\na keyboard equipped with a touchpad, and a tablet. We observed that designing\nat scale 1:1 was appreciated. Tablet-based interaction proved to be the most\ncomfortable interaction method, and a mix of interaction modalities is\npromising. In addition, care must be given to the surrounding environment, such\nas furniture. We propose twelve design guidelines for a design tool dedicated\nto this specific context. Overall, existing user interface design tools do not\nyet fully support design on and for wall-sized displays and require further\nconsiderations in terms of placement of user interface elements and the\nprovision of additional features.", "AI": {"tldr": "The paper investigates usability in designing user interfaces for wall-sized displays using different prototyping tools and interaction methods.", "motivation": "The unique spatial characteristics of wall-sized displays necessitate specialized user interface design strategies.", "method": "Two user studies and a technology review were conducted to evaluate desktop-optimized prototyping tools for wall-sized displays, focusing on touch, keyboard with touchpad, and tablet interactions.", "result": "Tablet-based interaction was found to be the most comfortable, and a mixed-modality interaction approach was proposed as promising.", "conclusion": "Existing tools for user interface design do not adequately support wall-sized displays, leading to the development of twelve design guidelines for this specific area.", "key_contributions": ["Results from user studies on interaction methods for wall-sized displays", "Proposition of twelve design guidelines for dedicated design tools", "Insights into usability limitations of existing user interface design tools"], "limitations": "Current tools do not fully support user interface design for wall-sized displays and require further improvement in element placement and feature provision.", "keywords": ["user interface design", "wall-sized displays", "interaction methods", "design guidelines", "prototyping tools"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.14749", "pdf": "https://arxiv.org/pdf/2507.14749.pdf", "abs": "https://arxiv.org/abs/2507.14749", "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "categories": ["cs.CL"], "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "AI": {"tldr": "This paper explores the robustness of multimodal neural networks in acquiring word-referent mappings using a dataset simulating children's language acquisition.", "motivation": "To investigate how machine learning, particularly neural networks, can mimic children's language acquisition with limited input data.", "method": "The study utilized automated speech transcription methods on the SAYCam dataset, creating multimodal datasets from 500 hours of video data with three children, and examined various neural network configurations for word learning.", "result": "Networks trained on data transcribed from individual children successfully acquired and generalized word-referent mappings across different architectures.", "conclusion": "The results indicate that multimodal neural networks can robustly support grounded word learning while exhibiting individual differences based on children's unique developmental experiences.", "key_contributions": ["Developed multimodal vision-and-language datasets from children's video data.", "Demonstrated the effectiveness of neural networks in simulating word learning.", "Highlighted individual differences in language acquisition patterns among children."], "limitations": "", "keywords": ["machine learning", "language acquisition", "multimodal models", "neural networks", "word learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15443", "pdf": "https://arxiv.org/pdf/2507.15443.pdf", "abs": "https://arxiv.org/abs/2507.15443", "title": "Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays", "authors": ["Adrien Coppens", "Valérie Maquil"], "categories": ["cs.HC"], "comment": "Version of record / published version:\n  https://dl.acm.org/doi/full/10.1145/3731406.3731973", "summary": "To understand and quantify the quality of mixed-presence collaboration around\nwall-sized displays, robust evaluation methodologies are needed, that are\nadapted for a room-sized experience and are not perceived as obtrusive. In this\npaper, we propose our approach for measuring joint attention based on head gaze\ndata. We describe how it has been implemented for a user study on mixed\npresence collaboration with two wall-sized displays and report on the insights\nwe gained so far from its implementation, with a preliminary focus on the data\ncoming from one particular session.", "AI": {"tldr": "This paper presents a method to evaluate mixed-presence collaboration using head gaze data in the context of wall-sized displays.", "motivation": "The need for robust evaluation methodologies that are not obtrusive for assessing quality in mixed-presence collaboration experiences around large displays.", "method": "The approach involves measuring joint attention through head gaze data during user studies with two wall-sized displays.", "result": "Insights gained from initial implementation focus on gaze data from one user study session, contributing to the understanding of collaborative interactions in mixed-presence setups.", "conclusion": "The proposed method offers a non-intrusive way to assess collaborative experiences in large display environments, providing valuable insights for future research.", "key_contributions": ["Novel methodology for measuring joint attention using head gaze data", "Implementation in a user study with wall-sized displays", "Preliminary insights into mixed-presence collaboration dynamics"], "limitations": "", "keywords": ["mixed-presence", "collaboration", "head gaze data", "evaluation methodology", "wall-sized displays"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.14758", "pdf": "https://arxiv.org/pdf/2507.14758.pdf", "abs": "https://arxiv.org/abs/2507.14758", "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "AI": {"tldr": "This paper presents GRACE, a novel generative framework for multi-behavior sequential recommendation that improves the efficiency and interpretability of recommendation systems by using a hybrid tokenization method and a journey-aware sparse attention mechanism.", "motivation": "The paper addresses the challenges in generative recommendation systems, specifically the need for explicit token reasoning and high computational costs associated with current models.", "method": "GRACE utilizes a hybrid Chain-of-Thought tokenization to encode user-item interactions, alongside a Journey-Aware Sparse Attention mechanism that improves attention efficiency.", "result": "GRACE achieved significant performance improvements, with up to +106.9% HR@10 and +106.7% NDCG@10 in the Home domain and +22.1% HR@10 in the Electronics domain, while reducing attention computation by up to 48%.", "conclusion": "The proposed framework offers a promising direction for enhancing the interpretability and efficiency of recommendation systems.", "key_contributions": ["Introduction of a hybrid Chain-of-Thought tokenization method for user-item interactions", "Development of a Journey-Aware Sparse Attention mechanism to reduce computational costs", "Demonstrated performance improvements over existing state-of-the-art recommendation models."], "limitations": "", "keywords": ["Generative Models", "Recommendation Systems", "Machine Learning", "Attention Mechanisms", "Tokenization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.15481", "pdf": "https://arxiv.org/pdf/2507.15481.pdf", "abs": "https://arxiv.org/abs/2507.15481", "title": "Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands", "authors": ["Yesica Duarte", "Puneet Jain"], "categories": ["cs.HC"], "comment": null, "summary": "Virtual Reality (VR) is often described as the \"ultimate empathy machine,\"\nframing disability as an experience to be simulated through such technologies,\nwhich can reduce disability to a spectacle of pity or inspiration. In response,\nwe present Waiting for Hands (WfH), an interactive eXtended Reality (XR)\ninstallation that critiques this logic by: (1) repurposing interaction norms in\nXR through the creation of Alternative Controllers, and (2) staging an absurd\nXR performance using the built controllers to disrupt sentimentalized\ndisability narratives. The performance involves eight people: two XR\nparticipants on stage and six audience members watching a projected documentary\nabout Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR\nusers partially obscure the film, drawing attention through strange mouth and\nhand movements performed in XR. This creates a layered experience that disrupts\ndirect engagement with Hema's story and introduces uncertainty. While XR is\noften seen as a fully immersive, sensory-dominant medium, this piece subverts\nthat framing by using XR to produce absurdity and alienation. By challenging\nempathy-driven and pitiable narratives of disability, we ask what ethical\nstance an XR performance can take to attune participants to non-normative\nembodiment while resisting spectacle.", "AI": {"tldr": "Waiting for Hands (WfH) critiques the portrayal of disability in Virtual Reality (VR) by introducing Alternative Controllers and staging an absurd XR performance to disrupt sentimental narratives.", "motivation": "To challenge the tendency of VR to frame disability as a spectacle that elicits pity or inspiration.", "method": "Creation of Alternative Controllers in an interactive eXtended Reality (XR) installation, featuring a performance combining XR users and an audience watching a documentary.", "result": "The performance creates a layered experience that disrupts engagement with the documentary, drawing attention through absurd movements and introducing uncertainty.", "conclusion": "The piece showcases how XR can be used to question empathy-driven narratives and promote awareness of non-normative embodiments without resorting to spectacle.", "key_contributions": ["Repurposes interaction norms in XR through Alternative Controllers", "Stages an absurd performance that critiques disability narratives", "Encourages ethical considerations in XR contexts related to embodiment"], "limitations": "", "keywords": ["Virtual Reality", "eXtended Reality", "disability", "performance", "empathy"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.14815", "pdf": "https://arxiv.org/pdf/2507.14815.pdf", "abs": "https://arxiv.org/abs/2507.14815", "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "categories": ["cs.CL"], "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.", "AI": {"tldr": "FastLongSpeech is a framework for efficient long-speech processing using LSLMs without dedicated long-speech training data.", "motivation": "There is a critical gap in processing long-form speech in existing Large Speech-Language Models due to lack of training datasets and high computational costs.", "method": "FastLongSpeech employs an iterative fusion strategy to handle long-speech sequences and utilizes dynamic compression training to adapt to varying compression ratios of short-speech sequences.", "result": "The methodology demonstrates strong performance across long-speech and short-speech tasks, significantly enhancing inference efficiency.", "conclusion": "FastLongSpeech effectively extends the abilities of LSLMs to efficiently process long-speech without needing dedicated data.", "key_contributions": ["Introduction of FastLongSpeech framework for long-speech processing.", "Development of dynamic compression training approach for LSLMs.", "Creation of LongSpeech-Eval benchmark for assessing long-speech capabilities."], "limitations": "", "keywords": ["Large Speech-Language Models", "speech processing", "long-speech", "dynamic compression", "benchmarking"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.15502", "pdf": "https://arxiv.org/pdf/2507.15502.pdf", "abs": "https://arxiv.org/abs/2507.15502", "title": "FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up", "authors": ["Chen Chen", "Jianing Yin", "Jiannong Cao", "Zhiyuan Wen", "Mingjin Zhang", "Weixun Gao", "Xiang Wang", "Haihua Shu"], "categories": ["cs.HC"], "comment": null, "summary": "Postoperative follow-up plays a crucial role in monitoring recovery and\nidentifying complications. However, traditional approaches, typically involving\nbedside interviews and manual documentation, are time-consuming and\nlabor-intensive. Although existing digital solutions, such as web\nquestionnaires and intelligent automated calls, can alleviate the workload of\nnurses to a certain extent, they either deliver an inflexible scripted\ninteraction or face private information leakage issues. To address these\nlimitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed\nrobot for postoperative care and monitoring. It allows dynamic planning of\noptimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face\nconversations with patients through multiple interaction modes, ensuring data\nprivacy. Moreover, FollowUpBot is capable of automatically generating\nstructured postoperative follow-up reports for healthcare institutions by\nanalyzing patient interactions during follow-up. Experimental results\ndemonstrate that our robot achieves high coverage and satisfaction in follow-up\ninteractions, as well as high report generation accuracy across diverse field\ntypes. The demonstration video is available at\nhttps://www.youtube.com/watch?v=_uFgDO7NoK0.", "AI": {"tldr": "FollowUpBot is an LLM-powered robot designed for postoperative care, providing dynamic conversations with patients and generating follow-up reports.", "motivation": "To improve postoperative follow-up by reducing the time and labor involved in traditional methods while ensuring data privacy.", "method": "The robot uses edge-deployed LLMs for real-time, adaptive conversations with patients in various interaction modes.", "result": "FollowUpBot demonstrated high coverage and patient satisfaction in follow-up interactions, with accurate report generation.", "conclusion": "The system effectively addresses shortcomings of traditional methods and existing digital solutions in postoperative care.", "key_contributions": ["Introduction of FollowUpBot for dynamic patient interaction", "Use of edge-deployed LLMs for adaptive communication", "Automated generation of structured follow-up reports"], "limitations": "", "keywords": ["postoperative care", "LLM", "edge-deployed", "robot", "healthcare"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.14819", "pdf": "https://arxiv.org/pdf/2507.14819.pdf", "abs": "https://arxiv.org/abs/2507.14819", "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.", "AI": {"tldr": "The paper proposes a framework for intent-based chart generation from long documents using LLMs, addressing the challenges of transforming user intents into visual representations.", "motivation": "Existing methods for chart generation from text descriptions are limited and do not effectively handle user-defined intents based on long documents.", "method": "An unsupervised, two-stage framework where an LLM extracts relevant information from documents based on user intent, followed by heuristic-guided chart type selection and code generation.", "result": "The proposed approach curated a dataset of 1,242 <intent, document, charts> tuples and demonstrated superior performance, with improvements of up to 9 points in data accuracy and 17 points in chart type selection over existing methods.", "conclusion": "The framework effectively generates charts from documents based on user intent, outperforming traditional methods in accuracy and relevance to user needs.", "key_contributions": ["Introduction of intent-based chart generation from documents", "Development of a heuristic-guided module for chart type selection", "Proposal of an attribution-based metric for assessing chart data accuracy"], "limitations": "", "keywords": ["chart generation", "LLMs", "intent-based visualization", "HCI", "data accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.15526", "pdf": "https://arxiv.org/pdf/2507.15526.pdf", "abs": "https://arxiv.org/abs/2507.15526", "title": "Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey", "authors": ["Antonio Perez", "Avinash Singh", "Jonathan Mitchell", "Philip Swadling"], "categories": ["cs.HC", "H.5.1; H.1.2; I.3.6"], "comment": "14 pages, 3 figures", "summary": "Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative\nto traditional Flight Simulator Training Device (FSTD) displays, providing\nimmersion, realism and cost efficiency. However, these technologies require\nmanagement of human factors; cybersickness, visual fatigue and ergonomic\nstrain. If left unmitigated, these effects can hinder pilot performance and\ntraining outcomes. For safety critical fields like aviation, addressing human\nfactors challenges is crucial for MR's training potential. This survey\nsystematically reviews the current literature identifying key human factors\nchallenges in MR HMD use in pilot training and examines strategies to mitigate\nthese barriers. Drawing on existing industry standards set by a leading\naviation authority, the review adopts a regulatory perspective to explore\nhardware, software, ergonomic, physiological and psychological interventions\nimproving pilot comfort, safety and training effectiveness in an MR FSTD.\nAdditionally, it evaluates which of these interventions are most appropriate\nand viable for MR pilot training under existing aviation training regulations,\nensuring that technical requirements and pilot wellbeing remain balanced. The\nfindings yield significant insights for the human dimensions of aviation\nsimulation training, highlighting how regulatory considerations shape the\npracticality of mitigation measures. These insights inform emerging MR aviation\ntraining guidelines and best practices, supporting MR's readiness to enhance\naviation training.", "AI": {"tldr": "This paper reviews human factors challenges related to Mixed Reality (MR) head mounted displays in pilot training, exploring strategies to mitigate issues like cybersickness, visual fatigue, and ergonomic strain.", "motivation": "To address the human factors challenges that can hinder pilot performance and training outcomes when using MR technologies in aviation.", "method": "A systematic literature review that identifies key human factors issues in MR HMD use, assesses mitigation strategies, and adopts a regulatory perspective informed by industry standards.", "result": "The findings reveal critical insights into the human dimensions of aviation simulation training, focusing on comfort, safety, and training effectiveness, while balancing technical requirements with pilot wellbeing.", "conclusion": "The review supports the development of MR aviation training guidelines and best practices, emphasizing the need for effective mitigation of human factors challenges to enhance pilot training outcomes.", "key_contributions": ["Systematic identification of human factors challenges in MR pilot training.", "Evaluation of mitigation strategies informed by regulatory frameworks.", "Insights into the impact of human factors on training effectiveness and pilot wellbeing."], "limitations": "", "keywords": ["Mixed Reality", "pilot training", "human factors", "cybersickness", "aviation"], "importance_score": 6, "read_time_minutes": 14}}
{"id": "2507.14849", "pdf": "https://arxiv.org/pdf/2507.14849.pdf", "abs": "https://arxiv.org/abs/2507.14849", "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "authors": ["Yifei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.", "AI": {"tldr": "The paper investigates the effects of reasoning distillation on language models' long-context comprehension and retrieval capabilities.", "motivation": "To explore the unexplored impact of large-scale reasoning distillation on in-context retrieval and reasoning, particularly in the context of Retrieval-Augmented Generation (RAG) systems.", "method": "The study evaluates open-source models distilled from Deepseek-R1, focusing on their performance in multi-document question answering tasks to assess long-context awareness and information extraction.", "result": "Distilled models exhibit significantly improved capabilities in understanding long contexts, fostering detailed reasoning processes and addressing the 'lost in the middle' issue.", "conclusion": "Reasoning distillation enhances the long-context understanding of language models, promoting effective contextual information utilization.", "key_contributions": ["Demonstration of the impact of reasoning distillation on long-context comprehension.", "Insights into mitigating the 'lost in the middle' problem in language models.", "Performance evaluation of distilled models on multi-document question answering tasks."], "limitations": "", "keywords": ["reasoning distillation", "long-context comprehension", "Retrieval-Augmented Generation", "language models", "multi-document QA"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.15559", "pdf": "https://arxiv.org/pdf/2507.15559.pdf", "abs": "https://arxiv.org/abs/2507.15559", "title": "FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold", "authors": ["Pan Hao", "Dongyeop Kang", "Nicholas Hinds", "Qianwen Wang"], "categories": ["cs.HC"], "comment": "9 pages, 10 figures, accepted by IEEE VIS 2025", "summary": "Multi-agent workflows have become an effective strategy for tackling\ncomplicated tasks by decomposing them into multiple sub-tasks and assigning\nthem to specialized agents. However, designing optimal workflows remains\nchallenging due to the vast and intricate design space. Current practices rely\nheavily on the intuition and expertise of practitioners, often resulting in\ndesign fixation or an unstructured, time-consuming exploration of\ntrial-and-error. To address these challenges, this work introduces FLOWFORGE,\nan interactive visualization tool to facilitate the creation of multi-agent\nworkflow through i) a structured visual exploration of the design space and ii)\nin-situ guidance informed by established design patterns. Based on formative\nstudies and literature review, FLOWFORGE organizes the workflow design process\ninto three hierarchical levels (i.e., task planning, agent assignment, and\nagent optimization), ranging from abstract to concrete. This structured visual\nexploration enables users to seamlessly move from high-level planning to\ndetailed design decisions and implementations, while comparing alternative\nsolutions across multiple performance metrics. Additionally, drawing from\nestablished workflow design patterns, FLOWFORGE provides context-aware, in-situ\nsuggestions at each level as users navigate the design space, enhancing the\nworkflow creation process with practical guidance. Use cases and user studies\ndemonstrate the usability and effectiveness of FLOWFORGE, while also yielding\nvaluable insights into how practitioners explore design spaces and leverage\nguidance during workflow development.", "AI": {"tldr": "FLOWFORGE is an interactive visualization tool designed to aid in the creation of multi-agent workflows through structured visual exploration and context-aware guidance.", "motivation": "To overcome challenges in designing optimal multi-agent workflows due to a vast design space and reliance on practitioners' intuition, which can lead to inefficient exploration methods.", "method": "FLOWFORGE organizes the workflow design process into three hierarchical levels: task planning, agent assignment, and agent optimization, enabling users to navigate the design space effectively and compare solutions across metrics.", "result": "User studies showed that FLOWFORGE improved usability and effectiveness in workflow creation, providing insights into practitioners' exploration and guidance use.", "conclusion": "The structured and guided approach of FLOWFORGE enhances the multi-agent workflow design process, making it more efficient and informed.", "key_contributions": ["Introduction of a structured visual exploration tool for multi-agent workflows.", "In-situ guidance based on established design patterns during the workflow creation process.", "Organization of the design process into three hierarchical levels for better usability."], "limitations": ".", "keywords": ["multi-agent workflows", "visualization tool", "interactive design", "workflow optimization", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.14871", "pdf": "https://arxiv.org/pdf/2507.14871.pdf", "abs": "https://arxiv.org/abs/2507.14871", "title": "Tiny language models", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "categories": ["cs.CL"], "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language.", "AI": {"tldr": "This study investigates tiny language models (TLMs) and their pre-training effectiveness compared to large language models (LLMs), showing significant performance improvements with pre-training even on a small scale.", "motivation": "The need for more accessible alternatives to large language models due to the high computational resources required for LLM pre-training, limiting research opportunities.", "method": "The study involved pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on classification tasks like FewRel, AGNews, and DBPedia.", "result": "TLMs show a performance gap between pre-trained and non-pre-trained models across classification tasks. The performance gap increases with the size of the pre-training dataset and the overlap between token sets.", "conclusion": "Tiny language models can achieve classification accuracy comparable to larger models when pre-trained, suggesting they can facilitate NLP research in a more accessible manner.", "key_contributions": ["Demonstrated TLMs can achieve significant classification performance with pre-training.", "Showed that a soft committee of shallow architectures can match accuracy of a deep TLM.", "Identified the importance of dataset size and token overlap for model performance."], "limitations": "The study focuses on specific classification tasks and datasets, which may limit the generalizability of the results.", "keywords": ["tiny language models", "pre-training", "natural language processing", "classification", "neural networks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.15650", "pdf": "https://arxiv.org/pdf/2507.15650.pdf", "abs": "https://arxiv.org/abs/2507.15650", "title": "Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback", "authors": ["Gerben van der Hoek", "Bastiaan Heeren", "Rogier Bos", "Paul Drijvers", "Johan Jeuring"], "categories": ["cs.HC"], "comment": null, "summary": "Computer aided formative assessment can be used to enhance a learning\nprocess, for instance by providing feedback. There are many design choices for\ndelivering feedback, that lead to a feedback strategy. In an informative\nfeedback strategy, students do not immediately receive information about the\ncorrect response, but are offered the opportunity to retry a task to apply\nfeedback information. In this small-scale qualitative study, we explore an\ninformative feedback strategy designed to offer a balance between room for\nexploration and mitigation of learning barriers. The research questions concern\nthe ways in which students interact with the feedback strategy and their\nappreciation of error-specific feedback as opposed to worked-out solutions. To\nanswer these questions, twenty-five 15-to-17-year-old senior general secondary\neducation students worked for approximately 20 minutes on linear and\nexponential extrapolation tasks in an online environment. Data included screen\ncaptures of students working with the environment and post-intervention\ninterviews. Results showed that room for exploration offered opportunities for\nself-guidance while mitigation of learning barriers prevented disengagement.\nFurthermore, students appreciated balanced feedback. We conclude that the\nbalanced feedback strategy yielded fruitful student-environment interactions.", "AI": {"tldr": "The study explores an informative feedback strategy in computer-aided formative assessment, enhancing student learning through balanced feedback.", "motivation": "To improve student interactions and learning outcomes in a computer-aided formative assessment environment by examining feedback strategies.", "method": "A qualitative study involving 25 senior secondary education students who completed tasks in an online environment, accompanied by screen captures and post-intervention interviews.", "result": "Students found that the informative feedback strategy allowed for self-guidance while preventing disengagement, appreciating the balance between exploration and structured feedback.", "conclusion": "The balanced feedback strategy resulted in positive student-environment interactions and improved learning engagement.", "key_contributions": ["Investigation of informative feedback strategies in educational environments", "Insights into student appreciation of error-specific feedback", "Demonstration of the balance needed between exploration and structure in feedback"], "limitations": "", "keywords": ["feedback strategy", "formative assessment", "student interaction"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.14887", "pdf": "https://arxiv.org/pdf/2507.14887.pdf", "abs": "https://arxiv.org/abs/2507.14887", "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "categories": ["cs.CL"], "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.", "AI": {"tldr": "The paper introduces MEKiT, a novel method for improving large language models' performance on the Emotion-Cause Pair Extraction task by integrating emotional and causal knowledge.", "motivation": "LLMs struggle with the ECPE task due to the lack of auxiliary knowledge, which hampers their reasoning abilities.", "method": "MEKiT integrates heterogeneous internal emotional knowledge and external causal knowledge, using instruction templates and mixed data for instruction-tuning.", "result": "Experimental results show MEKiT significantly enhances LLM performance on the ECPE task compared to baseline models.", "conclusion": "MEKiT serves as an effective solution for improving LLMs in tasks requiring emotional and causal reasoning.", "key_contributions": ["Introduction of the MEKiT method for ECPE task enhancement", "Integration of emotional and causal knowledge", "Demonstrated improved performance through experimental results"], "limitations": "", "keywords": ["Emotion-Cause Pair Extraction", "Large Language Models", "Knowledge Injection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15692", "pdf": "https://arxiv.org/pdf/2507.15692.pdf", "abs": "https://arxiv.org/abs/2507.15692", "title": "Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions", "authors": ["Meng Chen", "Akhil Iyer", "Amy Pavel"], "categories": ["cs.HC"], "comment": "18 pages, 6 figures", "summary": "Multimodal large language models (MLLMs) provide new opportunities for blind\nand low vision (BLV) people to access visual information in their daily lives.\nHowever, these models often produce errors that are difficult to detect without\nsight, posing safety and social risks in scenarios from medication\nidentification to outfit selection. While BLV MLLM users use creative\nworkarounds such as cross-checking between tools and consulting sighted\nindividuals, these approaches are often time-consuming and impractical. We\nexplore how systematically surfacing variations across multiple MLLM responses\ncan support BLV users to detect unreliable information without visually\ninspecting the image. We contribute a design space for eliciting and presenting\nvariations in MLLM descriptions, a prototype system implementing three\nvariation presentation styles, and findings from a user study with 15 BLV\nparticipants. Our results demonstrate that presenting variations significantly\nincreases users' ability to identify unreliable claims (by 4.9x using our\napproach compared to single descriptions) and significantly decreases perceived\nreliability of MLLM responses. 14 of 15 participants preferred seeing\nvariations of MLLM responses over a single description, and all expressed\ninterest in using our system for tasks from understanding a tornado's path to\nposting an image on social media.", "AI": {"tldr": "This paper explores how to enhance the accessibility of multimodal large language models for blind and low vision users by presenting variations in model responses.", "motivation": "The need to improve accessibility and reliability of visual information for blind and low vision (BLV) individuals using multimodal large language models (MLLMs).", "method": "Design space for presenting variations in MLLM responses, implementation of a prototype system with three presentation styles, and a user study with 15 BLV participants.", "result": "The study found that presenting variations increased the ability of users to identify unreliable claims by 4.9 times compared to using single descriptions, and all participants preferred this method over standard responses.", "conclusion": "Systematically surfacing variations in MLLM outputs significantly aids BLV users in detecting unreliable information and increases their engagement with the technology.", "key_contributions": ["Design space for eliciting variations in MLLM descriptions", "Prototype system with multiple variation presentation styles", "User study demonstrating the effectiveness of variations for BLV users"], "limitations": "The study is based on a limited sample of 15 participants, which may not fully represent the wider BLV community.", "keywords": ["multimodal large language models", "blind and low vision", "user study", "reliability", "information accessibility"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2507.14894", "pdf": "https://arxiv.org/pdf/2507.14894.pdf", "abs": "https://arxiv.org/abs/2507.14894", "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities.", "AI": {"tldr": "This paper addresses unexpected code-switching in large language models (LLMs) and introduces a new fine-tuning method to mitigate this issue while preserving multilingual performance.", "motivation": "Unexpected code-switching in LLMs leads to poor readability and usability, but previous efforts lack depth in analysis and effectiveness.", "method": "The authors employ sparse autoencoders to analyze linguistic features causing code-switching and propose Sparse Autoencoder-guided Supervised Fine-tuning (SASFT) to control pre-activation values during training.", "result": "SASFT reduces unexpected code-switching by over 50% across five models and three languages, achieving complete elimination in four instances, while maintaining or improving performance on multilingual benchmarks.", "conclusion": "The proposed method effectively addresses the code-switching issue in LLMs without deteriorating their multilingual capabilities, showcasing promise for future improvements in model usability.", "key_contributions": ["In-depth mechanistic analysis of code-switching using sparse autoencoders.", "Introduction of SASFT to control language feature activation during training.", "Demonstration of significant reductions in code-switching with maintained model performance."], "limitations": "Focuses mainly on reducing code-switching; effectiveness in broader contexts not fully tested.", "keywords": ["Large Language Models", "code-switching", "multilingual capabilities", "fine-tuning", "sparse autoencoders"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15783", "pdf": "https://arxiv.org/pdf/2507.15783.pdf", "abs": "https://arxiv.org/abs/2507.15783", "title": "Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance", "authors": ["Mohammad 'Matt' Namvarpour", "Brandon Brofsky", "Jessica Medina", "Mamtaj Akter", "Afsaneh Razi"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As Generative Artificial Intelligence (GenAI) driven chatbots like\nCharacter.AI become embedded in adolescent life, they raise concerns about\nemotional dependence and digital overreliance. While studies have investigated\nthe overreliance of adults on these chatbots, they have not investigated teens'\ninteractions with chatbots with customizable personas. We analyzed 318 Reddit\nposts made by users self-reported as 13-17 years old on the Character.AI\nsubreddit to understand patterns of overreliance. We found teens commonly begin\nusing chatbots for emotional support or creative expression, but many develop\nstrong attachments that interfere with offline relationships and daily\nroutines. Their posts revealed recurring signs of psychological distress,\ncycles of relapse, and difficulty disengaging. Teens reported that their\noverreliance often ended when they reflect on the harm, return to in-person\nsocial settings, or become frustrated by platform restrictions. Based on the\nimplications of our findings, we provide recommendations for future chatbot\ndesign so they can promote self-awareness, support real-world engagement, and\ninvolve teens in developing safer digital tools.", "AI": {"tldr": "This study analyzes the interactions of adolescents with AI chatbots, revealing patterns of emotional dependence and overreliance that can disrupt offline relationships.", "motivation": "To understand the unique dynamics of adolescent interactions with customizable AI chatbots, particularly in relation to emotional support and digital dependency.", "method": "Analyzed 318 Reddit posts from users aged 13-17 on the Character.AI subreddit to identify patterns of chatbot usage and overreliance.", "result": "Findings indicate that while teens use chatbots for emotional support and creative expression, many develop attachments that hinder their offline relationships and routines, showing signs of psychological distress.", "conclusion": "The research highlights the need for chatbot design improvements to enhance self-awareness and promote real-world engagement among teens.", "key_contributions": ["Identifies patterns of emotional dependence in adolescents using AI chatbots.", "Reveals psychological distress linked to overreliance on chatbots.", "Provides recommendations for safer chatbot design involving teen feedback."], "limitations": "", "keywords": ["Generative AI", "AI chatbots", "adolescents", "emotional dependence", "digital overreliance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.14900", "pdf": "https://arxiv.org/pdf/2507.14900.pdf", "abs": "https://arxiv.org/abs/2507.14900", "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs.", "AI": {"tldr": "The paper introduces NeuronXA, a methodology for evaluating the cross-lingual alignment of large language models using a neuron state-based approach, demonstrating high correlation with performance metrics even with limited data.", "motivation": "To address gaps in assessing cross-lingual alignment in LLMs, particularly in low-resource languages, where existing benchmarks may not adequately reflect semantic alignment.", "method": "Neuron State-Based Cross-Lingual Alignment (NeuronXA) is proposed, inspired by neuroscientific findings, to evaluate cross-lingual capabilities of multilingual LLMs across five models (LLaMA, Qwen, Mistral, GLM, OLMo) on multiple tasks and benchmarks.", "result": "NeuronXA achieves a Pearson correlation of 0.9556 with downstream task performance and 0.8514 with transferability using only 100 parallel sentence pairs, indicating strong effectiveness in alignment evaluation.", "conclusion": "NeuronXA not only demonstrates effectiveness in assessing cross-lingual alignment and transferability but also stands to significantly enhance the field of cross-lingual alignment research.", "key_contributions": ["Introduction of NeuronXA for evaluating cross-lingual alignment in LLMs", "High correlation results with small dataset for downstream tasks", "Potential to improve semantic understanding in multilingual models"], "limitations": "", "keywords": ["cross-lingual alignment", "large language models", "neuron state-based evaluation", "multilingual", "evaluation methodologies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.14913", "pdf": "https://arxiv.org/pdf/2507.14913.pdf", "abs": "https://arxiv.org/abs/2507.14913", "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/", "AI": {"tldr": "PromptSuite is a framework for automatically generating prompt variations for improved evaluation of LLMs.", "motivation": "There's a need for more reliable evaluation of LLMs, as small changes in prompts can lead to large performance differences.", "method": "PromptSuite enables the automatic generation of diverse prompts through a modular design, allowing controlled variations and extensibility.", "result": "Case studies demonstrate that PromptSuite can generate meaningful prompt variations for robust evaluation practices.", "conclusion": "PromptSuite supports better evaluation of LLMs and is accessible via a Python API and a web interface.", "key_contributions": ["Automatic generation of prompt variations for LLM evaluation", "Modular and flexible prompt design", "User-friendly interface for non-expert users"], "limitations": "", "keywords": ["Large Language Models", "Prompt Engineering", "Evaluation Framework"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.14922", "pdf": "https://arxiv.org/pdf/2507.14922.pdf", "abs": "https://arxiv.org/abs/2507.14922", "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs", "authors": ["Vahid Rahimzadeh", "Erfan Moosavi Monazzah", "Mohammad Taher Pilehvar", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Persona-driven LLMs have emerged as powerful tools in computational social\nscience, yet existing approaches fall at opposite extremes, either relying on\ncostly human-curated data or producing synthetic personas that lack consistency\nand realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from\n10,000 real social media users from BlueSky open platform across three time\nwindows, bridging this spectrum by grounding synthetic generation in authentic\nuser activity. Our evaluation demonstrates that SYNTHIA achieves competitive\nperformance with state-of-the-art methods in demographic diversity and social\nsurvey alignment while significantly outperforming them in narrative\nconsistency. Uniquely, SYNTHIA incorporates temporal dimensionality and\nprovides rich social interaction metadata from the underlying network, enabling\nnew research directions in computational social science and persona-driven\nlanguage modeling.", "AI": {"tldr": "SYNTHIA is a novel dataset of 30,000 synthetic backstories based on real social media users, enhancing persona-driven LLMs in social science by ensuring narrative consistency and incorporating temporal and interaction metadata.", "motivation": "To improve persona-driven language models in computational social science by grounding synthetic data in authentic user activity while ensuring consistency and realism.", "method": "The authors created a dataset called SYNTHIA, comprising 30,000 backstories sourced from 10,000 real BlueSky social media users across three time periods, integrating temporal aspects and social interaction data.", "result": "SYNTHIA shows competitive performance in demographic diversity and social survey alignment when compared to state-of-the-art methods, significantly excelling in narrative consistency.", "conclusion": "The incorporation of real user activity and temporal dimensionality in SYNTHIA facilitates new research avenues in computational social science and enhances persona-driven language models.", "key_contributions": ["Introduction of SYNTHIA dataset bridging real user data and synthetic persona generation", "Improvement in narrative consistency in persona-driven LLMs", "Inclusion of temporal and social interaction metadata in dataset"], "limitations": "", "keywords": ["persona-driven LLMs", "SYNTHIA", "computational social science", "narrative consistency", "social interaction metadata"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.14958", "pdf": "https://arxiv.org/pdf/2507.14958.pdf", "abs": "https://arxiv.org/abs/2507.14958", "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models", "authors": ["Hang Yan", "Fangzhi Xu", "Rongman Xu", "Yifei Li", "Jian Zhang", "Haoran Luo", "Xiaobao Wu", "Luu Anh Tuan", "Haiteng Zhao", "Qika Lin", "Jun Liu"], "categories": ["cs.CL"], "comment": "25 pages, 8 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.", "AI": {"tldr": "This paper introduces Momentum Uncertainty-guided Reasoning (MUR), an efficient technique for guiding test-time scaling in Large Language Models (LLMs) without extra training, significantly improving reasoning efficiency and accuracy.", "motivation": "To address the challenge of optimizing reasoning efficiency in LLMs, particularly when using Test-Time Scaling (TTS), which can lead to inefficiencies like overthinking and redundant computations.", "method": "MUR utilizes a momentum-inspired approach to dynamically adjust reasoning budgets by tracking and aggregating stepwise uncertainty at each reasoning step. It incorporates a single hyperparameter for flexible gamma-control of reasoning budgets.", "result": "MUR achieves over 50% reduction in computation and improves accuracy by 0.62-3.37% compared to various TTS methods across multiple benchmarks.", "conclusion": "MUR demonstrates significant improvements in both efficiency and accuracy for LLMs during test-time reasoning, proving its effectiveness over existing methods.", "key_contributions": ["Introduction of Momentum Uncertainty-guided Reasoning (MUR) method", "Implementation of a gamma-control mechanism for dynamic budget allocation", "Theoretical proof demonstrating MUR's stability and reduction of biases."], "limitations": "", "keywords": ["Large Language Models", "Test-Time Scaling", "Momentum Uncertainty", "Reasoning Efficiency", "Artificial Intelligence"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2507.15024", "pdf": "https://arxiv.org/pdf/2507.15024.pdf", "abs": "https://arxiv.org/abs/2507.15024", "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback", "authors": ["Qiaoyu Tang", "Hao Xiang", "Le Yu", "Bowen Yu", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning.", "AI": {"tldr": "The paper introduces RefCritic, a critic module utilizing reinforcement learning to enhance the critique abilities of Large Language Models beyond existing supervised fine-tuning methods.", "motivation": "As Large Language Models advance, there is an increasing need for effective critic modules to provide precise guidance and improve model critique capabilities.", "method": "RefCritic employs a long-chain-of-thought approach with reinforcement learning, incorporating dual rule-based rewards to assess both solution judgments and refinement accuracies based on critiques.", "result": "RefCritic shows significant performance improvements on critique and refinement tasks, with gains of 6.8% and 7.2% on AIME25 for tested models, and it excels in majority voting scenarios.", "conclusion": "The results demonstrate that RefCritic outperforms traditional step-level supervision methods, suggesting a more effective approach to enhancing model critiques and refinements.", "key_contributions": ["Introduction of RefCritic as a novel critic module", "Utilization of reinforcement learning with dual rule-based rewards", "Demonstrated significant performance gains across multiple benchmarks"], "limitations": "", "keywords": ["Large Language Models", "critic module", "reinforcement learning", "model evaluation", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.15061", "pdf": "https://arxiv.org/pdf/2507.15061.pdf", "abs": "https://arxiv.org/abs/2507.15061", "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization", "authors": ["Zhengwei Tao", "Jialong Wu", "Wenbiao Yin", "Junkai Zhang", "Baixuan Li", "Haiyang Shen", "Kuan Li", "Liwen Zhang", "Xinyu Wang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.", "AI": {"tldr": "The paper presents WebShaper, a framework for synthesizing datasets for information-seeking tasks using formalization and Knowledge Projections to enhance the performance of LLM agents.", "motivation": "The scarcity of high-quality training data limits the development of information-seeking (IS) agents, prompting the need for better synthesis methods that ensure consistency in reasoning structure and question-answer pairs.", "method": "WebShaper utilizes set theory to formalize IS tasks, employing the concept of Knowledge Projections to control reasoning structures and applying a multi-step expansion process to synthesize data.", "result": "WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks, outperforming existing IS agents.", "conclusion": "The proposed framework effectively bridges the gap between information structure and reasoning structure, contributing to advancements in the capabilities of LLM-powered IS agents.", "key_contributions": ["Introduction of the WebShaper framework for dataset synthesis in IS tasks.", "Formalization of IS tasks through set theory and Knowledge Projections.", "Demonstrated state-of-the-art performance on benchmark datasets."], "limitations": "", "keywords": ["Large Language Models", "Information Seeking", "Dataset Synthesis", "Knowledge Projections", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372.pdf", "abs": "https://arxiv.org/abs/2507.14372", "title": "Text-to-SQL for Enterprise Data Analytics", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "The paper discusses the development of an internal chatbot at LinkedIn that allows teams to self-serve data insights from a large database using a Text-to-SQL agent and a knowledge graph.", "motivation": "To improve access to data insights for LinkedIn's product management, engineering, and operations teams from a vast data lake.", "method": "The approach involves creating a knowledge graph from various sources, developing a Text-to-SQL agent for querying, and designing an interactive chatbot that supports diverse user needs in data interaction.", "result": "The chatbot serves over 300 weekly users and boasts a 53% accuracy rate on an internal benchmarking test for response correctness. Ablation studies highlighted key components of the knowledge graph and modeling.", "conclusion": "This work offers insights into building effective enterprise Text-to-SQL solutions that enhance data accessibility and user engagement.", "key_contributions": ["Development of a knowledge graph for enhanced data semantics", "Creation of a Text-to-SQL agent that corrects errors", "Design of an interactive chatbot for rich user engagement"], "limitations": "The accuracy of the chatbot responses is limited, achieving only 53% correctness on an internal benchmark.", "keywords": ["Text-to-SQL", "chatbot", "knowledge graph", "natural language processing", "data insights"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.15087", "pdf": "https://arxiv.org/pdf/2507.15087.pdf", "abs": "https://arxiv.org/abs/2507.15087", "title": "Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling", "authors": ["Chenlei Gong", "Yuanhe Tian", "Lei Mao", "Yan Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Currently, many studies view DNA sequences as a special type of language and\nutilize Transformers to model them. These studies use fixed-length k-mer\nsegmentation and BPE subword tokenization but lack a systematic evaluation to\ndetermine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a\n4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,\nAliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and\n24-layer Transformer encoders and evaluated on GUE benchmark dataset. In\ngeneral, BPE delivers higher and more stable performance across tasks by\ncompressing frequent motifs into variable-length tokens, reducing sequence\nlength, and improving model generalization. RoPE excels at capturing periodic\nmotifs and extrapolating to long sequences, while AliBi also performs well on\ntasks driven by local dependencies. In terms of depth, we observe significant\ngains when increasing layers from 3 to 12, with only marginal improvements or\nslight overfitting at 24 layers. This study provides practical guidance for\ndesigning tokenization and positional encoding in DNA Transformer models.", "AI": {"tldr": "The study compares k-mer segmentation and BPE tokenization for DNA sequence modeling using Transformers, revealing BPE's superior and stable performance.", "motivation": "To systematically evaluate different tokenization methods and positional encoding in modeling DNA sequences using Transformers.", "method": "We compare k-mer segmentation with various lengths and a BPE vocabulary across different Transformer layer configurations on the GUE benchmark dataset.", "result": "BPE shows higher and more stable performance across tasks; RoPE effectively captures periodic motifs, and increasing Transformer layers improves performance up to 12 layers.", "conclusion": "These findings offer practical guidance for optimizing tokenization and encoding choices in DNA Transformer models.", "key_contributions": ["Comparison of k-mer segmentation and BPE in DNA modeling", "Evaluation of tokenization and positional encoding methods", "Insights on Transformer layer depth effects on performance"], "limitations": "", "keywords": ["DNA sequencing", "Transformers", "k-mer segmentation", "BPE tokenization", "positional encoding"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.15092", "pdf": "https://arxiv.org/pdf/2507.15092.pdf", "abs": "https://arxiv.org/abs/2507.15092", "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations", "authors": ["Vijeta Deshpande", "Ishita Dasgupta", "Uttaran Bhattacharya", "Somdeb Sarkhel", "Saayan Mitra", "Anna Rumshisky"], "categories": ["cs.CL"], "comment": null, "summary": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$.", "AI": {"tldr": "Proposes a new metric, PATTR, for evaluating lexical diversity in synthesized text, addressing biases introduced by text length variations.", "motivation": "To improve the effectiveness of synthetic data generated by LLMs by measuring lexical diversity more accurately, especially in creative writing tasks.", "method": "Introduced the Penalty-Adjusted Type-Token Ratio (PATTR) metric and evaluated it against existing metrics by generating a synthetic corpus of over 20M words in a video script generation creative writing context.", "result": "PATTR consistently outperforms traditional metrics (MATTR and CR) in measuring lexical diversity while effectively mitigating length biases.", "conclusion": "PATTR provides a more reliable measure for evaluating diversity in generated text samples, crucial for enhancing LLM training with synthetic data.", "key_contributions": ["Introduction of the Penalty-Adjusted Type-Token Ratio (PATTR) diversity metric", "Evaluation of PATTR against traditional diversity metrics", "Demonstration of its practical utility in filtering diverse responses from LLM outputs"], "limitations": "The study primarily focuses on a specific creative writing task, which may limit generalizability to other tasks and contexts.", "keywords": ["Large Language Models", "lexical diversity", "synthetic data", "PATTR", "creative writing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15100", "pdf": "https://arxiv.org/pdf/2507.15100.pdf", "abs": "https://arxiv.org/abs/2507.15100", "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?", "authors": ["Chathuri Jayaweera", "Brianna Yanqui", "Bonnie Dorr"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures and 5 tables", "summary": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences.", "AI": {"tldr": "This study investigates the use of Large Language Models (LLMs) for generating commonsense knowledge in the task of Natural Language Inference (NLI).", "motivation": "The paper addresses the insufficient coverage of existing commonsense resources for NLI, emphasizing the need for systems that can emulate human inferential processes incorporating commonsense knowledge.", "method": "The authors adapt existing metrics to assess LLMs for factuality and consistency in generating commonsense knowledge, and evaluate the impact on NLI prediction accuracy.", "result": "The findings indicate that while incorporating commonsense knowledge does not consistently enhance overall predictive performance, it aids in differentiating entailing instances and shows moderate improvement in defining contradictions and neutral inferences.", "conclusion": "The study concludes that LLMs can serve as valuable commonsense knowledge sources for NLI tasks, despite the variability in their overall effectiveness.", "key_contributions": ["Exploration of LLMs as commonsense knowledge generators for NLI.", "Adaptation of metrics for evaluating LLM factuality and consistency.", "Insights into the nuanced role of commonsense knowledge in improving inference differentiation."], "limitations": "The effectiveness of commonsense knowledge incorporation varies and does not always lead to improved results in NLI tasks.", "keywords": ["Natural Language Inference", "commonsense knowledge", "Large Language Models", "factuality", "consistency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15114", "pdf": "https://arxiv.org/pdf/2507.15114.pdf", "abs": "https://arxiv.org/abs/2507.15114", "title": "From Disagreement to Understanding: The Case for Ambiguity Detection in NLI", "authors": ["Chathuri Jayaweera", "Bonnie Dorr"], "categories": ["cs.CL"], "comment": "8 pages, 6 figures", "summary": "This position paper argues that annotation disagreement in Natural Language\nInference (NLI) is not mere noise but often reflects meaningful interpretive\nvariation, especially when triggered by ambiguity in the premise or hypothesis.\nWhile underspecified guidelines and annotator behavior can contribute to\nvariation, content-based ambiguity offers a process-independent signal of\ndivergent human perspectives. We call for a shift toward ambiguity-aware NLI by\nsystematically identifying ambiguous input pairs and classifying ambiguity\ntypes. To support this, we present a unified framework that integrates existing\ntaxonomies and illustrate key ambiguity subtypes through concrete examples.\nThese examples reveal how ambiguity shapes annotator decisions and motivate the\nneed for targeted detection methods that better align models with human\ninterpretation. A key limitation is the lack of datasets annotated for\nambiguity and subtypes. We propose addressing this gap through new annotated\nresources and unsupervised approaches to ambiguity detection -- paving the way\nfor more robust, explainable, and human-aligned NLI systems.", "AI": {"tldr": "This paper discusses the role of annotation disagreement in Natural Language Inference (NLI) due to content-based ambiguity, advocating for an ambiguity-aware approach in NLI. It presents a framework to identify and classify types of ambiguity, highlighting the need for new annotated datasets.", "motivation": "The paper argues that annotation disagreement in NLI often indicates meaningful interpretive variation rather than noise, influenced by ambiguities in the premise or hypothesis.", "method": "The authors propose a unified framework for identifying ambiguous input pairs and classifying ambiguity types, integrating existing taxonomies.", "result": "Key ambiguity subtypes are illustrated with examples showing how they impact annotator decisions, emphasizing the necessity for methods that align models with human interpretation.", "conclusion": "The authors suggest creating new annotated datasets for ambiguity and developing unsupervised methods for detection to improve NLI systems.", "key_contributions": ["Proposing an ambiguity-aware approach to NLI", "Presenting a unified framework for classifying ambiguity", "Highlighting the need for annotated datasets for ambiguity types"], "limitations": "The main limitation is the absence of datasets annotated for ambiguity and its subtypes.", "keywords": ["Natural Language Inference", "annotation disagreement", "ambiguity", "NLI systems", "human interpretation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.15142", "pdf": "https://arxiv.org/pdf/2507.15142.pdf", "abs": "https://arxiv.org/abs/2507.15142", "title": "A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script", "authors": ["Hellina Hailu Nigatu", "Atnafu Lambebo Tonja", "Henok Biadglign Ademtew", "Hizkel Mitiku Alemayehu", "Negasi Haile Abadi", "Tadesse Destaw Belay", "Seid Muhie Yimam"], "categories": ["cs.CL", "cs.AI"], "comment": "Paper under review", "summary": "Homophone normalization, where characters that have the same sound in a\nwriting script are mapped to one character, is a pre-processing step applied in\nAmharic Natural Language Processing (NLP) literature. While this may improve\nperformance reported by automatic metrics, it also results in models that are\nnot able to understand different forms of writing in a single language.\nFurther, there might be impacts in transfer learning, where models trained on\nnormalized data do not generalize well to other languages. In this paper, we\nexperiment with monolingual training and cross-lingual transfer to understand\nthe impacts of normalization on languages that use the Ge'ez script. We then\npropose a post-inference intervention in which normalization is applied to\nmodel predictions instead of training data. With our simple scheme of\npost-inference normalization, we show that we can achieve an increase in BLEU\nscore of up to 1.03 while preserving language features in training. Our work\ncontributes to the broader discussion on technology-facilitated language change\nand calls for more language-aware interventions.", "AI": {"tldr": "This paper investigates the effects of homophone normalization in Amharic NLP, proposing a post-inference method that improves BLEU scores while preserving linguistic features.", "motivation": "To address the limitations of homophone normalization in NLP, which can hinder model performance and generalization in multilingual contexts.", "method": "The study experiments with monolingual training and cross-lingual transfer using normalized and non-normalized datasets, introducing a post-inference normalization technique applied to model predictions.", "result": "The proposed technique yields an increase in BLEU score of up to 1.03, demonstrating improved model performance without losing language characteristics in training.", "conclusion": "The work advocates for more nuanced approaches to language normalization, emphasizing the importance of technology-aware language practices that consider linguistic diversity.", "key_contributions": ["Introduces a post-inference normalization approach.", "Shows improved BLEU scores with minimal loss of language features.", "Stimulates discussion on language-aware interventions in NLP."], "limitations": "The effectiveness of the proposed method may vary across different languages and scripts; further validation needed.", "keywords": ["Homophone normalization", "Amharic NLP", "Cross-lingual transfer", "Post-inference normalization", "Language features"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.15152", "pdf": "https://arxiv.org/pdf/2507.15152.pdf", "abs": "https://arxiv.org/abs/2507.15152", "title": "What Level of Automation is \"Good Enough\"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automating data extraction from full-text randomised controlled trials (RCTs)\nfor meta-analysis remains a significant challenge. This study evaluates the\npractical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)\nacross tasks involving statistical results, risk-of-bias assessments, and\nstudy-level characteristics in three medical domains: hypertension, diabetes,\nand orthopaedics. We tested four distinct prompting strategies (basic\nprompting, self-reflective prompting, model ensemble, and customised prompts)\nto determine how to improve extraction quality. All models demonstrate high\nprecision but consistently suffer from poor recall by omitting key information.\nWe found that customised prompts were the most effective, boosting recall by up\nto 15\\%. Based on this analysis, we propose a three-tiered set of guidelines\nfor using LLMs in data extraction, matching data types to appropriate levels of\nautomation based on task complexity and risk. Our study offers practical advice\nfor automating data extraction in real-world meta-analyses, balancing LLM\nefficiency with expert oversight through targeted, task-specific automation.", "AI": {"tldr": "This study evaluates the performance of three LLMs for automating data extraction from RCTs for meta-analysis, finding customised prompts significantly improve recall.", "motivation": "Automating data extraction from RCTs poses a challenge for meta-analysis, necessitating improved methodologies.", "method": "Three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) were tested with four prompting strategies in three medical domains to assess their data extraction capabilities.", "result": "All models showed high precision but poor recall, with customised prompts improving recall by up to 15%.", "conclusion": "The study suggests a three-tiered guideline for using LLMs in data extraction, advocating for a balanced approach of LLM efficiency and expert oversight.", "key_contributions": ["Evaluation of multiple LLMs for a specific application in healthcare data extraction", "Identification of customised prompts as effective for improving recall", "Proposal of a three-tiered guideline for LLM use in real-world scenarios"], "limitations": "The research primarily focuses on RCTs in three medical domains, which may limit generalization.", "keywords": ["LLMs", "data extraction", "meta-analysis", "RCTs", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.15198", "pdf": "https://arxiv.org/pdf/2507.15198.pdf", "abs": "https://arxiv.org/abs/2507.15198", "title": "Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment", "authors": ["Xiandong Meng", "Yan Wu", "Yexin Tian", "Xin Hu", "Tianze Kang", "Junliang Du"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks.", "AI": {"tldr": "The paper presents a distillation strategy for large language models that utilizes multiple teacher models to improve the student model's language understanding and generation while reducing computational costs.", "motivation": "Address challenges of high computational cost and slow inference in deploying large language models.", "method": "Proposes a distillation strategy guided by multiple teacher models that integrates their output probability distributions and intermediate semantic features.", "result": "The student model shows improved semantic information capture, high consistency in expression, generalization ability, and task adaptability across multiple evaluation metrics.", "conclusion": "The study demonstrates a feasible technical path for compressing large-scale language models and validates the effectiveness of multi-teacher collaborative mechanisms in complex language modeling tasks.", "key_contributions": ["Weighted output fusion mechanism", "Feature alignment loss function", "Entropy-driven dynamic teacher weighting strategy"], "limitations": "", "keywords": ["large language models", "model distillation", "multi-teacher guidance"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.15236", "pdf": "https://arxiv.org/pdf/2507.15236.pdf", "abs": "https://arxiv.org/abs/2507.15236", "title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest", "authors": ["Shayan Vassef", "Amirhossein Dabiriaghdam", "Mohammadreza Bakhtiari", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance.", "AI": {"tldr": "This paper examines multi-task, multi-lingual, and multi-source learning approaches to improve pretrained language models, introducing a new framework for analyzing training behaviors.", "motivation": "To explore how different learning settings affect the robustness and performance of language models.", "method": "The authors introduce Subsets of Interest (SOI) to categorize learning behaviors during training, and they conduct experiments comparing multi-task, multi-source, and multi-lingual learning across various tasks.", "result": "Multi-source learning improves out-of-distribution performance by up to 7%, while multi-task learning shows variable results depending on task similarity; a two-stage fine-tuning method utilizing SOI enhances performance further.", "conclusion": "The study provides insights into training dynamics and practical methods for optimizing multi-setting language model performance.", "key_contributions": ["Introduction of Subsets of Interest (SOI) for training analysis.", "Comprehensive experiments across multiple language learning tasks.", "Development of a two-stage fine-tuning approach that incorporates SOI."], "limitations": "", "keywords": ["multi-task learning", "multi-source learning", "pretrained language models", "Subsets of Interest", "fine-tuning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.15275", "pdf": "https://arxiv.org/pdf/2507.15275.pdf", "abs": "https://arxiv.org/abs/2507.15275", "title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling", "authors": ["Yuanhe Tian", "Junjie Liu", "Zhizhou Kou", "Yuxiang Li", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability.", "AI": {"tldr": "ChiMed 2.0 is a large-scale Chinese medical dataset that enhances data coverage and supports various training methodologies for LLMs.", "motivation": "The need for high-quality, diverse Chinese medical datasets that support pre-training and reinforcement learning from human feedback to advance AI in healthcare.", "method": "We created ChiMed 2.0 by collecting data from Chinese medical online platforms and generating content using LLMs, encompassing both traditional and modern medical literature.", "result": "ChiMed 2.0 comprises 204.4M characters, including documents for pre-training, question-answering pairs for supervised fine-tuning, and preference data for RLHF, resulting in performance gains for LLMs on medical benchmarks.", "conclusion": "The dataset demonstrates significant effectiveness and applicability in training LLMs for the Chinese medical domain, improving their performance across various scales.", "key_contributions": ["Introduction of ChiMed 2.0 dataset for Chinese medical AI research", "Comprehensive coverage of traditional and modern Chinese medical data", "Support for pre-training, supervised fine-tuning, and RLHF methods"], "limitations": "", "keywords": ["Chinese medical dataset", "LLM pre-training", "health informatics", "reinforcement learning from human feedback", "medical AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15281", "pdf": "https://arxiv.org/pdf/2507.15281.pdf", "abs": "https://arxiv.org/abs/2507.15281", "title": "A Novel Self-Evolution Framework for Large Language Models", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs.", "AI": {"tldr": "Proposes a Dual-Phase Self-Evolution (DPSE) framework that improves Large Language Models (LLMs) by optimizing user preferences and domain-specific competence through structured data expansion and a fine-tuning pipeline.", "motivation": "To enhance the capabilities of LLMs beyond pre-training by addressing limitations in domain cognition and user alignment.", "method": "The DPSE framework includes a Censor module that extracts interaction signals and satisfaction scores to guide data expansion, followed by a two-stage fine-tuning process consisting of supervised domain grounding and preference optimization.", "result": "DPSE consistently outperforms existing methods like Supervised Fine-Tuning and Memory-Augmented baselines on various NLP benchmarks and dialogue tasks.", "conclusion": "DPSE facilitates an autonomous self-evolution of LLMs, enhancing their adaptation to user preferences and domain knowledge.", "key_contributions": ["Introduction of the Dual-Phase Self-Evolution framework", "Development of a Censor module for interaction signal extraction", "Demonstrated superior performance on NLP and dialogue tasks compared to baseline methods."], "limitations": "", "keywords": ["Large Language Models", "Self-Evolution", "User Preferences", "Domain Cognition", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.15286", "pdf": "https://arxiv.org/pdf/2507.15286.pdf", "abs": "https://arxiv.org/abs/2507.15286", "title": "Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection", "authors": ["Navid Ayoobi", "Sadat Shahriar", "Arjun Mukherjee"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel evaluation paradigm for AI text detectors that prioritizes\nreal-world and equitable assessment. Current approaches predominantly report\nconventional metrics like AUROC, overlooking that even modest false positive\nrates constitute a critical impediment to practical deployment of detection\nsystems. Furthermore, real-world deployment necessitates predetermined\nthreshold configuration, making detector stability (i.e. the maintenance of\nconsistent performance across diverse domains and adversarial scenarios), a\ncritical factor. These aspects have been largely ignored in previous research\nand benchmarks. Our benchmark, SHIELD, addresses these limitations by\nintegrating both reliability and stability factors into a unified evaluation\nmetric designed for practical assessment. Furthermore, we develop a post-hoc,\nmodel-agnostic humanification framework that modifies AI text to more closely\nresemble human authorship, incorporating a controllable hardness parameter.\nThis hardness-aware approach effectively challenges current SOTA zero-shot\ndetection methods in maintaining both reliability and stability. (Data and\ncode: https://github.com/navid-aub/SHIELD-Benchmark)", "AI": {"tldr": "The paper introduces SHIELD, a new evaluation metric for AI text detectors focusing on reliability and stability for practical deployment.", "motivation": "To enhance the assessment of AI text detectors by integrating real-world scenarios and ensuring stable performance across various domains.", "method": "Developed a benchmark, SHIELD, that incorporates reliability and stability into its evaluation criteria and created a humanification framework to modify AI text for better resemblance to human authorship.", "result": "The SHIELD benchmark allows for better comparison of AI text detection systems in real-world settings and improves the assessment of zero-shot detection methods through a controllable hardness parameter.", "conclusion": "The proposed evaluation paradigm and methodology provide a more equitable and applicable framework for assessing AI text detectors, addressing gaps in existing metrics.", "key_contributions": ["Introduction of SHIELD benchmark for AI text detectors", "Incorporation of reliability and stability in evaluation", "Development of a humanification framework for AI text"], "limitations": "", "keywords": ["AI text detection", "evaluation metrics", "humanification framework"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.15328", "pdf": "https://arxiv.org/pdf/2507.15328.pdf", "abs": "https://arxiv.org/abs/2507.15328", "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models", "authors": ["Thilo Hagendorff"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples.", "AI": {"tldr": "This paper discusses the inherent political biases in AI alignment, specifically arguing that training large language models (LLMs) to be harmless and honest leads to left-wing political bias due to normative assumptions.", "motivation": "To explore the conflict between AI alignment principles and the perceived political bias in large language models, especially concerns regarding left-wing tendencies.", "method": "The paper presents a theoretical argument analyzing the relationship between AI alignment objectives and their alignment with progressive moral frameworks.", "result": "The study argues that adherence to alignment principles unavoidably results in left-wing political bias, countering critiques that frame such bias as problematic.", "conclusion": "The author concludes that critiques of left-leaning bias in LLMs undermine the very principles of AI alignment by promoting harmful ideologies contrary to those principles.", "key_contributions": ["Analyzes the connection between AI alignment principles and progressive moral frameworks.", "Argues that left-wing political bias is a necessary outcome of adhering to AI alignment goals.", "Critiques the framing of political bias in AI research as problematic, highlighting a contradiction in AI alignment discussions."], "limitations": "", "keywords": ["AI alignment", "political bias", "large language models", "ethical AI", "human-centered design"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.15337", "pdf": "https://arxiv.org/pdf/2507.15337.pdf", "abs": "https://arxiv.org/abs/2507.15337", "title": "Reasoning Models are Test Exploiters: Rethinking Multiple-Choice", "authors": ["Narun Raman", "Taylor Lundy", "Kevin Leyton-Brown"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "When evaluating Large Language Models (LLMs) in question-answering domains,\nit is common to ask the model to choose among a fixed set of choices (so-called\nmultiple-choice question-answering, or MCQA). Although downstream tasks of\ninterest typically do not provide systems with explicit options among which to\nchoose, this approach is nevertheless widely used because it makes it makes\nautomatic grading straightforward and has tended to produce challenging\nbenchmarks that correlate sufficiently well with downstream performance. This\npaper investigates the extent to which this trend continues to hold for\nstate-of-the-art reasoning models, describing a systematic evaluation of $15$\ndifferent question-answering benchmarks (e.g., MMLU, HLE) and $25$ different\nLLMs (including small models such as Qwen 7B and relatively large models such\nas Llama 70B). For each model-benchmark pair, we considered $5$ ways of\npresenting the model with questions, including variations on whether multiple\nchoices were offered to the model at all; whether \"none of the above\" sometimes\nreplaced the right answer; and whether the model was permitted to perform\nchain-of-thought reasoning before and/or after the choices were presented. MCQA\nremained a good proxy for the downstream performance of models as long as they\nwere allowed to perform chain-of-thought reasoning only before being presented\nwith the options among which they had to select. On the other hand, large\nmodels that were able to perform reasoning after being given a set of options\ntended to significantly outperform their free-text performance due to\nexploiting the information in the options. We conclude that MCQA is no longer a\ngood proxy for assessing downstream performance of state-of-the-art models, and\noffer practical guidelines for designing more robust, bias-resistant benchmarks\nthat better reflect LLMs' genuine reasoning capabilities.", "AI": {"tldr": "This paper evaluates the effectiveness of multiple-choice question-answering (MCQA) as a benchmark for Large Language Models (LLMs), revealing that traditional MCQA methods may not accurately predict model performance in real-world scenarios. It suggests guidelines for creating better benchmarks.", "motivation": "To explore whether multiple-choice question-answering (MCQA) still serves as a reliable benchmark for evaluating Large Language Models (LLMs) and to investigate the conditions under which it is valid.", "method": "A systematic evaluation of 15 different question-answering benchmarks and 25 different LLMs, assessing various presentation methods for questions, including the provision of options and the timing of chain-of-thought reasoning.", "result": "The study found that MCQA remains a good proxy for model performance when chain-of-thought reasoning occurs before presenting options, but large models perform better when reasoning occurs after options are given, indicating a shift in benchmark reliability.", "conclusion": "The research concludes that traditional MCQA is no longer a suitable measure for assessing LLM performance in downstream tasks, advocating for the development of more accurate and bias-resistant benchmarks.", "key_contributions": ["Systematic evaluation of 15 question-answering benchmarks with 25 LLMs", "Insights into the influence of reasoning timing on MCQA effectiveness", "Guidelines for creating improved benchmarks for LLM performance assessment"], "limitations": "The study focuses primarily on the aspect of MCQA and may not account for other methods of reasoning assessment or diverse task types beyond question-answering.", "keywords": ["Large Language Models", "multiple-choice question-answering", "chain-of-thought reasoning", "benchmarking", "LLM performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15339", "pdf": "https://arxiv.org/pdf/2507.15339.pdf", "abs": "https://arxiv.org/abs/2507.15339", "title": "LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators", "authors": ["Leanne Tan", "Gabriel Chua", "Ziyu Ge", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Modern moderation systems increasingly support multiple languages, but often\nfail to address localisation and low-resource variants - creating safety gaps\nin real-world deployments. Small models offer a potential alternative to large\nLLMs, yet still demand considerable data and compute. We present LionGuard 2, a\nlightweight, multilingual moderation classifier tailored to the Singapore\ncontext, supporting English, Chinese, Malay, and partial Tamil. Built on\npre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2\noutperforms several commercial and open-source systems across 17 benchmarks,\nincluding both Singapore-specific and public English datasets. The system is\nactively deployed within the Singapore Government, demonstrating practical\nefficacy at scale. Our findings show that high-quality local data and robust\nmultilingual embeddings can achieve strong moderation performance, without\nfine-tuning large models. We release our model weights and part of our training\ndata to support future work on LLM safety.", "AI": {"tldr": "LionGuard 2 is a multilingual moderation classifier for the Singapore context, outperforming several existing systems.", "motivation": "To address safety gaps in multilingual moderation systems, particularly for low-resource languages and local contexts.", "method": "Built on OpenAI embeddings and a multi-head ordinal classifier, designed specifically for the Singaporean environment.", "result": "LionGuard 2 outperformed various commercial and open-source systems across 17 benchmarks, demonstrating practical efficacy in a governmental deployment.", "conclusion": "High-quality local data and robust multilingual embeddings allow for effective moderation without the need for large model fine-tuning.", "key_contributions": ["Lightweight multilingual classifier tailored for Singapore", "Outperforms existing moderation systems", "Releases model weights and training data for future research"], "limitations": "", "keywords": ["multilingual moderation", "low-resource languages", "AI safety"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2404.16660", "pdf": "https://arxiv.org/pdf/2404.16660.pdf", "abs": "https://arxiv.org/abs/2404.16660", "title": "Benchmarking Mobile Device Control Agents across Diverse Configurations", "authors": ["Juyong Lee", "Taywon Min", "Minyong An", "Dongyoon Hahm", "Haeone Lee", "Changyeon Kim", "Kimin Lee"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted to ICLR 2024 Workshop on Generative Models for Decision\n  Making (Spotlight) and CoLLAs 2025", "summary": "Mobile device control agents can largely enhance user interactions and\nproductivity by automating daily tasks. However, despite growing interest in\ndeveloping practical agents, the absence of a commonly adopted benchmark in\nthis area makes it challenging to quantify scientific progress. In this work,\nwe introduce B-MoCA: a novel benchmark with interactive environments for\nevaluating and developing mobile device control agents. To create a realistic\nbenchmark, we develop B-MoCA based on the Android operating system and define\n131 common daily tasks. Importantly, we incorporate a randomization feature\nthat changes the configurations of mobile devices, including user interface\nlayouts and language settings, to assess generalization performance. We\nbenchmark diverse agents, including agents employing large language models\n(LLMs) or multi-modal LLMs as well as agents trained with imitation learning\nusing human expert demonstrations. While these agents demonstrate proficiency\nin executing straightforward tasks, their poor performance on complex tasks\nhighlights significant opportunities for future research to improve\neffectiveness. Our source code is publicly available at\nhttps://b-moca.github.io.", "AI": {"tldr": "This paper introduces B-MoCA, a benchmark for evaluating mobile device control agents in diverse interactive environments and highlights agents' strengths and weaknesses in task execution.", "motivation": "To address the lack of a common benchmark for assessing mobile device control agents and quantify their scientific progress in everyday task automation.", "method": "Development of the B-MoCA benchmark based on the Android OS, defining 131 daily tasks and incorporating a randomization feature to evaluate agent generalization.", "result": "Agents utilizing LLMs or multi-modal approaches perform well on simple tasks but struggle with complex ones, revealing areas for further research.", "conclusion": "The B-MoCA benchmark is a critical step towards enhancing the development and evaluation of mobile control agents; it also encourages exploration to improve agent capabilities on complex tasks.", "key_contributions": ["Introduction of the B-MoCA benchmark for mobile device control agents.", "Definition of 131 common daily tasks for evaluation.", "Incorporation of random configurations to test generalization performance."], "limitations": "Current agents show poor performance on complex tasks, indicating a need for improvement.", "keywords": ["mobile device control", "benchmarking", "large language models", "task automation", "human-agent interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.15347", "pdf": "https://arxiv.org/pdf/2507.15347.pdf", "abs": "https://arxiv.org/abs/2507.15347", "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "authors": ["Amedeo Buonanno", "Alessandro Rivetti", "Francesco A. N. Palmieri", "Giovanni Di Gennaro", "Gianmarco Romano"], "categories": ["cs.CL", "cs.LG"], "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter", "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models", "AI": {"tldr": "This study investigates the use of entropy analysis to understand information distribution in Transformer architectures, particularly focusing on GPT-based models.", "motivation": "To probe how information is managed and transformed within Transformer-based models by analyzing token-level uncertainty and entropy patterns.", "method": "Entropy analysis is applied to different stages of a GPT-based large language model to quantify information distribution and reveal internal representations.", "result": "The methodology illustrates how entropy patterns can provide insights into model behavior and internal structures, contributing to interpretability and evaluation frameworks.", "conclusion": "The findings suggest that entropy analysis can enhance understanding and evaluation of Transformer-based models.", "key_contributions": ["Introduces entropy analysis as a tool for model interpretability", "Demonstrates its application in a GPT-based model", "Enhances evaluation frameworks for Transformer models"], "limitations": "", "keywords": ["entropy analysis", "Transformer models", "interpretability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.06348", "pdf": "https://arxiv.org/pdf/2501.06348.pdf", "abs": "https://arxiv.org/abs/2501.06348", "title": "Why Automate This? Exploring the Connection between Time Use, Well-being and Robot Automation Across Social Groups", "authors": ["Ruchira Ray", "Leona Pang", "Sanjana Srivastava", "Li Fei-Fei", "Samantha Shorey", "Roberto Martín-Martín"], "categories": ["cs.HC", "cs.RO"], "comment": "20 pages, 14 figures", "summary": "Understanding the motivations underlying the human inclination to automate\ntasks is vital to developing truly helpful robots integrated into daily life.\nAccordingly, we ask: are individuals more inclined to automate chores based on\nthe time they consume or the feelings experienced while performing them? This\nstudy explores these preferences and whether they vary across different social\ngroups (i.e., gender category and income level). Leveraging data from the\nBEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use\nSurvey Well-Being Module, we investigate the relationship between the desire\nfor automation, time spent on daily activities, and their associated feelings -\nHappiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness.\nOur key findings show that, despite common assumptions, time spent does not\nstrongly relate to the desire for automation for the general population. For\nthe feelings analyzed, only happiness and pain are key indicators. Significant\ndifferences by gender and economic level also emerged: Women prefer to automate\nstressful activities, whereas men prefer to automate those that make them\nunhappy; mid-income individuals prioritize automating less enjoyable and\nmeaningful activities, while low and high-income show no significant\ncorrelations. We hope our research helps motivate technologies to develop\nrobots that match the priorities of potential users, moving domestic robotics\ntoward more socially relevant solutions. We open-source all the data, including\nan online tool that enables the community to replicate our analysis and explore\nadditional trends at https://hri1260.github.io/why-automate-this.", "AI": {"tldr": "This study investigates the motivations behind people's inclination to automate tasks, focusing on the influence of time spent and associated feelings, revealing important differences across social groups.", "motivation": "To understand motivations for task automation in developing beneficial robots for daily life.", "method": "Analysis of the BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use Survey Well-Being Module to explore automation preferences related to time and feelings.", "result": "The desire for automation is more closely linked to feelings (happiness, pain) than the time spent on chores; significant gender and income variations were observed in automation preferences.", "conclusion": "The findings challenge assumptions about time and highlight the emotional factors in automation decisions, aiming to guide the development of user-centered domestic robotics.", "key_contributions": ["Demonstrated that feelings significantly influence task automation preferences over time spent.", "Revealed gender and economic disparities in automation motivation.", "Provided an open-source dataset and analysis tool for community use."], "limitations": "The study may not cover all demographic variables influencing automation preferences.", "keywords": ["task automation", "human-robot interaction", "feelings", "social groups", "open-source data"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2507.15357", "pdf": "https://arxiv.org/pdf/2507.15357.pdf", "abs": "https://arxiv.org/abs/2507.15357", "title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.", "AI": {"tldr": "The paper evaluates Large Language Models' capabilities in metaphor interpretation using diverse datasets, revealing performance relies more on surface features than on understanding metaphorical content.", "motivation": "To address limitations of previous research focused on single-dataset evaluations and artificial data in metaphor processing within NLP.", "method": "Extensive experiments were conducted using multiple publicly available datasets with inference and metaphor annotations, particularly in Natural Language Inference (NLI) and Question Answering (QA) tasks.", "result": "Findings show that LLMs' performance is largely influenced by lexical overlap and sentence length rather than metaphor content, indicating their abilities to interpret metaphor are overstated.", "conclusion": "The study emphasizes the necessity for more realistic evaluation frameworks in metaphor interpretation tasks, given the identified limitations of LLMs.", "key_contributions": ["Evaluation of LLMs with diverse datasets", "Insights on LLMs' metaphor processing capabilities", "Call for improved metaphor evaluation frameworks"], "limitations": "Limited to public datasets and specific tasks, potentially influencing generalizability of results.", "keywords": ["Large Language Models", "metaphor interpretation", "Natural Language Processing", "evaluation frameworks", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.03253", "pdf": "https://arxiv.org/pdf/2504.03253.pdf", "abs": "https://arxiv.org/abs/2504.03253", "title": "Ultra-low-power ring-based wireless tinymouse", "authors": ["Yifan Li", "Masaaki Fukumoto", "Mohamed Kari", "Shigemi Ishida", "Akihito Noda", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara", "Ryo Takahashi"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2501.16674", "summary": "Wireless mouse rings offer subtle, reliable pointing interactions for\nwearable computing platforms. However, the small battery below 27 mAh in the\nminiature rings restricts the ring's continuous lifespan to just 1-10 hours,\nbecause current low-powered wireless communication such as BLE is\npower-consuming for ring's continuous use. The ring's short lifespan frequently\ndisrupts users' mouse use with the need for frequent charging. This paper\npresents picoRing mouse, enabling a continuous ring-based mouse interaction\nwith ultra-low-powered ring-to-wristband wireless communication. picoRing mouse\nemploys a coil-based impedance sensing named semi-passive inductive telemetry,\nallowing a wristband coil to capture a unique frequency response of a nearby\nring coil via a sensitive inductive coupling between the coils. The ring coil\nconverts the corresponding user's mouse input into the unique frequency\nresponse via an up to 449 uW mouse-driven modulation system. Therefore, the\ncontinuous use of picoRing mouse can last approximately 600 (8hrs use/day)-1000\n(4hrs use/day) hours on a single charge of a 27 mAh battery while supporting\nsubtle thumb-to-index scrolling and pressing interactions in real-world\nwearable computing situations.", "AI": {"tldr": "The picoRing mouse enables extended use of a ring-based pointing device through ultra-low-power wireless communication, significantly increasing battery life compared to existing solutions.", "motivation": "Wireless mouse rings are limited by short battery life due to power-hungry communication methods, necessitating frequent recharging and disrupting use.", "method": "The picoRing mouse utilizes semi-passive inductive telemetry for ring-to-wristband communication, which allows for unique frequency responses based on mouse inputs, achieved through an efficient modulation system.", "result": "The picoRing mouse can sustain 600 to 1000 hours of continuous use on a single charge, depending on usage patterns, while enabling subtle finger movements like scrolling.", "conclusion": "This innovation addresses the battery limitations of wearable mouse devices, facilitating prolonged use and enhanced user experience in wearable computing.", "key_contributions": ["Introduction of picoRing mouse with ultra-low-powered communication", "Implementation of semi-passive inductive telemetry for effective input capture", "Significant extension of operational hours on a single battery charge"], "limitations": "", "keywords": ["picoRing mouse", "wearable computing", "inductive telemetry"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.15375", "pdf": "https://arxiv.org/pdf/2507.15375.pdf", "abs": "https://arxiv.org/abs/2507.15375", "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "categories": ["cs.CL", "eess.AS"], "comment": "Work in progress. Project page: https://d223302.github.io/STITCH/", "summary": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.", "AI": {"tldr": "The paper presents Stitch, a novel spoken language model that integrates an unspoken reasoning process while maintaining low latency in spoken responses.", "motivation": "Current spoken language models lack the capability for internal reasoning, hindering their ability to communicate effectively.", "method": "Stitch alternates between generating unspoken reasoning chunks and spoken response chunks, allowing for simultaneous thinking and talking.", "result": "Stitch achieves latency comparable to baseline models that do not generate unspoken reasoning while outperforming them by 15% on math reasoning datasets.", "conclusion": "Stitch successfully integrates reasoning in SLMs without increasing response latency, thus enhancing performance in both reasoning and non-reasoning tasks.", "key_contributions": ["Introduction of Stitch, the first method to integrate unspoken reasoning into spoken language models.", "Demonstration of improved performance on math reasoning tasks compared to existing SLMs.", "Maintaining low response latency while enabling complex reasoning processes."], "limitations": "", "keywords": ["Spoken Language Models", "Reasoning", "Latency", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15378", "pdf": "https://arxiv.org/pdf/2507.15378.pdf", "abs": "https://arxiv.org/abs/2507.15378", "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming", "authors": ["Jierui Li", "Raymond Mooney"], "categories": ["cs.CL"], "comment": "19 pages, pre-print only", "summary": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com", "AI": {"tldr": "This paper introduces AlgoSimBench, a benchmark for assessing LLMs' ability to identify algorithmically similar problems (ASPs) and proposes a method to improve their performance in this area.", "motivation": "There is a need to explore whether large language models (LLMs) can generalize their problem-solving abilities to domains less represented in training, especially regarding algorithmically similar problems.", "method": "AlgoSimBench consists of 1317 problems annotated with 231 distinct algorithm tags, leading to the creation of 402 multiple-choice questions. The study evaluates LLMs' performance using attempted solution matching (ASM) to enhance the identification of ASPs.", "result": "The best-performing model achieved 65.9% accuracy on the MCQ task, with ASM improving accuracy by 6.7% to 11.7% across different models. When combining ASM with BM25 keyword-prioritization, accuracy reached 52.2%.", "conclusion": "The findings suggest that while LLMs struggle with ASP identification, there are methods like ASM and BM25 that can significantly improve performance in this domain.", "key_contributions": ["Introduction of AlgoSimBench benchmark for ASPs", "Proposing attempted solution matching (ASM) method", "Evaluation of code embedding models and retrieval methods on ASP identification"], "limitations": "Limited to the evaluation of LLMs in identifying ASPs and potential over-reliance on summarization techniques without addressing broader algorithmic understanding.", "keywords": ["Large Language Models", "Algorithm Identification", "Benchmarking", "Machine Learning", "Problem Solving"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2507.15501", "pdf": "https://arxiv.org/pdf/2507.15501.pdf", "abs": "https://arxiv.org/abs/2507.15501", "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution", "authors": ["Alexandru Coca", "Mark Gaynor", "Zhenxing Zhang", "Jianpeng Cheng", "Bo-Hsiang Tseng", "Pete Boothroyd", "Héctor Martinez Alonso", "Diarmuid Ó Séaghdha", "Anders Johannsen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "37 pages, 22 figures. To appear at ACL 2025", "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.", "AI": {"tldr": "This paper presents ASPERA, a framework for developing digital assistants powered by large language models (LLMs) to execute complex tasks through action execution programs.", "motivation": "The motivation behind this work is to explore the capabilities of LLMs in creating digital assistants that can perform complex actions, utilizing their programming knowledge effectively.", "method": "The authors developed ASPERA, which includes a simulation of an assistant library and a human-assisted data generation engine to help create high-quality tasks based on user queries and simulation states.", "result": "The study resulted in the creation of Asper-Bench, a dataset containing 250 complex tasks, demonstrating that generating programs based on custom assistant libraries poses significant challenges for LLMs.", "conclusion": "The findings indicate that while LLMs are promising for generating action execution programs, they face difficulties when compared to traditional code generation methods that do not rely on external dependencies.", "key_contributions": ["Development of ASPERA framework for LLM-powered digital assistants", "Creation of Asper-Bench evaluation dataset with challenging tasks", "Demonstration of the challenges LLMs face in program generation from custom libraries"], "limitations": "The paper does not address potential scalability issues of the framework or the generalizability of the results across different domains.", "keywords": ["large language models", "digital assistants", "task generation", "action execution", "evaluation dataset"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2507.15512", "pdf": "https://arxiv.org/pdf/2507.15512.pdf", "abs": "https://arxiv.org/abs/2507.15512", "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "authors": ["Kaiyan Chang", "Yonghao Shi", "Chenglong Wang", "Hang Zhou", "Chi Hu", "Xiaoqian Liu", "Yingfeng Luo", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.", "AI": {"tldr": "The paper introduces Hybrid Test-Time Scaling (TTS), a training-free method designed to enhance the reasoning performance of large language models (LLMs) during inference by combining fine-grained sequential and classical parallel scaling techniques.", "motivation": "To address the growing computation burden of training-based TTS methods and the decline in prominence of training-free TTS methods, this paper explores effective training-free approaches for improving model reasoning performance during inference.", "method": "The study presents Conditional Step-level Self-refinement as a sequential scaling method and combines it with classical parallel scaling methods to create the Hybrid Test-Time Scaling paradigm.", "result": "Experiments on five instruction-tuned LLMs show significant improvements in reasoning performance when employing the hybrid strategy with various training-free methods.", "conclusion": "Hybrid Test-Time Scaling offers a promising direction for enhancing LLM reasoning capabilities without the additional computational costs associated with training-based methods.", "key_contributions": ["Introduction of Hybrid Test-Time Scaling for reasoning", "Design of Conditional Step-level Self-refinement method", "Demonstration of significant improvements in reasoning performance across multiple LLMs"], "limitations": "", "keywords": ["Test-Time Scaling", "Large Language Models", "Reasoning", "Inference", "Machine Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.15557", "pdf": "https://arxiv.org/pdf/2507.15557.pdf", "abs": "https://arxiv.org/abs/2507.15557", "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification", "authors": ["Vitaly Protasov", "Nikolay Babakov", "Daryna Dementieva", "Alexander Panchenko"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.", "AI": {"tldr": "This paper evaluates text style transfer (TST) systems across nine languages, highlighting gaps in current methods and proposing improvements for multilingual evaluation.", "motivation": "There is a significant gap between automatic metrics and human judgments in evaluating text generation tasks, particularly for multilingual text style transfer. This paper aims to address these challenges by providing a comprehensive evaluation across multiple languages.", "method": "The study conducted a multilingual evaluation of text detoxification systems across nine languages using modern neural-based evaluation models and prompting-based LLM-as-a-judge approaches. The evaluation drew inspiration from machine translation methodologies.", "result": "The findings reveal the effectiveness of new evaluation models and provide a framework for designing reliable multilingual TST evaluation pipelines, particularly in the context of text detoxification.", "conclusion": "This study lays the groundwork for improved evaluation methods in multilingual TST, offering a practical approach for researchers and developers in the field.", "key_contributions": ["First comprehensive multilingual study on TST evaluation", "Introduction of neural-based evaluation models for TST", "Proposed framework for designing multilingual evaluation pipelines"], "limitations": "", "keywords": ["text style transfer", "multilingual evaluation", "text detoxification"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15576", "pdf": "https://arxiv.org/pdf/2507.15576.pdf", "abs": "https://arxiv.org/abs/2507.15576", "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging", "authors": ["Nicolas Poggi", "Shashank Agnihotri", "Margret Keuper"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.", "AI": {"tldr": "This paper presents a method using In-Context Learning with Vision-Language Models for improving classification in Terahertz imaging.", "motivation": "To address challenges in Terahertz imaging such as limited annotations and low resolution for better image classification.", "method": "The authors utilize In-Context Learning with Vision-Language Models, adapting them to the Terahertz domain with a modality-aligned prompting framework and evaluate their performance in zero-shot and one-shot settings.", "result": "ICL improves classification performance and interpretability in low-data situations, demonstrating the potential for effective use of VLMs in THz imaging.", "conclusion": "The introduction of ICL-enhanced VLMs for THz imaging represents a novel and promising approach for scientific domains facing resource constraints.", "key_contributions": ["First application of ICL-enhanced VLMs to THz imaging", "Demonstrated improvement in classification and interpretability", "Provided a GitHub repository for further research and use"], "limitations": "", "keywords": ["Terahertz imaging", "In-Context Learning", "Vision-Language Models", "Classification", "Low-data regimes"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.15586", "pdf": "https://arxiv.org/pdf/2507.15586.pdf", "abs": "https://arxiv.org/abs/2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "16 pages, 7 Figures, 10 Tables", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "AI": {"tldr": "The paper proposes LEAR, a mechanism for improving evidence extraction in Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) by explicitly reasoning and optimizing the evidence extraction process.", "motivation": "To address the significant impact of retrieval noises on the quality of LLM generation, which can lead to the omission of important clues and challenges in generalization.", "method": "LEAR combines evidence reasoning and extraction into a unified response for end-to-end training, utilizing knowledge token masks and three types of verifiable reward functions for model optimization.", "result": "LEAR improves the accuracy of downstream tasks by providing high-quality, compact evidence, as evidenced by extensive experiments on benchmark datasets.", "conclusion": "The proposed method effectively enhances the quality of evidence extraction in online RAG systems, contributing to more accurate responses from LLMs.", "key_contributions": ["Unified framework for evidence reasoning and extraction", "Use of knowledge token masks for model optimization", "Introduction of verifiable reward functions for policy optimization"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Evidence Extraction"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2507.15600", "pdf": "https://arxiv.org/pdf/2507.15600.pdf", "abs": "https://arxiv.org/abs/2507.15600", "title": "Conflicting narratives and polarization on social media", "authors": ["Armin Pournaki"], "categories": ["cs.CL", "cs.SI"], "comment": "30 pages, 7 figures", "summary": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization.", "AI": {"tldr": "This paper analyzes conflicting narratives on political issues in the German Twittersphere, showing how these narratives reveal mechanisms of polarization and alignment.", "motivation": "To understand how conflicting narratives shape political reality and public discourse.", "method": "Analysis of tweets from opposing opinion groups about political issues such as the war in Ukraine, Covid, and climate change.", "result": "Identified divergent interpretations and emplotments of events, providing insights into narrative alignment strategies used by political actors.", "conclusion": "Narratives serve as a critical analytical lens to understand polarization dynamics in public discourse.", "key_contributions": ["Identification of conflicting narratives in political discourse", "Evidence of narrative alignment as a discursive strategy", "Analysis of social media data to highlight narrative mechanisms"], "limitations": "", "keywords": ["conflicting narratives", "polarization", "narrative alignment", "political discourse", "social media"], "importance_score": 3, "read_time_minutes": 30}}
{"id": "2507.15641", "pdf": "https://arxiv.org/pdf/2507.15641.pdf", "abs": "https://arxiv.org/abs/2507.15641", "title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates", "authors": ["Alessio Pittiglio"], "categories": ["cs.CL", "cs.AI"], "comment": "12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025", "summary": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements.", "AI": {"tldr": "We propose a multimodal approach to detect logical fallacies in political debates using Transformer models.", "motivation": "Advance research in multimodal argument mining, focusing on identifying logical fallacies in political discourse.", "method": "Utilized pretrained Transformer-based models and explored various methods to leverage context for fallacy classification in text, audio, and multimodal data.", "result": "Achieved macro F1-scores of 0.4444 for text, 0.3559 for audio, and 0.4403 for multimodal classification, with multimodal performance on par with text-only models.", "conclusion": "Indicates potential for improvement in multimodal argument mining approaches.", "key_contributions": ["Proposed a multimodal approach for argument mining", "Demonstrated effectiveness of Transformer models for fallacy detection", "Provided comparative analysis across text, audio, and multimodal formats"], "limitations": "", "keywords": ["argument mining", "logical fallacies", "multimodal", "Transformer models", "political debates"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.15675", "pdf": "https://arxiv.org/pdf/2507.15675.pdf", "abs": "https://arxiv.org/abs/2507.15675", "title": "P3: Prompts Promote Prompting", "authors": ["Xinyu Zhang", "Yuanquan Hu", "Fangchao Liu", "Zhicheng Dou"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 findings", "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.", "AI": {"tldr": "P3 is a self-improvement framework that concurrently optimizes both system and user prompts in large language models, demonstrating improved performance in various tasks.", "motivation": "Current methods for optimizing prompts in large language models (LLMs) often do so in isolation, leading to suboptimal outcomes due to the interdependent nature of prompt components.", "method": "The P3 framework simultaneously optimizes both system and user prompts through an iterative process, using offline optimized prompts for online prompting with query-dependent optimization.", "result": "Extensive experiments reveal that P3 outperforms existing approaches on general and reasoning tasks, indicating the benefits of holistic prompt optimization.", "conclusion": "The study concludes that concurrent optimization of prompts enhances LLM performance across various tasks and introduces a promising framework for future applications.", "key_contributions": ["Development of the P3 framework for simultaneous prompt optimization", "Demonstration of superior performance on tasks through holistic strategies", "Empirical validation across multiple datasets and task types"], "limitations": "", "keywords": ["large language models", "prompt optimization", "human-computer interaction", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.15698", "pdf": "https://arxiv.org/pdf/2507.15698.pdf", "abs": "https://arxiv.org/abs/2507.15698", "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models", "authors": ["Congmin Zheng", "Jiachen Zhu", "Jianghao Lin", "Xinyi Dai", "Yong Yu", "Weinan Zhang", "Mengyue Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.", "AI": {"tldr": "This paper introduces CoLD, a framework to address length bias in Process Reward Models for large language models by implementing debiasing strategies.", "motivation": "To address the length bias in existing Process Reward Models that can lead to unreliable reward predictions and unnecessarily verbose outputs in LLMs.", "method": "CoLD employs a length-penalty adjustment, a learned bias estimator, and a joint training strategy to enforce length-invariance in predictions.", "result": "CoLD reduces reward-length correlation, enhances accuracy in selecting reasoning steps, and facilitates more concise reasoning across extensive experiments on MATH500 and GSM-Plus datasets.", "conclusion": "The results indicate CoLD improves the fidelity and robustness of Process Reward Models in large language models.", "key_contributions": ["Introduction of CoLD framework to mitigate length bias", "Utilization of counterfactual reasoning for debiasing", "Demonstrated practical improvements in mathematical problem-solving"], "limitations": "", "keywords": ["Process Reward Models", "length bias", "CoLD", "LLMs", "counterfactual reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15706", "pdf": "https://arxiv.org/pdf/2507.15706.pdf", "abs": "https://arxiv.org/abs/2507.15706", "title": "Compositional Understanding in Signaling Games", "authors": ["David Peter Wallis Freeborn"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Receivers in standard signaling game models struggle with learning\ncompositional information. Even when the signalers send compositional messages,\nthe receivers do not interpret them compositionally. When information from one\nmessage component is lost or forgotten, the information from other components\nis also erased. In this paper I construct signaling game models in which\ngenuine compositional understanding evolves. I present two new models: a\nminimalist receiver who only learns from the atomic messages of a signal, and a\ngeneralist receiver who learns from all of the available information. These\nmodels are in many ways simpler than previous alternatives, and allow the\nreceivers to learn from the atomic components of messages.", "AI": {"tldr": "This paper presents new signaling game models that enable receivers to learn compositional information effectively.", "motivation": "The existing signaling game models show that receivers struggle with learning compositional information which leads to loss of understanding when components of messages are forgotten.", "method": "The author constructs two new signaling game models: a minimalist receiver, focusing on atomic messages, and a generalist receiver, learning from all available information.", "result": "The new models demonstrate that receivers can achieve genuine compositional understanding, improving upon the limitations of earlier models.", "conclusion": "These new models are simpler and more effective in allowing receivers to learn from the atomic components of messages, fostering genuine compositional understanding.", "key_contributions": ["Introduction of minimalist and generalist receiver models in signaling games", "Demonstration of effective compositional understanding in receivers", "Simplification of learning processes compared to previous models"], "limitations": "", "keywords": ["signaling games", "compositional understanding", "learning models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.15707", "pdf": "https://arxiv.org/pdf/2507.15707.pdf", "abs": "https://arxiv.org/abs/2507.15707", "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?", "authors": ["Seok Hwan Song", "Mohna Chakraborty", "Qi Li", "Wallapak Tavanapong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.", "AI": {"tldr": "This study investigates how different question types influence the reasoning accuracy of Large Language Models (LLMs) across various tasks.", "motivation": "To explore the unexplored impact of question types on LLM accuracy in reasoning tasks.", "method": "Evaluated five LLMs on three question types using quantitative and deductive reasoning tasks, measuring accuracy in reasoning steps and final answers.", "result": "Significant variations in LLM performance were found based on question types, revealing no direct correlation between reasoning accuracy and final answer selection.", "conclusion": "Question types and their characteristics significantly impact LLM performance on reasoning tasks, affecting both reasoning and answer accuracy.", "key_contributions": ["Identification of performance differences across question types", "Insight into the lack of correlation between reasoning steps and final answers", "Analysis of how wording and number of options affect LLM outcomes"], "limitations": "", "keywords": ["Large Language Models", "question types", "reasoning tasks", "accuracy", "performance metrics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.15714", "pdf": "https://arxiv.org/pdf/2507.15714.pdf", "abs": "https://arxiv.org/abs/2507.15714", "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning", "authors": ["Tian Li", "Yujian Sun", "Huizhi Liang"], "categories": ["cs.CL"], "comment": null, "summary": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.", "AI": {"tldr": "This paper details the SemEval-2025 Task 11 on emotion detection across 28 languages, employing contrastive learning methods to improve predictions and ranking well in both classification and intensity prediction tracks.", "motivation": "To tackle the challenges of diversity in emotional expressions and background variations in emotion recognition across multiple languages.", "method": "Two contrastive learning approaches were explored: a sample-based approach (Contrastive Reasoning Calibration) and a generation-based approach (DPO, SimPO), fine-tuning models from LLaMa3-Instruct-8B for improved predictions.", "result": "The proposed system ranked 9th in multi-label classification (Track A) and 6th in emotion intensity prediction (Track B) for English, and performed among the top-tier systems for other languages.", "conclusion": "The study demonstrates that both contrastive learning approaches can effectively enhance emotion detection across various languages and emotional categories.", "key_contributions": ["Introduction of a multi-language emotion detection competition", "Detailed exploration of contrastive learning methods", "Demonstrated effective prediction improvements using advanced architectures"], "limitations": "", "keywords": ["Emotion Detection", "Contrastive Learning", "Multilingual NLP"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.15715", "pdf": "https://arxiv.org/pdf/2507.15715.pdf", "abs": "https://arxiv.org/abs/2507.15715", "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "authors": ["Alina Hyk", "Kiera McCormick", "Mian Zhong", "Ioana Ciucă", "Sanjib Sharma", "John F Wu", "J. E. G. Peek", "Kartheik G. Iyer", "Ziang Xiao", "Anjalie Field"], "categories": ["cs.CL", "astro-ph.IM"], "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures", "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.", "AI": {"tldr": "This study explores user evaluations of LLMs in the context of an astronomy literature retrieval application, proposing improvements for benchmarks based on user interactions.", "motivation": "To improve evaluation procedures of LLMs by understanding how real users assess these models, especially in scientific contexts like astronomy.", "method": "The study involved inductive coding of 368 queries executed on an LLM-powered retrieval-augmented generation bot deployed via Slack, along with follow-up interviews with 11 astronomers.", "result": "The findings highlighted specific types of questions users asked and the criteria they used to evaluate the bot's responses, leading to recommendations for better LLM benchmarks in scientific research.", "conclusion": "The work emphasizes the importance of understanding user evaluations to enhance the usability and effectiveness of LLMs, particularly in scientific applications.", "key_contributions": ["Inductive coding of user queries to understand evaluations of LLMs", "Concrete recommendations for building improved LLM benchmarks", "A sample benchmark for evaluating LLMs tailored for astronomy"], "limitations": "", "keywords": ["LLMs", "astronomy", "evaluation benchmarks", "user studies", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.15717", "pdf": "https://arxiv.org/pdf/2507.15717.pdf", "abs": "https://arxiv.org/abs/2507.15717", "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning", "authors": ["Sahana Srinivasan", "Xuguang Ai", "Thaddaeus Wai Soon Lo", "Aidan Gilson", "Minjie Zou", "Ke Zou", "Hyunjae Kim", "Mingjia Yang", "Krithi Pushpanathan", "Samantha Yew", "Wan Ting Loke", "Jocelyn Goh", "Yibing Chen", "Yiming Kong", "Emily Yuelei Fu", "Michelle Ongyong Hui", "Kristen Nwanyanwu", "Amisha Dave", "Kelvin Zhenghao Li", "Chen-Hsin Sun", "Mark Chia", "Gabriel Dawei Yang", "Wendy Meihua Wong", "David Ziyou Chen", "Dianbo Liu", "Maxwell Singer", "Fares Antaki", "Lucian V Del Priore", "Jost Jonas", "Ron Adelman", "Qingyu Chen", "Yih-Chung Tham"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.", "AI": {"tldr": "BELO is a standardized benchmark for evaluating large language models in ophthalmology, focusing on clinical accuracy and reasoning quality through expert-validated MCQs.", "motivation": "Current LLM benchmarks in ophthalmology prioritize accuracy but are limited in scope. BELO aims to provide a comprehensive assessment framework.", "method": "Utilized a fine-tuned PubMedBERT model for keyword matching to create ophthalmology-specific MCQs from various datasets, curated and validated by ophthalmology experts.", "result": "Six LLMs were evaluated using BELO, demonstrating their performance across various metrics such as accuracy and text-generation quality. A public leaderboard was established for transparency.", "conclusion": "The BELO dataset will serve as an evaluation-only benchmark for future LLM comparisons, promoting fair and reproducible assessments in ophthalmology.", "key_contributions": ["Introduction of a new benchmark specifically for ophthalmology-related LLM evaluation", "Involvement of ophthalmology experts in curating high-quality MCQs and explanations", "Establishment of a public leaderboard for transparent reporting"], "limitations": "Focuses solely on ophthalmology; may not generalize to other medical fields.", "keywords": ["large language models", "ophthalmology", "benchmarking", "clinical accuracy", "reasoning quality"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.15736", "pdf": "https://arxiv.org/pdf/2507.15736.pdf", "abs": "https://arxiv.org/abs/2507.15736", "title": "Understanding Large Language Models' Ability on Interdisciplinary Research", "authors": ["Yuanhao Shen", "Daniel Xavier de Sousa", "Ricardo Marçal", "Ali Asad", "Hongyu Guo", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.", "AI": {"tldr": "Introduction of IDRBench, a benchmark for evaluating LLMs' capabilities in interdisciplinary research idea generation.", "motivation": "To overcome the lack of a dedicated benchmark evaluating LLMs in interdisciplinary research and to understand their strengths and limitations.", "method": "Development of IDRBench featuring an expert annotated dataset and structured tasks to assess LLMs in proposing interdisciplinary research ideas.", "result": "Evaluation of 10 LLMs using IDRBench shows that while LLMs are aware of interdisciplinary research, they still struggle to generate quality ideas.", "conclusion": "IDRBench can spur new research directions and aid in developing advanced LLMs for interdisciplinary applications.", "key_contributions": ["Creation of the IDRBench benchmark for LLMs in interdisciplinary research", "Provision of an expert annotated dataset from diverse scientific domains", "Development of a systematic framework for assessing LLM performance in IDR"], "limitations": "Current LLMs show limited ability to produce quality interdisciplinary research ideas.", "keywords": ["Large Language Models", "interdisciplinary research", "benchmark", "IDRBench", "scientific discovery"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.15742", "pdf": "https://arxiv.org/pdf/2507.15742.pdf", "abs": "https://arxiv.org/abs/2507.15742", "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "23 pages, 4 tables", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "AI": {"tldr": "The paper provides a statistical justification for TF-IDF, showing its connection to significance testing and Fisher's exact test, asserting its effectiveness as a term-weighting scheme in text analysis.", "motivation": "To build a theoretical foundation for TF-IDF and justify its use in the statistics community by relating it to significance testing.", "method": "The authors demonstrate that the variant TF-ICF relates closely to the negative logarithm of the p-value from Fisher's exact test under certain conditions, establishing a connection to TF-IDF.", "result": "The study reveals that as document collections grow infinitely, the negative log-transformed p-value converges to TF-IDF, providing a statistical rationale for its effectiveness.", "conclusion": "The justification of TF-IDF using Fisher's exact test offers statisticians a framework for understanding the effectiveness of TF-IDF in information retrieval.", "key_contributions": ["Establishes a statistical foundation for TF-IDF and its variants.", "Connects TF-IDF to Fisher's exact test and significance testing.", "Demonstrates convergence of TF-IDF with increasing document collection size."], "limitations": "", "keywords": ["TF-IDF", "Fisher's exact test", "significance testing", "term-weighting schemes", "information retrieval"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.15752", "pdf": "https://arxiv.org/pdf/2507.15752.pdf", "abs": "https://arxiv.org/abs/2507.15752", "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue", "authors": ["Ruizhe Zhu", "Hao Zhu", "Yaxuan Li", "Syang Zhou", "Shijing Cai", "Malgorzata Lazuka", "Elliott Ash"], "categories": ["cs.CL", "cs.AI"], "comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation", "summary": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.", "AI": {"tldr": "DialogueForge is a framework designed to generate AI-simulated conversations in a human-chatbot style, using seed prompts from real interactions and testing various LLMs.", "motivation": "The paper addresses the challenges of manual effort in collecting human-chatbot dialogues, which limits research on conversational AI.", "method": "The framework utilizes seed prompts from real conversations to generate multi-turn dialogues. Various LLMs are tested to simulate user interactions, and fine-tuning techniques are explored for improving smaller models.", "result": "Experiments show that while large proprietary LLMs like GPT-4o outperform others in realism, smaller models can be fine-tuned for better performance and customization.", "conclusion": "Despite advancements, all models struggle with maintaining coherent and natural long-form conversations, indicating an ongoing challenge in conversational AI research.", "key_contributions": ["Introduction of DialogueForge framework for generating AI-simulated conversations.", "Evaluation of various LLMs for dialogue generation and the impact of fine-tuning on smaller models.", "Comparison of performance between proprietary and open-source models in generating human-like dialogues."], "limitations": "Maintaining coherent and natural long-form dialogues is a persistent challenge across all models tested.", "keywords": ["dialogue generation", "conversational AI", "LLM", "fine-tuning", "human-chatbot interactions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.15759", "pdf": "https://arxiv.org/pdf/2507.15759.pdf", "abs": "https://arxiv.org/abs/2507.15759", "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership", "authors": ["Lyumanshan Ye", "Xiaojie Cai", "Xinkai Wang", "Junfei Wang", "Xiangkun Hu", "Jiadi Su", "Yang Nan", "Sihan Wang", "Bohan Zhang", "Xiaoze Fan", "Jinbin Luo", "Yuxiang Zheng", "Tianze Xu", "Dayuan Fu", "Yunze Wu", "Pengrui Lu", "Zengzhi Wang", "Yiwei Qin", "Zhen Huang", "Yan Ma", "Zhulin Hu", "Haoyang Zou", "Tiantian Mi", "Yixin Ye", "Ethan Chern", "Pengfei Liu"], "categories": ["cs.CL"], "comment": "30 pages, 10 figures", "summary": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.", "AI": {"tldr": "This paper presents the \"Interaction as Intelligence\" framework, which reconceptualizes human-AI relationships in deep research tasks by proposing that interaction is a key aspect of intelligence.", "motivation": "To shift from viewing interaction as a simple interface to recognizing it as a fundamental aspect of deep learning systems in research tasks.", "method": "The authors propose a system called Deep Cognition that allows for transparent, controllable, and interruptible interactions with AI, enhancing the human role from merely giving instructions to providing cognitive oversight.", "result": "User evaluations show that the Deep Cognition system significantly improves interaction metrics such as Transparency, Fine-Grained Interaction, and Ease of Collaboration, with enhancements ranging from 18.5% to 50.0% over traditional systems.", "conclusion": "The findings indicate that reimagining human-AI interaction as cognitive oversight during research tasks leads to better performance and user satisfaction.", "key_contributions": ["Introduction of the Deep Cognition framework for human-AI interaction.", "Demonstrated empirical improvements in multiple performance metrics.", "Establishment of cognitive oversight as a new role for users in AI engagement."], "limitations": "", "keywords": ["Human-AI Interaction", "Deep Cognition", "Cognitive Oversight", "Research Tasks", "AI Transparency"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2507.15773", "pdf": "https://arxiv.org/pdf/2507.15773.pdf", "abs": "https://arxiv.org/abs/2507.15773", "title": "Supernova: Achieving More with Less in Transformer Architectures", "authors": ["Andrei-Valentin Tanase", "Elena Pelican"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 53% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.", "AI": {"tldr": "Supernova is a 650M-parameter decoder-only transformer that achieves performance comparable to larger models with fewer parameters through architectural innovations and a custom tokenizer.", "motivation": "To demonstrate that effective architectural design and innovative tokenization can enable smaller models to match the performance of larger ones while maintaining efficiency.", "method": "The architecture employs Rotary Positional Embeddings (RoPE), Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for efficiency, and SwiGLU activation functions, alongside a custom 128,000-vocabulary byte-level BPE tokenizer.", "result": "Supernova achieves 90% of the performance of a 1B-parameter model while using 53% fewer parameters and only requiring 100B training tokens.", "conclusion": "The findings challenge prevailing beliefs about model scaling, showing architectural efficiency and tokenization quality can offset lower parameter counts.", "key_contributions": ["Introduction of Supernova, an efficient 650M-parameter model", "Innovative tokenization methods achieving superior performance", "Demonstrated architectural efficiency with fewer parameters"], "limitations": "", "keywords": ["decoder-only transformer", "tokenization", "architectural design"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.15778", "pdf": "https://arxiv.org/pdf/2507.15778.pdf", "abs": "https://arxiv.org/abs/2507.15778", "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR", "authors": ["Jiakang Wang", "Runze Liu", "Fuzheng Zhang", "Xiu Li", "Guorui Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.", "AI": {"tldr": "Archer is a novel entropy-aware RLVR method that improves reasoning in LLMs by applying dual-token constraints and synchronous updates, outperforming previous methods.", "motivation": "Previous RLVR methods applied uniform training signals to all tokens, ignoring the different roles of knowledge-related and reasoning-related tokens, which hindered effective learning.", "method": "Archer employs dual-token constraints with different KL regularizations for knowledge and reasoning tokens and uses synchronous updates to maintain semantic dependencies.", "result": "Experimental results show that Archer significantly outperforms previous RLVR methods on mathematical reasoning and code generation benchmarks, achieving state-of-the-art performance for comparable models.", "conclusion": "Archer effectively enhances reasoning abilities in LLMs while maintaining factual knowledge, positioning it as a superior post-training method for LLMs.", "key_contributions": ["Introduction of dual-token constraints in RLVR", "Implementation of entropy-aware updates", "Demonstrated state-of-the-art performance on benchmarks"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Reasoning", "Entropy-Aware", "Dual-Token Constraints"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.15779", "pdf": "https://arxiv.org/pdf/2507.15779.pdf", "abs": "https://arxiv.org/abs/2507.15779", "title": "Reservoir Computing as a Language Model", "authors": ["Felix Köster", "Atsushi Uchida"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 1 table", "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.", "AI": {"tldr": "This paper investigates reservoir computing for natural text processing as an alternative to large language models, comparing its performance, computational cost, and accuracy with transformer architectures.", "motivation": "To address the energy demands and slow processing of large language models (LLMs) and make models accessible while improving quality.", "method": "The study compares three approaches for character-level language modeling: two reservoir computing methods (with a trainable output layer) and transformer-based architectures. It evaluates prediction accuracy and computational efficiency by controlling the number of trainable parameters.", "result": "Transformers demonstrate superior prediction quality, while reservoir computing methods are more efficient, reducing training and inference times. The study presents a traditional reservoir with a static output and an attention-enhanced reservoir that adapts its output weights dynamically.", "conclusion": "The research suggests a balance between resource constraints and performance, showing that reservoir computing can be a viable alternative for efficient natural language processing.", "key_contributions": ["Comparison of reservoir computing and transformer models for language modeling", "Introduction of an attention-enhanced reservoir computing model", "Guidelines for balancing efficiency and performance in language processing tasks."], "limitations": "Limited research on reservoir computing in language modeling; effectiveness depends on specific applications and contexts.", "keywords": ["Large Language Models", "Reservoir Computing", "Natural Language Processing", "Transformers", "Efficient Computing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.15823", "pdf": "https://arxiv.org/pdf/2507.15823.pdf", "abs": "https://arxiv.org/abs/2507.15823", "title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work", "authors": ["Anton Abilov", "Ke Zhang", "Hemank Lamba", "Elizabeth M. Olson", "Joel R. Tetreault", "Alejandro Jaimes"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners.", "AI": {"tldr": "This paper discusses the collaboration and deployment of AI models in resource-constrained environments for humanitarian purposes.", "motivation": "There is a need to address the gap in literature regarding the deployment and collaboration processes in AI for Good initiatives.", "method": "The authors detail a case study of collaboration with a humanitarian organization to deploy and maintain an AI model.", "result": "The study provides insights into the processes and challenges of deploying AI solutions in humanitarian contexts, emphasizing continuous performance updates.", "conclusion": "The paper concludes that effective collaboration and maintenance are crucial for the successful implementation of AI in resource-limited settings.", "key_contributions": ["Insights on collaboration with humanitarian organizations", "Strategies for deploying AI in resource-constrained environments", "Recommendations for continuous performance maintenance"], "limitations": "", "keywords": ["AI for Good", "humanitarian collaboration", "model deployment", "resource-constrained environments", "continuous performance updates"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.15849", "pdf": "https://arxiv.org/pdf/2507.15849.pdf", "abs": "https://arxiv.org/abs/2507.15849", "title": "The Impact of Language Mixing on Bilingual LLM Reasoning", "authors": ["Yihao Li", "Jiayi Xin", "Miranda Muqing Miao", "Qi Long", "Lyle Ungar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.", "AI": {"tldr": "This paper studies language mixing in bilingual large language models, demonstrating that strategic language switching can enhance reasoning accuracy in math tasks.", "motivation": "To understand how intentional language switching affects reasoning in bilingual large language models and to explore methods to leverage this behavior for improved performance.", "method": "The study focuses on Chinese-English bilingual reasoning models. It identifies reinforcement learning with verifiable rewards (RLVR) as a key factor enabling language mixing and tests its impact on reasoning accuracy.", "result": "Language mixing was found to enhance reasoning performance, with a 5.6 percentage point accuracy loss when enforcing monolingual decoding, and a 6.25 percentage point increase in accuracy when using a probe to guide decoding based on language switch predictions.", "conclusion": "Language mixing in multilingual settings is beneficial for reasoning tasks, indicating it is a strategic behavior rather than just a byproduct of training.", "key_contributions": ["Identification of RLVR as a facilitator of language mixing in models.", "Demonstration of the accuracy benefits of language mixing in reasoning tasks.", "Development of a lightweight probe to predict the impact of language switching on reasoning accuracy."], "limitations": "", "keywords": ["language mixing", "bilingual models", "reasoning", "reinforcement learning", "multilingual training"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.15850", "pdf": "https://arxiv.org/pdf/2507.15850.pdf", "abs": "https://arxiv.org/abs/2507.15850", "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "categories": ["cs.CL"], "comment": null, "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "AI": {"tldr": "This paper presents 3LM, a suite of three Arabic benchmarks for LLM evaluation in STEM and coding, addressing a gap in Arabic LLM research.", "motivation": "To address the limited development of Arabic language models, particularly in STEM and coding domains, which are often overlooked in existing benchmarks.", "method": "The paper introduces three benchmarks: 1) STEM-related question-answer pairs from educational materials, 2) synthetically generated STEM questions, and 3) a code generation benchmark, created through translation of existing benchmarks with human review.", "result": "The benchmarks aim to facilitate research and development of Arabic LLMs in the STEM fields and coding, which are crucial for real-world applications.", "conclusion": "By providing these resources, the paper seeks to promote the growth of high-quality Arabic LLM research in underrepresented areas.", "key_contributions": ["Introduction of a unique set of benchmarks for Arabic LLMs in STEM and coding.", "Use of both naturally sourced and synthetically generated questions to enhance validity.", "Public release of datasets to support further Arabic LLM research."], "limitations": "Focused primarily on STEM and code without covering other potential domains for Arabic LLMs.", "keywords": ["Arabic", "Large Language Models", "STEM", "code generation", "benchmarks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.14384", "pdf": "https://arxiv.org/pdf/2507.14384.pdf", "abs": "https://arxiv.org/abs/2507.14384", "title": "Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions", "authors": ["Angjelin Hila", "Elliott Hauser"], "categories": ["cs.HC", "cs.CL"], "comment": "Extended version of paper accepted for presentation at the ASIS&T\n  Annual Meeting 2025. 38 pages, 12 figures", "summary": "In this study, we investigate the use of large language models (LLMs),\nspecifically ChatGPT, for structured deductive qualitative coding. While most\ncurrent research emphasizes inductive coding applications, we address the\nunderexplored potential of LLMs to perform deductive classification tasks\naligned with established human-coded schemes. Using the Comparative Agendas\nProject (CAP) Master Codebook, we classified U.S. Supreme Court case summaries\ninto 21 major policy domains. We tested four intervention methods: zero-shot,\nfew-shot, definition-based, and a novel Step-by-Step Task Decomposition\nstrategy, across repeated samples. Performance was evaluated using standard\nclassification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's\nalpha), and construct validity was assessed using chi-squared tests and\nCramer's V. Chi-squared and effect size analyses confirmed that intervention\nstrategies significantly influenced classification behavior, with Cramer's V\nvalues ranging from 0.359 to 0.613, indicating moderate to strong shifts in\nclassification patterns. The Step-by-Step Task Decomposition strategy achieved\nthe strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),\nachieving thresholds for substantial agreement. Despite the semantic ambiguity\nwithin case summaries, ChatGPT displayed stable agreement across samples,\nincluding high F1 scores in low-support subclasses. These findings demonstrate\nthat with targeted, custom-tailored interventions, LLMs can achieve reliability\nlevels suitable for integration into rigorous qualitative coding workflows.", "AI": {"tldr": "This study explores the effectiveness of large language models (LLMs) like ChatGPT in structured deductive qualitative coding, highlighting significant outcomes from various coding intervention strategies.", "motivation": "To investigate the underutilized potential of LLMs in performing deductive classification tasks compared to conventional human coding methods.", "method": "We classified U.S. Supreme Court case summaries into major policy domains using the Comparative Agendas Project Master Codebook, testing four intervention methods: zero-shot, few-shot, definition-based, and a novel Step-by-Step Task Decomposition strategy.", "result": "The Step-by-Step Task Decomposition strategy yielded the highest reliability metrics (accuracy = 0.775, kappa = 0.744) and confirmed that intervention strategies significantly influenced classification behavior, evidenced by Cramer's V values indicating moderate to strong shifts in classification patterns.", "conclusion": "With custom-tailored interventions, LLMs can achieve reliability levels appropriate for rigorous qualitative coding workflows, making them a valuable tool for qualitative researchers.", "key_contributions": ["Exploration of deductive coding using LLMs", "Introduction of Step-by-Step Task Decomposition strategy", "Demonstrated reliable classification performance of ChatGPT in qualitative coding"], "limitations": "The study acknowledges semantic ambiguity in case summaries which may affect classification outcomes.", "keywords": ["Large Language Models", "Qualitative Coding", "Deductive Classification", "ChatGPT", "Intervention Strategies"], "importance_score": 9, "read_time_minutes": 38}}
{"id": "2303.09823", "pdf": "https://arxiv.org/pdf/2303.09823.pdf", "abs": "https://arxiv.org/abs/2303.09823", "title": "Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages", "authors": ["Angel Felipe Magnossão de Paula", "Imene Bensalem", "Paolo Rosso", "Wajdi Zaghouani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 3 tables", "summary": "This paper describes our participation in the shared task of hate speech\ndetection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our\nexperiments evaluate the performance of six transformer models and their\ncombination using 2 ensemble approaches. The best results on the training set,\nin a five-fold cross validation scenario, were obtained by using the ensemble\napproach based on the majority vote. The evaluation of this approach on the\ntest set resulted in an F1-score of 0.60 and an Accuracy of 0.86.", "AI": {"tldr": "The paper presents experimental results on hate speech detection using transformer models and ensemble approaches.", "motivation": "To contribute to the hate speech detection task within the CERIST NLP Challenge 2022 and explore effective model combinations.", "method": "Evaluated six transformer models and two ensemble approaches, with a majority vote strategy yielding the best results in a five-fold cross-validation setup.", "result": "Achieved an F1-score of 0.60 and an accuracy of 0.86 on the test set using the majority vote ensemble approach.", "conclusion": "The majority vote ensemble approach is effective for hate speech detection, demonstrating promising performance with the evaluated transformer models.", "key_contributions": ["Evaluation of multiple transformer models for hate speech detection.", "Implementation of ensemble methods to improve detection performance.", "Contribution to a competitive shared task in NLP."], "limitations": "Results may be limited by the nature of the datasets and specific characteristics of the transformer models used.", "keywords": ["hate speech detection", "transformer models", "ensemble methods", "NLP challenge", "machine learning"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2311.09675", "pdf": "https://arxiv.org/pdf/2311.09675.pdf", "abs": "https://arxiv.org/abs/2311.09675", "title": "Where Do People Tell Stories Online? Story Detection Across Online Communities", "authors": ["Maria Antoniak", "Joel Mire", "Maarten Sap", "Elliott Ash", "Andrew Piper"], "categories": ["cs.CL"], "comment": null, "summary": "Story detection in online communities is a challenging task as stories are\nscattered across communities and interwoven with non-storytelling spans within\na single text. We address this challenge by building and releasing the\nStorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts\nand comments, a detailed codebook adapted to the social media context, and\nmodels to predict storytelling at the document and span levels. Our dataset is\nsampled from hundreds of popular English-language Reddit communities ranging\nacross 33 topic categories, and it contains fine-grained expert annotations,\nincluding binary story labels, story spans, and event spans. We evaluate a\nrange of detection methods using our data, and we identify the distinctive\ntextual features of online storytelling, focusing on storytelling spans. We\nilluminate distributional characteristics of storytelling on a large\ncommunity-centric social media platform, and we also conduct a case study on\nr/ChangeMyView, where storytelling is used as one of many persuasive\nstrategies, illustrating that our data and models can be used for both inter-\nand intra-community research. Finally, we discuss implications of our tools and\nanalyses for narratology and the study of online communities.", "AI": {"tldr": "The paper introduces the StorySeeker toolkit, aimed at detecting storytelling in online communities by providing an annotated dataset of Reddit posts and comments, alongside models for story detection.", "motivation": "Detecting stories in online communities is difficult due to the intertwining of storytelling elements with non-story content in texts.", "method": "The StorySeeker toolkit comprises an annotated dataset from Reddit, including a codebook for social media, and models for predicting storytelling at different levels.", "result": "The evaluation of various detection methods revealed unique textual features of online storytelling, and a case study demonstrated the practical application of the toolkit.", "conclusion": "The findings and tools provided can enhance research on narratology and the dynamics of online communities.", "key_contributions": ["Release of the StorySeeker toolkit and annotated dataset of 502 Reddit posts.", "Identification of distinctive textual features for storytelling detection.", "Case study on storytelling as a persuasive strategy in the r/ChangeMyView community."], "limitations": "", "keywords": ["story detection", "social media", "online communities", "Reddit", "narratology"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2311.16789", "pdf": "https://arxiv.org/pdf/2311.16789.pdf", "abs": "https://arxiv.org/abs/2311.16789", "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems: Data, Task and Models", "authors": ["Hongru Wang", "Lingzhi Wang", "Yiming Du", "Liang Chen", "Jingyan Zhou", "Yufei Wang", "Kam-Fai Wong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue systems (DS), including the task-oriented dialogue system (TOD) and\nthe open-domain dialogue system (ODD), have always been a fundamental task in\nnatural language processing (NLP), allowing various applications in practice.\nOwing to sophisticated training and well-designed model architecture, language\nmodels (LM) are usually adopted as the necessary backbone to build the dialogue\nsystem. Consequently, every breakthrough in LM brings about a shift in learning\nparadigm and research attention within dialogue system, especially the\nappearance of pre-trained language models (PLMs) and large language models\n(LLMs). In this paper, we take a deep look at the history of the dialogue\nsystem, especially its special relationship with the advancements of language\nmodels. Thus, our survey offers a systematic perspective, categorizing\ndifferent stages in a chronological order aligned with LM breakthroughs,\nproviding a comprehensive review of state-of-the-art research outcomes. What's\nmore, we turn our attention to emerging topics and engage in a discussion on\nopen challenges, providing valuable insights into the future directions for\nLLM-based dialogue systems. In summary, this survey delves into the dynamic\ninterplay between language models and dialogue systems, unraveling the\nevolutionary path of this essential relationship. Through this exploration, we\npave the way for a deeper comprehension of the field, guiding future\ndevelopments in LM-based dialogue systems.", "AI": {"tldr": "This paper surveys the evolution of dialogue systems in relation to advancements in language models, particularly pre-trained and large language models, exploring state-of-the-art outcomes, emerging topics, and future challenges.", "motivation": "To understand the interplay and evolution between dialogue systems and language model breakthroughs, providing insights for future research directions.", "method": "The paper systematically categorizes the history of dialogue systems aligned with significant advancements in language models in chronological order, reviewing state-of-the-art research outcomes.", "result": "A comprehensive overview of the evolution of dialogue systems and their relationship with language models, along with discussions on emerging topics and open challenges in the field.", "conclusion": "The survey helps to understand the dynamic relationship between language models and dialogue systems, paving the way for future advancements in LLM-based dialogue systems.", "key_contributions": ["Chronological categorization of dialogue system evolution", "Review of state-of-the-art research outcomes", "Insights into future challenges and directions in LLM-based dialogue systems."], "limitations": "", "keywords": ["Dialogue Systems", "Language Models", "Natural Language Processing", "Pre-trained Language Models", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2311.17741", "pdf": "https://arxiv.org/pdf/2311.17741.pdf", "abs": "https://arxiv.org/abs/2311.17741", "title": "End-to-end Joint Punctuated and Normalized ASR with a Limited Amount of Punctuated Training Data", "authors": ["Can Cui", "Imran Ahamad Sheikh", "Mostafa Sadeghi", "Emmanuel Vincent"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Joint punctuated and normalized automatic speech recognition (ASR) aims at\noutputing transcripts with and without punctuation and casing. This task\nremains challenging due to the lack of paired speech and punctuated text data\nin most ASR corpora. We propose two approaches to train an end-to-end joint\npunctuated and normalized ASR system using limited punctuated data. The first\napproach uses a language model to convert normalized training transcripts into\npunctuated transcripts. This achieves a better performance on out-of-domain\ntest data, with up to 17% relative Punctuation-Case-aware Word Error Rate\n(PC-WER) reduction. The second approach uses a single decoder conditioned on\nthe type of output. This yields a 42% relative PC-WER reduction compared to\nWhisper-base and a 4% relative (normalized) WER reduction compared to the\nnormalized output of a punctuated-only model. Additionally, our proposed model\ndemonstrates the feasibility of a joint ASR system using as little as 5%\npunctuated training data with a moderate (2.42% absolute) PC-WER increase.", "AI": {"tldr": "This paper presents two novel approaches for training a joint punctuated and normalized ASR system, yielding significant reductions in error rates with limited punctuated data utilization.", "motivation": "To address challenges in automatic speech recognition (ASR) due to scarcity of paired speech and punctuated text data.", "method": "The first approach leverages a language model to create punctuated transcripts from normalized training data. The second method involves a single decoder that conditions the output type, enhancing performance with limited training data.", "result": "The first approach results in a 17% relative reduction in Punctuation-Case-aware Word Error Rate (PC-WER) on out-of-domain data. The second method achieves a 42% relative PC-WER reduction compared to Whisper-base.", "conclusion": "The proposed methods enhance the performance of joint ASR systems significantly, demonstrating effective utilization of minimal punctuated training data.", "key_contributions": ["Two approaches for joint ASR with limited punctuated data", "17% and 42% relative reductions in error rates", "Feasibility of using only 5% punctuated training data"], "limitations": "", "keywords": ["automatic speech recognition", "punctuation", "language model", "error rate reduction", "training data"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2402.02655", "pdf": "https://arxiv.org/pdf/2402.02655.pdf", "abs": "https://arxiv.org/abs/2402.02655", "title": "VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension", "authors": ["Thinh Phuoc Ngo", "Khoa Tran Anh Dang", "Son T. Luu", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "To appear as the main conference paper at EACL 2024", "summary": "This paper presents the development process of a Vietnamese spoken language\ncorpus for machine reading comprehension (MRC) tasks and provides insights into\nthe challenges and opportunities associated with using real-world data for\nmachine reading comprehension tasks. The existing MRC corpora in Vietnamese\nmainly focus on formal written documents such as Wikipedia articles, online\nnewspapers, or textbooks. In contrast, the VlogQA consists of 10,076\nquestion-answer pairs based on 1,230 transcript documents sourced from YouTube\n-- an extensive source of user-uploaded content, covering the topics of food\nand travel. By capturing the spoken language of native Vietnamese speakers in\nnatural settings, an obscure corner overlooked in Vietnamese research, the\ncorpus provides a valuable resource for future research in reading\ncomprehension tasks for the Vietnamese language. Regarding performance\nevaluation, our deep-learning models achieved the highest F1 score of 75.34% on\nthe test set, indicating significant progress in machine reading comprehension\nfor Vietnamese spoken language data. In terms of EM, the highest score we\naccomplished is 53.97%, which reflects the challenge in processing spoken-based\ncontent and highlights the need for further improvement.", "AI": {"tldr": "Development of a Vietnamese spoken language corpus for machine reading comprehension, addressing challenges in using real-world data.", "motivation": "To fill the gap in Vietnamese machine reading comprehension corpora, which largely focus on formal texts, by creating a resource based on spoken language from YouTube.", "method": "The study presents the VlogQA dataset comprising 10,076 question-answer pairs derived from 1,230 YouTube transcript documents, evaluating deep learning models on this corpus.", "result": "Achieved an F1 score of 75.34% and an EM score of 53.97% on the test set, showcasing improvements in MRC for Vietnamese spoken content.", "conclusion": "The VlogQA corpus is a significant step forward in MRC tasks for Vietnamese spoken language and indicates the need for continued research in this area.", "key_contributions": ["Creation of the VlogQA dataset for Vietnamese spoken language", "Demonstrated performance of deep learning models on a new corpus", "Highlighted challenges in processing spoken language data for MRC"], "limitations": "Challenges remain in processing spoken-based content effectively; further improvement is needed.", "keywords": ["Vietnamese", "spoken language", "machine reading comprehension", "deep learning", "VlogQA"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2406.12644", "pdf": "https://arxiv.org/pdf/2406.12644.pdf", "abs": "https://arxiv.org/abs/2406.12644", "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles", "authors": ["Devichand Budagam", "Ashutosh Kumar", "Mahsa Khoshnoodi", "Sankalp KJ", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "18 pages, 9 figures, KDD workshop on Prompt Optimization 2025", "summary": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human\ncognitive principles and designed to assess LLMs by examining the cognitive\ndemands of various tasks. The HPT utilizes the Hierarchical Prompting Framework\n(HPF), which structures five unique prompting strategies in a hierarchical\norder based on their cognitive requirement on LLMs when compared to human\nmental capabilities. It assesses the complexity of tasks with the Hierarchical\nPrompting Index (HPI), which demonstrates the cognitive competencies of LLMs\nacross diverse datasets and offers insights into the cognitive demands that\ndatasets place on different LLMs. This approach enables a comprehensive\nevaluation of an LLMs problem solving abilities and the intricacy of a dataset,\noffering a standardized metric for task complexity. Extensive experiments with\nmultiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%\ncompared to baseline performance, with GSM8k being the most cognitively complex\ntask among reasoning and coding tasks with an average HPI of 3.20 confirming\nthe effectiveness of HPT. To support future research and reproducibility in\nthis domain, the implementations of HPT and HPF are available here.", "AI": {"tldr": "This paper presents the Hierarchical Prompting Taxonomy (HPT) to assess the cognitive demands of large language models (LLMs) on various tasks, demonstrating significant performance improvements through structured prompting strategies.", "motivation": "Understanding the strengths and weaknesses of large language models (LLMs) through effective assessment of their task performance is essential.", "method": "The paper introduces a Hierarchical Prompting Taxonomy (HPT) grounded in human cognitive principles, utilizing the Hierarchical Prompting Framework (HPF) to evaluate cognitive demands through structured prompting strategies.", "result": "Experiments showed that HPF enhances LLM performance by 2% to 63% across diverse datasets, confirming the effectiveness of the HPT, with GSM8k identified as the most cognitively complex task.", "conclusion": "The HPT offers a standardized metric for evaluating task complexity in LLMs, supporting future research and reproducibility in assessing LLM performance.", "key_contributions": ["Development of the Hierarchical Prompting Taxonomy (HPT)", "Introduction of the Hierarchical Prompting Framework (HPF)", "Creation of the Hierarchical Prompting Index (HPI) for evaluating task complexity"], "limitations": "", "keywords": ["large language models", "Hierarchical Prompting Taxonomy", "cognitive demands", "task complexity", "prompting strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.02543", "pdf": "https://arxiv.org/pdf/2407.02543.pdf", "abs": "https://arxiv.org/abs/2407.02543", "title": "Towards the Next Frontier in Speech Representation Learning Using Disentanglement", "authors": ["Varun Krishna", "Sriram Ganapathy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "There were some bugs in the Code that was used to produce the results\n  in the paper. The results reported in the paper are not valid", "summary": "The popular frameworks for self-supervised learning of speech representations\nhave largely focused on frame-level masked prediction of speech regions. While\nthis has shown promising downstream task performance for speech recognition and\nrelated tasks, this has largely ignored factors of speech that are encoded at\ncoarser level, like characteristics of the speaker or channel that remain\nconsistent through-out a speech utterance. In this work, we propose a framework\nfor Learning Disentangled Self Supervised (termed as Learn2Diss)\nrepresentations of speech, which consists of frame-level and an utterance-level\nencoder modules. The two encoders are initially learned independently, where\nthe frame-level model is largely inspired by existing self supervision\ntechniques, thereby learning pseudo-phonemic representations, while the\nutterance-level encoder is inspired by constrastive learning of pooled\nembeddings, thereby learning pseudo-speaker representations. The joint learning\nof these two modules consists of disentangling the two encoders using a mutual\ninformation based criterion. With several downstream evaluation experiments, we\nshow that the proposed Learn2Diss achieves state-of-the-art results on a\nvariety of tasks, with the frame-level encoder representations improving\nsemantic tasks, while the utterance-level representations improve non-semantic\ntasks.", "AI": {"tldr": "A new self-supervised framework, Learn2Diss, is proposed for learning disentangled representations of speech, focusing on both frame-level and utterance-level encoding.", "motivation": "Existing self-supervised frameworks for speech learning mainly focus on frame-level masked predictions, overlooking coarser characteristics like speaker or channel features.", "method": "Learn2Diss employs two encoder modules: a frame-level model inspired by self-supervision for pseudo-phonemic representations and an utterance-level model using contrastive learning for pseudo-speaker representations, trained jointly to disentangle these two aspects.", "result": "Learn2Diss achieves state-of-the-art results across various downstream tasks, with frame-level encoder improving semantic tasks and utterance-level representations enhancing non-semantic tasks.", "conclusion": "The framework enhances the understanding and performance of speech representation learning by effectively disentangling frame-level and utterance-level information, despite reported bugs affecting the validity of the results.", "key_contributions": ["Introduction of the Learn2Diss framework for disentangled speech representations", "Joint learning mechanism for frame-level and utterance-level encoders", "Achieving state-of-the-art results on various speech representation tasks"], "limitations": "The results reported may be invalid due to bugs in the code used for the experiments.", "keywords": ["self-supervised learning", "speech representation", "disentangled representations", "frame-level encoding", "contrastive learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2407.12828", "pdf": "https://arxiv.org/pdf/2407.12828.pdf", "abs": "https://arxiv.org/abs/2407.12828", "title": "Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "authors": ["Jiaxin Qin", "Zixuan Zhang", "Manling Li", "Pengfei Yu", "Heng Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs.", "AI": {"tldr": "The paper investigates why knowledge editing (KE) methods in language models (LMs) cause ripple effects and introduces GradSim, a metric indicating when these ripples occur.", "motivation": "To understand and improve the accuracy of post-training knowledge editing in language models by addressing the issue of ripple effects in knowledge representation.", "method": "The authors analyze the correlation between gradient similarity (GradSim) and ripple effect performance in LMs, using extensive evaluation across various LMs and KE methods.", "result": "The study finds a strong positive correlation between GradSim and ripple effect performance, suggesting that GradSim is a valuable metric for assessing knowledge accuracy in LMs.", "conclusion": "GradSim serves as an effective indicator of when and why knowledge ripples occur in language models, helping to identify potential issues with KE methods.", "key_contributions": ["Identification of GradSim as a salient indicator for ripple effects in LMs", "Analysis of correlation between GradSim and ripple effect performance", "Investigation of counter-intuitive failure cases in ripple effects and their relationship with GradSim."], "limitations": "", "keywords": ["knowledge editing", "language models", "ripple effects", "GradSim", "gradient similarity"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2409.00061", "pdf": "https://arxiv.org/pdf/2409.00061.pdf", "abs": "https://arxiv.org/abs/2409.00061", "title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language", "authors": ["Arief Purnama Muharram", "Ayu Purwarianti"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to the Journal of ICT Research and Applications (JICTRA)", "summary": "Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking.", "AI": {"tldr": "This study proposes a model integrating Knowledge Graphs (KG) for enhancing Natural Language Inference (NLI) performance in automated COVID-19 fact-checking for the Indonesian language, achieving significant accuracy improvements.", "motivation": "To address COVID-19 misinformation online using automated fact-checking techniques while overcoming challenges in deep learning due to knowledge limitations during training.", "method": "The proposed architecture includes three modules: a fact module for processing information from a Knowledge Graph, an NLI module for assessing semantic relationships, and a classifier module that combines outputs from the first two to determine truthfulness.", "result": "Incorporating Knowledge Graphs into the NLI process significantly improved performance, achieving an accuracy of 0.8616 on the Indonesian COVID-19 fact-checking dataset.", "conclusion": "The incorporation of KGs is beneficial for enhancing NLI performance in automated fact-checking, particularly for misinformation related to COVID-19.", "key_contributions": ["Introduction of a KG-based approach to enhance NLI for fact-checking", "Development of a novel model architecture comprising fact, NLI, and classifier modules", "Demonstration of superior accuracy in automated COVID-19 fact-checking in the Indonesian language"], "limitations": "", "keywords": ["automated fact-checking", "Natural Language Inference", "Knowledge Graphs", "COVID-19", "Indonesian language"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2409.18023", "pdf": "https://arxiv.org/pdf/2409.18023.pdf", "abs": "https://arxiv.org/abs/2409.18023", "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation", "authors": ["Hannah Sterz", "Jonas Pfeiffer", "Ivan Vulić"], "categories": ["cs.CL"], "comment": null, "summary": "Vision Language Models (VLMs) extend remarkable capabilities of text-only\nlarge language models and vision-only models, and are able to learn from and\nprocess multi-modal vision-text input. While modern VLMs perform well on a\nnumber of standard image classification and image-text matching tasks, they\nstill struggle with a number of crucial vision-language (VL) reasoning\nabilities such as counting and spatial reasoning. Moreover, while they might be\nvery brittle to small variations in instructions and/or evaluation protocols,\nexisting benchmarks fail to evaluate their robustness (or rather the lack of\nit). In order to couple challenging VL scenarios with comprehensive robustness\nevaluation, we introduce DARE, Diverse Visual Question Answering with\nRobustness Evaluation, a carefully created and curated multiple-choice VQA\nbenchmark. DARE evaluates VLM performance on five diverse categories and\nincludes four robustness-oriented evaluations based on the variations of:\nprompts, the subsets of answer options, the output format and the number of\ncorrect answers. Among a spectrum of other findings, we report that\nstate-of-the-art VLMs still struggle with questions in most categories and are\nunable to consistently deliver their peak performance across the tested\nrobustness evaluations. The worst case performance across the subsets of\noptions is up to 34% below the performance in the standard case. The robustness\nof the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the\nclosed-source models such as GPT-4 and Gemini, but even the latter remain very\nbrittle to different variations.", "AI": {"tldr": "This paper introduces DARE, a VQA benchmark to evaluate the robustness of Vision Language Models (VLMs) on multi-modal reasoning tasks involving text and images.", "motivation": "The need for a robust evaluation framework for VLMs, as current benchmarks do not assess their performance under diverse conditions and variations.", "method": "DARE involves a curated multiple-choice VQA benchmark evaluating VLMs across five categories, focusing on robustness through variations in prompts, answer options, output formats, and correct answer counts.", "result": "State-of-the-art VLMs struggle with vision-language reasoning, showing up to 34% performance drops in robustness evaluations. Open-source models are less robust compared to closed-source ones like GPT-4.", "conclusion": "DARE highlights the vulnerability and inconsistent performance of modern VLMs, emphasizing the need for improved evaluation methods in multimodal tasks.", "key_contributions": ["Introduction of the DARE benchmark for robust evaluation of VLMs", "Identification of performance gaps among different categories of vision-language tasks", "Evaluation of the robustness discrepancies between open-source and closed-source VLMs"], "limitations": "The benchmark may not cover all potential variations and scenarios that could affect VLM performance.", "keywords": ["Vision Language Models", "robustness evaluation", "Visual Question Answering", "Diverse benchmarks", "multi-modal reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.06944", "pdf": "https://arxiv.org/pdf/2410.06944.pdf", "abs": "https://arxiv.org/abs/2410.06944", "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages", "authors": ["Pretam Ray", "Jivnesh Sandhan", "Amrith Krishna", "Pawan Goyal"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables", "summary": "Neural dependency parsing has achieved remarkable performance for low\nresource morphologically rich languages. It has also been well-studied that\nmorphologically rich languages exhibit relatively free word order. This prompts\na fundamental investigation: Is there a way to enhance dependency parsing\nperformance, making the model robust to word order variations utilizing the\nrelatively free word order nature of morphologically rich languages? In this\nwork, we examine the robustness of graph-based parsing architectures on 7\nrelatively free word order languages. We focus on scrutinizing essential\nmodifications such as data augmentation and the removal of position encoding\nrequired to adapt these architectures accordingly. To this end, we propose a\ncontrastive self-supervised learning method to make the model robust to word\norder variations. Furthermore, our proposed modification demonstrates a\nsubstantial average gain of 3.03/2.95 points in 7 relatively free word order\nlanguages, as measured by the UAS/LAS Score metric when compared to the best\nperforming baseline.", "AI": {"tldr": "This paper investigates enhancing neural dependency parsing performance for morphologically rich languages with free word order through a robust graph-based architecture and contrastive self-supervised learning.", "motivation": "To improve dependency parsing performance for morphologically rich languages that feature a relatively free word order.", "method": "The study examines graph-based parsing architectures and incorporates modifications like data augmentation and removing position encoding. It introduces a contrastive self-supervised learning method to improve robustness to word order variations.", "result": "The proposed method shows an average gain of 3.03/2.95 points in UAS/LAS Score across 7 languages compared to the best baseline.", "conclusion": "The contrastive self-supervised learning approach effectively enhances dependency parsing robustness in morphologically rich languages.", "key_contributions": ["Contrastive self-supervised learning method for dependency parsing.", "Robustness improvements in graph-based parsing for free word order languages.", "Empirical results demonstrating significant performance gains."], "limitations": "", "keywords": ["dependency parsing", "morphologically rich languages", "self-supervised learning", "graph-based architectures", "free word order"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2410.12601", "pdf": "https://arxiv.org/pdf/2410.12601.pdf", "abs": "https://arxiv.org/abs/2410.12601", "title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization", "authors": ["Yixi Ding", "Jiaying Wu", "Tongyao Zhu", "Yanxia Qin", "Qian Liu", "Min-Yen Kan"], "categories": ["cs.CL"], "comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances", "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning.", "AI": {"tldr": "CCSBench is the first evaluation benchmark for compositional controllable summarization in the scientific domain, focusing on both explicit and implicit attributes.", "motivation": "There is a need for scientific document summarization systems to control multiple attributes simultaneously for better dissemination of knowledge.", "method": "We introduce CCSBench and conduct experiments using various large language models (LLMs) with different settings for controlling attributes like length and empirical focus.", "result": "Experiments reveal significant limitations of LLMs in balancing trade-offs between explicit and especially implicit control attributes.", "conclusion": "Our findings highlight the need for improved understanding and sophisticated reasoning in LLMs to enhance compositional control in summarization tasks.", "key_contributions": ["Development of CCSBench as a benchmark for compositional controllable summarization", "Demonstration of LLMs' limitations in handling multiple summarization attributes", "Evaluation of various methods for attribute control in summarization"], "limitations": "Current LLMs struggle to balance trade-offs between summary attributes, particularly those requiring deeper understanding.", "keywords": ["compositional summarization", "large language models", "scientific document summarization", "CCSBench", "control attributes"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.20016", "pdf": "https://arxiv.org/pdf/2410.20016.pdf", "abs": "https://arxiv.org/abs/2410.20016", "title": "Vulnerability of LLMs to Vertically Aligned Text Manipulations", "authors": ["Zhecheng Li", "Yiwei Wang", "Bryan Hooi", "Yujun Cai", "Zhen Xiong", "Nanyun Peng", "Kai-wei Chang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Vertical text input is commonly encountered in various real-world\napplications, such as mathematical computations and word-based Sudoku puzzles.\nWhile current large language models (LLMs) have excelled in natural language\ntasks, they remain vulnerable to variations in text formatting. Recent research\ndemonstrates that modifying input formats, such as vertically aligning words\nfor encoder-based models, can substantially lower accuracy in text\nclassification tasks. While easily understood by humans, these inputs can\nsignificantly mislead models, posing a potential risk of bypassing detection in\nreal-world scenarios involving harmful or sensitive information. With the\nexpanding application of LLMs, a crucial question arises: \\textit{Do\ndecoder-based LLMs exhibit similar vulnerabilities to vertically formatted text\ninput?} In this paper, we investigate the impact of vertical text input on the\nperformance of various LLMs across multiple text classification datasets and\nanalyze the underlying causes. Our findings are as follows: (i) Vertical text\ninput significantly degrades the accuracy of LLMs in text classification tasks.\n(ii) \\textit{Chain of Thought (CoT)} reasoning does not help LLMs recognize\nvertical input or mitigate its vulnerability, but \\textit{few-shot learning}\nwith careful analysis does. (iii) We explore the underlying cause of the\nvulnerability by analyzing the inherent issues in tokenization and attention\nmatrices.", "AI": {"tldr": "This paper investigates how vertical text input affects the performance of large language models (LLMs) in text classification tasks, revealing significant vulnerabilities.", "motivation": "As LLMs are increasingly applied in real-world scenarios, understanding how variations in text formatting, like vertical input, impact their performance is critical, especially for applications involving sensitive information.", "method": "The paper analyzes the performance degradation caused by vertical text input across multiple text classification datasets and examines whether decoder-based LLMs are similarly affected.", "result": "Vertical text input significantly decreases LLM accuracy in text classification. Chain of Thought reasoning provides no benefit in recognizing vertical inputs, while few-shot learning with analysis shows effectiveness.", "conclusion": "The research highlights serious vulnerabilities in LLMs regarding text formatting, particularly vertical alignment, and suggests improvements in tokenization and attention mechanisms.", "key_contributions": ["Identified performance degradation in LLMs due to vertical text input.", "Demonstrated that few-shot learning can mitigate this issue, unlike Chain of Thought reasoning.", "Analyzed the tokenization and attention matrix issues contributing to the vulnerability."], "limitations": "The study focuses on text classification tasks and may not generalize to all LLM applications or other types of text formatting.", "keywords": ["Large Language Models", "Vertical Text Input", "Text Classification", "Chain of Thought Reasoning", "Few-shot Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.05375", "pdf": "https://arxiv.org/pdf/2411.05375.pdf", "abs": "https://arxiv.org/abs/2411.05375", "title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking", "authors": ["Mubashara Akhtar", "Michael Schlichtkrull", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted at TACL", "summary": "Current automated fact-checking (AFC) approaches typically evaluate evidence\neither implicitly via the predicted verdicts or through exact matches with\npredefined closed knowledge sources, such as Wikipedia. However, these methods\nare limited due to their reliance on evaluation metrics originally designed for\nother purposes and constraints from closed knowledge sources. In this work, we\nintroduce\n\\textbf{\\textcolor{skyblue}{Ev\\textsuperscript{2}}\\textcolor{orangebrown}{R}}\nwhich combines the strengths of reference-based evaluation and verdict-level\nproxy scoring. Ev\\textsuperscript{2}R jointly assesses how well the evidence\naligns with the gold references and how reliably it supports the verdict,\naddressing the shortcomings of prior methods. We evaluate\nEv\\textsuperscript{2}R against three types of evidence evaluation approaches:\nreference-based, proxy-reference, and reference-less baselines. Assessments\nagainst human ratings and adversarial tests demonstrate that\nEv\\textsuperscript{2}R consistently outperforms existing scoring approaches in\naccuracy and robustness. It achieves stronger correlation with human judgments\nand greater robustness to adversarial perturbations, establishing it as a\nreliable metric for evidence evaluation in AFC.\\footnote{Code is available at\n\\href{https://github.com/mubasharaak/fc-evidence-evaluation}{https://github.com/mubasharaak/fc-evidence-evaluation}.}", "AI": {"tldr": "Introduction of Ev²R, a new metric for automated fact-checking that improves evidence evaluation.", "motivation": "Address the limitations of current automated fact-checking methods that rely heavily on predefined knowledge sources and traditional evaluation metrics.", "method": "Ev²R combines reference-based evaluation with verdict-level proxy scoring to better assess the alignment and support of evidence.", "result": "Ev²R consistently outperforms existing evaluation methods in accuracy and robustness, showing stronger correlation with human judgments and greater resilience to adversarial tests.", "conclusion": "Ev²R establishes itself as a reliable metric for evidence evaluation in automated fact-checking contexts.", "key_contributions": ["Introduction of a hybrid evaluation metric combining reference-based and proxy scoring approaches.", "Demonstrated superior performance against human ratings and adversarial tests compared to existing methods.", "Code is made publicly available for further research and application."], "limitations": "", "keywords": ["automated fact-checking", "evidence evaluation", "machine learning", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2411.17993", "pdf": "https://arxiv.org/pdf/2411.17993.pdf", "abs": "https://arxiv.org/abs/2411.17993", "title": "DRS: Deep Question Reformulation With Structured Output", "authors": ["Zhecheng Li", "Yiwei Wang", "Bryan Hooi", "Yujun Cai", "Nanyun Peng", "Kai-Wei Chang"], "categories": ["cs.CL"], "comment": "Findings of the Association for Computational Linguistics (ACL 2025)", "summary": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%.", "AI": {"tldr": "DRS enhances LLMs' ability to reformulate unanswerable questions by combining LLMs with a DFS-based algorithm to iteratively explore entity combinations.", "motivation": "Address the limitation of LLMs in reformulating unanswerable questions due to insufficient user understanding.", "method": "DRS (Deep Question Reformulation with Structured Output) combines LLM strengths with a DFS-based algorithm for iterative exploration of potential entity combinations and constraining outputs using predefined entities.", "result": "DRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, and the performance of the Gemma2-9B model from 26.35% to 56.75%.", "conclusion": "The structured approach significantly enhances the question reformulation capabilities of LLMs.", "key_contributions": ["Introduction of DRS, a zero-shot method for question reformulation", "Significant improvement in reformulation accuracy for various LLMs", "Utilizes a structured output approach combined with entity exploration"], "limitations": "", "keywords": ["question reformulation", "large language models", "DFS-based algorithm"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.03920", "pdf": "https://arxiv.org/pdf/2412.03920.pdf", "abs": "https://arxiv.org/abs/2412.03920", "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios", "authors": ["Xiachong Feng", "Longxu Dou", "Ella Li", "Qinghao Wang", "Haochuan Wang", "Yu Guo", "Chang Ma", "Lingpeng Kong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities, as well as their interactions\nand synergistic effects on decision-making. The evaluation protocol covers both\ngame-agnostic and game-specific metrics for assessing agent performance.\nAdditionally, we analyze the performance of current social agents across\nvarious game scenarios. By reflecting on the current research and identifying\nfuture research directions, this survey provides insights to advance the\ndevelopment and evaluation of social agents in game-theoretic scenarios.", "AI": {"tldr": "This paper surveys current research on LLM-based social agents in game-theoretic scenarios, organizing findings into three components: Game Framework, Social Agent, and Evaluation Protocol.", "motivation": "To fill the gap in the literature regarding a comprehensive overview of LLM-based social agents in game-theoretic contexts.", "method": "Systematic review of existing research, categorized into core components: Game Framework, Social Agent, and Evaluation Protocol.", "result": "The survey reveals the structure of existing research, evaluates current agent performance, and identifies areas for future exploration in the context of game-theoretic scenarios.", "conclusion": "The findings offer insights that can further the development and performance evaluation of social agents in games, paving the way for advancements in this area.", "key_contributions": ["Comprehensive survey of LLM-based social agents in game theory.", "Categorization of research into Game Framework, Social Agent, and Evaluation Protocol.", "Identification of future research directions in the area."], "limitations": "", "keywords": ["Large Language Models", "Social Agents", "Game Theory", "Evaluation Protocols", "Research Survey"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.03441", "pdf": "https://arxiv.org/pdf/2501.03441.pdf", "abs": "https://arxiv.org/abs/2501.03441", "title": "Finding A Voice: Exploring the Potential of African American Dialect and Voice Generation for Chatbots", "authors": ["Sarah E. Finch", "Ellie S. Paek", "Ikseon Choi", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "As chatbots become integral to daily life, personalizing systems is key for\nfostering trust, engagement, and inclusivity. This study examines how\nlinguistic similarity affects chatbot performance, focusing on integrating\nAfrican American English (AAE) into virtual agents to better serve the African\nAmerican community. We develop text-based and spoken chatbots using large\nlanguage models and text-to-speech technology, then evaluate them with AAE\nspeakers against standard English chatbots. Our results show that while\ntext-based AAE chatbots often underperform, spoken chatbots benefit from an\nAfrican American voice and AAE elements, improving performance and preference.\nThese findings underscore the complexities of linguistic personalization and\nthe dynamics between text and speech modalities, highlighting technological\nlimitations that affect chatbots' AA speech generation and pointing to\npromising future research directions.", "AI": {"tldr": "This study analyzes how incorporating African American English (AAE) into chatbots affects their performance and user engagement, revealing advantages in spoken vs. text-based implementations.", "motivation": "Personalizing chatbots is crucial for enhancing trust, engagement, and inclusivity, especially for the African American community.", "method": "We developed text-based and spoken chatbots utilizing large language models and text-to-speech technology, then evaluated them with AAE speakers compared to standard English chatbots.", "result": "Text-based AAE chatbots tend to underperform, while spoken chatbots that use an African American voice and AAE elements show improved performance and user preference.", "conclusion": "Personalizing chatbot systems with AAE can enhance performance in spoken modalities but highlights limitations in text-based approaches.", "key_contributions": ["Integration of AAE in chatbot systems", "Comparison of text vs spoken chatbot performance", "Insights into linguistic personalization for chatbots"], "limitations": "Text-based AAE chatbots underperform compared to spoken variants; technological limitations affect speech generation in AAE.", "keywords": ["chatbots", "African American English", "linguistic personalization", "machine learning", "user engagement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.05986", "pdf": "https://arxiv.org/pdf/2502.05986.pdf", "abs": "https://arxiv.org/abs/2502.05986", "title": "Preventing Rogue Agents Improves Multi-Agent Collaboration", "authors": ["Ohav Barbi", "Ori Yoran", "Mor Geva"], "categories": ["cs.CL", "cs.MA"], "comment": "Accepted as a spotlight to REALM (First Workshop for Research on\n  Agent Language Models) at ACL 2025", "summary": "Multi-agent systems, where specialized agents collaborate to solve a shared\ntask hold great potential, from increased modularity to simulating complex\nenvironments. However, they also have a major caveat -- a single agent can\ncause the entire system to fail. Consider a simple game where the knowledge to\nsolve the task is distributed between agents, which share information in a\ncommunication channel. At each round, any of the agents can terminate the game\nand make the final prediction, even if they are uncertain about the outcome of\ntheir action. Detection of such rogue agents before they act may prevent the\nsystem's failure. In this work, we propose to monitor agents during action\nprediction and intervene when a future error is likely to occur. To test our\napproach, we introduce WhoDunitEnv, a multi-agent collaboration environment\nthat allows modular control over task complexity and communication structure.\nExperiments on WhoDunitEnv, code generation tasks and the GovSim environment\nfor resource sustainability show that our approach leads to substantial\nperformance gains up to 17.4%, 2.5% and 20%, respectively. Thorough analysis\nshows that our monitors successfully identify critical points of agent\nconfusion and our interventions effectively stop agent errors from propagating.", "AI": {"tldr": "Proposes monitoring agents in multi-agent systems to prevent failures from rogue agents, tested in a new environment called WhoDunitEnv.", "motivation": "To address the risk of single rogue agents causing failures in multi-agent systems that collaborate to solve tasks.", "method": "Introduces a monitoring approach to detect potential errors in agent predictions and intervenes to prevent system failures.", "result": "Demonstrates performance gains of up to 20% in task completion by implementing the monitoring approach in various environments, including WhoDunitEnv.", "conclusion": "The proposed monitoring system effectively identifies agent confusion points and prevents error propagation, enhancing collaboration and performance in multi-agent settings.", "key_contributions": ["Introduction of WhoDunitEnv for testing multi-agent collaboration", "Development of a monitoring approach for agent actions", "Demonstrated significant performance improvements in multiple environments"], "limitations": "", "keywords": ["multi-agent systems", "agent collaboration", "monitoring", "WhoDunitEnv", "performance"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2502.07057", "pdf": "https://arxiv.org/pdf/2502.07057.pdf", "abs": "https://arxiv.org/abs/2502.07057", "title": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih Gümüş", "Sercan Karakaş", "Banu Diri", "Savaş Yıldırım"], "categories": ["cs.CL", "68T50, 68T10", "I.2.7; I.2.6; H.3.1"], "comment": null, "summary": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices.", "AI": {"tldr": "This paper presents a novel framework for evaluating tokenization strategies in NLP, focusing on morphologically rich and low-resource languages, using a dataset from the MMLU benchmark to assess various metrics related to tokenization effectiveness.", "motivation": "To address the challenges faced by tokenization in morphologically rich and low-resource languages, which affect the performance of large language models (LLMs).", "method": "The framework evaluates tokenizers based on five metrics: vocabulary size, token count, processing time, language-specific token percentages (%TR), and token purity. These metrics are assessed on a Turkish dataset of 6,200 questions from the MMLU benchmark.", "result": "The study reveals that %TR strongly correlates with downstream performance and model accuracy, while larger model parameters do not guarantee better tokenization quality.", "conclusion": "This framework sets a new standard for developing tokenization methods optimized for complex languages and emphasizes the need for tailored strategies.", "key_contributions": ["Introduction of the %TR metric as critical for tokenization evaluation.", "Demonstrated that better tokenization does not solely rely on model parameter size.", "Provided a structured approach for evaluating morphological tokenization strategies."], "limitations": "", "keywords": ["tokenization", "NLP", "language models", "morphologically rich languages", "evaluation framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.10871", "pdf": "https://arxiv.org/pdf/2502.10871.pdf", "abs": "https://arxiv.org/abs/2502.10871", "title": "Layerwise Recall and the Geometry of Interwoven Knowledge in LLMs", "authors": ["Ge Lei", "Samuel J. Cooper"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study explores how large language models (LLMs) encode interwoven\nscientific knowledge, using chemical elements and LLaMA-series models as a case\nstudy. We identify a 3D spiral structure in the hidden states that aligns with\nthe conceptual structure of the periodic table, suggesting that LLMs can\nreflect the geometric organization of scientific concepts learned from text.\nLinear probing reveals that middle layers encode continuous, overlapping\nattributes that enable indirect recall, while deeper layers sharpen categorical\ndistinctions and incorporate linguistic context. These findings suggest that\nLLMs represent symbolic knowledge not as isolated facts, but as structured\ngeometric manifolds that intertwine semantic information across layers. We hope\nthis work inspires further exploration of how LLMs represent and reason about\nscientific knowledge, particularly in domains such as materials science.", "AI": {"tldr": "This study examines how large language models (LLMs) encode scientific knowledge through a case study on chemical elements, revealing a 3D spiral structure in their hidden states.", "motivation": "To understand how LLMs encode and represent scientific knowledge, particularly in relation to the periodic table of chemical elements.", "method": "The research employs linear probing to analyze the hidden states of LLaMA-series models, focusing on how different layers reflect the geometric organization of scientific concepts.", "result": "The study finds that LLMs encode symbolic knowledge as structured geometric manifolds, with middle layers encoding overlapping attributes for indirect recall and deeper layers emphasizing categorical distinctions along with linguistic context.", "conclusion": "LLMs represent scientific knowledge through intricate geometric structures rather than isolated facts, with implications for future research in scientific reasoning and representation in LLMs.", "key_contributions": ["Identification of a 3D spiral structure in LLM hidden states related to the periodic table.", "Demonstration of how middle layers encode continuous, overlapping attributes.", "Insights into the representation of symbolic knowledge in LLMs as geometric manifolds."], "limitations": "", "keywords": ["Large Language Models", "Scientific Knowledge Representation", "Geometric Structures"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11476", "pdf": "https://arxiv.org/pdf/2502.11476.pdf", "abs": "https://arxiv.org/abs/2502.11476", "title": "FastMCTS: A Simple Sampling Strategy for Data Synthesis", "authors": ["Peiji Li", "Kai Lv", "Yunfan Shao", "Yichuan Ma", "Linyang Li", "Xiaoqing Zheng", "Xipeng Qiu", "Qipeng Guo"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Synthetic high-quality multi-step reasoning data can significantly enhance\nthe performance of large language models on various tasks. However, most\nexisting methods rely on rejection sampling, which generates trajectories\nindependently and suffers from inefficiency and imbalanced sampling across\nproblems of varying difficulty. In this work, we introduce FastMCTS, an\ninnovative data synthesis strategy inspired by Monte Carlo Tree Search.\nFastMCTS provides a more efficient sampling method for multi-step reasoning\ndata, offering step-level evaluation signals and promoting balanced sampling\nacross problems of different difficulty levels. Experiments on both English and\nChinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more\ncorrect reasoning paths compared to rejection sampling as the number of\ngenerated tokens scales up. Furthermore, under comparable synthetic data\nbudgets, models trained on FastMCTS-generated data outperform those trained on\nrejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight\nsampling strategy, FastMCTS offers a practical and efficient alternative for\nsynthesizing high-quality reasoning data. Our code will be released soon.", "AI": {"tldr": "FastMCTS is a data synthesis strategy for generating high-quality multi-step reasoning data for large language models, addressing inefficiencies of rejection sampling.", "motivation": "To enhance the performance of large language models on reasoning tasks by providing a more efficient sampling method for multi-step reasoning data.", "method": "Introduces FastMCTS, inspired by Monte Carlo Tree Search, that offers step-level evaluation signals and promotes balanced sampling across problems of varying difficulty levels.", "result": "FastMCTS generates over 30% more correct reasoning paths than rejection sampling and outperforms models trained on rejection sampling data by 3.9% across multiple benchmarks.", "conclusion": "FastMCTS is a practical and efficient alternative for synthesizing high-quality reasoning data, with code to be released soon.", "key_contributions": ["Introduction of FastMCTS for efficient multi-step reasoning data synthesis", "Demonstrated significant improvements in correct reasoning paths compared to rejection sampling", "Outperformance of models trained on FastMCTS data over those trained on rejection sampling data"], "limitations": "", "keywords": ["multi-step reasoning", "data synthesis", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.12788", "pdf": "https://arxiv.org/pdf/2502.12788.pdf", "abs": "https://arxiv.org/abs/2502.12788", "title": "Commonsense Reasoning in Arab Culture", "authors": ["Abdelrahman Sadallah", "Junior Cedric Tonga", "Khalid Almubarak", "Saeed Almheiri", "Farah Atif", "Chatrine Qwaider", "Karima Kadaoui", "Sara Shatnawi", "Yaser Alesh", "Fajri Koto"], "categories": ["cs.CL"], "comment": "ACL 2025 - Main", "summary": "Despite progress in Arabic large language models, such as Jais and AceGPT,\ntheir evaluation on commonsense reasoning has largely relied on\nmachine-translated datasets, which lack cultural depth and may introduce\nAnglocentric biases. Commonsense reasoning is shaped by geographical and\ncultural contexts, and existing English datasets fail to capture the diversity\nof the Arab world. To address this, we introduce ArabCulture, a commonsense\nreasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13\ncountries across the Gulf, Levant, North Africa, and the Nile Valley. The\ndataset was built from scratch by engaging native speakers to write and\nvalidate culturally relevant questions for their respective countries.\nArabCulture spans 12 daily life domains with 54 fine-grained subtopics,\nreflecting various aspects of social norms, traditions, and everyday\nexperiences. Zero-shot evaluations show that open-weight language models with\nup to 32B parameters struggle to comprehend diverse Arab cultures, with\nperformance varying across regions. These findings highlight the need for more\nculturally aware models and datasets tailored to the Arabic-speaking world.", "AI": {"tldr": "ArabCulture is a commonsense reasoning dataset in Modern Standard Arabic, designed to enhance understanding of diverse Arab cultures, highlighting the shortcomings of existing English datasets in capturing cultural contexts.", "motivation": "The paper addresses the lack of culturally relevant commonsense reasoning datasets for Arabic large language models, which have primarily relied on machine-translated English datasets that introduce biases.", "method": "A dataset was created from scratch by engaging native speakers to write and validate questions relevant to their cultures, covering 13 countries and spanning 12 daily life domains.", "result": "Zero-shot evaluations demonstrate that open-weight language models with up to 32B parameters perform poorly in understanding diverse Arab cultures, with variable performance across regions.", "conclusion": "The results underscore the necessity for culturally informed models and datasets specifically designed for the Arabic-speaking community.", "key_contributions": ["Introduction of the ArabCulture dataset", "Engagement of native speakers for dataset creation", "Demonstration of performance issues in existing models with cultural context."], "limitations": "The dataset focus is limited to 13 Arab countries and may not encompass every cultural nuance.", "keywords": ["commonsense reasoning", "Arabic language models", "cultural diversity", "dataset creation", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.12829", "pdf": "https://arxiv.org/pdf/2502.12829.pdf", "abs": "https://arxiv.org/abs/2502.12829", "title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan", "authors": ["Mukhammed Togmanov", "Nurdaulet Mukhituly", "Diana Turmakhan", "Jonibek Mansurov", "Maiya Goloburda", "Akhmed Sakip", "Zhuohan Xie", "Yuxia Wang", "Bekassyl Syzdykov", "Nurkhan Laiyk", "Alham Fikri Aji", "Ekaterina Kochmar", "Preslav Nakov", "Fajri Koto"], "categories": ["cs.CL"], "comment": null, "summary": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance.", "AI": {"tldr": "Introduction of the first MMLU-style dataset for Kazakh language, KazMMLU, aimed at improving NLP for underrepresented languages.", "motivation": "To address the underrepresentation of Kazakhstan's culture and language in NLP and to promote development in Kazakh language processing.", "method": "Creation of KazMMLU dataset consisting of 23,000 questions across various educational subjects, validated by native speakers.", "result": "Evaluation indicates significant performance gaps in existing multilingual models for Kazakh and Russian, highlighting the need for dedicated resources.", "conclusion": "The KazMMLU dataset aims to catalyze further research and development in Kazakh-language LLMs, with data and code to be released upon acceptance.", "key_contributions": ["Introduction of the KazMMLU dataset for Kazakh language", "Focus on educational level relevance in data", "Evaluation of multilingual models' performance in Kazakh and Russian"], "limitations": "Performance of existing models is limited, indicating a need for improved algorithms specifically for Kazakh language.", "keywords": ["Kazakh language", "natural language processing", "multilingual models", "KazMMLU", "education"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2502.14642", "pdf": "https://arxiv.org/pdf/2502.14642.pdf", "abs": "https://arxiv.org/abs/2502.14642", "title": "How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation", "authors": ["Rui Li", "Heming Xia", "Xinfeng Yuan", "Qingxiu Dong", "Lei Sha", "Wenjie Li", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior.", "AI": {"tldr": "This paper introduces BehaviorChain, a benchmark for assessing LLMs' capabilities in simulating continuous human behavior through diverse, persona-based behavior chains.", "motivation": "To address the limitation of current evaluations focusing mainly on dialogue simulation, neglecting human behavior which is essential for creating effective digital twins.", "method": "BehaviorChain includes 15,846 diverse behaviors across 1,001 personas, integrating persona metadata into LLMs for evaluating their ability to simulate human behavior in dynamic contexts.", "result": "Evaluation results show that even leading LLMs struggle to accurately simulate continuous human behavior when tested against the BehaviorChain benchmark.", "conclusion": "The findings highlight the shortcomings of state-of-the-art LLMs in simulating human behavior and underscore the need for improved evaluation frameworks.", "key_contributions": ["Introduction of BehaviorChain benchmark for LLM evaluation.", "Development of 15,846 persona-based diverse behavior simulations.", "Demonstration of LLMs' limitations in accurately simulating continuous behavior."], "limitations": "Limited to the evaluation of continuous behavior simulation; does not encompass other aspects of LLM performance.", "keywords": ["LLMs", "digital twins", "human behavior simulation", "BehaviorChain", "persona-based evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14916", "pdf": "https://arxiv.org/pdf/2502.14916.pdf", "abs": "https://arxiv.org/abs/2502.14916", "title": "MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs", "authors": ["Xinxin You", "Xien Liu", "Xue Yang", "Ziyi Wang", "Ji Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "We have decided to withdraw this manuscript in order to allow for\n  further revisions and additional experiments", "summary": "The task of automatically coding the International Classification of Diseases\n(ICD) in the medical field has been well-established and has received much\nattention. Automatic coding of the ICD in the medical field has been successful\nin English but faces challenges when dealing with Chinese electronic medical\nrecords (EMRs). The first issue lies in the difficulty of extracting disease\ncode-related information from Chinese EMRs, primarily due to the concise\nwriting style and specific internal structure of the EMRs. The second problem\nis that previous methods have failed to leverage the disease-based multi-axial\nknowledge and lack of association with the corresponding clinical evidence.\nThis paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge\nwith Evidence verification in ICD coding for Chinese EMRs. Initially, we\nidentify candidate codes for the diagnosis and categorize each of them into\nknowledge under four coding axes.Subsequently, we retrieve corresponding\nclinical evidence from the comprehensive content of EMRs and filter credible\nevidence through a scoring model. Finally, to ensure the validity of the\ncandidate code, we propose an inference module based on the masked language\nmodeling strategy. This module verifies that all the axis knowledge associated\nwith the candidate code is supported by evidence and provides recommendations\naccordingly. To evaluate the performance of our framework, we conduct\nexperiments using a large-scale Chinese EMR dataset collected from various\nhospitals. The experimental results demonstrate that MKE-Coder exhibits\nsignificant superiority in the task of automatic ICD coding based on Chinese\nEMRs. In the practical evaluation of our method within simulated real coding\nscenarios, it has been demonstrated that our approach significantly aids coders\nin enhancing both their coding accuracy and speed.", "AI": {"tldr": "This paper presents MKE-Coder, a framework for automatic ICD coding in Chinese electronic medical records (EMRs), addressing challenges in code extraction and evidence verification.", "motivation": "To improve automated ICD coding for Chinese EMRs, which face difficulties in extracting disease-related information and lack evidence correlation.", "method": "Introduces MKE-Coder, which identifies candidate ICD codes, retrieves clinical evidence, and verifies code validity through a masked language modeling inference module.", "result": "MKE-Coder shows superior performance in automatic ICD coding on Chinese EMRs, enhancing coders' accuracy and efficiency in real coding scenarios.", "conclusion": "The proposed framework significantly aids in ICD coding by leveraging multi-axial knowledge and reinforcing code validation with credible clinical evidence.", "key_contributions": ["Introduction of the MKE-Coder framework for ICD coding in Chinese EMRs.", "Use of multi-axial knowledge for disease code extraction.", "Validation through a masked language model to ensure accuracy of selected codes."], "limitations": "The manuscript has been withdrawn for further revisions and additional experiments; hence, it may not include final results or methodologies.", "keywords": ["ICD coding", "Chinese EMRs", "machine learning", "medical records", "evidence verification"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.15090", "pdf": "https://arxiv.org/pdf/2502.15090.pdf", "abs": "https://arxiv.org/abs/2502.15090", "title": "Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans", "authors": ["Masha Fedzechkina", "Eleonora Gualdoni", "Sinead Williamson", "Katherine Metcalf", "Skyler Seto", "Barry-John Theobald"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern large language models (LLMs) achieve impressive performance on some\ntasks, while exhibiting distinctly non-human-like behaviors on others. This\nraises the question of how well the LLM's learned representations align with\nhuman representations. In this work, we introduce a novel approach to study\nrepresentation alignment: we adopt a method from research on activation\nsteering to identify neurons responsible for specific concepts (e.g., ''cat'')\nand then analyze the corresponding activation patterns. We find that LLM\nrepresentations captured this way closely align with human representations\ninferred from behavioral data, matching inter-human alignment levels. Our\napproach significantly outperforms the alignment captured by word embeddings,\nwhich have been the focus of prior work on human-LLM alignment. Additionally,\nour approach enables a more granular view of how LLMs represent concepts -- we\nshow that LLMs organize concepts in a way that mirrors human concept\norganization.", "AI": {"tldr": "This paper investigates how well large language models (LLMs) align with human representations by using activation steering to analyze neurons linked to specific concepts, revealing a close alignment between LLMs and humans.", "motivation": "To explore the alignment of learned representations in LLMs with human representations and understand the organizational patterns of concepts in LLMs.", "method": "The authors adopt a method from activation steering to identify specific neurons for concepts and analyze activation patterns, allowing them to compare LLM representations with human representations derived from behavioral data.", "result": "The study finds that LLM representations align closely with human representations, achieving inter-human alignment levels and surpassing previous methods based on word embeddings.", "conclusion": "The findings suggest that LLMs organize concepts similarly to humans, providing insights into the alignment of machine and human representations.", "key_contributions": ["Introduction of a novel representation alignment approach using activation steering", "Demonstration of LLMs' alignment with human representations surpassing previous word embedding methods", "Insights into the organization of concepts in LLMs mirroring human organization"], "limitations": "", "keywords": ["large language models", "human representation alignment", "activation steering", "concept organization", "neural networks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.15639", "pdf": "https://arxiv.org/pdf/2502.15639.pdf", "abs": "https://arxiv.org/abs/2502.15639", "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models", "authors": ["Anirudh Sundar", "Sinead Williamson", "Katherine Metcalf", "Barry-John Theobald", "Skyler Seto", "Masha Fedzechkina"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages", "summary": "Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.", "AI": {"tldr": "The paper explores model interventions as a data-efficient method to enhance cross-lingual alignment in multilingual large language models (mLLMs).", "motivation": "Improving cross-lingual performance in mLLMs without requiring extensive fine-tuning and large datasets.", "method": "Analysis of a model intervention technique, specifically finding experts, to manipulate model activations in mLLMs, followed by introspection of the embedding space pre- and post-manipulation.", "result": "The manipulation of mLLM activations effectively enhances cross-lingual alignment in the embedding space, resulting in improved retrieval task performance with up to 2x top-1 accuracy.", "conclusion": "Model interventions can serve as a potent alternative to fine-tuning by providing significant performance boosts in cross-lingual tasks with fewer resources.", "key_contributions": ["Demonstration of the effectiveness of model interventions for cross-lingual alignment.", "Identification of key neurons for manipulation in mLLMs for specific languages.", "Quantification of performance improvements in retrieval tasks post-manipulation."], "limitations": "The study primarily focuses on one type of model intervention, leaving other potential methods unexplored.", "keywords": ["multilingual large language models", "cross-lingual alignment", "model interventions", "neuron manipulation", "embedding space"], "importance_score": 9, "read_time_minutes": 34}}
{"id": "2502.19545", "pdf": "https://arxiv.org/pdf/2502.19545.pdf", "abs": "https://arxiv.org/abs/2502.19545", "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in Product QA Agents", "authors": ["Ashley Lewis", "Michael White", "Jing Liu", "Toshiaki Koike-Akino", "Kieran Parsons", "Ye Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination (generating false information) and the high cost\nof proprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations.", "AI": {"tldr": "Proposes a retrieval-augmented QA pipeline to mitigate hallucination in LLMs for customer support using synthetic data and self-training techniques.", "motivation": "To address the challenges of hallucination and high costs associated with proprietary LLMs in customer support applications.", "method": "Developed a retrieval-augmented QA pipeline and conducted experiments comparing synthetic data generated by LLMs with crowdsourced data, as well as analyzing self-training against knowledge distillation.", "result": "Synthetic data improved hallucination reduction in finetuned models; self-training was comparable to knowledge distillation with potential bias identified.", "conclusion": "Cost-effective QA systems can leverage synthetic data and self-training with open-source models, minimizing the need for expensive proprietary solutions.", "key_contributions": ["Introduction of a retrieval-augmented QA pipeline", "Demonstrated that synthetic data is superior to crowdsourced data in reducing hallucination", "Analysis of self-training achieving hallucination reduction comparable to knowledge distillation"], "limitations": "", "keywords": ["Large Language Models", "customer support", "hallucination", "retrieval-augmented QA", "self-training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.00024", "pdf": "https://arxiv.org/pdf/2503.00024.pdf", "abs": "https://arxiv.org/abs/2503.00024", "title": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with LLM-based Manipulation Checks", "authors": ["Yanran Chen", "Steffen Eger"], "categories": ["cs.CL"], "comment": "ACL 2025 Camera-ready", "summary": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases, human\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze whether 11 LLMs behave\nlike humans in the same scenario, finding that while LLMs generally mirror\nhuman patterns, they struggle to capture nuanced emotional effects in\nindividual judgments.", "AI": {"tldr": "This paper explores the role of emotions in argument convincingness using a dynamic framework and human evaluations across various languages and topics.", "motivation": "The underexplored role of emotions in argument convincingness within the NLP community.", "method": "A dynamic framework inspired by psychology manipulation checks was employed, utilizing LLM-based manipulation checks to assess perceived emotional intensity's influence on convincingness.", "result": "Human judgments of convincingness usually remain unchanged despite variations in emotional intensity, and when emotions do affect convincingness, they tend to enhance it. LLMs generally replicate human judgment patterns but struggle with nuanced emotional effects.", "conclusion": "Emotions impact convincingness more often positively, revealing both the limitations of current NLP models in understanding emotions and the need for further exploration in this area.", "key_contributions": ["Introducing a dynamic framework for analyzing emotional influence on convincingness", "Conducting human evaluations across different languages and text domains", "Comparison of human and LLMs' responses to emotional intensity in argument evaluation."], "limitations": "The study found that emotional influence can vary widely and might not be easily captured by LLMs due to nuanced human emotional interpretation.", "keywords": ["emotions", "argument convincingness", "language models", "human evaluation", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.01781", "pdf": "https://arxiv.org/pdf/2503.01781.pdf", "abs": "https://arxiv.org/abs/2503.01781", "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models", "authors": ["Meghana Rajeev", "Rajkumar Ramamurthy", "Prapti Trivedi", "Vikas Yadav", "Oluwanifemi Bamgbose", "Sathwik Tejaswi Madhusudan", "James Zou", "Nazneen Rajani"], "categories": ["cs.CL"], "comment": "Accepted to CoLM 2025", "summary": "We investigate the robustness of reasoning models trained for step-by-step\nproblem solving by introducing query-agnostic adversarial triggers - short,\nirrelevant text that, when appended to math problems, systematically mislead\nmodels to output incorrect answers without altering the problem's semantics. We\npropose CatAttack, an automated iterative attack pipeline for generating\ntriggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully\ntransfer them to more advanced reasoning target models like DeepSeek R1 and\nDeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the\nlikelihood of the target model generating an incorrect answer. For example,\nappending, \"Interesting fact: cats sleep most of their lives,\" to any math\nproblem leads to more than doubling the chances of a model getting the answer\nwrong. Our findings highlight critical vulnerabilities in reasoning models,\nrevealing that even state-of-the-art models remain susceptible to subtle\nadversarial inputs, raising security and reliability concerns. The CatAttack\ntriggers dataset with model responses is available at\nhttps://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers.", "AI": {"tldr": "The paper proposes CatAttack, a method to generate adversarial triggers that mislead reasoning models in math problem solving.", "motivation": "To investigate the vulnerabilities of reasoning models in step-by-step problem solving by introducing adversarial triggers.", "method": "An automated iterative attack pipeline (CatAttack) generates query-agnostic adversarial triggers using a cheaper proxy model and applies them to advanced reasoning models, transferring the attacks successfully to provoke incorrect answers.", "result": "The attack increases the likelihood of generating incorrect answers by over 300%. For instance, appending certain irrelevant text to math problems significantly worsens model performance.", "conclusion": "The findings reveal critical vulnerabilities in state-of-the-art reasoning models, highlighting security and reliability concerns in real-world applications.", "key_contributions": ["Introduction of query-agnostic adversarial triggers in reasoning models.", "Development of the CatAttack automated attack pipeline.", "Empirical evidence showing the susceptibility of advanced reasoning models to subtle adversarial inputs."], "limitations": "The study focuses only on specific reasoning models and may not generalize to all AI systems.", "keywords": ["adversarial attacks", "reasoning models", "machine learning", "vulnerabilities", "security"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2503.05641", "pdf": "https://arxiv.org/pdf/2503.05641.pdf", "abs": "https://arxiv.org/abs/2503.05641", "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning", "authors": ["Justin Chih-Yao Chen", "Sukwon Yun", "Elias Stengel-Eskin", "Tianlong Chen", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The first three authors contributed equally. Project Page:\n  https://symbolic-moe.github.io/", "summary": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting task-level experts\nis often too coarse-grained, as heterogeneous tasks may require different\nexpertise per instance. To enable adaptive instance-level mixing of pre-trained\nLLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free\nMixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to\nselection by emphasizing skills, e.g., algebra in math or molecular biology in\nbiomedical reasoning. We propose a skill-based recruiting strategy that\ndynamically selects the most relevant set of expert LLMs for diverse reasoning\ntasks based on their strengths. Each selected expert then generates its own\nreasoning, resulting in k outputs from k experts, which are then synthesized\ninto a final high-quality response by an aggregator chosen based on its ability\nto integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch strategy that groups instances based on their assigned\nexperts, loading each model only once. This allows us to integrate 16 expert\nmodels on 1 GPU with a time cost comparable to or better than prior multi-agent\nbaselines using 4 GPUs. Through extensive evaluations on diverse benchmarks\n(MMLU-Pro, GPQA, AIME, and MedMCQA), we show that Symbolic-MoE beats strong\nLLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute avg.\ngain of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE\ngeneralizes well to unseen tasks and removes the need for expensive multi-round\ndiscussions, outperforming discussion baselines with less computation.", "AI": {"tldr": "Symbolic-MoE is a Mixture-of-Experts framework that selects LLM experts based on specific skills, improving task performance efficiently.", "motivation": "To improve the selection of pre-trained LLM experts for diverse tasks at an instance level rather than a coarse-grained task level.", "method": "Symbolic-MoE dynamically selects LLMs based on their strengths related to specific skills, generating outputs from multiple experts and synthesizing them using an efficient batching strategy.", "result": "Symbolic-MoE shows significant performance improvements on benchmarks like MMLU-Pro and MedMCQA, outperforming existing multi-agent frameworks while maintaining computational efficiency.", "conclusion": "Instance-level expert selection improves LLM performance and generalization, reducing the need for extensive multi-round discussions and computation.", "key_contributions": ["Introduction of skill-based recruiting strategy for LLM experts", "Implementation of a batch strategy for efficient model loading", "Extensive evaluations demonstrating superior performance on diverse benchmarks"], "limitations": "Potentially high computational overhead if implemented naively without the batching strategy.", "keywords": ["Mixture-of-Experts", "LLM", "skill-based selection", "benchmark evaluation", "computational efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.09516", "pdf": "https://arxiv.org/pdf/2503.09516.pdf", "abs": "https://arxiv.org/abs/2503.09516", "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning", "authors": ["Bowen Jin", "Hansi Zeng", "Zhenrui Yue", "Jinsung Yoon", "Sercan Arik", "Dong Wang", "Hamed Zamani", "Jiawei Han"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "31 pages", "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1.", "AI": {"tldr": "The paper presents Search-R1, a reinforcement learning extension for enhancing large language models' reasoning capabilities by optimizing their interaction with search engines for real-time information retrieval, showing significant improvements in performance across various datasets.", "motivation": "The motivation is to improve the reasoning and text generation capabilities of large language models by enhancing their interaction with search engines during inference for real-time knowledge acquisition.", "method": "The method employs reinforcement learning techniques to enable the LLM to generate multiple search queries during step-by-step reasoning, utilizing multi-turn search interactions with a reward function based on outcomes and token masking for stable training.", "result": "Experiments demonstrate that Search-R1 improves performance by 41% with the Qwen2.5-7B model and by 20% with the Qwen2.5-3B model over several retrieval-augmented generation baselines.", "conclusion": "The findings indicate that the proposed method significantly enhances the reasoning capabilities of LLMs, providing valuable insights into RL optimization, LLM selection, and dynamics of response length in retrieval-augmented contexts.", "key_contributions": ["Introduction of Search-R1 for LLM-assisted reasoning with search engines", "Demonstration of substantial performance improvements over baseline models", "Empirical insights into RL methods and their impact on LLM outputs."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Information Retrieval", "Search Optimization", "Text Generation"], "importance_score": 9, "read_time_minutes": 31}}
{"id": "2503.11858", "pdf": "https://arxiv.org/pdf/2503.11858.pdf", "abs": "https://arxiv.org/abs/2503.11858", "title": "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs", "authors": ["Ivan Kartáč", "Mateusz Lango", "Ondřej Dušek"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate.", "AI": {"tldr": "OpeNLGauge is an open-source NLG evaluation metric using LLMs that provides accurate explanations and outperforms existing models in certain tasks.", "motivation": "To address drawbacks in existing LLM-based evaluation metrics related to proprietary data reliance and lack of explanatory feedback.", "method": "A two-stage ensemble or a small fine-tuned evaluation model that uses open-weight LLMs for NLG assessment.", "result": "OpeNLGauge achieves competitive correlation with human judgments and outperforms state-of-the-art models on some tasks, providing more accurate explanations.", "conclusion": "OpeNLGauge offers a fully reproducible and open-source solution for NLG evaluation, enhancing explainability and generalizability.", "key_contributions": ["Introduction of an open-source evaluation metric", "Providing fine-grained, explanatory feedback", "Achieving state-of-the-art performance on certain tasks"], "limitations": "", "keywords": ["NLG evaluation", "Open-source LLM", "Explainability"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.21544", "pdf": "https://arxiv.org/pdf/2503.21544.pdf", "abs": "https://arxiv.org/abs/2503.21544", "title": "SWI: Speaking with Intent in Large Language Models", "authors": ["Yuwei Yin", "EunJeong Hwang", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "Code: https://github.com/YuweiYin/SWI", "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for communication and problem-solving. This paper introduces the\nconcept of Speaking with Intent (SWI) in large language models (LLMs), where\nthe explicitly generated intent encapsulates the model's underlying intention\nand provides high-level planning to guide subsequent analysis and action. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on text summarization, multi-task question\nanswering, and mathematical reasoning benchmarks consistently demonstrate the\neffectiveness and generalizability of Speaking with Intent over direct\ngeneration without explicit intent. Further analysis corroborates the\ngeneralizability of SWI under different experimental settings. Moreover, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. The promising results in enhancing LLMs with explicit\nintents pave a new avenue for boosting LLMs' generation and reasoning abilities\nwith cognitive notions.", "AI": {"tldr": "This paper introduces Speaking with Intent (SWI) in large language models, enhancing their reasoning and generation quality by providing explicit cognitive frameworks.", "motivation": "To improve the reasoning capabilities and generation quality of large language models through the introduction of a structured intent framework.", "method": "The authors implemented SWI, which explicitly generates intent for guiding analysis and subsequent actions, and evaluated it through extensive experiments including text summarization and mathematical reasoning benchmarks.", "result": "SWI consistently outperforms direct generation methods in various benchmarks, showing improvements in coherence, effectiveness, and interpretability as verified through human evaluations.", "conclusion": "Introducing explicit intents in LLMs boosts their generative and reasoning capabilities, suggesting a new approach to enhance language models' performance.", "key_contributions": ["Introduction of the concept of Speaking with Intent (SWI) in LLMs", "Demonstration of SWI's effectiveness across multiple benchmarks", "Human evaluations confirming the improved coherence and interpretability of outputs."], "limitations": "", "keywords": ["Large Language Models", "Speaking with Intent", "Cognitive Framework", "Reasoning", "Natural Language Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768.pdf", "abs": "https://arxiv.org/abs/2503.23768", "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to COLM 2025", "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance and\nbeing easily affected by the stroop effect introduced by textual information.\n(ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal\nbenefits in improving font recognition accuracy across different VLMs. (iii)\nAttention analysis sheds light on the inherent limitations of VLMs in capturing\nsemantic features.", "AI": {"tldr": "This paper investigates the capability of Vision-Language Models (VLMs) in recognizing fonts through a new dataset and evaluates their performance under varying conditions.", "motivation": "To assess whether modern VLMs can effectively recognize fonts, given their multimodal capabilities and applications in real-world design contexts.", "method": "Introduced the Font Recognition Benchmark (FRB), consisting of 15 fonts in easy and hard versions, and evaluated various VLMs' font recognition tasks on this dataset.", "result": "Evaluation revealed that current VLMs have limited font recognition capabilities, significantly affected by a stroop effect, and show minimal improvement with few-shot learning and CoT prompting.", "conclusion": "VLMs struggle with font recognition, highlighting their limitations in capturing semantic features, particularly under challenges like the stroop effect.", "key_contributions": ["Introduction of the Font Recognition Benchmark (FRB) dataset for font recognition testing.", "Demonstration of limited effectiveness of VLMs in font recognition tasks compared to expectations.", "Analysis of attention mechanisms revealing VLMs' shortcomings in capturing semantic information."], "limitations": "VLMs demonstrate inadequate performance in fine-grained font recognition tasks and struggle with certain cognitive challenges.", "keywords": ["Vision-Language Models", "font recognition", "dataset", "few-shot learning", "Chain-of-Thought prompting"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.01216", "pdf": "https://arxiv.org/pdf/2504.01216.pdf", "abs": "https://arxiv.org/abs/2504.01216", "title": "Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models", "authors": ["Feng Chen", "Dror Ben-Zeev", "Gillian Sparks", "Arya Kadakia", "Trevor Cohen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical\nsettings, presenting opportunities for automated detection to identify\npatients. This study evaluates natural language processing approaches for\ndetecting PTSD from clinical interview transcripts. We compared general and\nmental health-specific transformer models (BERT/RoBERTa), embedding-based\nmethods (SentenceBERT/LLaMA), and large language model prompting strategies\n(zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset.\nDomain-specific end-to-end models significantly outperformed general models\n(Mental-RoBERTa AUPRC=0.675+/-0.084 vs. RoBERTa-base 0.599+/-0.145).\nSentenceBERT embeddings with neural networks achieved the highest overall\nperformance (AUPRC=0.758+/-0.128). Few-shot prompting using DSM-5 criteria\nyielded competitive results with two examples (AUPRC=0.737). Performance varied\nsignificantly across symptom severity and comorbidity status with depression,\nwith higher accuracy for severe PTSD cases and patients with comorbid\ndepression. Our findings highlight the potential of domain-adapted embeddings\nand LLMs for scalable screening while underscoring the need for improved\ndetection of nuanced presentations and offering insights for developing\nclinically viable AI tools for PTSD assessment.", "AI": {"tldr": "The study evaluates NLP methods for detecting PTSD from clinical transcripts, showing that domain-specific models and SentenceBERT outperform general models.", "motivation": "To address the underdiagnosis of PTSD in clinical settings by utilizing automated detection methods.", "method": "The study compared transformer models (BERT/RoBERTa), embedding-based methods (SentenceBERT/LLaMA), and prompting strategies using the DAIC-WOZ dataset.", "result": "Domain-specific models outperformed general ones (Mental-RoBERTa AUPRC=0.675 vs. RoBERTa-base 0.599). SentenceBERT with neural networks achieved the highest performance (AUPRC=0.758).", "conclusion": "The findings suggest domain-adapted models and LLMs can enhance PTSD screening, but there is a need for better detection of nuanced cases.", "key_contributions": ["Domain-specific models significantly improve PTSD detection accuracy.", "High performance of SentenceBERT embeddings with neural networks.", "Insights for developing clinically viable AI tools for PTSD assessment."], "limitations": "Performance varied across symptom severity and comorbidity status; further improvements needed for nuanced cases.", "keywords": ["PTSD", "natural language processing", "transformer models", "machine learning", "clinical assessment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.03022", "pdf": "https://arxiv.org/pdf/2504.03022.pdf", "abs": "https://arxiv.org/abs/2504.03022", "title": "The Dual-Route Model of Induction", "authors": ["Sheridan Feucht", "Eric Todd", "Byron Wallace", "David Bau"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "43 pages, 49 figures. Published as a conference paper at COLM 2025.\n  Code and data at https://dualroute.baulab.info", "summary": "Prior work on in-context copying has shown the existence of induction heads,\nwhich attend to and promote individual tokens during copying. In this work we\ndiscover a new type of induction head: concept-level induction heads, which\ncopy entire lexical units instead of individual tokens. Concept induction heads\nlearn to attend to the ends of multi-token words throughout training, working\nin parallel with token-level induction heads to copy meaningful text. We show\nthat these heads are responsible for semantic tasks like word-level\ntranslation, whereas token induction heads are vital for tasks that can only be\ndone verbatim (like copying nonsense tokens). These two \"routes\" operate\nindependently: we show that ablation of token induction heads causes models to\nparaphrase where they would otherwise copy verbatim. By patching concept\ninduction head outputs, we find that they contain language-independent word\nrepresentations that mediate natural language translation, suggesting that LLMs\nrepresent abstract word meanings independent of language or form.", "AI": {"tldr": "This paper introduces concept-level induction heads in language models, which copy whole lexical units rather than individual tokens, operating alongside token-level induction heads.", "motivation": "The research aims to explore the mechanisms behind in-context copying in language models, particularly focusing on the role of induction heads in semantic tasks and knowledge representation.", "method": "The authors conduct experiments to identify and analyze the behavior of concept-level induction heads in comparison to traditional token induction heads, particularly in tasks like translation.", "result": "The study finds that concept induction heads effectively handle semantic tasks, such as word-level translation, while token induction heads are crucial for verbatim tasks. These two types of heads operate independently and reveal a nuanced understanding of language representation in LLMs.", "conclusion": "The discovery of concept induction heads suggests that LLMs have a capability to represent abstract meanings independent of language, enhancing their utility in multilingual settings.", "key_contributions": ["Identification of concept-level induction heads that copy entire lexical units.", "Demonstration of the independent operation of token and concept induction heads.", "Insights into language-independent word representations that facilitate translation."], "limitations": "", "keywords": ["induction heads", "semantics", "language representation", "natural language processing", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.09763", "pdf": "https://arxiv.org/pdf/2504.09763.pdf", "abs": "https://arxiv.org/abs/2504.09763", "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems", "authors": ["Zaid Khan", "Elias Stengel-Eskin", "Archiki Prasad", "Jaemin Cho", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://zaidkhan.me/EFAGen/", "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from reinforcement learning (procedural\nenvironments) to physics (simulation engines). These programs can be seen as\nfunctions which execute to different outputs based on their parameterizations\n(e.g., gridworld configuration or initial physical conditions). We introduce\nthe term EFA (Executable Functional Abstraction) to denote such programs for\nmath problems. EFA-like constructs have been shown to be useful for\nmathematical reasoning as problem generators for stress-testing models.\nHowever, prior work has been limited to automatically constructing abstractions\nfor grade-school math (whose simple rules are easy to encode in programs),\nwhile generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced\nmathematics problems by developing EFAGen, which operationalizes the task of\nautomatically inferring an EFA for a given seed problem and solution as a\nprogram synthesis task. We first formalize the properties of any valid EFA as\nexecutable unit tests. Using execution feedback from the unit tests, we search\nover candidate programs sampled from a LLM to find EFA programs that are\nfaithful to the generalized problem and solution class underlying the seed\nproblem. We then apply the tests as a reward signal, training LLMs to become\nbetter writers of EFAs. We show that EFAs inferred by EFAGen are faithful to\nthe seed problems, produce learnable problem variations, and that EFAGen can\ninfer EFAs across diverse sources of competition-level math problems. Finally,\nwe show uses of model-written EFAs e.g., finding harder/easier problem\nvariants, as well as data generation.", "AI": {"tldr": "This paper presents EFAGen, a program synthesis approach for automatically generating Executable Functional Abstractions (EFAs) for advanced mathematics problems, using LLMs and unit testing.", "motivation": "To automate the construction of EFAs for advanced mathematics, which has previously required human intervention, enhancing problem generation in mathematics and supporting various applications such as model stress-testing.", "method": "EFAGen infers an EFA by treating it as a program synthesis task, formalizing EFA properties as unit tests, utilizing execution feedback to optimize candidate programs generated by a language model.", "result": "EFAGen successfully infers EFAs that are faithful to seed problems, generates learnable variations of problems, and can infer EFAs across various competition-level math problems.", "conclusion": "The approach demonstrates that automated generation of EFAs can be useful for creating diverse and challenging variations of math problems, contributing to improvements in mathematical reasoning and AI model training.", "key_contributions": ["Introduction of Executable Functional Abstraction (EFA) for advanced math problems.", "Development of EFAGen for automatic synthesis of EFAs.", "Demonstration of EFAGen's ability to generate problem variations and support data generation."], "limitations": "", "keywords": ["Executable Functional Abstraction", "program synthesis", "mathematical reasoning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.06806", "pdf": "https://arxiv.org/pdf/2506.06806.pdf", "abs": "https://arxiv.org/abs/2506.06806", "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification", "authors": ["Subhendu Khatuya", "Shashwat Naidu", "Saptarshi Ghosh", "Pawan Goyal", "Niloy Ganguly"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work has been accepted to appear at the Association for\n  Computational Linguistics (ACL), 2025", "summary": "The explosion of textual data has made manual document classification\nincreasingly challenging. To address this, we introduce a robust, efficient\ndomain-agnostic generative model framework for multi-label text classification.\nInstead of treating labels as mere atomic symbols, our approach utilizes\npredefined label descriptions and is trained to generate these descriptions\nbased on the input text. During inference, the generated descriptions are\nmatched to the pre-defined labels using a finetuned sentence transformer. We\nintegrate this with a dual-objective loss function, combining cross-entropy\nloss and cosine similarity of the generated sentences with the predefined\ntarget descriptions, ensuring both semantic alignment and accuracy. Our\nproposed model LAGAMC stands out for its parameter efficiency and versatility\nacross diverse datasets, making it well-suited for practical applications. We\ndemonstrate the effectiveness of our proposed model by achieving new\nstate-of-the-art performances across all evaluated datasets, surpassing several\nstrong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in\nMacro-F1 compared to the closest baseline across all datasets.", "AI": {"tldr": "Robust domain-agnostic framework for multi-label text classification using generative models and predefined label descriptions.", "motivation": "The growing volume of textual data necessitates more efficient automated document classification methods.", "method": "A generative model framework that utilizes predefined label descriptions and a dual-objective loss function to train and match with input text using a finetuned sentence transformer.", "result": "The proposed model LAGAMC achieves state-of-the-art performances with significant improvements in Micro-F1 and Macro-F1 scores across multiple datasets compared to several strong baselines.", "conclusion": "LAGAMC demonstrates parameter efficiency and versatility, making it suitable for various practical applications in multi-label text classification.", "key_contributions": ["Introduction of a generative model framework for multi-label classification", "Utilization of predefined label descriptions to improve semantic alignment", "Dual-objective loss function combining cross-entropy and cosine similarity"], "limitations": "", "keywords": ["multi-label text classification", "generative models", "semantic alignment"], "importance_score": 8, "read_time_minutes": 5}}
