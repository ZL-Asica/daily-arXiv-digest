{"id": "2509.09815", "pdf": "https://arxiv.org/pdf/2509.09815.pdf", "abs": "https://arxiv.org/abs/2509.09815", "title": "Merging Bodies, Dividing Conflict: Body-Swapping in Mixed Reality Increases Closeness Yet Weakens the Joint Simon Effect", "authors": ["Yuan He", "Brendan Rooney", "Rachel McDonnell"], "categories": ["cs.HC"], "comment": null, "summary": "Mixed Reality (MR) presents novel opportunities to investigate how\nindividuals perceive themselves and others during shared, augmented experiences\nwithin a common physical environment. Previous research has demonstrated that\nusers can embody avatars in MR, temporarily extending their sense of self.\nHowever, there has been limited exploration of body-swapping, a condition in\nwhich two individuals simultaneously inhabit each other's avatars, and its\npotential effects on social interaction in immersive environments. To address\nthis gap, we adapted the Joint Simon Task (JST), a well-established implicit\nparadigm, to examine how body-swapping influences the cognitive and perceptual\nboundaries between self and other. Our results indicate that body-swapping led\nparticipants to experience themselves and their partner as functioning like a\nsingle, unified system, as in two bodies operating as one agent. This suggests\npossible cognitive and perceptual changes that go beyond simple collaboration.\nOur findings have significant implications for the design of MR systems\nintended to support collaboration, empathy, social learning, and therapeutic\ninterventions through shared embodiment."}
{"id": "2509.09840", "pdf": "https://arxiv.org/pdf/2509.09840.pdf", "abs": "https://arxiv.org/abs/2509.09840", "title": "Designing and Evaluating AI Margin Notes in Document Reader Software", "authors": ["Nikhita Joshi", "Daniel Vogel"], "categories": ["cs.HC"], "comment": null, "summary": "AI capabilities for document reader software are usually presented in\nseparate chat interfaces. We explore integrating AI into document comments, a\nconcept we formalize as AI margin notes. Three design parameters characterize\nthis approach: margin notes are integrated with the text while chat interfaces\nare not; selecting text for a margin note can be automated through AI or\nmanual; and the generation of a margin note can involve AI to various degrees.\nTwo experiments investigate integration and selection automation, with results\nshowing participants prefer integrated AI margin notes and manual selection. A\nthird experiment explores human and AI involvement through six alternative\ntechniques. Techniques with less AI involvement resulted in more psychological\nownership, but faster and less effortful designs are generally preferred.\nSurprisingly, the degree of AI involvement had no measurable effect on reading\ncomprehension. Our work shows that AI margin notes are desirable and\ncontributes implications for their design."}
{"id": "2509.09870", "pdf": "https://arxiv.org/pdf/2509.09870.pdf", "abs": "https://arxiv.org/abs/2509.09870", "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks", "authors": ["Hasibur Rahman", "Smit Desai"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable conversational agents (CAs) to express\ndistinctive personalities, raising new questions about how such designs shape\nuser perceptions. This study investigates how personality expression levels and\nuser-agent personality alignment influence perceptions in goal-oriented tasks.\nIn a between-subjects experiment (N=150), participants completed travel\nplanning with CAs exhibiting low, medium, or high expression across the Big\nFive traits, controlled via our novel Trait Modulation Keys framework. Results\nrevealed an inverted-U relationship: medium expression produced the most\npositive evaluations across Intelligence, Enjoyment, Anthropomorphism,\nIntention to Adopt, Trust, and Likeability, significantly outperforming both\nextremes. Personality alignment further enhanced outcomes, with Extraversion\nand Emotional Stability emerging as the most influential traits. Cluster\nanalysis identified three distinct compatibility profiles, with \"Well-Aligned\"\nusers reporting substantially positive perceptions. These findings demonstrate\nthat personality expression and strategic trait alignment constitute optimal\ndesign targets for CA personality, offering design implications as LLM-based\nCAs become increasingly prevalent."}
{"id": "2509.09888", "pdf": "https://arxiv.org/pdf/2509.09888.pdf", "abs": "https://arxiv.org/abs/2509.09888", "title": "Climate Data for Power Systems Applications: Lessons in Reusing Wildfire Smoke Data for Solar PV Studies", "authors": ["Arleth Salinas", "Irtaza Sohail", "Valerio Pascucci", "Pantelis Stefanakis", "Saud Amjad", "Aashish Panta", "Roland Schigas", "Timothy Chun-Yiu Chui", "Nicolas Duboc", "Mostafa Farrokhabadi", "Roland Stull"], "categories": ["cs.HC"], "comment": "This paper has been accepted for the upcoming 59th Hawaii\n  International Conference on System Sciences (HICSS-59)", "summary": "Data reuse is using data for a purpose distinct from its original intent. As\ndata sharing becomes more prevalent in science, enabling effective data reuse\nis increasingly important. In this paper, we present a power systems case study\nof data repurposing for enabling data reuse. We define data repurposing as the\nprocess of transforming data to fit a new research purpose. In our case study,\nwe repurpose a geospatial wildfire smoke forecast dataset into a historical\ndataset. We analyze its efficacy toward analyzing wildfire smoke impact on\nsolar photovoltaic energy production. We also provide documentation and\ninteractive demos for using the repurposed dataset. We identify key enablers of\ndata reuse including metadata standardization, contextual documentation, and\ncommunication between data creators and reusers. We also identify obstacles to\ndata reuse such as risk of misinterpretation and barriers to efficient data\naccess. Through an iterative approach to data repurposing, we demonstrate how\nleveraging and expanding knowledge transfer infrastructures like online\ndocumentation, interactive visualizations, and data streaming directly address\nthese obstacles. The findings facilitate big data use from other domains for\npower systems applications and grid resiliency."}
{"id": "2509.09699", "pdf": "https://arxiv.org/pdf/2509.09699.pdf", "abs": "https://arxiv.org/abs/2509.09699", "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline."}
{"id": "2509.09910", "pdf": "https://arxiv.org/pdf/2509.09910.pdf", "abs": "https://arxiv.org/abs/2509.09910", "title": "Seeing Identity in Data: Can Anthropographics Uncover Racial Homophily in Emotional Responses?", "authors": ["Poorna Talkad Sukumar", "Maurizio Porfiri", "Oded Nov"], "categories": ["cs.HC"], "comment": null, "summary": "Racial homophily refers to the tendency of individuals to associate with\nothers of the same racial or ethnic background. A recent study found no\nevidence of racial homophily in responses to mass shooting data visualizations.\nTo increase the likelihood of detecting an effect, we redesigned the experiment\nby replacing bar charts with anthropographics and expanding the sample size. In\na crowdsourced study (N=720), we showed participants a pictograph of mass\nshooting victims in the United States, with victims from one of three racial\ngroups (Hispanic, Black, or White) highlighted. Each participant was assigned a\nvisualization highlighting either their own racial group or a different racial\ngroup, allowing us to assess the influence of racial concordance on changes in\naffect (emotion). We found that, across all conditions, racial concordance had\na modest but significant effect on changes in affect, with participants\nexperiencing greater negative affect change when viewing visualizations\nhighlighting their own race. This study provides initial evidence that racial\nhomophily can emerge in responses to data visualizations, particularly when\nusing anthropographics."}
{"id": "2509.09700", "pdf": "https://arxiv.org/pdf/2509.09700.pdf", "abs": "https://arxiv.org/abs/2509.09700", "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution."}
{"id": "2509.09916", "pdf": "https://arxiv.org/pdf/2509.09916.pdf", "abs": "https://arxiv.org/abs/2509.09916", "title": "Immersive Invaders: Privacy Threats from Deceptive Design in Virtual Reality Games and Applications", "authors": ["Hilda Hadan", "Michaela Valiquette", "Lennart E. Nacke", "Leah Zhang-Kennedy"], "categories": ["cs.HC"], "comment": "28 pages, 4 tables, 3 figures. For related materials, see:\n  https://osf.io/axzve/", "summary": "Virtual Reality (VR) technologies offer immersive experiences but collect\nsubstantial user data. While deceptive design is well-studied in 2D platforms,\nlittle is known about its manifestation in VR environments and its impact on\nuser privacy. This research investigates deceptive designs in privacy\ncommunication and interaction mechanisms of 12 top-rated VR games and\napplications through autoethnographic evaluation of the applications and\nthematic analysis of privacy policies. We found that while many deceptive\ndesigns rely on 2D interfaces, some VR-unique features, while not directly\nenabling deception, amplified data disclosure behaviors, and obscured actual\ndata practices. Convoluted privacy policies and manipulative consent practices\nfurther hinder comprehension and increase privacy risks. We also observed\nprivacy-preserving design strategies and protective considerations in VR\nprivacy policies. We offer recommendations for ethical VR design that balance\nimmersive experiences with strong privacy protections, guiding researchers,\ndesigners, and policymakers to improve privacy in VR environments."}
{"id": "2509.09701", "pdf": "https://arxiv.org/pdf/2509.09701.pdf", "abs": "https://arxiv.org/abs/2509.09701", "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "authors": ["JungHo Jung", "Junhyun Lee"], "categories": ["cs.CL"], "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset."}
{"id": "2509.10003", "pdf": "https://arxiv.org/pdf/2509.10003.pdf", "abs": "https://arxiv.org/abs/2509.10003", "title": "Beyond the Silence: How Men Navigate Infertility Through Digital Communities and Data Sharing", "authors": ["Tawfiq Ammari", "Zarah Khondoker", "Yihan Wang", "Nikki Roda"], "categories": ["cs.HC"], "comment": null, "summary": "Men experiencing infertility face unique challenges navigating Traditional\nMasculinity Ideologies that discourage emotional expression and help-seeking.\nThis study examines how Reddit's r/maleinfertility community helps overcome\nthese barriers through digital support networks. Using topic modeling (115\ntopics), network analysis (11 micro-communities), and time-lagged regression on\n11,095 posts and 79,503 comments from 8,644 users, we found the community\nfunctions as a hybrid space: informal diagnostic hub, therapeutic commons, and\ngoverned institution. Medical advice dominates discourse (63.3\\%), while\nemotional support (7.4\\%) and moderation (29.2\\%) create essential\ninfrastructure. Sustained engagement correlates with actionable guidance and\naffiliation language, not emotional processing. Network analysis revealed\nstructurally cohesive but topically diverse clusters without echo chamber\ncharacteristics. Cross-posters (20\\% of users) who bridge r/maleinfertility and\nthe gender-mixed r/infertility community serve as navigators and mentors,\ntransferring knowledge between spaces. These findings inform trauma-informed\ndesign for stigmatized health communities, highlighting role-aware systems and\nnavigation support."}
{"id": "2509.09702", "pdf": "https://arxiv.org/pdf/2509.09702.pdf", "abs": "https://arxiv.org/abs/2509.09702", "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows."}
{"id": "2509.10015", "pdf": "https://arxiv.org/pdf/2509.10015.pdf", "abs": "https://arxiv.org/abs/2509.10015", "title": "A Framework for AI-Supported Mediation in Community-based Online Collaboration", "authors": ["Soobin Cho", "Mark Zachry", "David W. McDonald"], "categories": ["cs.HC"], "comment": null, "summary": "Online spaces involve diverse communities engaging in various forms of\ncollaboration, which naturally give rise to discussions, some of which\ninevitably escalate into conflict or disputes. To address such situations, AI\nhas primarily been used for moderation. While moderation systems are important\nbecause they help maintain order, common moderation strategies of removing or\nsuppressing content and users rarely address the underlying disagreements or\nthe substantive content of disputes. Mediation, by contrast, fosters\nunderstanding, reduces emotional tension, and facilitates consensus through\nguided negotiation. Mediation not only enhances the quality of collaborative\ndecisions but also strengthens relationships among group members. For this\nreason, we argue for shifting focus toward AI-supported mediation. In this\nwork, we propose an information-focused framework for AI-supported mediation\ndesigned for community-based collaboration. Within this framework, we\nhypothesize that AI must acquire and reason over three key types of\ninformation: content, culture, and people."}
{"id": "2509.09703", "pdf": "https://arxiv.org/pdf/2509.09703.pdf", "abs": "https://arxiv.org/abs/2509.09703", "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>."}
{"id": "2509.10043", "pdf": "https://arxiv.org/pdf/2509.10043.pdf", "abs": "https://arxiv.org/abs/2509.10043", "title": "Inclusive by design: Developing Barrier-Free Authentication for Blind and Low Vision Users through the ALIAS Project", "authors": ["Clara Toussaint", "Benjamin Chateau", "Pierre-Guillaume Gourio-Jewell", "Emilie Bonnefoy", "Nicolas Louveton"], "categories": ["cs.HC"], "comment": null, "summary": "Authentication is the cornerstone of information security in our daily lives.\nHowever, disabled users such as Blind and Low-Vision (BLV) ones are left behind\nin digital services due to the lack of accessibility. According to the World\nHealth Organization, 36 million people are blind worldwide. It is estimated\nthat there will be 115 million by 2050, due to the ageing of the population.\nYet accessing digital services has become increasingly essential. At the same\ntime, cyber threats targeting individuals have also increased strongly in the\nlast few years. The ALIAS project addresses the need for accessible digital\nauthentication solutions for BLV users facing challenges with digital\ntechnology. Security systems can inhibit access for these individuals as they\nbecome more complex. This project aims to create a barrier-free authentication\nsystem based on cognitive ergonomics and user experience (UX) design methods\nspecifically for BLV users. This paper presents an overview of current research\nin this area. We also identify research gaps, and finally, we present our\nproject's methodology and approach. First, we will build a knowledge base on\nthe digital practices and cognitive models of BLV users during authentication.\nThis information will support the development of prototypes, which will be\ntested and refined through two iterations before finalizing the operational\nversion."}
{"id": "2509.09704", "pdf": "https://arxiv.org/pdf/2509.09704.pdf", "abs": "https://arxiv.org/abs/2509.09704", "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment."}
{"id": "2509.10064", "pdf": "https://arxiv.org/pdf/2509.10064.pdf", "abs": "https://arxiv.org/abs/2509.10064", "title": "From customer survey feedback to software improvements: Leveraging the full potential of data", "authors": ["Erik Bertram", "Nina Hollender", "Sebastian Juhl", "Sandra Loop", "Martin Schrepp"], "categories": ["cs.HC"], "comment": "10 pages, 8 figures, published in Springer Nature", "summary": "Converting customer survey feedback data into usable insights has always been\na great challenge for large software enterprises. Despite the improvements on\nthis field, a major obstacle often remains when drawing the right conclusions\nout of the data and channeling them into the software development process. In\nthis paper we present a practical end-to-end approach of how to extract useful\ninformation out of a data set and leverage the information to drive change. We\ndescribe how to choose the right metrics to measure, gather appropriate\nfeedback from customer end-users, analyze the data by leveraging methods from\ninferential statistics, make the data transparent, and finally drive change\nwith the results. Furthermore, we present an example of a UX prototype\ndashboard that can be used to communicate the analyses to stakeholders within\nthe company."}
{"id": "2509.09705", "pdf": "https://arxiv.org/pdf/2509.09705.pdf", "abs": "https://arxiv.org/abs/2509.09705", "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency."}
{"id": "2509.10081", "pdf": "https://arxiv.org/pdf/2509.10081.pdf", "abs": "https://arxiv.org/abs/2509.10081", "title": "Understanding Expert Exploration in EHR Visualization Tools: The ParcoursVis Use Case", "authors": ["Ambre Assor", "Jean-Daniel Fekete"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce our ongoing work toward an insight-based evaluation methodology\naimed at understanding practitioners' mental models when exploring medical\ndata. It is based on ParcoursVis, a Progressive Visual Analytics system\ndesigned to visualize event sequences derived from Electronic Health Records at\nscale (millions of patients, billions of events), developed in collaboration\nwith the Emergency Departments of 16 Parisian hospitals and with the French\nSocial Security. Building on prior usability validation, our current evaluation\nfocuses on the insights generated by expert users and aims to better understand\nthe exploration strategies they employ when engaging with exploration\nvisualization tools. We describe our system and outline our evaluation\nprotocol, analysis strategy, and preliminary findings. Building on this\napproach and our pilot results, we contribute a design protocol for conducting\ninsight-based studies under real-world constraints, including the availability\nof health practitioners whom we were fortunate to interview. Our findings\nhighlight a loop, where the use of the system helps refine data variables\nidentification and the system itself. We aim to shed light on generated\ninsights, to highlight the utility of exploratory tools in health data analysis\ncontexts."}
{"id": "2509.09708", "pdf": "https://arxiv.org/pdf/2509.09708.pdf", "abs": "https://arxiv.org/abs/2509.09708", "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space."}
{"id": "2509.10327", "pdf": "https://arxiv.org/pdf/2509.10327.pdf", "abs": "https://arxiv.org/abs/2509.10327", "title": "MusicScaffold: Bridging Machine Efficiency and Human Growth in Adolescent Creative Education through Generative AI", "authors": ["Zhejing Hu", "Yan Liu", "Zhi Zhang", "Gong Chen", "Bruce X. B. Yu", "Junxian Li", "Jiannong Cao"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescence is marked by strong creative impulses but limited strategies for\nstructured expression, often leading to frustration or disengagement. While\ngenerative AI lowers technical barriers and delivers efficient outputs, its\nrole in fostering adolescents' expressive growth has been overlooked. We\npropose MusicScaffold, the first adolescent-centered framework that repositions\nAI as a guide, coach, and partner, making expressive strategies transparent and\nlearnable, and supporting autonomy. In a four-week study with middle school\nstudents (ages 12--14), MusicScaffold enhanced cognitive specificity,\nbehavioral self-regulation, and affective confidence in music creation. By\nreframing generative AI as a scaffold rather than a generator, this work\nbridges the machine efficiency of generative systems with human growth in\nadolescent creative education."}
{"id": "2509.09709", "pdf": "https://arxiv.org/pdf/2509.09709.pdf", "abs": "https://arxiv.org/abs/2509.09709", "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "authors": ["Jing Ren", "Weiqi Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts."}
{"id": "2509.10331", "pdf": "https://arxiv.org/pdf/2509.10331.pdf", "abs": "https://arxiv.org/abs/2509.10331", "title": "Who Decides How Knowing Becomes Doing? Redistributing Authority in Human-AI Music Co-Creation", "authors": ["Zhejing Hu", "Yan Liu", "Zhi Zhang", "Gong Chen", "Bruce X. B. Yu", "Jiannong Cao"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "In the era of human-AI co-creation, the maxim \"knowing is easy, doing is\nhard\" is redefined. AI has the potential to ease execution, yet the essence of\n\"hard\" lies in who governs the translation from knowing to doing. Mainstream\ntools often centralize interpretive authority and homogenize expression,\nsuppressing marginal voices. To address these challenges, we introduce the\nfirst systematic framework for redistributing authority in the knowing-doing\ncycle, built on three principles, namely contestability, agency, and plurality.\nThrough interactive studies with 180 music practitioners, complemented by\nin-depth interviews, we demonstrate that these principles reshape human-AI\nauthority relations and reactivate human creative expression. The findings\nestablish a new paradigm for critical computing and human-AI co-creation that\nadvances from critique to practice."}
{"id": "2509.09710", "pdf": "https://arxiv.org/pdf/2509.09710.pdf", "abs": "https://arxiv.org/abs/2509.09710", "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems."}
{"id": "2509.10370", "pdf": "https://arxiv.org/pdf/2509.10370.pdf", "abs": "https://arxiv.org/abs/2509.10370", "title": "The Language of Approval: Identifying the Drivers of Positive Feedback Online", "authors": ["Agam Goyal", "Charlotte Lambert", "Eshwar Chandrasekharan"], "categories": ["cs.HC"], "comment": "Preprint: 21 pages, 7 figures, 7 tables", "summary": "Positive feedback via likes and awards is central to online governance, yet\nwhich attributes of users' posts elicit rewards -- and how these vary across\nauthors and communities -- remains unclear. To examine this, we combine\nquasi-experimental causal inference with predictive modeling on 11M posts from\n100 subreddits. We identify linguistic patterns and stylistic attributes\ncausally linked to rewards, controlling for author reputation, timing, and\ncommunity context. For example, overtly complicated language, tentative style,\nand toxicity reduce rewards. We use our set of curated features to train models\nthat can detect highly-upvoted posts with high AUC. Our audit of community\nguidelines highlights a ``policy-practice gap'' -- most rules focus primarily\non civility and formatting requirements, with little emphasis on the attributes\nidentified to drive positive feedback. These results inform the design of\ncommunity guidelines, support interfaces that teach users how to craft\ndesirable contributions, and moderation workflows that emphasize positive\nreinforcement over purely punitive enforcement."}
{"id": "2509.09711", "pdf": "https://arxiv.org/pdf/2509.09711.pdf", "abs": "https://arxiv.org/abs/2509.09711", "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications."}
{"id": "2509.10427", "pdf": "https://arxiv.org/pdf/2509.10427.pdf", "abs": "https://arxiv.org/abs/2509.10427", "title": "My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom", "authors": ["Jiayi Ye", "Chaoran Chen", "Yue Huang", "Yanfang Ye", "Toby Jia-Jun Li", "Xiangliang Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "AI VTubers, where the performer is not human but algorithmically generated,\nintroduce a new context for fandom. While human VTubers have been substantially\nstudied for their cultural appeal, parasocial dynamics, and community\neconomies, little is known about how audiences engage with their AI\ncounterparts. To address this gap, we present a qualitative study of\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\nyet entertaining interactions, cement loyalty through collective emotional\nevents that trigger anthropomorphic projection, and sustain attachment via the\nAI's consistent persona. Financial support emerges not as a reward for\nperformance but as a participatory mechanism for shaping livestream content,\nestablishing a resilient fan economy built on ongoing interaction. These\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\noffer implications for designing transparent and sustainable AI-mediated\ncommunities."}
{"id": "2509.09712", "pdf": "https://arxiv.org/pdf/2509.09712.pdf", "abs": "https://arxiv.org/abs/2509.09712", "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "authors": ["Talha Tahir"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm."}
{"id": "2509.09702", "pdf": "https://arxiv.org/pdf/2509.09702.pdf", "abs": "https://arxiv.org/abs/2509.09702", "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows."}
{"id": "2509.09713", "pdf": "https://arxiv.org/pdf/2509.09713.pdf", "abs": "https://arxiv.org/abs/2509.09713", "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks."}
{"id": "2509.09799", "pdf": "https://arxiv.org/pdf/2509.09799.pdf", "abs": "https://arxiv.org/abs/2509.09799", "title": "Distinguishing Startle from Surprise Events Based on Physiological Signals", "authors": ["Mansi Sharma", "Alexandre Duchevet", "Florian Daiber", "Jean-Paul Imbert", "Maurice Rekrut"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Unexpected events can impair attention and delay decision-making, posing\nserious safety risks in high-risk environments such as aviation. In particular,\nreactions like startle and surprise can impact pilot performance in different\nways, yet are often hard to distinguish in practice. Existing research has\nlargely studied these reactions separately, with limited focus on their\ncombined effects or how to differentiate them using physiological data. In this\nwork, we address this gap by distinguishing between startle and surprise events\nbased on physiological signals using machine learning and multi-modal fusion\nstrategies. Our results demonstrate that these events can be reliably\npredicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.\nTo further validate the robustness of our model, we extended the evaluation to\ninclude a baseline condition, successfully differentiating between Startle,\nSurprise, and Baseline states with a highest mean accuracy of 74.9% with\nXGBoost and Late Fusion."}
{"id": "2509.09714", "pdf": "https://arxiv.org/pdf/2509.09714.pdf", "abs": "https://arxiv.org/abs/2509.09714", "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "authors": ["Serge Lionel Nikiema", "Albérick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline Bénédicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawendé F. Bissyande"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content."}
{"id": "2509.09823", "pdf": "https://arxiv.org/pdf/2509.09823.pdf", "abs": "https://arxiv.org/abs/2509.09823", "title": "SoilSound: Smartphone-based Soil Moisture Estimation", "authors": ["Yixuan Gao", "Tanvir Ahmed", "Shuang He", "Zhongqi Cheng", "Rajalakshmi Nandakumar"], "categories": ["cs.SD", "cs.AI", "cs.ET", "cs.HC", "eess.SP"], "comment": "12 pages, 8 figures", "summary": "Soil moisture monitoring is essential for agriculture and environmental\nmanagement, yet existing methods require either invasive probes disturbing the\nsoil or specialized equipment, limiting access to the public. We present\nSoilSound, an ubiquitous accessible smartphone-based acoustic sensing system\nthat can measure soil moisture without disturbing the soil. We leverage the\nbuilt-in speaker and microphone to perform a vertical scan mechanism to\naccurately measure moisture without any calibration. Unlike existing work that\nuse transmissive properties, we propose an alternate model for acoustic\nreflections in soil based on the surface roughness effect to enable moisture\nsensing without disturbing the soil. The system works by sending acoustic\nchirps towards the soil and recording the reflections during a vertical scan,\nwhich are then processed and fed to a convolutional neural network for\non-device soil moisture estimation with negligible computational, memory, or\npower overhead. We evaluated the system by training with curated soils in boxes\nin the lab and testing in the outdoor fields and show that SoilSound achieves a\nmean absolute error (MAE) of 2.39% across 10 different locations. Overall, the\nevaluation shows that SoilSound can accurately track soil moisture levels\nranging from 15.9% to 34.0% across multiple soil types, environments, and\nusers; without requiring any calibration or disturbing the soil, enabling\nwidespread moisture monitoring for home gardeners, urban farmers, citizen\nscientists, and agricultural communities in resource-limited settings."}
{"id": "2509.09715", "pdf": "https://arxiv.org/pdf/2509.09715.pdf", "abs": "https://arxiv.org/abs/2509.09715", "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale."}
{"id": "2509.09889", "pdf": "https://arxiv.org/pdf/2509.09889.pdf", "abs": "https://arxiv.org/abs/2509.09889", "title": "Using the Pepper Robot to Support Sign Language Communication", "authors": ["Giulia Botta", "Marco Botta", "Cristina Gena", "Alessandro Mazzei", "Massimo Donini", "Alberto Lillo"], "categories": ["cs.RO", "cs.HC"], "comment": "paper presented at ICSR2025", "summary": "Social robots are increasingly experimented in public and assistive settings,\nbut their accessibility for Deaf users remains quite underexplored. Italian\nSign Language (LIS) is a fully-fledged natural language that relies on complex\nmanual and non-manual components. Enabling robots to communicate using LIS\ncould foster more inclusive human robot interaction, especially in social\nenvironments such as hospitals, airports, or educational settings. This study\ninvestigates whether a commercial social robot, Pepper, can produce\nintelligible LIS signs and short signed LIS sentences. With the help of a Deaf\nstudent and his interpreter, an expert in LIS, we co-designed and implemented\n52 LIS signs on Pepper using either manual animation techniques or a MATLAB\nbased inverse kinematics solver. We conducted a exploratory user study\ninvolving 12 participants proficient in LIS, both Deaf and hearing.\nParticipants completed a questionnaire featuring 15 single-choice video-based\nsign recognition tasks and 2 open-ended questions on short signed sentences.\nResults shows that the majority of isolated signs were recognized correctly,\nalthough full sentence recognition was significantly lower due to Pepper's\nlimited articulation and temporal constraints. Our findings demonstrate that\neven commercially available social robots like Pepper can perform a subset of\nLIS signs intelligibly, offering some opportunities for a more inclusive\ninteraction design. Future developments should address multi-modal enhancements\n(e.g., screen-based support or expressive avatars) and involve Deaf users in\nparticipatory design to refine robot expressivity and usability."}
{"id": "2509.09723", "pdf": "https://arxiv.org/pdf/2509.09723.pdf", "abs": "https://arxiv.org/abs/2509.09723", "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "authors": ["Kai R. Larsen", "Sen Yan", "Roland Müller", "Lan Sang", "Mikko Rönkkö", "Ravi Starzl", "Donald Edmondson"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis."}
{"id": "2509.10010", "pdf": "https://arxiv.org/pdf/2509.10010.pdf", "abs": "https://arxiv.org/abs/2509.10010", "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian Möller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots."}
{"id": "2509.09724", "pdf": "https://arxiv.org/pdf/2509.09724.pdf", "abs": "https://arxiv.org/abs/2509.09724", "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities."}
{"id": "2509.10216", "pdf": "https://arxiv.org/pdf/2509.10216.pdf", "abs": "https://arxiv.org/abs/2509.10216", "title": "RFSeek and Ye Shall Find", "authors": ["Noga H. Rotman", "Tiago Ferreira", "Hila Peleg", "Mark Silberstein", "Alexandra Silva"], "categories": ["cs.NI", "cs.HC", "cs.LG"], "comment": "7 pages", "summary": "Requests for Comments (RFCs) are extensive specification documents for\nnetwork protocols, but their prose-based format and their considerable length\noften impede precise operational understanding. We present RFSeek, an\ninteractive tool that automatically extracts visual summaries of protocol logic\nfrom RFCs. RFSeek leverages large language models (LLMs) to generate\nprovenance-linked, explorable diagrams, surfacing both official state machines\nand additional logic found only in the RFC text. Compared to existing RFC\nvisualizations, RFSeek's visual summaries are more transparent and easier to\naudit against their textual source. We showcase the tool's potential through a\nseries of use cases, including guided knowledge extraction and semantic\ndiffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.\n  In practice, RFSeek not only reconstructs the RFC diagrams included in some\nspecifications, but, more interestingly, also uncovers important logic such as\nnodes or edges described in the text but missing from those diagrams. RFSeek\nfurther derives new visualization diagrams for complex RFCs, with QUIC as a\nrepresentative case. Our approach, which we term \\emph{Summary Visualization},\nhighlights a promising direction: combining LLMs with formal, user-customized\nvisualizations to enhance protocol comprehension and support robust\nimplementations."}
{"id": "2509.09725", "pdf": "https://arxiv.org/pdf/2509.09725.pdf", "abs": "https://arxiv.org/abs/2509.09725", "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L."}
{"id": "2504.04198", "pdf": "https://arxiv.org/pdf/2504.04198.pdf", "abs": "https://arxiv.org/abs/2504.04198", "title": "Evaluating the Usability of Microgestures for Text Editing Tasks in Virtual Reality", "authors": ["Xiang Li", "Wei He", "Per Ola Kristensson"], "categories": ["cs.HC", "cs.MM"], "comment": "14 pages, IEEE Transactions on Visualization and Computer Graphics", "summary": "As virtual reality (VR) continues to evolve, traditional input methods such\nas handheld controllers and gesture systems often face challenges with\nprecision, social accessibility, and user fatigue. These limitations motivate\nthe exploration of microgestures, which promise more subtle, ergonomic, and\ndevice-free interactions. We introduce microGEXT, a lightweight\nmicrogesture-based system designed for text editing in VR without external\nsensors, which utilizes small, subtle hand movements to reduce physical strain\ncompared to standard gestures. We evaluated microGEXT in three user studies. In\nStudy 1 ($N=20$), microGEXT reduced overall edit time and fatigue compared to a\nray-casting + pinch menu baseline, the default text editing approach in\ncommercial VR systems. Study 2 ($N=20$) found that microGEXT performed well in\nshort text selection tasks but was slower for longer text ranges. In Study 3\n($N=10$), participants found microGEXT intuitive for open-ended\ninformation-gathering tasks. Across all studies, microGEXT demonstrated\nenhanced user experience and reduced physical effort, offering a promising\nalternative to traditional VR text editing techniques."}
{"id": "2509.09726", "pdf": "https://arxiv.org/pdf/2509.09726.pdf", "abs": "https://arxiv.org/abs/2509.09726", "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "categories": ["cs.CL"], "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant."}
{"id": "2505.09583", "pdf": "https://arxiv.org/pdf/2505.09583.pdf", "abs": "https://arxiv.org/abs/2505.09583", "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media", "authors": ["Yuchen Wu", "Mingduo Zhao", "John Canny"], "categories": ["cs.HC"], "comment": null, "summary": "Many online platforms incorporate engagement signals, such as likes, into\ntheir interface design to boost engagement. However, these signals can\nunintentionally elevate content that may not support normatively desirable\nbehavior, especially when toxic content correlates strongly with popularity\nindicators. In this study, we propose structured prosocial feedback as a\ncomplementary signal, which highlights content quality based on normative\ncriteria. We design and implement an LLM-based feedback system, which evaluates\nuser comments based on principles from positive psychology, such as individual\nwell-being. A pre-registered user study then examines how existing peer-based\n(popularity) and the new expert-based feedback interact to shape users'\nreposting behavior in a social media setting. Results show that peer feedback\nincreases conformity to popularity cues, while expert feedback shifts choices\ntoward normatively higher-quality content. This illustrates the added value of\nnormative cues and underscores the potential benefits of incorporating such\nsignals into platform feedback systems to foster healthier online environments."}
{"id": "2509.09727", "pdf": "https://arxiv.org/pdf/2509.09727.pdf", "abs": "https://arxiv.org/abs/2509.09727", "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "authors": ["Andy Zhu", "Yingjun Du"], "categories": ["cs.CL", "cs.CE"], "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems."}
{"id": "2505.15971", "pdf": "https://arxiv.org/pdf/2505.15971.pdf", "abs": "https://arxiv.org/abs/2505.15971", "title": "A Paradigm for Creative Ownership", "authors": ["Tejaswi Polimetla", "Katy Ilonka Gero", "Elena Leah Glassman"], "categories": ["cs.HC"], "comment": null, "summary": "As generative AI tools become embedded in creative practice, questions of\nownership in co-creative contexts are pressing. Yet studies of human-AI\ncollaboration often invoke \"ownership\" without definition: sometimes conflating\nit with other concepts, and other times leaving interpretation to participants.\nThis inconsistency makes findings difficult to compare across or even within\nstudies. We introduce a framework of creative ownership comprising three\ndimensions - Person, Process, and System - each with three subdimensions,\noffering a shared language for both system design and HCI research. In\nsemi-structured interviews with 21 creative professionals, we found that\nparticipants' initial references to ownership (e.g., embodiment, control,\nconcept) were fully encompassed by the framework, demonstrating its coverage.\nOnce introduced, however, they also articulated and prioritized the remaining\nsubdimensions, underscoring how the framework expands reflection and enables\nricher insights. Our contributions include 1) the framework, 2) a web-based\nvisualization tool, and 3) empirical findings on its utility."}
{"id": "2509.09728", "pdf": "https://arxiv.org/pdf/2509.09728.pdf", "abs": "https://arxiv.org/abs/2509.09728", "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "categories": ["cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice."}
{"id": "2507.04005", "pdf": "https://arxiv.org/pdf/2507.04005.pdf", "abs": "https://arxiv.org/abs/2507.04005", "title": "Exploring a Gamified Personality Assessment Method through Interaction with LLM Agents Embodying Different Personalities", "authors": ["Baiqiao Zhang", "Xiangxian Li", "Chao Zhou", "Xinyu Gai", "Juan Liu", "Xue Yang", "Xiaojuan Ma", "Yong-jin Liu", "Yulong Bian"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The low-intrusion and automated personality assessment is receiving\nincreasing attention in psychology and human-computer interaction fields. This\nstudy explores an interactive approach for personality assessment, focusing on\nthe multiplicity of personality representation. We propose a framework of\nGamified Personality Assessment through Multi-Personality Representations\n(Multi-PR GPA). The framework leverages Large Language Models to empower\nvirtual agents with different personalities. These agents elicit multifaceted\nhuman personality representations through engaging in interactive games.\nDrawing upon the multi-type textual data generated throughout the interaction,\nit achieves two modes of personality assessment (i.e., Direct Assessment and\nQuestionnaire-based Assessment) and provides interpretable insights. Grounded\nin the classic Big Five personality theory, we developed a prototype system and\nconducted a user study to evaluate the efficacy of Multi-PR GPA. The results\naffirm the effectiveness of our approach in personality assessment and\ndemonstrate its superior performance when considering the multiplicity of\npersonality representation."}
{"id": "2509.09729", "pdf": "https://arxiv.org/pdf/2509.09729.pdf", "abs": "https://arxiv.org/abs/2509.09729", "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias Müller", "Rico Sennrich", "Sarah Ebling"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters."}
{"id": "2508.08672", "pdf": "https://arxiv.org/pdf/2508.08672.pdf", "abs": "https://arxiv.org/abs/2508.08672", "title": "Imposing AI: Deceptive design patterns against sustainability", "authors": ["Anaëlle Beignon", "Thomas Thibault", "Nolwenn Maudet"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI is being massively deployed in digital services, at a scale\nthat will result in significant environmental harm. We document how tech\ncompanies are transforming established user interfaces to impose AI use and\nshow how and to what extent these strategies fit within established deceptive\npattern categories. We identify two main design strategies that are implemented\nto impose AI use in both personal and professional contexts: imposing AI\nfeatures in interfaces at the expense of existing non-AI features and promoting\nnarratives about AI that make it harder to resist using it. We discuss\nopportunities for regulating the imposed adoption of AI features, which would\ninevitably lead to negative environmental effects."}
{"id": "2509.09731", "pdf": "https://arxiv.org/pdf/2509.09731.pdf", "abs": "https://arxiv.org/abs/2509.09731", "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring."}
{"id": "2410.14334", "pdf": "https://arxiv.org/pdf/2410.14334.pdf", "abs": "https://arxiv.org/abs/2410.14334", "title": "Evaluating the Evaluators: Towards Human-aligned Metrics for Missing Markers Reconstruction", "authors": ["Taras Kucherenko", "Derek Peristy", "Judith Bütepage"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted at the ACM International Conference on Multimedia 2025 (ACM\n  MM'25)", "summary": "Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. Additionally, we introduce and\nevaluate a set of better-correlated metrics that can drive progress in the\nfield."}
{"id": "2509.09734", "pdf": "https://arxiv.org/pdf/2509.09734.pdf", "abs": "https://arxiv.org/abs/2509.09734", "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems."}
{"id": "2410.20600", "pdf": "https://arxiv.org/pdf/2410.20600.pdf", "abs": "https://arxiv.org/abs/2410.20600", "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way Intelligibility Protocol", "authors": ["Harshvardhan Mestha", "Karan Bania", "Shreyas V", "Sidong Liu", "Ashwin Srinivasan"], "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems."}
{"id": "2509.09735", "pdf": "https://arxiv.org/pdf/2509.09735.pdf", "abs": "https://arxiv.org/abs/2509.09735", "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "authors": ["Willem Huijzer", "Jieying Chen"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI."}
{"id": "2412.14982", "pdf": "https://arxiv.org/pdf/2412.14982.pdf", "abs": "https://arxiv.org/abs/2412.14982", "title": "Efficient Motion Sickness Assessment: Recreation of On-Road Driving on a Compact Test Track", "authors": ["Huseyin Harmankaya", "Adrian Brietzke", "Rebecca Pham-Xuan", "Barys Shyrokau", "Riender Happee", "Georgios Papaioannou"], "categories": ["cs.RO", "cs.ET", "cs.HC"], "comment": null, "summary": "The ability to engage in other activities during the ride is considered by\nconsumers as one of the key reasons for the adoption of automated vehicles.\nHowever, engagement in non-driving activities will provoke occupants' motion\nsickness, deteriorating their overall comfort and thereby risking acceptance of\nautomated driving. Therefore, it is critical to extend our understanding of\nmotion sickness and unravel the modulating factors that affect it through\nexperiments with participants. Currently, most experiments are conducted on\npublic roads (realistic but not reproducible) or test tracks (feasible with\nprototype automated vehicles). This research study develops a method to design\nan optimal path and speed reference to efficiently replicate on-road motion\nsickness exposure on a small test track. The method uses model predictive\ncontrol to replicate the longitudinal and lateral accelerations collected from\non-road drives on a test track of 70 m by 175 m. A within-subject experiment\n(47 participants) was conducted comparing the occupants' motion sickness\noccurrence in test-track and on-road conditions, with the conditions being\ncross-randomized. The results illustrate no difference and no effect of the\ncondition on the occurrence of the average motion sickness across the\nparticipants. Meanwhile, there is an overall correspondence of individual\nsickness levels between on-road and test-track. This paves the path for the\nemployment of our method for a simpler, safer and more replicable assessment of\nmotion sickness."}
{"id": "2509.09801", "pdf": "https://arxiv.org/pdf/2509.09801.pdf", "abs": "https://arxiv.org/abs/2509.09801", "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "authors": ["Brennen Hill"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks."}
{"id": "2502.00858", "pdf": "https://arxiv.org/pdf/2502.00858.pdf", "abs": "https://arxiv.org/abs/2502.00858", "title": "Learning to Plan with Personalized Preferences", "authors": ["Manjie Xu", "Xinyi Yang", "Wei Liang", "Chi Zhang", "Yixin Zhu"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Effective integration of AI agents into daily life requires them to\nunderstand and adapt to individual human preferences, particularly in\ncollaborative roles. Although recent studies on embodied intelligence have\nadvanced significantly, they typically adopt generalized approaches that\noverlook personal preferences in planning. We address this limitation by\ndeveloping agents that not only learn preferences from few demonstrations but\nalso learn to adapt their planning strategies based on these preferences. Our\nresearch leverages the observation that preferences, though implicitly\nexpressed through minimal demonstrations, can generalize across diverse\nplanning scenarios. To systematically evaluate this hypothesis, we introduce\nPreference-based Planning (PbP) benchmark, an embodied benchmark featuring\nhundreds of diverse preferences spanning from atomic actions to complex\nsequences. Our evaluation of SOTA methods reveals that while symbol-based\napproaches show promise in scalability, significant challenges remain in\nlearning to generate and execute plans that satisfy personalized preferences.\nWe further demonstrate that incorporating learned preferences as intermediate\nrepresentations in planning significantly improves the agent's ability to\nconstruct personalized plans. These findings establish preferences as a\nvaluable abstraction layer for adaptive planning, opening new directions for\nresearch in preference-guided plan generation and execution."}
{"id": "2509.09804", "pdf": "https://arxiv.org/pdf/2509.09804.pdf", "abs": "https://arxiv.org/abs/2509.09804", "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "categories": ["cs.CL"], "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language."}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996.pdf", "abs": "https://arxiv.org/abs/2507.04996", "title": "Agentic Vehicles for Human-Centered Mobility Systems", "authors": ["Jiangbo Yu"], "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Autonomous\nvehicles (AuVs) are therefore understood as systems that perceive their\nenvironment and execute pre-programmed tasks independently of external input,\nconsistent with the SAE levels of automated driving. Yet recent research and\nreal-world deployments have begun to showcase vehicles that exhibit behaviors\noutside the scope of this definition. These include natural language\ninteraction with humans, goal adaptation, contextual reasoning, external tool\nuse, and the handling of unforeseen ethical dilemmas, enabled in part by\nmultimodal large language models (LLMs). These developments highlight not only\na gap between technical autonomy and the broader cognitive and social\ncapacities required for human-centered mobility, but also the emergence of a\nform of vehicle intelligence that currently lacks a clear designation. To\naddress this gap, the paper introduces the concept of agentic vehicles (AgVs):\nvehicles that integrate agentic AI systems to reason, adapt, and interact\nwithin complex environments. It synthesizes recent advances in agentic systems\nand suggests how AgVs can complement and even reshape conventional autonomy to\nensure mobility services are aligned with user and societal needs. The paper\nconcludes by outlining key challenges in the development and governance of AgVs\nand their potential role in shaping future agentic transportation systems."}
{"id": "2509.09852", "pdf": "https://arxiv.org/pdf/2509.09852.pdf", "abs": "https://arxiv.org/abs/2509.09852", "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "categories": ["cs.CL"], "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS."}
{"id": "2508.05524", "pdf": "https://arxiv.org/pdf/2508.05524.pdf", "abs": "https://arxiv.org/abs/2508.05524", "title": "GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs", "authors": ["Sefat E. Rahman", "Tushar M. Athawale", "Paul Rosen"], "categories": ["cs.GR", "cs.CG", "cs.HC"], "comment": null, "summary": "Reeb graphs are an important tool for abstracting and representing the\ntopological structure of a function defined on a manifold. We have identified\nthree properties for faithfully representing Reeb graphs in a visualization:\nthey should be constrained to the boundary, compact, and aligned with the\nfunction gradient. Existing algorithms for drawing Reeb graphs are agnostic to\nor violate these properties. In this paper, we introduce an algorithm to\ngenerate Reeb graph visualizations, called GASP, that is cognizant of these\nproperties, thereby producing visualizations that are more representative of\nthe underlying data. To demonstrate the improvements, the resulting Reeb graphs\nare evaluated both qualitatively and quantitatively against the geometric\nbarycenter algorithm, using its implementation available in the Topology\nToolKit (TTK), a widely adopted tool for calculating and visualizing Reeb\ngraphs."}
{"id": "2509.09871", "pdf": "https://arxiv.org/pdf/2509.09871.pdf", "abs": "https://arxiv.org/abs/2509.09871", "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "authors": ["Bastián González-Bustamante", "Nando Verelst", "Carla Cisternas"], "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.09969", "pdf": "https://arxiv.org/pdf/2509.09969.pdf", "abs": "https://arxiv.org/abs/2509.09969", "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI."}
{"id": "2509.09990", "pdf": "https://arxiv.org/pdf/2509.09990.pdf", "abs": "https://arxiv.org/abs/2509.09990", "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "categories": ["cs.CL"], "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks."}
{"id": "2509.10004", "pdf": "https://arxiv.org/pdf/2509.10004.pdf", "abs": "https://arxiv.org/abs/2509.10004", "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection."}
{"id": "2509.10010", "pdf": "https://arxiv.org/pdf/2509.10010.pdf", "abs": "https://arxiv.org/abs/2509.10010", "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian Möller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots."}
{"id": "2509.10035", "pdf": "https://arxiv.org/pdf/2509.10035.pdf", "abs": "https://arxiv.org/abs/2509.10035", "title": "Linguistic trajectories of bipolar disorder on social media", "authors": ["Laurin Plank", "Armin Zlomuzica"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health."}
{"id": "2509.10040", "pdf": "https://arxiv.org/pdf/2509.10040.pdf", "abs": "https://arxiv.org/abs/2509.10040", "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "categories": ["cs.CL"], "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction."}
{"id": "2509.10078", "pdf": "https://arxiv.org/pdf/2509.10078.pdf", "abs": "https://arxiv.org/abs/2509.10078", "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication."}
{"id": "2509.10087", "pdf": "https://arxiv.org/pdf/2509.10087.pdf", "abs": "https://arxiv.org/abs/2509.10087", "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "categories": ["cs.CL"], "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information."}
{"id": "2509.10095", "pdf": "https://arxiv.org/pdf/2509.10095.pdf", "abs": "https://arxiv.org/abs/2509.10095", "title": "Arabic Large Language Models for Medical Text Generation", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "categories": ["cs.CL"], "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments."}
{"id": "2509.10108", "pdf": "https://arxiv.org/pdf/2509.10108.pdf", "abs": "https://arxiv.org/abs/2509.10108", "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems."}
{"id": "2509.10116", "pdf": "https://arxiv.org/pdf/2509.10116.pdf", "abs": "https://arxiv.org/abs/2509.10116", "title": "Prominence-aware automatic speech recognition for conversational speech", "authors": ["Julian Linke", "Barbara Schuppler"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper investigates prominence-aware automatic speech recognition (ASR)\nby combining prominence detection and speech recognition for conversational\nAustrian German. First, prominence detectors were developed by fine-tuning\nwav2vec2 models to classify word-level prominence. The detector was then used\nto automatically annotate prosodic prominence in a large corpus. Based on those\nannotations, we trained novel prominence-aware ASR systems that simultaneously\ntranscribe words and their prominence levels. The integration of prominence\ninformation did not change performance compared to our baseline ASR system,\nwhile reaching a prominence detection accuracy of 85.53% for utterances where\nthe recognized word sequence was correct. This paper shows that\ntransformer-based models can effectively encode prosodic information and\nrepresents a novel contribution to prosody-enhanced ASR, with potential\napplications for linguistic research and prosody-informed dialogue systems."}
{"id": "2509.10127", "pdf": "https://arxiv.org/pdf/2509.10127.pdf", "abs": "https://arxiv.org/abs/2509.10127", "title": "Population-Aligned Persona Generation for LLM-based Social Simulation", "authors": ["Zhengyu Hu", "Zheyuan Xiao", "Max Xiong", "Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Kaize Ding", "Ziang Xiao", "Nicholas Jing Yuan", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications."}
{"id": "2509.10129", "pdf": "https://arxiv.org/pdf/2509.10129.pdf", "abs": "https://arxiv.org/abs/2509.10129", "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce\n\\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that\ndecouples answer generation from spatial localization. This design makes it\napplicable to existing VLMs, including proprietary systems where fine-tuning is\nnot feasible. Through systematic evaluation, we provide quantitative insights\ninto the gap between textual accuracy and spatial grounding, showing that\ncorrect answers often lack reliable localization. Our standardized framework\nhighlights these shortcomings and establishes a benchmark for future research\ntoward more interpretable and robust document information extraction VLMs."}
{"id": "2509.10179", "pdf": "https://arxiv.org/pdf/2509.10179.pdf", "abs": "https://arxiv.org/abs/2509.10179", "title": "Benchmark of stylistic variation in LLM-generated texts", "authors": ["Jiří Milička", "Anna Marklová", "Václav Cvrček"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions."}
{"id": "2509.10184", "pdf": "https://arxiv.org/pdf/2509.10184.pdf", "abs": "https://arxiv.org/abs/2509.10184", "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations", "authors": ["Leen Almajed", "Abeer ALdayel"], "categories": ["cs.CL"], "comment": "This paper is under review", "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems."}
{"id": "2509.10199", "pdf": "https://arxiv.org/pdf/2509.10199.pdf", "abs": "https://arxiv.org/abs/2509.10199", "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "authors": ["Miklós Sebők", "Viktor Kovács", "Martin Bánóczy", "Daniel Møller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "categories": ["cs.CL", "I.7; I.2; J.4"], "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs."}
{"id": "2509.10208", "pdf": "https://arxiv.org/pdf/2509.10208.pdf", "abs": "https://arxiv.org/abs/2509.10208", "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning", "authors": ["Shengqiang Fu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models."}
{"id": "2509.10377", "pdf": "https://arxiv.org/pdf/2509.10377.pdf", "abs": "https://arxiv.org/abs/2509.10377", "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs", "authors": ["Yixiao Zhou", "Ziyu Zhao", "Dongzhou Cheng", "zhiliang wu", "Jie Gui", "Yi Yang", "Fei Wu", "Yu Cheng", "Hehe Fan"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025", "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice."}
{"id": "2509.10414", "pdf": "https://arxiv.org/pdf/2509.10414.pdf", "abs": "https://arxiv.org/abs/2509.10414", "title": "Is In-Context Learning Learning?", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability."}
{"id": "2509.10417", "pdf": "https://arxiv.org/pdf/2509.10417.pdf", "abs": "https://arxiv.org/abs/2509.10417", "title": "Long Context Automated Essay Scoring with Language Models", "authors": ["Christopher Ormerod", "Gitit Kehat"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures, 2 tables", "summary": "Transformer-based language models are architecturally constrained to process\ntext of a fixed maximum length. Essays written by higher-grade students\nfrequently exceed the maximum allowed length for many popular open-source\nmodels. A common approach to addressing this issue when using these models for\nAutomated Essay Scoring is to truncate the input text. This raises serious\nvalidity concerns as it undermines the model's ability to fully capture and\nevaluate organizational elements of the scoring rubric, which requires long\ncontexts to assess. In this study, we evaluate several models that incorporate\narchitectural modifications of the standard transformer architecture to\novercome these length limitations using the Kaggle ASAP 2.0 dataset. The models\nconsidered in this study include fine-tuned versions of XLNet, Longformer,\nModernBERT, Mamba, and Llama models."}
{"id": "2509.10436", "pdf": "https://arxiv.org/pdf/2509.10436.pdf", "abs": "https://arxiv.org/abs/2509.10436", "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment", "authors": ["Shadikur Rahman", "Aroosa Hameed", "Gautam Srivastava", "Syed Muhammad Danish"], "categories": ["cs.CL"], "comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing", "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture."}
{"id": "2509.10446", "pdf": "https://arxiv.org/pdf/2509.10446.pdf", "abs": "https://arxiv.org/abs/2509.10446", "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL", "authors": ["Rui Lu", "Zhenyu Hou", "Zihan Wang", "Hanchen Zhang", "Xiao Liu", "Yujiang Li", "Shi Feng", "Jie Tang", "Yuxiao Dong"], "categories": ["cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive."}
{"id": "2509.10452", "pdf": "https://arxiv.org/pdf/2509.10452.pdf", "abs": "https://arxiv.org/abs/2509.10452", "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers", "authors": ["Akshat Pandey", "Karun Kumar", "Raphael Tang"], "categories": ["cs.CL", "cs.LG"], "comment": "5 pages, 2 figures", "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios."}
{"id": "2509.09177", "pdf": "https://arxiv.org/pdf/2509.09177.pdf", "abs": "https://arxiv.org/abs/2509.09177", "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL", "authors": ["Hanyi Mao", "Quanjia Xiao", "Lei Pang", "Haixiao Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping\ndirectly in the importance-sampling (IS) weight space. We revisit\nsequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping\nis transplanted to sequences: a fixed clip range systematically reweights short\nvs. long responses, distorting the effective objective. Theoretically, we\nformalize length fairness via a Length Reweighting Error (LRE) and prove that\nsmall LRE yields a directional cosine guarantee between the clipped and true\nupdates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the\nsequence log-IS ratio with a band that applies a KL-corrected drift term and\nscales as $\\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,\nstabilizes training, and outperforms all baselines across multiple evaluation\ndatasets."}
{"id": "2509.09681", "pdf": "https://arxiv.org/pdf/2509.09681.pdf", "abs": "https://arxiv.org/abs/2509.09681", "title": "DB3 Team's Solution For Meta KDD Cup' 25", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents the db3 team's winning solution for the Meta CRAG-MM\nChallenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,\nmulti-turn question answering benchmark (CRAG-MM), we developed a comprehensive\nframework that integrates tailored retrieval pipelines for different tasks with\na unified LLM-tuning approach for hallucination control. Our solution features\n(1) domain-specific retrieval pipelines handling image-indexed knowledge\ngraphs, web sources, and multi-turn conversations; and (2) advanced refusal\ntraining using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd\nplace in Task 2, and 1st place in Task 3, securing the grand prize for\nexcellence in ego-centric queries through superior handling of first-person\nperspective challenges."}
{"id": "2509.09684", "pdf": "https://arxiv.org/pdf/2509.09684.pdf", "abs": "https://arxiv.org/abs/2509.09684", "title": "Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation", "authors": ["Bruno Yui Yamate", "Thais Rodrigues Neubauer", "Marcelo Fantinato", "Sarajane Marques Peres"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": "33 pages", "summary": "This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)\nbenchmark dataset designed for the text-to-SQL task in the process mining\ndomain. Text-to-SQL conversion facilitates natural language querying of\ndatabases, increasing accessibility for users without SQL expertise and\nproductivity for those that are experts. The text-2-SQL-4-PM dataset is\ncustomized to address the unique challenges of process mining, including\nspecialized vocabularies and single-table relational structures derived from\nevent logs. The dataset comprises 1,655 natural language utterances, including\nhuman-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods\ninclude manual curation by experts, professional translations, and a detailed\nannotation process to enable nuanced analyses of task complexity. Additionally,\na baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility\nof the dataset for text-to-SQL applications. The results show that\ntext-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering\nbroader applicability for semantic parsing and other natural language\nprocessing tasks."}
{"id": "2509.09688", "pdf": "https://arxiv.org/pdf/2509.09688.pdf", "abs": "https://arxiv.org/abs/2509.09688", "title": "AI-Powered Assistant for Long-Term Access to RHIC Knowledge", "authors": ["Mohammad Atif", "Vincent Garonne", "Eric Lancon", "Jerome Lauret", "Alexandr Prozorov", "Michal Vranovsky"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National\nLaboratory concludes 25 years of operation, preserving not only its vast data\nholdings ($\\sim$1 ExaByte) but also the embedded scientific knowledge becomes a\ncritical priority. The RHIC Data and Analysis Preservation Plan (DAPP)\nintroduces an AI-powered assistant system that provides natural language access\nto documentation, workflows, and software, with the aim of supporting\nreproducibility, education, and future discovery. Built upon Large Language\nModels using Retrieval-Augmented Generation and the Model Context Protocol,\nthis assistant indexes structured and unstructured content from RHIC\nexperiments and enables domain-adapted interaction. We report on the\ndeployment, computational performance, ongoing multi-experiment integration,\nand architectural features designed for a sustainable and explainable long-term\nAI access. Our experience illustrates how modern AI/ML tools can transform the\nusability and discoverability of scientific legacy data."}
{"id": "2509.09689", "pdf": "https://arxiv.org/pdf/2509.09689.pdf", "abs": "https://arxiv.org/abs/2509.09689", "title": "Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors", "authors": ["Himanshu Thakur", "Eshani Agrawal", "Smruthi Mukund"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "A long-standing challenge in developing accurate recommendation models is\nsimulating user behavior, mainly due to the complex and stochastic nature of\nuser interactions. Towards this, one promising line of work has been the use of\nLarge Language Models (LLMs) for simulating user behavior. However, aligning\nthese general-purpose large pre-trained models with user preferences\nnecessitates: (i) effectively and continously parsing large-scale tabular\nuser-item interaction data, (ii) overcoming pre-training-induced inductive\nbiases to accurately learn user specific knowledge, and (iii) achieving the\nformer two at scale for millions of users. While most previous works have\nfocused on complex methods to prompt an LLM or fine-tune it on tabular\ninteraction datasets, our approach shifts the focus to extracting robust\ntextual user representations using a frozen LLM and simulating cost-effective,\nresource-efficient user agents powered by fine-tuned Small Language Models\n(SLMs). Further, we showcase a method for training multiple low-rank adapters\nfor groups of users or \\textit{persona}, striking an optimal balance between\nscalability and performance of user behavior agents. Our experiments provide\ncompelling empirical evidence of the efficacy of our methods, demonstrating\nthat user agents developed using our approach have the potential to bridge the\ngap between offline metrics and real-world performance of recommender systems."}
{"id": "2509.09706", "pdf": "https://arxiv.org/pdf/2509.09706.pdf", "abs": "https://arxiv.org/abs/2509.09706", "title": "Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks", "authors": ["Taniya Gidatkar", "Oluwaseun Ajao", "Matthew Shardlow"], "categories": ["cs.CR", "cs.AI", "cs.CL", "I.2; H.3.3"], "comment": "8 pages, 4 tables, to appear in proceedings of Recent Advances in\n  Natural Language Processing (RANLP 2025) and ACL Anthology", "summary": "This study evaluates the resilience of large language models (LLMs) against\nadversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.\nUsing systematically designed adversarial tests through TextFooler and\nBERTAttack, we found significant variations in model robustness. RoBERTa-Base\nand FlanT5 demonstrated remarkable resilience, maintaining accuracy even when\nsubjected to sophisticated attacks, with attack success rates of 0%. In\ncontrast. BERT-Base showed considerable vulnerability, with TextFooler\nachieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.\nOur research reveals that while certain LLMs have developed effective defensive\nmechanisms, these safeguards often require substantial computational resources.\nThis study contributes to the understanding of LLM security by identifying\nexisting strengths and weaknesses in current safeguarding approaches and\nproposes practical recommendations for developing more efficient and effective\ndefensive strategies."}
{"id": "2509.09707", "pdf": "https://arxiv.org/pdf/2509.09707.pdf", "abs": "https://arxiv.org/abs/2509.09707", "title": "LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm", "authors": ["Camilo Chacón Sartori", "Martín Isla Pino", "Pedro Pinacho-Davidson", "Christian Blum"], "categories": ["cs.NE", "cs.AI", "cs.CL"], "comment": "Submitted to a journal for review", "summary": "Integrating Large Language Models (LLMs) within metaheuristics opens a novel\npath for solving complex combinatorial optimization problems. While most\nexisting approaches leverage LLMs for code generation to create or refine\nspecific heuristics, they often overlook the structural properties of\nindividual problem instances. In this work, we introduce a novel framework that\nintegrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the\nNP-hard Longest Run Subsequence problem. Our approach extends the\ninstance-driven heuristic bias paradigm by introducing a human-LLM\ncollaborative process to co-design and implement a set of computationally\nefficient metrics. The LLM analyzes these instance-specific metrics to generate\na tailored heuristic bias, which steers the BRKGA toward promising areas of the\nsearch space. We conduct a comprehensive experimental evaluation, including\nrigorous statistical tests, convergence and behavioral analyses, and targeted\nablation studies, comparing our method against a standard BRKGA baseline across\n1,050 generated instances of varying complexity. Results show that our\ntop-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically\nsignificant improvements over the baseline, particularly on the most complex\ninstances. Our findings confirm that leveraging an LLM to produce an a priori,\ninstance-driven heuristic bias is a valuable approach for enhancing\nmetaheuristics in complex optimization domains."}
{"id": "2509.09716", "pdf": "https://arxiv.org/pdf/2509.09716.pdf", "abs": "https://arxiv.org/abs/2509.09716", "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions", "authors": ["Jun Zhan", "Mingyang Han", "Yuxuan Xie", "Chen Wang", "Dong Zhang", "Kexin Huang", "Haoxiang Shi", "DongXiao Wang", "Tengtao Song", "Qinyuan Cheng", "Shimin Li", "Jun Song", "Xipeng Qiu", "Bo Zheng"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\n\\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}."}
{"id": "2509.09722", "pdf": "https://arxiv.org/pdf/2509.09722.pdf", "abs": "https://arxiv.org/abs/2509.09722", "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "authors": ["Taylor Archibald", "Tony Martinez"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models."}
{"id": "2509.09740", "pdf": "https://arxiv.org/pdf/2509.09740.pdf", "abs": "https://arxiv.org/abs/2509.09740", "title": "HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets", "authors": ["Ying Yuan", "Xing-Yue Monica Ge", "Aaron Archer Waterman", "Tommaso Biancalani", "David Richmond", "Yogesh Pandit", "Avtar Singh", "Russell Littman", "Jin Liu", "Jan-Christian Huetter", "Vladimir Ermakov"], "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large-scale single-cell and Perturb-seq investigations routinely involve\nclustering cells and subsequently annotating each cluster with Gene-Ontology\n(GO) terms to elucidate the underlying biological programs. However, both\nstages, resolution selection and functional annotation, are inherently\nsubjective, relying on heuristics and expert curation. We present\nHYPOGENEAGENT, a large language model (LLM)-driven framework, transforming\ncluster annotation into a quantitatively optimizable task. Initially, an LLM\nfunctioning as a gene-set analyst analyzes the content of each gene program or\nperturbation module and generates a ranked list of GO-based hypotheses,\naccompanied by calibrated confidence scores. Subsequently, we embed every\npredicted description with a sentence-embedding model, compute pair-wise cosine\nsimilarities, and let the agent referee panel score (i) the internal\nconsistency of the predictions, high average similarity within the same\ncluster, termed intra-cluster agreement (ii) their external distinctiveness,\nlow similarity between clusters, termed inter-cluster separation. These two\nquantities are combined to produce an agent-derived resolution score, which is\nmaximized when clusters exhibit simultaneous coherence and mutual exclusivity.\nWhen applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary\ntest, our Resolution Score selects clustering granularities that exhibit\nalignment with known pathway compared to classical metrics such silhouette\nscore, modularity score for gene functional enrichment summary. These findings\nestablish LLM agents as objective adjudicators of cluster resolution and\nfunctional annotation, thereby paving the way for fully automated,\ncontext-aware interpretation pipelines in single-cell multi-omics studies."}
{"id": "2509.09775", "pdf": "https://arxiv.org/pdf/2509.09775.pdf", "abs": "https://arxiv.org/abs/2509.09775", "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture", "authors": ["Aleksandr Boldachev"], "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.SE"], "comment": "22 pages, 6 figures", "summary": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework."}
{"id": "2509.09864", "pdf": "https://arxiv.org/pdf/2509.09864.pdf", "abs": "https://arxiv.org/abs/2509.09864", "title": "Latency and Token-Aware Test-Time Compute", "authors": ["Jenny Y. Huang", "Mehul Damani", "Yousef El-Kurdi", "Ramon Astudillo", "Wei Sun"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time scaling has emerged as a powerful way to improve large\nlanguage model (LLM) performance by generating multiple candidate responses and\nselecting among them. However, existing work on dynamic allocation for\ntest-time compute typically considers only parallel generation methods such as\nbest-of-N, overlooking incremental decoding methods like beam search, and has\nlargely ignored latency, focusing only on token usage. We formulate\ninference-time scaling as a problem of dynamic compute allocation and method\nselection, where the system must decide which strategy to apply and how much\ncompute to allocate on a per-query basis. Our framework explicitly incorporates\nboth token cost and wall-clock latency, the latter being critical for user\nexperience and particularly for agentic workflows where models must issue\nmultiple queries efficiently. Experiments on reasoning benchmarks show that our\napproach consistently outperforms static strategies, achieving favorable\naccuracy-cost trade-offs while remaining practical for deployment."}
{"id": "2509.09867", "pdf": "https://arxiv.org/pdf/2509.09867.pdf", "abs": "https://arxiv.org/abs/2509.09867", "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO", "authors": ["Yago Romano Matinez", "Jesse Roberts"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLMs promise to assist humans -- not just by answering questions, but by\noffering useful guidance across a wide range of tasks. But how far does that\nassistance go? Can a large language model based agent actually help someone\naccomplish their goal as an active participant? We test this question by\nengaging an LLM in UNO, a turn-based card game, asking it not to win but\ninstead help another player to do so. We built a tool that allows decoder-only\nLLMs to participate as agents within the RLCard game environment. These models\nreceive full game-state information and respond using simple text prompts under\ntwo distinct prompting strategies. We evaluate models ranging from small (1B\nparameters) to large (70B parameters) and explore how model scale impacts\nperformance. We find that while all models were able to successfully outperform\na random baseline when playing UNO, few were able to significantly aid another\nplayer."}
{"id": "2509.09870", "pdf": "https://arxiv.org/pdf/2509.09870.pdf", "abs": "https://arxiv.org/abs/2509.09870", "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks", "authors": ["Hasibur Rahman", "Smit Desai"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable conversational agents (CAs) to express\ndistinctive personalities, raising new questions about how such designs shape\nuser perceptions. This study investigates how personality expression levels and\nuser-agent personality alignment influence perceptions in goal-oriented tasks.\nIn a between-subjects experiment (N=150), participants completed travel\nplanning with CAs exhibiting low, medium, or high expression across the Big\nFive traits, controlled via our novel Trait Modulation Keys framework. Results\nrevealed an inverted-U relationship: medium expression produced the most\npositive evaluations across Intelligence, Enjoyment, Anthropomorphism,\nIntention to Adopt, Trust, and Likeability, significantly outperforming both\nextremes. Personality alignment further enhanced outcomes, with Extraversion\nand Emotional Stability emerging as the most influential traits. Cluster\nanalysis identified three distinct compatibility profiles, with \"Well-Aligned\"\nusers reporting substantially positive perceptions. These findings demonstrate\nthat personality expression and strategic trait alignment constitute optimal\ndesign targets for CA personality, offering design implications as LLM-based\nCAs become increasingly prevalent."}
{"id": "2509.09987", "pdf": "https://arxiv.org/pdf/2509.09987.pdf", "abs": "https://arxiv.org/abs/2509.09987", "title": "Whisper Has an Internal Word Aligner", "authors": ["Sung-Lin Yeh", "Yen Meng", "Hao Tang"], "categories": ["eess.AS", "cs.CL"], "comment": "ASRU 2025", "summary": "There is an increasing interest in obtaining accurate word-level timestamps\nfrom strong automatic speech recognizers, in particular Whisper. Existing\napproaches either require additional training or are simply not competitive.\nThe evaluation in prior work is also relatively loose, typically using a\ntolerance of more than 200 ms. In this work, we discover attention heads in\nWhisper that capture accurate word alignments and are distinctively different\nfrom those that do not. Moreover, we find that using characters produces finer\nand more accurate alignments than using wordpieces. Based on these findings, we\npropose an unsupervised approach to extracting word alignments by filtering\nattention heads while teacher forcing Whisper with characters. Our approach not\nonly does not require training but also produces word alignments that are more\naccurate than prior work under a stricter tolerance between 20 ms and 100 ms."}
{"id": "2509.10031", "pdf": "https://arxiv.org/pdf/2509.10031.pdf", "abs": "https://arxiv.org/abs/2509.10031", "title": "Unified Learnable 2D Convolutional Feature Extraction for ASR", "authors": ["Peter Vieting", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Neural front-ends represent a promising approach to feature extraction for\nautomatic speech recognition (ASR) systems as they enable to learn specifically\ntailored features for different tasks. Yet, many of the existing techniques\nremain heavily influenced by classical methods. While this inductive bias may\nease the system design, our work aims to develop a more generic front-end for\nfeature extraction. Furthermore, we seek to unify the front-end architecture\ncontrasting with existing approaches that apply a composition of several layer\ntopologies originating from different sources. The experiments systematically\nshow how to reduce the influence of existing techniques to achieve a generic\nfront-end. The resulting 2D convolutional front-end is parameter-efficient and\nsuitable for a scenario with limited computational resources unlike large\nmodels pre-trained on unlabeled audio. The results demonstrate that this\ngeneric unified approach is not only feasible but also matches the performance\nof existing supervised learnable feature extractors."}
{"id": "2509.10105", "pdf": "https://arxiv.org/pdf/2509.10105.pdf", "abs": "https://arxiv.org/abs/2509.10105", "title": "VARCO-VISION-2.0 Technical Report", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "categories": ["cs.CV", "cs.CL"], "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model."}
{"id": "2509.10143", "pdf": "https://arxiv.org/pdf/2509.10143.pdf", "abs": "https://arxiv.org/abs/2509.10143", "title": "Error Analysis in a Modular Meeting Transcription System", "authors": ["Peter Vieting", "Simon Berger", "Thilo von Neumann", "Christoph Boeddeker", "Ralf Schlüter", "Reinhold Haeb-Umbach"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Meeting transcription is a field of high relevance and remarkable progress in\nrecent years. Still, challenges remain that limit its performance. In this\nwork, we extend a previously proposed framework for analyzing leakage in speech\nseparation with proper sensitivity to temporal locality. We show that there is\nsignificant leakage to the cross channel in areas where only the primary\nspeaker is active. At the same time, the results demonstrate that this does not\naffect the final performance much as these leaked parts are largely ignored by\nthe voice activity detection (VAD). Furthermore, different segmentations are\ncompared showing that advanced diarization approaches are able to reduce the\ngap to oracle segmentation by a third compared to a simple energy-based VAD. We\nadditionally reveal what factors contribute to the remaining difference. The\nresults represent state-of-the-art performance on LibriCSS among systems that\ntrain the recognition module on LibriSpeech data only."}
{"id": "2509.10401", "pdf": "https://arxiv.org/pdf/2509.10401.pdf", "abs": "https://arxiv.org/abs/2509.10401", "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Zhen Lin", "Yue Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution."}
{"id": "2405.13798", "pdf": "https://arxiv.org/pdf/2405.13798.pdf", "abs": "https://arxiv.org/abs/2405.13798", "title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models", "authors": ["Tyler Bell", "Avinash Mudireddy", "Ivan Johnson-Eversoll", "Soura Dasgupta", "Raghu Mudumbai"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "We prove a new asymptotic un-equipartition property for the perplexity of\nlong texts generated by a language model and present supporting experimental\nevidence from open-source models. Specifically we show that the logarithmic\nperplexity of any large text generated by a language model must asymptotically\nconverge to the average entropy of its token distributions. This defines a\n``typical set'' that all long synthetic texts generated by a language model\nmust belong to. We refine the concept of ''typical set'' to include only\ngrammatically correct texts. We then show that this refined typical set is a\nvanishingly small subset of all possible grammatically correct texts for a very\ngeneral definition of grammar. This means that language models are strongly\nconstrained in the range of their possible behaviors and outputs. We make no\nsimplifying assumptions (such as stationarity) about the statistics of language\nmodel outputs, and therefore our results are directly applicable to practical\nreal-world models without any approximations. We discuss possible applications\nof the typical set concept to problems such as detecting synthetic texts and\nmembership inference in training datasets."}
{"id": "2406.18173", "pdf": "https://arxiv.org/pdf/2406.18173.pdf", "abs": "https://arxiv.org/abs/2406.18173", "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs", "authors": ["Wenhao Li", "Mingbao Lin", "Yunshan Zhong", "Shuicheng Yan", "Rongrong Ji"], "categories": ["cs.CL"], "comment": "This article was not accepted, and its quality is not very good.\n  Therefore, we have decided to withdraw the submission and will not resubmit\n  it elsewhere", "summary": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases."}
{"id": "2409.14664", "pdf": "https://arxiv.org/pdf/2409.14664.pdf", "abs": "https://arxiv.org/abs/2409.14664", "title": "Direct Judgement Preference Optimization", "authors": ["Peifeng Wang", "Austin Xu", "Yilun Zhou", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Auto-evaluation is crucial for assessing response quality and offering\nfeedback for model development. Recent studies have explored training large\nlanguage models (LLMs) as generative judges to evaluate and critique other\nmodels' outputs. In this work, we investigate the idea of learning from both\npositive and negative data with preference optimization to enhance the\nevaluation capabilities of LLM judges across an array of different use cases.\nWe achieve this by employing three approaches to collect the preference pairs\nfor different use cases, each aimed at improving our generative judge from a\ndifferent perspective. Our comprehensive study over a wide range of benchmarks\ndemonstrates the effectiveness of our method. In particular, our generative\njudge achieves the best performance on 10 out of 13 benchmarks, outperforming\nstrong baselines like GPT-4o and specialized judge models. Further analysis\nshow that our judge model robustly counters inherent biases such as position\nand length bias, flexibly adapts to any evaluation protocol specified by\npractitioners, and provides helpful language feedback for improving downstream\ngenerator models."}
{"id": "2410.16708", "pdf": "https://arxiv.org/pdf/2410.16708.pdf", "abs": "https://arxiv.org/abs/2410.16708", "title": "Atomic Fact Decomposition Helps Attributed Question Answering", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Attributed Question Answering (AQA) aims to provide both a trustworthy answer\nand a reliable attribution report for a given question. Retrieval is a widely\nadopted approach, including two general paradigms: Retrieval-Then-Read (RTR)\nand post-hoc retrieval. Recently, Large Language Models (LLMs) have shown\nremarkable proficiency, prompting growing interest in AQA among researchers.\nHowever, RTR-based AQA often suffers from irrelevant knowledge and rapidly\nchanging information, even when LLMs are adopted, while post-hoc\nretrieval-based AQA struggles with comprehending long-form answers with complex\nlogic, and precisely identifying the content needing revision and preserving\nthe original intent. To tackle these problems, this paper proposes an Atomic\nfact decomposition-based Retrieval and Editing (ARE) framework, which\ndecomposes the generated long-form answers into molecular clauses and atomic\nfacts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are\nfine-tuned using a well-constructed dataset, generated from large scale\nKnowledge Graphs (KGs). This process involves extracting one-hop neighbors from\na given set of entities and transforming the result into coherent long-form\ntext. Subsequently, ARE leverages a search engine to retrieve evidences related\nto atomic facts, inputting these evidences into an LLM-based verifier to\ndetermine whether the facts require expansion for re-retrieval or editing.\nFurthermore, the edited facts are backtracked into the original answer, with\nevidence aggregated based on the relationship between molecular clauses and\natomic facts. Extensive evaluations demonstrate the superior performance of our\nproposed method over the state-of-the-arts on several datasets, with an\nadditionally proposed new metric $Attr_{p}$ for evaluating the precision of\nevidence attribution."}
{"id": "2410.18889", "pdf": "https://arxiv.org/pdf/2410.18889.pdf", "abs": "https://arxiv.org/abs/2410.18889", "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance", "authors": ["Omer Nahum", "Nitay Calderon", "Orgad Keller", "Idan Szpektor", "Roi Reichart"], "categories": ["cs.CL"], "comment": null, "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. We\nconduct a case study on four factual consistency datasets from the TRUE\nbenchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale\nratings of summary quality across multiple dimensions. We empirically analyze\nthe labeling quality of existing datasets and compare expert, crowd-sourced,\nand LLM-based annotations in terms of the agreement, label quality, and\nefficiency, demonstrating the strengths and limitations of each annotation\nmethod. Our findings reveal a substantial number of label errors, which, when\ncorrected, induce a significant upward shift in reported model performance.\nThis suggests that many of the LLMs' so-called mistakes are due to label errors\nrather than genuine model failures. Additionally, we discuss the implications\nof mislabeled data and propose methods to mitigate them in training to improve\nperformance."}
{"id": "2412.00559", "pdf": "https://arxiv.org/pdf/2412.00559.pdf", "abs": "https://arxiv.org/abs/2412.00559", "title": "Polish-English medical knowledge transfer: A new benchmark and results", "authors": ["Łukasz Grzybowski", "Jakub Pokrywka", "Michał Ciesiółka", "Jeremi I. Kaczmarek", "Marek Kubis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice."}
{"id": "2412.01340", "pdf": "https://arxiv.org/pdf/2412.01340.pdf", "abs": "https://arxiv.org/abs/2412.01340", "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls", "authors": ["Sheikh Shafayat", "Dongkeun Yoon", "Woori Jang", "Jiwoo Choi", "Alice Oh", "Seohyon Jung"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."}
{"id": "2412.10924", "pdf": "https://arxiv.org/pdf/2412.10924.pdf", "abs": "https://arxiv.org/abs/2412.10924", "title": "Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning", "authors": ["Julia Witte Zimmerman", "Denis Hudon", "Kathryn Cramer", "Alejandro J. Ruiz", "Calla Beauregard", "Ashley Fehr", "Mikaela Irene Fudolig", "Bradford Demarest", "Yoshi Meke Bird", "Milo Z. Trujillo", "Christopher M. Danforth", "Peter Sheridan Dodds"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]"}
{"id": "2502.10990", "pdf": "https://arxiv.org/pdf/2502.10990.pdf", "abs": "https://arxiv.org/abs/2502.10990", "title": "FinMTEB: Finance Massive Text Embedding Benchmark", "authors": ["Yixuan Tang", "Yi Yang"], "categories": ["cs.CL", "cs.IR"], "comment": "EMNLP 2025, https://github.com/yixuantt/FinMTEB", "summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models."}
{"id": "2504.11829", "pdf": "https://arxiv.org/pdf/2504.11829.pdf", "abs": "https://arxiv.org/abs/2504.11829", "title": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development."}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204.pdf", "abs": "https://arxiv.org/abs/2505.13204", "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 Main", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."}
{"id": "2505.14160", "pdf": "https://arxiv.org/pdf/2505.14160.pdf", "abs": "https://arxiv.org/abs/2505.14160", "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual vision-language models (VLMs) promise universal image-text\nretrieval, yet their social biases remain underexplored. We perform the first\nsystematic audit of four public multilingual CLIP variants: M-CLIP, NLLB-CLIP,\nCAPIVARA-CLIP, and the debiased SigLIP-2, covering ten languages that differ in\nresource availability and morphological gender marking. Using balanced subsets\nof FairFace and the PATA stereotype suite in a zero-shot setting, we quantify\nrace and gender bias and measure stereotype amplification. Contrary to the\nintuition that multilinguality mitigates bias, every model exhibits stronger\ngender skew than its English-only baseline. CAPIVARA-CLIP shows its largest\nbiases precisely in the low-resource languages it targets, while the shared\nencoder of NLLB-CLIP and SigLIP-2 transfers English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this leakage.\nAlthough SigLIP-2 reduces agency and communion skews, it inherits -- and in\ncaption-sparse contexts (e.g., Xhosa) amplifies -- the English anchor's crime\nassociations. Highly gendered languages consistently magnify all bias types,\nyet gender-neutral languages remain vulnerable whenever cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics thus mask\nlanguage-specific hot spots, underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual VLM research."}
{"id": "2505.17222", "pdf": "https://arxiv.org/pdf/2505.17222.pdf", "abs": "https://arxiv.org/abs/2505.17222", "title": "Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts", "authors": ["Georgios Chochlakis", "Peter Wu", "Arjun Bedi", "Marcus Ma", "Kristina Lerman", "Shrikanth Narayanan"], "categories": ["cs.CL"], "comment": "Accepted to the Main Proceedings of EMNLP, 2025. 20 pages, 16\n  figures, 10 tables", "summary": "Modeling complex subjective tasks in Natural Language Processing, such as\nrecognizing emotion and morality, is considerably challenging due to\nsignificant variation in human annotations. This variation often reflects\nreasonable differences in semantic interpretations rather than mere noise,\nnecessitating methods to distinguish between legitimate subjectivity and error.\nWe address this challenge by exploring label verification in these contexts\nusing Large Language Models (LLMs). First, we propose a simple In-Context\nLearning binary filtering baseline that estimates the reasonableness of a\ndocument-label pair. We then introduce the Label-in-a-Haystack setting: the\nquery and its label(s) are included in the demonstrations shown to LLMs, which\nare prompted to predict the label(s) again, while receiving task-specific\ninstructions (e.g., emotion recognition) rather than label copying. We show how\nthe failure to copy the label(s) to the output of the LLM are task-relevant and\ninformative. Building on this, we propose the Label-in-a-Haystack Rectification\n(LiaHR) framework for subjective label correction: when the model outputs\ndiverge from the reference gold labels, we assign the generated labels to the\nexample instead of discarding it. This approach can be integrated into\nannotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,\nhuman evaluations, and ecological validity studies verify the utility of LiaHR\nfor label correction. Code is available at https://github.com/gchochla/liahr."}
{"id": "2505.18383", "pdf": "https://arxiv.org/pdf/2505.18383.pdf", "abs": "https://arxiv.org/abs/2505.18383", "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities", "authors": ["Abdellah El Mekki", "Houdaifa Atou", "Omer Nacar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development."}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634.pdf", "abs": "https://arxiv.org/abs/2505.19634", "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios."}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899.pdf", "abs": "https://arxiv.org/abs/2506.07899", "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably-without retraining or forgetting previous\ninformation-remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks for LLaMA-3 and Mistral\nbackbones demonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting."}
{"id": "2507.13335", "pdf": "https://arxiv.org/pdf/2507.13335.pdf", "abs": "https://arxiv.org/abs/2507.13335", "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes", "authors": ["Tyler Loakman", "William Thorne", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "Accepted to Findings of EMNLP 2025", "summary": "Humour, as a complex language form, is derived from myriad aspects of life.\nWhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes, we investigate whether the ability of Large Language\nModels (LLMs) to explain humour depends on the particular form. We compare\nmodels' joke explanation abilities from simple puns to complex topical humour\nthat requires esoteric knowledge of real-world entities and events. To this\nend, we curate a dataset of 600 jokes across 4 joke types and manually write\nhigh-quality explanations. These jokes include heterographic and homographic\npuns, contemporary internet humour, and topical jokes. Using this dataset, we\ncompare the zero-shot abilities of a range of LLMs to accurately and\ncomprehensively explain jokes of different types, identifying key research gaps\nin the task of humour explanation. We find that none of the tested models\n(including reasoning models) are capable of reliably generating adequate\nexplanations of all joke types, further highlighting the narrow focus of most\nexisting works on overly simple joke forms."}
{"id": "2507.20241", "pdf": "https://arxiv.org/pdf/2507.20241.pdf", "abs": "https://arxiv.org/abs/2507.20241", "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models", "authors": ["Yi Feng", "Jiaqi Wang", "Wenxuan Zhang", "Zhuang Chen", "Yutong Shen", "Xiyao Xiao", "Minlie Huang", "Liping Jing", "Jian Yu"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications."}
{"id": "2508.08791", "pdf": "https://arxiv.org/pdf/2508.08791.pdf", "abs": "https://arxiv.org/abs/2508.08791", "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments", "authors": ["Junjie Ye", "Changhao Jiang", "Zhengyin Du", "Yufei Xu", "Xuesong Yao", "Zhiheng Xi", "Xiaoran Fan", "Qi Zhang", "Tao Gui", "Xuanjing Huang", "Jiecao Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models."}
{"id": "2508.09337", "pdf": "https://arxiv.org/pdf/2508.09337.pdf", "abs": "https://arxiv.org/abs/2508.09337", "title": "Decoding Neural Emotion Patterns through Large Language Model Embeddings", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression."}
{"id": "2509.01328", "pdf": "https://arxiv.org/pdf/2509.01328.pdf", "abs": "https://arxiv.org/abs/2509.01328", "title": "Can Large Language Models Master Complex Card Games?", "authors": ["Wei Wang", "Felix Henry", "Junzhe Chen", "Dan Zhang", "Shiyu Huang", "Evgeny Kharlamov", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs."}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806.pdf", "abs": "https://arxiv.org/abs/2509.06806", "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."}
{"id": "2509.07980", "pdf": "https://arxiv.org/pdf/2509.07980.pdf", "abs": "https://arxiv.org/abs/2509.07980", "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning", "authors": ["Tong Zheng", "Hongming Zhang", "Wenhao Yu", "Xiaoyang Wang", "Runpeng Dai", "Rui Liu", "Huiwen Bao", "Chengsong Huang", "Heng Huang", "Dong Yu"], "categories": ["cs.CL"], "comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/", "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1."}
{"id": "2412.12039", "pdf": "https://arxiv.org/pdf/2412.12039.pdf", "abs": "https://arxiv.org/abs/2412.12039", "title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection", "authors": ["Ira Ceka", "Feitong Qiao", "Anik Dey", "Aastha Valecha", "Gail Kaiser", "Baishakhi Ray"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "comment": null, "summary": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on safety-critical code tasks such as vulnerability detection.\nTypically, static analysis (SA) tools, like CodeQL, CodeGuru Security, etc.,\nare used for vulnerability detection. SA relies on predefined, manually-crafted\nrules for flagging various vulnerabilities. Thus, effectiveness of SA in\ndetecting vulnerabilities depends on human experts and is known to report high\nerror rates. In this study we investigate whether LLM prompting can be an\neffective alternative to these static analyzers in the partial code setting. We\npropose prompting strategies that integrate natural language instructions of\nvulnerabilities with contrastive chain-of-thought reasoning, augmented using\ncontrastive samples from a synthetic dataset. Our findings demonstrate that\nsecurity-aware prompting techniques can be effective alternatives to the\nlaborious, hand-crafted rules of static analyzers, which often result in high\nfalse negative rates in the partial code setting. When leveraging SOTA\nreasoning models such as DeepSeek-R1, each of our prompting strategies exceeds\nthe static analyzer baseline, with the best strategies improving accuracy by as\nmuch as 31.6%, F1-scores by 71.7%, pairwise accuracies by 60.4%, and reducing\nFNR by as much as 37.6%."}
{"id": "2412.19087", "pdf": "https://arxiv.org/pdf/2412.19087.pdf", "abs": "https://arxiv.org/abs/2412.19087", "title": "MoPD: Mixture-of-Prompts Distillation for Vision-Language Models", "authors": ["Yang Chen", "Shuai Fu", "Yu Zhang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Soft prompt learning methods are effective for adapting vision-language\nmodels (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a\ntendency of existing methods that they overfit seen classes and exhibit\ndegraded performance on unseen classes. This limitation is due to the inherent\nbias in the training data towards the seen classes. To address this issue, we\npropose a novel soft prompt learning method, named Mixture-of-Prompts\nDistillation (MoPD), which can effectively transfer useful knowledge from hard\nprompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft\nprompt (a.k.a. student prompt), thereby enhancing the generalization ability of\nsoft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a\ngating network that learns to select hard prompts used for prompt distillation.\nExtensive experiments demonstrate that the proposed MoPD method outperforms\nstate-of-the-art baselines especially on on unseen classes."}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996.pdf", "abs": "https://arxiv.org/abs/2507.04996", "title": "Agentic Vehicles for Human-Centered Mobility Systems", "authors": ["Jiangbo Yu"], "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Autonomous\nvehicles (AuVs) are therefore understood as systems that perceive their\nenvironment and execute pre-programmed tasks independently of external input,\nconsistent with the SAE levels of automated driving. Yet recent research and\nreal-world deployments have begun to showcase vehicles that exhibit behaviors\noutside the scope of this definition. These include natural language\ninteraction with humans, goal adaptation, contextual reasoning, external tool\nuse, and the handling of unforeseen ethical dilemmas, enabled in part by\nmultimodal large language models (LLMs). These developments highlight not only\na gap between technical autonomy and the broader cognitive and social\ncapacities required for human-centered mobility, but also the emergence of a\nform of vehicle intelligence that currently lacks a clear designation. To\naddress this gap, the paper introduces the concept of agentic vehicles (AgVs):\nvehicles that integrate agentic AI systems to reason, adapt, and interact\nwithin complex environments. It synthesizes recent advances in agentic systems\nand suggests how AgVs can complement and even reshape conventional autonomy to\nensure mobility services are aligned with user and societal needs. The paper\nconcludes by outlining key challenges in the development and governance of AgVs\nand their potential role in shaping future agentic transportation systems."}
{"id": "2508.13654", "pdf": "https://arxiv.org/pdf/2508.13654.pdf", "abs": "https://arxiv.org/abs/2508.13654", "title": "Input-Time Scaling", "authors": ["Rapheal Huang", "Weilong Guo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."}
{"id": "2508.19005", "pdf": "https://arxiv.org/pdf/2508.19005.pdf", "abs": "https://arxiv.org/abs/2508.19005", "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm"}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.03897", "pdf": "https://arxiv.org/pdf/2509.03897.pdf", "abs": "https://arxiv.org/abs/2509.03897", "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation", "authors": ["Xiaofu Chen", "Israfel Salazar", "Yova Kementchedjhieva"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS."}
{"id": "2509.09009", "pdf": "https://arxiv.org/pdf/2509.09009.pdf", "abs": "https://arxiv.org/abs/2509.09009", "title": "Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison", "authors": ["Marianna Nezhurina", "Jörg Franke", "Taishi Nakamura", "Timur Carstensen", "Niccolò Ajroldi", "Ville Komulainen", "David Salinas", "Jenia Jitsev"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Model weights and intermediate checkpoints are available at\n  https://huggingface.co/collections/open-sci/open-sci-ref-001-685905e598be658fbcebff4f;\n  code for reproducing training, evaluation and raw experiments data at\n  https://github.com/LAION-AI/open-sci-ref-0.01", "summary": "We introduce open-sci-ref, a family of dense transformer models trained as\nresearch baselines across multiple model (0.13B to 1.7B parameters) and token\nscales (up to 1T) on 8 recent open reference datasets. Evaluating the models on\nvarious standardized benchmarks, our training runs set establishes reference\npoints that enable researchers to assess the sanity and quality of alternative\ntraining approaches across scales and datasets. Intermediate checkpoints allow\ncomparison and studying of the training dynamics. The established reference\nbaselines allow training procedures to be compared through their scaling\ntrends, aligning them on a common compute axis. Comparison of open reference\ndatasets reveals that training on NemoTron-CC HQ consistently outperforms other\nreference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to\nintermediate training checkpoints, the release includes logs, code, and\ndownstream evaluations to simplify reproduction, standardize comparison, and\nfacilitate future research."}
{"id": "2509.09332", "pdf": "https://arxiv.org/pdf/2509.09332.pdf", "abs": "https://arxiv.org/abs/2509.09332", "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible. To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io"}
{"id": "2509.09631", "pdf": "https://arxiv.org/pdf/2509.09631.pdf", "abs": "https://arxiv.org/abs/2509.09631", "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech", "authors": ["Ngoc-Son Nguyen", "Hieu-Nghia Huynh-Nguyen", "Thanh V. T. Tran", "Truong-Son Hy", "Van Nguyen"], "categories": ["cs.SD", "cs.CL", "cs.CV"], "comment": null, "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines."}
