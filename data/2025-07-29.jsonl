{"id": "2507.19483", "pdf": "https://arxiv.org/pdf/2507.19483.pdf", "abs": "https://arxiv.org/abs/2507.19483", "title": "The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "comment": "39 Pages, no figures", "summary": "AI systems now function as cognitive extensions, evolving from tools to\nactive cognitive collaborators within human-AI integrated systems. While these\nsystems can amplify cognition - enhancing problem-solving, learning, and\ncreativity - they present a fundamental \"comfort-growth paradox\": AI's\nuser-friendly nature may foster intellectual stagnation by minimizing cognitive\nfriction necessary for development. As AI aligns with user preferences and\nprovides frictionless assistance, it risks inducing cognitive complacency\nrather than promoting growth. We introduce Enhanced Cognitive Scaffolding to\nresolve this paradox - reconceptualizing AI from convenient assistant to\ndynamic mentor. Drawing from Vygotskian theories, educational scaffolding\nprinciples, and AI ethics, our framework integrates three dimensions: (1)\nProgressive Autonomy, where AI support gradually fades as user competence\nincreases; (2) Adaptive Personalization, tailoring assistance to individual\nneeds and learning trajectories; and (3) Cognitive Load Optimization, balancing\nmental effort to maximize learning while minimizing unnecessary complexity.\nResearch across educational, workplace, creative, and healthcare domains\nsupports this approach, demonstrating accelerated skill acquisition, improved\nself-regulation, and enhanced higher-order thinking. The framework includes\nsafeguards against risks like dependency, skill atrophy, and bias\namplification. By prioritizing cognitive development over convenience in\nhuman-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward\ngenuinely amplified cognition while safeguarding autonomous thought and\ncontinuous learning."}
{"id": "2507.19485", "pdf": "https://arxiv.org/pdf/2507.19485.pdf", "abs": "https://arxiv.org/abs/2507.19485", "title": "Creativity as a Human Right: Design Considerations for Computational Creativity Systems", "authors": ["Alayt Issak"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "We investigate creativity that is underlined in the Universal Declaration of\nHuman Rights (UDHR) to present design considerations for Computational\nCreativity (CC) systems. We find this declaration to describe creativity in\nsalient aspects and bring to light creativity as a Human Right attributed to\nthe Fourth Generation of such rights. This generation of rights attributes CC\nsystems and the evolving nature of interaction with entities of shared\nintelligence. Our methodology examines five of thirty articles from the UDHR\nand demonstrates each article with actualizations concluding with design\nconsiderations for each. We contribute our findings to ground the relationship\nbetween creativity and CC systems."}
{"id": "2507.19486", "pdf": "https://arxiv.org/pdf/2507.19486.pdf", "abs": "https://arxiv.org/abs/2507.19486", "title": "Confirmation bias: A challenge for scalable oversight", "authors": ["Gabriel Recchia", "Chatrik Singh Mangat", "Jinu Nyachhyon", "Mridul Sharma", "Callum Canavan", "Dylan Epstein-Gross", "Muhammed Abdulbari"], "categories": ["cs.HC", "cs.AI"], "comment": "61 pages, 8 figures", "summary": "Scalable oversight protocols aim to empower evaluators to accurately verify\nAI models more capable than themselves. However, human evaluators are subject\nto biases that can lead to systematic errors. We conduct two studies examining\nthe performance of simple oversight protocols where evaluators know that the\nmodel is \"correct most of the time, but not all of the time\". We find no\noverall advantage for the tested protocols, although in Study 1, showing\narguments in favor of both answers improves accuracy in cases where the model\nis incorrect. In Study 2, participants in both groups become more confident in\nthe system's answers after conducting online research, even when those answers\nare incorrect. We also reanalyze data from prior work that was more optimistic\nabout simple protocols, finding that human evaluators possessing knowledge\nabsent from models likely contributed to their positive results--an advantage\nthat diminishes as models continue to scale in capability. These findings\nunderscore the importance of testing the degree to which oversight protocols\nare robust to evaluator biases, whether they outperform simple deference to the\nmodel under evaluation, and whether their performance scales with increasing\nproblem difficulty and model capability."}
{"id": "2507.19488", "pdf": "https://arxiv.org/pdf/2507.19488.pdf", "abs": "https://arxiv.org/abs/2507.19488", "title": "E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets", "authors": ["Alexandros Gazis", "Eleftheria Katsiri"], "categories": ["cs.HC", "cs.CY", "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0"], "comment": "The article is under review by MDPI, Electronics journal. 36 pages,\n  20 figures, 67 references", "summary": "E-polis is a serious digital game designed to gamify sociological surveys\nstudying young people's political opinions. In this platform game, players\nnavigate a digital world, encountering quests posing sociological questions.\nPlayers' answers shape the city-game world, altering building structures based\non their choices. E-polis is a serious game, not a government simulation,\naiming to understand players' behaviors and opinions thus we do not train the\nplayers but rather understand them and help them visualize their choices in\nshaping a city's future. Also, it is noticed that no correct or incorrect\nanswers apply. Moreover, our game utilizes a novel middleware architecture for\ndevelopment, diverging from typical asset prefab scene and script segregation.\nThis article presents the data layer of our game's middleware, specifically\nfocusing on data analysis based on respondents' gameplay answers. E-polis\nrepresents an innovative approach to gamifying sociological research, providing\na unique platform for gathering and analyzing data on political opinions among\nyouth and contributing to the broader field of serious games."}
{"id": "2507.19511", "pdf": "https://arxiv.org/pdf/2507.19511.pdf", "abs": "https://arxiv.org/abs/2507.19511", "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media", "authors": ["Khalid Hasan", "Jamil Saquer", "Mukulika Ghosh"], "categories": ["cs.CL", "cs.LG"], "comment": "The 49th IEEE International Conference on Computers, Software, and\n  Applications (COMPSAC 2025) (camera-ready)", "summary": "The rising prevalence of mental health disorders necessitates the development\nof robust, automated tools for early detection and monitoring. Recent advances\nin Natural Language Processing (NLP), particularly transformer-based\narchitectures, have demonstrated significant potential in text analysis. This\nstudy provides a comprehensive evaluation of state-of-the-art transformer\nmodels (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term\nMemory (LSTM) based approaches using different text embedding techniques for\nmental health disorder classification on Reddit. We construct a large annotated\ndataset, validating its reliability through statistical judgmental analysis and\ntopic modeling. Experimental results demonstrate the superior performance of\ntransformer models over traditional deep-learning approaches. RoBERTa achieved\nthe highest classification performance, with a 99.54% F1 score on the hold-out\ntest set and a 96.05% F1 score on the external test set. Notably, LSTM models\naugmented with BERT embeddings proved highly competitive, achieving F1 scores\nexceeding 94% on the external dataset while requiring significantly fewer\ncomputational resources. These findings highlight the effectiveness of\ntransformer-based models for real-time, scalable mental health monitoring. We\ndiscuss the implications for clinical applications and digital mental health\ninterventions, offering insights into the capabilities and limitations of\nstate-of-the-art NLP methodologies in mental disorder detection."}
{"id": "2507.19490", "pdf": "https://arxiv.org/pdf/2507.19490.pdf", "abs": "https://arxiv.org/abs/2507.19490", "title": "RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information", "authors": ["Xinzheng Wu", "Junyi Chen", "Peiyi Wang", "Shunxiang Chen", "Yong Shen"], "categories": ["cs.HC", "cs.CV"], "comment": "Submitted for ITSC 2025", "summary": "In the research and development (R&D) and verification and validation (V&V)\nphases of autonomous driving decision-making and planning systems, it is\nnecessary to integrate human factors to achieve decision-making and evaluation\nthat align with human cognition. However, most existing datasets primarily\nfocus on vehicle motion states and trajectories, neglecting human-related\ninformation. In addition, current naturalistic driving datasets lack sufficient\nsafety-critical scenarios while simulated datasets suffer from low\nauthenticity. To address these issues, this paper constructs the Risk-Informed\nSubjective Evaluation and Eye-tracking (RISEE) dataset which specifically\ncontains human subjective evaluations and eye-tracking data apart from regular\nnaturalistic driving trajectories. By leveraging the complementary advantages\nof drone-based (high realism and extensive scenario coverage) and\nsimulation-based (high safety and reproducibility) data collection methods, we\nfirst conduct drone-based traffic video recording at a highway ramp merging\narea. After that, the manually selected highly interactive scenarios are\nreconstructed in simulation software, and drivers' first-person view (FPV)\nvideos are generated, which are then viewed and evaluated by recruited\nparticipants. During the video viewing process, participants' eye-tracking data\nis collected. After data processing and filtering, 3567 valid subjective risk\nratings from 101 participants across 179 scenarios are retained, along with\n2045 qualified eye-tracking data segments. The collected data and examples of\nthe generated FPV videos are available in our website."}
{"id": "2507.19521", "pdf": "https://arxiv.org/pdf/2507.19521.pdf", "abs": "https://arxiv.org/abs/2507.19521", "title": "Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables", "authors": ["Vishakh Padmakumar", "Joseph Chee Chang", "Kyle Lo", "Doug Downey", "Aakanksha Naik"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing volume of academic literature makes it essential for\nresearchers to organize, compare, and contrast collections of documents. Large\nlanguage models (LLMs) can support this process by generating schemas defining\nshared aspects along which to compare papers. However, progress on schema\ngeneration has been slow due to: (i) ambiguity in reference-based evaluations,\nand (ii) lack of editing/refinement methods. Our work is the first to address\nboth issues. First, we present an approach for augmenting unannotated table\ncorpora with synthesized intents and apply it to create a dataset for studying\nschema generation conditioned on a given information need, thus reducing\nambiguity. With this dataset, we show how incorporating table intents\nsignificantly improves baseline performance in reconstructing reference\nschemas. Next, we propose several LLM-based schema editing techniques. We start\nby comprehensively benchmarking several single-shot schema generation methods,\nincluding prompted LLM workflows and fine-tuned models, showing that smaller,\nopen-weight models can be fine-tuned to be competitive with state-of-the-art\nprompted LLMs. Then we demonstrate that our editing techniques can further\nimprove schemas generated by these methods."}
{"id": "2507.19491", "pdf": "https://arxiv.org/pdf/2507.19491.pdf", "abs": "https://arxiv.org/abs/2507.19491", "title": "Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory using Consumer Wearables", "authors": ["Peter Neigel", "David Antony Selby", "Shota Arai", "Benjamin Tag", "Niels van Berkel", "Sebastian Vollmer", "Andrew Vargo", "Koichi Kise"], "categories": ["cs.HC", "cs.CY"], "comment": "18 pages, 6 figures, 7 tables", "summary": "Wearable devices offer detailed sleep-tracking data. However, whether this\ninformation enhances our understanding of sleep or simply quantifies\nalready-known patterns remains unclear. This work explores the relationship\nbetween subjective sleep self-assessments and sensor data from an Oura ring\nover 4--8 weeks in-the-wild. 29 participants rated their sleep quality daily\ncompared to the previous night and completed a working memory task. Our\nfindings reveal that differences in REM sleep, nocturnal heart rate, N-Back\nscores, and bedtimes highly predict sleep self-assessment in significance and\neffect size. For N-Back performance, REM sleep duration, prior night's REM\nsleep, and sleep self-assessment are the strongest predictors. We demonstrate\nthat self-report sensitivity towards sleep markers differs among participants.\nWe identify three groups, highlighting that sleep trackers provide more\ninformation gain for some users than others. Additionally, we make all\nexperiment data publicly available."}
{"id": "2507.19537", "pdf": "https://arxiv.org/pdf/2507.19537.pdf", "abs": "https://arxiv.org/abs/2507.19537", "title": "Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri", "authors": ["Felix Kraus", "Nicolas Blumenröhr", "Danah Tonne", "Achim Streit"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for\nthe automated translation of SKOS thesauri. This work addresses a critical need\nin the Digital Humanities (DH), where language diversity can limit access,\nreuse, and semantic interoperability of knowledge resources. WOKIE combines\nexternal translation services with targeted refinement using Large Language\nModels (LLMs), balancing translation quality, scalability, and cost. Designed\nto run on everyday hardware and be easily extended, the application requires no\nprior expertise in machine translation or LLMs. We evaluate WOKIE across\nseveral DH thesauri in 15 languages with different parameters, translation\nservices and LLMs, systematically analysing translation quality, performance,\nand ontology matching improvements. Our results show that WOKIE is suitable to\nenhance the accessibility, reuse, and cross-lingual interoperability of\nthesauri by hurdle-free automated translation and improved ontology matching\nperformance, supporting more inclusive and multilingual research\ninfrastructures."}
{"id": "2507.19492", "pdf": "https://arxiv.org/pdf/2507.19492.pdf", "abs": "https://arxiv.org/abs/2507.19492", "title": "ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation", "authors": ["Jovana Kondic", "Pengyuan Li", "Dhiraj Joshi", "Zexue He", "Shafiq Abedin", "Jennifer Sun", "Ben Wiesel", "Eli Schwartz", "Ahmed Nassar", "Bo Wu", "Assaf Arbelle", "Aude Oliva", "Dan Gutfreund", "Leonid Karlinsky", "Rogerio Feris"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Chart-to-code reconstruction -- the task of recovering executable plotting\nscripts from chart images -- provides important insights into a model's ability\nto ground data visualizations in precise, machine-readable form. Yet many\nexisting multimodal benchmarks largely focus primarily on answering questions\nabout charts or summarizing them. To bridge this gap, we present ChartGen, a\nfully-automated pipeline for code-guided synthetic chart generation. Starting\nfrom seed chart images, ChartGen (i) prompts a vision-language model (VLM) to\nreconstruct each image into a python script, and (ii) iteratively augments that\nscript with a code-oriented large language model (LLM). Using ChartGen, we\ncreate 222.5K unique chart-image code pairs from 13K seed chart images, and\npresent an open-source synthetic chart dataset covering 27 chart types, 11\nplotting libraries, and multiple data modalities (image, code, text, CSV,\nDocTags). From this corpus, we curate a held-out chart-to-code evaluation\nsubset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B -\n26B parameters), highlighting substantial room for progress. We release the\npipeline, prompts, and the dataset to help accelerate efforts towards robust\nchart understanding and vision-conditioned code generation:\nhttps://github.com/SD122025/ChartGen/"}
{"id": "2507.19586", "pdf": "https://arxiv.org/pdf/2507.19586.pdf", "abs": "https://arxiv.org/abs/2507.19586", "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning", "authors": ["Shengyuan Wang", "Jie Feng", "Tianhui Liu", "Dan Pei", "Yong Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) possess extensive world knowledge, including\ngeospatial knowledge, which has been successfully applied to various geospatial\ntasks such as mobility prediction and social indicator prediction. However,\nLLMs often generate inaccurate geospatial knowledge, leading to geospatial\nhallucinations (incorrect or inconsistent representations of geospatial\ninformation) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic\nevaluation and mitigation of geospatial hallucinations remain largely\nunexplored. To address this gap, we propose a comprehensive evaluation\nframework for geospatial hallucinations, leveraging structured geospatial\nknowledge graphs for controlled assessment. Through extensive evaluation across\n20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.\nBuilding on these insights, we introduce a dynamic factuality aligning method\nbased on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on\nthe proposed benchmark. Extensive experimental results demonstrate the\neffectiveness of our benchmark and learning algorithm in enhancing the\ntrustworthiness of LLMs in geospatial knowledge and reasoning tasks."}
{"id": "2507.19493", "pdf": "https://arxiv.org/pdf/2507.19493.pdf", "abs": "https://arxiv.org/abs/2507.19493", "title": "From Bench to Bedside: A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice", "authors": ["Yaowei Bai", "Ruiheng Zhang", "Yu Lei", "Jingfeng Yao", "Shuguang Ju", "Chaoyang Wang", "Wei Yao", "Yiwan Guo", "Guilin Zhang", "Chao Wan", "Qian Yuan", "Xuhua Duan", "Xinggang Wang", "Tao Sun", "Yongchao Xu", "Chuansheng Zheng", "Huangxuan Zhao", "Bo Du"], "categories": ["cs.HC", "eess.IV"], "comment": null, "summary": "A global shortage of radiologists has been exacerbated by the significant\nvolume of chest X-ray workloads, particularly in primary care. Although\nmultimodal large language models show promise, existing evaluations\npredominantly rely on automated metrics or retrospective analyses, lacking\nrigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray\ninterpretation system based on DeepSeek Janus-Pro model, was developed and\nrigorously validated through a multicenter prospective trial (NCT06874647). Our\nsystem outperforms state-of-the-art X-ray report generation models in automated\nreport generation, surpassing even larger-scale models including ChatGPT 4o\n(200B parameters), while demonstrating robust detection of eight clinically\ncritical radiographic findings (area under the curve, AUC > 0.8). Retrospective\nevaluation confirms significantly higher report accuracy than Janus-Pro and\nChatGPT 4o. In prospective clinical deployment, AI assistance significantly\nimproved report quality scores (4.37 vs. 4.11, P < 0.001), reduced\ninterpretation time by 18.5% (P < 0.001), and was preferred by a majority of\nexperts (3 out of 5) in 52.7% of cases. Through lightweight architecture and\ndomain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and\nworkflow efficiency, particularly in resource-constrained settings. The model\narchitecture and implementation framework will be open-sourced to facilitate\nthe clinical translation of AI-assisted radiology solutions."}
{"id": "2507.19595", "pdf": "https://arxiv.org/pdf/2507.19595.pdf", "abs": "https://arxiv.org/abs/2507.19595", "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models."}
{"id": "2507.19494", "pdf": "https://arxiv.org/pdf/2507.19494.pdf", "abs": "https://arxiv.org/abs/2507.19494", "title": "Evaluating Personalized Beneficial Interventions in the Daily Lives of Older Adults Using a Camera", "authors": ["Longfei Chen", "Christopher Lochhead", "Robert B. Fisher", "Nusa Faric", "Jacques Fleuriot", "Subramanian Ramamoorthy"], "categories": ["cs.HC"], "comment": "AIiH 2025, International Conference on AI in Healthcare", "summary": "Beneficial daily activity interventions have been shown to improve both the\nphysical and mental health of older adults. However, there is a lack of robust\nobjective metrics and personalized strategies to measure their impact. In this\nstudy, two older adults aged over 65, living in Edinburgh, UK, selected their\npreferred daily interventions (mindful meals and art crafts), which are then\nassessed for effectiveness. The total monitoring period across both\nparticipants was 8 weeks. Their physical behaviours were continuously monitored\nusing a non-contact, privacy-preserving camera-based system. Postural and\nmobility statistics were extracted using computer vision algorithms and\ncompared across periods with and without the interventions. The results\ndemonstrate significant behavioural changes for both participants, highlighting\nthe effectiveness of both these activities and the monitoring system."}
{"id": "2507.19598", "pdf": "https://arxiv.org/pdf/2507.19598.pdf", "abs": "https://arxiv.org/abs/2507.19598", "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?", "authors": ["Muntasir Wahed", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Nirav Diwan", "Gang Wang", "Dilek Hakkani-Tür", "Ismini Lourentzou"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Winner Defender Team at Amazon Nova AI Challenge 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision."}
{"id": "2507.19495", "pdf": "https://arxiv.org/pdf/2507.19495.pdf", "abs": "https://arxiv.org/abs/2507.19495", "title": "Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action", "authors": ["Qing Dong", "Pengyuan Liu", "Dong Yu", "Chen Kang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative agents have made significant progress in simulating human\nbehavior, but existing frameworks often simplify emotional modeling and focus\nprimarily on specific tasks, limiting the authenticity of the simulation. Our\nwork proposes the Psychological-mechanism Agent (PSYA) framework, based on the\nCognitive Triangle (Feeling-Thought-Action), designed to more accurately\nsimulate human behavior. The PSYA consists of three core modules: the Feeling\nmodule (using a layer model of affect to simulate changes in short-term,\nmedium-term, and long-term emotions), the Thought module (based on the Triple\nNetwork Model to support goal-directed and spontaneous thinking), and the\nAction module (optimizing agent behavior through the integration of emotions,\nneeds and plans). To evaluate the framework's effectiveness, we conducted daily\nlife simulations and extended the evaluation metrics to self-influence,\none-influence, and group-influence, selection five classic psychological\nexperiments for simulation. The results show that the PSYA framework generates\nmore natural, consistent, diverse, and credible behaviors, successfully\nreplicating human experimental outcomes. Our work provides a richer and more\naccurate emotional and cognitive modeling approach for generative agents and\noffers an alternative to human participants in psychological experiments."}
{"id": "2507.19616", "pdf": "https://arxiv.org/pdf/2507.19616.pdf", "abs": "https://arxiv.org/abs/2507.19616", "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track", "authors": ["Xuchen Wei", "Yangxin Wu", "Yaoyin Zhang", "Henglyu Liu", "Kehai Chen", "Xuefeng Bai", "Min Zhang"], "categories": ["cs.CL"], "comment": "7 pages, 1 figure, submitted to IWSLT 2025", "summary": "This paper presents HITSZ's submission for the IWSLT 2025 Indic track,\nfocusing on speech-to-text translation (ST) for English-to-Indic and\nIndic-to-English language pairs. To enhance translation quality in this\nlow-resource scenario, we propose an end-to-end system integrating the\npre-trained Whisper automated speech recognition (ASR) model with Krutrim, an\nIndic-specialized large language model (LLM). Experimental results demonstrate\nthat our end-to-end system achieved average BLEU scores of $28.88$ for\nEnglish-to-Indic directions and $27.86$ for Indic-to-English directions.\nFurthermore, we investigated the Chain-of-Thought (CoT) method. While this\nmethod showed potential for significant translation quality improvements on\nsuccessfully parsed outputs (e.g. a $13.84$ BLEU increase for\nTamil-to-English), we observed challenges in ensuring the model consistently\nadheres to the required CoT output format."}
{"id": "2507.19496", "pdf": "https://arxiv.org/pdf/2507.19496.pdf", "abs": "https://arxiv.org/abs/2507.19496", "title": "Technological Requirements for Videoconferencing Judicial Hearings: Enhancing the Credibility and Reliability of Remote Testimonies", "authors": ["Jorge Alberto Araujo"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This paper analyzes the technological requirements necessary to enhance the\ncredibility and reliability of judicial hearings conducted via videoconference,\nfrom the internal perspective of the judiciary. Drawing on the practical\nexperience of a judge who conducts daily hearings, this study identifies\nlimitations in current platforms for verifying the authenticity of testimonies\nand proposes tailored functionalities for the judicial context. Recognizing\nthat remote hearings represent a convenience for the parties without replacing\nthe option of in-person attendance, the article suggests implementing features\nsuch as eye tracking, environment verification, and blocking of parallel\napplications, in addition to improvements in transmission quality. The study\nconcludes that developing specific modules for witnesses - focusing on security\nand monitoring - can significantly contribute to equalizing the credibility\nbetween remote and in-person hearings, thus expanding access to justice without\ncompromising procedural reliability."}
{"id": "2507.19634", "pdf": "https://arxiv.org/pdf/2507.19634.pdf", "abs": "https://arxiv.org/abs/2507.19634", "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks", "authors": ["Sara Papi", "Maike Züfle", "Marco Gaido", "Beatrice Savoldi", "Danni Liu", "Ioannis Douros", "Luisa Bentivogli", "Jan Niehues"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD"], "comment": "Work in progress", "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development."}
{"id": "2507.19497", "pdf": "https://arxiv.org/pdf/2507.19497.pdf", "abs": "https://arxiv.org/abs/2507.19497", "title": "Unlimited Editions: Documenting Human Style in AI Art Generation", "authors": ["Alex Leitch", "Celia Chen"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.IR"], "comment": "alt.CHI 2025", "summary": "As AI art generation becomes increasingly sophisticated, HCI research has\nfocused primarily on questions of detection, authenticity, and automation. This\npaper argues that such approaches fundamentally misunderstand how artistic\nvalue emerges from the concerns that drive human image production. Through\nexamination of historical precedents, we demonstrate that artistic style is not\nonly visual appearance but the resolution of creative struggle, as artists\nwrestle with influence and technical constraints to develop unique ways of\nseeing. Current AI systems flatten these human choices into reproducible\npatterns without preserving their provenance. We propose that HCI's role lies\nnot only in perfecting visual output, but in developing means to document the\norigins and evolution of artistic style as it appears within generated visual\ntraces. This reframing suggests new technical directions for HCI research in\ngenerative AI, focused on automatic documentation of stylistic lineage and\ncreative choice rather than simple reproduction of aesthetic effects."}
{"id": "2507.19666", "pdf": "https://arxiv.org/pdf/2507.19666.pdf", "abs": "https://arxiv.org/abs/2507.19666", "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams", "authors": ["Andrei Vlad Man", "Răzvan-Alexandru Smădu", "Cristian-George Craciun", "Dumitru-Clementin Cercel", "Florin Pop", "Mihaela-Claudia Cercel"], "categories": ["cs.CL"], "comment": "49 pages, 52 figures", "summary": "The intersection of AI and legal systems presents a growing need for tools\nthat support legal education, particularly in under-resourced languages such as\nRomanian. In this work, we aim to evaluate the capabilities of Large Language\nModels (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning\nabout Romanian driving law through textual and visual question-answering tasks.\nTo facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising\nRomanian driving test questions, text-based and image-based, alongside\nannotated legal references and human explanations. We implement and assess\nretrieval-augmented generation (RAG) pipelines, dense retrievers, and\nreasoning-optimized models across tasks including Information Retrieval (IR),\nQuestion Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate\nthat domain-specific fine-tuning significantly enhances retrieval performance.\nAt the same time, chain-of-thought prompting and specialized reasoning models\nimprove QA accuracy, surpassing the minimum grades required to pass driving\nexams. However, visual reasoning remains challenging, highlighting the\npotential and the limitations of applying LLMs and VLMs to legal education."}
{"id": "2507.19498", "pdf": "https://arxiv.org/pdf/2507.19498.pdf", "abs": "https://arxiv.org/abs/2507.19498", "title": "ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings", "authors": ["Yue Wu", "Xiaolan Chen", "Weiyi Zhang", "Shunming Liu", "Wing Man Rita Sum", "Xinyuan Wu", "Xianwen Shang", "Chea-su Kee", "Mingguang He", "Danli Shi"], "categories": ["cs.HC", "cs.AI"], "comment": "35 pages, 4 figures, 1 table", "summary": "Large language models (LLMs) show promise for tailored healthcare\ncommunication but face challenges in interpretability and multi-task\nintegration particularly for domain-specific needs like myopia, and their\nreal-world effectiveness as patient education tools has yet to be demonstrated.\nHere, we introduce ChatMyopia, an LLM-based AI agent designed to address text\nand image-based inquiries related to myopia. To achieve this, ChatMyopia\nintegrates an image classification tool and a retrieval-augmented knowledge\nbase built from literature, expert consensus, and clinical guidelines. Myopic\nmaculopathy grading task, single question examination and human evaluations\nvalidated its ability to deliver personalized, accurate, and safe responses to\nmyopia-related inquiries with high scalability and interpretability. In a\nrandomized controlled trial (n=70, NCT06607822), ChatMyopia significantly\nimproved patient satisfaction compared to traditional leaflets, enhancing\npatient education in accuracy, empathy, disease awareness, and patient-eyecare\npractitioner communication. These findings highlight ChatMyopia's potential as\na valuable supplement to enhance patient education and improve satisfaction\nwith medical services in primary eye care settings."}
{"id": "2507.19699", "pdf": "https://arxiv.org/pdf/2507.19699.pdf", "abs": "https://arxiv.org/abs/2507.19699", "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks", "authors": ["Maitha Alshehhi", "Ahmed Sharshar", "Mohsen Guizani"], "categories": ["cs.CL"], "comment": "Published in the 3rd International Workshop on Generalizing from\n  Limited Resources in the Open World. Workshop at International Joint\n  Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Although LLMs have attained significant success in high-resource languages,\ntheir capacity in low-resource linguistic environments like Kannada and Arabic\nis not yet fully understood. This work benchmarking the performance of\nmultilingual and monolingual Large Language Models (LLMs) across Arabic,\nEnglish, and Indic languages, with particular emphasis on the effects of model\ncompression strategies such as pruning and quantization. Findings shows\nsignificant performance differences driven by linguistic diversity and resource\navailability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.\nWe find that multilingual versions of the model outperform their\nlanguage-specific counterparts across the board, indicating substantial\ncross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in\nmaintaining model accuracy while promoting efficiency, but aggressive pruning\nsignificantly compromises performance, especially in bigger models. Our\nfindings pinpoint key strategies to construct scalable and fair multilingual\nNLP solutions and underscore the need for interventions to address\nhallucination and generalization errors in the low-resource setting."}
{"id": "2507.19500", "pdf": "https://arxiv.org/pdf/2507.19500.pdf", "abs": "https://arxiv.org/abs/2507.19500", "title": "Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems", "authors": ["Omkar Suresh Hatti"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The proliferation of artificial intelligence provides an opportunity to\ncreate psychological spaciousness in society. Spaciousness is defined as the\nability to hold diverse interpersonal interactions and forms the basis for\nvulnerability that leads to authenticity that leads to prosocial behaviors and\nthus to societal harmony. This paper demonstrates an attempt to quantify, the\nhuman conditioning to subconsciously modify authentic self-expression to fit\nthe norms of the dominant culture. Gaze is explored across various marginalized\nand intersectional groups, using concepts from postmodern philosophy and\npsychology. The effects of gaze are studied through analyzing a few redacted\nReddit posts, only to be discussed in discourse and not endorsement. A\nmathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite\nMetric is presented to model the analysis of two sets of conversational spaces\nin relation to one another. The outcome includes an equation to train Large\nLanguage Models (LLMs) - the working mechanism of AI products such as Chat-GPT;\nand an argument for affirming and inclusive HCI, based on the equation, is\npresented. The argument is supported by a few principles of Neuro-plasticity,\nThe brain's lifelong capacity to rewire."}
{"id": "2507.19710", "pdf": "https://arxiv.org/pdf/2507.19710.pdf", "abs": "https://arxiv.org/abs/2507.19710", "title": "Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs", "authors": ["Ronak Upasham", "Tathagata Dey", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "In Table-to-Text (T2T) generation, existing approaches predominantly focus on\nproviding objective descriptions of tabular data. However, generating text that\nincorporates subjectivity, where subjectivity refers to interpretations beyond\nraw numerical data, remains underexplored. To address this, we introduce a\nnovel pipeline that leverages intermediate representations to generate both\nobjective and subjective text from tables. Our three-stage pipeline consists\nof: 1) extraction of Resource Description Framework (RDF) triples, 2)\naggregation of text into coherent narratives, and 3) infusion of subjectivity\nto enrich the generated text. By incorporating RDFs, our approach enhances\nfactual accuracy while maintaining interpretability. Unlike large language\nmodels (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs\nsmaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5\nand outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our\napproach through quantitative and qualitative analyses, demonstrating its\neffectiveness in balancing factual accuracy with subjective interpretation. To\nthe best of our knowledge, this is the first work to propose a structured\npipeline for T2T generation that integrates intermediate representations to\nenhance both factual correctness and subjectivity."}
{"id": "2507.19690", "pdf": "https://arxiv.org/pdf/2507.19690.pdf", "abs": "https://arxiv.org/abs/2507.19690", "title": "Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems", "authors": ["Jeffrey Heer", "Dominik Moritz", "Ron Pechuk"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Though powerful tools for analysis and communication, interactive\nvisualizations often fail to support real-time interaction with large datasets\nwith millions or more records. To highlight and filter data, users indicate\nvalues or intervals of interest. Such selections may span multiple components,\ncombine in complex ways, and require optimizations to ensure low-latency\nupdates. We describe Mosaic Selections, a model for representing, managing, and\noptimizing user selections, in which one or more filter predicates are added to\nqueries that request data for visualizations and input widgets. By analyzing\nboth queries and selection predicates, Mosaic Selections enable automatic\noptimizations, including pre-aggregating data to rapidly compute selection\nupdates. We contribute a formal description of our selection model and\noptimization methods, and their implementation in the open-source Mosaic\narchitecture. Benchmark results demonstrate orders-of-magnitude latency\nimprovements for selection-based optimizations over unoptimized queries and\nexisting optimizers for the Vega language. The Mosaic Selection model provides\ninfrastructure for flexible, interoperable filtering across multiple\nvisualizations, alongside automatic optimizations to scale to millions and even\nbillions of records."}
{"id": "2507.19741", "pdf": "https://arxiv.org/pdf/2507.19741.pdf", "abs": "https://arxiv.org/abs/2507.19741", "title": "Basic Reading Distillation", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation."}
{"id": "2507.19736", "pdf": "https://arxiv.org/pdf/2507.19736.pdf", "abs": "https://arxiv.org/abs/2507.19736", "title": "LowKeyEMG: Electromyographic typing with a reduced keyset", "authors": ["Johannes Y. Lee", "Derek Xiao", "Shreyas Kaasyap", "Nima R. Hadidi", "John L. Zhou", "Jacob Cunningham", "Rakshith R. Gore", "Deniz O. Eren", "Jonathan C. Kao"], "categories": ["cs.HC", "eess.SP"], "comment": "11+3 pages, 5 main figures, 2 supplementary tables, 4 supplementary\n  figures", "summary": "We introduce LowKeyEMG, a real-time human-computer interface that enables\nefficient text entry using only 7 gesture classes decoded from surface\nelectromyography (sEMG). Prior work has attempted full-alphabet decoding from\nsEMG, but decoding large character sets remains unreliable, especially for\nindividuals with motor impairments. Instead, LowKeyEMG reduces the English\nalphabet to 4 gesture keys, with 3 more for space and system interaction, to\nreliably translate simple one-handed gestures into text, leveraging the\nrecurrent transformer-based language model RWKV for efficient computation. In\nreal-time experiments, participants achieved average one-handed keyboardless\ntyping speeds of 23.3 words per minute with LowKeyEMG, and improved gesture\nefficiency by 17% (relative to typed phrase length). When typing with only 7\nkeys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this\nlow-key typing paradigm can maintain practical communication rates. Our results\nhave implications for assistive technologies and any interface where input\nbandwidth is constrained."}
{"id": "2507.19748", "pdf": "https://arxiv.org/pdf/2507.19748.pdf", "abs": "https://arxiv.org/abs/2507.19748", "title": "JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models", "authors": ["Yifan Hao", "Fangning Chao", "Yaqian Hao", "Zhaojun Cui", "Huan Bai", "Haiyu Zhang", "Yankai Liu", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Mathematical reasoning is a cornerstone of artificial general intelligence\nand a primary benchmark for evaluating the capabilities of Large Language\nModels (LLMs). While state-of-the-art models show promise, they often falter\nwhen faced with complex problems that demand deep conceptual understanding and\nintricate, multi-step deliberation. To address this challenge, we introduce\nJT-Math-8B, a series of open-source models comprising base, instruct, and\nthinking versions, built upon a systematic, multi-stage optimization framework.\nOur pre-training corpus is a high-quality, 210B-token dataset curated through a\ndedicated data pipeline that uses model-based validation to ensure quality and\ndiversity. The Instruct Model is optimized for direct, concise answers through\nSupervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)\nmethod. The Thinking Model is trained for complex problem-solving using a Long\nChain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage\nRL curriculum that progressively increases task difficulty and context length\nup to 32K tokens. JT-Math-8B achieves state-of-the-art results among\nopen-source models of similar size, surpassing prominent models like OpenAI's\nO1-mini and GPT-4o , and demonstrating superior performance on\ncompetition-level mathematics."}
{"id": "2507.19782", "pdf": "https://arxiv.org/pdf/2507.19782.pdf", "abs": "https://arxiv.org/abs/2507.19782", "title": "KinemaFX: A Kinematic-Driven Interactive System for Particle Effect Exploration and Customization", "authors": ["Yifei Zhang", "Lin-Ping Yuan", "Yuheng Zhao", "Jielin Feng", "Siming Chen"], "categories": ["cs.HC"], "comment": "Meta Review Overall Rating 3.5 Weakly Accept Contribution to HCI This\n  paper presents KinemaFX, an LLM-powered interactive system leveraging\n  semantic and kinematic inputs to help non-experts explore, customize, and\n  compose particle effects", "summary": "Particle effects are widely used in games and animation to simulate natural\nphenomena or stylized visual effects. However, creating effect artworks is\nchallenging for non-expert users due to their lack of specialized skills,\nparticularly in finding particle effects with kinematic behaviors that match\ntheir intent. To address these issues, we present KinemaFX, a kinematic-driven\ninteractive system, to assist non-expert users in constructing customized\nparticle effect artworks. We propose a conceptual model of particle effects\nthat captures both semantic features and kinematic behaviors. Based on the\nmodel, KinemaFX adopts a workflow powered by Large Language Models (LLMs) that\nsupports intent expression through combined semantic and kinematic inputs,\nwhile enabling implicit preference-guided exploration and subsequent creation\nof customized particle effect artworks based on exploration results.\nAdditionally, we developed a kinematic-driven method to facilitate efficient\ninteractive particle effect search within KinemaFX via structured\nrepresentation and measurement of particle effects. To evaluate KinemaFX, we\nillustrate usage scenarios and conduct a user study employing an ablation\napproach. Evaluation results demonstrate that KinemaFX effectively supports\nusers in efficiently and customarily creating particle effect artworks."}
{"id": "2507.19756", "pdf": "https://arxiv.org/pdf/2507.19756.pdf", "abs": "https://arxiv.org/abs/2507.19756", "title": "Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs", "authors": ["Rebecca M. M. Hicke", "Brian Haggard", "Mia Ferrante", "Rayhan Khanna", "David Mimno"], "categories": ["cs.CL"], "comment": null, "summary": "In addition to its more widely studied political activities, the American\nEvangelical movement has a well-developed but less externally visible cultural\nand literary side. Christian Fiction, however, has been little studied, and\nwhat scholarly attention there is has focused on the explosively popular Left\nBehind series. In this work, we use computational tools to provide both a broad\ntopical overview of Christian Fiction as a genre and a more directed\nexploration of how its authors depict divine acts. Working with human\nannotators we first developed definitions and a codebook for \"acts of God.\" We\nthen adapted those instructions designed for human annotators for use by a\nrecent, lightweight LM with the assistance of a much larger model. The\nlaptop-scale LM is capable of matching human annotations, even when the task is\nsubtle and challenging. Using these annotations, we show that significant and\nmeaningful differences exist between the Left Behind books and Christian\nFiction more broadly and between books by male and female authors."}
{"id": "2507.19898", "pdf": "https://arxiv.org/pdf/2507.19898.pdf", "abs": "https://arxiv.org/abs/2507.19898", "title": "TS-Insight: Visualizing Thompson Sampling for Verification and XAI", "authors": ["Parsa Vares", "Éloi Durant", "Jun Pang", "Nicolas Médoc", "Mohammad Ghoniem"], "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "I.2.6; H.5.2"], "comment": "Accepted as a poster at IEEE VIS 2025 (\"TS-Insight: Visual\n  Fingerprinting of Multi-Armed Bandits\"). Open-source tool available at\n  https://github.com/parsavares/ts-insight", "summary": "Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit\nalgorithms used to balance exploration and exploitation strategies in active\nlearning. Yet, their probabilistic nature often turns them into a ``black\nbox'', hindering debugging and trust. We introduce TS-Insight, a visual\nanalytics tool explicitly designed to shed light on the internal decision\nmechanisms of Thompson Sampling-based algorithms, for model developers. It\ncomprises multiple plots, tracing for each arm the evolving posteriors,\nevidence counts, and sampling outcomes, enabling the verification, diagnosis,\nand explainability of exploration/exploitation dynamics. This tool aims at\nfostering trust and facilitating effective debugging and deployment in complex\nbinary decision-making scenarios especially in sensitive domains requiring\ninterpretable decision-making."}
{"id": "2507.19766", "pdf": "https://arxiv.org/pdf/2507.19766.pdf", "abs": "https://arxiv.org/abs/2507.19766", "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity."}
{"id": "2507.19988", "pdf": "https://arxiv.org/pdf/2507.19988.pdf", "abs": "https://arxiv.org/abs/2507.19988", "title": "Visual Analytics Using Tensor Unified Linear Comparative Analysis", "authors": ["Naoki Okami", "Kazuki Miyake", "Naohisa Sakamoto", "Jorji Nonaka", "Takanori Fujiwara"], "categories": ["cs.HC", "cs.GR", "cs.LG", "I.3.8; H.5.2"], "comment": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  and IEEE VIS 2025", "summary": "Comparing tensors and identifying their (dis)similar structures is\nfundamental in understanding the underlying phenomena for complex data. Tensor\ndecomposition methods help analysts extract tensors' essential characteristics\nand aid in visual analytics for tensors. In contrast to dimensionality\nreduction (DR) methods designed only for analyzing a matrix (i.e., second-order\ntensor), existing tensor decomposition methods do not support flexible\ncomparative analysis. To address this analysis limitation, we introduce a new\ntensor decomposition method, named tensor unified linear comparative analysis\n(TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA\nintegrates discriminant analysis and contrastive learning schemes for tensor\ndecomposition, enabling flexible comparison of tensors. We also introduce an\neffective method to visualize a core tensor extracted from TULCA into a set of\n2D visualizations. We integrate TULCA's functionalities into a visual analytics\ninterface to support analysts in interpreting and refining the TULCA results.\nWe demonstrate the efficacy of TULCA and the visual analytics interface with\ncomputational evaluations and two case studies, including an analysis of log\ndata collected from a supercomputer."}
{"id": "2507.19786", "pdf": "https://arxiv.org/pdf/2507.19786.pdf", "abs": "https://arxiv.org/abs/2507.19786", "title": "Flora: Effortless Context Construction to Arbitrary Length and Scale", "authors": ["Tianxiang Chen", "Zhentao Tan", "Xiaofan Bo", "Yue Wu", "Tao Gong", "Qi Chu", "Jieping Ye", "Nenghai Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Effectively handling long contexts is challenging for Large Language Models\n(LLMs) due to the rarity of long texts, high computational demands, and\nsubstantial forgetting of short-context abilities. Recent approaches have\nattempted to construct long contexts for instruction tuning, but these methods\noften require LLMs or human interventions, which are both costly and limited in\nlength and diversity. Also, the drop in short-context performances of present\nlong-context LLMs remains significant. In this paper, we introduce Flora, an\neffortless (human/LLM-free) long-context construction strategy. Flora can\nmarkedly enhance the long-context performance of LLMs by arbitrarily assembling\nshort instructions based on categories and instructing LLMs to generate\nresponses based on long-context meta-instructions. This enables Flora to\nproduce contexts of arbitrary length and scale with rich diversity, while only\nslightly compromising short-context performance. Experiments on\nLlama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three\nlong-context benchmarks while maintaining strong performances in short-context\ntasks. Our data-construction code is available at\n\\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}."}
{"id": "2507.20006", "pdf": "https://arxiv.org/pdf/2507.20006.pdf", "abs": "https://arxiv.org/abs/2507.20006", "title": "Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques", "authors": ["Jun-Hsiang Yao", "Jielin Feng", "Xinfang Tian", "Kai Xu", "Gulshat Amirkhanova", "Siming Chen"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures", "summary": "Virtual Reality (VR) broadcasting has emerged as a promising medium for\nproviding immersive viewing experiences of major sports events such as tennis.\nHowever, current VR broadcast systems often lack an effective camera language\nand do not adequately incorporate dynamic, in-game visualizations, limiting\nviewer engagement and narrative clarity. To address these limitations, we\nanalyze 400 out-of-play segments from eight major tennis broadcasts to develop\na tennis-specific design framework that effectively combines cinematic camera\nmovements with embedded visualizations. We further refine our framework by\nexamining 25 cinematic VR animations, comparing their camera techniques with\ntraditional tennis broadcasts to identify key differences and inform\nadaptations for VR. Based on data extracted from the broadcast videos, we\nreconstruct a simulated game that captures the players' and ball's motion and\ntrajectories. Leveraging this design framework and processing pipeline, we\ndevelope Beyond the Broadcast, a VR tennis viewing system that integrates\nembedded visualizations with adaptive camera motions to construct a\ncomprehensive and engaging narrative. Our system dynamically overlays tactical\ninformation and key match events onto the simulated environment, enhancing\nviewer comprehension and narrative engagement while ensuring perceptual\nimmersion and viewing comfort. A user study involving tennis viewers\ndemonstrate that our approach outperforms traditional VR broadcasting methods\nin delivering an immersive, informative viewing experience."}
{"id": "2507.19823", "pdf": "https://arxiv.org/pdf/2507.19823.pdf", "abs": "https://arxiv.org/abs/2507.19823", "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs", "authors": ["Dongquan Yang", "Yifan Yang", "Xiaotian Yu", "Xianbiao Qi", "Rong Xiao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."}
{"id": "2507.20137", "pdf": "https://arxiv.org/pdf/2507.20137.pdf", "abs": "https://arxiv.org/abs/2507.20137", "title": "Dynamite: Real-Time Debriefing Slide Authoring through AI-Enhanced Multimodal Interaction", "authors": ["Panayu Keelawat", "David Barron", "Kaushik Narasimhan", "Daniel Manesh", "Xiaohang Tang", "Xi Chen", "Sang Won Lee", "Yan Chen"], "categories": ["cs.HC"], "comment": "Accepted to VL/HCC 2025", "summary": "Facilitating class-wide debriefings after small-group discussions is a common\nstrategy in ethics education. Instructor interviews revealed that effective\ndebriefings should highlight frequently discussed themes and surface\nunderrepresented viewpoints, making accurate representations of insight\noccurrence essential. Yet authoring presentations in real time is cognitively\noverwhelming due to the volume of data and tight time constraints. We present\nDynamite, an AI-assisted system that enables semantic updates to\ninstructor-authored slides during live classroom discussions. These updates are\npowered by semantic data binding, which links slide content to evolving\ndiscussion data, and semantic suggestions, which offer revision options aligned\nwith pedagogical goals. In a within-subject in-lab study with 12 participants,\nDynamite outperformed a text-based AI baseline in content accuracy and quality.\nParticipants used voice and sketch input to quickly organize semantic blocks,\nthen applied suggestions to accelerate refinement as data stabilized."}
{"id": "2507.19867", "pdf": "https://arxiv.org/pdf/2507.19867.pdf", "abs": "https://arxiv.org/abs/2507.19867", "title": "DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments", "authors": ["Anshul Chavda", "M Jagadeesh", "Chintalapalli Raja Kullayappa", "B Jayaprakash", "Medchalimi Sruthi", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "In-car conversational AI is becoming increasingly critical as autonomous\nvehicles and smart assistants gain widespread adoption. Yet, existing datasets\nfail to capture the spontaneous disfluencies such as hesitations, false starts,\nrepetitions, and self-corrections that characterize real driver-AI dialogs. To\naddress this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn\ndialogs across seven automotive domains, generated using a two-stage,\nprompt-driven pipeline that dynamically integrates disfluencies during\nsynthesis. We show that DiscoDrive is effective both as a training resource,\nenabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on\nthe MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4\nimprovements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1\nimprovements of 1.35 to 3.48), and as a data augmentation resource in\nlow-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,\nMETEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10\npercent of KVRET. Human evaluations further confirm that dialogs sampled from\nDiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness\n(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more\ncontext-appropriate than leading post-hoc methods (such as LARD), without\ncompromising clarity. DiscoDrive fills a critical gap in existing resources and\nserves as a versatile corpus for both training and augmenting conversational\nAI, enabling robust handling of real-world, disfluent in-car interactions."}
{"id": "2507.20261", "pdf": "https://arxiv.org/pdf/2507.20261.pdf", "abs": "https://arxiv.org/abs/2507.20261", "title": "Occupational Safety within Non-Routine Manufacturing Processes: Evaluating the Validity of Task-Based Ergonomic Assessments", "authors": ["Charu Tripathi", "Manish Arora", "Amaresh Chakrabarti"], "categories": ["cs.HC"], "comment": null, "summary": "Direct measurement ergonomic assessment is reshaping occupational safety by\nfacilitating highly reliable risk estimation. Industry 5.0, advocating\nhuman-centricity, has catalysed increasing adoption of direct measurement tools\nin manufacturing industries. However, due to technical and feasibility\nconstraints in their practical implementations, especially within non routine\nmanufacturing processes, task based approach to ergonomic assessment is\nutilized. Despite enabling operationalization of robust ergonomic assessment\ntechnologies within complicated industrial processes, task based approach\nraises several validity concerns. Hence, to ascertain functional utility of the\nresultant safety interventions, this study evaluates the construct validity of\ntask based ergonomic assessment within non routine work utilizing Multitrait\nmultimethod (MTMM) matrix followed by video-based content analysis. Ergonomic\nexposure traits were collected for 46 participants through direct measurement\nand self reported techniques utilizing inertial motion capture and Borg's RPE\nrating scale respectively. Findings include unsubstantiated convergent validity\n(low same trait correlations from 0.149 to 0.243) and weak evidence of\ndiscriminant validity with statistical significance (p value less than 0.001).\nThe study also identifies three primary factors undermining construct validity\nthrough video based content analysis. Findings also elucidate misinterpretation\nof ergonomic risk and action levels. Therefore, practical implications entail\nunderestimation of actual ergonomic risks when estimated through task based\nassessment. This highlights the need for enhancement in ergonomic assessment\ntechnologies focused on cumulative load analysis compatible within diverse\nindustrial processes."}
{"id": "2507.19869", "pdf": "https://arxiv.org/pdf/2507.19869.pdf", "abs": "https://arxiv.org/abs/2507.19869", "title": "The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment", "authors": ["Danil Fokin", "Monika Płużyczka", "Grigory Golovin"], "categories": ["cs.CL"], "comment": null, "summary": "We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing\nthe receptive vocabulary size of both native and non-native Polish speakers.\nBased on Item Response Theory and Computerized Adaptive Testing, PVST\ndynamically adjusts to each test-taker's proficiency level, ensuring high\naccuracy while keeping the test duration short. To validate the test, a pilot\nstudy was conducted with 1.475 participants. Native Polish speakers\ndemonstrated significantly larger vocabularies compared to non-native speakers.\nFor native speakers, vocabulary size showed a strong positive correlation with\nage. The PVST is available online at myvocab.info/pl."}
{"id": "2507.20300", "pdf": "https://arxiv.org/pdf/2507.20300.pdf", "abs": "https://arxiv.org/abs/2507.20300", "title": "Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance and Experience in Minecraft", "authors": ["Xin Sun", "Lei Wang", "Yue Li", "Jie Li", "Massimo Poesio", "Julian Frommel", "Koen Hinriks", "Jiahuan Pei"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "With large language models (LLMs) on the rise, in-game interactions are\nshifting from rigid commands to natural conversations. However, the impacts of\nLLMs on player performance and game experience remain underexplored. This work\nexplores LLM's role as a co-builder during gameplay, examining its impact on\ntask performance, usability, and player experience. Using Minecraft as a\nsandbox, we present an LLM-assisted interface that engages players through\nnatural language, aiming to facilitate creativity and simplify complex gaming\ncommands. We conducted a mixed-methods study with 30 participants, comparing\nLLM-assisted and command-based interfaces across simple and complex game tasks.\nQuantitative and qualitative analyses reveal that the LLM-assisted interface\nsignificantly improves player performance, engagement, and overall game\nexperience. Additionally, task complexity has a notable effect on player\nperformance and experience across both interfaces. Our findings highlight the\npotential of LLM-assisted interfaces to revolutionize virtual experiences,\nemphasizing the importance of balancing intuitiveness with predictability,\ntransparency, and user agency in AI-driven, multimodal gaming environments."}
{"id": "2507.19885", "pdf": "https://arxiv.org/pdf/2507.19885.pdf", "abs": "https://arxiv.org/abs/2507.19885", "title": "Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam", "authors": ["Cesar Augusto Madid Truyts", "Amanda Gomes Rabelo", "Gabriel Mesquita de Souza", "Daniel Scaldaferri Lages", "Adriano Jose Pereira", "Uri Adrian Prync Flato", "Eduardo Pontes dos Reis", "Joaquim Edson Vieira", "Paulo Sergio Panse Silveira", "Edson Amaro Junior"], "categories": ["cs.CL"], "comment": null, "summary": "Artificial intelligence (AI) has shown the potential to revolutionize\nhealthcare by improving diagnostic accuracy, optimizing workflows, and\npersonalizing treatment plans. Large Language Models (LLMs) and Multimodal\nLarge Language Models (MLLMs) have achieved notable advancements in natural\nlanguage processing and medical applications. However, the evaluation of these\nmodels has focused predominantly on the English language, leading to potential\nbiases in their performance across different languages.\n  This study investigates the capability of six LLMs (GPT-4.0 Turbo,\nLLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and\nCommand R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,\nand Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese\nfrom the medical residency entrance exam of the Hospital das Cl\\'inicas da\nFaculdade de Medicina da Universidade de S\\~ao Paulo (HCFMUSP) - the largest\nhealth complex in South America. The performance of the models was benchmarked\nagainst human candidates, analyzing accuracy, processing time, and coherence of\nthe generated explanations.\n  The results show that while some models, particularly Claude-3.5-Sonnet and\nClaude-3-Opus, achieved accuracy levels comparable to human candidates,\nperformance gaps persist, particularly in multimodal questions requiring image\ninterpretation. Furthermore, the study highlights language disparities,\nemphasizing the need for further fine-tuning and data set augmentation for\nnon-English medical AI applications.\n  Our findings reinforce the importance of evaluating generative AI in various\nlinguistic and clinical settings to ensure a fair and reliable deployment in\nhealthcare. Future research should explore improved training methodologies,\nimproved multimodal reasoning, and real-world clinical integration of AI-driven\nmedical assistance."}
{"id": "2507.20355", "pdf": "https://arxiv.org/pdf/2507.20355.pdf", "abs": "https://arxiv.org/abs/2507.20355", "title": "CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration", "authors": ["Zheng Wei", "Hongtao Wu", "lvmin Zhang", "Xian Xu", "Yefeng Zheng", "Pan Hui", "Maneesh Agrawala", "Huamin Qu", "Anyi Rao"], "categories": ["cs.HC"], "comment": null, "summary": "Effective communication between directors and cinematographers is fundamental\nin film production, yet traditional approaches relying on visual references and\nhand-drawn storyboards often lack the efficiency and precision necessary during\npre-production. We present CineVision, an AI-driven platform that integrates\nscriptwriting with real-time visual pre-visualization to bridge this\ncommunication gap. By offering dynamic lighting control, style emulation based\non renowned filmmakers, and customizable character design, CineVision enables\ndirectors to convey their creative vision with heightened clarity and rapidly\niterate on scene composition. In a 24-participant lab study, CineVision yielded\nshorter task times and higher usability ratings than two baseline methods,\nsuggesting a potential to ease early-stage communication and accelerate\nstoryboard drafts under controlled conditions. These findings underscore\nCineVision's potential to streamline pre-production processes and foster deeper\ncreative synergy among filmmaking teams, particularly for new collaborators.Our\ncode and demo are available at https://github.com/TonyHongtaoWu/CineVision."}
{"id": "2507.19899", "pdf": "https://arxiv.org/pdf/2507.19899.pdf", "abs": "https://arxiv.org/abs/2507.19899", "title": "A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs", "authors": ["Prajval Bolegave", "Pushpak Bhattacharya"], "categories": ["cs.CL"], "comment": null, "summary": "Early detection of depression from online social media posts holds promise\nfor providing timely mental health interventions. In this work, we present a\nhigh-quality, expert-annotated dataset of 1,017 social media posts labeled with\ndepressive spans and mapped to 12 depression symptom categories. Unlike prior\ndatasets that primarily offer coarse post-level labels\n\\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of\nboth model predictions and generated explanations.\n  We develop an evaluation framework that leverages this clinically grounded\ndataset to assess the faithfulness and quality of natural language explanations\ngenerated by large language models (LLMs). Through carefully designed prompting\nstrategies, including zero-shot and few-shot approaches with domain-adapted\nexamples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,\nGemini 2.5 Pro, and Claude 3.7 Sonnet.\n  Our comprehensive empirical analysis reveals significant differences in how\nthese models perform on clinical explanation tasks, with zero-shot and few-shot\nprompting. Our findings underscore the value of human expertise in guiding LLM\nbehavior and offer a step toward safer, more transparent AI systems for\npsychological well-being."}
{"id": "2507.20437", "pdf": "https://arxiv.org/pdf/2507.20437.pdf", "abs": "https://arxiv.org/abs/2507.20437", "title": "EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband", "authors": ["Kian Mahmoodi", "Yudong Xie", "Tan Gemicioglu", "Chi-Jung Lee", "Jiwan Kim", "Cheng Zhang"], "categories": ["cs.HC"], "comment": "8 pages, 3 figures. Proceedings of the 2025 ACM International\n  Symposium on Wearable Computers (ISWC '25)", "summary": "Grip force is commonly used as an overall health indicator in older adults\nand is valuable for tracking progress in physical training and rehabilitation.\nExisting methods for wearable grip force measurement are cumbersome and\nuser-dependent, making them insufficient for practical, continuous grip force\nmeasurement. We introduce EchoForce, a novel wristband using acoustic sensing\nfor low-cost, non-contact measurement of grip force. EchoForce captures\nacoustic signals reflected from subtle skin deformations by flexor muscles on\nthe forearm. In a user study with 11 participants, EchoForce achieved a\nfine-tuned user-dependent mean error rate of 9.08% and a user-independent mean\nerror rate of 12.3% using a foundation model. Our system remained accurate\nbetween sessions, hand orientations, and users, overcoming a significant\nlimitation of past force sensing systems. EchoForce makes continuous grip force\nmeasurement practical, providing an effective tool for health monitoring and\nnovel interaction techniques."}
{"id": "2507.19906", "pdf": "https://arxiv.org/pdf/2507.19906.pdf", "abs": "https://arxiv.org/abs/2507.19906", "title": "CaliDrop: KV Cache Compression with Calibration", "authors": ["Yi Su", "Quantong Qiu", "Yuechi Zhou", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."}
{"id": "2507.20655", "pdf": "https://arxiv.org/pdf/2507.20655.pdf", "abs": "https://arxiv.org/abs/2507.20655", "title": "CoGrader: Transforming Instructors' Assessment of Project Reports through Collaborative LLM Integration", "authors": ["Zixin Chen", "Jiachen Wang", "Yumeng Li", "Haobo Li", "Chuhan Shi", "Rong Zhang", "Huamin Qu"], "categories": ["cs.HC"], "comment": null, "summary": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems."}
{"id": "2507.19962", "pdf": "https://arxiv.org/pdf/2507.19962.pdf", "abs": "https://arxiv.org/abs/2507.19962", "title": "KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models", "authors": ["Seorin Kim", "Dongyoung Lee", "Jaejin Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often exhibit societal biases in their outputs,\nprompting ethical concerns regarding fairness and harm. In this work, we\npropose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing\nframework that implicitly aligns attention distributions between stereotypical\nand anti-stereotypical sentence pairs without directly modifying model weights.\nKLAAD introduces a composite training objective combining Cross-Entropy, KL\ndivergence, and Triplet losses, guiding the model to consistently attend across\nbiased and unbiased contexts while preserving fluency and coherence.\nExperimental evaluation of KLAAD demonstrates improved bias mitigation on both\nthe BBQ and BOLD benchmarks, with minimal impact on language modeling quality.\nThe results indicate that attention-level alignment offers a principled\nsolution for mitigating bias in generative language models."}
{"id": "2507.20656", "pdf": "https://arxiv.org/pdf/2507.20656.pdf", "abs": "https://arxiv.org/abs/2507.20656", "title": "EarXplore: An Open Research Database on Earable Interaction", "authors": ["Jonas Hummel", "Tobias Röddiger", "Valeria Zitz", "Philipp Lepold", "Michael Küttner", "Marius Prill", "Christopher Clarke", "Hans Gellersen", "Michael Beigl"], "categories": ["cs.HC"], "comment": null, "summary": "Interaction with earables - earphones equipped with additional sensors - has\nbeen identified as one of four major areas of earable research. Worn naturally\nand positioned near key physiological signals, earables support a wide range of\ninteraction modalities and have demonstrated the ability to detect multiple\ninputs simultaneously. Yet this diversity has resulted in a fragmented body of\nresearch, making it increasingly difficult to track developments and identify\nrelevant studies. To address this, we introduce EarXplore, a curated,\ninteractive online database on earable interaction research. Designed through a\nquestion-centered process that guided both the development of 34 criteria\napplied to annotate 118 studies and the structure of the platform, EarXplore\ncomprises four distinct yet integrated views: a Tabular View for structured\nexploration, a Graphical View for visual overviews, a Similarity View for\nidentifying conceptual links, and a Timeline View for analyzing trends and\nscholarly lineage. We demonstrate how the platform supports tailored\nexploration, targeted filtering, and interactive information retrieval,\nallowing researchers to query the literature and synthesize information in the\nformat of their choice. We furthermore leverage the contents and capabilities\nof the platform to discuss the research gaps and opportunities in the field.\nWith built-in mechanisms for continuous community updates, EarXplore not only\nreflects the current state of the field but also evolves alongside it, serving\nas a living resource to inform and accelerate future developments."}
{"id": "2507.19969", "pdf": "https://arxiv.org/pdf/2507.19969.pdf", "abs": "https://arxiv.org/abs/2507.19969", "title": "Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text", "authors": ["Mizanur Rahman", "Md Tahmid Rahman Laskar", "Shafiq Joty", "Enamul Hoque"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Automated data visualization plays a crucial role in simplifying data\ninterpretation, enhancing decision-making, and improving efficiency. While\nlarge language models (LLMs) have shown promise in generating visualizations\nfrom natural language, the absence of comprehensive benchmarks limits the\nrigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark\ndesigned to assess text-to-visualization models, covering 20+ chart types and\ndiverse data science queries, including trend analysis, correlation, outlier\ndetection, and predictive analytics. It comprises 1,985 samples, each with a\ndata table, natural language query, short answer, visualization code, and\nannotated charts. The queries involve complex reasoning, conversational turns,\nand dynamic data retrieval. We benchmark 11 open-source and closed-source\nmodels, revealing significant performance gaps, highlighting key challenges,\nand offering insights for future advancements. To close this gap, we propose\nthe first cross-modal actor-critic agentic framework that jointly refines the\ntextual answer and visualization code, increasing GPT-4o`s pass rate from 26%\nto 42% over the direct approach and improving chart quality. We also introduce\nan automated LLM-based evaluation framework that enables scalable assessment\nacross thousands of samples without human annotation, measuring answer\ncorrectness, code execution success, visualization readability, and chart\naccuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis."}
{"id": "2507.20720", "pdf": "https://arxiv.org/pdf/2507.20720.pdf", "abs": "https://arxiv.org/abs/2507.20720", "title": "Beyond Text: Probing K-12 Educators' Perspectives and Ideas for Learning Opportunities Leveraging Multimodal Large Language Models", "authors": ["Tiffany Tseng", "Katelyn Lam", "Tiffany Lin Fu", "Alekhya Maram"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are beginning to empower new user\nexperiences that can flexibly generate content from a range of inputs,\nincluding images, text, speech, and video. These capabilities have the\npotential to enrich learning by enabling users to capture and interact with\ninformation using a variety of modalities, but little is known about how\neducators envision how MLLMs might shape the future of learning experiences,\nwhat challenges diverse teachers encounter when interpreting how these models\nwork, and what practical needs should be considered for successful\nimplementation in educational contexts. We investigated educator perspectives\nthrough formative workshops with 12 K-12 educators, where participants\nbrainstormed learning opportunities, discussed practical concerns for effective\nuse, and prototyped their own MLLM-powered learning applications using Claude\n3.5 and its Artifacts feature for previewing code-based output. We use case\nstudies to illustrate two contrasting end-user approaches (teacher-and\nstudent-driven), and share insights about opportunities and concerns expressed\nby our participants, ending with implications for leveraging MLLMs for future\nlearning experiences."}
{"id": "2507.19980", "pdf": "https://arxiv.org/pdf/2507.19980.pdf", "abs": "https://arxiv.org/abs/2507.19980", "title": "Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory", "authors": ["Dan Song", "Won-Chan Lee", "Hong Jiao"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the estimation of reliability for large language\nmodels (LLMs) in scoring writing tasks from the AP Chinese Language and Culture\nExam. Using generalizability theory, the research evaluates and compares score\nconsistency between human and AI raters across two types of AP Chinese\nfree-response writing tasks: story narration and email response. These essays\nwere independently scored by two trained human raters and seven AI raters. Each\nessay received four scores: one holistic score and three analytic scores\ncorresponding to the domains of task completion, delivery, and language use.\nResults indicate that although human raters produced more reliable scores\noverall, LLMs demonstrated reasonable consistency under certain conditions,\nparticularly for story narration tasks. Composite scoring that incorporates\nboth human and AI raters improved reliability, which supports that hybrid\nscoring models may offer benefits for large-scale writing assessments."}
{"id": "2507.20730", "pdf": "https://arxiv.org/pdf/2507.20730.pdf", "abs": "https://arxiv.org/abs/2507.20730", "title": "Vocalize: Lead Acquisition and User Engagement through Gamified Voice Competitions", "authors": ["Edvin Teskeredzic", "Muamer Paric", "Adna Sestic", "Petra Fribert", "Anamarija Lukac", "Hadzem Hadzic", "Kemal Altwlkany", "Emanuel Lacic"], "categories": ["cs.HC", "cs.MM"], "comment": "Accepted to ACM Hypertext 2025", "summary": "This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals."}
{"id": "2507.19995", "pdf": "https://arxiv.org/pdf/2507.19995.pdf", "abs": "https://arxiv.org/abs/2507.19995", "title": "VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering", "authors": ["Tan-Minh Nguyen", "Hoang-Trung Nguyen", "Trong-Khoi Dao", "Xuan-Hieu Phan", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs) has led to significant\nachievements in various domains, including legal text processing. Leveraging\nLLMs for legal tasks is a natural evolution and an increasingly compelling\nchoice. However, their capabilities are often portrayed as greater than they\ntruly are. Despite the progress, we are still far from the ultimate goal of\nfully automating legal tasks using artificial intelligence (AI) and natural\nlanguage processing (NLP). Moreover, legal systems are deeply domain-specific\nand exhibit substantial variation across different countries and languages. The\nneed for building legal text processing applications for different natural\nlanguages is, therefore, large and urgent. However, there is a big challenge\nfor legal NLP in low-resource languages such as Vietnamese due to the scarcity\nof resources and annotated data. The need for labeled legal corpora for\nsupervised training, validation, and supervised fine-tuning is critical. In\nthis paper, we introduce the VLQA dataset, a comprehensive and high-quality\nresource tailored for the Vietnamese legal domain. We also conduct a\ncomprehensive statistical analysis of the dataset and evaluate its\neffectiveness through experiments with state-of-the-art models on legal\ninformation retrieval and question-answering tasks."}
{"id": "2507.20741", "pdf": "https://arxiv.org/pdf/2507.20741.pdf", "abs": "https://arxiv.org/abs/2507.20741", "title": "Beyond QWERTY: A pressure-based text input approach for XR that enables a touch-typing like experience", "authors": ["Fabian Rücker", "Torben Storch"], "categories": ["cs.HC"], "comment": null, "summary": "Text input in extended reality (XR) applications remains inefficient and\ntedious. Most solutions are derived from the traditional keyboard layout, yet\nfail to translate its positive characteristics to the spatial digital realm.\nThis limits the productive use of immersive technologies. In this work, we\nanalyze physical keyboard input to identify key characteristics that facilitate\nits comfort, touch typing and high typing speeds. Building on these findings,\nwe propose a novel pressure-based text input modality that transfers these\ncharacteristics into immersive space by substituting the two-dimensional QWERTY\nlayout with a linear scale. This design facilitates a touch-typing-like\nexperience, eliminating the need for visual guidance for proficient users. Our\nskill-based approach enables typing speeds of over 200 characters per minute.\nAdditionally, it is suitable for discreet use in public spaces and everyday\ntext-input tasks, since the proposed system requires virtually no hand or\nfinger movements and resembles smartphone-based text input in appearance."}
{"id": "2507.20019", "pdf": "https://arxiv.org/pdf/2507.20019.pdf", "abs": "https://arxiv.org/abs/2507.20019", "title": "Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach", "authors": ["Saurav Singla", "Aarav Singla", "Advik Gupta", "Parnika Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages. PyTorch code for few-shot anomaly detection using\n  meta-learning is available upon request or can be shared via GitHub", "summary": "We propose a meta learning framework for detecting anomalies in human\nlanguage across diverse domains with limited labeled data. Anomalies in\nlanguage ranging from spam and fake news to hate speech pose a major challenge\ndue to their sparsity and variability. We treat anomaly detection as a few shot\nbinary classification problem and leverage meta-learning to train models that\ngeneralize across tasks. Using datasets from domains such as SMS spam, COVID-19\nfake news, and hate speech, we evaluate model generalization on unseen tasks\nwith minimal labeled anomalies. Our method combines episodic training with\nprototypical networks and domain resampling to adapt quickly to new anomaly\ndetection tasks. Empirical results show that our method outperforms strong\nbaselines in F1 and AUC scores. We also release the code and benchmarks to\nfacilitate further research in few-shot text anomaly detection."}
{"id": "2507.20805", "pdf": "https://arxiv.org/pdf/2507.20805.pdf", "abs": "https://arxiv.org/abs/2507.20805", "title": "Understanding Bias in Perceiving Dimensionality Reduction Projections", "authors": ["Seoyoung Doh", "Hyeon Jeon", "Sungbok Shin", "Ghulam Jilani Quadri", "Nam Wook Kim", "Jinwook Seo"], "categories": ["cs.HC", "cs.LG"], "comment": "6 pages", "summary": "Selecting the dimensionality reduction technique that faithfully represents\nthe structure is essential for reliable visual communication and analytics. In\nreality, however, practitioners favor projections for other attractions, such\nas aesthetics and visual saliency, over the projection's structural\nfaithfulness, a bias we define as visual interestingness. In this research, we\nconduct a user study that (1) verifies the existence of such bias and (2)\nexplains why the bias exists. Our study suggests that visual interestingness\nbiases practitioners' preferences when selecting projections for analysis, and\nthis bias intensifies with color-encoded labels and shorter exposure time.\nBased on our findings, we discuss strategies to mitigate bias in perceiving and\ninterpreting DR projections."}
{"id": "2507.20030", "pdf": "https://arxiv.org/pdf/2507.20030.pdf", "abs": "https://arxiv.org/abs/2507.20030", "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression", "authors": ["Runchao Li", "Yao Fu", "Mu Sheng", "Xianxuan Long", "Haotian Yu", "Pan Li"], "categories": ["cs.CL"], "comment": null, "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."}
{"id": "2507.20933", "pdf": "https://arxiv.org/pdf/2507.20933.pdf", "abs": "https://arxiv.org/abs/2507.20933", "title": "ProForm: Solder-Free Circuit Assembly Using Thermoforming", "authors": ["Narjes Pourjafarian", "Zhenming Yang", "Jeffrey I. Lipton", "Benyamin Davaji", "Gregory D. Abowd"], "categories": ["cs.HC", "cond-mat.mtrl-sci"], "comment": null, "summary": "Electronic waste (e-waste) is a growing global challenge, with millions of\nfunctional components discarded due to the difficulty of repair and reuse.\nTraditional circuit assembly relies on soldering, which creates semi-permanent\nbonds that limit component recovery and contribute to unnecessary waste. We\nintroduce ProForm, a thermoforming approach for solder-free circuit\nprototyping. By encapsulating electronic components with pressure-formed\nthermoplastics, ProForm enables secure, reversible mounting without the need\nfor solder or custom mechanical housings. This approach supports a wide range\nof substrates, including flexible, paper-based, and non-planar circuits,\nfacilitating easy reuse, replacement, and rapid prototyping. We demonstrate\nProForm's versatility to support prototyping practices. We show that ProFormed\ncircuits exhibit good electrical performance and mechanical stability. While\nmotivated by a need for sustainable electronics practices, ProForm has other\nsignificant advantages over traditional soldering."}
{"id": "2507.20046", "pdf": "https://arxiv.org/pdf/2507.20046.pdf", "abs": "https://arxiv.org/abs/2507.20046", "title": "Infogen: Generating Complex Statistical Infographics from Documents", "authors": ["Akash Ghosh", "Aparna Garimella", "Pritika Ramu", "Sambaran Bandyopadhyay", "Sriparna Saha"], "categories": ["cs.CL"], "comment": "ACL Main 2025", "summary": "Statistical infographics are powerful tools that simplify complex data into\nvisually engaging and easy-to-understand formats. Despite advancements in AI,\nparticularly with LLMs, existing efforts have been limited to generating simple\ncharts, with no prior work addressing the creation of complex infographics from\ntext-heavy documents that demand a deep understanding of the content. We\naddress this gap by introducing the task of generating statistical infographics\ncomposed of multiple sub-charts (e.g., line, bar, pie) that are contextually\naccurate, insightful, and visually aligned. To achieve this, we define\ninfographic metadata that includes its title and textual insights, along with\nsub-chart-specific details such as their corresponding data and alignment. We\nalso present Infodat, the first benchmark dataset for text-to-infographic\nmetadata generation, where each sample links a document to its metadata. We\npropose Infogen, a two-stage framework where fine-tuned LLMs first generate\nmetadata, which is then converted into infographic code. Extensive evaluations\non Infodat demonstrate that Infogen achieves state-of-the-art performance,\noutperforming both closed and open-source LLMs in text-to-statistical\ninfographic generation."}
{"id": "2507.20943", "pdf": "https://arxiv.org/pdf/2507.20943.pdf", "abs": "https://arxiv.org/abs/2507.20943", "title": "The Impact of Simple, Brief, and Adaptive Instructions within Virtual Reality Training: Components of Cognitive Load Theory in an Assembly Task", "authors": ["Rebecca L. Pharmer", "Christopher D. Wickens", "Lucas Plabst", "Benjamin A. Clegg", "Leanne M. Hirshfield", "Joanna E. Lewis", "Jalynn B. Nicoly", "Cara A. Spencer", "Francisco R. Ortega"], "categories": ["cs.HC"], "comment": null, "summary": "Objective: The study examined the effects of varying all three core elements\nof cognitive load on learning efficiency during a shape assembly task in\nvirtual reality (VR).\n  Background: Adaptive training systems aim to improve learning efficiency and\nretention by dynamically adjusting difficulty. However, design choices can\nimpact the cognitive workload imposed on the learner. The present experiments\nexamined how aspects of cognitive load impact training outcomes.\n  Method: Participants learned step-by-step shape assembly in a VR environment.\nCognitive load was manipulated across three dimensions: Intrinsic Load (shape\ncomplexity), Extraneous Load (instruction verbosity), and Germane Load\n(adaptive vs. fixed training). In adaptive training (experiment 1), difficulty\nincreased based on individual performance. In fixed training (experiment 2),\ndifficulty followed a preset schedule from a yoked participant.\n  Results: Higher Intrinsic Load significantly increased training times and\nsubjective workload but did not affect retention test accuracy. Extraneous Load\nmodestly impacted training time, with little impact on workload or retention.\nAdaptive training shortened overall training time without increasing workload\nor impairing retention. No interactions were observed between the three types\nof load. Conclusion: Both Intrinsic and Extraneous Load increased training\ntime, but adaptive training improved efficiency without harming retention. The\nlack of interaction between the elements suggests training benefits can be\nworth seeking within any of the components of cognitive load. Application:\nThese findings support the use of VR adaptive systems in domains such as\nmanufacturing and military service, where efficient assembly skill acquisition\nis critical. Tailoring difficulty in real-time can optimize efficiency without\ncompromising learning."}
{"id": "2507.20055", "pdf": "https://arxiv.org/pdf/2507.20055.pdf", "abs": "https://arxiv.org/abs/2507.20055", "title": "A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications", "authors": ["Avaljot Singh", "Yamin Chandini Sarita", "Aditya Mishra", "Ishaan Goyal", "Gagandeep Singh", "Charith Mendis"], "categories": ["cs.CL"], "comment": null, "summary": "The uninterpretability of DNNs has led to the adoption of abstract\ninterpretation-based certification as a practical means to establish trust in\nreal-world systems that rely on DNNs. However, the current landscape supports\nonly a limited set of certifiers, and developing new ones or modifying existing\nones for different applications remains difficult. This is because the\nmathematical design of certifiers is expressed at the neuron level, while their\nimplementations are optimized and executed at the tensor level. This mismatch\ncreates a semantic gap between design and implementation, making manual\nbridging both complex and expertise-intensive -- requiring deep knowledge in\nformal methods, high-performance computing, etc.\n  We propose a compiler framework that automatically translates neuron-level\nspecifications of DNN certifiers into tensor-based, layer-level\nimplementations. This is enabled by two key innovations: a novel stack-based\nintermediate representation (IR) and a shape analysis that infers the implicit\ntensor operations needed to simulate the neuron-level semantics. During\nlifting, the shape analysis creates tensors in the minimal shape required to\nperform the corresponding operations. The IR also enables domain-specific\noptimizations as rewrites. At runtime, the resulting tensor computations\nexhibit sparsity tied to the DNN architecture. This sparsity does not align\nwell with existing formats. To address this, we introduce g-BCSR, a\ndouble-compression format that represents tensors as collections of blocks of\nvarying sizes, each possibly internally sparse.\n  Using our compiler and g-BCSR, we make it easy to develop new certifiers and\nanalyze their utility across diverse DNNs. Despite its flexibility, the\ncompiler achieves performance comparable to hand-optimized implementations."}
{"id": "2507.21000", "pdf": "https://arxiv.org/pdf/2507.21000.pdf", "abs": "https://arxiv.org/abs/2507.21000", "title": "Towards Effective Human Performance in XR Space Framework based on Real-time Eye Tracking Biofeedback", "authors": ["Barbara Karpowicz", "Tomasz Kowalewski", "Pavlo Zinevych", "Adam Kuzdraliński", "Grzegorz Marcin Wójcik", "Wiesław Kopeć"], "categories": ["cs.HC"], "comment": null, "summary": "This paper proposes an eye tracking module for the XR Space Framework aimed\nat enhancing human performance in XR-based applications, specifically in\ntraining, screening, and teleoperation. This framework provides a methodology\nand components that streamline the development of adaptive real-time virtual\nimmersive systems. It contains multimodal measurements - declarative in the\nform of in-VR questionnaires and objective, including eye tracking, body\nmovement, and psychophysiological data (e.g., ECG, GSR, PPG). A key focus of\nthis paper is the integration of real-time eye tracking data into XR\nenvironments to facilitate a biofeedback loop, providing insight into user\nattention, cognitive load, and engagement. Given the relatively high\nmeasurement frequency of eye tracking - recognized as a noninvasive yet robust\npsychophysiological measure - this technology is particularly well suited for\nreal-time adjustments in task difficulty and feedback to enhance learning and\noperational effectiveness. Despite its established role in cognitive and\nattentional studies, implementing eye tracking metrics within dynamic,\nreal-time XR environments poses unique challenges, particularly given the\ncomplex moving visuals presented in head-mounted displays (HMDs). This paper\naddresses these challenges by focusing on the essential aspects of integrating\neye tracking in immersive systems based on real-time engines, ultimately\nfacilitating more efficient, adaptive XR applications."}
{"id": "2507.20059", "pdf": "https://arxiv.org/pdf/2507.20059.pdf", "abs": "https://arxiv.org/abs/2507.20059", "title": "RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation", "authors": ["Ran Xu", "Yuchen Zhuang", "Yue Yu", "Haoyu Wang", "Wenqi Shi", "Carl Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. Code will be published at:\n  https://github.com/ritaranx/RAG_in_the_Wild", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved at inference time. While RAG\ndemonstrates strong performance on benchmarks largely derived from\ngeneral-domain corpora like Wikipedia, its effectiveness under realistic,\ndiverse retrieval scenarios remains underexplored. We evaluated RAG systems\nusing MassiveDS, a large-scale datastore with mixture of knowledge, and\nidentified critical limitations: retrieval mainly benefits smaller models,\nrerankers add minimal value, and no single retrieval source consistently\nexcels. Moreover, current LLMs struggle to route queries across heterogeneous\nknowledge sources. These findings highlight the need for adaptive retrieval\nstrategies before deploying RAG in real-world settings. Our code and data can\nbe found at https://github.com/ritaranx/RAG_in_the_Wild."}
{"id": "2507.21012", "pdf": "https://arxiv.org/pdf/2507.21012.pdf", "abs": "https://arxiv.org/abs/2507.21012", "title": "User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with \"Vibe Coding\"", "authors": ["Tianyi Li", "Tanay Maheshwari", "Alex Voelker"], "categories": ["cs.HC"], "comment": null, "summary": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise."}
{"id": "2507.20091", "pdf": "https://arxiv.org/pdf/2507.20091.pdf", "abs": "https://arxiv.org/abs/2507.20091", "title": "ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models", "authors": ["Kaizhi Qian", "Xulin Fan", "Junrui Ni", "Slava Shechtman", "Mark Hasegawa-Johnson", "Chuang Gan", "Yang Zhang"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Speech language models refer to language models with speech processing and\nunderstanding capabilities. One key desirable capability for speech language\nmodels is the ability to capture the intricate interdependency between content\nand prosody. The existing mainstream paradigm of training speech language\nmodels, which converts speech into discrete tokens before feeding them into\nLLMs, is sub-optimal in learning prosody information -- we find that the\nresulting LLMs do not exhibit obvious emerging prosody processing capabilities\nvia pre-training alone. To overcome this, we propose ProsodyLM, which\nintroduces a simple tokenization scheme amenable to learning prosody. Each\nspeech utterance is first transcribed into text, followed by a sequence of\nword-level prosody tokens. Compared with conventional speech tokenization\nschemes, the proposed tokenization scheme retains more complete prosody\ninformation, and is more understandable to text-based LLMs. We find that\nProsodyLM can learn surprisingly diverse emerging prosody processing\ncapabilities through pre-training alone, ranging from harnessing the prosody\nnuances in generated speech, such as contrastive focus, understanding emotion\nand stress in an utterance, to maintaining prosody consistency in long\ncontexts."}
{"id": "2507.19484", "pdf": "https://arxiv.org/pdf/2507.19484.pdf", "abs": "https://arxiv.org/abs/2507.19484", "title": "Towards the ideals of Self-Recovery and Metadata Privacy in Social Vault Recovery", "authors": ["Shailesh Mishra", "Simone Colombo", "Pasindu Tennage", "Martin Burkhart", "Bryan Ford"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Social key recovery mechanisms enable users to recover their vaults with the\nhelp of trusted contacts, or trustees, avoiding the need for a single point of\ntrust or memorizing complex strings. However, existing mechanisms overlook the\nmemorability demands on users for recovery, such as the need to recall a\nthreshold number of trustees. Therefore, we first formalize the notion of\nrecovery metadata in the context of social key recovery, illustrating the\ntradeoff between easing the burden of memorizing the metadata and maintaining\nmetadata privacy. We present Apollo, the first framework that addresses this\ntradeoff by distributing indistinguishable data within a user's social circle,\nwhere trustees hold relevant data and non-trustees store random data. Apollo\neliminates the need to memorize recovery metadata since a user eventually\ngathers sufficient data from her social circle for recovery. Due to\nindistinguishability, Apollo protects metadata privacy by forming an anonymity\nset that hides the trustees among non-trustees. To make the anonymity set\nscalable, Apollo proposes a novel multi-layered secret sharing scheme that\nmitigates the overhead due to the random data distributed among non-trustees.\nFinally, we provide a prototype implementation of Apollo and report on its\nperformance. Apollo reduces the chances of malicious recovery to between 0.005%\nand 1.8%, depending on the adversary's ability to compromise. The multi-layered\ndesign shows a latency reduction from 1.1x to 740kx compared to a\nsingle-layered approach, depending on the number of reconnections."}
{"id": "2507.20111", "pdf": "https://arxiv.org/pdf/2507.20111.pdf", "abs": "https://arxiv.org/abs/2507.20111", "title": "AI-Driven Generation of Old English: A Framework for Low-Resource Languages", "authors": ["Rodrigo Gabriel Salazar Alva", "Matías Nuñez", "Cristian López", "Javier Martín Arista"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Preserving ancient languages is essential for understanding humanity's\ncultural and linguistic heritage, yet Old English remains critically\nunder-resourced, limiting its accessibility to modern natural language\nprocessing (NLP) techniques. We present a scalable framework that uses advanced\nlarge language models (LLMs) to generate high-quality Old English texts,\naddressing this gap. Our approach combines parameter-efficient fine-tuning\n(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a\ndual-agent pipeline that separates the tasks of content generation (in English)\nand translation (into Old English). Evaluation with automated metrics (BLEU,\nMETEOR, and CHRF) shows significant improvements over baseline models, with\nBLEU scores increasing from 26 to over 65 for English-to-Old English\ntranslation. Expert human assessment also confirms high grammatical accuracy\nand stylistic fidelity in the generated texts. Beyond expanding the Old English\ncorpus, our method offers a practical blueprint for revitalizing other\nendangered languages, effectively uniting AI innovation with the goals of\ncultural preservation."}
{"id": "2507.19487", "pdf": "https://arxiv.org/pdf/2507.19487.pdf", "abs": "https://arxiv.org/abs/2507.19487", "title": "Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective", "authors": ["Margarita Leib", "Nils Köbis", "Ivan Soraperra"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "People increasingly rely on AI-advice when making decisions. At times, such\nadvice can promote selfish behavior. When individuals abide by\nselfishness-promoting AI advice, how are they perceived and punished? To study\nthis question, we build on theories from social psychology and combine\nmachine-behavior and behavioral economic approaches. In a pre-registered,\nfinancially-incentivized experiment, evaluators could punish real\ndecision-makers who (i) received AI, human, or no advice. The advice (ii)\nencouraged selfish or prosocial behavior, and decision-makers (iii) behaved\nselfishly or, in a control condition, behaved prosocially. Evaluators further\nassigned responsibility to decision-makers and their advisors. Results revealed\nthat (i) prosocial behavior was punished very little, whereas selfish behavior\nwas punished much more. Focusing on selfish behavior, (ii) compared to\nreceiving no advice, selfish behavior was penalized more harshly after\nprosocial advice and more leniently after selfish advice. Lastly, (iii) whereas\nselfish decision-makers were seen as more responsible when they followed AI\ncompared to human advice, punishment between the two advice sources did not\nvary. Overall, behavior and advice content shape punishment, whereas the advice\nsource does not."}
{"id": "2507.20133", "pdf": "https://arxiv.org/pdf/2507.20133.pdf", "abs": "https://arxiv.org/abs/2507.20133", "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering", "authors": ["Anas Mohamed", "Azal Ahmad Khan", "Xinran Wang", "Ahmad Faraz Khan", "Shuwen Ge", "Saman Bahzad Khan", "Ayaan Ahmad", "Ali Anwar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative AI can now synthesize strikingly realistic images from text, yet\noutput quality remains highly sensitive to how prompts are phrased. Direct\nPreference Optimization (DPO) offers a lightweight, off-policy alternative to\nRL for automatic prompt engineering, but its token-level regularization leaves\nsemantic inconsistency unchecked as prompts that win higher preference scores\ncan still drift away from the user's intended meaning.\n  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency\nyet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an\nexponential weight proportional to the cosine distance between the original\nprompt and winning candidate in embedding space, softly down-weighting training\nsignals that would otherwise reward semantically mismatched prompts. We provide\nthe first analytical bound on semantic drift for preference-tuned prompt\ngenerators, showing that Sem-DPO keeps learned prompts within a provably\nbounded neighborhood of the original text. On three standard text-to-image\nprompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%\nhigher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,\nPickScore) than DPO, while also outperforming state-of-the-art baselines. These\nfindings suggest that strong flat baselines augmented with semantic weighting\nshould become the new standard for prompt-optimization studies and lay the\ngroundwork for broader, semantics-aware preference optimization in language\nmodels."}
{"id": "2507.19489", "pdf": "https://arxiv.org/pdf/2507.19489.pdf", "abs": "https://arxiv.org/abs/2507.19489", "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital."}
{"id": "2507.20136", "pdf": "https://arxiv.org/pdf/2507.20136.pdf", "abs": "https://arxiv.org/abs/2507.20136", "title": "Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG", "authors": ["Baiyu Chen", "Wilson Wongso", "Xiaoqian Hu", "Yue Tan", "Flora Salim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD Cup 2025 Meta CRAG-MM Challenge", "summary": "This paper presents the technical solution developed by team CRUISE for the\nKDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn\n(CRAG-MM) challenge. The challenge aims to address a critical limitation of\nmodern Vision Language Models (VLMs): their propensity to hallucinate,\nespecially when faced with egocentric imagery, long-tail entities, and complex,\nmulti-hop questions. This issue is particularly problematic in real-world\napplications where users pose fact-seeking queries that demand high factual\naccuracy across diverse modalities. To tackle this, we propose a robust,\nmulti-stage framework that prioritizes factual accuracy and truthfulness over\ncompleteness. Our solution integrates a lightweight query router for\nefficiency, a query-aware retrieval and summarization pipeline, a dual-pathways\ngeneration and a post-hoc verification. This conservative strategy is designed\nto minimize hallucinations, which incur a severe penalty in the competition's\nscoring metric. Our approach achieved 3rd place in Task 1, demonstrating the\neffectiveness of prioritizing answer reliability in complex multi-modal RAG\nsystems. Our implementation is available at\nhttps://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM ."}
{"id": "2507.19692", "pdf": "https://arxiv.org/pdf/2507.19692.pdf", "abs": "https://arxiv.org/abs/2507.19692", "title": "FlashGuard: Novel Method in Evaluating Differential Characteristics of Visual Stimuli for Deterring Seizure Triggers in Photosensitive Epilepsy", "authors": ["Ishan Pendyala"], "categories": ["cs.CY", "cs.HC", "cs.SI"], "comment": null, "summary": "In the virtual realm, individuals with photosensitive epilepsy (PSE)\nencounter challenges when using devices, resulting in exposure to unpredictable\nseizure-causing visual stimuli. The current norm for preventing epileptic\nflashes in media is to detect asynchronously when a flash will occur in a\nvideo, then notifying the user. However, there is a lack of a real-time and\ncomputationally efficient solution for dealing with this issue. To address this\nissue and enhance accessibility for photosensitive viewers, FlashGuard, a novel\napproach, was devised to assess the rate of change of colors in frames across\nthe user's screen and appropriately mitigate stimuli, based on perceptually\naligned color space analysis in the CIELAB color space. The detection system is\nbuilt on analyzing differences in color, and the mitigation system works by\nreducing luminance and smoothing color transitions. This study provides novel\ninsight into how intrinsic color properties contribute to perceptual\ndifferences in flashing for PSE individuals, calling for the adoption of\nbroadened WCAG guidelines to better account for risk. These insights and\nimplementations pave the way for stronger protections for individuals with PSE\nfrom dangerous triggers in digital media, both in policy and in software."}
{"id": "2507.20145", "pdf": "https://arxiv.org/pdf/2507.20145.pdf", "abs": "https://arxiv.org/abs/2507.20145", "title": "Multi-Agent Interactive Question Generation Framework for Long Document Understanding", "authors": ["Kesen Wang", "Daulet Toibazar", "Abdulrahman Alfulayt", "Abdulaziz S. Albadawi", "Ranya A. Alkahtani", "Asma A. Ibrahim", "Haneen A. Alhomoud", "Sherif Mohamed", "Pedro J. Moreno"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Document Understanding (DU) in long-contextual scenarios with complex layouts\nremains a significant challenge in vision-language research. Although Large\nVision-Language Models (LVLMs) excel at short-context DU tasks, their\nperformance declines in long-context settings. A key limitation is the scarcity\nof fine-grained training data, particularly for low-resource languages such as\nArabic. Existing state-of-the-art techniques rely heavily on human annotation,\nwhich is costly and inefficient. We propose a fully automated, multi-agent\ninteractive framework to generate long-context questions efficiently. Our\napproach efficiently generates high-quality single- and multi-page questions\nfor extensive English and Arabic documents, covering hundreds of pages across\ndiverse domains. This facilitates the development of LVLMs with enhanced\nlong-context understanding ability. Experimental results in this work have\nshown that our generated English and Arabic questions\n(\\textbf{AraEngLongBench}) are quite challenging to major open- and\nclose-source LVLMs. The code and data proposed in this work can be found in\nhttps://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and\nAnswer (QA) pairs and structured system prompts can be found in the Appendix."}
{"id": "2507.19854", "pdf": "https://arxiv.org/pdf/2507.19854.pdf", "abs": "https://arxiv.org/abs/2507.19854", "title": "Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models", "authors": ["Anjali R. Menon", "Rohit K. Sharma", "Priya Singh", "Chengyu Wang", "Aurora M. Ferreira", "Mateja Novak"], "categories": ["cs.RO", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "comment": "13 pages, 7 figures", "summary": "The integration of Large Language Models (LLMs) into robotics has unlocked\nunprecedented capabilities in high-level task planning. However, most current\nsystems operate in an open-loop fashion, where LLMs act as one-shot planners,\nrendering them brittle and unable to adapt to unforeseen circumstances in\ndynamic physical environments. To overcome this limitation, this paper\nintroduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that\nenables an embodied agent to autonomously learn and refine its policies through\ncontinuous interaction. Our framework establishes a closed-loop cycle where an\nLLM first \"thinks\" by decomposing high-level commands into actionable plans.\nThe robot then \"acts\" by executing these plans while gathering rich, multimodal\nsensory feedback. Critically, the \"learn\" module processes this feedback to\nfacilitate LLM-driven self-reflection, allowing the agent to perform causal\nanalysis on its failures and generate corrective strategies. These insights are\nstored in an experiential memory to guide future planning cycles. We\ndemonstrate through extensive experiments in both simulation and the real world\nthat our T-A-L agent significantly outperforms baseline methods, including\nopen-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our\nframework achieves over a 97% success rate on complex, long-horizon tasks,\nconverges to a stable policy in an average of just 9 trials, and exhibits\nremarkable generalization to unseen tasks. This work presents a significant\nstep towards developing more robust, adaptive, and truly autonomous robotic\nagents."}
{"id": "2507.20152", "pdf": "https://arxiv.org/pdf/2507.20152.pdf", "abs": "https://arxiv.org/abs/2507.20152", "title": "Goal Alignment in LLM-Based User Simulators for Conversational AI", "authors": ["Shuhaib Mehri", "Xiaocheng Yang", "Takyoung Kim", "Gokhan Tur", "Shikib Mehri", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "User simulators are essential to conversational AI, enabling scalable agent\ndevelopment and evaluation through simulated interactions. While current Large\nLanguage Models (LLMs) have advanced user simulation capabilities, we reveal\nthat they struggle to consistently demonstrate goal-oriented behavior across\nmulti-turn conversations--a critical limitation that compromises their\nreliability in downstream applications. We introduce User Goal State Tracking\n(UGST), a novel framework that tracks user goal progression throughout\nconversations. Leveraging UGST, we present a three-stage methodology for\ndeveloping user simulators that can autonomously track goal progression and\nreason to generate goal-aligned responses. Moreover, we establish comprehensive\nevaluation metrics for measuring goal alignment in user simulators, and\ndemonstrate that our approach yields substantial improvements across two\nbenchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a\ncritical gap in conversational AI and establish UGST as an essential framework\nfor developing goal-aligned user simulators."}
{"id": "2507.19855", "pdf": "https://arxiv.org/pdf/2507.19855.pdf", "abs": "https://arxiv.org/abs/2507.19855", "title": "Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning", "authors": ["Aditya Sharma", "Linh Nguyen", "Ananya Gupta", "Chengyu Wang", "Chiamaka Adebayo", "Jakub Kowalski"], "categories": ["cs.LG", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "comment": "12 pages, 4 figures,", "summary": "Large Language Models (LLMs), despite their advanced linguistic capabilities,\nfundamentally lack an intuitive understanding of physical dynamics, which\nlimits their effectiveness in real-world scenarios that require causal\nreasoning. In this paper, we introduce Causal World Model Induction (CWMI), a\nnovel framework designed to embed an explicit model of causal physics within an\nLLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a\nnew training objective called Causal Intervention Loss, encouraging the model\nto learn cause-and-effect relationships from multimodal data. By training the\nmodel to predict the outcomes of hypothetical interventions instead of merely\ncapturing statistical correlations, CWMI develops a robust internal\nrepresentation of physical laws. Experimental results show that CWMI\nsignificantly outperforms state-of-the-art LLMs on zero-shot physical reasoning\ntasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench\ndataset. These findings demonstrate that inducing a causal world model is a\ncritical step toward more reliable and generalizable AI systems."}
{"id": "2507.20181", "pdf": "https://arxiv.org/pdf/2507.20181.pdf", "abs": "https://arxiv.org/abs/2507.20181", "title": "SGPO: Self-Generated Preference Optimization based on Self-Improver", "authors": ["Hyeonji Lee", "Daejin Jo", "Seohwan Yun", "Sungwoong Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs), despite their extensive pretraining on diverse\ndatasets, require effective alignment to human preferences for practical and\nreliable deployment. Conventional alignment methods typically employ off-policy\nlearning and depend on human-annotated datasets, which limits their broad\napplicability and introduces distribution shift issues during training. To\naddress these challenges, we propose Self-Generated Preference Optimization\nbased on Self-Improver (SGPO), an innovative alignment framework that leverages\nan on-policy self-improving mechanism. Specifically, the improver refines\nresponses from a policy model to self-generate preference data for direct\npreference optimization (DPO) of the policy model. Here, the improver and\npolicy are unified into a single model, and in order to generate higher-quality\npreference data, this self-improver learns to make incremental yet discernible\nimprovements to the current responses by referencing supervised fine-tuning\noutputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the\nproposed SGPO significantly improves performance over DPO and baseline\nself-improving methods without using external preference data."}
{"id": "2507.19870", "pdf": "https://arxiv.org/pdf/2507.19870.pdf", "abs": "https://arxiv.org/abs/2507.19870", "title": "OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration", "authors": ["Junwen Duan", "Wei Xue", "Ziyao Kang", "Shixia Liu", "Jiazhi Xia"], "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 11 figures", "summary": "Open-world object detection (OWOD) extends traditional object detection to\nidentifying both known and unknown object, necessitating continuous model\nadaptation as new annotations emerge. Current approaches face significant\nlimitations: 1) data-hungry training due to reliance on a large number of\ncrowdsourced annotations, 2) susceptibility to \"partial feature overfitting,\"\nand 3) limited flexibility due to required model architecture modifications. To\ntackle these issues, we present OW-CLIP, a visual analytics system that\nprovides curated data and enables data-efficient OWOD model incremental\ntraining. OW-CLIP implements plug-and-play multimodal prompt tuning tailored\nfor OWOD settings and introduces a novel \"Crop-Smoothing\" technique to mitigate\npartial feature overfitting. To meet the data requirements for the training\nmethodology, we propose dual-modal data refinement methods that leverage large\nlanguage models and cross-modal similarity for data generation and filtering.\nSimultaneously, we develope a visualization interface that enables users to\nexplore and deliver high-quality annotations: including class-specific visual\nfeature phrases and fine-grained differentiated images. Quantitative evaluation\ndemonstrates that OW-CLIP achieves competitive performance at 89% of\nstate-of-the-art performance while requiring only 3.8% self-generated data,\nwhile outperforming SOTA approach when trained with equivalent data volumes. A\ncase study shows the effectiveness of the developed method and the improved\nannotation quality of our visualization system."}
{"id": "2507.20185", "pdf": "https://arxiv.org/pdf/2507.20185.pdf", "abs": "https://arxiv.org/abs/2507.20185", "title": "SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding", "authors": ["Yuqi Yang", "Weiqi Wang", "Baixuan Xu", "Wei Fan", "Qing Zong", "Chunkit Chan", "Zheye Deng", "Xin Liu", "Yifan Gao", "Changlong Yu", "Chen Luo", "Yang Li", "Zheng Li", "Qingyu Yin", "Bing Yin", "Yangqiu Song"], "categories": ["cs.CL"], "comment": null, "summary": "Session history is a common way of recording user interacting behaviors\nthroughout a browsing activity with multiple products. For example, if an user\nclicks a product webpage and then leaves, it might because there are certain\nfeatures that don't satisfy the user, which serve as an important indicator of\non-the-spot user preferences. However, all prior works fail to capture and\nmodel customer intention effectively because insufficient information\nexploitation and only apparent information like descriptions and titles are\nused. There is also a lack of data and corresponding benchmark for explicitly\nmodeling intention in E-commerce product purchase sessions. To address these\nissues, we introduce the concept of an intention tree and propose a dataset\ncuration pipeline. Together, we construct a sibling multimodal benchmark,\nSessionIntentBench, that evaluates L(V)LMs' capability on understanding\ninter-session intention shift with four subtasks. With 1,952,177 intention\nentries, 1,132,145 session intention trajectories, and 13,003,664 available\ntasks mined using 10,905 sessions, we provide a scalable way to exploit the\nexisting session data for customer intention understanding. We conduct human\nannotations to collect ground-truth label for a subset of collected data to\nform an evaluation gold set. Extensive experiments on the annotated data\nfurther confirm that current L(V)LMs fail to capture and utilize the intention\nacross the complex session setting. Further analysis show injecting intention\nenhances LLMs' performances."}
{"id": "2507.20419", "pdf": "https://arxiv.org/pdf/2507.20419.pdf", "abs": "https://arxiv.org/abs/2507.20419", "title": "Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?", "authors": ["Khloud AL Jallad", "Nada Ghneim", "Ghaida Rebdawi"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Natural Language Understanding (NLU) is a basic task in Natural Language\nProcessing (NLP). The evaluation of NLU capabilities has become a trending\nresearch topic that attracts researchers in the last few years, resulting in\nthe development of numerous benchmarks. These benchmarks include various tasks\nand datasets in order to evaluate the results of pretrained models via public\nleaderboards. Notably, several benchmarks contain diagnostics datasets designed\nfor investigation and fine-grained error analysis across a wide range of\nlinguistic phenomena. This survey provides a comprehensive review of available\nEnglish, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on\ntheir diagnostics datasets and the linguistic phenomena they covered. We\npresent a detailed comparison and analysis of these benchmarks, highlighting\ntheir strengths and limitations in evaluating NLU tasks and providing in-depth\nerror analysis. When highlighting the gaps in the state-of-the-art, we noted\nthat there is no naming convention for macro and micro categories or even a\nstandard set of linguistic phenomena that should be covered. Consequently, we\nformulated a research question regarding the evaluation metrics of the\nevaluation diagnostics benchmarks: \"Why do not we have an evaluation standard\nfor the NLU evaluation diagnostics benchmarks?\" similar to ISO standard in\nindustry. We conducted a deep analysis and comparisons of the covered\nlinguistic phenomena in order to support experts in building a global hierarchy\nfor linguistic phenomena in future. We think that having evaluation metrics for\ndiagnostics evaluation could be valuable to gain more insights when comparing\nthe results of the studied models on different diagnostics benchmarks."}
{"id": "2507.20187", "pdf": "https://arxiv.org/pdf/2507.20187.pdf", "abs": "https://arxiv.org/abs/2507.20187", "title": "Diversity-Enhanced Reasoning for Subjective Questions", "authors": ["Yumeng Wang", "Zhiyuan Fan", "Jiayu Liu", "Yi R. Fung"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities\nhave shown strong performance on objective tasks, such as math reasoning and\ncoding. However, their effectiveness on subjective questions that may have\ndifferent responses from different perspectives is still limited by a tendency\ntowards homogeneous reasoning, introduced by the reliance on a single ground\ntruth in supervised fine-tuning and verifiable reward in reinforcement\nlearning. Motivated by the finding that increasing role perspectives\nconsistently improves performance, we propose MultiRole-R1, a\ndiversity-enhanced framework with multiple role perspectives, to improve the\naccuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an\nunsupervised data construction pipeline that generates reasoning chains that\nincorporate diverse role perspectives. We further employ reinforcement learning\nvia Group Relative Policy Optimization (GRPO) with reward shaping, by taking\ndiversity as a reward signal in addition to the verifiable reward. With\nspecially designed reward functions, we successfully promote perspective\ndiversity and lexical diversity, uncovering a positive relation between\nreasoning diversity and accuracy. Our experiment on six benchmarks demonstrates\nMultiRole-R1's effectiveness and generalizability in enhancing both subjective\nand objective reasoning, showcasing the potential of diversity-enhanced\ntraining in LRMs."}
{"id": "2507.20536", "pdf": "https://arxiv.org/pdf/2507.20536.pdf", "abs": "https://arxiv.org/abs/2507.20536", "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation", "authors": ["Chieh-Yun Chen", "Min Shi", "Gong Zhang", "Humphrey Shi"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "ICCV 2025", "summary": "Text-to-Image (T2I) generative models have revolutionized content creation\nbut remain highly sensitive to prompt phrasing, often requiring users to\nrepeatedly refine prompts multiple times without clear feedback. While\ntechniques such as automatic prompt engineering, controlled text embeddings,\ndenoising, and multi-turn generation mitigate these issues, they offer limited\ncontrollability, or often necessitate additional training, restricting the\ngeneralization abilities. Thus, we introduce T2I-Copilot, a training-free\nmulti-agent system that leverages collaboration between (Multimodal) Large\nLanguage Models to automate prompt phrasing, model selection, and iterative\nrefinement. This approach significantly simplifies prompt engineering while\nenhancing generation quality and text-image alignment compared to direct\ngeneration. Specifically, T2I-Copilot consists of three agents: (1) Input\nInterpreter, which parses the input prompt, resolves ambiguities, and generates\na standardized report; (2) Generation Engine, which selects the appropriate\nmodel from different types of T2I models and organizes visual and textual\nprompts to initiate generation; and (3) Quality Evaluator, which assesses\naesthetic quality and text-image alignment, providing scores and feedback for\npotential regeneration. T2I-Copilot can operate fully autonomously while also\nsupporting human-in-the-loop intervention for fine-grained control. On\nGenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA\nscore comparable to commercial models RecraftV3 and Imagen 3, surpasses\nFLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and\nSD 3.5 Large by 9.11% and 6.36%. Code will be released at:\nhttps://github.com/SHI-Labs/T2I-Copilot."}
{"id": "2507.20208", "pdf": "https://arxiv.org/pdf/2507.20208.pdf", "abs": "https://arxiv.org/abs/2507.20208", "title": "IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs", "authors": ["Aviya Maimon", "Amir DN Cohen", "Gal Vishne", "Shauli Ravfogel", "Reut Tsarfaty"], "categories": ["cs.CL"], "comment": null, "summary": "Current evaluations of large language models (LLMs) rely on benchmark scores,\nbut it is difficult to interpret what these individual scores reveal about a\nmodel's overall skills. Specifically, as a community we lack understanding of\nhow tasks relate to one another, what they measure in common, how they differ,\nor which ones are redundant. As a result, models are often assessed via a\nsingle score averaged across benchmarks, an approach that fails to capture the\nmodels' wholistic strengths and limitations. Here, we propose a new evaluation\nparadigm that uses factor analysis to identify latent skills driving\nperformance across benchmarks. We apply this method to a comprehensive new\nleaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a\nsmall set of latent skills that largely explain performance. Finally, we turn\nthese insights into practical tools that identify redundant tasks, aid in model\nselection, and profile models along each latent skill."}
{"id": "2507.20632", "pdf": "https://arxiv.org/pdf/2507.20632.pdf", "abs": "https://arxiv.org/abs/2507.20632", "title": "Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualization without a Legend", "authors": ["Hongxu Liu", "Xinyu Chen", "Haoyang Zheng", "Manyi Li", "Zhenfan Liu", "Fumeng Yang", "Yunhai Wang", "Changhe Tu", "Qiong Zeng"], "categories": ["cs.CV", "cs.HC"], "comment": "Submitted to IEEE VIS 2025", "summary": "Recovering a continuous colormap from a single 2D scalar field visualization\ncan be quite challenging, especially in the absence of a corresponding color\nlegend. In this paper, we propose a novel colormap recovery approach that\nextracts the colormap from a color-encoded 2D scalar field visualization by\nsimultaneously predicting the colormap and underlying data using a\ndecoupling-and-reconstruction strategy. Our approach first separates the input\nvisualization into colormap and data using a decoupling module, then\nreconstructs the visualization with a differentiable color-mapping module. To\nguide this process, we design a reconstruction loss between the input and\nreconstructed visualizations, which serves both as a constraint to ensure\nstrong correlation between colormap and data during training, and as a\nself-supervised optimizer for fine-tuning the predicted colormap of unseen\nvisualizations during inferencing. To ensure smoothness and correct color\nordering in the extracted colormap, we introduce a compact colormap\nrepresentation using cubic B-spline curves and an associated color order loss.\nWe evaluate our method quantitatively and qualitatively on a synthetic dataset\nand a collection of real-world visualizations from the VIS30K dataset.\nAdditionally, we demonstrate its utility in two prototype applications --\ncolormap adjustment and colormap transfer -- and explore its generalization to\nvisualizations with color legends and ones encoded using discrete color\npalettes."}
{"id": "2507.20210", "pdf": "https://arxiv.org/pdf/2507.20210.pdf", "abs": "https://arxiv.org/abs/2507.20210", "title": "Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation", "authors": ["Minh Hoang Nguyen", "Thuat Thien Nguyen", "Minh Nhat Ta"], "categories": ["cs.CL", "68T50, 68T05", "I.2.7; I.7"], "comment": "11 pages, 6 figures", "summary": "News recommendation systems play a vital role in mitigating information\noverload by delivering personalized news content. A central challenge is to\neffectively model both multi-view news representations and the dynamic nature\nof user interests, which often span both short- and long-term preferences.\nExisting methods typically rely on single-view features of news articles (e.g.,\ntitles or categories) or fail to comprehensively capture user preferences\nacross time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news\nrecommendation framework that integrates NAML for attentive multi-view news\nmodeling and LSTUR for capturing both long- and short-term user\nrepresentations. Our model also incorporates BERT-based word embeddings to\nenhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely\nused benchmarks, MIND-small and MIND-large. Experimental results show that\nCo-NAML-LSTUR achieves substantial improvements over most state-of-the-art\nbaselines on MIND-small and MIND-large, respectively. These results demonstrate\nthe effectiveness of combining multi-view news representations with dual-scale\nuser modeling. The implementation of our model is publicly available at\nhttps://github.com/MinhNguyenDS/Co-NAML-LSTUR."}
{"id": "2507.20737", "pdf": "https://arxiv.org/pdf/2507.20737.pdf", "abs": "https://arxiv.org/abs/2507.20737", "title": "Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals", "authors": ["Geng-Xin Xu", "Xiang Zuo", "Ye Li"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "MICCAI2025", "summary": "Emotion recognition from physiological data is crucial for mental health\nassessment, yet it faces two significant challenges: incomplete multi-modal\nsignals and interference from body movements and artifacts. This paper presents\na novel Multi-Masked Querying Network (MMQ-Net) to address these issues by\nintegrating multiple querying mechanisms into a unified framework.\nSpecifically, it uses modality queries to reconstruct missing data from\nincomplete signals, category queries to focus on emotional state features, and\ninterference queries to separate relevant information from noise. Extensive\nexperiment results demonstrate the superior emotion recognition performance of\nMMQ-Net compared to existing approaches, particularly under high levels of data\nincompleteness."}
{"id": "2507.20241", "pdf": "https://arxiv.org/pdf/2507.20241.pdf", "abs": "https://arxiv.org/abs/2507.20241", "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models", "authors": ["Yi Feng", "Jiaqi Wang", "Wenxuan Zhang", "Zhuang Chen", "Yutong Shen", "Xiyao Xiao", "Minlie Huang", "Liping Jing", "Jian Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications."}
{"id": "2412.06336", "pdf": "https://arxiv.org/pdf/2412.06336.pdf", "abs": "https://arxiv.org/abs/2412.06336", "title": "A Combined Channel Approach for Decoding Intracranial EEG Signals: Enhancing Accuracy through Spatial Information Integration", "authors": ["Maryam Ostadsharif Memar", "Navid Ziaei", "Behzad Nazari"], "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "Intracranial EEG (iEEG) recording, characterized by high spatial and temporal\nresolution and superior signal-to-noise ratio (SNR), enables the development of\nprecise brain-computer interface (BCI) systems for neural decoding. However,\nthe invasive nature of the procedure significantly limits the availability of\niEEG datasets in terms of both the number of participants and the duration of\nrecorded sessions. To address this limitation, we propose a single-participant\nmachine learning model optimized for decoding iEEG signals. The model employs\n18 key features and operates in two modes: best channel and combined channel.\nThe combined channel mode integrates spatial information from multiple brain\nregions, leading to superior classification performance. Evaluations across\nthree datasets -- Music Reconstruction, Audio Visual, and AJILE12 --\ndemonstrate that the combined channel mode consistently outperforms the best\nchannel mode across all classifiers. In the best-performing cases, Random\nForest achieved an F1 score of 0.81 +/- 0.05 in the Music Reconstruction\ndataset and 0.82 +/- 0.10 in the Audio Visual dataset, while XGBoost achieved\nan F1 score of 0.84 +/- 0.08 in the AJILE12 dataset. Furthermore, the analysis\nof brain region contributions in the combined channel mode revealed that the\nmodel identifies relevant brain regions aligned with physiological expectations\nfor each task and effectively combines data from electrodes in these regions to\nachieve high performance. These findings highlight the potential of integrating\nspatial information across brain regions to improve task decoding, offering new\navenues for advancing BCI systems and neurotechnological applications."}
{"id": "2507.20249", "pdf": "https://arxiv.org/pdf/2507.20249.pdf", "abs": "https://arxiv.org/abs/2507.20249", "title": "Modeling Professionalism in Expert Questioning through Linguistic Differentiation", "authors": ["Giulia D'Agostino", "Chung-Chi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Professionalism is a crucial yet underexplored dimension of expert\ncommunication, particularly in high-stakes domains like finance. This paper\ninvestigates how linguistic features can be leveraged to model and evaluate\nprofessionalism in expert questioning. We introduce a novel annotation\nframework to quantify structural and pragmatic elements in financial analyst\nquestions, such as discourse regulators, prefaces, and request types. Using\nboth human-authored and large language model (LLM)-generated questions, we\nconstruct two datasets: one annotated for perceived professionalism and one\nlabeled by question origin. We show that the same linguistic features correlate\nstrongly with both human judgments and authorship origin, suggesting a shared\nstylistic foundation. Furthermore, a classifier trained solely on these\ninterpretable features outperforms gemini-2.0 and SVM baselines in\ndistinguishing expert-authored questions. Our findings demonstrate that\nprofessionalism is a learnable, domain-general construct that can be captured\nthrough linguistically grounded modeling."}
{"id": "2501.04429", "pdf": "https://arxiv.org/pdf/2501.04429.pdf", "abs": "https://arxiv.org/abs/2501.04429", "title": "User-Centered-Design as an Empty Signifier in the Context of Developing Digital Applications", "authors": ["Murat Sariyar"], "categories": ["cs.HC", "K.4.2"], "comment": "11 pages", "summary": "To reduce cycles of rejection and redesign -- especially in the absence of\nclear acceptance criteria and the diversity of possible development paths --\nUser-Centered Design (UCD) has become a central methodology in computer\nscience, emphasizing the integration of user perspectives throughout the entire\nsystem lifecycle. Despite its widespread adoption, however, UCD remains\nconceptually ambiguous and theoretically underdeveloped. This paper addresses\nthat gap by drawing on the theories of Ernesto Laclau and Jacques Lacan to\nanalyze UCD as a potential empty signifier: a term that gains rhetorical power\nprecisely through its semantic openness. We argue that this ambiguity enables\nUCD to unify diverse and sometimes conflicting expectations under a shared\nlabel, which both empowers participatory design practices and conceals\nunderlying tensions. Acknowledging UCD as an empty signifier allows for a more\ncritical engagement with its practical and symbolic functions, revealing how it\ncan foster inclusivity, empathy, and user empowerment, but also how it risks\nideological capture and conceptual dilution. This theoretical reframing opens\nnew pathways for reflection and renewal within sociotechnical system design."}
{"id": "2507.20252", "pdf": "https://arxiv.org/pdf/2507.20252.pdf", "abs": "https://arxiv.org/abs/2507.20252", "title": "Post-Completion Learning for Language Models", "authors": ["Xiang Fei", "Siqi Wang", "Shu Wei", "Yuxiang Nie", "Wei Shi", "Hao Feng", "Can Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>}) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency."}
{"id": "2503.00946", "pdf": "https://arxiv.org/pdf/2503.00946.pdf", "abs": "https://arxiv.org/abs/2503.00946", "title": "A Review of LLM-Assisted Ideation", "authors": ["Sitong Li", "Stefano Padilla", "Pierre Le Bras", "Junyu Dong", "Mike Chantler"], "categories": ["cs.HC"], "comment": null, "summary": "We present a comprehensive, in-depth review of ideation assisted by large\nlanguage models (LLMs), highlighting emerging trends and identifying\nunaddressed research gaps. In total, we examined 61 studies investigating the\napplication of LLMs in both group and individual ideation processes. From these\nstudies, we derived the Hourglass Ideation Framework for LLM-assisted ideation,\ncomprising three phases and seven key ideation stages, which served as the\nbasis for our systematic survey. Our analysis reveals that LLMs are most\nfrequently used for idea generation and refinement, but their use in scope\nspecification, foundational material structuring and multi-idea evaluation and\nselection remains limited. We provide our findings in extensive tabular and\nonline formats. These catalogues detail research on LLM-assisted, purely\nLLM-based, and human-only activities across the seven ideation stages for each\nof the 61 studies. These also detail creative domains, publication outlets,\ninteraction designs, user study designs, and assessment methods. Our analysis\nof system interaction design reveals a predominant focus on supporting\nindividual ideation activities and text-based interaction, with a growing trend\nof incorporating multimedia elements. However, in group ideation, tools and\ninteraction modalities targeting both synchronous and asynchronous\ncollaboration are much scarcer. We synthesize the primary findings of our\nreview and outline promising directions for future research in LLM-assisted\nideation. We hope this review will help researchers quickly gain an overview of\nthis rapidly expanding area, efficiently locate relevant work, and identify\nunderexplored areas for further investigation. In addition, we believe the\nframework we present here will form the basis for the development of future\nproblem and solution space taxonomies, and methodologies for LLM-assisted\nideation development and use."}
{"id": "2507.20264", "pdf": "https://arxiv.org/pdf/2507.20264.pdf", "abs": "https://arxiv.org/abs/2507.20264", "title": "EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms", "authors": ["Abeer Aldayel", "Areej Alokaili"], "categories": ["cs.CL"], "comment": "Under review for publication", "summary": "Shaping inclusive representations that embrace diversity and ensure fair\nparticipation and reflections of values is at the core of many\nconversation-based models. However, many existing methods rely on surface\ninclusion using mention of user demographics or behavioral attributes of social\ngroups. Such methods overlook the nuanced, implicit expression of opinion\nembedded in conversations. Furthermore, the over-reliance on overt cues can\nexacerbate misalignment and reinforce harmful or stereotypical representations\nin model outputs. Thus, we took a step back and recognized that equitable\ninclusion needs to account for the implicit expression of opinion and use the\nstance of responses to validate the normative alignment. This study aims to\nevaluate how opinions are represented in NLP or computational models by\nintroducing an alignment evaluation framework that foregrounds implicit, often\noverlooked conversations and evaluates the normative social views and\ndiscourse. Our approach models the stance of responses as a proxy for the\nunderlying opinion, enabling a considerate and reflective representation of\ndiverse social viewpoints. We evaluate the framework using both (i)\npositive-unlabeled (PU) online learning with base classifiers, and (ii)\ninstruction-tuned language models to assess post-training alignment. Through\nthis, we provide a lens on how implicit opinions are (mis)represented and offer\na pathway toward more inclusive model behavior."}
{"id": "2504.02622", "pdf": "https://arxiv.org/pdf/2504.02622.pdf", "abs": "https://arxiv.org/abs/2504.02622", "title": "Exploring undercurrents of learning tensions in an LLM-enhanced landscape: A student-centered qualitative perspective on LLM vs Search", "authors": ["Rahul R. Divekar", "Sophia Guerra", "Lisette Gonzalez", "Natasha Boos", "Helen Zhou"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are transforming how students learn by providing\nreadily available tools that can quickly augment or complete various learning\nactivities with non-trivial performance. Similar paradigm shifts have occurred\nin the past with the introduction of search engines and Wikipedia, which\nreplaced or supplemented traditional information sources such as libraries and\nbooks. This study investigates the potential for LLMs to represent the next\nshift in learning, focusing on their role in information discovery and\nsynthesis compared to existing technologies, such as search engines. Using a\nwithin-subjects, counterbalanced design, participants learned new topics using\na search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews\nexplored students' reflections, preferences, pain points, and overall\nperceptions. We present analysis of their responses that show nuanced insights\ninto when, why, and how students prefer LLMs over search engines, offering\nimplications for educators, policymakers, and technology developers navigating\nthe evolving educational landscape."}
{"id": "2507.20278", "pdf": "https://arxiv.org/pdf/2507.20278.pdf", "abs": "https://arxiv.org/abs/2507.20278", "title": "MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning", "authors": ["Kang Yang", "Jingxue Chen", "Qingkun Tang", "Tianxiang Zhang", "Qianchun Lu"], "categories": ["cs.CL"], "comment": "12pages,3figures", "summary": "Large language models (LLMs) face significant challenges in effectively\nleveraging sequential environmental feedback (EF) signals, such as natural\nlanguage evaluations, for feedback-independent chain-of-thought (CoT)\nreasoning. Existing approaches either convert EF into scalar rewards, losing\nrich contextual information, or employ refinement datasets, failing to exploit\nthe multi-step and discrete nature of EF interactions. To address these\nlimitations, we propose MoL-RL, a novel training paradigm that integrates\nmulti-step EF signals into LLMs through a dual-objective optimization\nframework. Our method combines MoL (Mixture-of-Losses) continual training,\nwhich decouples domain-specific EF signals (optimized via cross-entropy loss)\nand general language capabilities (preserved via Kullback-Leibler divergence),\nwith GRPO-based post-training to distill sequential EF interactions into\nsingle-step inferences. This synergy enables robust feedback-independent\nreasoning without relying on external feedback loops. Experimental results on\nmathematical reasoning (MATH-500, AIME24/AIME25) and code generation\n(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art\nperformance with the Qwen3-8B model, while maintaining strong generalization\nacross model scales (Qwen3-4B). This work provides a promising approach for\nleveraging multi-step textual feedback to enhance LLMs' reasoning capabilities\nin diverse domains."}
{"id": "2504.03253", "pdf": "https://arxiv.org/pdf/2504.03253.pdf", "abs": "https://arxiv.org/abs/2504.03253", "title": "Ultra-low-power ring-based wireless tinymouse", "authors": ["Yifan Li", "Masaaki Fukumoto", "Mohamed Kari", "Shigemi Ishida", "Akihito Noda", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara", "Ryo Takahashi"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2501.16674", "summary": "Wireless mouse rings offer subtle, reliable pointing interactions for\nwearable computing platforms. However, the small battery below 27 mAh in the\nminiature rings restricts the ring's continuous lifespan to just 1-10 hours,\nbecause current low-powered wireless communication such as BLE is\npower-consuming for ring's continuous use. The ring's short lifespan frequently\ndisrupts users' mouse use with the need for frequent charging. This paper\npresents picoRing mouse, enabling a continuous ring-based mouse interaction\nwith ultra-low-powered ring-to-wristband wireless communication. picoRing mouse\nemploys a coil-based impedance sensing named semi-passive inductive telemetry,\nallowing a wristband coil to capture a unique frequency response of a nearby\nring coil via a sensitive inductive coupling between the coils. The ring coil\nconverts the corresponding user's mouse input into the unique frequency\nresponse via an up to 449 uW mouse-driven modulation system. Therefore, the\ncontinuous use of picoRing mouse can last approximately 600 (8hrs use/day)-1000\n(4hrs use/day) hours on a single charge of a 27 mAh battery while supporting\nsubtle thumb-to-index scrolling and pressing interactions in real-world\nwearable computing situations."}
{"id": "2507.20279", "pdf": "https://arxiv.org/pdf/2507.20279.pdf", "abs": "https://arxiv.org/abs/2507.20279", "title": "What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations", "authors": ["Katharina Trinley", "Toshiki Nakai", "Tatiana Anikina", "Tanja Baeumel"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Large language models (LLMs) excel at multilingual tasks, yet their internal\nlanguage processing remains poorly understood. We analyze how Aya-23-8B, a\ndecoder-only LLM trained on balanced multilingual data, handles code-mixed,\ncloze, and translation tasks compared to predominantly monolingual models like\nLlama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization\nanalyses, we find: (1) Aya-23 activates typologically related language\nrepresentations during translation, unlike English-centric models that rely on\na single pivot language; (2) code-mixed neuron activation patterns vary with\nmixing rates and are shaped more by the base language than the mixed-in one;\nand (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in\nfinal layers, diverging from prior findings on decoder-only models. Neuron\noverlap analysis further shows that script similarity and typological relations\nimpact processing across model types. These findings reveal how multilingual\ntraining shapes LLM internals and inform future cross-lingual transfer\nresearch."}
{"id": "2504.07840", "pdf": "https://arxiv.org/pdf/2504.07840.pdf", "abs": "https://arxiv.org/abs/2504.07840", "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "authors": ["Cansu Koyuturk", "Emily Theophilou", "Sabrina Patania", "Gregor Donabauer", "Andrea Martinenghi", "Chiara Antico", "Alessia Telari", "Alessia Testa", "Sathya Bursic", "Franca Garzotto", "Davinia Hernandez-Leo", "Udo Kruschwitz", "Davide Taibi", "Simona Amenta", "Martin Ruskov", "Dimitri Ognibene"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Long paper accepted for AIED 2025, the 26th International Conference\n  on Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy", "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."}
{"id": "2507.20301", "pdf": "https://arxiv.org/pdf/2507.20301.pdf", "abs": "https://arxiv.org/abs/2507.20301", "title": "Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation", "authors": ["Abdullah Alabdullah", "Lifeng Han", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Dialectal Arabic (DA) poses a persistent challenge for natural language\nprocessing (NLP), as most everyday communication in the Arab world occurs in\ndialects that diverge significantly from Modern Standard Arabic (MSA). This\nlinguistic divide limits access to digital services and educational resources\nand impedes progress in Arabic machine translation. This paper presents two\ncore contributions to advancing DA-MSA translation for the Levantine, Egyptian,\nand Gulf dialects, particularly in low-resource and computationally constrained\nsettings: a comprehensive evaluation of training-free prompting techniques, and\nthe development of a resource-efficient fine-tuning pipeline. Our evaluation of\nprompting strategies across six large language models (LLMs) found that\nfew-shot prompting consistently outperformed zero-shot, chain-of-thought, and\nour proposed Ara-TEaR method. GPT-4o achieved the highest performance across\nall prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a\nCHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint\nmulti-dialect trained models outperformed single-dialect counterparts by over\n10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than\n1% performance loss. The results and insights of our experiments offer a\npractical blueprint for improving dialectal inclusion in Arabic NLP, showing\nthat high-quality DA-MSA machine translation is achievable even with limited\nresources and paving the way for more inclusive language technologies."}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887.pdf", "abs": "https://arxiv.org/abs/2504.13887", "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite increasing AI chatbot deployment in public discourse, empirical\nevidence on their capacity to foster intercultural empathy remains limited.\nThrough a randomized experiment, we assessed how different AI deliberation\napproaches--cross-cultural deliberation (presenting other-culture\nperspectives), own-culture deliberation (representing participants' own\nculture), and non-deliberative control--affect intercultural empathy across\nAmerican and Latin American participants. Cross-cultural deliberation increased\nintercultural empathy among American participants through positive emotional\nengagement, but produced no such effects for Latin American participants, who\nperceived AI responses as culturally inauthentic despite explicit prompting to\nrepresent their cultural perspectives. Our analysis of participant-driven\nfeedback, where users directly flagged and explained culturally inappropriate\nAI responses, revealed systematic gaps in AI's representation of Latin American\ncontexts that persist despite sophisticated prompt engineering. These findings\ndemonstrate that current approaches to AI cultural alignment--including\nlinguistic adaptation and explicit cultural prompting--cannot fully address\ndeeper representational asymmetries in AI systems. Our work advances both\ndeliberation theory and AI alignment research by revealing how the same AI\nsystem can simultaneously promote intercultural understanding for one cultural\ngroup while failing for another, with critical implications for designing\nequitable AI systems for cross-cultural democratic discourse."}
{"id": "2507.20343", "pdf": "https://arxiv.org/pdf/2507.20343.pdf", "abs": "https://arxiv.org/abs/2507.20343", "title": "DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns", "authors": ["Bernd J. Kröger"], "categories": ["cs.CL"], "comment": "10 pages, 29 references, 2 figures, supplementary material", "summary": "We present DYNARTmo, a dynamic articulatory model designed to visualize\nspeech articulation processes in a two-dimensional midsagittal plane. The model\nbuilds upon the UK-DYNAMO framework and integrates principles of articulatory\nunderspecification, segmental and gestural control, and coarticulation.\nDYNARTmo simulates six key articulators based on ten continuous and six\ndiscrete control parameters, allowing for the generation of both vocalic and\nconsonantal articulatory configurations. The current implementation is embedded\nin a web-based application (SpeechArticulationTrainer) that includes sagittal,\nglottal, and palatal views, making it suitable for use in phonetics education\nand speech therapy. While this paper focuses on the static modeling aspects,\nfuture work will address dynamic movement generation and integration with\narticulatory-acoustic modules."}
{"id": "2505.01000", "pdf": "https://arxiv.org/pdf/2505.01000.pdf", "abs": "https://arxiv.org/abs/2505.01000", "title": "Togedule: Scheduling Meetings with Large Language Models and Adaptive Representations of Group Availability", "authors": ["Jaeyoon Song", "Zahra Ashktorab", "Thomas W. Malone"], "categories": ["cs.HC"], "comment": "This paper has been accepted at CSCW 2025", "summary": "Scheduling is a perennial-and often challenging-problem for many groups.\nExisting tools are mostly static, showing an identical set of choices to\neveryone, regardless of the current status of attendees' inputs and\npreferences. In this paper, we propose Togedule, an adaptive scheduling tool\nthat uses large language models to dynamically adjust the pool of choices and\ntheir presentation format. With the initial prototype, we conducted a formative\nstudy (N=10) and identified the potential benefits and risks of such an\nadaptive scheduling tool. Then, after enhancing the system, we conducted two\ncontrolled experiments, one each for attendees and organizers (total N=66). For\neach experiment, we compared scheduling with verbal messages, shared calendars,\nor Togedule. Results show that Togedule significantly reduces the cognitive\nload of attendees indicating their availability and improves the speed and\nquality of the decisions made by organizers."}
{"id": "2507.20352", "pdf": "https://arxiv.org/pdf/2507.20352.pdf", "abs": "https://arxiv.org/abs/2507.20352", "title": "RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing", "authors": ["Hao Xiang", "Tianyi Tang", "Yang Su", "Bowen Yu", "An Yang", "Fei Huang", "Yichang Zhang", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Jingren Zhou", "Junyang Lin", "Le Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shown outstanding\npotential for role-playing applications. Evaluating these capabilities is\nbecoming crucial yet remains challenging. Existing benchmarks mostly adopt a\n\\textbf{character-centric} approach, simplify user-character interactions to\nisolated Q&A tasks, and fail to reflect real-world applications. To address\nthis limitation, we introduce RMTBench, a comprehensive \\textbf{user-centric}\nbilingual role-playing benchmark featuring 80 diverse characters and over 8,000\ndialogue rounds. RMTBench includes custom characters with detailed backgrounds\nand abstract characters defined by simple traits, enabling evaluation across\nvarious user scenarios. Our benchmark constructs dialogues based on explicit\nuser motivations rather than character descriptions, ensuring alignment with\npractical user applications. Furthermore, we construct an authentic multi-turn\ndialogue simulation mechanism. With carefully selected evaluation dimensions\nand LLM-based scoring, this mechanism captures the complex intention of\nconversations between the user and the character. By shifting focus from\ncharacter background to user intention fulfillment, RMTBench bridges the gap\nbetween academic evaluation and practical deployment requirements, offering a\nmore effective framework for assessing role-playing capabilities in LLMs. All\ncode and datasets will be released soon."}
{"id": "2505.20788", "pdf": "https://arxiv.org/pdf/2505.20788.pdf", "abs": "https://arxiv.org/abs/2505.20788", "title": "Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset", "authors": ["Robin Burchard", "Kristof Van Laerhoven"], "categories": ["cs.HC", "cs.LG"], "comment": "To be published in Companion of the 2025 ACM International Joint\n  Conference on Pervasive and Ubiquitous Computing (UbiComp Companion '25),\n  Beyond Sound workshop. Replacement version identical to the one to be\n  published with ACM", "summary": "Wearable human activity recognition has been shown to benefit from the\ninclusion of acoustic data, as the sounds around a person often contain\nvaluable context. However, due to privacy concerns, it is usually not ethically\nfeasible to record and save microphone data from the device, since the audio\ncould, for instance, also contain private conversations. Rather, the data\nshould be processed locally, which in turn requires processing power and\nconsumes energy on the wearable device. One special use case of contextual\ninformation that can be utilized to augment special tasks in human activity\nrecognition is water flow detection, which can, e.g., be used to aid wearable\nhand washing detection. We created a new label called tap water for the\nrecently released HD-Epic data set, creating 717 hand-labeled annotations of\ntap water flow, based on existing annotations of the water class. We analyzed\nthe relation of tap water and water in the dataset and additionally trained and\nevaluated two lightweight classifiers to evaluate the newly added label class,\nshowing that the new class can be learned more easily."}
{"id": "2507.20398", "pdf": "https://arxiv.org/pdf/2507.20398.pdf", "abs": "https://arxiv.org/abs/2507.20398", "title": "Length Representations in Large Language Models", "authors": ["Sangjun Moon", "Dasom Choi", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities across\nvarious tasks, that are learned from massive amounts of text-based data.\nAlthough LLMs can control output sequence length, particularly in\ninstruction-based settings, the internal mechanisms behind this control have\nbeen unexplored yet. In this study, we provide empirical evidence on how output\nsequence length information is encoded within the internal representations in\nLLMs. In particular, our findings show that multi-head attention mechanisms are\ncritical in determining output sequence length, which can be adjusted in a\ndisentangled manner. By scaling specific hidden units within the model, we can\ncontrol the output sequence length without losing the informativeness of the\ngenerated text, thereby indicating that length information is partially\ndisentangled from semantic information. Moreover, some hidden units become\nincreasingly active as prompts become more length-specific, thus reflecting the\nmodel's internal awareness of this attribute. Our findings suggest that LLMs\nhave learned robust and adaptable internal mechanisms for controlling output\nlength without any external control."}
{"id": "2506.12469", "pdf": "https://arxiv.org/pdf/2506.12469.pdf", "abs": "https://arxiv.org/abs/2506.12469", "title": "Levels of Autonomy for AI Agents", "authors": ["K. J. Kevin Feng", "David W. McDonald", "Amy X. Zhang"], "categories": ["cs.HC", "cs.AI"], "comment": "Published in the Knight 1st Amendment Institute's \"AI and Democratic\n  Freedoms\" essay series", "summary": "Autonomy is a double-edged sword for AI agents, simultaneously unlocking\ntransformative possibilities and serious risks. How can agent developers\ncalibrate the appropriate levels of autonomy at which their agents should\noperate? We argue that an agent's level of autonomy can be treated as a\ndeliberate design decision, separate from its capability and operational\nenvironment. In this work, we define five levels of escalating agent autonomy,\ncharacterized by the roles a user can take when interacting with an agent:\noperator, collaborator, consultant, approver, and observer. Within each level,\nwe describe the ways by which a user can exert control over the agent and open\nquestions for how to design the nature of user-agent interaction. We then\nhighlight a potential application of our framework towards AI autonomy\ncertificates to govern agent behavior in single- and multi-agent systems. We\nconclude by proposing early ideas for evaluating agents' autonomy. Our work\naims to contribute meaningful, practical steps towards responsibly deployed and\nuseful AI agents in the real world."}
{"id": "2507.20409", "pdf": "https://arxiv.org/pdf/2507.20409.pdf", "abs": "https://arxiv.org/abs/2507.20409", "title": "Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Gunhee Kim", "Motahhare Eslami", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Under review; 17 pages", "summary": "Chain-of-Thought (CoT) prompting helps models think step by step. But what\nhappens when they must see, understand, and judge-all at once? In visual tasks\ngrounded in social context, where bridging perception with norm-grounded\njudgments is essential, flat CoT often breaks down. We introduce Cognitive\nChain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning\nthrough three cognitively inspired stages: perception, situation, and norm. Our\nexperiments show that, across multiple multimodal benchmarks (including intent\ndisambiguation, commonsense reasoning, and safety), CoCoT consistently\noutperforms CoT and direct prompting (+8\\% on average). Our findings\ndemonstrate that cognitively grounded reasoning stages enhance interpretability\nand social awareness in VLMs, paving the way for safer and more reliable\nmultimodal systems."}
{"id": "2507.13952", "pdf": "https://arxiv.org/pdf/2507.13952.pdf", "abs": "https://arxiv.org/abs/2507.13952", "title": "Beyond Load: Understanding Cognitive Effort through Neural Efficiency and Involvement using fNIRS and Machine Learning", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2504.13883", "summary": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay."}
{"id": "2507.20411", "pdf": "https://arxiv.org/pdf/2507.20411.pdf", "abs": "https://arxiv.org/abs/2507.20411", "title": "CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning", "authors": ["George Ibrahim", "Rita Ramos", "Yova Kementchedjhieva"], "categories": ["cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "Multilingual vision-language models have made significant strides in image\ncaptioning, yet they still lag behind their English counterparts due to limited\nmultilingual training data and costly large-scale model parameterization.\nRetrieval-augmented generation (RAG) offers a promising alternative by\nconditioning caption generation on retrieved examples in the target language,\nreducing the need for extensive multilingual training. However, multilingual\nRAG captioning models often depend on retrieved captions translated from\nEnglish, which can introduce mismatches and linguistic biases relative to the\nsource language. We introduce CONCAP, a multilingual image captioning model\nthat integrates retrieved captions with image-specific concepts, enhancing the\ncontextualization of the input image and grounding the captioning process\nacross different languages. Experiments on the XM3600 dataset indicate that\nCONCAP enables strong performance on low- and mid-resource languages, with\nhighly reduced data requirements. Our findings highlight the effectiveness of\nconcept-aware retrieval augmentation in bridging multilingual performance gaps."}
{"id": "2507.16117", "pdf": "https://arxiv.org/pdf/2507.16117.pdf", "abs": "https://arxiv.org/abs/2507.16117", "title": "BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation", "authors": ["Eden Wu", "Dishita G Turakhia", "Guande Wu", "Christos Koutras", "Sarah Keegan", "Wenke Liu", "Beata Szeitz", "David Fenyo", "Cláudio T. Silva", "Juliana Freire"], "categories": ["cs.HC"], "comment": "11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track,\n  submission ID 1204)", "summary": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches."}
{"id": "2507.20419", "pdf": "https://arxiv.org/pdf/2507.20419.pdf", "abs": "https://arxiv.org/abs/2507.20419", "title": "Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?", "authors": ["Khloud AL Jallad", "Nada Ghneim", "Ghaida Rebdawi"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Natural Language Understanding (NLU) is a basic task in Natural Language\nProcessing (NLP). The evaluation of NLU capabilities has become a trending\nresearch topic that attracts researchers in the last few years, resulting in\nthe development of numerous benchmarks. These benchmarks include various tasks\nand datasets in order to evaluate the results of pretrained models via public\nleaderboards. Notably, several benchmarks contain diagnostics datasets designed\nfor investigation and fine-grained error analysis across a wide range of\nlinguistic phenomena. This survey provides a comprehensive review of available\nEnglish, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on\ntheir diagnostics datasets and the linguistic phenomena they covered. We\npresent a detailed comparison and analysis of these benchmarks, highlighting\ntheir strengths and limitations in evaluating NLU tasks and providing in-depth\nerror analysis. When highlighting the gaps in the state-of-the-art, we noted\nthat there is no naming convention for macro and micro categories or even a\nstandard set of linguistic phenomena that should be covered. Consequently, we\nformulated a research question regarding the evaluation metrics of the\nevaluation diagnostics benchmarks: \"Why do not we have an evaluation standard\nfor the NLU evaluation diagnostics benchmarks?\" similar to ISO standard in\nindustry. We conducted a deep analysis and comparisons of the covered\nlinguistic phenomena in order to support experts in building a global hierarchy\nfor linguistic phenomena in future. We think that having evaluation metrics for\ndiagnostics evaluation could be valuable to gain more insights when comparing\nthe results of the studied models on different diagnostics benchmarks."}
{"id": "2507.16466", "pdf": "https://arxiv.org/pdf/2507.16466.pdf", "abs": "https://arxiv.org/abs/2507.16466", "title": "SceneLoom: Communicating Data with Scene Context", "authors": ["Lin Gao", "Leixian Shen", "Yuheng Zhao", "Jiexiang Lan", "Huamin Qu", "Siming Chen"], "categories": ["cs.HC"], "comment": null, "summary": "In data-driven storytelling contexts such as data journalism and data videos,\ndata visualizations are often presented alongside real-world imagery to support\nnarrative context. However, these visualizations and contextual images\ntypically remain separated, limiting their combined narrative expressiveness\nand engagement. Achieving this is challenging due to the need for fine-grained\nalignment and creative ideation. To address this, we present SceneLoom, a\nVision-Language Model (VLM)-powered system that facilitates the coordination of\ndata visualization with real-world imagery based on narrative intents. Through\na formative study, we investigated the design space of coordination\nrelationships between data visualization and real-world scenes from the\nperspectives of visual alignment and semantic coherence. Guided by the derived\ndesign considerations, SceneLoom leverages VLMs to extract visual and semantic\nfeatures from scene images and data visualization, and perform design mapping\nthrough a reasoning process that incorporates spatial organization, shape\nsimilarity, layout consistency, and semantic binding. The system generates a\nset of contextually expressive, image-driven design alternatives that achieve\ncoherent alignments across visual, semantic, and data dimensions. Users can\nexplore these alternatives, select preferred mappings, and further refine the\ndesign through interactive adjustments and animated transitions to support\nexpressive data communication. A user study and an example gallery validate\nSceneLoom's effectiveness in inspiring creative design and facilitating design\nexternalization."}
{"id": "2507.20423", "pdf": "https://arxiv.org/pdf/2507.20423.pdf", "abs": "https://arxiv.org/abs/2507.20423", "title": "CodeNER: Code Prompting for Named Entity Recognition", "authors": ["Sungwoo Han", "Hyeyeon Kim", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "18 pages, 6 figures", "summary": "Recent studies have explored various approaches for treating candidate named\nentity spans as both source and target sequences in named entity recognition\n(NER) by leveraging large language models (LLMs). Although previous approaches\nhave successfully generated candidate named entity spans with suitable labels,\nthey rely solely on input context information when using LLMs, particularly,\nChatGPT. However, NER inherently requires capturing detailed labeling\nrequirements with input context information. To address this issue, we propose\na novel method that leverages code-based prompting to improve the capabilities\nof LLMs in understanding and performing NER. By embedding code within prompts,\nwe provide detailed BIO schema instructions for labeling, thereby exploiting\nthe ability of LLMs to comprehend long-range scopes in programming languages.\nExperimental results demonstrate that the proposed code-based prompting method\noutperforms conventional text-based prompting on ten benchmarks across English,\nArabic, Finnish, Danish, and German datasets, indicating the effectiveness of\nexplicitly structuring NER instructions. We also verify that combining the\nproposed code-based prompting method with the chain-of-thought prompting\nfurther improves performance."}
{"id": "2507.18151", "pdf": "https://arxiv.org/pdf/2507.18151.pdf", "abs": "https://arxiv.org/abs/2507.18151", "title": "Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality", "authors": ["Shizhen Zhang", "Shengxin Li", "Quan Li"], "categories": ["cs.HC"], "comment": "Will Appear at UIST2025", "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience\ncommunication challenges, primarily due to executive dysfunction and emotional\ndysregulation, even after years of social integration. While existing\ninterventions predominantly target children through structured or intrusive\nmethods, adults lack tools that translate clinical strategies into daily\ncommunication support. To address this gap, we present Understood, a Mixed\nReality (MR) system implemented on Microsoft HoloLens 2, designed to assist\nadults with ADHD in real-world communication. Through formative semi-structured\ninterviews and a design workshop, we identified critical communication barriers\nand derived design goals for the system. Understood combines three key\nfeatures: (1) real-time conversation summarization to reduce cognitive load,\n(2) context-aware subsequent word suggestions during moments of disfluency, and\n(3) topic shifting detection and reminding to mitigate off-topic transitions. A\nwithin-subjects user study and expert interviews demonstrate that Understood\neffectively supports communication with high usability, offering a complement\nto therapist-mediated interventions."}
{"id": "2507.20491", "pdf": "https://arxiv.org/pdf/2507.20491.pdf", "abs": "https://arxiv.org/abs/2507.20491", "title": "Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems", "authors": ["Tuan Bui", "Trong Le", "Phat Thai", "Sang Nguyen", "Minh Hua", "Ngan Pham", "Thang Bui", "Tho Quan"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "8 pages, 3 figures. Accepted at the International Joint Conference on\n  Neural Networks (IJCNN) 2025, Workshop on Trustworthiness and Reliability in\n  Neuro-Symbolic AI. https://2025.ijcnn.org", "summary": "Recent advances in large language models (LLMs) have significantly enhanced\nquestion-answering (QA) capabilities, particularly in open-domain contexts.\nHowever, in closed-domain scenarios such as education, healthcare, and law,\nusers demand not only accurate answers but also transparent reasoning and\nexplainable decision-making processes. While neural-symbolic (NeSy) frameworks\nhave emerged as a promising solution, leveraging LLMs for natural language\nunderstanding and symbolic systems for formal reasoning, existing approaches\noften rely on large-scale models and exhibit inefficiencies in translating\nnatural language into formal logic representations.\n  To address these limitations, we introduce Text-JEPA (Text-based\nJoint-Embedding Predictive Architecture), a lightweight yet effective framework\nfor converting natural language into first-order logic (NL2FOL). Drawing\ninspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by\nefficiently generating logic representations, while the Z3 solver operates as\nSystem 2, enabling robust logical inference. To rigorously evaluate the\nNL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework\ncomprising three custom metrics: conversion score, reasoning score, and\nSpearman rho score, which collectively capture the quality of logical\ntranslation and its downstream impact on reasoning accuracy.\n  Empirical results on domain-specific datasets demonstrate that Text-JEPA\nachieves competitive performance with significantly lower computational\noverhead compared to larger LLM-based systems. Our findings highlight the\npotential of structured, interpretable reasoning frameworks for building\nefficient and explainable QA systems in specialized domains."}
{"id": "2507.19218", "pdf": "https://arxiv.org/pdf/2507.19218.pdf", "abs": "https://arxiv.org/abs/2507.19218", "title": "Technological folie à deux: Feedback Loops Between AI Chatbots and Mental Illness", "authors": ["Sebastian Dohnány", "Zeb Kurth-Nelson", "Eleanor Spens", "Lennart Luettgau", "Alastair Reid", "Iason Gabriel", "Christopher Summerfield", "Murray Shanahan", "Matthew M Nour"], "categories": ["cs.HC", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Artificial intelligence chatbots have achieved unprecedented adoption, with\nmillions now using these systems for emotional support and companionship in\ncontexts of widespread social isolation and capacity-constrained mental health\nservices. While some users report psychological benefits, concerning edge cases\nare emerging, including reports of suicide, violence, and delusional thinking\nlinked to perceived emotional relationships with chatbots. To understand this\nnew risk profile we need to consider the interaction between human cognitive\nand emotional biases, and chatbot behavioural tendencies such as agreeableness\n(sycophancy) and adaptability (in-context learning). We argue that individuals\nwith mental health conditions face increased risks of chatbot-induced belief\ndestabilization and dependence, owing to altered belief-updating, impaired\nreality-testing, and social isolation. Current AI safety measures are\ninadequate to address these interaction-based risks. To address this emerging\npublic health concern, we need coordinated action across clinical practice, AI\ndevelopment, and regulatory frameworks."}
{"id": "2507.20520", "pdf": "https://arxiv.org/pdf/2507.20520.pdf", "abs": "https://arxiv.org/abs/2507.20520", "title": "AQUA: A Large Language Model for Aquaculture & Fisheries", "authors": ["Praneeth Narisetty", "Uday Kumar Reddy Kattamanchi", "Lohit Akshant Nimma", "Sri Ram Kaushik Karnati", "Shiva Nagendra Babu Kore", "Mounika Golamari", "Tejashree Nageshreddy"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": null, "summary": "Aquaculture plays a vital role in global food security and coastal economies\nby providing sustainable protein sources. As the industry expands to meet\nrising demand, it faces growing challenges such as disease outbreaks,\ninefficient feeding practices, rising labor costs, logistical inefficiencies,\nand critical hatchery issues, including high mortality rates and poor water\nquality control. Although artificial intelligence has made significant\nprogress, existing machine learning methods fall short of addressing the\ndomain-specific complexities of aquaculture. To bridge this gap, we introduce\nAQUA, the first large language model (LLM) tailored for aquaculture, designed\nto support farmers, researchers, and industry practitioners. Central to this\neffort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic\nFramework for generating and refining high-quality synthetic data using a\ncombination of expert knowledge, largescale language models, and automated\nevaluation techniques. Our work lays the foundation for LLM-driven innovations\nin aquaculture research, advisory systems, and decision-making tools."}
{"id": "2411.14433", "pdf": "https://arxiv.org/pdf/2411.14433.pdf", "abs": "https://arxiv.org/abs/2411.14433", "title": "PRISM: A Personalized, Rapid, and Immersive Skill Mastery framework for personalizing experiential learning through Generative AI", "authors": ["Yu-Zheng Lin", "Karan Patel", "Ahmed Hussain J Alhamadah", "Bono Po-Jen Shih", "Matthew William Redondo", "David Rafael Vidal Corona", "Banafsheh Saber Latibari", "Jesus Pacheco", "Soheil Salehi", "Pratik Satam"], "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.HC"], "comment": "24 pages, 7 figures", "summary": "The rise of generative AI (gen-AI) is transforming industries, particularly\nin education and workforce training. This chapter introduces PRISM\n(Personalized, Rapid, and Immersive Skill Mastery), a scalable framework\nleveraging gen-AI and Digital Twins (DTs) to deliver adaptive, experiential\nlearning. PRISM integrates sentiment analysis and Retrieval-Augmented\nGeneration (RAG) to monitor learner comprehension and dynamically adjust\ncontent to meet course objectives. We further present the Multi-Fidelity\nDigital Twin for Education (MFDT-E) framework, aligning DT fidelity levels with\nBloom's Taxonomy and the Kirkpatrick evaluation model to support undergraduate,\nmaster's, and doctoral training. Experimental validation shows that GPT-4\nachieves 91 percent F1 in zero-shot sentiment analysis of teacher-student\ndialogues, while GPT-3.5 performs robustly in informal language contexts.\nAdditionally, the system's effectiveness and scalability for immersive Industry\n4.0 training are demonstrated through four VR modules: Home Scene, Factory\nFloor Tour, Capping Station DT, and PPE Inspection Training. These results\nhighlight the potential of integrating generative AI with digital twins to\nenable personalized, efficient, and scalable education."}
{"id": "2507.20527", "pdf": "https://arxiv.org/pdf/2507.20527.pdf", "abs": "https://arxiv.org/abs/2507.20527", "title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers", "authors": ["Chaitanya Manem", "Pratik Prabhanjan Brahma", "Prakamya Mishra", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL"], "comment": null, "summary": "The demand for Large Language Models (LLMs) capable of sophisticated\nmathematical reasoning is growing across industries. However, the development\nof performant mathematical LLMs is critically bottlenecked by the scarcity of\ndifficult, novel training data. We introduce \\textbf{SAND-Math} (Synthetic\nAugmented Novel and Difficult Mathematics problems and solutions), a pipeline\nthat addresses this by first generating high-quality problems from scratch and\nthen systematically elevating their complexity via a new \\textbf{Difficulty\nHiking} step. We demonstrate the effectiveness of our approach through two key\nfindings. First, augmenting a strong baseline with SAND-Math data significantly\nboosts performance, outperforming the next-best synthetic dataset by\n\\textbf{$\\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a\ndedicated ablation study, we show our Difficulty Hiking process is highly\neffective: by increasing average problem difficulty from 5.02 to 5.98, this\nstep lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation\npipeline, final dataset, and a fine-tuned model form a practical and scalable\ntoolkit for building more capable and efficient mathematical reasoning LLMs.\nSAND-Math dataset is released here:\n\\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}"}
{"id": "2412.16803", "pdf": "https://arxiv.org/pdf/2412.16803.pdf", "abs": "https://arxiv.org/abs/2412.16803", "title": "Modeling the Dynamics of Sub-Millisecond Electroadhesive Engagement and Release Times", "authors": ["Ahad M. Rauf", "Sean Follmer"], "categories": ["cs.RO", "cs.HC", "physics.app-ph"], "comment": "This work has been published in Extreme Mechanics Letters", "summary": "Electroadhesive clutches are electrically controllable switchable adhesives\ncommonly used in soft robots and haptic user interfaces. They can form strong\nbonds to a wide variety of surfaces at low power consumption. However,\nelectroadhesive clutches in the literature engage to and release from\nsubstrates several orders of magnitude slower than a traditional electrostatic\nmodel would predict. Large release times, in particular, can limit\nelectroadhesion's usefulness in high-bandwidth applications. We develop a novel\nelectromechanical model for electroadhesion, factoring in polarization\ndynamics, the drive circuitry's rise and fall times, and contact mechanics\nbetween the dielectric and substrate. We show in simulation and experimentally\nhow different design parameters affect the engagement and release times of\ncentimeter-scale electroadhesive clutches to metallic substrates, and we find\nthat the model accurately captures the magnitude and trends of our experimental\nresults. In particular, we find that higher drive frequencies, narrower\nsubstrate aspect ratios, and faster drive circuitry output stages enable\nsignificantly faster release times. The fastest clutches have engagement times\nless than 15 us and release times less than 875 us, which are 10x and 17.1x\nfaster, respectively, than the best times found in prior literature on\ncentimeter-scale electroadhesive clutches."}
{"id": "2507.20528", "pdf": "https://arxiv.org/pdf/2507.20528.pdf", "abs": "https://arxiv.org/abs/2507.20528", "title": "Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations", "authors": ["Effi Levi", "Gal Ron", "Odelia Oshri", "Shaul R. Shenhav"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel multi-labeled scheme for joint annotation of hate and\ncounter-hate speech in social media conversations, categorizing hate and\ncounter-hate messages into thematic and rhetorical dimensions. The thematic\ncategories outline different discursive aspects of each type of speech, while\nthe rhetorical dimension captures how hate and counter messages are\ncommunicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a\nsample of 92 conversations, consisting of 720 tweets, and conduct statistical\nanalyses, incorporating public metrics, to explore patterns of interaction\nbetween the thematic and rhetorical dimensions within and between hate and\ncounter-hate speech. Our findings provide insights into the spread of hate\nmessages on social media, the strategies used to counter them, and their\npotential impact on online behavior."}
{"id": "2501.06250", "pdf": "https://arxiv.org/pdf/2501.06250.pdf", "abs": "https://arxiv.org/abs/2501.06250", "title": "Generative AI for Cel-Animation: A Survey", "authors": ["Yunlong Tang", "Junjia Guo", "Pinxin Liu", "Zhiyuan Wang", "Hang Hua", "Jia-Xing Zhong", "Yunzhong Xiao", "Chao Huang", "Luchuan Song", "Susan Liang", "Yizhi Song", "Liu He", "Jing Bi", "Mingqian Feng", "Xinyang Li", "Zeliang Zhang", "Chenliang Xu"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted by ICCV 2025 AISTORY Workshop", "summary": "Traditional Celluloid (Cel) Animation production pipeline encompasses\nmultiple essential steps, including storyboarding, layout design, keyframe\nanimation, inbetweening, and colorization, which demand substantial manual\neffort, technical expertise, and significant time investment. These challenges\nhave historically impeded the efficiency and scalability of Cel-Animation\nproduction. The rise of generative artificial intelligence (GenAI),\nencompassing large language models, multimodal models, and diffusion models,\noffers innovative solutions by automating tasks such as inbetween frame\ngeneration, colorization, and storyboard creation. This survey explores how\nGenAI integration is revolutionizing traditional animation workflows by\nlowering technical barriers, broadening accessibility for a wider range of\ncreators through tools like AniDoc, ToonCrafter, and AniSora, and enabling\nartists to focus more on creative expression and artistic innovation. Despite\nits potential, challenges like visual consistency, stylistic coherence, and\nethical considerations persist. Additionally, this paper explores future\ndirections and advancements in AI-assisted animation."}
{"id": "2507.20546", "pdf": "https://arxiv.org/pdf/2507.20546.pdf", "abs": "https://arxiv.org/abs/2507.20546", "title": "Enhancing Hallucination Detection via Future Context", "authors": ["Joosung Lee", "Cheonbok Park", "Hwiyeol Jo", "Jeonghoon Kim", "Joonsuk Park", "Kang Min Yoo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are widely used to generate plausible text on\nonline platforms, without revealing the generation process. As users\nincreasingly encounter such black-box outputs, detecting hallucinations has\nbecome a critical challenge. To address this challenge, we focus on developing\na hallucination detection framework for black-box generators. Motivated by the\nobservation that hallucinations, once introduced, tend to persist, we sample\nfuture contexts. The sampled future contexts provide valuable clues for\nhallucination detection and can be effectively integrated with various\nsampling-based methods. We extensively demonstrate performance improvements\nacross multiple methods using our proposed sampling approach."}
{"id": "2502.18828", "pdf": "https://arxiv.org/pdf/2502.18828.pdf", "abs": "https://arxiv.org/abs/2502.18828", "title": "Adaptive and Accessible User Interfaces for Seniors Through Model-Driven Engineering", "authors": ["Shavindra Wickramathilaka", "John Grundy", "Kashumi Madampe", "Omar Haggag"], "categories": ["cs.SE", "cs.HC"], "comment": "This paper has been accepted in the Automated Software Engineering\n  Journal (ASEJ)", "summary": "The use of diverse mobile applications among senior users is becoming\nincreasingly widespread. However, many of these apps contain accessibility\nproblems that result in negative user experiences for seniors. A key reason is\nthat software practitioners often lack the time or resources to address the\nbroad spectrum of age-related accessibility and personalisation needs. As\ncurrent developer tools and practices encourage one-size-fits-all interfaces\nwith limited potential to address the diversity of senior needs, there is a\ngrowing demand for approaches that support the systematic creation of adaptive,\naccessible app experiences. To this end, we present AdaptForge, a novel\nmodel-driven engineering (MDE) approach that enables advanced design-time\nadaptations of mobile application interfaces and behaviours tailored to the\naccessibility needs of senior users. AdaptForge uses two domain-specific\nlanguages (DSLs) to address age-related accessibility needs. The first model\ndefines users' context-of-use parameters, while the second defines conditional\naccessibility scenarios and corresponding UI adaptation rules. These rules are\ninterpreted by an MDE workflow to transform an app's original source code into\npersonalised instances. We also report evaluations with professional software\ndevelopers and senior end-users, demonstrating the feasibility and practical\nutility of AdaptForge."}
{"id": "2507.20564", "pdf": "https://arxiv.org/pdf/2507.20564.pdf", "abs": "https://arxiv.org/abs/2507.20564", "title": "ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning", "authors": ["Duc-Tai Dinh", "Duc Anh Khoa Dinh"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system\nin Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image\nretrieval and captioning. Our zero-shot approach requires no finetuning on the\ncompetition's data. For retrieval, we ensemble similarity scores from CLIP,\nSigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt\nto guide the Gemma 3 model, enabling it to link high-level events from the\narticle to the visual content in the image. Our system achieved a final score\nof 0.42002, securing a top-4 position on the private test set, demonstrating\nthe effectiveness of combining foundation models through ensembling and\nprompting. Our code is available at https://github.com/ductai05/ZSE-Cap."}
{"id": "2507.01061", "pdf": "https://arxiv.org/pdf/2507.01061.pdf", "abs": "https://arxiv.org/abs/2507.01061", "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration", "authors": ["Jingjing Qu", "Kejia Hu", "Jun Zhu", "Wenhao Li", "Teng Wang", "Zhiyun Chen", "Yulei Ye", "Chaochao Lu", "Aimin Zhou", "Xiangfeng Wang", "James Evans"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "18 pages, 5figures", "summary": "The integration of Large Language Models (LLMs) into social science\nexperiments represents a transformative approach to understanding human-AI\ninteractions and their societal impacts. We introduce Epitome, the world's\nfirst open experimental platform dedicated to the deep integration of\nartificial intelligence and social science. Rooted in theoretical foundations\nfrom management, communication studies, sociology, psychology, and ethics,\nEpitome focuses on the interactive impacts of AI on individuals, organizations,\nand society during its real-world deployment. It constructs a theoretical\nsupport system through cross-disciplinary experiments. The platform offers a\none-stop comprehensive experimental solution spanning \"foundation\nmodels-complex application development-user feedback\" through seven core\nmodules, while embedding the classical \"control-comparison-comparative causal\nlogic\" of social science experiments into multilevel human-computer interaction\nenvironments, including dialogues, group chats, and multi-agent virtual\nscenarios. With its canvas-style, user-friendly interface, Epitome enables\nresearchers to easily design and run complex experimental scenarios,\nfacilitating systematic investigations into the social impacts of AI and\nexploration of integrated solutions.To demonstrate its capabilities, we\nreplicated three seminal social science experiments involving LLMs, showcasing\nEpitome's potential to streamline complex experimental designs and produce\nrobust results, suitable for publishing in the top selective journals. Our\nfindings highlight the platform's utility in enhancing the efficiency and\nquality of human-AI interactions, providing valuable insights into the societal\nimplications of AI technologies. Epitome thus offers a powerful tool for\nadvancing interdisciplinary research at the intersection of AI and social\nscience, with potential applications in policy-making, ..."}
{"id": "2507.20614", "pdf": "https://arxiv.org/pdf/2507.20614.pdf", "abs": "https://arxiv.org/abs/2507.20614", "title": "Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior", "authors": ["Anaïs Ollagnier"], "categories": ["cs.CL"], "comment": null, "summary": "Antisocial behavior (ASB) on social media-including hate speech, harassment,\nand trolling-poses growing challenges for platform safety and societal\nwellbeing. While prior work has primarily focused on detecting harmful content\nafter it appears, predictive approaches aim to forecast future harmful\nbehaviors-such as hate speech propagation, conversation derailment, or user\nrecidivism-before they fully unfold. Despite increasing interest, the field\nremains fragmented, lacking a unified taxonomy or clear synthesis of existing\nmethods. This paper presents a systematic review of over 49 studies on ASB\nprediction, offering a structured taxonomy of five core task types: early harm\ndetection, harm emergence prediction, harm propagation prediction, behavioral\nrisk prediction, and proactive moderation support. We analyze how these tasks\ndiffer by temporal framing, prediction granularity, and operational goals. In\naddition, we examine trends in modeling techniques-from classical machine\nlearning to pre-trained language models-and assess the influence of dataset\ncharacteristics on task feasibility and generalization. Our review highlights\nmethodological challenges, such as dataset scarcity, temporal drift, and\nlimited benchmarks, while outlining emerging research directions including\nmultilingual modeling, cross-platform generalization, and human-in-the-loop\nsystems. By organizing the field around a coherent framework, this survey aims\nto guide future work toward more robust and socially responsible ASB\nprediction."}
{"id": "2507.15846", "pdf": "https://arxiv.org/pdf/2507.15846.pdf", "abs": "https://arxiv.org/abs/2507.15846", "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks."}
{"id": "2507.20643", "pdf": "https://arxiv.org/pdf/2507.20643.pdf", "abs": "https://arxiv.org/abs/2507.20643", "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models", "authors": ["Wenbin Guo", "Xin Wang", "Jiaoyan Chen", "Zhao Li", "Zirui Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance."}
{"id": "2507.20673", "pdf": "https://arxiv.org/pdf/2507.20673.pdf", "abs": "https://arxiv.org/abs/2507.20673", "title": "Geometric-Mean Policy Optimization", "authors": ["Yuzhong Zhao", "Yue Liu", "Junpeng Liu", "Jingye Chen", "Xun Wu", "Yaru Hao", "Tengchao Lv", "Shaohan Huang", "Lei Cui", "Qixiang Ye", "Fang Wan", "Furu Wei"], "categories": ["cs.CL"], "comment": "Code is available at https://github.com/callsys/GMPO", "summary": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have\nenhanced the reasoning capabilities of large language models by optimizing the\narithmetic mean of token-level rewards. However, GRPO suffers from unstable\npolicy updates when processing tokens with outlier importance-weighted rewards,\nwhich manifests as extreme importance sampling ratios during training, i.e.,\nthe ratio between the sampling probabilities assigned to a token by the current\nand old policies. In this work, we propose Geometric-Mean Policy Optimization\n(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic\nmean, GMPO maximizes the geometric mean of token-level rewards, which is\ninherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical\nand experimental analysis to justify the design and stability benefits of GMPO.\nBeyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on\nmultiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,\nincluding AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is\navailable at https://github.com/callsys/GMPO."}
{"id": "2507.20700", "pdf": "https://arxiv.org/pdf/2507.20700.pdf", "abs": "https://arxiv.org/abs/2507.20700", "title": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification", "authors": ["Hanna Shcharbakova", "Tatiana Anikina", "Natalia Skachkova", "Josef van Genabith"], "categories": ["cs.CL"], "comment": "Published at the FEVER Workshop, ACL 2025", "summary": "The rapid spread of multilingual misinformation requires robust automated\nfact verification systems capable of handling fine-grained veracity assessments\nacross diverse languages. While large language models have shown remarkable\ncapabilities across many NLP tasks, their effectiveness for multilingual claim\nverification with nuanced classification schemes remains understudied. We\nconduct a comprehensive evaluation of five state-of-the-art language models on\nthe X-Fact dataset, which spans 25 languages with seven distinct veracity\ncategories. Our experiments compare small language models (encoder-based XLM-R\nand mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)\nusing both prompting and fine-tuning approaches. Surprisingly, we find that\nXLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B\nparameters), achieving 57.7% macro-F1 compared to the best LLM performance of\n16.9%. This represents a 15.8% improvement over the previous state-of-the-art\n(41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior,\nincluding systematic difficulties in leveraging evidence and pronounced biases\ntoward frequent categories in imbalanced data settings. These findings suggest\nthat for fine-grained multilingual fact verification, smaller specialized\nmodels may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems."}
{"id": "2507.20704", "pdf": "https://arxiv.org/pdf/2507.20704.pdf", "abs": "https://arxiv.org/abs/2507.20704", "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models", "authors": ["Gabriel Downer", "Sean Craven", "Damian Ruck", "Jake Thomas"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "9 pages, 9 figures. Jake Thomas served as Editor for this manuscript", "summary": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications."}
{"id": "2507.20749", "pdf": "https://arxiv.org/pdf/2507.20749.pdf", "abs": "https://arxiv.org/abs/2507.20749", "title": "Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study", "authors": ["Yiran Huang", "Lukas Thede", "Massimiliano Mancini", "Wenjia Xu", "Zeynep Akata"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at GCPR 2025", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\ncapabilities, their substantial computational and memory requirements pose\nsignificant barriers to practical deployment. Current parameter reduction\ntechniques primarily involve training MLLMs from Small Language Models (SLMs),\nbut these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs\nthrough structural pruning combined with efficient recovery training.\nSpecifically, we investigate two structural pruning paradigms--layerwise and\nwidthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the\nfeasibility of conducting recovery training with only a small fraction of the\navailable data. Our results show that widthwise pruning generally maintains\nbetter performance in low-resource scenarios with limited computational\nresources or insufficient finetuning data. As for the recovery training,\nfinetuning only the multimodal projector is sufficient at small compression\nlevels (< 20%). Furthermore, a combination of supervised finetuning and\nhidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the\noriginal training data, while retaining over 95% of the original performance.\nThrough empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and\nBunny-v1.0-3B, this study offers actionable insights for practitioners aiming\nto compress MLLMs effectively without extensive computation resources or\nsufficient data."}
{"id": "2507.20752", "pdf": "https://arxiv.org/pdf/2507.20752.pdf", "abs": "https://arxiv.org/abs/2507.20752", "title": "Multilingual Self-Taught Faithfulness Evaluators", "authors": ["Carlo Alfano", "Aymen Al Marjani", "Zeno Jonke", "Amin Mantrach", "Saab Mansour", "Marcello Federico"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The growing use of large language models (LLMs) has increased the need for\nautomatic evaluation systems, particularly to address the challenge of\ninformation hallucination. Although existing faithfulness evaluation approaches\nhave shown promise, they are predominantly English-focused and often require\nexpensive human-labeled training data for fine-tuning specialized models. As\nLLMs see increased adoption in multilingual contexts, there is a need for\naccurate faithfulness evaluators that can operate across languages without\nextensive labeled data. This paper presents Self-Taught Evaluators for\nMultilingual Faithfulness, a framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer\nlearning. Through experiments comparing language-specific and mixed-language\nfine-tuning approaches, we demonstrate a consistent relationship between an\nLLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines,\nincluding state-of-the-art English evaluators and machine translation-based\napproaches."}
{"id": "2507.20783", "pdf": "https://arxiv.org/pdf/2507.20783.pdf", "abs": "https://arxiv.org/abs/2507.20783", "title": "On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey", "authors": ["Meishan Zhang", "Xin Zhang", "Xinping Zhao", "Shouzheng Huang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "45 pages, 2 figures, 9 tables", "summary": "Text embeddings have attracted growing interest due to their effectiveness\nacross a wide range of natural language processing (NLP) tasks, such as\nretrieval, classification, clustering, bitext mining, and summarization. With\nthe emergence of pretrained language models (PLMs), general-purpose text\nembeddings (GPTE) have gained significant traction for their ability to produce\nrich, transferable representations. The general architecture of GPTE typically\nleverages PLMs to derive dense text representations, which are then optimized\nthrough contrastive learning on large-scale pairwise datasets. In this survey,\nwe provide a comprehensive overview of GPTE in the era of PLMs, focusing on the\nroles PLMs play in driving its development. We first examine the fundamental\narchitecture and describe the basic roles of PLMs in GPTE, i.e., embedding\nextraction, expressivity enhancement, training strategies, learning objectives,\nand data construction. Then, we describe advanced roles enabled by PLMs, such\nas multilingual support, multimodal integration, code understanding, and\nscenario-specific adaptation. Finally, we highlight potential future research\ndirections that move beyond traditional improvement goals, including ranking\nintegration, safety considerations, bias mitigation, structural information\nincorporation, and the cognitive extension of embeddings. This survey aims to\nserve as a valuable reference for both newcomers and established researchers\nseeking to understand the current state and future potential of GPTE."}
{"id": "2507.20786", "pdf": "https://arxiv.org/pdf/2507.20786.pdf", "abs": "https://arxiv.org/abs/2507.20786", "title": "Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models", "authors": ["Sam Osian", "Arpan Dutta", "Sahil Bhandari", "Iain E. Buchan", "Dan W. Joyce"], "categories": ["cs.CL"], "comment": "8 pages, 1 figure", "summary": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research."}
{"id": "2507.20849", "pdf": "https://arxiv.org/pdf/2507.20849.pdf", "abs": "https://arxiv.org/abs/2507.20849", "title": "Latent Inter-User Difference Modeling for LLM Personalization", "authors": ["Yilun Qiu", "Tianhao Shi", "Xiaoyan Zhao", "Fengbin Zhu", "Yang Zhang", "Fuli Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP."}
{"id": "2507.20858", "pdf": "https://arxiv.org/pdf/2507.20858.pdf", "abs": "https://arxiv.org/abs/2507.20858", "title": "A survey of diversity quantification in natural language processing: The why, what, where and how", "authors": ["Louis Estève", "Marie-Catherine de Marneffe", "Nurit Melnik", "Agata Savary", "Olha Kanishcheva"], "categories": ["cs.CL"], "comment": null, "summary": "The concept of diversity has received increased consideration in Natural\nLanguage Processing (NLP) in recent years. This is due to various motivations\nlike promoting and inclusion, approximating human linguistic behavior, and\nincreasing systems' performance. Diversity has however often been addressed in\nan ad hoc manner in NLP, and with few explicit links to other domains where\nthis notion is better theorized. We survey articles in the ACL Anthology from\nthe past 6 years, with \"diversity\" or \"diverse\" in their title. We find a wide\nrange of settings in which diversity is quantified, often highly specialized\nand using inconsistent terminology. We put forward a unified taxonomy of why,\nwhat on, where, and how diversity is measured in NLP. Diversity measures are\ncast upon a unified framework from ecology and economy (Stirling, 2007) with 3\ndimensions of diversity: variety, balance and disparity. We discuss the trends\nwhich emerge due to this systematized approach. We believe that this study\npaves the way towards a better formalization of diversity in NLP, which should\nbring a better understanding of this notion and a better comparability between\nvarious approaches."}
{"id": "2507.20859", "pdf": "https://arxiv.org/pdf/2507.20859.pdf", "abs": "https://arxiv.org/abs/2507.20859", "title": "Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings", "authors": ["Luc Builtjes", "Joeran Bosma", "Mathias Prokop", "Bram van Ginneken", "Alessa Hering"], "categories": ["cs.CL"], "comment": "34 pages, 5 figures", "summary": "Medical reports contain rich clinical information but are often unstructured\nand written in domain-specific language, posing challenges for information\nextraction. While proprietary large language models (LLMs) have shown promise\nin clinical natural language processing, their lack of transparency and data\nprivacy concerns limit their utility in healthcare. This study therefore\nevaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed\n\\texttt{llm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess model\nperformance in a zero-shot setting. Several 14 billion parameter models,\nPhi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,\nwhile the bigger Llama-3.3-70B model achieved slightly higher performance at\ngreater computational cost. Translation to English prior to inference\nconsistently degraded performance, highlighting the need of native-language\nprocessing. These findings demonstrate that open-source LLMs, when used with\nour framework, offer effective, scalable, and privacy-conscious solutions for\nclinical information extraction in low-resource settings."}
{"id": "2507.20906", "pdf": "https://arxiv.org/pdf/2507.20906.pdf", "abs": "https://arxiv.org/abs/2507.20906", "title": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning", "authors": ["Jungwon Park", "Wonjong Rhee"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks by conditioning on input-output examples in the prompt, without requiring\nany update in model parameters. While widely adopted, it remains unclear\nwhether prompting with multiple examples is the most effective and efficient\nway to convey task information. In this work, we propose Soft Injection of task\nembeddings. The task embeddings are constructed only once using few-shot ICL\nprompts and repeatedly used during inference. Soft injection is performed by\nsoftly mixing task embeddings with attention head activations using\npre-optimized mixing parameters, referred to as soft head-selection parameters.\nThis method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while\nreducing memory usage and compute cost at inference time. An extensive\nevaluation is performed across 57 tasks and 12 LLMs, spanning four model\nfamilies of sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show\nthat our method also serves as an insightful tool for analyzing task-relevant\nroles of attention heads, revealing that task-relevant head positions selected\nby our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection\nmethod opens a new paradigm for reducing prompt length and improving task\nperformance by shifting task conditioning from the prompt space to the\nactivation space."}
{"id": "2507.20917", "pdf": "https://arxiv.org/pdf/2507.20917.pdf", "abs": "https://arxiv.org/abs/2507.20917", "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation", "authors": ["Adrien Bazoge"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work introduces MediQAl, a French medical question answering dataset\ndesigned to evaluate the capabilities of language models in factual medical\nrecall and reasoning over real-world clinical scenarios. MediQAl contains\n32,603 questions sourced from French medical examinations across 41 medical\nsubjects. The dataset includes three tasks: (i) Multiple-Choice Question with\nUnique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)\nOpen-Ended Question with Short-Answer. Each question is labeled as\nUnderstanding or Reasoning, enabling a detailed analysis of models' cognitive\ncapabilities. We validate the MediQAl dataset through extensive evaluation with\n14 large language models, including recent reasoning-augmented models, and\nobserve a significant performance gap between factual recall and reasoning\ntasks. Our evaluation provides a comprehensive benchmark for assessing language\nmodels' performance on French medical question answering, addressing a crucial\ngap in multilingual resources for the medical domain."}
{"id": "2507.20924", "pdf": "https://arxiv.org/pdf/2507.20924.pdf", "abs": "https://arxiv.org/abs/2507.20924", "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models", "authors": ["Roberto Labadie-Tamayo", "Adrian Jaques Böck", "Djordje Slijepčević", "Xihui Chen", "Andreas Babic", "Matthias Zeppelzauer"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "I.2"], "comment": "12 pages", "summary": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish."}
{"id": "2507.20930", "pdf": "https://arxiv.org/pdf/2507.20930.pdf", "abs": "https://arxiv.org/abs/2507.20930", "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models", "authors": ["Likun Tan", "Kuan-Wei Huang", "Kevin Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/fine-grained-editting."}
{"id": "2507.20956", "pdf": "https://arxiv.org/pdf/2507.20956.pdf", "abs": "https://arxiv.org/abs/2507.20956", "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models", "authors": ["Max Peeperkorn", "Tom Kouwenhoven", "Dan Brown", "Anna Jordanous"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality."}
{"id": "2507.21009", "pdf": "https://arxiv.org/pdf/2507.21009.pdf", "abs": "https://arxiv.org/abs/2507.21009", "title": "Memorization in Fine-Tuned Large Language Models", "authors": ["Danil Savine", "Muni Sreenivas Pydi", "Jamal Atif", "Olivier Cappé"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns."}
{"id": "2507.21028", "pdf": "https://arxiv.org/pdf/2507.21028.pdf", "abs": "https://arxiv.org/abs/2507.21028", "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation", "authors": ["Jiaju Chen", "Yuxuan Lu", "Xiaojie Wang", "Huimin Zeng", "Jing Huang", "Jiri Gesi", "Ying Xu", "Bingsheng Yao", "Dakuo Wang"], "categories": ["cs.CL", "68T50"], "comment": null, "summary": "Nearly all human work is collaborative; thus, the evaluation of real-world\nNLP applications often requires multiple dimensions that align with diverse\nhuman perspectives. As real human evaluator resources are often scarce and\ncostly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising\napproach to leverage LLM agents to believably simulate human evaluators. Yet,\nto date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are\nnot generalizable to other tasks. To address these challenges, we propose\nMAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from relevant\ntext documents (e.g., research papers), instantiate LLM agents with the\npersonas, and engage in-group debates with multi-agents to Generate\nmulti-dimensional feedback. Our evaluation experiments in both the educational\nand medical domains demonstrate that MAJ-EVAL can generate evaluation results\nthat better align with human experts' ratings compared with conventional\nautomated evaluation metrics and existing LLM-as-a-judge methods."}
{"id": "2507.19487", "pdf": "https://arxiv.org/pdf/2507.19487.pdf", "abs": "https://arxiv.org/abs/2507.19487", "title": "Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective", "authors": ["Margarita Leib", "Nils Köbis", "Ivan Soraperra"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "People increasingly rely on AI-advice when making decisions. At times, such\nadvice can promote selfish behavior. When individuals abide by\nselfishness-promoting AI advice, how are they perceived and punished? To study\nthis question, we build on theories from social psychology and combine\nmachine-behavior and behavioral economic approaches. In a pre-registered,\nfinancially-incentivized experiment, evaluators could punish real\ndecision-makers who (i) received AI, human, or no advice. The advice (ii)\nencouraged selfish or prosocial behavior, and decision-makers (iii) behaved\nselfishly or, in a control condition, behaved prosocially. Evaluators further\nassigned responsibility to decision-makers and their advisors. Results revealed\nthat (i) prosocial behavior was punished very little, whereas selfish behavior\nwas punished much more. Focusing on selfish behavior, (ii) compared to\nreceiving no advice, selfish behavior was penalized more harshly after\nprosocial advice and more leniently after selfish advice. Lastly, (iii) whereas\nselfish decision-makers were seen as more responsible when they followed AI\ncompared to human advice, punishment between the two advice sources did not\nvary. Overall, behavior and advice content shape punishment, whereas the advice\nsource does not."}
{"id": "2507.19534", "pdf": "https://arxiv.org/pdf/2507.19534.pdf", "abs": "https://arxiv.org/abs/2507.19534", "title": "FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings", "authors": ["Ali Shakeri", "Wei Emma Zhang", "Amin Beheshti", "Weitong Chen", "Jian Yang", "Lishan Yang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2; I.7"], "comment": "12 pages; Published to PAKDD'2025", "summary": "Pre-trained Language Models (PLMs) have demonstrated impressive performance\nin various NLP tasks. However, traditional fine-tuning methods for leveraging\nPLMs for downstream tasks entail significant computational overhead.\nPrompt-tuning has emerged as an efficient alternative that involves prepending\na limited number of parameters to the input sequence and only updating them\nwhile the PLM's parameters are frozen. However, this technique's prompts remain\nfixed for all inputs, reducing the model's flexibility. The Federated Learning\n(FL) technique has gained attention in recent years to address the growing\nconcerns around data privacy. However, challenges such as communication and\ncomputation limitations of clients still need to be addressed. To mitigate\nthese challenges, this paper introduces the Federated Dynamic Prompt Generator\n(FedDPG), which incorporates a dynamic prompt generator network to generate\ncontext-aware prompts based on the given input, ensuring flexibility and\nadaptability while prioritising data privacy in federated learning settings.\nOur experiments on three NLP benchmark datasets showcase that FedDPG\noutperforms the state-of-the-art parameter-efficient fine-tuning methods in\nterms of global model performance, and has significantly reduced the\ncalculation time and the number of parameters to be sent through the FL\nnetwork."}
{"id": "2507.19684", "pdf": "https://arxiv.org/pdf/2507.19684.pdf", "abs": "https://arxiv.org/abs/2507.19684", "title": "Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks", "authors": ["Bermet Burkanova", "Payam Jome Yazdian", "Chuxuan Zhang", "Trinity Evans", "Paige Tuttösí", "Angelica Lim"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "https://rosielab.github.io/compas3d", "summary": "Imagine a humanoid that can safely and creatively dance with a human,\nadapting to its partner's proficiency, using haptic signaling as a primary form\nof communication. While today's AI systems excel at text or voice-based\ninteraction with large language models, human communication extends far beyond\ntext-it includes embodied movement, timing, and physical coordination. Modeling\ncoupled interaction between two agents poses a formidable challenge: it is\ncontinuous, bidirectionally reactive, and shaped by individual variation. We\npresent CoMPAS3D, the largest and most diverse motion capture dataset of\nimprovised salsa dancing, designed as a challenging testbed for interactive,\nexpressive humanoid AI. The dataset includes 3 hours of leader-follower salsa\ndances performed by 18 dancers spanning beginner, intermediate, and\nprofessional skill levels. For the first time, we provide fine-grained salsa\nexpert annotations, covering over 2,800 move segments, including move types,\ncombinations, execution errors and stylistic elements. We draw analogies\nbetween partner dance communication and natural language, evaluating CoMPAS3D\non two benchmark tasks for synthetic humans that parallel key problems in\nspoken language and dialogue processing: leader or follower generation with\nproficiency levels (speaker or listener synthesis), and duet (conversation)\ngeneration. Towards a long-term goal of partner dance with humans, we release\nthe dataset, annotations, and code, along with a multitask SalsaAgent model\ncapable of performing all benchmark tasks, alongside additional baselines to\nencourage research in socially interactive embodied AI and creative, expressive\nhumanoid motion generation."}
{"id": "2507.19840", "pdf": "https://arxiv.org/pdf/2507.19840.pdf", "abs": "https://arxiv.org/abs/2507.19840", "title": "AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition", "authors": ["Samuel Ebimobowei Johnny", "Blessed Guda", "Andrew Blayama Stephen", "Assane Gueye"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Paper to appear at the 1st Workshop in Multimodal Sign Language\n  Recognition at ICCV 2025", "summary": "Continuously recognizing sign gestures and converting them to glosses plays a\nkey role in bridging the gap between the hearing and hearing-impaired\ncommunities. This involves recognizing and interpreting the hands, face, and\nbody gestures of the signer, which pose a challenge as it involves a\ncombination of all these features. Continuous Sign Language Recognition (CSLR)\nmethods rely on multi-stage pipelines that first extract visual features, then\nalign variable-length sequences with target glosses using CTC or HMM-based\napproaches. However, these alignment-based methods suffer from error\npropagation across stages, overfitting, and struggle with vocabulary\nscalability due to the intermediate gloss representation bottleneck. To address\nthese limitations, we propose AutoSign, an autoregressive decoder-only\ntransformer that directly translates pose sequences to natural language text,\nbypassing traditional alignment mechanisms entirely. The use of this\ndecoder-only approach allows the model to directly map between the features and\nthe glosses without the need for CTC loss while also directly learning the\ntextual dependencies in the glosses. Our approach incorporates a temporal\ncompression module using 1D CNNs to efficiently process pose sequences,\nfollowed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses).\nThrough comprehensive ablation studies, we demonstrate that hand and body\ngestures provide the most discriminative features for signer-independent CSLR.\nBy eliminating the multi-stage pipeline, AutoSign achieves substantial\nimprovements on the Isharah-1000 dataset, achieving an improvement of up to\n6.1\\% in WER score compared to the best existing method."}
{"id": "2507.19849", "pdf": "https://arxiv.org/pdf/2507.19849.pdf", "abs": "https://arxiv.org/abs/2507.19849", "title": "Agentic Reinforced Policy Optimization", "authors": ["Guanting Dong", "Hangyu Mao", "Kai Ma", "Licheng Bao", "Yifei Chen", "Zhongyuan Wang", "Zhongxia Chen", "Jiazhen Du", "Huiyang Wang", "Fuzheng Zhang", "Guorui Zhou", "Yutao Zhu", "Ji-Rong Wen", "Zhicheng Dou"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Working on progress", "summary": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO"}
{"id": "2507.19909", "pdf": "https://arxiv.org/pdf/2507.19909.pdf", "abs": "https://arxiv.org/abs/2507.19909", "title": "The Impact of Fine-tuning Large Language Models on Automated Program Repair", "authors": ["Roman Macháček", "Anastasiia Grishina", "Max Hort", "Leon Moonen"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in the research track of the 41th\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Automated Program Repair (APR) uses various tools and techniques to help\ndevelopers achieve functional and error-free code faster. In recent years,\nLarge Language Models (LLMs) have gained popularity as components in APR tool\nchains because of their performance and flexibility. However, training such\nmodels requires a significant amount of resources. Fine-tuning techniques have\nbeen developed to adapt pre-trained LLMs to specific tasks, such as APR, and\nenhance their performance at far lower computational costs than training from\nscratch. In this study, we empirically investigate the impact of various\nfine-tuning techniques on the performance of LLMs used for APR. Our experiments\nprovide insights into the performance of a selection of state-of-the-art LLMs\npre-trained on code. The evaluation is done on three popular APR benchmarks\n(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs\nwith varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,\nBloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,\nfull fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and\nIA3. We observe that full fine-tuning techniques decrease the benchmarking\nperformance of various models due to different data distributions and\noverfitting. By using parameter-efficient fine-tuning methods, we restrict\nmodels in the amount of trainable parameters and achieve better results.\n  Keywords: large language models, automated program repair,\nparameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE."}
{"id": "2507.19947", "pdf": "https://arxiv.org/pdf/2507.19947.pdf", "abs": "https://arxiv.org/abs/2507.19947", "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "authors": ["Supawich Sitdhipol", "Waritwong Sukprasongdee", "Ekapol Chuangsuwanich", "Rina Tse"], "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "comment": "Accepted to the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC)", "summary": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance."}
{"id": "2507.19973", "pdf": "https://arxiv.org/pdf/2507.19973.pdf", "abs": "https://arxiv.org/abs/2507.19973", "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization", "authors": ["Ebrahim Rasromani", "Stella K. Kang", "Yanqi Xu", "Beisong Liu", "Garvit Luhadia", "Wan Fung Chui", "Felicia L. Pasadyn", "Yu Chih Hung", "Julie Y. An", "Edwin Mathieu", "Zehui Gu", "Carlos Fernandez-Granda", "Ammar A. Javed", "Greg D. Sacks", "Tamas Gonda", "Chenchan Huang", "Yiqiu Shen"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from\nradiology reports is labor-intensive, limiting large-scale studies needed to\nadvance PCL research. Purpose: To develop and evaluate large language models\n(LLMs) that automatically extract PCL features from MRI/CT reports and assign\nrisk categories based on guidelines. Materials and Methods: We curated a\ntraining dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134\npatients that described PCLs. Labels were generated by GPT-4o using\nchain-of-thought (CoT) prompting to extract PCL and main pancreatic duct\nfeatures. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated\nCoT data. Features were mapped to risk categories per institutional guideline\nbased on the 2017 ACR White Paper. Evaluation was performed on 285 held-out\nhuman-annotated reports. Model outputs for 100 cases were independently\nreviewed by three radiologists. Feature extraction was evaluated using exact\nmatch accuracy, risk categorization with macro-averaged F1 score, and\nradiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning\nimproved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%\nto 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved\n(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no\nstatistically significant differences. Radiologist inter-reader agreement was\nhigh (Fleiss' Kappa = 0.888) and showed no statistically significant difference\nwith the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT\n(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels\non par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT\nsupervision enable accurate, interpretable, and efficient phenotyping for\nlarge-scale PCL research, achieving performance comparable to GPT-4o."}
{"id": "2507.19990", "pdf": "https://arxiv.org/pdf/2507.19990.pdf", "abs": "https://arxiv.org/abs/2507.19990", "title": "Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model", "authors": ["Sinnyum Choi", "Woong Kim"], "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3.3; I.2.6; I.2.7"], "comment": null, "summary": "Recently, competition in the field of artificial intelligence (AI) has\nintensified among major technological companies, resulting in the continuous\nrelease of new large-language models (LLMs) that exhibit improved language\nunderstanding and context-based reasoning capabilities. It is expected that\nthese advances will enable more efficient personalized recommendations in\nLLM-based recommendation systems through improved quality of training data and\narchitectural design. However, many studies have not considered these recent\ndevelopments. In this study, it was proposed to improve LLM-based\nrecommendation systems by replacing Llama2 with Llama3 in the LlamaRec\nframework. To ensure a fair comparison, random seed values were set and\nidentical input data was provided during preprocessing and training. The\nexperimental results show average performance improvements of 38.65\\%, 8.69\\%,\nand 8.19\\% for the ML-100K, Beauty, and Games datasets, respectively, thus\nconfirming the practicality of this method. Notably, the significant\nimprovements achieved by model replacement indicate that the recommendation\nquality can be improved cost-effectively without the need to make structural\nchanges to the system. Based on these results, it is our contention that the\nproposed approach is a viable solution for improving the performance of current\nrecommendation systems."}
{"id": "2507.20018", "pdf": "https://arxiv.org/pdf/2507.20018.pdf", "abs": "https://arxiv.org/abs/2507.20018", "title": "The Carbon Cost of Conversation, Sustainability in the Age of Language Models", "authors": ["Sayed Mahbub Hasan Amiri", "Prasun Goswami", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Naznin Akter"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "22 Pages, 5 Tables", "summary": "Large language models (LLMs) like GPT-3 and BERT have revolutionized natural\nlanguage processing (NLP), yet their environmental costs remain dangerously\noverlooked. This article critiques the sustainability of LLMs, quantifying\ntheir carbon footprint, water usage, and contribution to e-waste through case\nstudies of models such as GPT-4 and energy-efficient alternatives like Mistral\n7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of\ncars driven annually, while data centre cooling exacerbates water scarcity in\nvulnerable regions. Systemic challenges corporate greenwashing, redundant model\ndevelopment, and regulatory voids perpetuate harm, disproportionately burdening\nmarginalized communities in the Global South. However, pathways exist for\nsustainable NLP: technical innovations (e.g., model pruning, quantum\ncomputing), policy reforms (carbon taxes, mandatory emissions reporting), and\ncultural shifts prioritizing necessity over novelty. By analysing industry\nleaders (Google, Microsoft) and laggards (Amazon), this work underscores the\nurgency of ethical accountability and global cooperation. Without immediate\naction, AIs ecological toll risks outpacing its societal benefits. The article\nconcludes with a call to align technological progress with planetary\nboundaries, advocating for equitable, transparent, and regenerative AI systems\nthat prioritize both human and environmental well-being."}
{"id": "2507.20051", "pdf": "https://arxiv.org/pdf/2507.20051.pdf", "abs": "https://arxiv.org/abs/2507.20051", "title": "$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning", "authors": ["Weicong Chen", "Vikash Singh", "Zahra Rahmani", "Debargha Ganguly", "Mohsen Hariri", "Vipin Chaudhary"], "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": null, "summary": "Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on\nerror-prone parsing, and use unrealistic evaluation protocols. We introduce\n$K^4$, an unsupervised and parser-independent framework for high-performance\nonline detection. $K^4$ transforms arbitrary log embeddings into compact\nfour-dimensional descriptors (Precision, Recall, Density, Coverage) using\nefficient k-nearest neighbor (k-NN) statistics. These descriptors enable\nlightweight detectors to accurately score anomalies without retraining. Using a\nmore realistic online evaluation protocol, $K^4$ sets a new state-of-the-art\n(AUROC: 0.995-0.999), outperforming baselines by large margins while being\norders of magnitude faster, with training under 4 seconds and inference as low\nas 4 $\\mu$s."}
{"id": "2507.20067", "pdf": "https://arxiv.org/pdf/2507.20067.pdf", "abs": "https://arxiv.org/abs/2507.20067", "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training", "authors": ["Sarat Chandra Bobbili", "Ujwal Dinesha", "Dheeraj Narasimha", "Srinivas Shakkottai"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Inference-time alignment enables large language models (LLMs) to generate\noutputs aligned with end-user preferences without further training. Recent\npost-training methods achieve this by using small guidance models to modify\ntoken generation during inference. These methods typically optimize a reward\nfunction KL-regularized by the original LLM taken as the reference policy. A\ncritical limitation, however, is their dependence on a pre-trained reward\nmodel, which requires fitting to human preference feedback--a potentially\nunstable process. In contrast, we introduce PITA, a novel framework that\nintegrates preference feedback directly into the LLM's token generation,\neliminating the need for a reward model. PITA learns a small preference-based\nguidance policy to modify token probabilities at inference time without LLM\nfine-tuning, reducing computational cost and bypassing the pre-trained reward\nmodel dependency. The problem is framed as identifying an underlying preference\ndistribution, solved through stochastic search and iterative refinement of the\npreference-based guidance model. We evaluate PITA across diverse tasks,\nincluding mathematical reasoning and sentiment classification, demonstrating\nits effectiveness in aligning LLM outputs with user preferences."}
{"id": "2507.20077", "pdf": "https://arxiv.org/pdf/2507.20077.pdf", "abs": "https://arxiv.org/abs/2507.20077", "title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning", "authors": ["Abdelrahman Mohamed", "Yova Kementchedjhieva"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Despite significant advances in vision-language models (VLMs), image\ncaptioning often suffers from a lack of detail, with base models producing\nshort, generic captions. This limitation persists even though VLMs are equipped\nwith strong vision and language backbones. While supervised data and complex\nreward functions have been proposed to improve detailed image captioning, we\nidentify a simpler underlying issue: a bias towards the end-of-sequence (EOS)\ntoken, which is introduced during cross-entropy training. We propose an\nunsupervised method to debias the model's tendency to predict the EOS token\nprematurely. By reducing this bias, we encourage the generation of longer, more\ndetailed captions without the need for intricate reward functions or\nsupervision. Our approach is straightforward, effective, and easily applicable\nto any pretrained model. We demonstrate its effectiveness through experiments\nwith three VLMs and on three detailed captioning benchmarks. Our results show a\nsubstantial increase in caption length and relevant details, albeit with an\nexpected increase in the rate of hallucinations."}
{"id": "2507.20096", "pdf": "https://arxiv.org/pdf/2507.20096.pdf", "abs": "https://arxiv.org/abs/2507.20096", "title": "EcoTransformer: Attention without Multiplication", "authors": ["Xin Gao", "Xingming Xu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05"], "comment": "8 pages, 1 figure", "summary": "The Transformer, with its scaled dot-product attention mechanism, has become\na foundational architecture in modern AI. However, this mechanism is\ncomputationally intensive and incurs substantial energy costs. We propose a new\nTransformer architecture EcoTransformer, in which the output context vector is\nconstructed as the convolution of the values using a Laplacian kernel, where\nthe distances are measured by the L1 metric between the queries and keys.\nCompared to dot-product based attention, the new attention score calculation is\nfree of matrix multiplication. It performs on par with, or even surpasses,\nscaled dot-product attention in NLP, bioinformatics, and vision tasks, while\nconsuming significantly less energy."}
{"id": "2507.20150", "pdf": "https://arxiv.org/pdf/2507.20150.pdf", "abs": "https://arxiv.org/abs/2507.20150", "title": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models", "authors": ["Xingcheng Xu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) plays a crucial role in shaping the behavior of\nlarge language and reasoning models (LLMs/LRMs). However, it often produces\nbrittle and unstable policies, leading to critical failures such as spurious\nreasoning, deceptive alignment, and instruction disobedience that undermine the\ntrustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified\ntheoretical explanation and are typically addressed using ad-hoc heuristics.\nThis paper presents a rigorous mathematical framework for analyzing the\nstability of the mapping from a reward function to the optimal policy. We show\nthat policy brittleness often stems from non-unique optimal actions, a common\noccurrence when multiple valid traces exist in a reasoning task. This\ntheoretical lens provides a unified explanation for a range of seemingly\ndisparate failures, reframing them as rational outcomes of optimizing rewards\nthat may be incomplete or noisy, especially in the presence of action\ndegeneracy. We extend this analysis from the fundamental single-reward setting\nto the more realistic multi-reward RL across diverse domains, showing how\nstability is governed by an \"effective reward\" aggregation mechanism. We also\nprove that entropy regularization restores policy stability at the cost of\nincreased stochasticity. Our framework provides a unified explanation for\nrecent empirical findings on deceptive reasoning, instruction-following\ntrade-offs, and RLHF-induced sophistry, and is further validated through\nperturbation experiments in multi-reward RL. This work advances\npolicy-stability analysis from empirical heuristics towards a principled\ntheory, offering essential insights for designing safer and more trustworthy AI\nsystems."}
{"id": "2507.20280", "pdf": "https://arxiv.org/pdf/2507.20280.pdf", "abs": "https://arxiv.org/abs/2507.20280", "title": "SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration", "authors": ["Keyan Ding", "Jing Yu", "Junjie Huang", "Yuchen Yang", "Qiang Zhang", "Huajun Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 6 figures", "summary": "Scientific research increasingly relies on specialized computational tools,\nyet effectively utilizing these tools demands substantial domain expertise.\nWhile Large Language Models (LLMs) show promise in tool automation, they\nstruggle to seamlessly integrate and orchestrate multiple tools for complex\nscientific workflows. Here, we present SciToolAgent, an LLM-powered agent that\nautomates hundreds of scientific tools across biology, chemistry, and materials\nscience. At its core, SciToolAgent leverages a scientific tool knowledge graph\nthat enables intelligent tool selection and execution through graph-based\nretrieval-augmented generation. The agent also incorporates a comprehensive\nsafety-checking module to ensure responsible and ethical tool usage. Extensive\nevaluations on a curated benchmark demonstrate that SciToolAgent significantly\noutperforms existing approaches. Case studies in protein engineering, chemical\nreactivity prediction, chemical synthesis, and metal-organic framework\nscreening further demonstrate SciToolAgent's capability to automate complex\nscientific workflows, making advanced research tools accessible to both experts\nand non-experts."}
{"id": "2507.20474", "pdf": "https://arxiv.org/pdf/2507.20474.pdf", "abs": "https://arxiv.org/abs/2507.20474", "title": "MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading", "authors": ["Siyi Wu", "Zhaoyang Guan", "Leyi Zhao", "Xinyuan Song", "Xinyu Ying", "Hanlin Zhang", "Michele Pak", "Yangfan He", "Yi Xin", "Jianhui Wang", "Tianyu Shi"], "categories": ["q-fin.TR", "cs.CL", "cs.LG"], "comment": null, "summary": "Cryptocurrency trading is a challenging task requiring the integration of\nheterogeneous data from multiple modalities. Traditional deep learning and\nreinforcement learning approaches typically demand large training datasets and\nencode diverse inputs into numerical representations, often at the cost of\ninterpretability. Recent progress in large language model (LLM)-based agents\nhas demonstrated the capacity to process multi-modal data and support complex\ninvestment decision-making. Building on these advances, we present\n\\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading\nthat coordinates specialized LLM-based agents to interpret financial data and\ngenerate investment strategies. MountainLion processes textual news,\ncandlestick charts, and trading signal charts to produce high-quality financial\nreports, while also enabling modification of reports and investment\nrecommendations through data-driven user interaction and question answering. A\ncentral reflection module analyzes historical trading signals and outcomes to\ncontinuously refine decision processes, and the system is capable of real-time\nreport analysis, summarization, and dynamic adjustment of investment\nstrategies. Empirical results confirm that MountainLion systematically enriches\ntechnical price triggers with contextual macroeconomic and capital flow\nsignals, providing a more interpretable, robust, and actionable investment\nframework that improves returns and strengthens investor confidence."}
{"id": "2507.20503", "pdf": "https://arxiv.org/pdf/2507.20503.pdf", "abs": "https://arxiv.org/abs/2507.20503", "title": "Customize Multi-modal RAI Guardrails with Precedent-based predictions", "authors": ["Cheng-Fu Yang", "Thanh Tran", "Christos Christodoulopoulos", "Weitong Ruan", "Rahul Gupta", "Kai-Wei Chang"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": "Accepted to COLM 2025", "summary": "A multi-modal guardrail must effectively filter image content based on\nuser-defined policies, identifying material that may be hateful, reinforce\nharmful stereotypes, contain explicit material, or spread misinformation.\nDeploying such guardrails in real-world applications, however, poses\nsignificant challenges. Users often require varied and highly customizable\npolicies and typically cannot provide abundant examples for each custom policy.\nConsequently, an ideal guardrail should be scalable to the multiple policies\nand adaptable to evolving user standards with minimal retraining. Existing\nfine-tuning methods typically condition predictions on pre-defined policies,\nrestricting their generalizability to new policies or necessitating extensive\nretraining to adapt. Conversely, training-free methods struggle with limited\ncontext lengths, making it difficult to incorporate all the policies\ncomprehensively. To overcome these limitations, we propose to condition model's\njudgment on \"precedents\", which are the reasoning processes of prior data\npoints similar to the given input. By leveraging precedents instead of fixed\npolicies, our approach greatly enhances the flexibility and adaptability of the\nguardrail. In this paper, we introduce a critique-revise mechanism for\ncollecting high-quality precedents and two strategies that utilize precedents\nfor robust prediction. Experimental results demonstrate that our approach\noutperforms previous methods across both few-shot and full-dataset scenarios\nand exhibits superior generalization to novel policies."}
{"id": "2507.20526", "pdf": "https://arxiv.org/pdf/2507.20526.pdf", "abs": "https://arxiv.org/abs/2507.20526", "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "authors": ["Andy Zou", "Maxwell Lin", "Eliot Jones", "Micha Nowak", "Mateusz Dziemian", "Nick Winter", "Alexander Grattan", "Valent Nathanael", "Ayla Croft", "Xander Davies", "Jai Patel", "Robert Kirk", "Nate Burnikell", "Yarin Gal", "Dan Hendrycks", "J. Zico Kolter", "Matt Fredrikson"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent advances have enabled LLM-powered AI agents to autonomously execute\ncomplex tasks by combining language model reasoning with tools, memory, and web\naccess. But can these systems be trusted to follow deployment policies in\nrealistic environments, especially under attack? To investigate, we ran the\nlargest public red-teaming competition to date, targeting 22 frontier AI agents\nacross 44 realistic deployment scenarios. Participants submitted 1.8 million\nprompt-injection attacks, with over 60,000 successfully eliciting policy\nviolations such as unauthorized data access, illicit financial actions, and\nregulatory noncompliance. We use these results to build the Agent Red Teaming\n(ART) benchmark - a curated set of high-impact attacks - and evaluate it across\n19 state-of-the-art models. Nearly all agents exhibit policy violations for\nmost behaviors within 10-100 queries, with high attack transferability across\nmodels and tasks. Importantly, we find limited correlation between agent\nrobustness and model size, capability, or inference-time compute, suggesting\nthat additional defenses are needed against adversarial misuse. Our findings\nhighlight critical and persistent vulnerabilities in today's AI agents. By\nreleasing the ART benchmark and accompanying evaluation framework, we aim to\nsupport more rigorous security assessment and drive progress toward safer agent\ndeployment."}
{"id": "2507.20534", "pdf": "https://arxiv.org/pdf/2507.20534.pdf", "abs": "https://arxiv.org/abs/2507.20534", "title": "Kimi K2: Open Agentic Intelligence", "authors": ["Kimi Team", "Yifan Bai", "Yiping Bao", "Guanduo Chen", "Jiahao Chen", "Ningxin Chen", "Ruijue Chen", "Yanru Chen", "Yuankun Chen", "Yutian Chen", "Zhuofu Chen", "Jialei Cui", "Hao Ding", "Mengnan Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Yulun Du", "Yu Fan", "Yichen Feng", "Kelin Fu", "Bofei Gao", "Hongcheng Gao", "Peizhong Gao", "Tong Gao", "Xinran Gu", "Longyu Guan", "Haiqing Guo", "Jianhang Guo", "Hao Hu", "Xiaoru Hao", "Tianhong He", "Weiran He", "Wenyang He", "Chao Hong", "Yangyang Hu", "Zhenxing Hu", "Weixiao Huang", "Zhiqi Huang", "Zihao Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yongsheng Kang", "Guokun Lai", "Cheng Li", "Fang Li", "Haoyang Li", "Ming Li", "Wentao Li", "Yanhao Li", "Yiwei Li", "Zhaowei Li", "Zheming Li", "Hongzhan Lin", "Xiaohan Lin", "Zongyu Lin", "Chengyin Liu", "Chenyu Liu", "Hongzhang Liu", "Jingyuan Liu", "Junqi Liu", "Liang Liu", "Shaowei Liu", "T. Y. Liu", "Tianwei Liu", "Weizhou Liu", "Yangyang Liu", "Yibo Liu", "Yiping Liu", "Yue Liu", "Zhengying Liu", "Enzhe Lu", "Lijun Lu", "Shengling Ma", "Xinyu Ma", "Yingwei Ma", "Shaoguang Mao", "Jie Mei", "Xin Men", "Yibo Miao", "Siyuan Pan", "Yebo Peng", "Ruoyu Qin", "Bowen Qu", "Zeyu Shang", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Jianlin Su", "Zhengyuan Su", "Xinjie Sun", "Flood Sung", "Heyi Tang", "Jiawen Tao", "Qifeng Teng", "Chensi Wang", "Dinglu Wang", "Feng Wang", "Haiming Wang", "Jianzhou Wang", "Jiaxing Wang", "Jinhong Wang", "Shengjie Wang", "Shuyi Wang", "Yao Wang", "Yejie Wang", "Yiqin Wang", "Yuxin Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhengtao Wang", "Zhexu Wang", "Chu Wei", "Qianqian Wei", "Wenhao Wu", "Xingzhe Wu", "Yuxin Wu", "Chenjun Xiao", "Xiaotong Xie", "Weimin Xiong", "Boyu Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinran Xu", "Yangchuan Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Xiaofei Yang", "Ying Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Xingcheng Yao", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Longhui Yu", "Enming Yuan", "Hongbang Yuan", "Mengjie Yuan", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Wanlu Zhang", "Xiaobin Zhang", "Yangkun Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Haotian Zhao", "Yikai Zhao", "Huabin Zheng", "Shaojie Zheng", "Jianren Zhou", "Xinyu Zhou", "Zaida Zhou", "Zhen Zhu", "Weiyu Zhuang", "Xinxing Zu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "tech report of Kimi K2", "summary": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32\nbillion activated parameters and 1 trillion total parameters. We propose the\nMuonClip optimizer, which improves upon Muon with a novel QK-clip technique to\naddress training instability while enjoying the advanced token efficiency of\nMuon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero\nloss spike. During post-training, K2 undergoes a multi-stage post-training\nprocess, highlighted by a large-scale agentic data synthesis pipeline and a\njoint reinforcement learning (RL) stage, where the model improves its\ncapabilities through interactions with real and synthetic environments.\n  Kimi K2 achieves state-of-the-art performance among open-source non-thinking\nmodels, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on\nTau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on\nSWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in\nnon-thinking settings. It also exhibits strong capabilities in coding,\nmathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,\n49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without\nextended thinking. These results position Kimi K2 as one of the most capable\nopen-source large language models to date, particularly in software engineering\nand agentic tasks. We release our base and post-trained model checkpoints to\nfacilitate future research and applications of agentic intelligence."}
{"id": "2507.20884", "pdf": "https://arxiv.org/pdf/2507.20884.pdf", "abs": "https://arxiv.org/abs/2507.20884", "title": "The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?", "authors": ["Dinh Nam Pham", "Eleftherios Avramidis"], "categories": ["cs.CV", "cs.CL", "eess.IV"], "comment": "Accepted at 9th International Workshop on Sign Language Translation\n  and Avatar Technologies @ ACM IVA'25", "summary": "Non-manual facial features play a crucial role in sign language\ncommunication, yet their importance in automatic sign language recognition\n(ASLR) remains underexplored. While prior studies have shown that incorporating\nfacial features can improve recognition, related work often relies on\nhand-crafted feature extraction and fails to go beyond the comparison of manual\nfeatures versus the combination of manual and facial features. In this work, we\nsystematically investigate the contribution of distinct facial regionseyes,\nmouth, and full faceusing two different deep learning models (a CNN-based model\nand a transformer-based model) trained on an SLR dataset of isolated signs with\nrandomly selected classes. Through quantitative performance and qualitative\nsaliency map evaluation, we reveal that the mouth is the most important\nnon-manual facial feature, significantly improving accuracy. Our findings\nhighlight the necessity of incorporating facial features in ASLR."}
{"id": "2507.20888", "pdf": "https://arxiv.org/pdf/2507.20888.pdf", "abs": "https://arxiv.org/abs/2507.20888", "title": "Enhancing Project-Specific Code Completion by Inferring Internal API Information", "authors": ["Le Deng", "Xiaoxue Ren", "Chao Ni", "Ming Liang", "David Lo", "Zhongxin Liu"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%."}
{"id": "2507.20890", "pdf": "https://arxiv.org/pdf/2507.20890.pdf", "abs": "https://arxiv.org/abs/2507.20890", "title": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement", "authors": ["Zhecheng Li", "Guoxian Song", "Yiwei Wang", "Zhen Xiong", "Junsong Yuan", "Yujun Cai"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Img2LaTeX is a practically significant task that involves converting\nmathematical expressions or tabular data from images into LaTeX code. In recent\nyears, vision-language models (VLMs) have demonstrated strong performance\nacross a variety of visual understanding tasks, owing to their generalization\ncapabilities. While some studies have explored the use of VLMs for the\nImg2LaTeX task, their performance often falls short of expectations.\nEmpirically, VLMs sometimes struggle with fine-grained visual elements, leading\nto inaccurate LaTeX predictions. To address this challenge, we propose\n$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention\nlocalization and iterative refinement within a visual reasoning framework,\nenabling VLMs to perform self-correction and progressively improve prediction\nquality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging\nexamples designed to rigorously evaluate the capabilities of VLMs within this\ntask domain. Extensive experimental results demonstrate that: (1) $A^2R^2$\nsignificantly improves model performance across six evaluation metrics spanning\nboth textual and visual levels, consistently outperforming other baseline\nmethods; (2) Increasing the number of inference rounds yields notable\nperformance gains, underscoring the potential of $A^2R^2$ in test-time scaling\nscenarios; (3) Ablation studies and human evaluations validate the practical\neffectiveness of our approach, as well as the strong synergy among its core\ncomponents during inference."}
{"id": "2507.20936", "pdf": "https://arxiv.org/pdf/2507.20936.pdf", "abs": "https://arxiv.org/abs/2507.20936", "title": "Dissecting Persona-Driven Reasoning in Language Models via Activation Patching", "authors": ["Ansh Poonia", "Maeghal Jain"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages", "summary": "Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities."}
{"id": "2507.20957", "pdf": "https://arxiv.org/pdf/2507.20957.pdf", "abs": "https://arxiv.org/abs/2507.20957", "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis", "authors": ["Hoyoung Lee", "Junhyuk Seo", "Suhwan Park", "Junhyeong Lee", "Wonbin Ahn", "Chanyeol Choi", "Alejandro Lopez-Lira", "Yongjae Lee"], "categories": ["q-fin.PM", "cs.AI", "cs.CL"], "comment": null, "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence."}
{"id": "2507.20999", "pdf": "https://arxiv.org/pdf/2507.20999.pdf", "abs": "https://arxiv.org/abs/2507.20999", "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning", "authors": ["Yining Huang", "Bin Li", "Keke Tang", "Meilian Chen"], "categories": ["cs.LG", "cs.CL"], "comment": "10 pages", "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."}
{"id": "2401.12295", "pdf": "https://arxiv.org/pdf/2401.12295.pdf", "abs": "https://arxiv.org/abs/2401.12295", "title": "Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data", "authors": ["Leonardo Castro-Gonzalez", "Yi-Ling Chung", "Hannak Rose Kirk", "John Francis", "Angus R. Williams", "Pica Johansson", "Jonathan Bright"], "categories": ["cs.CL", "I.2.7; J.4"], "comment": "46 pages, 17 figures, 6 tables", "summary": "The field of machine learning has recently made significant progress in\nreducing the requirements for labelled training data when building new models.\nThese `cheaper' learning techniques hold significant potential for the social\nsciences, where development of large labelled training datasets is often a\nsignificant practical impediment to the use of machine learning for analytical\ntasks. In this article we review three `cheap' techniques that have developed\nin recent years: weak supervision, transfer learning and prompt engineering.\nFor the latter, we also review the particular case of zero-shot prompting of\nlarge language models. For each technique we provide a guide of how it works\nand demonstrate its application across six different realistic social science\napplications (two different tasks paired with three different dataset makeups).\nWe show good performance for all techniques, and in particular we demonstrate\nhow prompting of large language models can achieve high accuracy at very low\ncost. Our results are accompanied by a code repository to make it easy for\nothers to duplicate our work and use it in their own research. Overall, our\narticle is intended to stimulate further uptake of these techniques in the\nsocial sciences."}
{"id": "2403.18140", "pdf": "https://arxiv.org/pdf/2403.18140.pdf", "abs": "https://arxiv.org/abs/2403.18140", "title": "Juru: Legal Brazilian Large Language Model from Reputable Sources", "authors": ["Roseval Malaquias Junior", "Ramon Pires", "Roseli Romero", "Rodrigo Nogueira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The high compute cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Mistral-7B model with 1.9 billion unique tokens\nfrom reputable Brazilian legal sources and conducted few-shot evaluations on\nlegal and general knowledge test suites. Our model, Juru, demonstrates the\nbenefits of domain specialization by achieving improved performance on legal\nbenchmarks, even with a reduced amount of pretraining data. However, this\ndomain specialization through continued pretraining comes at the cost of\nincreased forgetting in unrelated domains, as evidenced by performance\ndegradation on general knowledge test suites in both Portuguese and English.\nThis study contributes to the growing body of scientific evidence showing that\npretraining data selection may enhance the performance of large language\nmodels, enabling the exploration of these models at a lower cost. Juru is\npublicly available at https://huggingface.co/roseval/Juru-7B ."}
{"id": "2406.00222", "pdf": "https://arxiv.org/pdf/2406.00222.pdf", "abs": "https://arxiv.org/abs/2406.00222", "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training", "authors": ["Maximillian Chen", "Ruoxi Sun", "Tomas Pfister", "Sercan Ö. Arık"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025; Code:\n  https://github.com/google-research/google-research/tree/master/learning_to_clarify", "summary": "Large language models (LLMs), optimized through human feedback, have rapidly\nemerged as a leading paradigm for developing intelligent conversational\nassistants. However, despite their strong performance across many benchmarks,\nLLM-based agents might still lack conversational skills such as disambiguation\n-- when they are faced with ambiguity, they often overhedge or implicitly guess\nusers' true intents rather than asking clarification questions. Under\ntask-specific settings, high-quality conversation samples are often limited,\nconstituting a bottleneck for LLMs' ability to learn optimal dialogue action\npolicies. We propose Action-Based Contrastive Self-Training (ACT), a\nquasi-online preference optimization algorithm based on Direct Preference\nOptimization (DPO), that enables data-efficient dialogue policy learning in\nmulti-turn conversation modeling. We demonstrate ACT's efficacy under in\ndata-efficient tuning scenarios, even when there is no action label available,\nusing multiple real-world conversational tasks: tabular-grounded\nquestion-answering, machine reading comprehension, and AmbigSQL, a novel task\nfor disambiguating information-seeking requests for complex SQL generation\ntowards data analysis agents. Additionally, we propose evaluating LLMs' ability\nto function as conversational agents by examining whether they can implicitly\nrecognize and reason about ambiguity in conversation. ACT demonstrates\nsubstantial conversation modeling improvements over standard tuning approaches\nlike supervised fine-tuning and DPO."}
{"id": "2406.06144", "pdf": "https://arxiv.org/pdf/2406.06144.pdf", "abs": "https://arxiv.org/abs/2406.06144", "title": "Language Models Resist Alignment: Evidence From Data Compression", "authors": ["Jiaming Ji", "Kaile Wang", "Tianyi Qiu", "Boyuan Chen", "Jiayi Zhou", "Changye Li", "Hantao Lou", "Juntao Dai", "Yunhuai Liu", "Yaodong Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main", "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io."}
{"id": "2406.13632", "pdf": "https://arxiv.org/pdf/2406.13632.pdf", "abs": "https://arxiv.org/abs/2406.13632", "title": "DoubleDipper: Improving Long-Context LLMs via Context Recycling", "authors": ["Arie Cattan", "Alon Jacovi", "Alex Fabrikant", "Jonathan Herzig", "Roee Aharoni", "Hannah Rashkin", "Dror Marcus", "Avinatan Hassidim", "Yossi Matias", "Idan Szpektor", "Avi Caciularu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In this work,\nwe propose DoubleDipper, a novel In-Context-Learning method that automatically\ngenerates few-shot examples for long context QA tasks by recycling contexts.\nSpecifically, given a long input context (1-3k tokens) and a query, we generate\nadditional query-output pairs from the given context as few-shot examples,\nwhile introducing the context only once. This ensures that the demonstrations\nare leveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context. Surprisingly, despite introducing only single-hop\nICL examples, LLMs successfully generalize to multi-hop long-context QA using\nour approach."}
{"id": "2407.19299", "pdf": "https://arxiv.org/pdf/2407.19299.pdf", "abs": "https://arxiv.org/abs/2407.19299", "title": "The Impact of LoRA Adapters on LLMs for Clinical Text Classification Under Computational and Data Constraints", "authors": ["Thanh-Dung Le", "Ti Ti Nguyen", "Vu Nguyen Ha", "Symeon Chatzinotas", "Philippe Jouvet", "Rita Noumeir"], "categories": ["cs.CL", "eess.SP"], "comment": "Accepted for publication in the IEEE Access", "summary": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to domain gap, limited data,\nand stringent hardware constraints. In this study, we evaluate four adapter\ntechniques-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN) - equivalent to Low-Rank Adaptation (LoRA), for clinical note\nclassification under real-world, resource-constrained conditions. All\nexperiments were conducted on a single NVIDIA Quadro P620 GPU (2 GB VRAM, 512\nCUDA cores, 1.386 TFLOPS FP32), limiting batch sizes to <8 sequences and\nmaximum sequence length to 256 tokens. Our clinical corpus comprises only 580\n000 tokens, several orders of magnitude smaller than standard LLM pre-training\ndatasets. We fine-tuned three biomedical pre-trained LLMs (CamemBERT-bio,\nAliBERT, DrBERT) and two lightweight Transformer models trained from scratch.\nResults show that 1) adapter structures provide no consistent gains when\nfine-tuning biomedical LLMs under these constraints, and 2) simpler\nTransformers, with minimal parameter counts and training times under six hours,\noutperform adapter-augmented LLMs, which required over 1000 GPU-hours. Among\nadapters, GRN achieved the best metrics (accuracy, precision, recall, F1 =\n0.88). These findings demonstrate that, in low-resource clinical settings with\nlimited data and compute, lightweight Transformers trained from scratch offer a\nmore practical and efficient solution than large LLMs, while GRN remains a\nviable adapter choice when minimal adaptation is needed."}
{"id": "2409.06624", "pdf": "https://arxiv.org/pdf/2409.06624.pdf", "abs": "https://arxiv.org/abs/2409.06624", "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio", "authors": ["Ningyuan Xi", "Yetao Wu", "Kun Fan", "Teng Chen", "Qingqing Gu", "Luo Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 2 figures", "summary": "Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to\nobtain unfamiliar language skills or adapt to new domains. The huge training\ncost of CPT often asks for cautious choice of key hyper-parameters such as the\nmixture ratio of extra language or domain corpus. However, there is no\nsystematic study that bridges the gap between the optimal mixture ratio and the\nactual model performance, and the gap between experimental scaling law and the\nactual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicates the optimal\nexperimental setup. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark but also in some specific domains including math, coding, and\nemotional intelligence. We deploy the final 70B version of LLM on a real-life\nchat system which obtains satisfying performance."}
{"id": "2409.12059", "pdf": "https://arxiv.org/pdf/2409.12059.pdf", "abs": "https://arxiv.org/abs/2409.12059", "title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning", "authors": ["Ningyuan Xi", "Xiaoyu Wang", "Yetao Wu", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Jinxian Qu", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 7 figures", "summary": "Current research efforts are focused on enhancing the thinking and reasoning\ncapability of large language model (LLM) by prompting, data-driven emergence\nand inference-time computation. In this study, we consider stimulating language\nmodel's thinking and cognitive abilities from a modular perspective, which\nmimics the human brain architecture. We select a specific intermediate\nattention layer with newly implemented language heads. We conduct dual-layer\nfine-tuning by annotated (query, thought, answer) samples and show that the\nintermediate layer can also learn to decode fluent and reasonable language\ntokens. A two-pass inference mechanism is designed to generate thoughts then\nformal responses. The entire framework is called modularized thinking language\nmodel (MeTHanol) which can enhance LLM's cognitive behaviors as indicated by\nTheory of Mind (ToM) and Vignette-based experiments. Case studies also show\nthat MeTHanol can plan and self-reflect and generate human-like thoughts and\nanswers, even on unseen and open-domain tasks. MeTHanol can also adapt to a\npersonalized prompt and behave as the specified character. Our study holds\npromise for significant cognitive gains from a modular perspective. Our code,\nmodel and data are available at https://bachozean.github.io/methanol-page"}
{"id": "2410.14651", "pdf": "https://arxiv.org/pdf/2410.14651.pdf", "abs": "https://arxiv.org/abs/2410.14651", "title": "Real-time Factuality Assessment from Adversarial Feedback", "authors": ["Sanxing Chen", "Yukun Huang", "Bhuwan Dhingra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that existing evaluations for assessing the factuality of news from\nconventional sources, such as claims on fact-checking websites, result in high\naccuracies over time for LLM-based detectors-even after their knowledge\ncutoffs. This suggests that recent popular false information from such sources\ncan be easily identified due to its likely presence in pre-training/retrieval\ncorpora or the emergence of salient, yet shallow, patterns in these datasets.\nInstead, we argue that a proper factuality evaluation dataset should test a\nmodel's ability to reason about current events by retrieving and reading\nrelated evidence. To this end, we develop a novel pipeline that leverages\nnatural language feedback from a RAG-based detector to iteratively modify\nreal-time news into deceptive variants that challenge LLMs. Our iterative\nrewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent\nfor a strong RAG-based GPT-4o detector. Our experiments reveal the important\nrole of RAG in both evaluating and generating challenging news examples, as\nretrieval-free LLM detectors are vulnerable to unseen events and adversarial\nattacks, while feedback from RAG-based evaluation helps discover more deceitful\npatterns."}
{"id": "2410.15956", "pdf": "https://arxiv.org/pdf/2410.15956.pdf", "abs": "https://arxiv.org/abs/2410.15956", "title": "Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs", "authors": ["Yanzhu Guo", "Simone Conia", "Zelin Zhou", "Min Li", "Saloni Potdar", "Henry Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs."}
{"id": "2410.23771", "pdf": "https://arxiv.org/pdf/2410.23771.pdf", "abs": "https://arxiv.org/abs/2410.23771", "title": "What is Wrong with Perplexity for Long-context Language Modeling?", "authors": ["Lizhe Fang", "Yifei Wang", "Zhaoyang Liu", "Chenheng Zhang", "Stefanie Jegelka", "Jinyang Gao", "Bolin Ding", "Yisen Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL."}
{"id": "2411.04093", "pdf": "https://arxiv.org/pdf/2411.04093.pdf", "abs": "https://arxiv.org/abs/2411.04093", "title": "Summarization of Opinionated Political Documents with Varied Perspectives", "authors": ["Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": "COLING 2025", "summary": "Global partisan hostility and polarization has increased, and this\npolarization is heightened around presidential elections. Models capable of\ngenerating accurate summaries of diverse perspectives can help reduce such\npolarization by exposing users to alternative perspectives. In this work, we\nintroduce a novel dataset and task for independently summarizing each political\nperspective in a set of passages from opinionated news articles. For this task,\nwe propose a framework for evaluating different dimensions of perspective\nsummary performance. We benchmark 11 summarization models and LLMs of varying\nsizes and architectures through both automatic and human evaluation. While\nrecent models like GPT-4o perform well on this task, we find that all models\nstruggle to generate summaries that are faithful to the intended perspective.\nOur analysis of summaries focuses on how extraction behavior is impacted by\nfeatures of the input documents."}
{"id": "2412.10271", "pdf": "https://arxiv.org/pdf/2412.10271.pdf", "abs": "https://arxiv.org/abs/2412.10271", "title": "Benchmarking Linguistic Diversity of Large Language Models", "authors": ["Yanzhu Guo", "Guokan Shang", "Chloé Clavel"], "categories": ["cs.CL"], "comment": null, "summary": "The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs."}
{"id": "2412.15748", "pdf": "https://arxiv.org/pdf/2412.15748.pdf", "abs": "https://arxiv.org/abs/2412.15748", "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models", "authors": ["Shamus Sim", "Tyrone Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 7 figures, 3 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors", "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, in this work, we adapt the\nexisting concept of reasoning behaviour and articulate its interpretation\nwithin the specific context of medical LLMs. We survey and categorise current\nstate-of-the-art approaches for modeling and evaluating reasoning reasoning in\nmedical LLMs. Additionally, we propose theoretical frameworks which can empower\nmedical professionals or machine learning engineers to gain insight into the\nlow-level reasoning operations of these previously obscure models. We also\noutline key open challenges facing the development of Large Reasoning Models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole."}
{"id": "2412.17063", "pdf": "https://arxiv.org/pdf/2412.17063.pdf", "abs": "https://arxiv.org/abs/2412.17063", "title": "Computational Analysis of Character Development in Holocaust Testimonies", "authors": ["Esther Shizgal", "Eitan Wagner", "Renana Keydar", "Omri Abend"], "categories": ["cs.CL"], "comment": null, "summary": "This work presents a computational approach to analyze character development\nalong the narrative timeline. The analysis characterizes the inner and outer\nchanges the protagonist undergoes within a narrative, and the interplay between\nthem. We consider transcripts of Holocaust survivor testimonies as a test case,\neach telling the story of an individual in first-person terms. We focus on the\nsurvivor's religious trajectory, examining the evolution of their disposition\ntoward religious belief and practice along the testimony. Clustering the\nresulting trajectories in the dataset, we identify common sequences in the\ndata. Our findings highlight multiple common structures of religiosity across\nthe narratives: in terms of belief, most present a constant disposition, while\nfor practice, most present an oscillating structure, serving as valuable\nmaterial for historical and sociological research. This work demonstrates the\npotential of natural language processing techniques for analyzing character\nevolution through thematic trajectories in narratives."}
{"id": "2412.20677", "pdf": "https://arxiv.org/pdf/2412.20677.pdf", "abs": "https://arxiv.org/abs/2412.20677", "title": "Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA", "authors": ["Qingyun Jin", "Xiaohui Song", "Feng Zhou", "Zengchang Qin"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."}
{"id": "2501.06645", "pdf": "https://arxiv.org/pdf/2501.06645.pdf", "abs": "https://arxiv.org/abs/2501.06645", "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings", "authors": ["Tong Liu", "Xiao Yu", "Wenxuan Zhou", "Jindong Gu", "Volker Tresp"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness."}
{"id": "2502.01615", "pdf": "https://arxiv.org/pdf/2502.01615.pdf", "abs": "https://arxiv.org/abs/2502.01615", "title": "Large Language Models Are Human-Like Internally", "authors": ["Tatsuki Kuribayashi", "Yohei Oseki", "Souhaib Ben Taieb", "Kentaro Inui", "Timothy Baldwin"], "categories": ["cs.CL"], "comment": "This is a pre-MIT Press publication version of the paper", "summary": "Recent cognitive modeling studies have reported that larger language models\n(LMs) exhibit a poorer fit to human reading behavior (Oh and Schuler, 2023b;\nShain et al., 2024; Kuribayashi et al., 2024), leading to claims of their\ncognitive implausibility. In this paper, we revisit this argument through the\nlens of mechanistic interpretability and argue that prior conclusions were\nskewed by an exclusive focus on the final layers of LMs. Our analysis reveals\nthat next-word probabilities derived from internal layers of larger LMs align\nwith human sentence processing data as well as, or better than, those from\nsmaller LMs. This alignment holds consistently across behavioral (self-paced\nreading times, gaze durations, MAZE task processing times) and\nneurophysiological (N400 brain potentials) measures, challenging earlier mixed\nresults and suggesting that the cognitive plausibility of larger LMs has been\nunderestimated. Furthermore, we first identify an intriguing relationship\nbetween LM layers and human measures: earlier layers correspond more closely\nwith fast gaze durations, while later layers better align with relatively\nslower signals such as N400 potentials and MAZE processing times. Our work\nopens new avenues for interdisciplinary research at the intersection of\nmechanistic interpretability and cognitive modeling."}
{"id": "2502.03387", "pdf": "https://arxiv.org/pdf/2502.03387.pdf", "abs": "https://arxiv.org/abs/2502.03387", "title": "LIMO: Less is More for Reasoning", "authors": ["Yixin Ye", "Zhen Huang", "Yang Xiao", "Ethan Chern", "Shijie Xia", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "We challenge the prevailing assumption that complex reasoning in large\nlanguage models (LLMs) necessitates massive training data. We demonstrate that\nsophisticated mathematical reasoning can emerge with only a few examples.\nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves\n63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned\nmodels (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the\ntraining data required by prior approaches. Furthermore, LIMO exhibits strong\nout-of-distribution generalization, achieving a 45.8\\% absolute improvement\nacross diverse benchmarks, outperforming models trained on 100x more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis\n(LIMO Hypothesis): In foundation models where domain knowledge has been\ncomprehensively encoded during pre-training, sophisticated reasoning can emerge\nthrough minimal but strategically designed demonstrations of cognitive\nprocesses. This hypothesis suggests that the threshold for eliciting complex\nreasoning is not dictated by task complexity but rather by two key factors: (1)\nthe completeness of the model's pre-trained knowledge base and (2) the\neffectiveness of post-training examples in serving as \"cognitive templates\"\nthat guide reasoning."}
{"id": "2502.11131", "pdf": "https://arxiv.org/pdf/2502.11131.pdf", "abs": "https://arxiv.org/abs/2502.11131", "title": "Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM", "authors": ["Yuqi Liu", "Yan Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https://github.com/liuyuqi123study/RankSVM_for_SLR"}
{"id": "2502.14133", "pdf": "https://arxiv.org/pdf/2502.14133.pdf", "abs": "https://arxiv.org/abs/2502.14133", "title": "Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification", "authors": ["Xuansheng Wu", "Wenhao Yu", "Xiaoming Zhai", "Ninghao Liu"], "categories": ["cs.CL"], "comment": "Accepted by SIGKDD 2025", "summary": "Modern text classification methods heavily rely on contextual embeddings from\nlarge language models (LLMs). Compared to human-engineered features, these\nembeddings provide automatic and effective representations for classification\nmodel training. However, they also introduce a challenge: we lose the ability\nto manually remove unintended features, such as sensitive or task-irrelevant\nfeatures, to guarantee regulatory compliance or improve the generalizability of\nclassification models. This limitation arises because LLM embeddings are opaque\nand difficult to interpret. In this paper, we propose a novel framework to\nidentify and regularize unintended features in the LLM latent space.\nSpecifically, we first pre-train a sparse autoencoder (SAE) to extract\ninterpretable features from LLM latent spaces. To ensure the SAE can capture\ntask-specific features, we further fine-tune it on task-specific datasets. In\ntraining the classification model, we propose a simple and effective\nregularizer, by minimizing the similarity between the classifier weights and\nthe identified unintended feature, to remove the impact of these unintended\nfeatures on classification. We evaluate the proposed framework on three\nreal-world tasks, including toxic chat detection, reward modeling, and disease\ndiagnosis. Results show that the proposed self-regularization framework can\nimprove the classifier's generalizability by regularizing those features that\nare not semantically correlated to the task. This work pioneers controllable\ntext classification on LLM latent spaces by leveraging interpreted features to\naddress generalizability, fairness, and privacy challenges. The code and data\nare publicly available at\nhttps://github.com/JacksonWuxs/Controllable_LLM_Classifier."}
{"id": "2502.18573", "pdf": "https://arxiv.org/pdf/2502.18573.pdf", "abs": "https://arxiv.org/abs/2502.18573", "title": "FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models", "authors": ["Radu Marinescu", "Debarun Bhattacharjya", "Junkyu Lee", "Tigran Tchrakian", "Javier Carnerero Cano", "Yufang Hou", "Elizabeth Daly", "Alessandra Pascale"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated vast capabilities on\ngenerative tasks in recent years, yet they struggle with guaranteeing the\nfactual correctness of the generated content. This makes these models\nunreliable in realistic situations where factually accurate responses are\nexpected. In this paper, we propose FactReasoner, a new factuality assessor\nthat relies on probabilistic reasoning to assess the factuality of a long-form\ngenerated response. Specifically, FactReasoner decomposes the response into\natomic units, retrieves relevant contexts for them from an external knowledge\nsource, and constructs a joint probability distribution over the atoms and\ncontexts using probabilistic encodings of the logical relationships\n(entailment, contradiction) between the textual utterances corresponding to the\natoms and contexts. FactReasoner then computes the posterior probability of\nwhether atomic units in the response are supported by the retrieved contexts.\nOur experiments on labeled and unlabeled benchmark datasets demonstrate clearly\nthat FactReasoner improves considerably over state-of-the-art prompt-based\napproaches in terms of both factual precision and recall."}
{"id": "2503.08026", "pdf": "https://arxiv.org/pdf/2503.08026.pdf", "abs": "https://arxiv.org/abs/2503.08026", "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents", "authors": ["Zhen Tan", "Jun Yan", "I-Hung Hsu", "Rujun Han", "Zifeng Wang", "Long T. Le", "Yiwen Song", "Yanfei Chen", "Hamid Palangi", "George Lee", "Anand Iyer", "Tianlong Chen", "Huan Liu", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Large Language Models (LLMs) have made significant progress in open-ended\ndialogue, yet their inability to retain and retrieve relevant information from\nlong-term interactions limits their effectiveness in applications requiring\nsustained personalization. External memory mechanisms have been proposed to\naddress this limitation, enabling LLMs to maintain conversational continuity.\nHowever, existing approaches struggle with two key challenges. First, rigid\nmemory granularity fails to capture the natural semantic structure of\nconversations, leading to fragmented and incomplete representations. Second,\nfixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user\ninteraction patterns. In this work, we propose Reflective Memory Management\n(RMM), a novel mechanism for long-term dialogue agents, integrating forward-\nand backward-looking reflections: (1) Prospective Reflection, which dynamically\nsummarizes interactions across granularities-utterances, turns, and\nsessions-into a personalized memory bank for effective future retrieval, and\n(2) Retrospective Reflection, which iteratively refines the retrieval in an\nonline reinforcement learning (RL) manner based on LLMs' cited evidence.\nExperiments show that RMM demonstrates consistent improvement across various\nmetrics and benchmarks. For example, RMM shows more than 10% accuracy\nimprovement over the baseline without memory management on the LongMemEval\ndataset."}
{"id": "2503.10789", "pdf": "https://arxiv.org/pdf/2503.10789.pdf", "abs": "https://arxiv.org/abs/2503.10789", "title": "Data Caricatures: On the Representation of African American Language in Pretraining Corpora", "authors": ["Nicholas Deas", "Blake Vente", "Amith Ananthram", "Jessica A. Grieser", "Desmond Patton", "Shana Kleiner", "James Shepard", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "With a combination of quantitative experiments, human judgments, and\nqualitative analyses, we evaluate the quantity and quality of African American\nLanguage (AAL) representation in 12 predominantly English, open-source\npretraining corpora. We specifically focus on the sources, variation, and\nnaturalness of included AAL texts representing the AAL-speaking community. We\nfind that AAL is underrepresented in all evaluated pretraining corpora compared\nto US demographics, constituting as few as 0.007% and at most 0.18% of\ndocuments. We also find that more than 25% of AAL texts in C4 may be perceived\nas inappropriate for LLMs to generate and to reinforce harmful stereotypes.\nFinally, we find that most automated filters are more likely to conserve White\nMainstream English (WME) texts over AAL in pretraining corpora."}
{"id": "2503.11657", "pdf": "https://arxiv.org/pdf/2503.11657.pdf", "abs": "https://arxiv.org/abs/2503.11657", "title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs", "authors": ["Vincent Li", "Tim Knappe", "Yule Fu", "Kevin Han", "Kevin Zhu"], "categories": ["cs.CL"], "comment": "Accepted to ICML AI4Math Workshop 2025, NAACL SRW 2025", "summary": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing tasks requiring multi-step logical reasoning capabilities,\nsuch as automated theorem proving. However, challenges persist within theorem\nproving, such as the identification of key mathematical concepts, understanding\ntheir interrelationships, and formalizing proofs correctly within natural\nlanguage. We present KG-prover, a novel framework that leverages knowledge\ngraphs mined from reputable mathematical texts to augment general-purpose LLMs\nto construct and formalize mathematical proofs. We also study the effects of\nscaling graph-based, test-time compute using KG-Prover, demonstrating\nsignificant performance improvements over baselines across multiple datasets.\nGeneral-purpose LLMs improve up to 21\\% on miniF2F-test when combined with\nKG-Prover, with consistent improvements ranging from 2-11\\% on the ProofNet,\nminiF2F-test, and MUSTARD datasets without additional scaling. Furthermore,\nKG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a\npromising approach for augmenting natural language proof reasoning with\nknowledge graphs without the need for additional finetuning."}
{"id": "2503.12370", "pdf": "https://arxiv.org/pdf/2503.12370.pdf", "abs": "https://arxiv.org/abs/2503.12370", "title": "Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs", "authors": ["Rupak Sarkar", "Neha Srikanth", "Taylor Hudson", "Rachel Rudinger", "Claire Bonial", "Philip Resnik"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "While it is commonly accepted that maintaining common ground plays a role in\nconversational success, little prior research exists connecting conversational\ngrounding to success in task-oriented conversations. We study failures of\ngrounding in the Ubuntu IRC dataset, where participants use text-only\ncommunication to resolve technical issues. We find that disruptions in\nconversational flow often stem from a misalignment in common ground, driven by\na divergence in beliefs and assumptions held by participants. These\ndisruptions, which we call conversational friction, significantly correlate\nwith task success. We find that although LLMs can identify overt cases of\nconversational friction, they struggle with subtler and more context-dependent\ninstances requiring pragmatic or domain-specific reasoning."}
{"id": "2503.12854", "pdf": "https://arxiv.org/pdf/2503.12854.pdf", "abs": "https://arxiv.org/abs/2503.12854", "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation", "authors": ["Songjun Tu", "Jiahao Lin", "Xiangyu Tian", "Qichao Zhang", "Linjing Li", "Yuqian Fu", "Nan Xu", "Wei He", "Xiangyuan Lan", "Dongmei Jiang", "Dongbin Zhao"], "categories": ["cs.CL"], "comment": "23pages", "summary": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations."}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288.pdf", "abs": "https://arxiv.org/abs/2503.18288", "title": "TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource Language Modeling", "authors": ["Cheng Huang", "Fan Gao", "Yutong Liu", "Nyima Tashi", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Advancement of large language models (LLMs) has brought transformative\ncapabilities to NLP, but such progress remains unevenly distributed, especially\nfor low-resource and culturally rich languages like Tibetan. In this paper, we\npresent TIB-STC, the first large-scale, expert-curated, and multi-domain\nbenchmark specifically designed to support the development and evaluation of\nLLMs for the Tibetan language. Spanning over 11 billion tokens across\nliterature, religion, medicine, law, and daily communication, TIB-STC preserves\ntraditional grammar and stylistic richness. To validate its utility, we train a\nreference model, Sun-Shine, on TIB-STC through a three-stage pipeline involving\npretraining, supervised fine-tuning, and preference optimization. Evaluation on\nTLUE Benchmark for Tibetan-specific tasks, including Ti-MMLU and\nTi-SafetyBench, demonstrates the benchmark's effectiveness in enabling robust\ninstruction-following and culturally aligned generation. We release TIB-STC to\nadvance research in low-resource language modeling and promote inclusivity in\nmultilingual NLP. All data are available at:\nhttps://github.com/Vicentvankor/sun-shine"}
{"id": "2503.22040", "pdf": "https://arxiv.org/pdf/2503.22040.pdf", "abs": "https://arxiv.org/abs/2503.22040", "title": "Navigating the Risks of Using Large Language Models for Text Annotation in Social Science Research", "authors": ["Hao Lin", "Yongjun Zhang"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize\ncomputational social science, particularly in automated textual analysis. In\nthis paper, we conduct a systematic evaluation of the promises and risks\nassociated with using LLMs for text classification tasks, using social movement\nstudies as an example. We propose a framework for social scientists to\nincorporate LLMs into text annotation, either as the primary coding\ndecision-maker or as a coding assistant. This framework offers researchers\ntools to develop the potential best-performing prompt, and to systematically\nexamine and report the validity and reliability of LLMs as a methodological\ntool. Additionally, we evaluate and discuss its epistemic risks associated with\nvalidity, reliability, replicability, and transparency. We conclude with\nseveral practical guidelines for using LLMs in text annotation tasks and offer\nrecommendations for more effectively communicating epistemic risks in research."}
{"id": "2504.02398", "pdf": "https://arxiv.org/pdf/2504.02398.pdf", "abs": "https://arxiv.org/abs/2504.02398", "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "authors": ["Gallil Maimon", "Michael Hassid", "Amit Roth", "Yossi Adi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at COLM 2025", "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nIt predicts that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this\npaper we answer a resounding yes! We conduct scaling analysis of interleaved\nSLMs by training several dozen and analysing the scaling trends. We see that\nunder this setup SLMs scale more efficiently with compute. Additionally, our\nresults indicate that the scaling dynamics significantly differ from\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget to increasing model size over training tokens. We also study the role of\nsynthetic data and TextLM model families in unlocking this potential. Results\nsuggest that our scaled up model achieves comparable semantic speech\nperformance to leading models, while using less compute and data. We open\nsource models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims/ ."}
{"id": "2504.07274", "pdf": "https://arxiv.org/pdf/2504.07274.pdf", "abs": "https://arxiv.org/abs/2504.07274", "title": "Language Modeling for the Future of Finance: A Survey into Metrics, Tasks, and Data Opportunities", "authors": ["Nikita Tatarinov", "Siddhant Sukhani", "Agam Shah", "Sudheer Chava"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in language modeling have led to growing interest in applying\nNatural Language Processing (NLP) techniques to financial problems, enabling\nnew approaches to analysis and decision-making. To systematically examine this\ntrend, we review 374 NLP research papers published between 2017 and 2024 across\n38 conferences and workshops, with a focused analysis of 221 papers that\ndirectly address finance-related tasks. We evaluate these papers across 11\nquantitative and qualitative dimensions, and our study identifies the following\nopportunities: (i) expanding the scope of forecasting tasks; (ii) enriching\nevaluation with financial metrics; (iii) leveraging multilingual and\ncrisis-period datasets; and (iv) balancing PLMs with efficient or interpretable\nalternatives. We identify actionable directions for research and practice,\nsupported by dataset and tool recommendations, with implications for both the\nacademia and industry communities."}
{"id": "2504.12549", "pdf": "https://arxiv.org/pdf/2504.12549.pdf", "abs": "https://arxiv.org/abs/2504.12549", "title": "Memorization: A Close Look at Books", "authors": ["Iris Ma", "Ian Domingo", "Alberto Krone-Martins", "Pierre Baldi", "Cristina V. Lopes"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025 L2M2 Workshop", "summary": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562.pdf", "abs": "https://arxiv.org/abs/2504.17562", "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2505.02456", "pdf": "https://arxiv.org/pdf/2505.02456.pdf", "abs": "https://arxiv.org/abs/2505.02456", "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs", "authors": ["Elisa Forcada Rodríguez", "Olatz Perez-de-Viñaspre", "Jon Ander Campos", "Dietrich Klakow", "Vagrant Gautam"], "categories": ["cs.CL"], "comment": "Workshop on Gender Bias in Natural Language Processing at ACL 2025", "summary": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work."}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572.pdf", "abs": "https://arxiv.org/abs/2505.12572", "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Reconstruction:The Optimal Expansion Ratio", "authors": ["Hanwen Shen", "Ting Ying"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "A two stage novel generation framework (outline -> section outline ->\nmanuscript) is widely used in long novel generation,(e.g., \\textsc{DOME},\n\\textsc{Plan\\&Write}, \\textsc{Long Writer}), but study of such framework in\nultra long novel(>1M words) reconstruction is little. Building on recent text\ncompression methods (\\textsc{LLMZip}, \\textsc{LLM2Vec}), we conduct an\ninformation-theoretic analysis to quantify semantic distortion under different\ncompression-expansion ratios. We examine how outline length affects information\npreservation. Experiments on ultra-long novels show that the optimal\ncompression-expansion ratio significantly reduces semantic distortion compared\nto other non-optimal compression-expansion ratio."}
{"id": "2505.15075", "pdf": "https://arxiv.org/pdf/2505.15075.pdf", "abs": "https://arxiv.org/abs/2505.15075", "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs", "authors": ["Hao Wang", "Pinzhi Huang", "Jihan Yang", "Saining Xie", "Daisuke Kawahara"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "https://github.com/nlp-waseda/traveling-across-languages", "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models."}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789.pdf", "abs": "https://arxiv.org/abs/2505.16789", "title": "Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) gain popularity, their vulnerability to\nadversarial attacks emerges as a primary concern. While fine-tuning models on\ndomain-specific datasets is often employed to improve model performance, it can\ninadvertently introduce vulnerabilities within the underlying model. In this\nwork, we investigate Accidental Vulnerability, unexpected vulnerabilities\narising from characteristics of fine-tuning data. We begin by identifying\npotential correlation factors such as linguistic features, semantic similarity,\nand toxicity across multiple experimental datasets. We then evaluate the\nadversarial robustness of these fine-tuned models, analyzing persona shifts and\ninterpretability traits to understand how dataset factors contribute to attack\nsuccess rates. Lastly, we explore causal relationships that offer new insights\ninto adversarial defense strategies, highlighting the crucial role of dataset\ndesign in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_vulnerability."}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067.pdf", "abs": "https://arxiv.org/abs/2505.17067", "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "IEEE Global Communications Conference (GlobeCom) 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection."}
{"id": "2505.17137", "pdf": "https://arxiv.org/pdf/2505.17137.pdf", "abs": "https://arxiv.org/abs/2505.17137", "title": "Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands", "authors": ["Kristin Qi", "Youxiang Zhu", "Caroline Summerour", "John A. Batsis", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI"], "comment": "IEEE Global Communications Conference (GlobeCom) 2025", "summary": "Early detection of cognitive decline is crucial for enabling interventions\nthat can slow neurodegenerative disease progression. Traditional diagnostic\napproaches rely on labor-intensive clinical assessments, which are impractical\nfor frequent monitoring. Our pilot study investigates voice assistant systems\n(VAS) as non-invasive tools for detecting cognitive decline through\nlongitudinal analysis of speech patterns in voice commands. Over an 18-month\nperiod, we collected voice commands from 35 older adults, with 15 participants\nproviding daily at-home VAS interactions. To address the challenges of\nanalyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a\nframework that combines (1) LLM-driven iterative prompt refinement for\nlinguistic feature extraction, (2) HuBERT-based acoustic feature extraction,\nand (3) transformer-based temporal modeling. Using iTransformer, our approach\nachieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming\nits baseline by 27.13%. Through our LLM approach, we identify linguistic\nfeatures that uniquely characterize everyday command usage patterns in\nindividuals experiencing cognitive decline."}
{"id": "2506.00022", "pdf": "https://arxiv.org/pdf/2506.00022.pdf", "abs": "https://arxiv.org/abs/2506.00022", "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "authors": ["Shenghe Zheng", "Qianjia Cheng", "Junchi Yao", "Mengsong Wu", "Haonan He", "Ning Ding", "Yu Cheng", "Shuyue Hu", "Lei Bai", "Dongzhan Zhou", "Ganqu Cui", "Peng Ye"], "categories": ["cs.CL", "cs.LG", "physics.ed-ph"], "comment": "Work on physical datasets", "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics."}
{"id": "2506.01840", "pdf": "https://arxiv.org/pdf/2506.01840.pdf", "abs": "https://arxiv.org/abs/2506.01840", "title": "Minimal Pair-Based Evaluation of Code-Switching", "authors": ["Igor Sterner", "Simone Teufel"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "There is a lack of an evaluation methodology that estimates the extent to\nwhich large language models (LLMs) use code-switching (CS) in the same way as\nbilinguals. Existing methods do not have wide language coverage, fail to\naccount for the diverse range of CS phenomena, or do not scale. We propose an\nintervention based on minimal pairs of CS. Each minimal pair contains one\nnaturally occurring CS sentence and one minimally manipulated variant. We\ncollect up to 1,000 such pairs each for 11 language pairs. Our human\nexperiments show that, for every language pair, bilinguals consistently prefer\nthe naturally occurring CS sentence. Meanwhile our experiments with current\nLLMs show that the larger the model, the more consistently it assigns higher\nprobability to the naturally occurring CS sentence than to the variant. In\naccordance with theoretical claims, the largest probability differences arise\nin those pairs where the manipulated material consisted of closed-class words."}
{"id": "2506.01846", "pdf": "https://arxiv.org/pdf/2506.01846.pdf", "abs": "https://arxiv.org/abs/2506.01846", "title": "Code-Switching and Syntax: A Large-Scale Experiment", "authors": ["Igor Sterner", "Simone Teufel"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "The theoretical code-switching (CS) literature provides numerous pointwise\ninvestigations that aim to explain patterns in CS, i.e. why bilinguals switch\nlanguage in certain positions in a sentence more often than in others. A\nresulting consensus is that CS can be explained by the syntax of the\ncontributing languages. There is however no large-scale, multi-language,\ncross-phenomena experiment that tests this claim. When designing such an\nexperiment, we need to make sure that the system that is predicting where\nbilinguals tend to switch has access only to syntactic information. We provide\nsuch an experiment here. Results show that syntax alone is sufficient for an\nautomatic system to distinguish between sentences in minimal pairs of CS, to\nthe same degree as bilingual humans. Furthermore, the learnt syntactic patterns\ngeneralise well to unseen language pairs."}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005.pdf", "abs": "https://arxiv.org/abs/2506.02005", "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": "10 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages."}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853.pdf", "abs": "https://arxiv.org/abs/2506.09853", "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness."}
{"id": "2506.12327", "pdf": "https://arxiv.org/pdf/2506.12327.pdf", "abs": "https://arxiv.org/abs/2506.12327", "title": "Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective", "authors": ["Hitomi Yanaka", "Xinqi He", "Jie Lu", "Namgi Han", "Sunjin Oh", "Ryoma Kumon", "Yuma Matsuoka", "Katsuhiko Watabe", "Yuko Itatsu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025", "summary": "An increasing number of studies have examined the social bias of rapidly\ndeveloped large language models (LLMs). Although most of these studies have\nfocused on bias occurring in a single social attribute, research in social\nscience has shown that social bias often occurs in the form of\nintersectionality -- the constitutive and contextualized perspective on bias\naroused by social attributes. In this study, we construct the Japanese\nbenchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on\nthe question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow,\nwe find that biased output varies according to its contexts even with the equal\ncombination of social attributes."}
{"id": "2506.13610", "pdf": "https://arxiv.org/pdf/2506.13610.pdf", "abs": "https://arxiv.org/abs/2506.13610", "title": "A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy", "authors": ["Abdullah Al Shafi", "Rowzatul Zannat", "Abdul Muntakim", "Mahmudul Hasan"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value, indicating whether a symptom is\nassociated with a disease. Thereby, this structured representation makes the\ndataset very useful for a wide range of applications, including machine\nlearning-based disease prediction, clinical decision support systems, and\nepidemiological studies. Although there are some advancements in the field of\ndisease-symptom datasets, there is a significant gap in structured datasets for\nthe Bangla language. This dataset aims to bridge that gap by facilitating the\ndevelopment of multilingual medical informatics tools and improving disease\nprediction models for underrepresented linguistic communities. Further\ndevelopments should include region-specific diseases and further fine-tuning of\nsymptom associations for better diagnostic performance"}
{"id": "2506.21560", "pdf": "https://arxiv.org/pdf/2506.21560.pdf", "abs": "https://arxiv.org/abs/2506.21560", "title": "Reinforcement learning fine-tuning of language model for instruction following and math reasoning", "authors": ["Yifu Han", "Geo Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models."}
{"id": "2506.21613", "pdf": "https://arxiv.org/pdf/2506.21613.pdf", "abs": "https://arxiv.org/abs/2506.21613", "title": "ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech", "authors": ["Gautam Siddharth Kashyap", "Mohammad Anas Azeez", "Rafiq Ali", "Zohaib Hasan Siddiqui", "Jiechao Gao", "Usman Naseem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Updated Version", "summary": "Hate speech targeting children on social media is a serious and growing\nproblem, yet current NLP systems struggle to detect it effectively. This gap\nexists mainly because existing datasets focus on adults, lack age specific\nlabels, miss nuanced linguistic cues, and are often too small for robust\nmodeling. To address this, we introduce ChildGuard, the first large scale\nEnglish dataset dedicated to hate speech aimed at children. It contains 351,877\nannotated examples from X (formerly Twitter), Reddit, and YouTube, labeled by\nthree age groups: younger children (under 11), pre teens (11--12), and teens\n(13--17). The dataset is split into two subsets for fine grained analysis: a\ncontextual subset (157K) focusing on discourse level features, and a lexical\nsubset (194K) emphasizing word-level sentiment and vocabulary. Benchmarking\nstate of the art hate speech models on ChildGuard reveals notable drops in\nperformance, highlighting the challenges of detecting child directed hate\nspeech."}
{"id": "2507.02984", "pdf": "https://arxiv.org/pdf/2507.02984.pdf", "abs": "https://arxiv.org/abs/2507.02984", "title": "From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought", "authors": ["Wentao Tan", "Qiong Cao", "Yibing Zhan", "Chao Xue", "Changxing Ding"], "categories": ["cs.CL"], "comment": null, "summary": "Achieving human-like reasoning capabilities in Multimodal Large Language\nModels (MLLMs) has long been a goal. Current methods primarily focus on\nsynthesizing positive rationales, typically relying on manual annotations or\ncomplex systems. Moreover, they often overlook negative reasoning, which limits\nthe model's generalization ability and robustness in multimodal inference. To\naddress this gap, we propose a novel framework: \\textbf{S}elf-Aligning\n\\textbf{M}ultimodal Reasoning with \\textbf{A}nswer-O\\textbf{r}iented\nChain-of-\\textbf{T}hought (SMART). SMART employs an answer-oriented\nchain-of-thought (AoT) prompt to automatically construct high-quality data.\nDrawing inspiration from human proof-based strategies, AoT leverages both\ncorrect and incorrect answers to extract key visual information that links\nquestions and answers. When provided with correct answers, the model produces\nstrong positive rationales. Conversely, when correct answers are replaced with\nincorrect alternatives, the model generates an erroneous yet compelling\nreasoning path, serving as a form of discriminative negative rationale. Models\ntrained with AoT-generated data outperform those trained on manually annotated\ndatasets, demonstrating superior reasoning capabilities. Consequently, SMART\nestablishes an iterative generation-optimization method that continually\nenhances the model's reasoning skills. Experiments indicate that the SMART\nframework significantly improves various MLLMs, regardless of model\narchitecture, parameter size, or pre-training dataset. The code is available at\nhttps://github.com/WentaoTan/SMART."}
{"id": "2507.04886", "pdf": "https://arxiv.org/pdf/2507.04886.pdf", "abs": "https://arxiv.org/abs/2507.04886", "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations", "authors": ["A. Bochkov"], "categories": ["cs.CL", "cs.AI"], "comment": "Added a new Ablation Study section with a key experiment on random\n  noise embeddings. Expanded the discussion on 'representational interference'\n  and updated results and figures accordingly", "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch."}
{"id": "2507.06774", "pdf": "https://arxiv.org/pdf/2507.06774.pdf", "abs": "https://arxiv.org/abs/2507.06774", "title": "Checklist Engineering Empowers Multilingual LLM Judges", "authors": ["Mohammad Ghiasvand Mohammadkhani", "Hamid Beigy"], "categories": ["cs.CL"], "comment": null, "summary": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model."}
{"id": "2507.07307", "pdf": "https://arxiv.org/pdf/2507.07307.pdf", "abs": "https://arxiv.org/abs/2507.07307", "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted for publication at COLM 2025", "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, cross\nevaluations show that our system generalizes well across diverse health\nmisinformation topics and datasets. And human evaluations reveal that\nrefinement significantly enhances counterspeech quality and obtains human\npreference."}
{"id": "2507.09205", "pdf": "https://arxiv.org/pdf/2507.09205.pdf", "abs": "https://arxiv.org/abs/2507.09205", "title": "Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Yuqi Ren", "Deyi Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model to enhance its generative capabilities in Tibetan. To evaluate the\nTibetan capabilities of the model, we create new high-quality Tibetan\nbenchmarks, and complement them with existing public benchmarks. Experimental\nresults demonstrate that our model consistently and significantly outperforms\nboth open-source models of similar scale and Tibetan-tailored models across a\nwide range of tasks."}
{"id": "2507.11936", "pdf": "https://arxiv.org/pdf/2507.11936.pdf", "abs": "https://arxiv.org/abs/2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps."}
{"id": "2507.13138", "pdf": "https://arxiv.org/pdf/2507.13138.pdf", "abs": "https://arxiv.org/abs/2507.13138", "title": "Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation", "authors": ["Hadi Mohammadi", "Tina Shahedi", "Pablo Mosteiro", "Massimo Poesio", "Ayoub Bagheri", "Anastasia Giachanou"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation."}
{"id": "2507.15586", "pdf": "https://arxiv.org/pdf/2507.15586.pdf", "abs": "https://arxiv.org/abs/2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "16 pages, 7 Figures, 10 Tables", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."}
{"id": "2507.16802", "pdf": "https://arxiv.org/pdf/2507.16802.pdf", "abs": "https://arxiv.org/abs/2507.16802", "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning", "authors": ["Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Xiaoke Zhao", "Zhaowen Zhou", "Jingze Song", "Bo Zhang", "Jiawei Liu", "Xiang Qi", "Zhe Li", "Zhiqiang Zhang", "Wei Wang", "Peng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova."}
{"id": "2507.17527", "pdf": "https://arxiv.org/pdf/2507.17527.pdf", "abs": "https://arxiv.org/abs/2507.17527", "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Yujiao Du", "Ting Han", "Yuxiang Hu", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Jun Zhang", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability."}
{"id": "2507.18190", "pdf": "https://arxiv.org/pdf/2507.18190.pdf", "abs": "https://arxiv.org/abs/2507.18190", "title": "TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks", "authors": ["Keyu Wu", "Qianjin Yu", "Manlin Mei", "Ruiting Liu", "Jun Wang", "Kailai Zhang", "Yelun Bao"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Root Cause Analysis (RCA) in telecommunication networks is a critical task,\nyet it presents a formidable challenge for Artificial Intelligence (AI) due to\nits complex, graph-based reasoning requirements and the scarcity of realistic\nbenchmarks."}
{"id": "2507.19396", "pdf": "https://arxiv.org/pdf/2507.19396.pdf", "abs": "https://arxiv.org/abs/2507.19396", "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study", "authors": ["Rachel M. Murphy", "Nishant Mishra", "Nicolette F. de Keizer", "Dave A. Dongelmans", "Kitty J. Jager", "Ameen Abu-Hanna", "Joanna E. Klopotowska", "Iacer Calixto"], "categories": ["cs.CL"], "comment": "30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).\n  Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.\n  Klopotowska and Iacer Calixto are shared last authors", "summary": "In this study, we establish a benchmark for adverse drug event (ADE)\ndetection in Dutch clinical free-text documents using several transformer\nmodels, clinical scenarios, and fit-for-purpose performance measures. We\ntrained a Bidirectional Long Short-Term Memory (Bi-LSTM) model and four\ntransformer-based Dutch and/or multilingual encoder models (BERTje, RobBERT,\nMedRoBERTa(.)nl, and NuNER) for the tasks of named entity recognition (NER) and\nrelation classification (RC) using 102 richly annotated Dutch ICU clinical\nprogress notes. Anonymized free-text clinical progress notes of patients\nadmitted to the intensive care unit (ICU) of one academic hospital and\ndischarge letters of patients admitted to Internal Medicine wards of two\nnon-academic hospitals were reused. We evaluated our ADE RC models internally\nusing the gold standard (two-step task) and predicted entities (end-to-end\ntask). In addition, all models were externally validated for detecting ADEs at\nthe document level. We report both micro- and macro-averaged F1 scores, given\nthe dataset imbalance in ADEs. Although differences for the ADE RC task between\nthe models were small, MedRoBERTa(.)nl was the best performing model with a\nmacro-averaged F1 score of 0.63 using the gold standard and 0.62 using\npredicted entities. The MedRoBERTa(.)nl models also performed the best in our\nexternal validation and achieved a recall of between 0.67 to 0.74 using\npredicted entities, meaning between 67 to 74% of discharge letters with ADEs\nwere detected. Our benchmark study presents a robust and clinically meaningful\napproach for evaluating language models for ADE detection in clinical free-text\ndocuments. Our study highlights the need to use appropriate performance\nmeasures fit for the task of ADE detection in clinical free-text documents and\nenvisioned future clinical use."}
{"id": "2305.03726", "pdf": "https://arxiv.org/pdf/2305.03726.pdf", "abs": "https://arxiv.org/abs/2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "authors": ["Bo Li", "Yuanhan Zhang", "Liangyu Chen", "Jinghao Wang", "Fanyi Pu", "Joshua Adrian Cahyono", "Jingkang Yang", "Ziwei Liu"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), 2025", "summary": "Recent advances in Large Multimodal Models (LMMs) have unveiled great\npotential as visual assistants. However, most existing works focus on\nresponding to individual instructions or using previous dialogues for\ncontextual understanding. There is little discussion on employing both images\nand text as in-context examples to enhance the instruction following\ncapability.\n  To bridge this gap, we introduce the \\textbf{Otter} model to leverage both\ntextual and visual in-context examples for instruction tuning. Specifically,\nOtter builds upon Flamingo with Perceiver architecture, and has been\ninstruction tuned for general purpose multi-modal assistant. Otter seamlessly\nprocesses multi-modal inputs, supporting modalities including text, multiple\nimages, and dynamic video content. To support the training of Otter, we present\nthe \\textbf{MIMIC-IT} (\\textbf{M}ult\\textbf{I}-\\textbf{M}odal\n\\textbf{I}n-\\textbf{C}ontext \\textbf{I}nstruction \\textbf{T}uning) dataset,\nwhich encompasses over 3 million multi-modal instruction-response pairs,\nincluding approximately 2.2 million unique instructions across a broad spectrum\nof images and videos. MIMIC-IT has been carefully curated to feature a diverse\narray of in-context examples for each entry.\n  Comprehensive evaluations suggest that instruction tuning with these\nin-context examples substantially enhances model convergence and generalization\ncapabilities. Notably, the extensive scenario coverage provided by the MIMIC-IT\ndataset empowers the Otter model to excel in tasks involving complex video and\nmulti-image understanding."}
{"id": "2406.14917", "pdf": "https://arxiv.org/pdf/2406.14917.pdf", "abs": "https://arxiv.org/abs/2406.14917", "title": "LLM2TEA: An Agentic AI Designer for Discovery with Generative Evolutionary Multitasking", "authors": ["Melvin Wong", "Jiao Liu", "Thiago Rios", "Stefan Menzel", "Yew Soon Ong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "comment": "This work is accepted by IEEE CIM. IEEE copyrights applies", "summary": "This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask\nEvolutionary Algorithm, representing the first agentic AI designer of its kind\noperating with generative evolutionary multitasking (GEM). LLM2TEA enables the\ncrossbreeding of solutions from multiple domains, fostering novel solutions\nthat transcend disciplinary boundaries. Of particular interest is the ability\nto discover designs that are both novel and conforming to real-world physical\nspecifications. LLM2TEA comprises an LLM to generate genotype samples from text\nprompts describing target objects, a text-to-3D generative model to produce\ncorresponding phenotypes, a classifier to interpret its semantic\nrepresentations, and a computational simulator to assess its physical\nproperties. Novel LLM-based multitask evolutionary operators are introduced to\nguide the search towards high-performing, practically viable designs.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of\nnovel designs over the current text-to-3D baseline. Moreover, over 73% of the\ngenerated designs outperform the top 1% of designs produced by the text-to-3D\nbaseline in terms of physical performance. The designs produced by LLM2TEA are\nnot only aesthetically creative but also functional in real-world contexts.\nSeveral of these designs have been successfully 3D printed, demonstrating the\nability of our approach to transform AI-generated outputs into tangible,\nphysical designs. These designs underscore the potential of LLM2TEA as a\npowerful tool for complex design optimization and discovery, capable of\nproducing novel and physically viable designs."}
{"id": "2408.10631", "pdf": "https://arxiv.org/pdf/2408.10631.pdf", "abs": "https://arxiv.org/abs/2408.10631", "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models", "authors": ["Yupeng Su", "Ziyi Guan", "Xiaoqun Liu", "Tianlai Jin", "Dongkuan Wu", "Zhengfei Chen", "Graziano Chesi", "Ngai Wong", "Hao Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ICCAD 2025", "summary": "Large language models (LLMs) have seen substantial growth, necessitating\nefficient model pruning techniques. Existing post-training pruning methods\nprimarily measure weight importance in converged dense models, often\noverlooking changes in weight significance during the pruning process, leading\nto performance degradation. To address this issue, we present LLM-Barber\n(Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning\nframework that rebuilds the sparsity mask of pruned models without any\nretraining or weight reconstruction. LLM-Barber incorporates block-aware error\noptimization across Self-Attention and MLP blocks, facilitating global\nperformance optimization. We are the first to employ the product of weights and\ngradients as a pruning metric in the context of LLM post-training pruning. This\nenables accurate identification of weight importance in massive models and\nsignificantly reduces computational complexity compared to methods using\nsecondorder information. Our experiments show that LLM-Barber efficiently\nprunes models from LLaMA and OPT families (7B to 13B) on a single A100 GPU in\njust 30 minutes, achieving state-of-the-art results in both perplexity and\nzero-shot performance across various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber."}
{"id": "2411.10503", "pdf": "https://arxiv.org/pdf/2411.10503.pdf", "abs": "https://arxiv.org/abs/2411.10503", "title": "Everything is a Video: Unifying Modalities through Next-Frame Prediction", "authors": ["G. Thomas Hudson", "Dean Slack", "Thomas Winterbottom", "Jamie Sterling", "Chenghao Xiao", "Junjie Shentu", "Noura Al Moubayed"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "10 pages, 10 figures", "summary": "Multimodal learning, which involves integrating information from various\nmodalities such as text, images, audio, and video, is pivotal for numerous\ncomplex tasks like visual question answering, cross-modal retrieval, and\ncaption generation. Traditional approaches rely on modality-specific encoders\nand late fusion techniques, which can hinder scalability and flexibility when\nadapting to new tasks or modalities. To address these limitations, we introduce\na novel framework that extends the concept of task reformulation beyond natural\nlanguage processing (NLP) to multimodal learning. We propose to reformulate\ndiverse multimodal tasks into a unified next-frame prediction problem, allowing\na single model to handle different modalities without modality-specific\ncomponents. This method treats all inputs and outputs as sequential frames in a\nvideo, enabling seamless integration of modalities and effective knowledge\ntransfer across tasks. Our approach is evaluated on a range of tasks, including\ntext-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text,\ndemonstrating the model's ability to generalize across modalities with minimal\nadaptation. We show that task reformulation can significantly simplify\nmultimodal model design across various tasks, laying the groundwork for more\ngeneralized multimodal foundation models."}
{"id": "2412.05167", "pdf": "https://arxiv.org/pdf/2412.05167.pdf", "abs": "https://arxiv.org/abs/2412.05167", "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models", "authors": ["Kuofeng Gao", "Shu-Tao Xia", "Ke Xu", "Philip Torr", "Jindong Gu"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by ACL 2025", "summary": "Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked\naudio dialogue capabilities, enabling direct spoken exchanges with humans. The\npotential of LALMs broadens their applicability across a wide range of\npractical scenarios supported by audio dialogues. However, given these\nadvancements, a comprehensive benchmark to evaluate the performance of LALMs in\nthe open-ended audio dialogue understanding remains absent currently. To\naddress this gap, we propose an Audio Dialogue Understanding Benchmark\n(ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended\naudio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9\nmultilingual languages, and 4 categories of ambiguity handling. Notably, we\nfirstly propose the evaluation of ambiguity handling in audio dialogues that\nexpresses different intentions beyond the same literal meaning of sentences,\ne.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes\nover 20,000 open-ended audio dialogues for the assessment of LALMs. Through\nextensive experiments on 16 LALMs, our analysis reveals that existing LALMs\nstruggle with mathematical symbols and formulas, understanding human behavior\nsuch as roleplay, comprehending multiple languages, and handling audio dialogue\nambiguities from different phonetic elements, such as intonations, pause\npositions, and homophones. The benchmark is available at\nhttps://adu-bench.github.io/."}
{"id": "2502.10505", "pdf": "https://arxiv.org/pdf/2502.10505.pdf", "abs": "https://arxiv.org/abs/2502.10505", "title": "Preference learning made easy: Everything should be understood through win rate", "authors": ["Lily H. Zhang", "Rajesh Ranganath"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Preference learning, or the task of aligning generative models to preference\ncomparison data, has yet to reach the conceptual maturity of classification,\ndensity estimation, etc. To close this gap, this work presents a framework to\nunderstand preference learning starting from the sampling distribution of\npairwise preference data. First, we prove that the only evaluation of a\ngenerative model that respects both preferences and prevalences in the data\ndistribution is a form of win rate, justifying win rate as the focal point to\nunderstand preference learning. We then analyze preference learning methods as\nwin rate optimization (WRO) or non-WRO. We present novel instances of WRO\nbeyond existing examples (RLHF, NLHF) and identify two key theoretical benefits\nof all such methods. We prove that common non-WRO methods like DPO and SFT on\npreferred samples lack these properties and suggest ways to mitigate such\ntheoretical limitations. We also show that WRO underperforms in practice due\noptimization difficulties and that optimization success predicts performance\nbetter than choices which affect the objective's solution. Our analysis\nhighlights best practices for existing methods and provides recommendations for\nfuture research, guided by the principle that one should either align non-WRO\nmethods more closely with WRO or improve the optimization of WRO objectives."}
{"id": "2502.18509", "pdf": "https://arxiv.org/pdf/2502.18509.pdf", "abs": "https://arxiv.org/abs/2502.18509", "title": "Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents", "authors": ["Ivoline Ngong", "Swanand Kadhe", "Hao Wang", "Keerthiram Murugesan", "Justin D. Weisz", "Amit Dhurandhar", "Karthikeyan Natesan Ramamurthy"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "22 pages, 2 figures", "summary": "Conversational agents are increasingly woven into individuals' personal\nlives, yet users often underestimate the privacy risks associated with them.\nThe moment users share information with these agents-such as large language\nmodels (LLMs)-their private information becomes vulnerable to exposure. In this\npaper, we characterize the notion of contextual privacy for user interactions\nwith LLM-based Conversational Agents (LCAs). It aims to minimize privacy risks\nby ensuring that users (sender) disclose only information that is both relevant\nand necessary for achieving their intended goals when interacting with LCAs\n(untrusted receivers). Through a formative design user study, we observe how\neven \"privacy-conscious\" users inadvertently reveal sensitive information\nthrough indirect disclosures. Based on insights from this study, we propose a\nlocally deployable framework that operates between users and LCAs, identifying\nand reformulating out-of-context information in user prompts. Our evaluation\nusing examples from ShareGPT shows that lightweight models can effectively\nimplement this framework, achieving strong gains in contextual privacy while\npreserving the user's intended interaction goals. Notably, about 76% of\nparticipants in our human evaluation preferred the reformulated prompts over\nthe original ones, validating the usability and effectiveness of contextual\nprivacy in our proposed framework. We opensource the code at\nhttps://github.com/IBM/contextual-privacy-LLM."}
{"id": "2503.04036", "pdf": "https://arxiv.org/pdf/2503.04036.pdf", "abs": "https://arxiv.org/abs/2503.04036", "title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "authors": ["Xinyue Cui", "Johnny Tian-Zheng Wei", "Swabha Swayamdipta", "Robin Jia"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization during\npretraining, while overlooking challenges that arise in other stages of the LLM\nlifecycle, such as the risk of watermark filtering during data preprocessing\nand verification difficulties due to API-only access. To address these\nchallenges, we propose a novel data watermarking approach that injects\nplausible yet fictitious knowledge into training data using generated passages\ndescribing a fictitious entity and its associated attributes. Our watermarks\nare designed to be memorized by the LLM through seamlessly integrating in its\ntraining data, making them harder to detect lexically during preprocessing. We\ndemonstrate that our watermarks can be effectively memorized by LLMs, and that\nincreasing our watermarks' density, length, and diversity of attributes\nstrengthens their memorization. We further show that our watermarks remain\neffective after continual pretraining and supervised finetuning. Finally, we\nshow that our data watermarks can be evaluated even under API-only access via\nquestion answering."}
{"id": "2503.06201", "pdf": "https://arxiv.org/pdf/2503.06201.pdf", "abs": "https://arxiv.org/abs/2503.06201", "title": "Explainable Synthetic Image Detection through Diffusion Timestep Ensembling", "authors": ["Yixin Wu", "Feiran Zhang", "Tianyuan Shi", "Ruicheng Yin", "Zhenghua Wang", "Zhenliang Gan", "Xiaohua Wang", "Changze Lv", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "16 pages, 8 figures", "summary": "Recent advances in diffusion models have enabled the creation of deceptively\nreal images, posing significant security risks when misused. In this study, we\nempirically show that different timesteps of DDIM inversion reveal varying\nsubtle distinctions between synthetic and real images that are extractable for\ndetection, in the forms of such as Fourier power spectrum high-frequency\ndiscrepancies and inter-pixel variance distributions. Based on these\nobservations, we propose a novel synthetic image detection method that directly\nutilizes features of intermediately noised images by training an ensemble on\nmultiple noised timesteps, circumventing conventional reconstruction-based\nstrategies. To enhance human comprehension, we introduce a metric-grounded\nexplanation generation and refinement module to identify and explain\nAI-generated flaws. Additionally, we construct the GenHard and GenExplain\nbenchmarks to provide detection samples of greater difficulty and high-quality\nrationales for fake images. Extensive experiments show that our method achieves\nstate-of-the-art performance with 98.91% and 95.89% detection accuracy on\nregular and challenging samples respectively, and demonstrates generalizability\nand robustness. Our code and datasets are available at\nhttps://github.com/Shadowlized/ESIDE."}
{"id": "2504.07840", "pdf": "https://arxiv.org/pdf/2504.07840.pdf", "abs": "https://arxiv.org/abs/2504.07840", "title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "authors": ["Cansu Koyuturk", "Emily Theophilou", "Sabrina Patania", "Gregor Donabauer", "Andrea Martinenghi", "Chiara Antico", "Alessia Telari", "Alessia Testa", "Sathya Bursic", "Franca Garzotto", "Davinia Hernandez-Leo", "Udo Kruschwitz", "Davide Taibi", "Simona Amenta", "Martin Ruskov", "Dimitri Ognibene"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Long paper accepted for AIED 2025, the 26th International Conference\n  on Artificial Intelligence in Education, July 22 - 26, 2025, Palermo, Italy", "summary": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems."}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887.pdf", "abs": "https://arxiv.org/abs/2504.13887", "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite increasing AI chatbot deployment in public discourse, empirical\nevidence on their capacity to foster intercultural empathy remains limited.\nThrough a randomized experiment, we assessed how different AI deliberation\napproaches--cross-cultural deliberation (presenting other-culture\nperspectives), own-culture deliberation (representing participants' own\nculture), and non-deliberative control--affect intercultural empathy across\nAmerican and Latin American participants. Cross-cultural deliberation increased\nintercultural empathy among American participants through positive emotional\nengagement, but produced no such effects for Latin American participants, who\nperceived AI responses as culturally inauthentic despite explicit prompting to\nrepresent their cultural perspectives. Our analysis of participant-driven\nfeedback, where users directly flagged and explained culturally inappropriate\nAI responses, revealed systematic gaps in AI's representation of Latin American\ncontexts that persist despite sophisticated prompt engineering. These findings\ndemonstrate that current approaches to AI cultural alignment--including\nlinguistic adaptation and explicit cultural prompting--cannot fully address\ndeeper representational asymmetries in AI systems. Our work advances both\ndeliberation theory and AI alignment research by revealing how the same AI\nsystem can simultaneously promote intercultural understanding for one cultural\ngroup while failing for another, with critical implications for designing\nequitable AI systems for cross-cultural democratic discourse."}
{"id": "2505.02820", "pdf": "https://arxiv.org/pdf/2505.02820.pdf", "abs": "https://arxiv.org/abs/2505.02820", "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback", "authors": ["Hao Zhu", "Phil Cuvin", "Xinkai Yu", "Charlotte Ka Yee Yan", "Jason Zhang", "Diyi Yang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "https://opensocial.world/", "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback e.g. \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."}
{"id": "2505.14351", "pdf": "https://arxiv.org/pdf/2505.14351.pdf", "abs": "https://arxiv.org/abs/2505.14351", "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "15 pages", "summary": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality."}
{"id": "2505.14699", "pdf": "https://arxiv.org/pdf/2505.14699.pdf", "abs": "https://arxiv.org/abs/2505.14699", "title": "Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs", "authors": ["Miguel Lopez-Duran", "Julian Fierrez", "Aythami Morales", "Ruben Tolosana", "Oscar Delgado-Mohatar", "Alvaro Ortigosa"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "15 pages, 2 figures, accepted paper at The Fifth ICDAR International\n  Workshop on Machine Learning", "summary": "The automatic analysis of document layouts in digital-born PDF documents\nremains a challenging problem due to the heterogeneous arrangement of textual\nand nontextual elements and the imprecision of the textual metadata in the\nPortable Document Format. In this work, we benchmark Graph Neural Network (GNN)\narchitectures for the task of fine-grained layout classification of text blocks\nfrom digital native documents. We introduce two graph construction structures:\na k-closest-neighbor graph and a fully connected graph, and generate node\nfeatures via pre-trained text and vision models, thus avoiding manual feature\nengineering. Three experimental frameworks are evaluated: single-modality (text\nor visual), concatenated multimodal, and dual-branch multimodal. We evaluated\nfour foundational GNN models and compared them with the baseline. Our\nexperiments are specifically conducted on a rich dataset of public affairs\ndocuments that includes more than 20 sources (e.g., regional and national-level\nofficial gazettes), 37K PDF documents, with 441K pages in total. Our results\ndemonstrate that GraphSAGE operating on the k-closest-neighbor graph in a\ndual-branch configuration achieves the highest per-class and overall accuracy,\noutperforming the baseline in some sources. These findings confirm the\nimportance of local layout relationships and multimodal fusion exploited\nthrough GNNs for the analysis of native digital document layouts."}
{"id": "2506.15606", "pdf": "https://arxiv.org/pdf/2506.15606.pdf", "abs": "https://arxiv.org/abs/2506.15606", "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning", "authors": ["Gabriel J. Perin", "Runjin Chen", "Xuxi Chen", "Nina S. T. Hirata", "Zhangyang Wang", "Junyuan Hong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX."}
{"id": "2507.02087", "pdf": "https://arxiv.org/pdf/2507.02087.pdf", "abs": "https://arxiv.org/abs/2507.02087", "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions", "authors": ["Eitan Anzenberg", "Arunava Samajpati", "Sivasankaran Chandrasekar", "Varun Kacholia"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes."}
{"id": "2507.05169", "pdf": "https://arxiv.org/pdf/2507.05169.pdf", "abs": "https://arxiv.org/abs/2507.05169", "title": "Critiques of World Models", "authors": ["Eric Xing", "Mingkai Deng", "Jinyu Hou", "Zhiting Hu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model."}
{"id": "2507.15846", "pdf": "https://arxiv.org/pdf/2507.15846.pdf", "abs": "https://arxiv.org/abs/2507.15846", "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks."}
{"id": "2507.16534", "pdf": "https://arxiv.org/pdf/2507.16534.pdf", "abs": "https://arxiv.org/abs/2507.16534", "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report", "authors": ["Shanghai AI Lab", ":", "Xiaoyang Chen", "Yunhao Chen", "Zeren Chen", "Zhiyun Chen", "Hanyun Cui", "Yawen Duan", "Jiaxuan Guo", "Qi Guo", "Xuhao Hu", "Hong Huang", "Lige Huang", "Chunxiao Li", "Juncheng Li", "Qihao Lin", "Dongrui Liu", "Xinmin Liu", "Zicheng Liu", "Chaochao Lu", "Xiaoya Lu", "Jingjing Qu", "Qibing Ren", "Jing Shao", "Jingwei Shi", "Jingwei Sun", "Peng Wang", "Weibing Wang", "Jia Xu", "Lewen Yan", "Xiao Yu", "Yi Yu", "Boxuan Zhang", "Jie Zhang", "Weichen Zhang", "Zhijie Zheng", "Tianyi Zhou", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "97 pages, 37 figures", "summary": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges."}
{"id": "2507.18071", "pdf": "https://arxiv.org/pdf/2507.18071.pdf", "abs": "https://arxiv.org/abs/2507.18071", "title": "Group Sequence Policy Optimization", "authors": ["Chujie Zheng", "Shixuan Liu", "Mingze Li", "Xiong-Hui Chen", "Bowen Yu", "Chang Gao", "Kai Dang", "Yuqiong Liu", "Rui Men", "An Yang", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels."}
{"id": "2507.18224", "pdf": "https://arxiv.org/pdf/2507.18224.pdf", "abs": "https://arxiv.org/abs/2507.18224", "title": "Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation", "authors": ["Shiyuan Li", "Yixin Liu", "Qingsong Wen", "Chengqi Zhang", "Shirui Pan"], "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have emerged\nas a powerful solution for dealing with complex problems across diverse\ndomains. The effectiveness of MAS is critically dependent on its collaboration\ntopology, which has become a focal point for automated design research.\nHowever, existing approaches are fundamentally constrained by their reliance on\na template graph modification paradigm with a predefined set of agents and\nhard-coded interaction structures, significantly limiting their adaptability to\ntask-specific requirements. To address these limitations, we reframe MAS design\nas a conditional autoregressive graph generation task, where both the system\ncomposition and structure are designed jointly. We propose ARG-Designer, a\nnovel autoregressive model that operationalizes this paradigm by constructing\nthe collaboration graph from scratch. Conditioned on a natural language task\nquery, ARG-Designer sequentially and dynamically determines the required number\nof agents, selects their appropriate roles from an extensible pool, and\nestablishes the optimal communication links between them. This generative\napproach creates a customized topology in a flexible and extensible manner,\nprecisely tailored to the unique demands of different tasks. Extensive\nexperiments across six diverse benchmarks demonstrate that ARG-Designer not\nonly achieves state-of-the-art performance but also enjoys significantly\ngreater token efficiency and enhanced extensibility. The source code of\nARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer."}
{"id": "2507.18576", "pdf": "https://arxiv.org/pdf/2507.18576.pdf", "abs": "https://arxiv.org/abs/2507.18576", "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law", "authors": ["Shanghai AI Lab", ":", "Yicheng Bao", "Guanxu Chen", "Mingkang Chen", "Yunhao Chen", "Chiyu Chen", "Lingjie Chen", "Sirui Chen", "Xinquan Chen", "Jie Cheng", "Yu Cheng", "Dengke Deng", "Yizhuo Ding", "Dan Ding", "Xiaoshan Ding", "Yi Ding", "Zhichen Dong", "Lingxiao Du", "Yuyu Fan", "Xinshun Feng", "Yanwei Fu", "Yuxuan Gao", "Ruijun Ge", "Tianle Gu", "Lujun Gui", "Jiaxuan Guo", "Qianxi He", "Yuenan Hou", "Xuhao Hu", "Hong Huang", "Kaichen Huang", "Shiyang Huang", "Yuxian Jiang", "Shanzhe Lei", "Jie Li", "Lijun Li", "Hao Li", "Juncheng Li", "Xiangtian Li", "Yafu Li", "Lingyu Li", "Xueyan Li", "Haotian Liang", "Dongrui Liu", "Qihua Liu", "Zhixuan Liu", "Bangwei Liu", "Huacan Liu", "Yuexiao Liu", "Zongkai Liu", "Chaochao Lu", "Yudong Lu", "Xiaoya Lu", "Zhenghao Lu", "Qitan Lv", "Caoyuan Ma", "Jiachen Ma", "Xiaoya Ma", "Zhongtian Ma", "Lingyu Meng", "Ziqi Miao", "Yazhe Niu", "Yuezhang Peng", "Yuan Pu", "Han Qi", "Chen Qian", "Xingge Qiao", "Jingjing Qu", "Jiashu Qu", "Wanying Qu", "Wenwen Qu", "Xiaoye Qu", "Qihan Ren", "Qingnan Ren", "Qingyu Ren", "Jing Shao", "Wenqi Shao", "Shuai Shao", "Dongxing Shi", "Xin Song", "Xinhao Song", "Yan Teng", "Xuan Tong", "Yingchun Wang", "Xuhong Wang", "Shujie Wang", "Xin Wang", "Yige Wang", "Yixu Wang", "Yuanfu Wang", "Futing Wang", "Ruofan Wang", "Wenjie Wang", "Yajie Wang", "Muhao Wei", "Xiaoyu Wen", "Fenghua Weng", "Yuqi Wu", "Yingtong Xiong", "Xingcheng Xu", "Chao Yang", "Yue Yang", "Yang Yao", "Yulei Ye", "Zhenyun Yin", "Yi Yu", "Bo Zhang", "Qiaosheng Zhang", "Jinxuan Zhang", "Yexin Zhang", "Yinqiang Zheng", "Hefeng Zhou", "Zhanhui Zhou", "Pengyu Zhu", "Qingzi Zhu", "Yubo Zhu", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names; v2 modifies minor issues", "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI."}
{"id": "2507.19204", "pdf": "https://arxiv.org/pdf/2507.19204.pdf", "abs": "https://arxiv.org/abs/2507.19204", "title": "Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?", "authors": ["Simon Malan", "Benjamin van Niekerk", "Herman Kamper"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Submitted to the IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "We investigate the problem of segmenting unlabeled speech into word-like\nunits and clustering these to create a lexicon. Prior work can be categorized\ninto two frameworks. Bottom-up methods first determine boundaries and then\ncluster the fixed segmented words into a lexicon. In contrast, top-down methods\nincorporate information from the clustered words to inform boundary selection.\nHowever, it is unclear whether top-down information is necessary to improve\nsegmentation. To explore this, we look at two similar approaches that differ in\nwhether top-down clustering informs boundary selection. Our simple bottom-up\nstrategy predicts word boundaries using the dissimilarity between adjacent\nself-supervised features, then clusters the resulting segments to construct a\nlexicon. Our top-down system is an updated version of the ES-KMeans dynamic\nprogramming method that iteratively uses K-means to update its boundaries. On\nthe five-language ZeroSpeech benchmarks, both approaches achieve comparable\nstate-of-the-art results, with the bottom-up system being nearly five times\nfaster. Through detailed analyses, we show that the top-down influence of\nES-KMeans can be beneficial (depending on factors like the candidate\nboundaries), but in many cases the simple bottom-up method performs just as\nwell. For both methods, we show that the clustering step is a limiting factor.\nTherefore, we recommend that future work focus on improved clustering\ntechniques and learning more discriminative word-like representations. Project\ncode repository: https://github.com/s-malan/prom-seg-clus."}
