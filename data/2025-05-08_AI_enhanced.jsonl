{"id": "2505.03807", "pdf": "https://arxiv.org/pdf/2505.03807.pdf", "abs": "https://arxiv.org/abs/2505.03807", "title": "Facilitating Video Story Interaction with Multi-Agent Collaborative System", "authors": ["Yiwen Zhang", "Jianing Hao", "Zhan Wang", "Hongling Sheng", "Wei Zeng"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "Prepared and submitted in 2024", "summary": "Video story interaction enables viewers to engage with and explore narrative\ncontent for personalized experiences. However, existing methods are limited to\nuser selection, specially designed narratives, and lack customization. To\naddress this, we propose an interactive system based on user intent. Our system\nuses a Vision Language Model (VLM) to enable machines to understand video\nstories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent\nSystem (MAS) to create evolving characters and scene experiences. It includes\nthree stages: 1) Video story processing, utilizing VLM and prior knowledge to\nsimulate human understanding of stories across three modalities. 2) Multi-space\nchat, creating growth-oriented characters through MAS interactions based on\nuser queries and story stages. 3) Scene customization, expanding and\nvisualizing various story scenes mentioned in dialogue. Applied to the Harry\nPotter series, our study shows the system effectively portrays emergent\ncharacter social behavior and growth, enhancing the interactive experience in\nthe video story world.", "AI": {"tldr": "A new interactive system enhances video story engagement by understanding user intent through a Vision Language Model, integrating retrieval-augmented generation and multi-agent systems for character development and scene customization.", "motivation": "Existing interactive video story methods are limited in customization and personalization, which can hinder viewer engagement and narrative exploration.", "method": "The system processes video stories using a Vision Language Model to simulate narrative understanding across modalities, incorporates a multi-agent system for character interaction, and allows scene customization based on user dialogue.", "result": "The system effectively demonstrated emergent character social behavior and growth in a study applied to the Harry Potter series, leading to enhanced interactivity in video narratives.", "conclusion": "The proposed interactive system significantly improves personalized engagement with video stories, suggesting new avenues for future narrative experiences.", "key_contributions": ["Introduction of a user intent-based interactive system for video stories", "Combining Vision Language Models with Retrieval-Augmented Generation and Multi-Agent Systems", "Demonstrated application on the Harry Potter series showing emergent character behaviors."], "limitations": "The study is based on a specific narrative (Harry Potter), and the efficacy in broader contexts or different genres is not tested.", "keywords": ["Interactive Systems", "Video Stories", "Vision Language Model", "Multi-Agent Systems", "Character Growth"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.03867", "pdf": "https://arxiv.org/pdf/2505.03867.pdf", "abs": "https://arxiv.org/abs/2505.03867", "title": "Scratch Copilot: Supporting Youth Creative Coding with AI", "authors": ["Stefania Druga", "Amy J. Ko"], "categories": ["cs.HC", "cs.AI"], "comment": "5 figures, 14 pages", "summary": "Creative coding platforms like Scratch have democratized programming for\nchildren, yet translating imaginative ideas into functional code remains a\nsignificant hurdle for many young learners. While AI copilots assist adult\nprogrammers, few tools target children in block-based environments. Building on\nprior research \\cite{druga_how_2021,druga2023ai, druga2023scratch}, we present\nCognimates Scratch Copilot: an AI-powered assistant integrated into a\nScratch-like environment, providing real-time support for ideation, code\ngeneration, debugging, and asset creation. This paper details the system\narchitecture and findings from an exploratory qualitative evaluation with 18\ninternational children (ages 7--12). Our analysis reveals how the AI Copilot\nsupported key creative coding processes, particularly aiding ideation and\ndebugging. Crucially, it also highlights how children actively negotiated the\nuse of AI, demonstrating strong agency by adapting or rejecting suggestions to\nmaintain creative control. Interactions surfaced design tensions between\nproviding helpful scaffolding and fostering independent problem-solving, as\nwell as learning opportunities arising from navigating AI limitations and\nerrors. Findings indicate Cognimates Scratch Copilot's potential to enhance\ncreative self-efficacy and engagement. Based on these insights, we propose\ninitial design guidelines for AI coding assistants that prioritize youth agency\nand critical interaction alongside supportive scaffolding.", "AI": {"tldr": "This paper introduces the Cognimates Scratch Copilot, an AI-powered assistant for children, which aids in coding processes such as ideation, code generation, and debugging within a Scratch-like environment.", "motivation": "The motivation is to assist children in translating imaginative ideas into functional code, addressing the challenges they face in creative coding environments.", "method": "The paper presents the system architecture of the Cognimates Scratch Copilot and includes findings from a qualitative evaluation involving 18 children aged 7 to 12, focusing on their interactions with the AI system.", "result": "The findings indicate that the AI Copilot supported key coding processes, particularly in ideation and debugging, while also revealing children's agency in negotiating AI suggestions.", "conclusion": "The study proposes initial design guidelines for AI coding assistants that emphasize youth agency and critical interaction while providing supportive scaffolding to enhance creative coding.", "key_contributions": ["Introduction of Cognimates Scratch Copilot for children in coding environments", "Highlighting children's agency in interaction with AI", "Proposing design guidelines for supportive AI coding tools"], "limitations": "The study is exploratory and based on a limited sample size of 18 children, potentially affecting the generalizability of the findings.", "keywords": ["AI coding assistant", "creative coding", "children", "Scratch", "youth agency"], "importance_score": 5, "read_time_minutes": 14}}
{"id": "2505.04184", "pdf": "https://arxiv.org/pdf/2505.04184.pdf", "abs": "https://arxiv.org/abs/2505.04184", "title": "State-of-the-Art HCI for Dementia Care: A Scoping Review of Recent Technological Advances", "authors": ["Yong Ma", "Yuchong Zhang", "Oda Elise Nordberg", "Arvid Rongve", "Miroslav Bachinski", "Morten Fjeld"], "categories": ["cs.HC"], "comment": "19 pages, 4 figures, conference", "summary": "Dementia significantly impacts cognitive, behavioral, and functional\nabilities, creating challenges for both individuals and caregivers. Recent\nadvancements in HCI have introduced innovative technological solutions to\nsupport people with dementia (PwD) and their caregivers. This scoping review\nsystematically examines 32 recent publications from leading digital libraries,\ncategorizing technological interventions into four key domains: Assistive and\nSmart Technology for Daily Life, Social Interaction and Communication,\nWell-being and Psychological Support, and Caregiver Support and Training. Our\nanalysis highlights how emerging technologies are transforming dementia care.\nThese technologies enhance quality of life by promoting independence, fostering\nsocial engagement, and providing emotional and cognitive support. However, the\nreview also identifies critical gaps, particularly in addressing the needs of\nindividuals with early-stage dementia and the lack of individualized support\nmechanisms. By emphasizing user-centered design, accessibility, and ethical\nconsiderations, this paper offers a structured roadmap for future research and\npractice in dementia care. It bridges the gap between technological innovation\nand the real-world needs of PwD and their caregivers, providing valuable\ninsights for researchers, practitioners, and policymakers. This review not only\nsynthesizes current advancements but also sets the stage for future HCI-driven\ninnovations in dementia care, aiming to improve outcomes for an aging global\npopulation.", "AI": {"tldr": "This scoping review examines 32 recent technological interventions aimed at supporting individuals with dementia and their caregivers, categorizing them into four key domains.", "motivation": "To address the significant challenges faced by individuals with dementia and their caregivers through technological advancements in Human-Computer Interaction.", "method": "Systematic review of 32 publications from leading digital libraries focusing on technological interventions for dementia care.", "result": "Technological advancements enhance quality of life for PwD by increasing independence, social engagement, and emotional support; however, critical gaps remain, especially for early-stage dementia support.", "conclusion": "The review provides insights into current advancements and proposes a roadmap for future research emphasizing user-centered design, accessibility, and ethical considerations in dementia care.", "key_contributions": ["Categorization of interventions into four domains: daily life support, social engagement, well-being support, and caregiver training.", "Highlighting critical gaps in support for early-stage dementia.", "Providing a structured roadmap for future HCI-driven research in dementia care."], "limitations": "Identified gaps in individualized support mechanisms and needs of early-stage dementia individuals.", "keywords": ["Dementia", "Human-Computer Interaction", "Assistive Technology", "Caregiver Support", "User-Centered Design"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2505.04210", "pdf": "https://arxiv.org/pdf/2505.04210.pdf", "abs": "https://arxiv.org/abs/2505.04210", "title": "Sick of being driven? -- Prevalence and modulating factors of carsickness in the European population in context of automated driving", "authors": ["Myriam Metzulat", "Barbara Metz", "Aaron Edelmann", "Alexandra Neukum", "Wilfried Kunde"], "categories": ["cs.HC"], "comment": null, "summary": "As in automated driving the driver becomes a passenger, carsickness might\nreduce comfort for susceptible individuals. Insights in the prevalence of\ncarsickness and its modulating factors are considered useful for the\ndevelopment of automated vehicles to mitigate or prevent its occurrence. An\nonline survey was conducted with N = 3999 participants in Spain, Sweden,\nPoland, and Germany. 30% of participants reported to have already experienced\ncarsickness as adult. The frequency of carsickness was modulated not only by\ndemographic factors (country, gender, age), but also by frequency of being a\npassenger, type of non-driving related task, road type, and the seating\nposition in car. Furthermore, the efficiency of applied countermeasures,\ntemporal aspects of carsickness development, as well as the relation of\ncarsickness with the acceptability of automated driving and the effect on\nsubjective fitness to drive was investigated. The results are discussed with\nfocus on automated driving.", "AI": {"tldr": "The study investigates the prevalence of carsickness in passengers of automated vehicles and factors influencing it through an online survey of 3999 participants from four countries.", "motivation": "Understanding carsickness is vital for designing automated vehicles that maximize passenger comfort.", "method": "An online survey was conducted with 3999 participants from Spain, Sweden, Poland, and Germany to assess the prevalence and factors influencing carsickness.", "result": "30% of participants reported experiencing carsickness as adults, influenced by demographic factors, non-driving tasks, road type, and seating position.", "conclusion": "Mitigating carsickness is essential for improving the acceptance and experience of automated driving.", "key_contributions": ["Identification of demographic and situational factors influencing carsickness", "Evaluation of countermeasures for mitigating carsickness in automated driving", "Exploration of carsickness impact on the acceptability of automated vehicles"], "limitations": "", "keywords": ["carsickness", "automated driving", "passenger comfort", "survey study", "HCI"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788.pdf", "abs": "https://arxiv.org/abs/2505.03788", "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks.", "AI": {"tldr": "A novel UQ calibration method for multi-modal LLMs improves accuracy by considering cross-modal consistency.", "motivation": "Current UQ methods underreport confidence in erroneous outputs of multi-modal LLMs, necessitating a better calibration strategy.", "method": "The approach combines self-consistency and cross-modal consistency to calibrate confidence in generated responses using a grounding model and temperature scaling.", "result": "Significant improvements in calibration on tasks such as medical question answering and visual question answering with models like LLaVA-Med and LLaVA.", "conclusion": "The proposed framework enhances the reliability of confidence estimates in multi-modal settings, leading to better output accuracy.", "key_contributions": ["Novel calibration method incorporating cross-modal consistency", "Utilization of grounding model confidence", "Application of temperature scaling to improve UQ"], "limitations": "", "keywords": ["uncertainty quantification", "multi-modal", "large language models", "cross-modal consistency", "calibration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04260", "pdf": "https://arxiv.org/pdf/2505.04260.pdf", "abs": "https://arxiv.org/abs/2505.04260", "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering", "authors": ["Jessica Y. Bo", "Tianyu Xu", "Ishan Chatterjee", "Katrina Passarella-Ward", "Achin Kulshrestha", "D Shin"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces.", "AI": {"tldr": "The paper presents a method for personalizing large language model responses using activation steering, making it easier for users to express their preferences in AI conversations.", "motivation": "Improving user satisfaction and retention in AI assistants by better aligning responses with user preferences.", "method": "Activation steering is used to guide LLMs towards interpretable preference dimensions during inference, allowing for lightweight personalization without needing extensive user history.", "result": "The study shows that preference-based steering effectively aligns conversations with hidden user preferences and reveals insights into user preferences for interface control and usability.", "conclusion": "Preference-based steering enhances the personalization of AI assistants, providing insights that can inform the design of interactive interfaces.", "key_contributions": ["Introduction of activation steering for personalized responses in LLMs", "User study revealing preferences for different chatbot interfaces", "Insights into user values around control, usability, and transparency"], "limitations": "The study is limited by a small sample size (n=14) and focuses on specific interactive chatbot interfaces.", "keywords": ["large language models", "activation steering", "personalization", "user preferences", "chatbot interfaces"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.03910", "pdf": "https://arxiv.org/pdf/2505.03910.pdf", "abs": "https://arxiv.org/abs/2505.03910", "title": "Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty", "authors": ["Gianluca Manzo", "Julia Ive"], "categories": ["cs.CL"], "comment": null, "summary": "Automating chest radiograph interpretation using Deep Learning (DL) models\nhas the potential to significantly improve clinical workflows, decision-making,\nand large-scale health screening. However, in medical settings, merely\noptimising predictive performance is insufficient, as the quantification of\nuncertainty is equally crucial. This paper investigates the relationship\nbetween predictive uncertainty, derived from Bayesian Deep Learning\napproximations, and human/linguistic uncertainty, as estimated from free-text\nradiology reports labelled by rule-based labellers. Utilising BERT as the model\nof choice, this study evaluates different binarisation methods for uncertainty\nlabels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in\nestimating predictive uncertainty. The results demonstrate good model\nperformance, but also a modest correlation between predictive and linguistic\nuncertainty, highlighting the challenges in aligning machine uncertainty with\nhuman interpretation nuances. Our findings suggest that while Bayesian\napproximations provide valuable uncertainty estimates, further refinement is\nnecessary to fully capture and utilise the subtleties of human uncertainty in\nclinical applications.", "AI": {"tldr": "This paper explores the role of uncertainty in chest radiograph interpretation using Bayesian Deep Learning, linking predictive uncertainty with human uncertainty from free-text radiology reports.", "motivation": "To enhance clinical workflows and decision-making in automated chest radiograph interpretation, it is crucial to not only optimize predictive performance but also quantify uncertainty.", "method": "Utilizes BERT to evaluate binarisation methods for uncertainty labels and assesses Monte Carlo Dropout and Deep Ensembles for estimating predictive uncertainty.", "result": "The study shows good model performance but a modest correlation between predictive and linguistic uncertainty, indicating challenges in aligning machine estimates with human interpretation nuances.", "conclusion": "Bayesian approximations can provide valuable uncertainty estimates, but further refinement is needed to capture the subtleties of human uncertainty in clinical contexts.", "key_contributions": ["Investigates relationship between predictive and human uncertainty in medical diagnostics.", "Evaluates binarisation methods for uncertainty labels and their impact on model performance.", "Highlights the necessity for improved methods to align machine and human uncertainty."], "limitations": "The correlation between predictive and linguistic uncertainty was modest, suggesting limitations in current approaches.", "keywords": ["Deep Learning", "Uncertainty Quantification", "Bayesian Approximations", "Chest Radiographs", "Clinical Decision Making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.04273", "pdf": "https://arxiv.org/pdf/2505.04273.pdf", "abs": "https://arxiv.org/abs/2505.04273", "title": "With Friends Like These, Who Needs Explanations? Evaluating User Understanding of Group Recommendations", "authors": ["Cedric Waterschoot", "Raciel Yera Toledo", "Nava Tintarev", "Francesco Barile"], "categories": ["cs.HC"], "comment": "This article will be part of the UMAP 2025 conference. (33rd ACM\n  Conference on User Modeling, Adaptation and Personalization (UMAP25), June\n  16-19, 2025, New York City, NY, USA)", "summary": "Group Recommender Systems (GRS) employing social choice-based aggregation\nstrategies have previously been explored in terms of perceived consensus,\nfairness, and satisfaction. At the same time, the impact of textual\nexplanations has been examined, but the results suggest a low effectiveness of\nthese explanations. However, user understanding remains fairly unexplored, even\nif it can contribute positively to transparent GRS. This is particularly\ninteresting to study in more complex or potentially unfair scenarios when user\npreferences diverge, such as in a minority scenario (where group members have\nsimilar preferences, except for a single member in a minority position). In\nthis paper, we analyzed the impact of different types of explanations on user\nunderstanding of group recommendations. We present a randomized controlled\ntrial (n = 271) using two between-subject factors: (i) the aggregation strategy\n(additive, least misery, and approval voting), and (ii) the modality of\nexplanation (no explanation, textual explanation, or multimodal explanation).\nWe measured both subjective (self-perceived by the user) and objective\nunderstanding (performance on model simulation, counterfactuals and error\ndetection). In line with recent findings on explanations for machine learning\nmodels, our results indicate that more detailed explanations, whether textual\nor multimodal, did not increase subjective or objective understanding. However,\nwe did find a significant effect of aggregation strategies on both subjective\nand objective understanding. These results imply that when constructing GRS,\npractitioners need to consider that the choice of aggregation strategy can\ninfluence the understanding of users. Post-hoc analysis also suggests that\nthere is value in analyzing performance on different tasks, rather than through\na single aggregated metric of understanding.", "AI": {"tldr": "This paper investigates the impact of different types of explanations on user understanding of Group Recommender Systems (GRS) across various aggregation strategies and modality of explanations.", "motivation": "The study aims to uncover the relationship between explanations in GRS and user understanding, particularly in minority preference scenarios, an area that has been relatively unexplored despite its relevance to transparent GRS.", "method": "A randomized controlled trial was conducted with 271 participants, assessing two factors: aggregation strategy (additive, least misery, and approval voting) and explanation modality (no explanation, textual explanation, or multimodal explanation). Understanding was measured through subjective self-perception and objective performance on tasks.", "result": "The study found that detailed explanations did not improve user understanding either subjectively or objectively. However, the aggregation strategy used significantly affected both aspects of user understanding.", "conclusion": "When designing GRS, it is crucial to consider the aggregation strategy as it influences users' understanding. A focus on multiple performance tasks is recommended for a more comprehensive assessment of understanding.", "key_contributions": ["Investigating the impact of explanations on GRS user understanding.", "Highlighting the importance of aggregation strategies in understanding.", "Providing insights on measuring understanding beyond single aggregated metrics."], "limitations": "The study's findings may not generalize to all contexts of GRS and rely on self-reported metrics which can introduce bias.", "keywords": ["Group Recommender Systems", "explanations", "user understanding", "aggregation strategies", "social choice"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.03970", "pdf": "https://arxiv.org/pdf/2505.03970.pdf", "abs": "https://arxiv.org/abs/2505.03970", "title": "A Reasoning-Focused Legal Retrieval Benchmark", "authors": ["Lucia Zheng", "Neel Guha", "Javokhir Arifov", "Sarah Zhang", "Michal Skreta", "Christopher D. Manning", "Peter Henderson", "Daniel E. Ho"], "categories": ["cs.CL"], "comment": "CS&Law 2025. For data, see\n  https://reglab.github.io/legal-rag-benchmarks/", "summary": "As the legal community increasingly examines the use of large language models\n(LLMs) for various legal applications, legal AI developers have turned to\nretrieval-augmented LLMs (\"RAG\" systems) to improve system performance and\nrobustness. An obstacle to the development of specialized RAG systems is the\nlack of realistic legal RAG benchmarks which capture the complexity of both\nlegal retrieval and downstream legal question-answering. To address this, we\nintroduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.\nOur tasks correspond to real-world legal research tasks, and were produced\nthrough annotation processes which resemble legal research. We describe the\nconstruction of these benchmarks and the performance of existing retriever\npipelines. Our results suggest that legal RAG remains a challenging\napplication, thus motivating future research.", "AI": {"tldr": "This paper introduces two benchmarks for legal retrieval-augmented large language models (RAG): Bar Exam QA and Housing Statute QA, aimed at evaluating legal question-answering performance.", "motivation": "The legal community's exploration of large language models (LLMs) for legal applications lacks realistic benchmarks for legal retrieval and question-answering, necessitating the creation of relevant testing frameworks.", "method": "The authors developed Bar Exam QA and Housing Statute QA benchmarks that simulate real-world legal research tasks through a comprehensive annotation process.", "result": "The existing retriever pipelines were evaluated on these benchmarks, highlighting the challenges in legal RAG applications and the need for ongoing research.", "conclusion": "The introduction of these benchmarks lays the groundwork for enhancing the performance and robustness of legal RAG systems in the future.", "key_contributions": ["Introduction of Bar Exam QA and Housing Statute QA benchmarks for legal RAG systems", "Benchmark tasks reflect real-world legal research scenarios", "Highlighting the challenges faced in legal retrieval and question-answering."], "limitations": "", "keywords": ["legal AI", "large language models", "retrieval-augmented generation", "legal benchmarks", "legal question answering"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.04433", "pdf": "https://arxiv.org/pdf/2505.04433.pdf", "abs": "https://arxiv.org/abs/2505.04433", "title": "Improving Inclusivity for Emotion Recognition Based on Face Tracking", "authors": ["Mats Ole Ellenberg", "Katja Krug"], "categories": ["cs.HC"], "comment": "Selected for ACM CHI 2025 Workshop \"Affective interaction and\n  affective computing - past, present and future\" -\n  https://doi.org/10.1145/3706599.3706743", "summary": "The limited expressiveness of virtual user representations in Mixed Reality\nand Virtual Reality can inhibit an integral part of communication: emotional\nexpression. Emotion recognition based on face tracking is often used to\ncompensate for this. However, emotional facial expressions are highly\nindividual, which is why many approaches have difficulties recognizing unique\nvariations of emotional expressions. We propose several strategies to improve\nface tracking systems for emotion recognition with and without user\nintervention for the Affective Interaction Workshop at CHI '25.", "AI": {"tldr": "This paper addresses the challenges of emotional expression in Mixed Reality and Virtual Reality due to limited virtual user representations and proposes strategies to enhance face tracking systems for emotion recognition.", "motivation": "The paper highlights the limitation of current virtual user representations in conveying emotional expressions in Mixed and Virtual Reality, which is essential for effective communication.", "method": "The authors propose several strategies for improving face tracking systems to enhance emotion recognition both with and without user intervention.", "result": "Strategies are expected to improve the accuracy and expressiveness of emotion recognition in virtual environments.", "conclusion": "Improving face tracking for emotion recognition can lead to more authentic and emotionally expressive interactions in Mixed Reality and Virtual Reality.", "key_contributions": ["Strategies for enhancing emotion recognition in virtual environments", "New approaches for individual variation in emotional expressions", "Consideration of user intervention in emotion recognition systems"], "limitations": "", "keywords": ["emotion recognition", "face tracking", "Mixed Reality", "Virtual Reality", "affective computing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.03973", "pdf": "https://arxiv.org/pdf/2505.03973.pdf", "abs": "https://arxiv.org/abs/2505.03973", "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte Højmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based optimization has shown remarkable potential in enhancing agentic\nsystems. However, the conventional approach of prompting LLM optimizer with the\nwhole training trajectories on training dataset in a single pass becomes\nuntenable as datasets grow, leading to context window overflow and degraded\npattern recognition. To address these challenges, we propose Fine-Grained\nOptimization (FGO), a scalable framework that divides large optimization tasks\ninto manageable subsets, performs targeted optimizations, and systematically\ncombines optimized components through progressive merging. Evaluation across\nALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms\nexisting approaches by 1.6-8.6% while reducing average prompt token consumption\nby 56.3%. Our framework provides a practical solution for scaling up LLM-based\noptimization of increasingly sophisticated agent systems. Further analysis\ndemonstrates that FGO achieves the most consistent performance gain in all\ntraining dataset sizes, showcasing its scalability and efficiency.", "AI": {"tldr": "Proposes Fine-Grained Optimization (FGO) framework for scalable LLM optimization by breaking down large tasks into manageable subsets, improving performance while reducing token consumption.", "motivation": "To address context window overflow and degraded pattern recognition in LLM optimizers due to growing datasets.", "method": "The proposed FGO framework divides large optimization tasks into subsets, performs targeted optimizations, and systematically merges optimized components.", "result": "FGO outperforms existing methods by 1.6-8.6% and reduces average prompt token consumption by 56.3% across benchmarks like ALFWorld, LogisticsQA, and GAIA.", "conclusion": "FGO provides a practical and efficient solution for enhancing LLM-based optimization in complex agent systems.", "key_contributions": ["Introduces a novel Fine-Grained Optimization framework.", "Demonstrates significant performance improvements over existing methods.", "Reduces token consumption substantially."], "limitations": "", "keywords": ["LLM", "optimization", "fine-grained", "agentic systems", "scalability"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.04446", "pdf": "https://arxiv.org/pdf/2505.04446.pdf", "abs": "https://arxiv.org/abs/2505.04446", "title": "Practice Support for Violin Bowing by Measuring Bow Pressure and Position", "authors": ["Yurina Mizuho", "Yuta Sugiura"], "categories": ["cs.HC", "H.5.2"], "comment": "27 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2411.05126", "summary": "The violin is one of the most popular musical instruments. Various parameters\nof bowing motion, such as pressure, position, and speed, are crucial for\nproducing a beautiful tone. However, mastering them is challenging and requires\nextensive practice. In this study, we aimed to support practice of bowing,\nfocusing on bow pressure. First, we compared the bowing movements, specifically\nbow pressure, bow position, and bow speed, of eight experienced players with\nthose of eight beginners. Next, we developed and evaluated a visual feedback\nsystem that displays bow pressure to support practice. We taught the identified\ndifferences to 14 beginners, dividing them into two groups: one practiced with\nan explanation, and the other with both an explanation and a feedback system.\nThese two experiments found that clarifying the characteristics unique to\nexperienced players can support practice.", "AI": {"tldr": "This study investigates the role of bow pressure in violin playing, comparing experienced and beginner players, and develops a visual feedback system to enhance practice.", "motivation": "To understand and improve the bowing technique in violin playing, focusing on the parameters that affect tone production.", "method": "The study involved comparing bow pressure, position, and speed between experienced and beginner violinists, followed by the development of a visual feedback system to assist beginners in practice.", "result": "The findings demonstrated that experienced players exhibited distinct bowing characteristics and that providing explicit feedback to beginners improved their bowing technique.", "conclusion": "The clarification of bowing characteristics for beginners, along with the introduction of a feedback system, significantly supports their learning process.", "key_contributions": ["Identification of differences in bowing techniques between experienced and beginner players", "Development of a visual feedback system to aid beginner violinists", "Evaluation of teaching methods incorporating feedback"], "limitations": "The study is limited to a small sample size and focuses only on bow pressure without considering other factors in string instrument performance.", "keywords": ["violin", "bowing technique", "visual feedback", "musical performance", "beginner training"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2505.03981", "pdf": "https://arxiv.org/pdf/2505.03981.pdf", "abs": "https://arxiv.org/abs/2505.03981", "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Timothy Ossowski", "Yu Gu", "Ying Jin", "Sid Kiblawi", "Sam Preston", "Mu Wei", "Paul Vozila", "Tristan Naumann", "Hoifung Poon"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.", "AI": {"tldr": "The paper introduces X-Reasoner, a vision-language model that extends reasoning capabilities beyond text and general domains, achieving strong performance on multimodal and medical benchmarks.", "motivation": "To explore the generalizability of reasoning across modalities and domains, particularly in light of existing work focusing mainly on text-only models.", "method": "X-Reasoner is post-trained on general-domain text through a two-stage process: an initial supervised fine-tuning with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards.", "result": "X-Reasoner outperforms state-of-the-art models on various general and medical benchmarks, showcasing successful transfer of reasoning capabilities.", "conclusion": "Continued training on domain-specific text can further enhance the performance of X-Reasoner, leading to the introduction of X-Reasoner-Med for specialized medical applications.", "key_contributions": ["Introduction of X-Reasoner model for multimodal reasoning", "Demonstration of generalizable reasoning capabilities from general-domain text", "Development of X-Reasoner-Med for specialized medical benchmarks"], "limitations": "", "keywords": ["multimodal reasoning", "generalizability", "vision-language model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.04487", "pdf": "https://arxiv.org/pdf/2505.04487.pdf", "abs": "https://arxiv.org/abs/2505.04487", "title": "A Design Space for the Critical Validation of LLM-Generated Tabular Data", "authors": ["Madhav Sachdeva", "Christopher Narayanan", "Marvin Wiedenkeller", "Jana Sedlakova", "Jürgen Bernard"], "categories": ["cs.HC"], "comment": "To appear at the 16th International EuroVis Workshop on Visual\n  Analytics (EuroVA'25)", "summary": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power.", "AI": {"tldr": "The paper presents a structured design space for validating LLM-generated tabular data, focusing on various analysis granularity and data sources.", "motivation": "To address the increasing need for critical validation of LLM-generated tabular data, leveraging its potential applications while ensuring reliability.", "method": "The design space is structured along two dimensions: Analysis Granularity and Data Source, coupled with a mapping of existing validation approaches and detailed discussion of select methods.", "result": "The authors map 19 validation approaches and detail characteristics of two approaches, showing the descriptive power of the validation tasks.", "conclusion": "A structured framework for validating LLM-generated tabular data is necessary to ensure the reliability of data-driven applications in academia and industry.", "key_contributions": ["Introduction of a two-dimensional design space for validation", "Mapping of existing validation methods to the design framework", "In-depth analysis of select validation approaches."], "limitations": "", "keywords": ["LLM-generated data", "tabular data validation", "validation framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016.pdf", "abs": "https://arxiv.org/abs/2505.04016", "title": "SLOT: Structuring the Output of Large Language Models", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments.", "AI": {"tldr": "SLOT transforms unstructured LLM outputs into structured formats, improving application reliability across various models.", "motivation": "To address the issue of LLMs generating outputs that deviate from predefined schemas, which hampers reliable application development.", "method": "SLOT uses a fine-tuned lightweight language model as a post-processing layer to achieve flexibility and accuracy in structured output generation.", "result": "The fine-tuned Mistral-7B model achieves 99.5% schema accuracy and 94.0% content similarity, outperforming Claude-3.5-Sonnet.", "conclusion": "SLOT enables even smaller LLMs to generate structured outputs comparable to larger models, which is valuable in resource-constrained environments.", "key_contributions": ["Model-agnostic approach to structure outputs from LLMs", "Systematic data curation and synthesis pipeline", "Formal evaluation methodology for accuracy and fidelity"], "limitations": "", "keywords": ["Structured Outputs", "Large Language Models", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04584", "pdf": "https://arxiv.org/pdf/2505.04584.pdf", "abs": "https://arxiv.org/abs/2505.04584", "title": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions", "authors": ["Chloe Qianhui Zhao", "Jie Cao", "Eason Chen", "Kenneth R. Koedinger", "Jionghao Lin"], "categories": ["cs.HC"], "comment": "14 pages, to be published at the 26th International Conference on\n  Artificial Intelligence in Education (AIED '25)", "summary": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback.", "AI": {"tldr": "This study explores AI-generated multimodal feedback in education, combining text from LLMs with relevant lecture slides to assess its impact on student learning and perceptions.", "motivation": "To investigate the benefits of multimodal feedback in education as a means to enhance student learning and experience compared to traditional text-only feedback.", "method": "An online crowdsourcing study with N=91 participants employed a 2x2 design comparing human vs AI feedback and with vs without relevant slides, assessing learning gains and student perceptions.", "result": "The study found significant pre-to-post learning gains in all conditions but no statistically significant differences in gains between the feedback types. Students found slide feedback helpful yet confusing, while AI feedback was seen as personalized but less trusted than human feedback.", "conclusion": "AI facilitates multimodal feedback that may enhance learning, but issues with clarity and trustworthiness need addressing to improve its adoption in educational settings.", "key_contributions": ["Combines textual feedback from LLMs with relevant lecture materials for enhanced learning support.", "Evaluates the effectiveness of AI feedback in comparison to human feedback across modalities.", "Provides insights into student perceptions and trust towards AI-generated content."], "limitations": "The differences in learning gains between conditions were not statistically significant, and students experienced difficulty in understanding slide feedback.", "keywords": ["Artificial Intelligence", "feedback generation", "multimodal learning", "Large Language Models", "educational technology"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2505.04072", "pdf": "https://arxiv.org/pdf/2505.04072.pdf", "abs": "https://arxiv.org/abs/2505.04072", "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs", "authors": ["Xu Huang", "Yuefeng Huang", "Weiwen Liu", "Xingshan Zeng", "Yasheng Wang", "Ruiming Tang", "Hong Xie", "Defu Lian"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 5 tables", "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench.", "AI": {"tldr": "This paper introduces Personalized Tool Invocation for Large Language Models, addressing user preferences and missing parameters during tool invocation, utilizing a data synthesis framework called PTool and a benchmark named PTBench.", "motivation": "To enhance the capability of Large Language Models by incorporating personalized constraints during tool invocation, which is critical for problem-solving and accessing timely information.", "method": "The authors propose PTool, a data synthesis framework that facilitates personalized tool invocation, and they develop PTBench, the first benchmark for evaluating this personalized approach.", "result": "Evaluation of the PTool framework shows improved performance and the potential for better user satisfaction in tool invocation tasks.", "conclusion": "The introduction of personalized constraints significantly enhances the usability of tool invocation in LLMs, suggesting that personalization is a crucial aspect of future tool interfaces.", "key_contributions": ["Introduction of the concept of Personalized Tool Invocation.", "Development of the PTool data synthesis framework.", "Creation of PTBench, the first benchmark for evaluating personalized tool invocation."], "limitations": "The study may be limited by a lack of diverse user profiles and preferences in the evaluation dataset.", "keywords": ["Personalized Tool Invocation", "Large Language Models", "PTool", "PTBench", "user preferences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.04073", "pdf": "https://arxiv.org/pdf/2505.04073.pdf", "abs": "https://arxiv.org/abs/2505.04073", "title": "Natural Language Generation in Healthcare: A Review of Methods and Applications", "authors": ["Mengxian Lyu", "Xiaohan Li", "Ziyi Chen", "Jinqian Pan", "Cheng Peng", "Sankalp Talankar", "Yonghui Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare.", "AI": {"tldr": "This paper reviews natural language generation (NLG) methods and applications in the medical domain, highlighting advancements and challenges in utilizing large language models (LLMs) for healthcare.", "motivation": "The increasing integration of NLG in healthcare necessitates a thorough review to understand its capabilities and limitations across diverse data modalities and clinical applications.", "method": "Systematic review of 113 scientific publications from 3,988 NLG-related articles, focusing on data modality, model architecture, clinical applications, and evaluation methods, adhering to PRISMA guidelines.", "result": "Identified key methods in NLG, categorized clinical applications, and assessed the effectiveness, limitations, and emerging challenges in healthcare applications of NLG.", "conclusion": "This review provides significant insights into NLG technologies and their applications in medicine, suggesting directions for future research to enhance healthcare outcomes using NLG.", "key_contributions": ["Comprehensive review of NLG methods in healthcare.", "Categorization of clinical applications of NLG.", "Assessment of capabilities and limitations of NLG models in medical contexts."], "limitations": "Review limited to scientific publications identified in literature search; may not encompass all relevant advancements in the field.", "keywords": ["natural language generation", "healthcare applications", "large language models", "clinical decision-making", "medical documentation"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152.pdf", "abs": "https://arxiv.org/abs/2505.04152", "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings.", "AI": {"tldr": "This paper presents a system using large language models (LLMs) to analyze and extract social signals in patient-provider dialogues, demonstrating capabilities to track 20 distinct signals that influence healthcare outcomes.", "motivation": "Improving communication between healthcare providers and patients is crucial for better health outcomes, and understanding social signals can enhance this interaction.", "method": "The authors designed task-specific prompts and evaluated the performance of various LLM architectures on a dataset with 20 annotated social signals.", "result": "The study introduces a system that tracks the 20 coded social signals, revealing patterns in LLM behavior and providing insights on enhancing LLM performance in healthcare dialogue analysis.", "conclusion": "The findings indicate the potential for LLMs to automate the analysis of social behaviors in clinical settings, which could improve patient-provider communication.", "key_contributions": ["Introduction of a system that tracks 20 distinct social signals in healthcare dialogues", "Evaluation of different LLM architectures and prompting styles", "Insights into enhancing LLM performance for social signal processing in clinical contexts."], "limitations": "The dataset is highly imbalanced, which may affect the robustness of the findings; further research is needed to generalize the results.", "keywords": ["Human-Computer Interaction", "large language models", "social signals", "healthcare communication", "emotion inference"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132.pdf", "abs": "https://arxiv.org/abs/2505.04132", "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public.", "AI": {"tldr": "This paper presents a three-step approach to improve the accessibility and comprehensibility of legal information for laypersons, including the use of large language models to generate legal questions.", "motivation": "To enhance access to legal information and make it comprehensible for individuals without legal education.", "method": "The approach involves three steps: translating legal documents into CLIC-pages, constructing a Legal Question Bank (LQB) with relevant legal questions, and designing an interactive CLIC Recommender (CRec) for user queries.", "result": "The study demonstrates that machine-generated questions (MGQs) are more scalable and diversified compared to human-composed questions (HCQs), although HCQs remain more precise.", "conclusion": "The three-step approach effectively makes legal knowledge more accessible to the public, and a prototype of the CRec is showcased as a solution.", "key_contributions": ["Formulated a three-step method for legal knowledge accessibility", "Developed a Legal Question Bank using large language models", "Created an interactive recommender system for legal inquiries"], "limitations": "", "keywords": ["Legal Information Access", "Natural Language Processing", "Legal Question Generation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.04135", "pdf": "https://arxiv.org/pdf/2505.04135.pdf", "abs": "https://arxiv.org/abs/2505.04135", "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models", "authors": ["Vihaan Miriyala", "Smrithi Bukkapatnam", "Lavanya Prahallad"], "categories": ["cs.CL", "cs.LG"], "comment": "5 pages", "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance.", "AI": {"tldr": "The paper investigates Chain-of-Thought (CoT) prompting with large language models to enhance sentiment classification accuracy in app store reviews, significantly improving results compared to simple prompting.", "motivation": "Traditional methods of sentiment analysis often overlook nuanced user feedback in app reviews, necessitating improved techniques for better classification.", "method": "The authors conducted an evaluation of CoT prompting against simple prompting using 2000 Amazon app reviews and compared predictions with human judgements.", "result": "CoT prompting improved classification accuracy from 84% to 93%, demonstrating its effectiveness in sentiment analysis.", "conclusion": "The findings underscore the advantages of incorporating explicit reasoning in LLMs for improved sentiment analysis.", "key_contributions": ["Demonstrated the effectiveness of Chain-of-Thought prompting in sentiment analysis.", "Quantified the accuracy improvement in sentiment classification from 84% to 93%.", "Showed the limitations of traditional numeric-based sentiment ratings."], "limitations": "The study is limited to app reviews from Amazon, which may not generalize to other contexts.", "keywords": ["Chain-of-Thought prompting", "large language models", "sentiment classification", "app store reviews"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.04146", "pdf": "https://arxiv.org/pdf/2505.04146.pdf", "abs": "https://arxiv.org/abs/2505.04146", "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety", "authors": ["Variath Madhupal Gautham Nair", "Vishal Varma Dantuluri"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure.", "AI": {"tldr": "The paper introduces the Unmasking the Canvas (UTC Benchmark; UTCB), a benchmark dataset aimed at evaluating the vulnerability of large language models to prompt-based jailbreaks in image generation.", "motivation": "To address the vulnerabilities in content safety checks of existing large language models that allow for the generation of compromising images through simple prompts.", "method": "The methodology involves structured prompt engineering, multilingual obfuscation techniques, and evaluation using the Groq-hosted LLaMA-3 model, supporting various prompting strategies and risk scoring.", "result": "The benchmark provides a dynamic framework for testing model vulnerabilities, categorizing outputs into verified and non-verified tiers, and includes rich metadata for all generated content.", "conclusion": "The UTCB aims to continuously evolve, adapting to new data sources and model behaviors to improve the assessment of LLM safety.", "key_contributions": ["Introduction of a structured benchmark for evaluating LLM vulnerabilities in image generation.", "Implementation of multilingual obfuscation techniques to enhance prompt testing.", "Development of a tiered system for verifying the safety of generated outputs."], "limitations": "The paper contains visual examples of adversarial inputs which may raise ethical concerns, and all outputs have been redacted.", "keywords": ["large language models", "image generation", "content safety", "adversarial inputs", "benchmark dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152.pdf", "abs": "https://arxiv.org/abs/2505.04152", "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings.", "AI": {"tldr": "The paper explores the use of large language models (LLMs) to analyze social signals in clinical dialogue, presenting a system that tracks 20 distinct social signals effectively.", "motivation": "To improve health and care outcomes by enhancing the analysis of interactions between healthcare providers and patients through understanding social signals.", "method": "The authors designed task-specific prompts and evaluated model performance across various architectures using an imbalanced dataset annotated with social signals like provider dominance and patient warmth.", "result": "The study introduces a system capable of tracking 20 coded social signals and reveals behavioral patterns of LLMs in understanding these signals.", "conclusion": "The findings suggest potential ways to enhance LLM performance in identifying social signals in clinical settings, contributing to the automation of conversation analysis.", "key_contributions": ["Development of a system to track 20 social signals in clinical dialogues", "Insights into LLM behavior concerning social signals", "Evaluation of different model configurations and their performance in healthcare contexts"], "limitations": "", "keywords": ["social signals", "large language models", "healthcare", "patient-provider interactions", "conversation analysis"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.04253", "pdf": "https://arxiv.org/pdf/2505.04253.pdf", "abs": "https://arxiv.org/abs/2505.04253", "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself", "authors": ["Maria Marina", "Nikolay Ivanov", "Sergey Pletenev", "Mikhail Salnikov", "Daria Galimzianova", "Nikita Krayko", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 2 tables", "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval.", "AI": {"tldr": "This paper presents lightweight LLM-independent adaptive retrieval methods that improve efficiency in QA systems while matching complex LLM-based methods' performance.", "motivation": "To address the hallucination issues in LLMs and the inefficiencies in existing retrieval methods, providing a more efficient alternative for adaptive retrieval.", "method": "The study investigates 27 features grouped into 7 categories to develop adaptive retrieval methods that utilize external information instead of relying on LLM-based uncertainty estimation.", "result": "The proposed methods demonstrate competitive QA performance compared to complex LLM-based approaches while offering significant efficiency improvements.", "conclusion": "The research highlights the effectiveness of utilizing external information for adaptive retrieval, underlining its potential to enhance performance in QA tasks without the heavy computational costs of LLMs.", "key_contributions": ["Introduction of LLM-independent adaptive retrieval methods", "Evaluation of 27 features for hybrid combinations", "Demonstration of performance efficiency gains compared to LLM-based methods."], "limitations": "The study may have limitations regarding the specific contexts or datasets where the proposed methods can be applied effectively.", "keywords": ["large language models", "adaptive retrieval", "efficient QA systems", "information retrieval", "external information"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.01495", "pdf": "https://arxiv.org/pdf/2410.01495.pdf", "abs": "https://arxiv.org/abs/2410.01495", "title": "OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition", "authors": ["Zheng Lian", "Haiyang Sun", "Licai Sun", "Haoyu Chen", "Lan Chen", "Hao Gu", "Zhuofan Wen", "Shun Chen", "Siyuan Zhang", "Hailiang Yao", "Bin Liu", "Rui Liu", "Shan Liang", "Ya Li", "Jiangyan Yi", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal Emotion Recognition (MER) is a critical research area that seeks\nto decode human emotions from diverse data modalities. However, existing\nmachine learning methods predominantly rely on predefined emotion taxonomies,\nwhich fail to capture the inherent complexity, subtlety, and multi-appraisal\nnature of human emotional experiences, as demonstrated by studies in psychology\nand cognitive science. To overcome this limitation, we advocate for introducing\nthe concept of open vocabulary into MER. This paradigm shift aims to enable\nmodels to predict emotions beyond a fixed label space, accommodating a flexible\nset of categories to better reflect the nuanced spectrum of human emotions. To\nachieve this, we propose a novel paradigm: Open-Vocabulary MER (OV-MER), which\nenables emotion prediction without being confined to predefined spaces.\nHowever, constructing a dataset that encompasses the full range of emotions for\nOV-MER is practically infeasible; hence, we present a comprehensive solution\nincluding a newly curated database, novel evaluation metrics, and a preliminary\nbenchmark. By advancing MER from basic emotions to more nuanced and diverse\nemotional states, we hope this work can inspire the next generation of MER,\nenhancing its generalizability and applicability in real-world scenarios. Code\nand dataset are available at: https://github.com/zeroQiaoba/AffectGPT.", "AI": {"tldr": "This paper introduces Open-Vocabulary Emotion Recognition (OV-MER), allowing for emotion prediction beyond predefined labels, addressing the complexity of human emotions.", "motivation": "Current machine learning methods in Multimodal Emotion Recognition are limited by fixed emotion taxonomies, which do not capture the complexity of human emotional experiences.", "method": "The paper proposes a novel paradigm called Open-Vocabulary MER (OV-MER), facilitating emotion prediction without confining to predefined categories and includes the creation of a comprehensive dataset and novel evaluation metrics.", "result": "The authors provide a newly curated database and preliminary benchmark for OV-MER, demonstrating improved generalizability and applicability in recognizing nuanced emotional states.", "conclusion": "By moving beyond basic emotions to a broader spectrum, this work aims to inspire advancements in MER and better applicability in various real-world contexts.", "key_contributions": ["Introduction of Open-Vocabulary MER (OV-MER)", "Development of a new comprehensive dataset for emotion recognition", "Novel evaluation metrics for assessing emotion prediction"], "limitations": "", "keywords": ["emotion recognition", "open vocabulary", "machine learning", "emotional states", "multimodal"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.04284", "pdf": "https://arxiv.org/pdf/2505.04284.pdf", "abs": "https://arxiv.org/abs/2505.04284", "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance", "authors": ["Sofia Jamil", "Aryan Dabad", "Bollampalli Areen Reddy", "Sriparna Saha", "Rajiv Misra", "Adil A. Shakur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable.", "AI": {"tldr": "This paper presents a novel method for summarizing adverse drug events (ADEs) in cancer treatment using a new dataset and framework called GASCADE.", "motivation": "To enhance pharmacovigilance and improve decision-making in cancer treatment by summarizing ADEs reported by patients.", "method": "Introduces the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset and the GASCAD framework, which combines LLMs for information extraction and T5 for summarization.", "result": "Demonstrates superior performance of the GASCADE framework in summarizing ADEs across various metrics, validated by both automated assessments and human evaluations.", "conclusion": "This work enhances understanding of patient concerns and supports more personalized cancer care, with publicly available code and dataset.", "key_contributions": ["Development of the MCADRS dataset for summarizing ADEs in cancer treatment", "Proposing the GASCADE framework that integrates LLMs with T5 for ADE summarization", "Application of novel alignment techniques in summarization tasks"], "limitations": "", "keywords": ["adverse drug events", "cancer treatment", "pharmacovigilance", "grouped summarization", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.10246", "pdf": "https://arxiv.org/pdf/2411.10246.pdf", "abs": "https://arxiv.org/abs/2411.10246", "title": "Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT", "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi", "Lei Liu", "Michael Flor"], "categories": ["cs.HC", "cs.CL"], "comment": "21 pages, 3 figures, 5 tables. Initially report in the edArXiv:xw6kz", "summary": "Collaborative problem solving (CPS) is widely recognized as a critical\n21st-century skill. Assessing CPS depends heavily on coding the communication\ndata using a construct-relevant framework, and this process has long been a\nmajor bottleneck to scaling up such assessments. Based on five datasets and two\ncoding frameworks, we demonstrate that ChatGPT can code communication data to a\nsatisfactory level, though performance varies across ChatGPT models, and\ndepends on the coding framework and task characteristics. Interestingly, newer\nreasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily\nyield better coding results. Additionally, we show that refining prompts based\non feedback from miscoded cases can improve coding accuracy in some instances,\nthough the effectiveness of this approach is not consistent across all tasks.\nThese findings offer practical guidance for researchers and practitioners in\ndeveloping scalable, efficient methods to analyze communication data in support\nof 21st-century skill assessment.", "AI": {"tldr": "The paper explores the effectiveness of ChatGPT in coding communication data for assessing Collaborative Problem Solving (CPS), highlighting variations in performance by model and framework.", "motivation": "To address the challenge of scaling assessments of Collaborative Problem Solving (CPS) skills by improving the coding process of communication data.", "method": "The study analyzes five datasets using two coding frameworks to evaluate how ChatGPT models perform in coding communication data relevant to CPS assessments.", "result": "ChatGPT can code communication data effectively, but its performance varies by model and task characteristics; performance does not necessarily improve with newer models, and prompt refinement can enhance accuracy in some cases.", "conclusion": "The findings provide insights for researchers and practitioners on developing efficient coding methods for analyzing communication data, supporting CPS skill assessments.", "key_contributions": ["Demonstrated ChatGPT's ability to code communication data for CPS assessments.", "Identified performance variations across different ChatGPT models and frameworks.", "Provided practical guidance on prompt refinement for improved coding accuracy."], "limitations": "Performance is inconsistent across different tasks, and not all refinement techniques improve results.", "keywords": ["Collaborative problem solving", "ChatGPT", "communication data", "CPS assessment", "artificial intelligence"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.04388", "pdf": "https://arxiv.org/pdf/2505.04388.pdf", "abs": "https://arxiv.org/abs/2505.04388", "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs", "authors": ["Dario Garcia-Gasulla", "Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Adrian Tormos", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Marta Gonzalez-Mallo", "Sergio Alvarez-Napagao", "Eduard Ayguadé-Parra", "Ulises Cortés"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886", "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.", "AI": {"tldr": "Aloe Beta models enhance open-source medical LLMs by optimizing data processing and improving safety and efficacy, demonstrating competitive performance in healthcare.", "motivation": "The development of competitive open-source models is essential to protect public interest in healthcare applications of LLMs.", "method": "The study builds on Llama 3.1 and Qwen 2.5, using a custom dataset with synthetic Chain of Thought examples, and evaluates model alignment using Direct Preference Optimization.", "result": "The Aloe Beta models perform competitively across healthcare benchmarks and have shown improvements in safety, particularly against bias and jailbreaking attacks.", "conclusion": "Aloe Beta models set a new standard in the open-source medical LLM field, balancing high performance with ethical considerations.", "key_contributions": ["Optimization of data preprocessing and training for LLMs", "Introduction of a comprehensive evaluation methodology", "Significant improvements in model safety and efficacy"], "limitations": "", "keywords": ["Large Language Models", "Healthcare", "Open-source", "Ethical AI", "Model Safety"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2411.15091", "pdf": "https://arxiv.org/pdf/2411.15091.pdf", "abs": "https://arxiv.org/abs/2411.15091", "title": "Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From AI Crawlers", "authors": ["Enze Liu", "Elisa Luo", "Shawn Shan", "Geoffrey M. Voelker", "Ben Y. Zhao", "Stefan Savage"], "categories": ["cs.HC"], "comment": "Accepted to IMC 25. Please cite the conference version", "summary": "The success of generative AI relies heavily on training on data scraped\nthrough extensive crawling of the Internet, a practice that has raised\nsignificant copyright, privacy, and ethical concerns. While few measures are\ndesigned to resist a resource-rich adversary determined to scrape a site,\ncrawlers can be impacted by a range of existing tools such as robots.txt, NoAI\nmeta tags, and active crawler blocking by reverse proxies.\n  In this work, we seek to understand the ability and efficacy of today's\nnetworking tools to protect content creators against AI-related crawling. For\ntargeted populations like human artists, do they have the technical knowledge\nand agency to utilize crawler-blocking tools such as robots.txt, and can such\ntools be effective? Using large scale measurements and a targeted user study of\n203 professional artists, we find strong demand for tools like robots.txt, but\nsignificantly constrained by critical hurdles in technical awareness, agency in\ndeploying them, and limited efficacy against unresponsive crawlers. We further\ntest and evaluate network-level crawler blockers provided by reverse proxies.\nDespite relatively limited deployment today, they offer stronger protections\nagainst AI crawlers, but still come with their own set of limitations.", "AI": {"tldr": "This paper investigates the effectiveness of current tools for protecting content against AI web crawlers, focusing on technical awareness and agency among human artists.", "motivation": "The paper addresses the copyright, privacy, and ethical concerns surrounding the use of crawlers by generative AI, particularly affecting content creators like artists.", "method": "The study utilizes large-scale measurements and a user study involving 203 professional artists to evaluate the demand and effectiveness of crawler-blocking tools like robots.txt and reverse proxies.", "result": "The findings indicate a strong demand for crawler-blocking tools among artists, but identify significant barriers in technical knowledge, deployment agency, and limited effectiveness against persistent crawlers.", "conclusion": "While reverse proxies can provide enhanced protection against AI crawlers, they are not widely implemented and have their own limitations.", "key_contributions": ["Assessment of tools used to protect against AI web crawling.", "User study revealing the technical challenges faced by artists.", "Evaluation of the efficacy of network-level crawler blockers."], "limitations": "Challenges include low technical awareness among artists and limited efficacy against sophisticated crawlers.", "keywords": ["AI crawling", "content protection", "robots.txt", "professional artists", "network security"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.04393", "pdf": "https://arxiv.org/pdf/2505.04393.pdf", "abs": "https://arxiv.org/abs/2505.04393", "title": "Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters", "authors": ["David Exler", "Mark Schutera", "Markus Reischl", "Luca Rettenberger"], "categories": ["cs.CL"], "comment": null, "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale.", "AI": {"tldr": "This paper evaluates the political biases of large language models (LLMs) using the context of German Bundestag votes, finding a prevalent bias towards left-leaning parties and highlighting the role of communication language and model origin in shaping these biases.", "motivation": "To assess the biases in LLMs as they are increasingly relied upon for information, understanding their influence on user opinions and decision-making processes, especially in political contexts.", "method": "The study quantifies the political bias of LLMs by using the Wahl-O-Mat score to measure alignment with German political parties, comparing models' alignment scores along with factors like communication language, model's origin, and release date.", "result": "Results show a significant bias towards left-leaning parties in larger LLMs and suggest communication language influences their political perspectives.", "conclusion": "The findings underscore the responsibility of corporations developing LLMs to mitigate biases as they can impact voter behavior and public opinion at large.", "key_contributions": ["Quantifies the political bias in LLMs using specific political metrics.", "Establishes a link between communication language and models' political biases.", "Highlights the implications of LLM biases on public opinion and decision-making."], "limitations": "Focuses on political bias in a specific context (German Bundestag votes), limiting generalizability to other areas or different political landscapes.", "keywords": ["political bias", "large language models", "Wahl-O-Mat", "user education", "LLM influence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.18438", "pdf": "https://arxiv.org/pdf/2411.18438.pdf", "abs": "https://arxiv.org/abs/2411.18438", "title": "Adaptive Gen-AI Guidance in Virtual Reality: A Multimodal Exploration of Engagement in Neapolitan Pizza-Making", "authors": ["Ka Hei Carrie Lau", "Sema Sen", "Philipp Stark", "Efe Bozkir", "Enkelejda Kasneci"], "categories": ["cs.HC"], "comment": null, "summary": "Virtual reality (VR) offers promising opportunities for procedural learning,\nparticularly in preserving intangible cultural heritage. Advances in generative\nartificial intelligence (Gen-AI) further enrich these experiences by enabling\nadaptive learning pathways. However, evaluating such adaptive systems using\ntraditional temporal metrics remains challenging due to the inherent\nvariability in Gen-AI response times. To address this, our study employs\nmultimodal behavioural metrics, including visual attention, physical\nexploratory behaviour, and verbal interaction, to assess user engagement in an\nadaptive VR environment. In a controlled experiment with 54 participants, we\ncompared three levels of adaptivity (high, moderate, and non-adaptive baseline)\nwithin a Neapolitan pizza-making VR experience. Results show that moderate\nadaptivity optimally enhances user engagement, significantly reducing\nunnecessary exploratory behaviour and increasing focused visual attention on\nthe AI avatar. Our findings suggest that a balanced level of adaptive AI\nprovides the most effective user support, offering practical design\nrecommendations for future adaptive educational technologies.", "AI": {"tldr": "The study explores using virtual reality (VR) and generative AI to enhance procedural learning and user engagement through adaptive learning pathways.", "motivation": "To evaluate user engagement in adaptive VR environments when traditional metrics fail due to variability in Gen-AI response times.", "method": "A controlled experiment with 54 participants assessed three levels of adaptivity (high, moderate, non-adaptive baseline) during a Neapolitan pizza-making VR experience, utilizing multimodal behavioral metrics.", "result": "Moderate adaptivity was found to significantly enhance user engagement by reducing unnecessary exploratory behavior and increasing focused visual attention on the AI avatar.", "conclusion": "A balanced level of adaptive AI provides effective user support and guides practical design recommendations for future educational technologies.", "key_contributions": ["Integration of Gen-AI in VR for procedural learning", "Use of multimodal behavioral metrics to evaluate user engagement", "Recommendations for adaptive educational technology design"], "limitations": "", "keywords": ["Virtual Reality", "Generative AI", "User Engagement", "Adaptive Learning", "Educational Technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.04406", "pdf": "https://arxiv.org/pdf/2505.04406.pdf", "abs": "https://arxiv.org/abs/2505.04406", "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025", "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++.", "AI": {"tldr": "This paper presents YABLoCo, a long context code generation benchmark for evaluating Large Language Models (LLMs) on function body generation in large C and C++ codebases.", "motivation": "The need for benchmarking LLMs on code generation tasks in large software repositories, which can contain millions of lines of code.", "method": "Introduction of YABLoCo, a benchmark featuring 215 function samples from large repositories, with accompanying metadata, documentations, and call graphs.", "result": "The benchmark facilitates evaluation of LLMs on function body generation in extensive C and C++ repositories ranging from 200K to 2,000K lines of code.", "conclusion": "The established benchmark and evaluation pipeline allow comprehensive assessment of LLM performance in real-world coding scenarios.", "key_contributions": ["Introduction of the YABLoCo benchmark for long context code generation in C and C++.", "Inclusion of large repositories with millions of lines of code for benchmark evaluation.", "Development of a scalable evaluation pipeline and visual analysis tool for generated code."], "limitations": "", "keywords": ["Large Language Models", "Code Generation", "Benchmarking", "C/C++", "Software Repositories"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.16566", "pdf": "https://arxiv.org/pdf/2501.16566.pdf", "abs": "https://arxiv.org/abs/2501.16566", "title": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models", "authors": ["Zheng Lian", "Haoyu Chen", "Lan Chen", "Haiyang Sun", "Licai Sun", "Yong Ren", "Zebang Cheng", "Bin Liu", "Rui Liu", "Xiaojiang Peng", "Jiangyan Yi", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "The emergence of multimodal large language models (MLLMs) advances multimodal\nemotion recognition (MER) to the next level, from naive discriminative tasks to\ncomplex emotion understanding with advanced video understanding abilities and\nnatural language description. However, the current community suffers from a\nlack of large-scale datasets with intensive, descriptive emotion annotations,\nas well as a multimodal-centric framework to maximize the potential of MLLMs\nfor emotion understanding. To address this, we establish a new benchmark for\nMLLM-based emotion understanding with a novel dataset (MER-Caption) and a new\nmodel (AffectGPT). Utilizing our model-based crowd-sourcing data collection\nstrategy, we construct the largest descriptive emotion dataset to date (by\nfar), featuring over 2K fine-grained emotion categories across 115K samples. We\nalso introduce the AffectGPT model, designed with pre-fusion operations to\nenhance multimodal integration. Finally, we present MER-UniBench, a unified\nbenchmark with evaluation metrics tailored for typical MER tasks and the\nfree-form, natural language output style of MLLMs. Extensive experimental\nresults show AffectGPT's robust performance across various MER tasks. We have\nreleased both the code and the dataset to advance research and development in\nemotion understanding: https://github.com/zeroQiaoba/AffectGPT.", "AI": {"tldr": "This paper presents a benchmark for multimodal emotion recognition (MER) using a novel dataset and model, addressing the need for extensive emotion annotations and an integrated framework for MLLMs.", "motivation": "The paper addresses the limitation of the current community regarding the availability of large-scale datasets with detailed emotion annotations for improving multimodal emotion recognition tasks.", "method": "The authors introduce a new dataset, MER-Caption, and a model called AffectGPT, which incorporates pre-fusion operations to enhance multimodal integration, along with a unified benchmarking framework, MER-UniBench, for evaluating MER tasks.", "result": "AffectGPT demonstrates robust performance across a variety of MER tasks, supported by a dataset consisting of over 115K samples and 2K fine-grained emotion categories.", "conclusion": "The study significantly advances the state-of-the-art in emotion understanding through its contributions of a novel dataset, model, and benchmarking framework, with resources available for further research.", "key_contributions": ["Introduction of the MER-Caption dataset for emotion recognition", "Development of the AffectGPT model for multimodal integration", "Creation of the MER-UniBench framework for evaluating MER tasks"], "limitations": "", "keywords": ["multimodal emotion recognition", "large language models", "emotion datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416.pdf", "abs": "https://arxiv.org/abs/2505.04416", "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.", "AI": {"tldr": "The paper introduces OBLIVIATE, a framework for unlearning sensitive content in large language models while maintaining their utility.", "motivation": "Address the risks of large language models memorizing sensitive, copyrighted, or toxic content.", "method": "The framework extracts target tokens, builds retain sets, and fine-tunes using a loss function with masking, distillation, and world fact components, implemented using low-rank adapters for efficiency.", "result": "Experiments on various datasets show that OBLIVIATE effectively mitigates membership inference attacks while preserving model utility and fluency.", "conclusion": "OBLIVIATE is a robust solution for unlearning while maintaining the integrity and performance of large language models.", "key_contributions": ["Introduction of a structured unlearning process to remove targeted data from LLMs.", "Utilization of low-rank adapters to enhance efficiency in the unlearning process.", "Demonstrated effectiveness against membership inference attacks with a minimal impact on retained data."], "limitations": "", "keywords": ["unlearning", "large language models", "membership inference", "model utility", "low-rank adapters"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2505.04507", "pdf": "https://arxiv.org/pdf/2505.04507.pdf", "abs": "https://arxiv.org/abs/2505.04507", "title": "Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts", "authors": ["Ilya Koziev"], "categories": ["cs.CL"], "comment": null, "summary": "The quality of natural language texts in fine-tuning datasets plays a\ncritical role in the performance of generative models, particularly in\ncomputational creativity tasks such as poem or song lyric generation. Fluency\ndefects in generated poems significantly reduce their value. However, training\ntexts are often sourced from internet-based platforms without stringent quality\ncontrol, posing a challenge for data engineers to manage defect levels\neffectively.\n  To address this issue, we propose the use of automated linguistic anomaly\ndetection to identify and filter out low-quality texts from training datasets\nfor creative models. In this paper, we present a comprehensive comparison of\nunsupervised and supervised text anomaly detection approaches, utilizing both\nsynthetic and human-labeled datasets. We also introduce the RUPOR dataset, a\ncollection of Russian-language human-labeled poems designed for cross-sentence\ngrammatical error detection, and provide the full evaluation code. Our work\naims to empower the community with tools and insights to improve the quality of\ntraining datasets for generative models in creative domains.", "AI": {"tldr": "This paper addresses the quality of fine-tuning datasets for generative models, focusing on automated linguistic anomaly detection to improve training data for computational creativity tasks like poetry generation.", "motivation": "The quality of natural language texts in fine-tuning datasets critically affects generative model performance, especially in tasks like poem generation where fluency is key.", "method": "A comprehensive comparison of unsupervised and supervised text anomaly detection methods was conducted using synthetic and human-labeled datasets.", "result": "The study introduces the RUPOR dataset, which is specifically designed for detecting grammatical errors in Russian-language poems, and presents evaluation code for the community.", "conclusion": "The findings aim to provide tools and insights for enhancing the quality of training datasets for generative models in creative fields.", "key_contributions": ["Introduction of automated linguistic anomaly detection for training datasets.", "Comparison of unsupervised and supervised detection methods.", "Release of the RUPOR dataset for grammatical error detection."], "limitations": "", "keywords": ["natural language processing", "generative models", "anomaly detection", "machine learning", "data quality"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.04519", "pdf": "https://arxiv.org/pdf/2505.04519.pdf", "abs": "https://arxiv.org/abs/2505.04519", "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs", "authors": ["Yehui Tang", "Yichun Yin", "Yaoyuan Wang", "Hang Zhou", "Yu Pan", "Wei Guo", "Ziyang Zhang", "Miao Rang", "Fangcheng Liu", "Naifu Zhang", "Binghan Li", "Yonghan Dong", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Yin Li", "Dandan Tu", "Can Chen", "Youliang Yan", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Botian Huang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Da Kuang", "Fei Liu", "Gang Huang", "Jiansheng Wei", "Jiarui Qin", "Jie Ran", "Jinpeng Li", "Jun Zhao", "Liang Dai", "Lin Li", "Liqun Deng", "Peifeng Qin", "Pengyuan Zeng", "Qiang Gu", "Shaohua Tang", "Shengjun Cheng", "Tao Gao", "Tao Yu", "Tianshu Li", "Tianyu Bi", "Wei He", "Weikai Mao", "Wenyong Huang", "Wulong Liu", "Xiabing Li", "Xianzhi Yu", "Xueyu Wu", "Xu He", "Yangkai Du", "Yan Xu", "Ye Tian", "Yimeng Wu", "Yongbing Huang", "Yong Tian", "Yong Zhu", "Yue Li", "Yufei Wang", "Yuhang Gai", "Yujun Li", "Yu Luo", "Yunsheng Ni", "Yusen Sun", "Zelin Chen", "Zhe Liu", "Zhicheng Liu", "Zhipeng Tu", "Zilin Ding", "Zongyuan Zhan"], "categories": ["cs.CL"], "comment": null, "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference.", "AI": {"tldr": "This paper presents efficient training methods for large sparse language models using Ascend NPUs, culminating in the development of Pangu Ultra MoE with 718 billion parameters.", "motivation": "With the rise of large language models (LLMs) using Mixture of Experts (MoE), there is a need to manage the challenges posed by their massive scales in terms of software and hardware systems.", "method": "The authors utilize simulation to compare various model hyperparameters and choose configurations optimal for Ascend NPUs, while focusing on Expert Parallelism to enhance communication and memory efficiency.", "result": "Achieved an MFU of 30.0% during the training of Pangu Ultra MoE on 6K Ascend NPUs, demonstrating performance on par with DeepSeek R1.", "conclusion": "The study provides a viable method for efficient training of large-scale sparse LLMs and insights into their behaviors for future research.", "key_contributions": ["Development of Pangu Ultra MoE with 718 billion parameters", "Demonstration of efficient training methods on Ascend NPUs", "Optimization of communication and memory efficiency in training processes"], "limitations": "", "keywords": ["Sparse LLMs", "Mixture of Experts", "Ascend NPUs", "Efficient Training", "Deep Learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531.pdf", "abs": "https://arxiv.org/abs/2505.04531", "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies.", "AI": {"tldr": "This paper reviews strategies for improving generative language modeling for low-resource languages (LRLs), identifying and evaluating various technical approaches and highlighting the need for more inclusive AI tools.", "motivation": "To address linguistic inequality in natural language processing (NLP) due to the dominance of high-resource languages in generative language models.", "method": "A systematic review of 54 studies focusing on techniques for data scarcity in generative language modeling for LRLs, analyzing monolingual data augmentation, back-translation, multilingual training, and prompt engineering.", "result": "The review reveals a strong reliance on transformer models, a concentration of research on a limited number of LRLs, and inconsistent evaluation methods across studies.", "conclusion": "Recommendations are provided on how to apply existing methods to a broader range of LRLs, along with a discussion of open challenges in creating equitable generative language systems.", "key_contributions": ["First systematic review on generative language modeling for low-resource languages.", "Evaluation of various technical approaches to address data scarcity.", "Recommendations for extending language modeling methods to underrepresented languages."], "limitations": "The study highlights a lack of consistent evaluation methods and an over-focus on a small subset of low-resource languages.", "keywords": ["natural language processing", "generative language modeling", "low-resource languages", "data scarcity", "inclusive AI"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.04588", "pdf": "https://arxiv.org/pdf/2505.04588.pdf", "abs": "https://arxiv.org/abs/2505.04588", "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "authors": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Yan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.", "AI": {"tldr": "ZeroSearch is a reinforcement learning framework for improving LLMs' search capabilities without relying on real search engines.", "motivation": "To enhance the reasoning and generation capabilities of LLMs through effective information searching and address challenges in existing RL approaches due to document quality and API costs.", "method": "ZeroSearch begins with supervised fine-tuning of the LLM to create a retrieval module, followed by RL training using a curriculum-based rollout strategy that gradually decreases document quality to improve reasoning.", "result": "ZeroSearch demonstrates that LLMs can effectively enhance their search capabilities, achieving performance comparable to real search engines with 7B and surpassing it with 14B models.", "conclusion": "The framework generalizes well across various LLM models and is compatible with different reinforcement learning algorithms.", "key_contributions": ["Introduction of the ZeroSearch framework for LLMs' search capabilities", "Use of a curriculum-based approach to RL training", "Demonstration of performance surpassing real search engines with larger models."], "limitations": "", "keywords": ["reinforcement learning", "large language models", "search capabilities", "information retrieval", "curriculum learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.02116", "pdf": "https://arxiv.org/pdf/2411.02116.pdf", "abs": "https://arxiv.org/abs/2411.02116", "title": "Advancements and limitations of LLMs in replicating human color-word associations", "authors": ["Makoto Fukushima", "Shusuke Eshita", "Hiroshige Fukuhara"], "categories": ["cs.CL", "cs.CV", "cs.GR", "cs.HC"], "comment": "20 pages, 7 figures, 3 tables", "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations.", "AI": {"tldr": "The study investigates the ability of large language models (LLMs) to match human color-word associations, revealing advancements and limitations across model generations.", "motivation": "To explore how well LLMs replicate human color-word associations, which are crucial in cognition and design, and to analyze performance variations across different color categories and associated words.", "method": "We compared various LLM generations (GPT-3 to GPT-4o) against a dataset of color-word associations collected from over 10,000 participants, evaluating their predictive accuracy in word assignments for different colors.", "result": "GPT-4o showed the highest accuracy in predicting color associations but reached only about 50% median performance, struggling particularly with categories related to emotions, despite aligning closely with human color discrimination patterns.", "conclusion": "The findings highlight advancements in LLM capabilities while also indicating significant systematic differences in how humans and LLMs relate colors to words, suggesting the need for further exploration of semantic memory structures between the two.", "key_contributions": ["Comparison of LLM performance across generations in color-word association tasks", "Demonstration of performance variations across different word categories and colors", "Identification of systematic differences in semantic memory between humans and LLMs"], "limitations": "The highest median performance was only around 50%, indicating ongoing limitations in LLMs' understanding of color-word associations, especially with emotional context.", "keywords": ["color-word association", "large language models", "human cognition", "semantic memory", "machine learning"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2305.16867", "pdf": "https://arxiv.org/pdf/2305.16867.pdf", "abs": "https://arxiv.org/abs/2305.16867", "title": "Playing repeated games with Large Language Models", "authors": ["Elif Akata", "Lion Schulz", "Julian Coda-Forno", "Seong Joon Oh", "Matthias Bethge", "Eric Schulz"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are increasingly used in applications where they interact with humans\nand other agents. We propose to use behavioural game theory to study LLM's\ncooperation and coordination behaviour. We let different LLMs play finitely\nrepeated $2\\times2$ games with each other, with human-like strategies, and\nactual human players. Our results show that LLMs perform particularly well at\nself-interested games like the iterated Prisoner's Dilemma family. However,\nthey behave sub-optimally in games that require coordination, like the Battle\nof the Sexes. We verify that these behavioural signatures are stable across\nrobustness checks. We additionally show how GPT-4's behaviour can be modulated\nby providing additional information about its opponent and by using a \"social\nchain-of-thought\" (SCoT) strategy. This also leads to better scores and more\nsuccessful coordination when interacting with human players. These results\nenrich our understanding of LLM's social behaviour and pave the way for a\nbehavioural game theory for machines.", "AI": {"tldr": "This paper investigates the cooperation and coordination behaviors of large language models (LLMs) through behavioral game theory, demonstrating their effectiveness in self-interested scenarios while highlighting limitations in coordination tasks.", "motivation": "To understand the cooperation and coordination behavior of LLMs when interacting with humans and other agents using behavioral game theory.", "method": "LLMs are analyzed in finitely repeated 2x2 games against each other and human players, focusing on strategies and outcomes in various game types.", "result": "LLMs excel in self-interested games like the iterated Prisoner's Dilemma, but struggle with coordination in games like the Battle of the Sexes. GPT-4's performance improves with opponent information and a 'social chain-of-thought' strategy.", "conclusion": "The findings enhance the comprehension of LLM behavior in social contexts and propose a framework for applying behavioral game theory to machine interactions.", "key_contributions": ["Demonstrated LLM's performance in game theory settings with human-like strategies.", "Identified strengths and weaknesses in LLM cooperation and coordination.", "Introduced the 'social chain-of-thought' strategy to enhance LLM interaction with humans."], "limitations": "The study may not account for all potential complexities in real-world human-LLM interactions.", "keywords": ["LLM", "behavioral game theory", "cooperation", "coordination", "social behavior"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2308.15022", "pdf": "https://arxiv.org/pdf/2308.15022.pdf", "abs": "https://arxiv.org/abs/2308.15022", "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models", "authors": ["Qingyue Wang", "Yanhe Fu", "Yanan Cao", "Shuai Wang", "Zhiliang Tian", "Liang Ding"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later.", "AI": {"tldr": "Proposes a method to enhance long-term memory in large language models for consistent responses in long conversations.", "motivation": "Address issues of memory recall and response consistency in long conversations with LLMs.", "method": "Recursively generates summaries/memory using LLMs to memorize conversation contexts and produce new memory for consistent dialogue.", "result": "Experiments show improved consistency in responses across both open and closed LLMs on public datasets, supporting long-context and retrieval-enhanced dialogues.", "conclusion": "The method offers a promising solution for LLMs to handle extremely long contexts effectively.", "key_contributions": ["Recursive memory generation for LLMs", "Enhanced consistency in long conversations", "Integration with long-context and retrieval-enhanced LLMs"], "limitations": "", "keywords": ["large language models", "long-term memory", "dialogue consistency", "recursion", "memory generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2403.19346", "pdf": "https://arxiv.org/pdf/2403.19346.pdf", "abs": "https://arxiv.org/abs/2403.19346", "title": "Large Language Models Are Struggle to Cope with Unreasonability in Math Problems", "authors": ["Jingyuan Ma", "Damai Dai", "Zihang Yuan", "Rui li", "Weilin Luo", "Bin Wang", "Qun Liu", "Lei Sha", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "32 pages, 8 figures", "summary": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting.", "AI": {"tldr": "This paper introduces the Unreasonable Math Problem benchmark to evaluate LLMs' capacity to address unconventional math problems with unreasonability.", "motivation": "To investigate the unexplored capacity of LLMs in handling math problems with internal inconsistencies and flawed assumptions.", "method": "The authors propose a novel benchmark called Unreasonable Math Problem (UMP), which consists of a collection of curated unreasonable math questions and conduct experiments across 19 LLMs.", "result": "The results show that even state-of-the-art models like GPT-4o have limited performance (0.6) on the UMP, and other reasoning models demonstrate instability and overthinking.", "conclusion": "The paper highlights both the potential and limitations of LLMs in recognizing unreasonable inputs in math problems.", "key_contributions": ["Introduction of the Unreasonable Math Problem benchmark", "Empirical evaluation of 19 LLMs on unreasonable math problems", "Insights into improving LLMs' recognition of unreasonable inputs"], "limitations": "The UMP may not cover all possible unreasonable scenarios and the performance metrics are limited to specific models tested.", "keywords": ["LLMs", "math problems", "benchmark", "reasoning", "unreasonability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.01495", "pdf": "https://arxiv.org/pdf/2406.01495.pdf", "abs": "https://arxiv.org/abs/2406.01495", "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "authors": ["Zi-Yi Dou", "Cheng-Fu Yang", "Xueqing Wu", "Kai-Wei Chang", "Nanyun Peng"], "categories": ["cs.CL"], "comment": null, "summary": "Finetuning language agents with reasoning-action trajectories is effective,\nbut obtaining these trajectories from human annotations or stronger models is\ncostly and sometimes impractical. In this paper, we investigate the use of\nself-training in language agents, which can generate supervision from the agent\nitself, offering a promising alternative without relying on human or stronger\nmodel demonstrations. Self-training, however, requires high-quality\nmodel-generated samples, which are hard to obtain for challenging language\nagent tasks. To address this, we present Reflection-Reinforced Self-Training\n(Re-ReST), which uses a \\textit{reflector} to refine low-quality generated\nsamples during self-training. The reflector takes the agent's output and\nfeedback from an external environment (e.g., unit test results in code\ngeneration) to produce improved samples. This technique enhances the quality of\ninferior samples and efficiently enriches the self-training dataset with\nhigher-quality samples. We conduct extensive experiments on open-source\nlanguage agents across tasks, including multi-hop question answering,\nsequential decision-making, code generation, visual question answering, and\ntext-to-image generation. The results demonstrate the effectiveness of\nself-training and Re-ReST in language agent tasks, with self-training improving\nbaselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further\nboosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also\nconfirm the efficiency of using a reflector to generate high-quality samples\nfor self-training. Moreover, we demonstrate a method to employ reflection\nduring inference without ground-truth feedback, addressing the limitation of\nprevious reflection work. Our code is released at\nhttps://github.com/PlusLabNLP/Re-ReST.", "AI": {"tldr": "This paper introduces Reflection-Reinforced Self-Training (Re-ReST) for finetuning language agents using self-generated supervision, enhancing the quality of model-generated samples to improve performance on various tasks.", "motivation": "Obtaining reasoning-action trajectories for finetuning language agents is costly and impractical; hence, there is a need for effective self-training methods.", "method": "Reflection-Reinforced Self-Training (Re-ReST) utilizes a 'reflector' to refine the quality of model-generated samples during self-training by incorporating feedback from external environments.", "result": "Self-training improved performance on HotpotQA by 7.6% and AlfWorld by 28.4%, while Re-ReST further enhanced these by 2.0% and 14.1%, respectively.", "conclusion": "The study validates the effectiveness of self-training and introduces a new approach to utilize reflection during inference without ground-truth feedback, addressing limitations of prior work.", "key_contributions": ["Introduction of Reflection-Reinforced Self-Training (Re-ReST) for language agents", "Demonstration of improved sample quality through a reflector mechanism", "Validation of self-training efficacy across various language agent tasks"], "limitations": "", "keywords": ["self-training", "language agents", "reflection", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.02044", "pdf": "https://arxiv.org/pdf/2406.02044.pdf", "abs": "https://arxiv.org/abs/2406.02044", "title": "Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA", "authors": ["Hussein Jawad", "Yassine Chenik", "Nicolas J. -B. Brunel"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) has exposed critical\nsecurity and ethical vulnerabilities, particularly their susceptibility to\nadversarial manipulations. This paper introduces QROA, a novel black-box\njailbreak method designed to identify adversarial suffixes that can bypass LLM\nalignment safeguards when appended to a malicious instruction. Unlike existing\nsuffix-based jailbreak approaches, QROA does not require access to the model's\nlogit or any other internal information. It also eliminates reliance on\nhuman-crafted templates, operating solely through the standard query-response\ninterface of LLMs. By framing the attack as an optimization bandit problem,\nQROA employs a surrogate model and token level optimization to efficiently\nexplore suffix variations. Furthermore, we propose QROA-UNV, an extension that\nidentifies universal adversarial suffixes for individual models, enabling\none-query jailbreaks across a wide range of instructions. Testing on multiple\nmodels demonstrates Attack Success Rate (ASR) greater than 80\\%. These findings\nhighlight critical vulnerabilities, emphasize the need for advanced defenses,\nand contribute to the development of more robust safety evaluations for secure\nAI deployment. The code is made public on the following link:\nhttps://github.com/qroa/QROA", "AI": {"tldr": "The paper introduces QROA, a black-box jailbreak method that identifies suffixes to bypass alignment safeguards of Large Language Models without needing model internals or human-crafted templates, achieving over 80% Attack Success Rate.", "motivation": "This work addresses the critical security and ethical vulnerabilities in Large Language Models, particularly their susceptibility to adversarial manipulations and the need for robust safety evaluations.", "method": "QROA frames the jailbreak attack as an optimization bandit problem, utilizing a surrogate model and token level optimization to explore suffix variations efficiently.", "result": "Testing across multiple models revealed an Attack Success Rate (ASR) greater than 80%, indicating significant vulnerabilities in LLMs.", "conclusion": "The findings highlight the urgent need for advanced defenses and more robust safety evaluations for the deployment of AI systems, with the QROA and QROA-UNV methods offering critical insights.", "key_contributions": ["QROA provides a novel approach to identify adversarial suffixes without requiring internal model access.", "It eliminates the reliance on human-designed templates for suffix generation.", "QROA-UNV allows for one-query jailbreaks across different instructions, enhancing the method's versatility."], "limitations": "", "keywords": ["Large Language Models", "adversarial attacks", "security vulnerabilities", "jailbreak", "optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.18501", "pdf": "https://arxiv.org/pdf/2406.18501.pdf", "abs": "https://arxiv.org/abs/2406.18501", "title": "Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming", "authors": ["Zhenghao Zhou", "Robert Frank", "R. Thomas McCoy"], "categories": ["cs.CL"], "comment": "This version is accepted to NAACL 2025\n  (https://aclanthology.org/2025.naacl-long.586/)", "summary": "Large language models (LLMs) have shown the emergent capability of in-context\nlearning (ICL). One line of research has claimed that ICL is functionally\nequivalent to gradient descent, a type of error-driven learning mechanism. In\nthis paper, we introduce a new way of diagnosing whether ICL is functionally\nperforming error-driven learning. Our approach is based on the inverse\nfrequency effect (IFE) -- a phenomenon in which an agent's behavior is\ninfluenced to a greater degree when presented with improbable examples as\ncompared to more likely ones. The IFE has previously been identified in\npsycholinguistics where humans exhibit the IFE in the context of structural\npriming (the tendency for people to produce sentence structures they have\nencountered recently). In that context, the IFE has been used as evidence that\nhuman structural priming must involve error-driven learning mechanisms. In our\nexperiments, we simulated structural priming with ICL and found that LLMs\nindeed display the IFE, with the effect being stronger in larger models. We\nconclude that at least in the case we studied, ICL is indeed a type of\nerror-driven learning, supporting the hypothesis that an error signal is\nimplicitly computed in the forward pass during ICL. Our results suggest that\nboth humans and LLMs make use of error-driven processing mechanisms in on-line\nprocessing.", "AI": {"tldr": "This paper investigates whether in-context learning (ICL) in large language models is an error-driven learning mechanism, finding evidence that it is akin to human structural priming through the inverse frequency effect (IFE).", "motivation": "To determine if in-context learning (ICL) within large language models (LLMs) is functionally similar to gradient descent and represents an error-driven learning mechanism.", "method": "Experiments were conducted simulating structural priming with ICL and analyzing the prevalence of the inverse frequency effect (IFE) in LLM responses.", "result": "LLMs exhibited the IFE, with a stronger effect observed in larger models, indicating that ICL aligns with error-driven learning processes.", "conclusion": "The findings suggest that ICL in LLMs may incorporate error-driven learning mechanisms similar to those found in human language processing.", "key_contributions": ["Introduces a novel method for diagnosing ICL as an error-driven learning mechanism.", "Demonstrates the application of the inverse frequency effect in evaluating LLM behavior.", "Provides experimental evidence supporting the hypothesis that LLMs utilize error-driven processing."], "limitations": "", "keywords": ["In-Context Learning", "Large Language Models", "Error-Driven Learning", "Inverse Frequency Effect", "Structural Priming"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.13184", "pdf": "https://arxiv.org/pdf/2408.13184.pdf", "abs": "https://arxiv.org/abs/2408.13184", "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning", "authors": ["Hourui Deng", "Hongjie Zhang", "Jie Ou", "Chaosheng Feng"], "categories": ["cs.CL"], "comment": "Accepted by ICIC 2025", "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.", "AI": {"tldr": "This paper presents the S2RCQL model to enhance spatial reasoning in LLMs for path-planning tasks, improving their performance in maze environments.", "motivation": "To address the challenges of spatial and context inconsistency hallucinations in LLMs during long-term path-planning in simple environments.", "method": "The study proposes the Spatial-to-Relational Transformation approach for converting spatial prompts into entity relations and a path-planning algorithm based on Q-learning to guide LLMs in optimal path learning.", "result": "S2RCQL achieved a 23%--40% improvement in success and optimality rates in path-planning tasks compared to existing methods.", "conclusion": "The proposed methods effectively mitigate hallucinations in LLMs and enhance their reasoning abilities, demonstrating significant improvements in path-planning.", "key_contributions": ["Introduction of the Spatial-to-Relational Transformation approach.", "Development of a Q-learning based path-planning algorithm for LLMs.", "Implementation of reverse curriculum learning to improve learning efficiency."], "limitations": "", "keywords": ["LLMs", "Spatial Reasoning", "Q-learning", "Path-planning", "Curriculum Learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2411.02116", "pdf": "https://arxiv.org/pdf/2411.02116.pdf", "abs": "https://arxiv.org/abs/2411.02116", "title": "Advancements and limitations of LLMs in replicating human color-word associations", "authors": ["Makoto Fukushima", "Shusuke Eshita", "Hiroshige Fukuhara"], "categories": ["cs.CL", "cs.CV", "cs.GR", "cs.HC"], "comment": "20 pages, 7 figures, 3 tables", "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations.", "AI": {"tldr": "The study examines the ability of Large Language Models (LLMs) to replicate human color-word associations, revealing a generational progression in performance but persistent limitations compared to human cognition.", "motivation": "To explore the capacity of LLMs in understanding and predicting color-word associations, an area that has not been extensively studied despite the advancements in LLM capabilities.", "method": "Comparison of LLM generations (GPT-3 to GPT-4o) against data collected from over 10,000 Japanese participants on their color-word associations using 17 colors and 80 words across various categories.", "result": "GPT-4o showed the highest accuracy in predicting color associations among LLMs, yet median performance was around 50%, with variations in success across word categories. LLMs performed well in categories like Rhythm and Landscape but struggled with Emotions.", "conclusion": "Despite improvements in LLMs' understanding of color-word associations, significant discrepancies remain compared to human associations, suggesting different underlying semantic memory structures.", "key_contributions": ["Demonstration of LLM performance progression in color-word association tasks across generations", "Establishment of a correlation between LLM color discrimination and human patterns", "Identification of specific word categories where LLMs excel or struggle"], "limitations": "The highest accuracy of LLMs was only about 50%, indicating room for improvement in understanding human-like associations.", "keywords": ["Large Language Models", "color-word associations", "Human-Computer Interaction", "Machine Learning", "semantics"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2501.05040", "pdf": "https://arxiv.org/pdf/2501.05040.pdf", "abs": "https://arxiv.org/abs/2501.05040", "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution", "authors": ["Chengxing Xie", "Bowen Li", "Chang Gao", "He Du", "Wai Lam", "Difan Zou", "Kai Chen"], "categories": ["cs.CL"], "comment": "Our code, data, and model will be released at\n  https://github.com/InternLM/SWE-Fixer", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\ncompetitive performance among open-source models with scores of 22.0% and\n30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on\nLite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,\nour approach requires only two model calls per instance, making it\nsignificantly more efficient than existing methods. These results highlight the\neffectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make\nour model, dataset, and code publicly available at\nhttps://github.com/InternLM/SWE-Fixer.", "AI": {"tldr": "SWE-Fixer is an open-source framework for resolving GitHub issues by fixing code, featuring a two-module system for file retrieval and code editing, achieving competitive performance with efficiency.", "motivation": "To address the limitations of existing proprietary LLMs in software engineering tasks and improve reproducibility and accessibility in code fixing.", "method": "SWE-Fixer includes a code file retrieval module using BM25 for initial file selection and a code editing module for generating code patches, trained on a newly compiled dataset of 110K GitHub issues.", "result": "SWE-Fixer achieves competitive results on SWE-Bench Lite (22.0%) and Verified (30.2%) benchmarks, reaching a state-of-the-art 24.7% and 32.8% with filtering techniques.", "conclusion": "SWE-Fixer demonstrates effectiveness in practical code-fixing tasks, easily accessible due to its open-source nature and efficient model usage.", "key_contributions": ["Introduction of SWE-Fixer, an open-source framework for GitHub issue resolution", "Development of a large dataset of GitHub issues and corresponding patches", "Significantly improved efficiency with only two model calls per instance"], "limitations": "", "keywords": ["Large Language Models", "Code fixing", "Open-source framework", "Software engineering", "Machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.00290", "pdf": "https://arxiv.org/pdf/2502.00290.pdf", "abs": "https://arxiv.org/abs/2502.00290", "title": "Estimating LLM Uncertainty with Logits", "authors": ["Huan Ma", "Jingdong Chen", "Joey Tianyi Zhou", "Guangyu Wang", "Changqing Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Fixed some data errors in Table 1", "summary": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.", "AI": {"tldr": "This paper introduces Logits-induced token uncertainty (LogTokU) to enhance uncertainty estimation in Large Language Models (LLMs), addressing the limitations of probability-based methods in token reliability assessment.", "motivation": "The rapid development of LLMs has led to their widespread application, but issues such as hallucinations pose reliability challenges. Understanding token reliability is crucial for improving LLM responses.", "method": "LogTokU framework estimates decoupled token uncertainty in real-time by utilizing evidence modeling, avoiding the need for multiple sampling processes.", "result": "Experimental results indicate that LogTokU significantly improves the effectiveness of uncertainty estimation in LLMs, thereby enhancing their reliability.", "conclusion": "LogTokU provides a promising approach for real-time uncertainty estimation that can be leveraged in downstream applications of LLMs.", "key_contributions": ["Introduction of Logits-induced token uncertainty (LogTokU) framework", "Real-time uncertainty estimation without multiple sampling", "Employing evidence modeling for token reliability assessment"], "limitations": "", "keywords": ["Large Language Models", "uncertainty estimation", "halluinations", "token reliability", "evidence modeling"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.01496", "pdf": "https://arxiv.org/pdf/2503.01496.pdf", "abs": "https://arxiv.org/abs/2503.01496", "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures", "authors": ["Disen Lan", "Weigao Sun", "Jiaxi Hu", "Jusen Du", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025, 15 pages", "summary": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.", "AI": {"tldr": "Liger is a novel approach for converting pretrained large language models (LLMs) into gated linear recurrent models, enhancing efficiency without additional parameters and achieving competitive performance.", "motivation": "The paper addresses the challenges of efficiently pretraining non-standard linear recurrent architectures from scratch, while improving deployment efficacy for pretrained LLMs.", "method": "Liger repurposes pretrained key matrix weights to form diverse gating mechanisms, utilizing lightweight fine-tuning through Low-Rank Adaptation (LoRA) to optimize performance without training new components from scratch.", "result": "Liger successfully matches the performance of linearized gated recurrent models to that of the original LLMs, achieving competitive results across various benchmarks, validated on models ranging from 1B to 8B parameters.", "conclusion": "Liger effectively enables the efficient deployment of linear recurrent models while maintaining high performance, showcasing the potential for hybrid attention mechanisms in LLMs.", "key_contributions": ["Introduction of Liger for linearizing LLMs to gated recurrent structures", "Repurposing key matrix weights for gating mechanisms", "Liger Attention as a hybrid attention mechanism that enhances performance"], "limitations": "", "keywords": ["linear recurrent modeling", "large language models", "Low-Rank Adaptation", "gated recurrent models", "hybrid attention"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.03186", "pdf": "https://arxiv.org/pdf/2503.03186.pdf", "abs": "https://arxiv.org/abs/2503.03186", "title": "Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation", "authors": ["Ben Hutchinson", "Celeste Rodríguez Louro", "Glenys Collard", "Ned Cooper"], "categories": ["cs.CL"], "comment": null, "summary": "In Australia, post-contact language varieties, including creoles and local\nvarieties of international languages, emerged as a result of forced contact\nbetween Indigenous communities and English speakers. These contact varieties\nare widely used, yet are poorly supported by language technologies. This gap\npresents barriers to participation in civil and economic society for Indigenous\ncommunities using these varieties, and reproduces minoritisation of\ncontemporary Indigenous sociolinguistic identities. This paper concerns three\nquestions regarding this context. First, can speech technologies support\nspeakers of Australian Aboriginal English, a local indigenised variety of\nEnglish? Second, what risks are inherent in such a project? Third, what\ntechnology development practices are appropriate for this context, and how can\nresearchers integrate meaningful community participation in order to mitigate\nrisks? We argue that opportunities do exist -- as well as risks -- and\ndemonstrate this through a case study exploring design practices in a\nreal-world project aiming to improve speech technologies for Australian\nAboriginal English. We discuss how we integrated culturally appropriate and\nparticipatory processes throughout the project. We call for increased support\nfor languages used by Indigenous communities, including contact varieties,\nwhich provide practical economic and socio-cultural benefits, provided that\nparticipatory and culturally safe practices are enacted.", "AI": {"tldr": "This paper explores the support of speech technologies for Australian Aboriginal English, highlighting opportunities and risks in development practices that include community participation.", "motivation": "To address the lack of support for post-contact language varieties used by Indigenous communities, which hinders their participation in society.", "method": "The paper discusses a case study focused on design practices in a project aimed at improving speech technologies for Australian Aboriginal English, incorporating participatory and culturally appropriate processes.", "result": "The case study demonstrates that integrating community involvement leads to better development practices, while also highlighting inherent risks.", "conclusion": "There are opportunities for technological support of Indigenous languages, but they must be approached with care to ensure inclusivity and safety for the communities involved.", "key_contributions": ["Identification of barriers faced by Indigenous communities using contact varieties", "Case study demonstrating successful participatory design practices", "Call for technological support inclusive of Indigenous languages and cultures"], "limitations": "Further research is needed to explore long-term sustainable support mechanisms for these languages.", "keywords": ["Australian Aboriginal English", "speech technologies", "participatory design", "Indigenous communities", "language technology"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.11280", "pdf": "https://arxiv.org/pdf/2503.11280.pdf", "abs": "https://arxiv.org/abs/2503.11280", "title": "High-Dimensional Interlingual Representations of Large Language Models", "authors": ["Bryan Wilie", "Samuel Cahyawijaya", "Junxian He", "Pascale Fung"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.", "AI": {"tldr": "This study investigates the interlingual representations in multilingual large language models (LLMs) and introduces a framework and metric to evaluate their alignments across diverse languages.", "motivation": "The nature of interlingual constructs in multilingual LLMs is unclear, with mixed evidence regarding the existence of unified representations.", "method": "The paper examines 31 languages with varying resources and typologies, introducing the Interlingual Local Overlap (ILO) score to measure interlingual alignment and researching the effects of single-language fine-tuning.", "result": "Findings show inconsistent cross-lingual alignments in multilingual LLMs, and that single-language fine-tuning disrupts interlingual alignment in early layers; freezing these layers helps maintain alignment and improves generalization.", "conclusion": "The framework and ILO score provide valuable insights into interlingual representation, emphasizing the importance of alignment for effective multilingual learning.", "key_contributions": ["Introduced the Interlingual Local Overlap (ILO) score for measuring interlingual alignment.", "Developed a framework for understanding shared and fragmented interlingual representations in multilingual LLMs.", "Demonstrated the impact of single-language fine-tuning on cross-lingual generalization."], "limitations": "The study focuses only on a limited set of languages, and further research is needed to explore additional linguistic contexts.", "keywords": ["interlingual representations", "multilingual LLMs", "cross-lingual alignment", "fine-tuning", "semantic subspace"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.21813", "pdf": "https://arxiv.org/pdf/2503.21813.pdf", "abs": "https://arxiv.org/abs/2503.21813", "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": ["Zhangcheng Qiang"], "categories": ["cs.CL", "cs.IR"], "comment": "15 pages, 4 figures, 5 tables, 2 prompt templates", "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.\nschema-matching) datasets in the Ontology Alignment Evaluation Initiative\n(OAEI), capturing hallucinations of different LLMs performing OM tasks. These\nOM-specific hallucinations are carefully classified into two primary categories\nand six sub-categories. We showcase the usefulness of the dataset in\nconstructing the LLM leaderboard and fine-tuning foundational LLMs for\nLLM-based OM systems.", "AI": {"tldr": "The paper introduces OAEI-LLM-T, a benchmark dataset aimed at addressing hallucinations in large language models for ontology matching tasks.", "motivation": "To address the persistent issue of hallucinations that occur in tasks using LLMs, particularly in the context of ontology matching systems.", "method": "The authors create the OAEI-LLM-T dataset based on existing schema-matching datasets, classifying hallucinations into two main categories and six sub-categories relevant to ontology matching.", "result": "The dataset demonstrates its usefulness by enabling the construction of an LLM leaderboard and allowing for the fine-tuning of foundational LLMs specifically for ontology matching applications.", "conclusion": "OAEI-LLM-T provides a valuable resource for improving LLM performance in ontology matching tasks by systematically addressing hallucinations.", "key_contributions": ["Introduction of the OAEI-LLM-T benchmark dataset for ontology matching", "Classification of hallucinations into specific categories", "Facilitation of LLM leaderboard and fine-tuning for better performance in OM tasks"], "limitations": "", "keywords": ["large language models", "ontology matching", "dataset", "hallucinations", "benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.10246", "pdf": "https://arxiv.org/pdf/2411.10246.pdf", "abs": "https://arxiv.org/abs/2411.10246", "title": "Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT", "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi", "Lei Liu", "Michael Flor"], "categories": ["cs.HC", "cs.CL"], "comment": "21 pages, 3 figures, 5 tables. Initially report in the edArXiv:xw6kz", "summary": "Collaborative problem solving (CPS) is widely recognized as a critical\n21st-century skill. Assessing CPS depends heavily on coding the communication\ndata using a construct-relevant framework, and this process has long been a\nmajor bottleneck to scaling up such assessments. Based on five datasets and two\ncoding frameworks, we demonstrate that ChatGPT can code communication data to a\nsatisfactory level, though performance varies across ChatGPT models, and\ndepends on the coding framework and task characteristics. Interestingly, newer\nreasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily\nyield better coding results. Additionally, we show that refining prompts based\non feedback from miscoded cases can improve coding accuracy in some instances,\nthough the effectiveness of this approach is not consistent across all tasks.\nThese findings offer practical guidance for researchers and practitioners in\ndeveloping scalable, efficient methods to analyze communication data in support\nof 21st-century skill assessment.", "AI": {"tldr": "This paper explores the use of ChatGPT for coding communication data in collaborative problem solving assessment, demonstrating that its performance varies by model and coding framework.", "motivation": "The need for scalable assessments of collaborative problem solving (CPS) skills has led to exploring AI's role in coding communication data effectively.", "method": "The study utilizes five datasets and two coding frameworks to evaluate the performance of various ChatGPT models in coding communication data for CPS assessment.", "result": "ChatGPT can code communication data at a satisfactory level, but its performance varies based on the specific model and task characteristics.", "conclusion": "Refining prompts based on feedback can enhance coding accuracy, although effectiveness varies across tasks, providing a pathway for improving communication data analysis.", "key_contributions": ["Demonstrated that ChatGPT can effectively code communication data for CPS assessment.", "Identified variations in performance across different ChatGPT models and coding frameworks.", "Showed that prompt refinement can improve coding accuracy in some cases."], "limitations": "Performance of coding varies by model, coding framework, and task; some approaches for improvement are inconsistent.", "keywords": ["Collaborative problem solving", "ChatGPT", "Communication data coding", "AI in assessment", "Skill assessment"], "importance_score": 6, "read_time_minutes": 15}}
