{"id": "2504.20049", "pdf": "https://arxiv.org/pdf/2504.20049.pdf", "abs": "https://arxiv.org/abs/2504.20049", "title": "It's the same but not the same: Do LLMs distinguish Spanish varieties?", "authors": ["Marina Mayor-Rocher", "Cristina Pozo", "Nina Melero", "Gonzalo Martínez", "María Grandury", "Pedro Reviriego"], "categories": ["cs.CL"], "comment": "in Spanish language", "summary": "In recent years, large language models (LLMs) have demonstrated a high\ncapacity for understanding and generating text in Spanish. However, with five\nhundred million native speakers, Spanish is not a homogeneous language but\nrather one rich in diatopic variations spanning both sides of the Atlantic. For\nthis reason, in this study, we evaluate the ability of nine language models to\nidentify and distinguish the morphosyntactic and lexical peculiarities of seven\nvarieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,\nPeninsular, Mexican and Central American and Rioplatense) through a\nmultiple-choice test. The results indicate that the Peninsular Spanish variety\nis the best identified by all models and that, among them, GPT-4o is the only\nmodel capable of recognizing the variability of the Spanish language.\n  --\n  En los \\'ultimos a\\~nos, los grandes modelos de lenguaje (LLMs, por sus\nsiglas en ingl\\'es) han demostrado una alta capacidad para comprender y generar\ntexto en espa\\~nol. Sin embargo, con quinientos millones de hablantes nativos,\nla espa\\~nola no es una lengua homog\\'enea, sino rica en variedades\ndiat\\'opicas que se extienden a ambos lados del Atl\\'antico. Por todo ello,\nevaluamos en este trabajo la capacidad de nueve modelos de lenguaje de\nidentificar y discernir las peculiaridades morfosint\\'acticas y l\\'exicas de\nsiete variedades de espa\\~nol (andino, antillano, caribe\\~no continental,\nchileno, espa\\~nol peninsular, mexicano y centroamericano y rioplatense)\nmediante un test de respuesta m\\'ultiple. Los resultados obtenidos indican que\nla variedad de espa\\~nol peninsular es la mejor identificada por todos los\nmodelos y que, de entre todos, GPT-4o es el \\'unico modelo capaz de identificar\nla variabilidad de la lengua espa\\~nola.", "AI": {"tldr": "This study evaluates nine language models' ability to distinguish morphosyntactic and lexical peculiarities of seven Spanish varieties, finding GPT-4o the most capable.", "motivation": "To assess the capacity of language models in recognizing the diversity of the Spanish language due to its regional variations.", "method": "A multiple-choice test was conducted to evaluate nine language models on their ability to identify different Spanish varieties.", "result": "The models showed varying levels of performance, with Peninsular Spanish being the most accurately identified and GPT-4o recognizing the diversity best.", "conclusion": "The study highlights the importance of recognizing the regional variations in language processing, particularly for Spanish.", "key_contributions": ["Evaluation of nine LLMs on Spanish language varieties", "Identification of Peninsular Spanish as the most recognizable variety", "Demonstration of GPT-4o's capability in recognizing language variability"], "limitations": "The study is limited to only seven varieties of Spanish and nine models, which may not represent all regional differences.", "future_work": "Further research could expand the evaluation to include more Spanish varieties and additional language models.", "keywords": ["large language models", "Spanish varieties", "morphosyntax", "lexical peculiarities", "GPT-4o"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2504.20051", "pdf": "https://arxiv.org/pdf/2504.20051.pdf", "abs": "https://arxiv.org/abs/2504.20051", "title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts", "authors": ["Frances Laureano De Leon", "Harish Tayyar Madabushi", "Mark G. Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Multiword expressions, characterised by non-compositional meanings and\nsyntactic irregularities, are an example of nuanced language. These expressions\ncan be used literally or idiomatically, leading to significant changes in\nmeaning. While large language models have demonstrated strong performance\nacross many tasks, their ability to handle such linguistic subtleties remains\nuncertain. Therefore, this study evaluates how state-of-the-art language models\nprocess the ambiguity of potentially idiomatic multiword expressions,\nparticularly in contexts that are less frequent, where models are less likely\nto rely on memorisation. By evaluating models across in Portuguese and\nGalician, in addition to English, and using a novel code-switched dataset and a\nnovel task, we find that large language models, despite their strengths,\nstruggle with nuanced language. In particular, we find that the latest models,\nincluding GPT-4, fail to outperform the xlm-roBERTa-base baselines in both\ndetection and semantic tasks, with especially poor performance on the novel\ntasks we introduce, despite its similarity to existing tasks. Overall, our\nresults demonstrate that multiword expressions, especially those which are\nambiguous, continue to be a challenge to models.", "AI": {"tldr": "This paper evaluates how large language models handle the ambiguity of multiword expressions in different languages, finding significant challenges in processing nuanced language.", "motivation": "To investigate the ability of large language models in processing ambiguous multiword expressions across different languages and contexts.", "method": "The authors evaluated various language models, including GPT-4, on a novel code-switched dataset involving Portuguese, Galician, and English in detecting and understanding multiword expressions.", "result": "The study found that large language models, including GPT-4, struggled with nuanced language involving multiword expressions, failing to outperform xlm-roBERTa-base baselines in detection and semantic tasks.", "conclusion": "Despite their advanced capabilities, large language models continue to face challenges in interpreting ambiguous multiword expressions, indicating a need for further improvements.", "key_contributions": ["Introduction of a novel code-switched dataset for evaluating multiword expressions.", "Demonstration of state-of-the-art language models' limitations in handling nuanced language.", "Comparison of performance across English, Portuguese, and Galician."], "limitations": "The study may not cover all dimensions of linguistic nuance and only evaluates specific models in limited contexts.", "future_work": "Future research could explore additional languages and a broader range of linguistic features affecting model performance.", "keywords": ["multiword expressions", "large language models", "ambiguity", "code-switching", "language tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.20086", "pdf": "https://arxiv.org/pdf/2504.20086.pdf", "abs": "https://arxiv.org/abs/2504.20086", "title": "Understanding and Mitigating Risks of Generative AI in Financial Services", "authors": ["Sebastian Gehrmann", "Claire Huang", "Xian Teng", "Sergei Yurovski", "Iyanuoluwa Shode", "Chirag S. Patel", "Arjun Bhorkar", "Naveen Thomas", "John Doucette", "David Rosenberg", "Mark Dredze", "David Rabinowitz"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "To responsibly develop Generative AI (GenAI) products, it is critical to\ndefine the scope of acceptable inputs and outputs. What constitutes a \"safe\"\nresponse is an actively debated question. Academic work puts an outsized focus\non evaluating models by themselves for general purpose aspects such as\ntoxicity, bias, and fairness, especially in conversational applications being\nused by a broad audience. In contrast, less focus is put on considering\nsociotechnical systems in specialized domains. Yet, those specialized systems\ncan be subject to extensive and well-understood legal and regulatory scrutiny.\nThese product-specific considerations need to be set in industry-specific laws,\nregulations, and corporate governance requirements. In this paper, we aim to\nhighlight AI content safety considerations specific to the financial services\ndomain and outline an associated AI content risk taxonomy. We compare this\ntaxonomy to existing work in this space and discuss implications of risk\ncategory violations on various stakeholders. We evaluate how existing\nopen-source technical guardrail solutions cover this taxonomy by assessing them\non data collected via red-teaming activities. Our results demonstrate that\nthese guardrails fail to detect most of the content risks we discuss.", "AI": {"tldr": "The paper discusses AI content safety in financial services, proposing a risk taxonomy and evaluating existing guardrails.", "motivation": "To responsibly develop Generative AI products, it is essential to define acceptable inputs and outputs, particularly focusing on specialized domains like financial services.", "method": "The authors outline an AI content risk taxonomy specific to the financial services domain and compare it to existing work. They also assess open-source technical guardrail solutions through red-teaming activities.", "result": "The evaluation shows that existing guardrails fail to detect most content risks identified in the proposed taxonomy.", "conclusion": "The findings highlight the gaps in current guardrail solutions and suggest urgent attention to product-specific AI content safety.", "key_contributions": ["Introduction of a novel AI content risk taxonomy for financial services", "Comparison of the risk taxonomy with existing work", "Evaluation of open-source guardrail solutions' effectiveness"], "limitations": "The study focuses specifically on the financial services domain and may not generalize to other areas.", "future_work": "Future research should explore tailored AI content safety mechanisms across different industries and improve current guardrail solutions.", "keywords": ["Generative AI", "Content safety", "Financial services", "AI risk taxonomy", "Red-teaming"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157.pdf", "abs": "https://arxiv.org/abs/2504.20157", "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Dongyeop Kang"], "categories": ["cs.CL"], "comment": null, "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.", "AI": {"tldr": "Meta Policy Optimization (MPO) is proposed as a solution to the limitations of reward-based alignment methods for large language models (LLMs), particularly focusing on the issues of reward hacking and the necessity of manual prompt engineering.", "motivation": "Reward-based alignment methods for LLMs face significant challenges including reward hacking and intensive prompt engineering, necessitating a more robust solution.", "method": "MPO integrates a meta-reward model that dynamically refines the reward model's prompt during training, adapting to the evolving context to provide a stable and effective reward signal.", "result": "MPO demonstrates performance comparable to or better than traditional methods relying on labor-intensive prompt engineering across various tasks without requiring specialized reward designs.", "conclusion": "MPO addresses both theoretical and practical challenges in reward-based RL alignment for LLMs, suggesting a pathway for more robust alignment strategies with the potential for broader application.", "key_contributions": ["Introduction of a meta-reward model that refines prompts dynamically during training", "Reduction of manual labor in reward prompt design", "Demonstration of effectiveness across diverse tasks without specialized reward designs"], "limitations": "", "future_work": "Further exploration of MPO's applicability in higher-level alignment frameworks and its integration into existing alignment strategies.", "keywords": ["Meta Policy Optimization", "large language models", "reward-based alignment", "meta-learning", "policy optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20308", "pdf": "https://arxiv.org/pdf/2504.20308.pdf", "abs": "https://arxiv.org/abs/2504.20308", "title": "Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South", "authors": ["Ozioma C. Oguine", "Oghenemaro Anuyah", "Zainab Agha", "Iris Melgarez", "Adriana Alvarado Garcia", "Karla Badillo-Urquiola"], "categories": ["cs.HC", "cs.CY"], "comment": "30 pages, 1 figure", "summary": "Youth online safety research in HCI has historically centered on perspectives\nfrom the Global North, often overlooking the unique particularities and\ncultural contexts of regions in the Global South. This paper presents a\nsystematic review of 66 youth online safety studies published between 2014 and\n2024, specifically focusing on regions in the Global South. Our findings reveal\na concentrated research focus in Asian countries and predominance of\nquantitative methods. We also found limited research on marginalized youth\npopulations and a primary focus on risks related to cyberbullying. Our analysis\nunderscores the critical role of cultural factors in shaping online safety,\nhighlighting the need for educational approaches that integrate social dynamics\nand awareness. We propose methodological recommendations and a future research\nagenda that encourages the adoption of situated, culturally sensitive\nmethodologies and youth-centered approaches to researching youth online safety\nregions in the Global South. This paper advocates for greater inclusivity in\nyouth online safety research, emphasizing the importance of addressing varied\nsociocultural contexts to better understand and meet the online safety needs of\nyouth in the Global South.", "AI": {"tldr": "This paper reviews youth online safety research in the Global South, highlighting the predominance of quantitative methods and the need for culturally sensitive approaches.", "motivation": "To address the lack of research on youth online safety in the Global South and emphasize the importance of cultural contexts.", "method": "Systematic review of 66 studies published between 2014 and 2024, focusing on regions in the Global South.", "result": "Findings indicate a focus on Asian countries, with most studies employing quantitative methods and emphasizing risks of cyberbullying while neglecting marginalized youth.", "conclusion": "The paper calls for culturally sensitive methodologies and youth-centered approaches to improve online safety research in the Global South.", "key_contributions": ["Systematic review of youth online safety in the Global South", "Highlighting the need for culturally sensitive research methodologies", "Proposals for future research agenda focusing on diverse sociocultural contexts"], "limitations": "Limited exploration of marginalized youth populations and reliance on quantitative methods.", "future_work": "Encouragement of situated, culturally aware research methodologies and inclusivity in youth online safety studies.", "keywords": ["youth online safety", "Global South", "cultural contexts", "cyberbullying", "inclusive research"], "importance_score": 4, "read_time_minutes": 30}}
{"id": "2504.20168", "pdf": "https://arxiv.org/pdf/2504.20168.pdf", "abs": "https://arxiv.org/abs/2504.20168", "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025. Code:\n  https://github.com/microsoft/mice_for_cats", "summary": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats.", "AI": {"tldr": "Introducing model-internal confidence estimators (MICE) to improve the calibration of model confidences for tool-using agents.", "motivation": "To enhance safety and utility of tool-using AI agents by improving the calibration of model confidences.", "method": "MICE decodes from intermediate layers of language models and computes similarity scores to assess confidence via a learned probabilistic classifier.", "result": "MICE outperforms or matches baselines on expected calibration error and significantly improves tool-calling utility for varying risk levels.", "conclusion": "MICE shows promise in being sample-efficient and generalizing to unseen APIs, making it a valuable tool for enhancing AI safety.", "key_contributions": ["Introduction of model-internal confidence estimators (MICE)", "Improvement in expected calibration error", "Enhanced tool-calling utility in risk-variable scenarios"], "limitations": "None specified.", "future_work": "Potential exploration of further integration with other model architectures and real-world applications of MICE.", "keywords": ["confidence estimation", "tool usage", "model calibration", "probabilistic classifier", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20320", "pdf": "https://arxiv.org/pdf/2504.20320.pdf", "abs": "https://arxiv.org/abs/2504.20320", "title": "\"I've talked to ChatGPT about my issues last night.\": Examining Mental Health Conversations with Large Language Models through Reddit Analysis", "authors": ["Kyuha Jung", "Gyuho Lee", "Yuanhui Huang", "Yunan Chen"], "categories": ["cs.HC"], "comment": "Forthcoming at the 28th ACM SIGCHI Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW '25) on October 18-22, 2025", "summary": "We investigate the role of large language models (LLMs) in supporting mental\nhealth by analyzing Reddit posts and comments about mental health conversations\nwith ChatGPT. Our findings reveal that users value ChatGPT as a safe,\nnon-judgmental space, often favoring it over human support due to its\naccessibility, availability, and knowledgeable responses. ChatGPT provides a\nrange of support, including actionable advice, emotional support, and\nvalidation, while helping users better understand their mental states.\nAdditionally, we found that ChatGPT offers innovative support for individuals\nfacing mental health challenges, such as assistance in navigating difficult\nconversations, preparing for therapy sessions, and exploring therapeutic\ninterventions. However, users also voiced potential risks, including the spread\nof incorrect health advice, ChatGPT's overly validating nature, and privacy\nconcerns. We discuss the implications of LLMs as tools for mental health\nsupport in both everyday health and clinical therapy settings and suggest\nstrategies to mitigate risks in LLM-powered interactions.", "AI": {"tldr": "The paper examines how large language models, particularly ChatGPT, support mental health through Reddit discussions, highlighting their advantages and potential risks.", "motivation": "To understand the function of LLMs in facilitating mental health support and the user perception surrounding this technology.", "method": "Analysis of Reddit posts and comments regarding mental health interactions with ChatGPT.", "result": "Users appreciate ChatGPT for its accessibility and for providing emotional support, advice, and validation, though concerns about misinformation and privacy arose.", "conclusion": "While LLMs can enhance mental health support, precautions are necessary to address the associated risks, particularly in clinical settings.", "key_contributions": ["Insights into user preferences for LLMs in mental health over human support", "Identification of innovative uses for LLMs in therapy preparation", "Discussion on mitigating risks associated with LLM use in mental health contexts"], "limitations": "The study is limited to Reddit conversations and may not reflect broader user experiences with LLMs for mental health.", "future_work": "Further research on LLMs in diverse mental health settings and strategies for improving accuracy and privacy.", "keywords": ["large language models", "mental health", "ChatGPT", "user experience", "health informatics"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2504.20220", "pdf": "https://arxiv.org/pdf/2504.20220.pdf", "abs": "https://arxiv.org/abs/2504.20220", "title": "A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports", "authors": ["Henning Schäfer", "Cynthia S. Schmidt", "Johannes Wutzkowsky", "Kamil Lorek", "Lea Reinartz", "Johannes Rückert", "Christian Temme", "Britta Böckmann", "Peter A. Horn", "Christoph M. Friedrich"], "categories": ["cs.CL", "cs.CV", "68T07", "I.7.5; I.4.7; I.2.7; H.3.3; J.3"], "comment": null, "summary": "Despite the growing adoption of electronic health records, many processes\nstill rely on paper documents, reflecting the heterogeneous real-world\nconditions in which healthcare is delivered. The manual transcription process\nis time-consuming and prone to errors when transferring paper-based data to\ndigital formats. To streamline this workflow, this study presents an\nopen-source pipeline that extracts and categorizes checkbox data from scanned\ndocuments. Demonstrated on transfusion reaction reports, the design supports\nadaptation to other checkbox-rich document types. The proposed method\nintegrates checkbox detection, multilingual optical character recognition (OCR)\nand multilingual vision-language models (VLMs). The pipeline achieves high\nprecision and recall compared against annually compiled gold-standards from\n2017 to 2024. The result is a reduction in administrative workload and accurate\nregulatory reporting. The open-source availability of this pipeline encourages\nself-hosted parsing of checkbox forms.", "AI": {"tldr": "This study introduces an open-source pipeline for extracting and categorizing checkbox data from scanned healthcare documents, aiming to improve data handling efficiencies.", "motivation": "To address the inefficiencies and errors in manual transcription processes related to healthcare data transfer from paper to digital formats.", "method": "The pipeline incorporates checkbox detection, multilingual OCR, and multilingual vision-language models, applied specifically to transfusion reaction reports while adaptable to other document types.", "result": "The proposed pipeline exhibits high precision and recall metrics and successfully reduces administrative workload along with ensuring accurate regulatory reporting.", "conclusion": "The open-source nature of the pipeline allows for broader application and self-hosting opportunities in parsing checkbox forms across various healthcare scenarios.", "key_contributions": ["Development of an open-source pipeline for checkbox data extraction.", "Integration of multilingual OCR and VLMs for improved accuracy.", "Demonstrated effectiveness on transfusion reaction reports with adaptable methodologies."], "limitations": "The study is limited to checkbox-rich document types and requires evaluation on a wider range of healthcare documents for robustness.", "future_work": "Future research may focus on extending the pipeline capabilities to other types of healthcare documents beyond checkbox forms and enhancing multi-lingual support.", "keywords": ["Electronic Health Records", "Checkbox Detection", "Multilingual OCR", "Data Extraction", "Vision-Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20342", "pdf": "https://arxiv.org/pdf/2504.20342.pdf", "abs": "https://arxiv.org/abs/2504.20342", "title": "Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI", "authors": ["Shou-Tzu Han"], "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2; H.1.2"], "comment": "10 pages, 5 figures, preliminary results, early-stage work intended\n  for future conference submission", "summary": "Reflexion is an AI-powered platform designed to enable structured emotional\nself-reflection at scale. By integrating real-time emotion detection, layered\nreflective prompting, and metaphorical storytelling generation, Reflexion\nempowers users to engage in autonomous emotional exploration beyond basic\nsentiment categorization. Grounded in theories of expressive writing, cognitive\nrestructuring, self-determination, and critical consciousness development, the\nsystem scaffolds a progressive journey from surface-level emotional recognition\ntoward value-aligned action planning. Initial pilot studies with diverse\nparticipants demonstrate positive outcomes in emotional articulation, cognitive\nreframing, and perceived psychological resilience. Reflexion represents a\npromising direction for scalable, theory-informed affective computing\ninterventions aimed at fostering emotional literacy and psychological growth\nacross educational, therapeutic, and public health contexts.", "AI": {"tldr": "Reflexion is an AI platform that facilitates structured emotional self-reflection using real-time emotion detection and metaphorical storytelling, aiming to enhance emotional literacy and psychological resilience.", "motivation": "The paper introduces Reflexion as a tool to support emotional self-reflection and exploration, moving beyond basic emotional recognition.", "method": "Reflexion integrates real-time emotion detection, layered reflective prompts, and storytelling generation, based on various psychological theories.", "result": "Initial pilot studies indicate improved emotional articulation, cognitive reframing, and perceived psychological resilience among participants.", "conclusion": "Reflexion shows potential as an effective intervention for emotional literacy and psychological growth in various settings.", "key_contributions": ["Introduction of a novel AI-powered tool for emotional self-reflection.", "Integration of various psychological theories into the platform's design.", "Preliminary evidence demonstrating positive emotional and cognitive outcomes."], "limitations": "The study presents preliminary results and is considered early-stage work intended for future refinement and conference submission.", "future_work": "Further development and testing of Reflexion to enhance its efficacy and applicability in diverse contexts.", "keywords": ["AI", "emotional self-reflection", "affective computing", "psychological resilience", "cognitive reframing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.20251", "pdf": "https://arxiv.org/pdf/2504.20251.pdf", "abs": "https://arxiv.org/abs/2504.20251", "title": "A Platform for Generating Educational Activities to Teach English as a Second Language", "authors": ["Aiala Rosá", "Santiago Góngora", "Juan Pablo Filevich", "Ignacio Sastre", "Laura Musto", "Brian Carpenter", "Luis Chiruzzo"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Unpublished report written in 2023", "summary": "We present a platform for the generation of educational activities oriented\nto teaching English as a foreign language. The different activities -- games\nand language practice exercises -- are strongly based on Natural Language\nProcessing techniques. The platform offers the possibility of playing\nout-of-the-box games, generated from resources created semi-automatically and\nthen manually curated. It can also generate games or exercises of greater\ncomplexity from texts entered by teachers, providing a stage of review and\nedition of the generated content before use. As a way of expanding the variety\nof activities in the platform, we are currently experimenting with image and\ntext generation. In order to integrate them and improve the performance of\nother neural tools already integrated, we are working on migrating the platform\nto a more powerful server. In this paper we describe the development of our\nplatform and its deployment for end users, discussing the challenges faced and\nhow we overcame them, and also detail our future work plans.", "AI": {"tldr": "A platform for generating English language educational activities using NLP techniques, offering both pre-made and customizable games and exercises.", "motivation": "The need for interactive and adaptable educational tools for teaching English as a foreign language to enhance learning engagement.", "method": "The platform utilizes Natural Language Processing for the creation of language exercises and games, allowing for semi-automated content generation and manual curation.", "result": "The platform allows teachers to generate a variety of educational activities, facilitates the integration of text and image generation, and has been successfully deployed for user engagement.", "conclusion": "The paper discusses the platform's development, the deployment challenges faced, and outlines future work to enhance performance and expand functionality.", "key_contributions": ["Development of an NLP-based platform for educational activities", "Integration of customizable game generation for language learning", "Plans to incorporate image and text generation improvements"], "limitations": "", "future_work": "Migrate the platform to a more powerful server and further enhance game complexity and integration of additional media types.", "keywords": ["Natural Language Processing", "Education Technology", "Game-Based Learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.20365", "pdf": "https://arxiv.org/pdf/2504.20365.pdf", "abs": "https://arxiv.org/abs/2504.20365", "title": "Thoughtful, Confused, or Untrustworthy: How Text Presentation Influences Perceptions of AI Writing Tools", "authors": ["David Zhou", "John R. Gallagher", "Sarah Sterman"], "categories": ["cs.HC"], "comment": "17 pages, 3 figures, ACM Creativity and Cognition 2025", "summary": "AI writing tools have been shown to dramatically change the way people write,\nyet the effects of AI text presentation are not well understood nor always\nintentionally designed. Although text presentation in existing large language\nmodel interfaces is linked to the speed of the underlying model, text\npresentation speed can impact perceptions of AI systems, potentially\ninfluencing whether AI suggestions are accepted or rejected. In this paper, we\nanalyze the effects of varying text generation speed in creative and\nprofessional writing scenarios on an online platform (n=297). We find that\nspeed is correlated with perceived humanness and trustworthiness of the AI\ntool, as well as the perceived quality of the generated text. We discuss its\nimplications on creative and writing processes, along with future steps in the\nintentional design of AI writing tool interfaces.", "AI": {"tldr": "This paper explores how varying text generation speed in AI writing tools affects users' perceptions of these tools and the quality of generated text.", "motivation": "To investigate the effects of AI text presentation speed on users' perceptions and decision-making in AI-assisted writing.", "method": "An online study with 297 participants analyzed the impact of different text generation speeds in creative and professional writing contexts.", "result": "The study found that faster text generation speeds enhance perceived humanness and trustworthiness of AI tools, as well as the quality of the generated content.", "conclusion": "Understanding the influence of text presentation speed can lead to better-designed AI writing interfaces that positively affect user interactions and acceptance.", "key_contributions": ["Demonstrated the correlation between text generation speed and user perceptions", "Provided insights into the intentional design of AI writing tools", "Identified implications for creative and writing processes involving AI"], "limitations": "The study is limited to specific writing scenarios and may not generalize to all types of AI tools or writing contexts.", "future_work": "Future research could explore text presentation in different AI applications and its effects on various user demographics and writing styles.", "keywords": ["AI writing tools", "text generation speed", "user perceptions", "AI trustworthiness", "interface design"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2504.20276", "pdf": "https://arxiv.org/pdf/2504.20276.pdf", "abs": "https://arxiv.org/abs/2504.20276", "title": "Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi", "authors": ["Dandan Chen Kaptur", "Yue Huang", "Xuejun Ryan Ji", "Yanhui Guo", "Bradley Kaptur"], "categories": ["cs.CL", "stat.AP"], "comment": "13 pages, Paper presented at the National Council on Measurement in\n  Education (NCME) Conference, Denver, Colorado, in April 2025", "summary": "This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),\nfor systematic reviews. We evaluated their performance by comparing\nLLM-generated codes with human-generated codes from a peer-reviewed systematic\nreview on assessment. Our findings suggested that the performance of LLMs\nfluctuates by data volume and question complexity for systematic reviews.", "AI": {"tldr": "This research focuses on evaluating the performance of GPT-4 and Kimi in generating systematic review codes compared to human-generated codes.", "motivation": "To assess the competency of Large Language Models (LLMs) like GPT-4 and Kimi in conducting systematic reviews.", "method": "Comparison of LLM-generated codes against human-generated codes from a peer-reviewed systematic review.", "result": "The performance of LLMs was found to vary based on the complexity of questions and the volume of data.", "conclusion": "LLMs show potential in systematic reviews but their effectiveness is contingent on specific factors such as question complexity.", "key_contributions": ["Evaluation of LLM capabilities in systematic reviews", "Comparison between LLM-generated and human-generated codes", "Insights into factors affecting LLM performance in systematic reviews"], "limitations": "Performance varies significantly based on data volume and question complexity, indicating limitations under specific conditions.", "future_work": "Further exploration of LLMs in different contexts and with varying data types to enhance systematic review processes.", "keywords": ["Large Language Models", "Systematic Reviews", "GPT-4", "Kimi", "Assessment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20369", "pdf": "https://arxiv.org/pdf/2504.20369.pdf", "abs": "https://arxiv.org/abs/2504.20369", "title": "Perception-aware Sampling for Scatterplot Visualizations", "authors": ["Zafeiria Moumoulidou", "Hamza Elhamdadi", "Ke Yang", "Subrata Mitra", "Cindy Xiong Bearfield", "Alexandra Meliou"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Visualizing data is often a crucial first step in data analytics workflows,\nbut growing data sizes pose challenges due to computational and visual\nperception limitations. As a result, data analysts commonly down-sample their\ndata and work with subsets. Deriving representative samples, however, remains a\nchallenge. This paper focuses on scatterplots, a widely-used visualization\ntype, and introduces a novel sampling objective -- perception-awareness --\naiming to improve sample efficacy by targeting humans' perception of a\nvisualization.\n  We make the following contributions: (1) We propose perception-augmented\ndatabases and design PAwS: a novel perception-aware sampling method for\nscatterplots that leverages saliency maps -- a computer vision tool for\npredicting areas of attention focus in visualizations -- and models\nperception-awareness via saliency, density, and coverage objectives. (2) We\ndesign ApproPAwS: a fast, perception-aware method for approximate\nvisualizations, which exploits the fact that small visual perturbations are\noften imperceptible to humans. (3) We introduce the concept of perceptual\nsimilarity as a metric for sample quality, and present a novel method that\ncompares saliency maps to measure it. (4) Our extensive experimental evaluation\nshows that our methods consistently outperform prior art in producing samples\nwith high perceptual similarity, while ApproPAwS achieves up to 100x speed-ups\nwith minimal loss in visual fidelity. Our user study shows that PAwS is often\npreferred by humans, validating our quantitative findings.", "AI": {"tldr": "This paper introduces perception-aware sampling methods for scatterplots to enhance data visualization in analytics workflows.", "motivation": "With increasing data sizes, effective visualization becomes challenging, necessitating improved sampling techniques to ensure meaningful data representation.", "method": "The paper proposes two methods: PAwS, a perception-aware sampling strategy utilizing saliency maps to focus on human attention, and ApproPAwS, an approximate visualization method achieving speed-ups while maintaining visual integrity.", "result": "Experimental results demonstrate that both methods achieve higher perceptual similarity compared to existing techniques, with ApproPAwS offering up to 100x speed improvements.", "conclusion": "The proposed perception-aware methods significantly enhance sample efficacy and are preferred in user studies, suggesting their practical application in data analytics.", "key_contributions": ["Introduction of perception-augmented databases", "Development of PAwS and ApproPAwS sampling methods", "Establishment of perceptual similarity as a sample quality metric"], "limitations": "The methods may have limitations in very high-dimensional data spaces where perception may vary more widely.", "future_work": "Further exploration of perception-aware sampling methods for other visualization types and larger data dimensions.", "keywords": ["Data Visualization", "Sampling Methods", "Perception-awareness"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304.pdf", "abs": "https://arxiv.org/abs/2504.20304", "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research.", "AI": {"tldr": "Introduction of UD-English-CHILDES, a new resource derived from CHILDES data providing unified annotation for child and child-directed speech.", "motivation": "To provide a consistent resource for computational and linguistic research by harmonizing previously dependency-annotated CHILDES data.", "method": "Development of a Universal Dependencies (UD) treebank with validated gold-standard annotations and an additional silver-standard dataset.", "result": "Creation of a treebank with over 48k sentences from 11 children and caregivers, alongside 1M silver-standard sentences.", "conclusion": "UD-English-CHILDES serves as a comprehensive resource for further research in child language acquisition and related fields.", "key_contributions": ["First officially released UD treebank from CHILDES data", "Validation of existing gold-standard annotations under UD v2 framework", "Provision of a large silver-standard dataset for various research applications"], "limitations": "", "future_work": "Encouragement for further enhancements and applications of the treebank in computational linguistics and child language research.", "keywords": ["UD-English-CHILDES", "dependency-annotated", "child-directed speech"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.20567", "pdf": "https://arxiv.org/pdf/2504.20567.pdf", "abs": "https://arxiv.org/abs/2504.20567", "title": "Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks", "authors": ["Tanmay Chakraborty", "Marion Koelle", "Jörg Schlötterer", "Nadine Schlicker", "Christian Wirth", "Christin Seifert"], "categories": ["cs.HC"], "comment": null, "summary": "Bayesian Optimisation (BO) is a family of methods for finding optimal\nparameters when the underlying function to be optimised is unknown. BO is used,\nfor example, for hyperparameter tuning in machine learning and as an expert\nsupport tool for tuning cyberphysical systems. For settings where humans are\ninvolved in the tuning task, methods have been developed to explain BO\n(Explainable Bayesian Optimization, XBO). However, there is little guidance on\nhow to present XBO results to humans so that they can tune the system\neffectively and efficiently. In this paper, we investigate how the XBO\nexplanation format affects users' task performance, task load, understanding\nand trust in XBO. We chose a task that is accessible to a wide range of users.\nSpecifically, we set up an egg cooking scenario with 6 parameters that\nparticipants had to adjust to achieve a perfect soft-boiled egg. We compared\nthree different explanation formats: a bar chart, a list of rules and a textual\nexplanation in a between-subjects online study with 213 participants. Our\nresults show that adding any type of explanation increases task success,\nreduces the number of trials needed to achieve success, and improves\ncomprehension and confidence. While explanations add more information for\nparticipants to process, we found no increase in user task load. We also found\nthat the aforementioned results were independent of the explanation format; all\nformats had a similar effect.This is an interesting finding for practical\napplications, as it suggests that explanations can be added to BO tuning tasks\nwithout the burden of designing or selecting specific explanation formats. In\nthe future, it would be interesting to investigate scenarios of prolonged use\nof the explanation formats and whether they have different effects on users'\nmental models of the underlying system.", "AI": {"tldr": "The paper investigates the impact of different explanation formats in Explainable Bayesian Optimization (XBO) on user performance and understanding in a parameter tuning task.", "motivation": "There is limited guidance on effectively presenting Explainable Bayesian Optimization (XBO) results to users involved in tuning tasks.", "method": "A between-subjects online study was conducted with 213 participants, where three explanation formats (bar chart, list of rules, textual explanation) were compared in a task involving tuning parameters for cooking a soft-boiled egg.", "result": "Adding explanations improved task success, reduced the number of trials needed for success, and enhanced comprehension and confidence, with no increase in task load; the effect was consistent across all explanation formats.", "conclusion": "Explanations can be incorporated into Bayesian Optimization tasks without the burden of selecting specific formats, potentially simplifying the implementation of such systems.", "key_contributions": ["Investigation of XBO explanation formats on user performance", "Demonstration of improved task success and user confidence with any explanation format", "Finding that explanation format does not affect task load among users"], "limitations": "The study did not explore prolonged use of explanation formats and their long-term effects on mental models.", "future_work": "Future research could investigate the effects of prolonged use of different explanation formats on users' mental models of the system.", "keywords": ["Explainable Bayesian Optimization", "user performance", "parameter tuning", "human-computer interaction", "explanation formats"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20323", "pdf": "https://arxiv.org/pdf/2504.20323.pdf", "abs": "https://arxiv.org/abs/2504.20323", "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation", "authors": ["Chao-Lin Liu", "Po-Hsien Wu", "Yi-Ting Yu"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "comment": "16 pages, 9 figures, 2 tables, the Nineteenth International Workshop\n  on Juris-Informatics (JURISIN 2025), associated with the Seventeenth JSAI\n  International Symposium on AI (JSAI-isAI 2025)", "summary": "This report addresses the challenge of limited labeled datasets for\ndeveloping legal recommender systems, particularly in specialized domains like\nlabor disputes. We propose a new approach leveraging the co-citation of legal\narticles within cases to establish similarity and enable algorithmic\nannotation. This method draws a parallel to the concept of case co-citation,\nutilizing cited precedents as indicators of shared legal issues. To evaluate\nthe labeled results, we employ a system that recommends similar cases based on\nplaintiffs' accusations, defendants' rebuttals, and points of disputes. The\nevaluation demonstrates that the recommender, with finetuned text embedding\nmodels and a reasonable BiLSTM module can recommend labor cases whose\nsimilarity was measured by the co-citation of the legal articles. This research\ncontributes to the development of automated annotation techniques for legal\ndocuments, particularly in areas with limited access to comprehensive legal\ndatabases.", "AI": {"tldr": "This report proposes a new approach to develop legal recommender systems using co-citation of legal articles within cases for similarity measurement and algorithmic annotation, especially for labor disputes.", "motivation": "The challenge of limited labeled datasets in specialized legal domains necessitates innovative approaches for developing recommender systems that can improve access to legal information and support decision-making.", "method": "The approach leverages the co-citation of legal articles to establish case similarity, employing finetuned text embedding models and a BiLSTM module to recommend relevant labor cases based on the similarities of plaintiffs' accusations, defendants' rebuttals, and points of dispute.", "result": "The evaluation of the system demonstrated that it can effectively recommend labor cases based on the co-citation of legal articles, facilitating automated annotation and enhancing the ability to navigate legal disputes.", "conclusion": "The research offers a significant advancement in automated annotation techniques for legal documents, particularly benefitting areas with limited access to legal databases.", "key_contributions": ["Introduction of a new algorithmic annotation approach using co-citation in legal articles", "Development of a recommender system for labor disputes", "Finetuning of text embedding models and integration of BiLSTM for improved case similarity recommendations."], "limitations": "The effectiveness may be constrained by the availability and quality of legal article citations in databases.", "future_work": "Exploration of expanding the approach to other areas of law and improving the models with larger datasets from comprehensive legal databases.", "keywords": ["legal recommender systems", "co-citation", "labor disputes", "text embedding", "BiLSTM"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2504.20741", "pdf": "https://arxiv.org/pdf/2504.20741.pdf", "abs": "https://arxiv.org/abs/2504.20741", "title": "In defence of post-hoc explanations in medical AI", "authors": ["Joshua Hatherley", "Lauritz Munch", "Jens Christian Bjerring"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Since the early days of the Explainable AI movement, post-hoc explanations\nhave been praised for their potential to improve user understanding, promote\ntrust, and reduce patient safety risks in black box medical AI systems.\nRecently, however, critics have argued that the benefits of post-hoc\nexplanations are greatly exaggerated since they merely approximate, rather than\nreplicate, the actual reasoning processes that black box systems take to arrive\nat their outputs. In this article, we aim to defend the value of post-hoc\nexplanations against this recent critique. We argue that even if post-hoc\nexplanations do not replicate the exact reasoning processes of black box\nsystems, they can still improve users' functional understanding of black box\nsystems, increase the accuracy of clinician-AI teams, and assist clinicians in\njustifying their AI-informed decisions. While post-hoc explanations are not a\n\"silver bullet\" solution to the black box problem in medical AI, we conclude\nthat they remain a useful strategy for addressing the black box problem in\nmedical AI.", "AI": {"tldr": "This paper defends the value of post-hoc explanations in medical AI, arguing they enhance user understanding and clinician accuracy despite not replicating exact reasoning processes.", "motivation": "To address criticisms of post-hoc explanations in AI, particularly in the medical field, which challenge their effectiveness in supporting user understanding and decision-making.", "method": "", "result": "The paper argues that post-hoc explanations improve users' understanding of black box systems and aid clinicians in making justifiable AI-informed decisions, despite limitations.", "conclusion": "Post-hoc explanations, while not perfect, are beneficial in navigating the complexities of black box AI in medical contexts.", "key_contributions": ["Defends post-hoc explanations in medical AI", "Explains their role in improving user understanding", "Discusses their importance in clinician decision-making"], "limitations": "", "future_work": "", "keywords": ["Explainable AI", "post-hoc explanations", "medical AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20355", "pdf": "https://arxiv.org/pdf/2504.20355.pdf", "abs": "https://arxiv.org/abs/2504.20355", "title": "Local Prompt Optimization", "authors": ["Yash Jain", "Vishal Chowdhary"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as Oral at NAACL 2025 (Main Conference)", "summary": "In recent years, the use of prompts to guide the output of Large Language\nModels have increased dramatically. However, even the best of experts struggle\nto choose the correct words to stitch up a prompt for the desired task. To\nsolve this, LLM driven prompt optimization emerged as an important problem.\nExisting prompt optimization methods optimize a prompt globally, where in all\nthe prompt tokens have to be optimized over a large vocabulary while solving a\ncomplex task. The large optimization space (tokens) leads to insufficient\nguidance for a better prompt. In this work, we introduce Local Prompt\nOptimization (LPO) that integrates with any general automatic prompt\nengineering method. We identify the optimization tokens in a prompt and nudge\nthe LLM to focus only on those tokens in its optimization step. We observe\nremarkable performance improvements on Math Reasoning (GSM8k and MultiArith)\nand BIG-bench Hard benchmarks across various automatic prompt engineering\nmethods. Further, we show that LPO converges to the optimal prompt faster than\nglobal methods.", "AI": {"tldr": "Introduction of Local Prompt Optimization (LPO) for improving the efficiency of prompt engineering in Large Language Models by optimizing specific tokens rather than the entire prompt.", "motivation": "Experts struggle with crafting effective prompts for Large Language Models, leading to the need for better optimization methods.", "method": "Local Prompt Optimization (LPO) selects and optimizes specific tokens in prompts, allowing for focused improvements in their generation.", "result": "Remarkable performance enhancements were observed in benchmarks like GSM8k, MultiArith, and BIG-bench Hard, with LPO converging to optimal prompts more quickly than traditional global methods.", "conclusion": "LPO demonstrates a more efficient and effective approach to prompt engineering by localizing the optimization process.", "key_contributions": ["Development of Local Prompt Optimization (LPO) technique", "Improved performance on Math Reasoning and BIG-bench benchmarks", "Faster convergence to optimal prompts compared to global methods"], "limitations": "", "future_work": "Exploring further enhancements to LPO and integrating it with various automatic prompt engineering techniques.", "keywords": ["Large Language Models", "Prompt Optimization", "Local Prompt Optimization"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2504.20782", "pdf": "https://arxiv.org/pdf/2504.20782.pdf", "abs": "https://arxiv.org/abs/2504.20782", "title": "Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces", "authors": ["Daniel Gaspar-Figueiredo", "Marta Fernández-Diego", "Silvia Abrahão", "Emilio Insfran"], "categories": ["cs.HC", "cs.SE"], "comment": "Accepted for publication at the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025)", "summary": "Adaptive User Interfaces (AUI) play a crucial role in modern software\napplications by dynamically adjusting interface elements to accommodate users'\ndiverse and evolving needs. However, existing adaptation strategies often lack\nreal-time responsiveness. Reinforcement Learning (RL) has emerged as a\npromising approach for addressing complex, sequential adaptation challenges,\nenabling adaptive systems to learn optimal policies based on previous\nadaptation experiences. Although RL has been applied to AUIs,integrating RL\nagents effectively within user interactions remains a challenge.\n  In this paper, we enhance a RL-based Adaptive User Interface adaption\nframework by incorporating personalized human feedback directly into the\nleaning process. Unlike prior approaches that rely on a single pre-trained RL\nmodel, our approach trains a unique RL agent for each user, allowing\nindividuals to actively shape their personal RL agent's policy, potentially\nleading to more personalized and responsive UI adaptations. To evaluate this\napproach, we conducted an empirical study to assess the impact of integrating\nhuman feedback into the RL-based Adaptive User Interface adaption framework and\nits effect on User Experience (UX). The study involved 33 participants\ninteracting with AUIs incorporating human feedback and non-adaptive user\ninterfaces in two domains: an e-learning platform and a trip-planning\napplication. The results suggest that incorporating human feedback into\nRL-driven adaptations significantly enhances UX, offering promising directions\nfor advancing adaptive capabilities and user-centered design in AUIs.", "AI": {"tldr": "The paper presents an enhanced framework for RL-based Adaptive User Interfaces that integrates personalized human feedback into the learning process, leading to improved user experience (UX).", "motivation": "To address the lack of real-time responsiveness in existing adaptive user interface strategies and to enhance personalization by incorporating user feedback in reinforcement learning.", "method": "The study developed a unique RL agent for each user, allowing them to actively influence their agent’s policy through feedback during interactions with adaptive user interfaces.", "result": "Empirical study results indicated that integrating human feedback into RL-based adaptations significantly improved user experience compared to non-adaptive interfaces.", "conclusion": "Enhancing RL-based AUIs with personalized human feedback can lead to more responsive and user-centered design, paving the way for future research in this area.", "key_contributions": ["Introduction of personalized RL agents for each user in adaptive interfaces", "Demonstration of the positive impact of human feedback on user experience in AUIs", "Evaluation across two application domains (e-learning and trip planning)", "Empirical evidence supporting UX enhancement through adaptive methods"], "limitations": "The study is limited to two application domains and a relatively small participant group, which may impact generalizability.", "future_work": "Future research should explore broader application domains and larger, more diverse participant groups to validate findings and improve RL-based UI adaptation.", "keywords": ["Adaptive User Interfaces", "Reinforcement Learning", "User Experience", "Personalized Feedback", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.20356", "pdf": "https://arxiv.org/pdf/2504.20356.pdf", "abs": "https://arxiv.org/abs/2504.20356", "title": "What Causes Knowledge Loss in Multilingual Language Models?", "authors": ["Maria Khelli", "Samuel Cahyawijaya", "Ayu Purwarianti", "Genta Indra Winata"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer in natural language processing (NLP) models enhances\nmultilingual performance by leveraging shared linguistic knowledge. However,\ntraditional methods that process all data simultaneously often fail to mimic\nreal-world scenarios, leading to challenges like catastrophic forgetting, where\nfine-tuning on new tasks degrades performance on previously learned ones. Our\nstudy explores this issue in multilingual contexts, focusing on linguistic\ndifferences affecting representational learning rather than just model\nparameters. We experiment with 52 languages using LoRA adapters of varying\nranks to evaluate non-shared, partially shared, and fully shared parameters.\nOur aim is to see if parameter sharing through adapters can mitigate forgetting\nwhile preserving prior knowledge. We find that languages using non-Latin\nscripts are more susceptible to catastrophic forgetting, whereas those written\nin Latin script facilitate more effective cross-lingual transfer.", "AI": {"tldr": "This study investigates cross-lingual transfer in NLP models, focusing on catastrophic forgetting and the use of LoRA adapters to enhance multilingual performance.", "motivation": "To address the challenges of catastrophic forgetting in NLP models during cross-lingual transfer.", "method": "The study experiments with 52 languages using LoRA adapters of varying ranks to evaluate the effects of non-shared, partially shared, and fully shared parameters.", "result": "The findings suggest that languages with non-Latin scripts are more prone to catastrophic forgetting than those with Latin scripts, indicating a difference in effectiveness for cross-lingual transfer depending on the script.", "conclusion": "Parameter sharing through adapters can help mitigate forgetting while maintaining prior knowledge, though effectiveness varies with language script.", "key_contributions": ["Investigation of catastrophic forgetting in multilingual NLP contexts", "Analysis of LoRA adapters in managing parameter sharing", "Comparison of non-Latin and Latin script languages in cross-lingual transfer efficiency"], "limitations": "The study primarily examines 52 languages; results may not generalize to all languages or other NLP tasks.", "future_work": "Further research could explore more languages and different adapter configurations to fully understand their impact on multilingual performance.", "keywords": ["Cross-lingual transfer", "catastrophic forgetting", "LoRA adapters", "multilingual NLP", "linguistic diversity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.20844", "pdf": "https://arxiv.org/pdf/2504.20844.pdf", "abs": "https://arxiv.org/abs/2504.20844", "title": "Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations", "authors": ["Angelika Kothe", "Volker Hohmann", "Giso Grimm"], "categories": ["cs.HC", "cs.SD"], "comment": null, "summary": "Interactive communication in virtual reality can be used in experimental\nparadigms to increase the ecological validity of hearing device evaluations.\nThis requires the virtual environment to elicit natural communication behaviour\nin listeners. This study evaluates the effect of virtual animated characters'\nhead movements on participants' communication behaviour and experience.\n  Triadic conversations were conducted between a test participant and two\nconfederates. To facilitate the manipulation of head movements, the\nconversation was conducted in telepresence using a system that transmitted\naudio, head movement data and video with low delay. The confederates were\nrepresented by virtual animated characters (avatars) with different levels of\nanimation: Static heads, automated head movement animations based on speech\nlevel onsets, and animated head movements based on the transmitted head\nmovements of the interlocutors. A condition was also included in which the\nvideos of the interlocutors' heads were embedded in the visual scene.\n  The results show significant effects of animation level on the participants'\nspeech and head movement behaviour as recorded by physical sensors, as well as\non the subjective sense of presence and the success of the conversation. The\nlargest effects were found for the range of head orientation during speech and\nthe perceived realism of avatars. Participants reported that they were spoken\nto in a more helpful way when the avatars showed head movements transmitted\nfrom the interlocutors than when the avatars' heads were static.\n  We therefore conclude that the representation of interlocutors must include\nsufficiently realistic head movements in order to elicit natural communication\nbehaviour.", "AI": {"tldr": "The study evaluates how virtual animated characters' head movements affect communication behavior in virtual reality, showing that realistic head movements enhance conversation success and participants' engagement.", "motivation": "To improve the ecological validity of hearing device evaluations in virtual environments by mimicking natural communication behavior.", "method": "Triadic conversations were conducted in telepresence, using a low-latency system to manipulate and transmit head movements between participants and avatars with varying levels of head movement animations.", "result": "Significant effects of the animation level on speech and head movement behavior were observed, with increased realism leading to enhanced presence and conversational outcomes.", "conclusion": "Realistic head movements in virtual avatars are crucial for fostering natural communication behavior among users.", "key_contributions": ["Demonstrated the impact of animated avatars on natural communication behaviors in VR.", "Showed that more realistic head movements improve conversational effectiveness and user experience.", "Introduced a telepresence method for evaluating communication behaviors with virtual characters."], "limitations": "Focused primarily on head movements without exploring other non-verbal cues or environmental factors that may influence communication.", "future_work": "Explore further non-verbal cues and their effects on communication in virtual reality settings, and assess the implications for various applications such as remote collaboration and education.", "keywords": ["Virtual Reality", "Communication Behavior", "Animated Avatars", "Ecological Validity", "Telepresence"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2504.20371", "pdf": "https://arxiv.org/pdf/2504.20371.pdf", "abs": "https://arxiv.org/abs/2504.20371", "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation", "authors": ["Zhibo Man", "Yuanmeng Chen", "Yujie Zhang", "Yufeng Chen", "Jinan Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, Large Language Models (LLMs) have achieved remarkable results in\nmachine translation. However, their performance in multi-domain translation\n(MDT) is less satisfactory; the meanings of words can vary across different\ndomains, highlighting the significant ambiguity inherent in MDT. Therefore,\nevaluating the disambiguation ability of LLMs in MDT remains an open problem.\nTo this end, we present an evaluation and analysis of LLMs on disambiguation in\nmulti-domain translation (DMDTEval), our systematic evaluation framework\nconsisting of three critical aspects: (1) we construct a translation test set\nwith multi-domain ambiguous word annotation, (2) we curate a diverse set of\ndisambiguation prompting templates, and (3) we design precise disambiguation\nmetrics, and study the efficacy of various prompting strategies on multiple\nstate-of-the-art LLMs. Our extensive experiments reveal a number of crucial\nfindings that we believe will pave the way and also facilitate further research\nin the critical area of improving the disambiguation of LLMs.", "AI": {"tldr": "This paper presents DMDTEval, a framework to evaluate the disambiguation ability of LLMs in multi-domain translation.", "motivation": "To address the inadequate performance of LLMs in multi-domain translation (MDT) due to ambiguity in meanings of words across domains.", "method": "The authors developed a systematic evaluation framework (DMDTEval) that includes constructing a translation test set with annotated ambiguous words, curating disambiguation prompting templates, and designing disambiguation metrics to evaluate various prompting strategies on LLMs.", "result": "The extensive experiments conducted through the framework revealed significant findings regarding the effectiveness of different prompting strategies in improving disambiguation.", "conclusion": "The findings aim to enhance understanding and further research into improving LLMs' disambiguation capabilities in multi-domain translation contexts.", "key_contributions": ["Development of the DMDTEval framework for evaluating LLMs in MDT", "Construction of a test set with multi-domain ambiguous word annotations", "Design of new disambiguation metrics and prompting strategies for LLMs"], "limitations": "The paper does not explore the real-world application scenarios of the developed framework and its methodologies.", "future_work": "Future research could focus on applying the framework to real-world MDT applications and exploring additional strategies for disambiguation.", "keywords": ["multi-domain translation", "disambiguation", "large language models", "evaluation framework", "prompting strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.20886", "pdf": "https://arxiv.org/pdf/2504.20886.pdf", "abs": "https://arxiv.org/abs/2504.20886", "title": "Mapping a Movement: Exploring a Proposed Police Training Facility in Atlanta and the Stop Cop City Movement through Online Maps", "authors": ["Camille Harris", "Clio Andris"], "categories": ["cs.HC"], "comment": "Supplementary material available at\n  https://doi.org/10.7910/DVN/PCQ294", "summary": "In 2021, the City of Atlanta and Atlanta Police Foundation launched plans to\nbuild a large police training facility in the South River Forest in\nunincorporated DeKalb County, GA. Residents of Atlanta and DeKalb County,\nenvironmental activists, police and prison abolitionists, and other activists\nand concerned individuals formed the movement in opposition to the facility,\nknown as the Stop Cop City / Defend the Atlanta Forest movement. Social media\nand digital maps became common tools for communicating information about the\nfacility and the movement. Here, we examine online maps about the facility and\nthe opposition movement, originating from grassroots organizations, the City of\nAtlanta, news media outlets, the Atlanta Police Foundation, and individuals. We\ngather and examine 32 publicly available maps collected through the Google\nSearch API, Twitter (now X), Instagram and reddit. Using a framework of\ncritical cartography, we conduct a content analysis of these maps to identify\nthe mapping technologies and techniques (data, cartographic elements, styles)\nused by different stakeholders and roles that maps and mapping technologies can\nplay in social movements. We examine the extent to which these maps provide\ndata to confirm or contradict concerns raised by grassroots organizations and\nlocal residents about the facility. We find that stakeholders and mapmakers use\ngeospatial tools in different ways and likely have varied access to mapping\ntechnologies. We argue that documenting the use of maps to communicate\ninformation about a contentious project can help enumerate community positions\nand perspectives, and we advocate for accessible mapmaking tools. We conclude\nby discussing the implications of accessibility of mapping technology and\nposting maps to social media, and share example map images that extend the\ngeographic information systems (GIS) techniques seen in the retrieved maps.", "AI": {"tldr": "This paper analyzes the use of online maps in the opposition movement against the Atlanta police training facility, examining their implications for social movements.", "motivation": "The motivation for this study is to understand how diverse stakeholders utilize maps and mapping technologies in social movements, particularly in opposition to contentious projects like the police training facility in Atlanta.", "method": "The study conducts a content analysis of 32 publicly available maps sourced from various platforms including Google Search API, Twitter, Instagram, and Reddit, using a framework of critical cartography.", "result": "The findings reveal that stakeholders use geospatial tools differently and exhibit varying access to mapping technologies, influencing how they communicate information about the facility and the opposition movement.", "conclusion": "The study concludes that documenting map usage in social movements can reveal community perspectives and stresses the importance of making mapmaking tools accessible to enhance public discourse.", "key_contributions": ["Analysis of online maps in social movements", "Insights into stakeholder mapping behaviors and access", "Recommendations for accessible mapmaking tools"], "limitations": "The study is limited to publicly available maps and may not capture all perspectives in the opposition movement.", "future_work": "Future research should explore the impact of accessible mapping tools on community engagement and the effectiveness of geospatial communication in social movements.", "keywords": ["police training facility", "mapping technologies", "social movements", "critical cartography", "community perspectives"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2504.20444", "pdf": "https://arxiv.org/pdf/2504.20444.pdf", "abs": "https://arxiv.org/abs/2504.20444", "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?", "authors": ["Mika Hämäläinen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst.", "AI": {"tldr": "This study investigates the primacy effect in three commercial LLMs by replicating a classic human experiment, revealing varied preferences in candidate descriptions based on the order of positive and negative adjectives.", "motivation": "To understand how the primacy effect influences the decision-making of large language models in selecting candidates based on adjectives used in descriptions.", "method": "Two experiments were conducted; the first presented candidates simultaneously in a single prompt and the second presented them separately, both focusing on 200 candidate pairs.", "result": "ChatGPT preferred positive adjectives first in simultaneous prompts, while Gemini showed no preference. In separate prompts, both ChatGPT and Claude favored negative adjectives first over positive ones when they did not rank equally.", "conclusion": "The findings indicate that LLMs display distinct decision-making patterns influenced by the order of adjectives, with implications for their response generation.", "key_contributions": ["Repurposing a classic psychological experiment to analyze LLM behavior.", "Demonstrating diverse preferences among different LLMs based on input structure.", "Highlighting the impact of adjective order on LLM decision-making."], "limitations": "The study only considers three LLMs, and the findings may not generalize to all models or contexts.", "future_work": "Exploration of additional LLMs, further understanding of the primacy effects across different tasks, and potential applications in enhancing LLM responses.", "keywords": ["primacy effect", "large language models", "candidate preference", "adjective order", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20976", "pdf": "https://arxiv.org/pdf/2504.20976.pdf", "abs": "https://arxiv.org/abs/2504.20976", "title": "Real-Time Wayfinding Assistant for Blind and Low-Vision Users", "authors": ["Dabbrata Das", "Argho Deb Das", "Farhan Sadaf"], "categories": ["cs.HC"], "comment": null, "summary": "Navigating unfamiliar places continues to be one of the most persistent and\nessential everyday obstacles for those who are blind or have limited vision\n(BLV). Existing assistive technologies, such as GPS-based navigation systems,\nAI-powered smart glasses, and sonar-equipped canes, often face limitations in\nreal-time obstacle avoidance, precise localization, and adaptability to dynamic\nsurroundings. To investigate potential solutions, we introduced PathFinder, a\nnovel map-less navigation system that explores different models for\nunderstanding 2D images, including Vision Language Models (VLMs), Large\nLanguage Models (LLMs), and employs monocular depth estimation for free-path\ndetection. Our approach integrates a Depth-First Search (DFS) algorithm on\ndepth images to determine the longest obstacle-free path, ensuring optimal\nroute selection while maintaining computational efficiency. We conducted\ncomparative evaluations against existing AI-powered navigation methods and\nperformed a usability study with BLV participants. The results demonstrate that\nPathFinder achieves a favorable balance between accuracy, computational\nefficiency, and real-time responsiveness. Notably, it reduces mean absolute\nerror (MAE) and improves decision-making speed in outdoor navigation compared\nto AI-based alternatives. Participant feedback emphasizes the system's\nusability and effectiveness in outside situations, but also identifies issues\nin complicated indoor locations and low-light conditions. Usability testing\nrevealed that 73% of participants understood how to use the app in about a\nminute, and 80% praised its balance of accuracy, quick response, and overall\nconvenience.", "AI": {"tldr": "PathFinder is a novel map-less navigation system for blind and visually limited individuals that utilizes Vision Language Models and depth estimation for obstacle-free pathway detection.", "motivation": "To address limitations in existing assistive technologies for navigation by blind or visually limited individuals, particularly in real-time obstacle avoidance and adaptability to dynamic environments.", "method": "PathFinder employs monocular depth estimation and integrates a Depth-First Search algorithm on depth images to identify the longest obstacle-free path, enriching traditional navigation systems with advanced AI techniques.", "result": "PathFinder demonstrates improved accuracy, computational efficiency, and real-time responsiveness compared to existing AI-powered navigation methods, showcasing reduced mean absolute error and enhanced decision-making speed.", "conclusion": "The usability study indicates PathFinder's effectiveness in outdoor navigation and its user-friendly design, although challenges remain in complex indoor and low-light environments.", "key_contributions": ["Introduction of PathFinder as a map-less navigation solution for BLV individuals.", "Integration of Vision Language Models and Depth-First Search for obstacle detection.", "Empirical evidence showcasing significant improvements in navigation accuracy and user satisfaction."], "limitations": "Issues in complex indoor locations and low-light conditions were identified during usability tests.", "future_work": "Further research is needed to enhance performance in challenging environments like indoor scenarios and low-light conditions.", "keywords": ["navigation", "assistive technology", "blind and visually impaired", "Vision Language Models", "depth estimation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20451", "pdf": "https://arxiv.org/pdf/2504.20451.pdf", "abs": "https://arxiv.org/abs/2504.20451", "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs", "authors": ["Daniel Lee", "Harsh Sharma", "Jieun Han", "Sunny Jeong", "Alice Oh", "Vered Shwartz"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SemEval-2025 Workshop (ACL 2025)", "summary": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation.", "AI": {"tldr": "This paper evaluates 13 models for translating English and Korean, finding LLMs outperform traditional MT yet struggle with culturally nuanced entity translation.", "motivation": "To improve machine translation between English and Korean while preserving cultural nuances and entity richness.", "method": "Evaluation of 13 models (LLMs and MT models) using both automatic metrics and human assessment by bilingual annotators.", "result": "LLMs outperform traditional MT systems, but encounter issues with culturally nuanced entity translation; performance varies by entity type.", "conclusion": "The study identifies gaps in automatic evaluation metrics and proposes a need for improved handling of culturally nuanced translations in future research.", "key_contributions": ["Development of an error taxonomy for translation errors", "Identification of entity translation challenges in MT", "Comparison of LLMs and traditional MT systems for cultural nuances"], "limitations": "Findings are based on specific language pairs and may not generalize to others; results may vary with different entity types or contexts.", "future_work": "Encouragement of further research on enhancing machine translation systems to better handle culturally nuanced content and entity translation.", "keywords": ["Machine Translation", "Cultural Nuances", "Language Models", "Entity Translation", "Error Taxonomy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.11357", "pdf": "https://arxiv.org/pdf/2406.11357.pdf", "abs": "https://arxiv.org/abs/2406.11357", "title": "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "authors": ["Zhonghao Li", "Xuming Hu", "Aiwei Liu", "Kening Zheng", "Sirui Huang", "Hui Xiong"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.MA"], "comment": "8 pages", "summary": "Large Language Models (LLMs) are limited by their parametric knowledge,\nleading to hallucinations in knowledge-extensive tasks. To address this,\nRetrieval-Augmented Generation (RAG) incorporates external document chunks to\nexpand LLM knowledge. Furthermore, compressing information from document chunks\nthrough extraction or summarization can improve LLM performance. Nonetheless,\nLLMs still struggle to notice and utilize scattered key information, a problem\nknown as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to\nrestructure the content for LLM to recognize the key information. We propose\n$\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that\noperates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a\nsingle decoder-only LLM to adaptively extract query-relevant contents verbatim\nalong with the necessary context, and section them based on their\ninterconnectedness, thereby highlights information distinction, and aligns\ndownstream LLMs with the original context effectively. Experiments show that a\ntrained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to\ndownstream LLM in improving answer accuracy, and outperforms other\nstate-of-the-art advanced RAG and concurrent compressing approaches in various\nsingle-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5%\ntokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared\nto the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that\ncan be seamlessly integrated with RAG systems, facilitating its application\nacross diverse open-source frameworks.", "AI": {"tldr": "This paper introduces $\textit{Refiner}$, an end-to-end extract-and-restructure paradigm designed to enhance the performance of Retrieval-Augmented Generation (RAG) by improving how LLMs utilize key information.", "motivation": "LLMs often experience hallucinations in knowledge-intensive tasks due to their limited parametric knowledge and struggle with utilizing scattered key information, often termed as the \"lost-in-the-middle\" syndrome.", "method": "Refiner leverages a single decoder-only LLM to adaptively extract content relevant to the query while also providing necessary context, restructuring this information for better connectivity and recognition by downstream LLMs.", "result": "Refiner, a 7B parameter model, significantly improves answer accuracy in downstream LLMs, achieving an 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop QA tasks compared to state-of-the-art alternatives.", "conclusion": "Refiner can be easily integrated with existing RAG systems, making it a flexible solution for enhancing LLM performance across various open-source frameworks.", "key_contributions": ["Introduces a new paradigm for extracting and restructuring content in RAG contexts.", "Demonstrates significant improvements in multi-hop QA tasks and reduction of token usage.", "Offers a plug-and-play solution for RAG systems."], "limitations": "", "future_work": "Exploration of further enhancements in the restructuring algorithms and broader applications across various NLP tasks.", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Information Structuring"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2504.20469", "pdf": "https://arxiv.org/pdf/2504.20469.pdf", "abs": "https://arxiv.org/abs/2504.20469", "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models", "authors": ["Enfa Fane", "Mihai Surdeanu", "Eduardo Blanco", "Steven R. Corman"], "categories": ["cs.CL", "cs.CY", "I.2.7"], "comment": "Accepted to The 19th International Workshop on Semantic Evaluation\n  (Semeval 2025)", "summary": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing.", "AI": {"tldr": "This study evaluates large language models' (LLMs) ability to classify framing roles in news narratives, highlighting the impact of input context and prompting strategies.", "motivation": "To understand how news narratives frame entities and their implications on societal perceptions.", "method": "The paper employs systematic experiments to assess the zero-shot capabilities of LLMs, focusing on input context, prompting strategies, and task decomposition.", "result": "A hierarchical classification approach yields a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, outperforming single-step methods.", "conclusion": "The study emphasizes the necessity for tailored prompt design and optimal input context to enhance LLM performance in framing classification tasks.", "key_contributions": ["Evaluation of LLM zero-shot capabilities in framing roles", "Introduction of a hierarchical approach for classification", "Demonstration of the variation in prompt effectiveness per task level"], "limitations": "", "future_work": "Further exploration of subtask-specific strategies and their impacts on LLM performance.", "keywords": ["large language models", "news narratives", "entity framing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20484", "pdf": "https://arxiv.org/pdf/2504.20484.pdf", "abs": "https://arxiv.org/abs/2504.20484", "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training", "authors": ["Linjuan Wu", "Haoran Wei", "Huan Lin", "Tianhao Li", "Baosong Yang", "Weiming Lu"], "categories": ["cs.CL"], "comment": "12 pages, 6 figures, Under Review", "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation.", "AI": {"tldr": "The paper proposes Cross-lingual In-context Pre-training (CrossIC-PT), a method for enhancing cross-lingual transfer in LLMs using semantically related bilingual texts.", "motivation": "To address the limitations of parallel resources in existing methods for enhancing cross-lingual transfer in language models.", "method": "CrossIC-PT samples are constructed by interleaving semantically related bilingual Wikipedia documents into a single context window and managing window size with a systematic segmentation policy.", "result": "CrossIC-PT improves multilingual performance across three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) in six target languages, achieving notable performance gains.", "conclusion": "The proposed approach shows promise in enhancing multilingual capabilities of LLMs without relying heavily on parallel resources.", "key_contributions": ["Introduction of Cross-lingual In-context Pre-training (CrossIC-PT) methodology.", "Demonstrated improvement in multilingual performance of state-of-the-art LLMs.", "Implementation of a semantic retrieval framework to enhance data availability."], "limitations": "The study is still under review, indicating unverified results; further validation on a wider range of languages is needed.", "future_work": "Investigation into more diverse cross-lingual datasets and potential scalability of the methodology.", "keywords": ["Cross-lingual transfer", "Multilingual performance", "Large language models"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.20500", "pdf": "https://arxiv.org/pdf/2504.20500.pdf", "abs": "https://arxiv.org/abs/2504.20500", "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation", "authors": ["Huimin Lu", "Masaru Isonuma", "Junichiro Mori", "Ichiro Sakata"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICLR 2025 (poster)", "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs.", "AI": {"tldr": "UniDetox is a universal method for detoxifying LLMs without model-specific tuning, using a novel dataset distillation approach and contrastive decoding.", "motivation": "To address the limitations of existing detoxification methods that are model-specific and require extensive hyperparameter tuning.", "method": "A dataset distillation technique utilizing contrastive decoding to create synthetic text data for detoxification across various LLMs.", "result": "Experiments show that detoxifying text distilled from GPT-2 effectively detoxifies larger models like OPT, Falcon, and LLaMA-2 without the need for separate hyperparameter tuning.", "conclusion": "UniDetox offers a streamlined and effective way to mitigate toxicity in LLMs while reducing politically biased content in the generated text.", "key_contributions": ["Universal application across multiple LLMs without model-specific tuning", "Use of a novel dataset distillation technique for producing detoxifying representations", "Reduction of politically biased content in LLM outputs"], "limitations": "", "future_work": "Exploration of further optimization techniques and evaluation of UniDetox on newer LLM architectures.", "keywords": ["detoxification", "large language models", "dataset distillation", "contrastive decoding", "bias reduction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.20547", "pdf": "https://arxiv.org/pdf/2504.20547.pdf", "abs": "https://arxiv.org/abs/2504.20547", "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "authors": ["Jesus Lovon", "Thouria Ben-Haddi", "Jules Di Scala", "Jose G. Moreno", "Lynda Tamine"], "categories": ["cs.CL"], "comment": null, "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement.", "AI": {"tldr": "This paper revisits the MIMIC-IV benchmark to standardize evaluation in medical text inputs, integrating EHR data with Hugging Face and exploring template usage for data conversion.", "motivation": "To address the lack of standardized evaluation benchmarks in the medical domain for natural language models, which hinders their adoption in health tasks.", "method": "Integration of MIMIC-IV data into Hugging Face datasets, followed by experiments using fine-tuned and zero-shot LLMs to assess their performance on the mortality of patients task.", "result": "Fine-tuned text-based models compete well against traditional tabular classifiers, while zero-shot LLMs perform poorly with EHR data.", "conclusion": "Text-based models show potential in medical applications, highlighting a need for further development in zero-shot learning approaches for EHR data.", "key_contributions": ["Integration of MIMIC-IV EHR data into the Hugging Face datasets library", "Investigation of template methods for converting EHR tabular data to text", "Evaluation of LLMs on mortality prediction tasks revealing strengths of fine-tuned models."], "limitations": "The study primarily focuses on mortality prediction, not addressing the full spectrum of EHR-related tasks.", "future_work": "Further exploration of text-based methods and improvement directions for zero-shot LLM performance on EHR data.", "keywords": ["MIMIC-IV", "electronic health records", "natural language processing", "health informatics", "LLM"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.20552", "pdf": "https://arxiv.org/pdf/2504.20552.pdf", "abs": "https://arxiv.org/abs/2504.20552", "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters", "authors": ["Baz Roland", "Kristina Malyseva", "Anna Pappa", "Tristan Cazenave"], "categories": ["cs.CL"], "comment": null, "summary": "This project introduces BrAIcht, an AI conversational agent that creates\ndialogues in the distinctive style of the famous German playwright Bertolt\nBrecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7\nbillion parameters and a modified version of the base Llama2 suitable for\nGerman language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of\nother German plays that are stylistically similar to Bertolt Brecht are used to\nform a more di-erse dataset. Due to the limited memory capacity, a\nparameterefficient fine-tuning technique called QLoRA is implemented to train\nthe large language model. The results, based on BLEU score and perplexity, show\nvery promising performance of BrAIcht in generating dialogues in the style of\nBertolt Brecht.", "AI": {"tldr": "BrAIcht is an AI conversational agent that generates dialogues in the style of Bertolt Brecht using a fine-tuned large language model.", "motivation": "To create a conversational agent that can replicate the distinctive dialogue style of German playwright Bertolt Brecht.", "method": "BrAIcht was fine-tuned using German LeoLM, a 7 billion parameter LLM, employing a parameter-efficient technique called QLoRA on a dataset containing 29 of Brecht's plays and 907 similar German plays.", "result": "The performance of BrAIcht in generating dialogues was evaluated using BLEU score and perplexity, yielding promising results.", "conclusion": "BrAIcht shows a significant capability in generating dialogues that mimic Bertolt Brecht's style, indicating the potential of using fine-tuned LLMs for creative applications in language generation.", "key_contributions": ["Introduction of BrAIcht, an AI agent for generating dialogues in Brecht's style", "Use of QLoRA for parameter-efficient fine-tuning", "Development of a diverse training dataset from multiple German plays"], "limitations": "", "future_work": "Further improvements on generating more nuanced dialogues and expanding the model's capabilities to other playwrights.", "keywords": ["AI conversational agent", "Bertolt Brecht", "large language model", "fine-tuning", "dialogue generation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581.pdf", "abs": "https://arxiv.org/abs/2504.20581", "title": "ClonEval: An Open Voice Cloning Benchmark", "authors": ["Iwona Christop", "Tomasz Kuczyński", "Marek Kubis"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard.", "AI": {"tldr": "Introduction of a benchmark for evaluating voice cloning text-to-speech models.", "motivation": "To create a standardized method for assessing the performance of voice cloning models.", "method": "Development of an evaluation protocol, an open-source library, and a leaderboard for benchmarking.", "result": "Provides a structured way to evaluate and compare voice cloning models through a software library and leaderboard.", "conclusion": "The benchmark serves as a comprehensive tool for researchers to evaluate their voice cloning technologies.", "key_contributions": ["Novel benchmark for voice cloning models", "Open-source performance assessment library", "Evaluation protocol and leaderboard for consistent assessment"], "limitations": "", "future_work": "Improvement of the benchmark and expansion to cover additional voice characteristics and model types.", "keywords": ["voice cloning", "text-to-speech", "benchmark", "evaluation", "leaderboard"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605.pdf", "abs": "https://arxiv.org/abs/2504.20605", "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "categories": ["cs.CL"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.", "AI": {"tldr": "The paper presents TF1-EN-3M, an open dataset of three million English-language fables generated by instruction-tuned models, providing a structured corpus for moral storytelling.", "motivation": "To provide a large, structured corpus of narratives paired with explicit ethical lessons, as modern NLP lacks such a resource.", "method": "The fables are generated using a combinatorial prompt engine based on a six-slot scaffold. A hybrid evaluation pipeline is used to assess grammar, creativity, moral clarity, template adherence, diversity, and readability.", "result": "An 8B-parameter Llama-3 variant produced high-quality fables efficiently, utilizing minimal GPU resources at a low cost.", "conclusion": "The TF1-EN-3M dataset facilitates research in various NLP areas, demonstrating effective narrative generation without reliance on large proprietary models.", "key_contributions": ["Release of the first open dataset of three million moral fables", "Introduction of a hybrid evaluation pipeline for narrative quality", "Demonstration of cost-effective narrative generation using smaller models"], "limitations": "", "future_work": "Future research can explore instruction following, narrative intelligence, and the integration of moral storytelling into child-friendly educational AI.", "keywords": ["moral storytelling", "NLP", "dataset", "instruction tuning", "narrative intelligence"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.20609", "pdf": "https://arxiv.org/pdf/2504.20609.pdf", "abs": "https://arxiv.org/abs/2504.20609", "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks", "authors": ["Xinyu Yao", "Mengdi Wang", "Bo Chen", "Xiaobing Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing.", "AI": {"tldr": "This paper presents WenyanGPT, a language model tailored for Classical Chinese, which outperforms existing models on Classical Chinese tasks.", "motivation": "Classical Chinese is crucial for the study of ancient literature, yet existing NLP models are optimized for Modern Chinese and perform poorly on Classical Chinese tasks.", "method": "The approach involves continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model to create WenyanGPT, along with developing an evaluation benchmark dataset named WenyanBENCH.", "result": "WenyanGPT significantly outperforms current advanced LLMs on various Classical Chinese tasks, as demonstrated by experimental results on WenyanBENCH.", "conclusion": "The paper highlights the importance of specialized models for Classical Chinese and makes resources publicly available to foster further research.", "key_contributions": ["Introduction of WenyanGPT for Classical Chinese processing", "Development of the WenyanBENCH evaluation benchmark dataset", "Public availability of training and evaluation data for research advancement"], "limitations": "", "future_work": "Encouraging further exploration and development in the domain of Classical Chinese processing.", "keywords": ["Classical Chinese", "natural language processing", "WenyanGPT", "language model", "evaluation benchmark"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2504.20643", "pdf": "https://arxiv.org/pdf/2504.20643.pdf", "abs": "https://arxiv.org/abs/2504.20643", "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations", "authors": ["Moran Mizrahi", "Chen Shani", "Gabriel Stanovsky", "Dan Jurafsky", "Dafna Shahaf"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 figures", "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI.", "AI": {"tldr": "Introducing a novel approach that enhances creativity in Large Language Models (LLMs) using structured representations, demonstrated through culinary recipe generation.", "motivation": "To address the limitations of creativity in existing Large Language Models by exploring structured representations and manipulations for idea generation.", "method": "The approach couples LLMs with structured representations and cognitively inspired techniques to generate creative outputs, specifically in the domain of culinary recipes.", "result": "The model, DishCOVER, was tested against GPT-4o, showing greater diversity and novelty in generated recipes, with expert evaluations highlighting superior coherence and feasibility.", "conclusion": "The proposed method surpasses existing LLMs in generating creative outputs, suggesting new avenues for research in structured creativity within AI.", "key_contributions": ["Introduction of a structured representation approach to enhance LLM creativity", "Demonstration of improved novelty and diversity in creative tasks", "Expert evaluations supporting the effectiveness of the proposed method"], "limitations": "", "future_work": "Inspiration for further exploration into structured creativity in AI and its applications across various domains.", "keywords": ["Large Language Models", "creativity", "structured representations", "cognitive manipulations", "culinary recipes"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20668", "pdf": "https://arxiv.org/pdf/2504.20668.pdf", "abs": "https://arxiv.org/abs/2504.20668", "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages", "authors": ["Ivan Vykopal", "Martin Hyben", "Robert Moro", "Michal Gregor", "Jakub Simko"], "categories": ["cs.CL"], "comment": null, "summary": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process.", "AI": {"tldr": "The paper presents a method using large language models (LLMs) to enhance the efficiency of fact-checkers by retrieving and summarizing previously fact-checked claims, thus reducing redundancy.", "motivation": "To address the global challenge of online disinformation by improving the efficiency of fact-checkers who verify claims.", "method": "The proposed method utilizes large language models to filter and summarize previously fact-checked claims, assessing their relevance for current claims.", "result": "The evaluation shows that LLMs can filter out irrelevant fact-checks effectively, streamlining the fact-checking process.", "conclusion": "The introduced approach demonstrates significant potential in reducing the workload of fact-checkers and improving their response time to new claims.", "key_contributions": ["Introduction of a method leveraging LLMs for relevance filtering of fact-checks", "Development of a tool that assists fact-checkers with concise summaries of past claims", "Evaluation through both automated and human assessments to validate effectiveness"], "limitations": "The paper does not extensively discuss the handling of nuanced claims that may require deeper contextual understanding beyond existing fact-checks.", "future_work": "Exploration of improving LLM capabilities to handle a wider range of claims and integrating more diverse data sources for fact-checking.", "keywords": ["disinformation", "fact-checking", "large language models", "efficiency", "validation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20678", "pdf": "https://arxiv.org/pdf/2504.20678.pdf", "abs": "https://arxiv.org/abs/2504.20678", "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)", "authors": ["Yaroslav Getman", "Tamás Grósz", "Mikko Kurimo", "Giampiero Salvi"], "categories": ["cs.CL", "eess.AS"], "comment": "First draft of the baseline paper for the NOCASA competition\n  (https://teflon.aalto.fi/nocasa-2025/), 5 pages", "summary": "This paper presents the \"Non-native Children's Automatic Speech Assessment\"\n(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA\nchallenges participants to develop new systems that can assess single-word\npronunciations of young second language (L2) learners as part of a gamified\npronunciation training app. To achieve this, several issues must be addressed,\nmost notably the limited nature of available training data and the highly\nunbalanced distribution among the pronunciation level categories. To expedite\nthe development, we provide a pseudo-anonymized training data (TeflonNorL2),\ncontaining 10,334 recordings from 44 speakers attempting to pronounce 205\ndistinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that\nshould be given in the game). In addition to the data, two already trained\nsystems are released as official baselines: an SVM classifier trained on the\nComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter\nachieves the best performance on the challenge test set, with an unweighted\naverage recall (UAR) of 36.37%.", "AI": {"tldr": "Introduction of the NOCASA competition for automatic speech assessment of L2 learners, emphasizing development challenges and baseline systems.", "motivation": "To create a gamified app for pronunciation training by assessing L2 learners' speech, addressing challenges like limited training data and category imbalance.", "method": "The competition provides pseudo-anonymized training data of 10,334 recordings from 44 speakers and features two baseline systems: an SVM classifier and a multi-task wav2vec 2.0 model.", "result": "The wav2vec 2.0 model achieves the best performance on the test set with an unweighted average recall of 36.37%.", "conclusion": "The NOCASA competition aims to stimulate development in speech assessment systems for young L2 learners, offering valuable datasets and models for further research.", "key_contributions": ["Introduction of NOCASA data competition for speech assessment", "Provision of a large dataset for L2 learners", "Release of baseline models for benchmarking"], "limitations": "Limited training data and highly unbalanced pronunciation level categories.", "future_work": "Encouraging further research into better assessment systems and data diversity to improve L2 learner training.", "keywords": ["automatic speech assessment", "non-native children", "pronunciation training", "gamification", "machine learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2504.20679", "pdf": "https://arxiv.org/pdf/2504.20679.pdf", "abs": "https://arxiv.org/abs/2504.20679", "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?", "authors": ["Wing Yan Li", "Zeqiang Wang", "Jon Johnson", "Suparna De"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science.", "AI": {"tldr": "This paper addresses the challenges of detecting semantically equivalent questions in longitudinal social science surveys to enhance harmonization across studies.", "motivation": "Automated detection of semantically equivalent questions is crucial for improving empirical research in social, economic, and health sciences.", "method": "The paper explores multiple unsupervised approaches, including probabilistic models, linear probing of language models, and pre-trained neural networks to identify concept equivalence in longitudinal survey data from 1946-2020.", "result": "IR-specialised neural models achieve the highest performance, with modest improvements when re-ranking results from the probabilistic model.", "conclusion": "The findings emphasize the need for better sensitivity to questions with high lexical overlap during the harmonization process of longitudinal studies.", "key_contributions": ["Introduction of a new information retrieval task for harmonizing longitudinal surveys", "Evaluation of various unsupervised methods on historical survey data", "Insights into model sensitivity regarding lexical overlap in questions"], "limitations": "Models show low sensitivity to questions with high lexical overlap, especially with mismatched sub-concepts.", "future_work": "Further research required to improve harmonization techniques and sensitivity of models to specific question structures.", "keywords": ["information retrieval", "longitudinal studies", "semantic equivalence", "unsupervised learning", "social science"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.20699", "pdf": "https://arxiv.org/pdf/2504.20699.pdf", "abs": "https://arxiv.org/abs/2504.20699", "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?", "authors": ["Evangelia Gogoulou", "Shorouq Zahra", "Liane Guillou", "Luise Dürlich", "Joakim Nivre"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task.", "AI": {"tldr": "Study evaluates open-access LLMs on hallucination detection in translation and paraphrasing tasks.", "motivation": "To address the problem of hallucination in LLM outputs and evaluate the capability of these models in detecting such occurrences across different tasks and languages.", "method": "Evaluation of various open-access LLMs on their ability to detect intrinsic hallucinations in translation and paraphrasing tasks, considering factors like model size, instruction tuning, and prompt choice.", "result": "Performance varies across models but is consistent across prompts; NLI models perform comparably well to LLM-based detectors for this task.", "conclusion": "The findings suggest that while LLMs can detect hallucinations, NLI models are also effective, indicating a broader range of viable options for detecting hallucination.", "key_contributions": ["Introduced the HalluciGen task for hallucination detection and generation.", "Showcased the variability in LLM performance across tasks and languages.", "Demonstrated the effectiveness of NLI models in detecting hallucinations."], "limitations": "The study may not cover all possible tasks or languages, limiting the generalizability of the results.", "future_work": "Further exploration of hallucination detection across additional tasks and model architectures.", "keywords": ["hallucination", "LLMs", "NLI models", "translation", "paraphrasing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20703", "pdf": "https://arxiv.org/pdf/2504.20703.pdf", "abs": "https://arxiv.org/abs/2504.20703", "title": "BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification", "authors": ["Foteini Papadopoulou", "Osman Mutlu", "Neris Özen", "Bas H. M. van der Velden", "Iris Hendrickx", "Ali Hürriyetoğlu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system developed for the SemEval-2025 Task 9: The\nFood Hazard Detection Challenge. The shared task's objective is to evaluate\nexplainable classification systems for classifying hazards and products in two\nlevels of granularity from food recall incident reports. In this work, we\npropose text augmentation techniques as a way to improve poor performance on\nminority classes and compare their effect for each category on various\ntransformer and machine learning models. We explore three word-level data\naugmentation techniques, namely synonym replacement, random word swapping, and\ncontextual word insertion. The results show that transformer models tend to\nhave a better overall performance. None of the three augmentation techniques\nconsistently improved overall performance for classifying hazards and products.\nWe observed a statistically significant improvement (P < 0.05) in the\nfine-grained categories when using the BERT model to compare the baseline with\neach augmented model. Compared to the baseline, the contextual words insertion\naugmentation improved the accuracy of predictions for the minority hazard\nclasses by 6%. This suggests that targeted augmentation of minority classes can\nimprove the performance of transformer models.", "AI": {"tldr": "The paper develops a system for the Food Hazard Detection Challenge, evaluating the impact of text augmentation techniques on classification performance of hazards in food recall reports.", "motivation": "To improve the classification of food hazards and products using explainable classification systems that can handle minority classes effectively.", "method": "The authors employed three word-level data augmentation techniques: synonym replacement, random word swapping, and contextual word insertion, and compared their effects on various transformer and machine learning models.", "result": "The results indicated that while transformer models generally performed better, the augmentation techniques did not consistently yield improvements across all categories; however, contextual word insertion showed a 6% accuracy gain for minority classes when using the BERT model.", "conclusion": "Targeted augmentation for minority classes can enhance the performance of transformer models, particularly for fine-grained categories in food hazard classification.", "key_contributions": ["Development of a classification system for food hazard detection", "Evaluation of text augmentation techniques across different models", "Demonstrated effectiveness of contextual word insertion for minority classes"], "limitations": "The three augmentation techniques did not consistently improve overall performance across all hazard categories.", "future_work": "Future research could explore additional augmentation methods or more sophisticated models that might better address the classification of minority classes.", "keywords": ["Food Hazard Detection", "Text Augmentation", "Transformer Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.20708", "pdf": "https://arxiv.org/pdf/2504.20708.pdf", "abs": "https://arxiv.org/abs/2504.20708", "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "authors": ["Hasan Abed Al Kader Hammoud", "Hani Itani", "Bernard Ghanem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.", "AI": {"tldr": "This paper challenges the reliance on final answers in LLM evaluations, proposing a method that analyzes intermediate reasoning steps (subthoughts) for improved accuracy.", "motivation": "The study aims to evaluate the assurance of final answers produced by LLMs and explore alternative reasoning paths.", "method": "The proposed method segments reasoning traces into subthoughts based on linguistic cues and generates continuations from the end-points of these subthoughts to extract potential answers.", "result": "Aggregating answers from different subthoughts using the mode often yields higher accuracy than relying solely on the initial answer, with improvements noted in accuracy on various LLMs and datasets (up to 13% and 10%).", "conclusion": "The findings suggest that analyzing subthoughts can enhance the identification of reliable answers and improve overall model accuracy in reasoning tasks.", "key_contributions": ["Introduction of the concept of 'subthoughts' for reasoning analysis in LLMs", "A method for aggregating intermediate answers to improve evaluation accuracy", "Demonstration of significant accuracy improvements across multiple LLMs and datasets."], "limitations": "", "future_work": "Future research could further explore the characteristics of subthoughts and their role in improving LLM performance in various reasoning tasks.", "keywords": ["Large Language Models", "subthoughts", "reasoning", "accuracy improvement", "mathematical reasoning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734.pdf", "abs": "https://arxiv.org/abs/2504.20734", "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.", "AI": {"tldr": "UniversalRAG is a novel retrieval-augmented generation framework that integrates knowledge from multiple heterogeneous sources with diverse modalities to improve factual accuracy in responses.", "motivation": "To overcome limitations of existing RAG approaches that focus on single modality or unified representation, leading to a modality gap.", "method": "UniversalRAG employs a modality-aware routing mechanism to identify the relevant modality-specific corpus for targeted retrieval and organizes each modality into multiple granularity levels for more precise querying.", "result": "UniversalRAG outperforms existing modality-specific and unified retrieval approaches across 8 benchmarks involving various modalities.", "conclusion": "The framework enhances the effectiveness of retrieval-augmented generation by allowing for cross-modal and granularity-sensitive knowledge integration.", "key_contributions": ["Introduction of a novel RAG framework that retrieves knowledge from heterogeneous modalities.", "Developed a modality-aware routing mechanism for more accurate retrieval.", "Organized modalities into multiple granularity levels to improve query handling."], "limitations": "", "future_work": "Further exploration of additional modalities and improvements in the routing mechanism for even better retrieval accuracy.", "keywords": ["Retrieval-Augmented Generation", "Heterogeneous Sources", "Modality-aware Routing"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2312.04470", "pdf": "https://arxiv.org/pdf/2312.04470.pdf", "abs": "https://arxiv.org/abs/2312.04470", "title": "GaitGuard: Towards Private Gait in Mixed Reality", "authors": ["Diana Romero", "Ruchi Jagdish Patel", "Athina Markopoulou", "Salma Elmalaki"], "categories": ["cs.HC", "cs.CR"], "comment": "22 pages, 12 figures, added scalability experiment", "summary": "Augmented/Mixed Reality (AR/MR) technologies usher in a new era of immersive,\ncollective experiences, differentiating them from traditional mobile systems.\nAs these technologies evolve, prioritizing privacy and security is critical.\nThis paper focuses on gait privacy, where gait, the way a person walks, can\nreveal sensitive information such as age, ethnicity, or disorders. We present\nGaitGuard, a real-time system that protects gait privacy against video-based\ngait extraction attacks in MR environments. GaitGuard leverages a\nmulti-threaded framework to efficiently process video frames, incorporating\ndedicated modules for stream capture, body detection and tracking, and privacy\nleak mitigation. We compare and combine multiple mitigation techniques,\noffering guidance to navigate the privacy-utility tradeoff. Through extensive\nexperiments covering 248 settings across mitigation regions, types, and tunable\nparameters, we assess the impact of these techniques on privacy, video quality,\nand system performance. GaitGuard reduces the confidence of video-based gait\nextraction attacks by introducing a substantial distribution shift\n(Jensen-Shannon Divergence of 0.63, indicating highly altered gait features)\nand a decrease in identification risks by up to 68%, while maintaining 29 FPS\nand preserving video clarity. GaitGuard provides a practical real-time solution\nfor privacy-preserving MR applications without affecting the MR user experience\nbased on 20 subjective user surveys.", "AI": {"tldr": "This paper presents GaitGuard, a real-time system that protects gait privacy in Augmented/Mixed Reality environments against video-based extraction attacks, achieving a balance between privacy and user experience.", "motivation": "With the rise of AR/MR technologies, there's a growing need to address privacy concerns, particularly regarding gait information that can reveal sensitive personal data.", "method": "GaitGuard employs a multi-threaded framework to process video frames, using specific modules for stream capture, body detection, tracking, and privacy leak mitigation. Multiple techniques for privacy protection and their impacts on privacy, video quality, and performance are thoroughly analyzed through experiments involving 248 settings.", "result": "GaitGuard significantly reduces the effectiveness of gait extraction attacks, achieving a Jensen-Shannon Divergence of 0.63 and a 68% reduction in identification risks, while maintaining a processing speed of 29 FPS and video clarity.", "conclusion": "GaitGuard offers an effective solution for ensuring gait privacy in MR applications, enhancing user security without compromising overall experience based on subjective user feedback.", "key_contributions": ["Development of GaitGuard for gait privacy in MR environments.", "Analysis of various gait mitigation techniques and their impacts.", "Real-time performance and privacy efficacy in user surveys."], "limitations": "The study primarily focuses on gait privacy, and further research may be needed to address other aspects of privacy in MR technologies.", "future_work": "Expansion of the framework to include additional privacy features and further testing in diverse real-world scenarios.", "keywords": ["Gait privacy", "Augmented reality", "Privacy-preserving", "Mixed reality", "Human-computer interaction"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752.pdf", "abs": "https://arxiv.org/abs/2504.20752", "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": null, "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.", "AI": {"tldr": "This paper extends the concept of grokking to real-world factual data by augmenting knowledge graphs with synthetic data, enhancing multi-step reasoning in Transformers.", "motivation": "To address deficiencies in multi-step factual reasoning in Transformers when real-world knowledge is sparse, leveraging the concept of grokking.", "method": "Augment existing knowledge graphs with synthetic data to surpass the threshold ratio of inferred to atomic facts, focusing on enhancing emergent reasoning capabilities in Transformers.", "result": "Achieved 95-100% accuracy on the 2WikiMultiHopQA multi-hop reasoning benchmark, outperforming strong baselines and matching current state-of-the-art results.", "conclusion": "Synthetic data, even when factually incorrect, can enhance reasoning circuits by promoting reliance on relational structures, thus improving model generalization.", "key_contributions": ["First to apply grokking to real-world factual data", "Demonstrated that factually incorrect synthetic data can improve reasoning capabilities", "Achieved state-of-the-art results in multi-hop reasoning benchmarks"], "limitations": "", "future_work": "Exploration of further enhancing reasoning capabilities in broader contexts and potential applications of grokking-based data augmentation.", "keywords": ["Transformers", "grokking", "multi-step reasoning", "synthetic data", "knowledge graphs"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2405.03844", "pdf": "https://arxiv.org/pdf/2405.03844.pdf", "abs": "https://arxiv.org/abs/2405.03844", "title": "Perception in Pixels: Effects of Avatar Representation in Video-Mediated Collaborative Interactions", "authors": ["Pitch Sinlapanuntakul", "Mark Zachry"], "categories": ["cs.HC", "H.5.1; H.5.3; J.4"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Interactive collaborative video is now a common part of remote work. Despite\nits prevalence, traditional video conferencing can be challenging, sometimes\ncausing social discomforts that undermine process and outcomes. Avatars on 2D\ndisplays offer a promising alternative for enhancing self-representation,\nbridging the gap between virtual reality (VR) and traditional non-immersive\nvideo. However, the use of such avatars in activity-oriented group settings\nremains underexplored. To address this gap, we conducted a mixed-methods,\nwithin-subject study investigating the impacts of avatar-mediated versus\ntraditional video representations on collaboration satisfaction and\nself-esteem. 32 participants (8 groups of 4 with pre-established relationships)\nengaged in goal-directed activities, followed by group interviews. Results\nindicate that avatars significantly enhance self-esteem and collaboration\nsatisfaction, while qualitative insights reveal the dynamic perceptions and\nexperiences of avatars, including benefits, challenges, and factors influencing\nadoption likelihood. Our study contributes to understanding and implications of\navatars as a camera-driven representation in video-mediated collaborative\ninteractions.", "AI": {"tldr": "This study investigates the effects of avatar-mediated video conferencing on collaboration satisfaction and self-esteem compared to traditional video. Findings indicate benefits in using avatars.", "motivation": "The paper explores challenges in traditional video conferencing and how avatars might enhance self-representation and collaboration in activity-oriented group settings.", "method": "A mixed-methods, within-subject study with 32 participants engaged in goal-directed activities, followed by qualitative group interviews to gather insights.", "result": "Results show that avatars significantly enhance self-esteem and collaboration satisfaction, with qualitative feedback highlighting both positives and challenges in their use.", "conclusion": "The study contributes to the understanding of avatar use in video-mediated collaboration, showing potential benefits while also highlighting challenges.", "key_contributions": ["Investigated avatar use in remote collaboration settings", "Provided evidence of increased collaboration satisfaction and self-esteem with avatars", "Highlighted user experiences and perceptions regarding avatar adoption"], "limitations": "The study is limited to 32 participants and may not generalize to all remote work contexts.", "future_work": "Further research is needed to explore avatar use in diverse settings and with different populations to assess broader implications.", "keywords": ["video conferencing", "avatars", "collaboration", "self-esteem", "mixed-methods"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2504.20769", "pdf": "https://arxiv.org/pdf/2504.20769.pdf", "abs": "https://arxiv.org/abs/2504.20769", "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption", "authors": ["Wenxiao Wang", "Parsa Hosseini", "Soheil Feizi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%.", "AI": {"tldr": "This paper introduces a method called chain-of-defensive-thought to enhance the robustness of large language models against reference corruption.", "motivation": "Chain-of-thought prompting has been successful in improving reasoning in large language models. This work aims to leverage these reasoning enhancements to increase the robustness of models in tasks beyond pure reasoning.", "method": "The method, chain-of-defensive-thought, involves providing a few exemplars with structured and defensive reasoning as demonstrations to large language models.", "result": "The approach significantly improves the models' robustness against reference corruption, as demonstrated by maintaining a high accuracy in the Natural Questions task despite reference corruption.", "conclusion": "Chain-of-defensive-thought is a simple yet effective method that can be broadly applied to enhance the performance and robustness of large language models.", "key_contributions": ["Introduction of the chain-of-defensive-thought method.", "Demonstrated empirical improvements in model robustness across diverse tasks.", "Revealed significant accuracy retention in the presence of prompt injection attacks."], "limitations": "", "future_work": "Further exploration of the applicability of chain-of-defensive-thought across various tasks and models.", "keywords": ["chain-of-thought", "robustness", "large language models", "reference corruption", "natural questions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.14634", "pdf": "https://arxiv.org/pdf/2409.14634.pdf", "abs": "https://arxiv.org/abs/2409.14634", "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination", "authors": ["Marissa Radensky", "Simra Shahid", "Raymond Fok", "Pao Siangliulue", "Tom Hope", "Daniel S. Weld"], "categories": ["cs.HC", "cs.AI", "H.5.2, I.2"], "comment": "Updated with new and improved user study", "summary": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas, and facet-based ideation is an established\nframework for idea generation. To see how large language models (LLMs) might\nassist in this process, we contribute a novel mixed-initiative ideation tool\ncalled Scideator. Starting from a user-provided set of scientific papers,\nScideator extracts key facets -- purposes, mechanisms, and evaluations -- from\nthese and related papers, allowing users to explore the idea space by\ninteractively recombining facets to synthesize inventive ideas. Scideator also\nhelps users gauge idea originality by searching the literature for overlaps,\nassessing idea novelty and providing explanations. To support these tasks,\nScideator introduces three LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty\nChecker. In a within-subjects user study (N=22) with computer-science\nresearchers comparing Scideator to a strong baseline, our tool provided\nsignificantly more creativity support, particularly with respect to\nexploration, which participants considered the most important factor for idea\ngeneration.", "AI": {"tldr": "A novel ideation tool, Scideator, blends facets from existing scientific papers to assist in creativity and idea generation using LLMs.", "motivation": "To explore how LLMs can aid in the scientific ideation process by blending existing ideas to generate new ones.", "method": "Scideator extracts key facets from user-provided scientific papers and related literature, allowing users to recombine these facets interactively. It includes LLM-powered modules for finding analogous papers, generating ideas, and checking idea novelty.", "result": "Scideator significantly enhanced creativity support in a study with computer-science researchers, particularly in exploring new ideas compared to a strong baseline.", "conclusion": "The findings suggest that user-initiated interaction with the tool leads to higher support for generating original ideas in scientific research.", "key_contributions": ["Introduction of an interactive ideation tool leveraging LLMs.", "Three innovative RAG modules to assist in ideation process.", "Empirical evidence showing improved creativity support through user studies."], "limitations": "The study had a small sample size (N=22) and may not represent the broader research community.", "future_work": "Future iterations could expand the user study and enhance the tool's capabilities based on feedback.", "keywords": ["Human-Computer Interaction", "Large Language Models", "Creativity Support", "Mixed-Initiative Systems", "Scientific Ideation"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2504.20771", "pdf": "https://arxiv.org/pdf/2504.20771.pdf", "abs": "https://arxiv.org/abs/2504.20771", "title": "Turing Machine Evaluation for Large Language Model", "authors": ["Haitao Wu", "Zongbo Han", "Huaxi Huang", "Changqing Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench.", "AI": {"tldr": "This paper presents TMBench, an evaluation framework for assessing the computational reasoning abilities of Large Language Models (LLMs) using Universal Turing Machine (UTM) simulation.", "motivation": "The paper addresses the need for rigorous evaluation of LLMs, particularly their computational reasoning abilities, crucial for complex tasks like code generation and multi-step problem-solving.", "method": "A framework based on Universal Turing Machine simulation is proposed, requiring LLMs to follow instructions and track states during computations.", "result": "TMBench shows a strong correlation (Pearson coefficient of 0.73) with other reasoning benchmarks, highlighting its effectiveness in evaluating LLMs' reasoning capabilities.", "conclusion": "Computational reasoning is a key dimension for assessing the advanced capabilities of LLMs, and TMBench provides a scalable and rigorous evaluation method.", "key_contributions": ["Development of TMBench for evaluating LLMs' computational reasoning", "Knowledge-agnostic and adjustable difficulty in evaluation", "Correlation of TMBench results with established reasoning benchmarks"], "limitations": "", "future_work": "Further enhancement and expansion of the benchmark to include more complex reasoning tasks.", "keywords": ["Large Language Models", "computational reasoning", "evaluation framework", "Turing Machine", "TMBench"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.16732", "pdf": "https://arxiv.org/pdf/2409.16732.pdf", "abs": "https://arxiv.org/abs/2409.16732", "title": "Perfectly to a Tee: Understanding User Perceptions of Personalized LLM-Enhanced Narrative Interventions", "authors": ["Ananya Bhattacharjee", "Sarah Yi Xu", "Pranav Rao", "Yuchen Zeng", "Jonah Meyerhoff", "Syed Ishtiaque Ahmed", "David C Mohr", "Michael Liut", "Alex Mariakakis", "Rachel Kornfield", "Joseph Jay Williams"], "categories": ["cs.HC"], "comment": null, "summary": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that personalized\nLLM-enhanced stories were perceived to be better than human-written ones in\nconveying key takeaways, promoting reflection, and reducing belief in negative\nthoughts. These stories were not only seen as more relatable but also similarly\nauthentic to human-written ones, highlighting the potential of LLMs in helping\nyoung adults manage their struggles. The findings of this work provide crucial\ndesign considerations for future narrative-based digital mental health\ninterventions, such as the need to maintain relatability without veering into\nimplausibility and refining the wording and tone of AI-enhanced content.", "AI": {"tldr": "The study explores how large language models can generate personalized narratives to help young adults manage personal struggles, finding these AI-generated stories more relatable and effective than human-written ones.", "motivation": "To illustrate the application of psychological theories through personalized narratives that resonate with individuals' experiences.", "method": "Employing large language models (LLMs) to create tailored stories for 346 young adults in different settings.", "result": "Personalized LLM-enhanced stories were perceived as superior to human-written narratives in terms of relatability, promoting reflection, and reducing negative thoughts.", "conclusion": "LLMs have significant potential in creating effective narrative-based digital mental health interventions for young adults.", "key_contributions": ["Demonstrated effectiveness of LLMs in generating personalized narratives for mental health.", "Established the perception of LLM-generated stories as relatable and authentic.", "Provided design considerations for future narrative-based mental health interventions."], "limitations": "The study may not generalize to other demographics beyond young adults.", "future_work": "Investigate the optimal balance between relatability and plausibility in AI-generated narratives.", "keywords": ["large language models", "mental health", "narrative generation", "personalization", "digital interventions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.20839", "pdf": "https://arxiv.org/pdf/2504.20839.pdf", "abs": "https://arxiv.org/abs/2504.20839", "title": "Universal language model with the intervention of quantum theory", "authors": ["D. -F. Qin"], "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "This paper examines language modeling based on the theory of quantum\nmechanics. It focuses on the introduction of quantum mechanics into the\nsymbol-meaning pairs of language in order to build a representation model of\nnatural language. At the same time, it is realized that word embedding, which\nis widely used as a basic technique for statistical language modeling, can be\nexplained and improved by the mathematical framework of quantum mechanics. On\nthis basis, this paper continues to try to use quantum statistics and other\nrelated theories to study the mathematical representation, natural evolution\nand statistical properties of natural language. It is also assumed that the\nsource of such quantum properties is the physicality of information. The\nfeasibility of using quantum theory to model natural language is pointed out\nthrough the construction of a experimental code. The paper discusses, in terms\nof applications, the possible help of the theory in constructing generative\nmodels that are popular nowadays. A preliminary discussion of future\napplications of the theory to quantum computers is also presented.", "AI": {"tldr": "This paper explores the application of quantum mechanics in language modeling, proposing improvements to word embedding techniques through quantum principles.", "motivation": "To build a representation model of natural language that incorporates quantum mechanics and addresses the limitations of traditional language modeling techniques.", "method": "The paper introduces quantum mechanics to the symbol-meaning pairs of language and proposes improvements to word embeddings using quantum statistics and related theories.", "result": "The feasibility of using quantum theory for natural language modeling is demonstrated through experimental code, suggesting improvements in generative models and future applications in quantum computing.", "conclusion": "This research suggests a new paradigm for language modeling that integrates quantum mechanics, potentially enhancing the understanding and application of natural language processing techniques.", "key_contributions": ["Introduction of quantum mechanics into language modeling", "Improvement of word embeddings through quantum statistical methods", "Preliminary framework for future research in quantum computing applications for natural language"], "limitations": "The practical implementation of quantum theories in language processing remains exploratory and requires extensive validation.", "future_work": "Further exploration of quantum properties in information theory and their applications in quantum computing for natural language processing.", "keywords": ["quantum mechanics", "language modeling", "word embeddings", "quantum statistics", "generative models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2501.01397", "pdf": "https://arxiv.org/pdf/2501.01397.pdf", "abs": "https://arxiv.org/abs/2501.01397", "title": "WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI", "authors": ["Wesley Hanwen Deng", "Wang Claire", "Howard Ziyu Han", "Jason I. Hong", "Kenneth Holstein", "Motahhare Eslami"], "categories": ["cs.HC"], "comment": null, "summary": "There has been growing interest from both practitioners and researchers in\nengaging end users in AI auditing, to draw upon users' unique knowledge and\nlived experiences. However, we know little about how to effectively scaffold\nend users in auditing in ways that can generate actionable insights for AI\npractitioners. Through formative studies with both users and AI practitioners,\nwe first identified a set of design goals to support user-engaged AI auditing.\nWe then developed WeAudit, a workflow and system that supports end users in\nauditing AI both individually and collectively. We evaluated WeAudit through a\nthree-week user study with user auditors and interviews with industry\nGenerative AI practitioners. Our findings offer insights into how WeAudit\nsupports users in noticing and reflecting upon potential AI harms and in\narticulating their findings in ways that industry practitioners can act upon.\nBased on our observations and feedback from both users and practitioners, we\nidentify several opportunities to better support user engagement in AI auditing\nprocesses. We discuss implications for future research to support effective and\nresponsible user engagement in AI auditing and red-teaming.", "AI": {"tldr": "The paper presents WeAudit, a system designed to engage end users in AI auditing, enhancing actionable insights for practitioners through a structured workflow.", "motivation": "There is a growing need to involve end users in AI auditing to leverage their unique insights and experiences, which could lead to better AI practices.", "method": "The authors conducted formative studies with users and AI practitioners to establish design goals, developed the WeAudit system, and evaluated it through a three-week user study and interviews.", "result": "WeAudit effectively helps users identify potential AI harms and communicate their findings to industry practitioners, facilitating better AI auditing processes.", "conclusion": "The study reveals critical opportunities for improving user engagement in AI auditing and emphasizes the need for responsible practices in this field.", "key_contributions": ["Introduction of WeAudit as a user-engaged AI auditing system", "Insights from user studies on AI auditing practices", "Identified opportunities for enhancing user involvement in AI auditing"], "limitations": "The study's findings are based on a limited user group and specific contexts, which may not generalize to all scenarios.", "future_work": "Further research is needed to explore scalable methods for user engagement in AI auditing and the implications of findings across different industries.", "keywords": ["AI auditing", "user engagement", "WeAudit", "machine learning", "ethical AI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.20849", "pdf": "https://arxiv.org/pdf/2504.20849.pdf", "abs": "https://arxiv.org/abs/2504.20849", "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry", "authors": ["Anum Afzal", "Alexandre Mercier", "Florian Matthes"], "categories": ["cs.CL"], "comment": null, "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent.", "AI": {"tldr": "This paper explores LLM-based data-to-text methods for generating diverse marketing texts, addressing issues of repetitiveness in automated content generation.", "motivation": "To tackle the problem of monotony in generated content through traditional methods, enabling more effective marketing text creation.", "method": "Utilizing Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 with fine-tuning, few-shot, and zero-shot techniques, while proposing a new metric, JaccDiv, to measure text diversity.", "result": "Demonstrated the ability to generate higher quality and more diverse marketing texts, establishing a baseline for future applications.", "conclusion": "The findings suggest that LLMs can produce better outcomes for marketing text generation across various sectors, reducing repetitive patterns.", "key_contributions": ["Introduction of JaccDiv metric for evaluating text diversity.", "Application of multiple LLMs for improving marketing text generation.", "Demonstration of LLM effectiveness beyond music, applicable in diverse fields."], "limitations": "Focus primarily on marketing texts; further research needed to expand applicability to other forms of content.", "future_work": "Exploration of additional applications of the proposed methods in different fields and further refinement of the JaccDiv metric.", "keywords": ["Data-to-Text", "Generative Models", "Marketing Texts", "Diversity Evaluation", "Language Models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.20922", "pdf": "https://arxiv.org/pdf/2504.20922.pdf", "abs": "https://arxiv.org/abs/2504.20922", "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures", "authors": ["Miguel Nogales", "Matteo Gambella", "Manuel Roveri"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Accepted to IJCNN 2025", "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs.", "AI": {"tldr": "DYNAMAX introduces a framework for early exits (EEs) in Mamba architectures, optimizing computational efficiency in LLMs and demonstrating their effectiveness across various NLP tasks.", "motivation": "To explore the integration of early exits in Mamba architectures, which remain underutilized compared to encoder-only Transformers, and to enhance computational efficiency in large language models (LLMs).", "method": "The study presents DYNAMAX, a framework that incorporates early exit mechanisms into Mamba architectures and transforms Mamba into an efficient classifier for EEs on both Mamba and transformer-based models, evaluated using multiple NLP datasets.", "result": "Experiments show that Mamba can efficiently serve as an early exit classifier, achieving a balance between computational cost savings and performance consistency across various NLP tasks.", "conclusion": "The adaptation of Mamba for early exits can significantly enhance computational efficiency and dynamic processing in LLMs, suggesting new avenues for research and application, especially in resource-limited environments.", "key_contributions": ["Introduction of DYNAMAX framework for early exits in Mamba architectures.", "Demonstration of Mamba as an efficient EE classifier for different LLMs.", "Empirical results showcasing the computational savings and performance of Mamba in various NLP tasks."], "limitations": "", "future_work": "Further exploration of Mamba's capabilities in varied computational settings and extending the framework's applicability to additional models and tasks.", "keywords": ["early exits", "Mamba architectures", "computational efficiency", "large language models", "dynamic processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20946", "pdf": "https://arxiv.org/pdf/2504.20946.pdf", "abs": "https://arxiv.org/abs/2504.20946", "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models", "authors": ["Tyler McDonald", "Ali Emami"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications.", "AI": {"tldr": "A new prompt engineering method, Trace-of-Thought Prompting, is introduced for enhancing arithmetic reasoning in open-source LLMs under 7 billion parameters, showing significant performance gains.", "motivation": "To improve the efficiency and adaptability of prompt engineering in LLMs for specialized tasks like arithmetic reasoning, especially using open-source models.", "method": "The paper proposes Trace-of-Thought Prompting, a zero-shot prompting technique that encourages LLMs to break down problems into observable subproblems to enhance reasoning.", "result": "The use of Trace-of-Thought Prompting led to performance improvements of up to 125% in language models with 7 billion parameters or fewer, demonstrating the effectiveness of this new approach.", "conclusion": "The approach highlights the utility of open-source LLMs in making AI research more accessible and effective, especially in computational linguistics tasks.", "key_contributions": ["Introduction of Trace-of-Thought Prompting for LLMs.", "Demonstration of significant performance gain (up to 125%) in smaller open-source models.", "Highlighting the advantages of open-source models over proprietary ones in research scalability."], "limitations": "", "future_work": "Exploration of further enhancements in prompt engineering techniques for various computational tasks using open-source models.", "keywords": ["prompt engineering", "Large Language Models", "open-source", "arithmetic reasoning", "computational linguistics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.20951", "pdf": "https://arxiv.org/pdf/2504.20951.pdf", "abs": "https://arxiv.org/abs/2504.20951", "title": "Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models", "authors": ["Maryna Vyshnyvetska"], "categories": ["cs.CL"], "comment": "12 pages, 1 figure", "summary": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity.", "AI": {"tldr": "The paper introduces a theoretical model called 'information gravity' to explain text generation in large language models (LLMs), leveraging concepts from field theory and geometry.", "motivation": "To develop a formal framework that elucidates the interaction between user queries and token generation in LLMs, addressing complex behaviors like hallucinations and variability in output.", "method": "The model conceptualizes user queries as objects with 'information mass' that influence the semantic space of LLMs, analogous to gravitational forces, thereby shaping the probabilities of token generation.", "result": "The model successfully explains several LLM behaviors, including the occurrence of hallucinations, sensitivity to how queries are phrased, and how sampling temperature affects the diversity of outputs.", "conclusion": "The 'information gravity' framework offers valuable insights into the underlying mechanisms of LLMs, potentially guiding future improvements in their design and utility.", "key_contributions": ["Introduction of the 'information gravity' model for LLM text generation", "Linking the behavior of LLMs to concepts from physics and geometry", "Providing explanations for specific LLM phenomena, such as hallucinations and query sensitivity"], "limitations": "The theoretical nature of the model requires empirical validation and may not account for all LLM behaviors.", "future_work": "Further research is needed to empirically test the model and refine its predictions based on real-world usage and performance.", "keywords": ["information gravity", "large language models", "text generation", "semantic space", "hallucinations"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2403.14562", "pdf": "https://arxiv.org/pdf/2403.14562.pdf", "abs": "https://arxiv.org/abs/2403.14562", "title": "Agentic AI: The Era of Semantic Decoding", "authors": ["Maxime Peyrard", "Martin Josifoski", "Robert West"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": "25 pages, 3 figures", "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.", "AI": {"tldr": "This paper introduces semantic decoding, a perspective on orchestrating collaborations between LLMs, human input, and tools, framing them as optimization procedures in semantic space.", "motivation": "To address the inherent limitations of LLMs by optimizing collaborative processes involving LLMs, human input, and various tools.", "method": "Conceptualizes LLMs as semantic processors that exchange semantic tokens to construct high-utility outputs, formalizing the transition from syntactic to semantic tokens.", "result": "Offers a new framework for AI systems that can manage greater complexity and capabilities by focusing on semantic rather than syntactic decoding.", "conclusion": "Identifies research opportunities and questions stemming from the semantic decoding perspective, suggesting a functional abstraction for computation based on meaningful concepts.", "key_contributions": ["Introduction of the semantic decoding framework", "Formalization of the transition from syntactic to semantic tokens", "Identification of new research opportunities in the field of AI optimization."], "limitations": "", "future_work": "Exploration of research questions and opportunities that arise from the semantic decoding perspective.", "keywords": ["semantic decoding", "LLMs", "collaboration", "semantic tokens", "AI systems"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2504.20964", "pdf": "https://arxiv.org/pdf/2504.20964.pdf", "abs": "https://arxiv.org/abs/2504.20964", "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification", "authors": ["Shangyu Li", "Juyong Jiang", "Tiancheng Zhao", "Jiasi Shen"], "categories": ["cs.CL", "cs.AI", "cs.OS", "cs.PL", "cs.SE"], "comment": null, "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench.", "AI": {"tldr": "OSVBench is a new benchmark designed to evaluate Large Language Models in generating specification code for operating system kernel verification tasks, demonstrating their performance limitations.", "motivation": "To create a standard benchmark for assessing the abilities of LLMs in generating accurate specification code required for verifying operating system kernels.", "method": "The benchmark reformulates the specification generation problem as a program synthesis task, offering LLMs a programming model along with specific verification assumptions and tasks, based on the Hyperkernel operating system.", "result": "Evaluation of 12 different LLMs showed that they struggled significantly with the specification generation tasks, particularly in handling long-context code generation of 20k-30k tokens.", "conclusion": "The benchmark reveals important performance disparities among LLMs and provides a useful tool for future research in LLMs' applications in program synthesis.", "key_contributions": ["Introduction of OSVBench benchmark for LLMs", "Evaluation of LLMs on a real-world operating system kernel", "Insights into LLM performance limitations in long-context tasks"], "limitations": "Limited performance of current LLMs on specification generation tasks; requires further refinement and testing.", "future_work": "Investigating ways to improve LLM performance on long-context specification generation tasks and expanding the benchmark.", "keywords": ["Large Language Models", "Operating System Verification", "Program Synthesis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20972", "pdf": "https://arxiv.org/pdf/2504.20972.pdf", "abs": "https://arxiv.org/abs/2504.20972", "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap", "authors": ["Yifan Wei", "Xiaoyan Yu", "Ran Song", "Hao Peng", "Angsheng Li"], "categories": ["cs.CL"], "comment": "The CR version will be updated subsequently", "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark.", "AI": {"tldr": "The paper proposes Knowledge Set Editing (KSE) and the SetKE method to address the Knowledge Element Overlap (KEO) phenomenon in Knowledge Editing, demonstrating improved performance with a new dataset, EditSet.", "motivation": "To enhance the effectiveness of Knowledge Editing in Large Language Models by addressing the challenges posed by Knowledge Element Overlap, which can lead to performance degradation during knowledge updates.", "method": "The authors propose a new approach called Knowledge Set Editing (KSE) that allows for the simultaneous editing of sets of triplets to better manage Knowledge Element Overlap (KEO) issues in existing datasets and editing methods.", "result": "Experimental results indicate that SetKE outperforms existing Knowledge Editing methods when dealing with KEO scenarios, showcasing its robustness in maintaining performance during knowledge updates.", "conclusion": "SetKE represents a significant advancement in Knowledge Editing for Large Language Models, providing a more efficient alternative that mitigates the impact of Knowledge Element Overlap.", "key_contributions": ["Introduction of Knowledge Set Editing (KSE) framework", "Development of SetKE method for simultaneous triplet editing", "Creation of EditSet dataset for benchmarking KEO scenarios"], "limitations": "The paper primarily focuses on KEO within knowledge editing and may not address other potential challenges in LLM updates.", "future_work": "Further exploration of KEO's effects on various knowledge editing techniques and applications in other domains.", "keywords": ["Knowledge Editing", "Large Language Models", "Knowledge Element Overlap", "SetKE", "EditSet"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799.pdf", "abs": "https://arxiv.org/abs/2504.00799", "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "authors": ["Xi Wang", "Fanfei Meng", "Shiyang Zhang", "Lan Li"], "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries.", "AI": {"tldr": "This study evaluates the accuracy of the Youdao electronic dictionary used by L2 learners in China, revealing issues in definitions and user consultation habits.", "motivation": "Investigating the reliability of electronic dictionaries which are vital tools for language learners, as their definitions are often taken for granted.", "method": "A combined approach of experimentation, user surveys, and dictionary critique, focusing on a translation task where participants engaged with Youdao.", "result": "Incomplete or misleading definitions led to serious misunderstandings among users, alongside problematic consultation behaviors.", "conclusion": "The study indicates a critical need for enhanced dictionary literacy training and improvements in AI models for E-dictionary construction.", "key_contributions": ["Identification of inaccuracies in electronic dictionary definitions", "Analysis of user consultation behavior", "Recommendations for improving AI-driven dictionary models"], "limitations": "Limited to Youdao as a case study, results may not generalize to other E-dictionaries.", "future_work": "Further research on a broader range of electronic dictionaries and analyses of user interactions and dictionary literacy efforts.", "keywords": ["Electronic Dictionaries", "Machine Learning", "Dictionary Literacy", "Language Learning", "AI Models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2308.09138", "pdf": "https://arxiv.org/pdf/2308.09138.pdf", "abs": "https://arxiv.org/abs/2308.09138", "title": "Semantic Consistency for Assuring Reliability of Large Language Models", "authors": ["Harsh Raj", "Vipul Gupta", "Domenic Rosati", "Subhabrata Majumdar"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "An updated version of this preprint is available at arXiv:2502.15924,\n  and has been accepted at the Transactions on Machine Learning Research", "summary": "Large Language Models (LLMs) exhibit remarkable fluency and competence across\nvarious natural language tasks. However, recent research has highlighted their\nsensitivity to variations in input prompts. To deploy LLMs in a safe and\nreliable manner, it is crucial for their outputs to be consistent when prompted\nwith expressions that carry the same meaning or intent. While some existing\nwork has explored how state-of-the-art LLMs address this issue, their\nevaluations have been confined to assessing lexical equality of single- or\nmulti-word answers, overlooking the consistency of generative text sequences.\nFor a more comprehensive understanding of the consistency of LLMs in open-ended\ntext generation scenarios, we introduce a general measure of semantic\nconsistency, and formulate multiple versions of this metric to evaluate the\nperformance of various LLMs. Our proposal demonstrates significantly higher\nconsistency and stronger correlation with human evaluations of output\nconsistency than traditional metrics based on lexical consistency. Finally, we\npropose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance\nsemantic consistency. When evaluated for closed-book question answering based\non answer variations from the TruthfulQA benchmark, A2C increases accuracy\nmetrics for pretrained and finetuned LLMs by up to 47%, and semantic\nconsistency metrics for instruction-tuned models by up to 7-fold.", "AI": {"tldr": "This paper introduces a measure of semantic consistency for LLM outputs and presents a prompting strategy to improve their consistency in open-ended text generation tasks.", "motivation": "The need for reliable output consistency from Large Language Models when prompted with semantically equivalent expressions.", "method": "Developing a measure of semantic consistency and a novel prompting strategy called Ask-to-Choose (A2C) for evaluating and enhancing LLM performance.", "result": "A2C improves accuracy metrics for LLMs by up to 47% and enhances semantic consistency metrics by up to 7-fold compared to traditional metrics.", "conclusion": "The proposed methods provide a more robust evaluation of LLM outputs and significant improvements in consistency and accuracy for generative tasks.", "key_contributions": ["Introduction of a general measure of semantic consistency for LLMs.", "Development of the Ask-to-Choose (A2C) prompting strategy.", "A2C's demonstrated effectiveness in improving LLM performance metrics."], "limitations": "The study focuses on specific metrics and may not encompass all aspects of LLM performance in diverse contexts.", "future_work": "Exploration of additional prompting strategies and further refinements to the consistency measure in varied language tasks.", "keywords": ["Large Language Models", "semantic consistency", "prompting strategy", "natural language generation", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2310.03903", "pdf": "https://arxiv.org/pdf/2310.03903.pdf", "abs": "https://arxiv.org/abs/2310.03903", "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models", "authors": ["Saaket Agashe", "Yue Fan", "Anthony Reyna", "Xin Eric Wang"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated emergent common-sense\nreasoning and Theory of Mind (ToM) capabilities, making them promising\ncandidates for developing coordination agents. This study introduces the\nLLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context\nof Pure Coordination Settings, where agents must cooperate to maximize gains.\nOur benchmark evaluates LLMs through two distinct tasks. The first is Agentic\nCoordination, where LLMs act as proactive participants in four pure\ncoordination games. The second is Coordination Question Answering (CoordQA),\nwhich tests LLMs on 198 multiple-choice questions across these games to\nevaluate three key abilities: Environment Comprehension, ToM Reasoning, and\nJoint Planning. Results from Agentic Coordination experiments reveal that\nLLM-Agents excel in multi-agent coordination settings where decision-making\nprimarily relies on environmental variables but face challenges in scenarios\nrequiring active consideration of partners' beliefs and intentions. The CoordQA\nexperiments further highlight significant room for improvement in LLMs' Theory\nof Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC)\nexperiments in the Agentic Coordination setting demonstrate that LLM agents,\nunlike RL methods, exhibit robustness to unseen partners. These findings\nindicate the potential of LLMs as Agents in pure coordination setups and\nunderscore areas for improvement. Code Available at\nhttps://github.com/eric-ai-lab/llm_coordination.", "AI": {"tldr": "This study presents the LLM-Coordination Benchmark to analyze LLMs in pure coordination settings, focusing on their emergent common-sense reasoning and Theory of Mind capabilities.", "motivation": "The motivation is to evaluate how large language models can function as agents in coordination scenarios, maximizing gains through cooperation.", "method": "The benchmark includes two tasks: Agentic Coordination, where LLMs engage in pure coordination games, and Coordination Question Answering (CoordQA), which tests their reasoning and planning capabilities through multiple-choice questions.", "result": "Results show that while LLMs perform well in environments based mainly on variables, they struggle with reasoning about partners' beliefs and intentions. There is significant room for improvement in their Theory of Mind reasoning and joint planning skills.", "conclusion": "The findings suggest that LLMs have potential as agents in coordination setups; however, they require further development in specific reasoning areas.", "key_contributions": ["Introduction of the LLM-Coordination Benchmark", "Identification of LLM capabilities in Agentic Coordination tasks", "Highlighting gaps in Theory of Mind reasoning and joint planning"], "limitations": "The study primarily focuses on specific coordination tasks, which may limit the generalizability of the findings to broader contexts.", "future_work": "Future research could explore advanced strategies for improving LLMs' reasoning about partners' beliefs and intentions in coordination tasks.", "keywords": ["Large Language Models", "Theory of Mind", "Coordination Games", "Agentic Coordination", "AI Agents"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2310.12059", "pdf": "https://arxiv.org/pdf/2310.12059.pdf", "abs": "https://arxiv.org/abs/2310.12059", "title": "Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education", "authors": ["Duc-Vu Nguyen", "Quoc-Nam Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at SoICT 2023", "summary": "In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only.", "AI": {"tldr": "This paper evaluates large language models' performance on multiple choice question answering tasks in Vietnamese, introducing a novel dataset for assessing MCSB ability.", "motivation": "To assess the performance of LLMs on MCQA tasks in Vietnamese, which has limited datasets compared to English, and to create a high-quality dataset focused on various subjects.", "method": "The paper evaluates LLMs in zero-shot, one-shot, and few-shot settings using the proposed structured dataset that incorporates LaTeX typing for subjects like mathematics and sciences.", "result": "The evaluation of LLMs like BLOOMZ, LLaMA, and GPT models indicates promising MCSB capabilities on Vietnamese benchmarks and the newly proposed dataset.", "conclusion": "The study provides insights into the performance of LLMs in Vietnamese MCQA tasks and introduces a dataset that can further aid research in this area.", "key_contributions": ["Creation of a novel Vietnamese MCQA dataset using structured LaTeX guidelines", "Evaluation of multiple well-known LLMs on Vietnamese benchmarks", "Insights into MCSB performance of LLMs for educational subjects."], "limitations": "The focus is primarily on Vietnamese, limiting the applicability of findings to other languages or cultures; further validation needed across diverse topics.", "future_work": "Further research could explore larger datasets and evaluate LLMs in more diverse contexts or with more complex question structures.", "keywords": ["Large Language Models", "Vietnamese", "Multiple Choice Question Answering", "Dataset Creation", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2403.14562", "pdf": "https://arxiv.org/pdf/2403.14562.pdf", "abs": "https://arxiv.org/abs/2403.14562", "title": "Agentic AI: The Era of Semantic Decoding", "authors": ["Maxime Peyrard", "Martin Josifoski", "Robert West"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": "25 pages, 3 figures", "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.", "AI": {"tldr": "This paper introduces semantic decoding, a novel approach that redefines interactions among LLMs, humans, and tools as optimization processes in semantic space, proposing new algorithms for optimizing meaningful outputs.", "motivation": "To address the inherent limitations of LLMs and enhance collaborative interactions between LLMs, human input, and tools by focusing on semantic processes.", "method": "The paper conceptualizes LLMs and other semantic processors as entities that exchange semantic tokens and formulates the orchestration of their interactions as semantic decoding algorithms.", "result": "The framework emphasizes optimizing within semantic space, offering a new perspective on AI system design that transcends traditional syntactic approaches.", "conclusion": "The authors provide insights into future research directions, proposing that semantic decoding can lead to more advanced AI capabilities by focusing on meaningful concepts.", "key_contributions": ["Introduction of the semantic decoding framework for AI systems", "Formalization of the transition from syntactic to semantic tokens", "Exploration of research questions related to semantic optimization"], "limitations": "", "future_work": "Further exploration of semantic decoding algorithms and their applications in complex AI systems.", "keywords": ["semantic decoding", "LLMs", "human-computer interaction", "semantic tokens", "AI optimization"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2407.15229", "pdf": "https://arxiv.org/pdf/2407.15229.pdf", "abs": "https://arxiv.org/abs/2407.15229", "title": "A Practical Analysis of Human Alignment with *PO", "authors": ["Kian Ahrabian", "Xihui Lin", "Barun Patra", "Vishrav Chaudhary", "Alon Benhaim", "Jay Pujara", "Xia Song"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 findings papers. 9 pages, 7 figures, 3 tables", "summary": "At the forefront of state-of-the-art human alignment methods are preference\noptimization methods (*PO). Prior research has often concentrated on\nidentifying the best-performing method, typically involving a grid search over\nhyperparameters, which can be impractical for general practitioners. In this\npaper, we examine the robustness of existing state-of-the-art methods to\nvarying hyperparameters in a realistic out-of-distribution (OOD) scenario that\nmirrors real-world applications of human alignment. Our goal is to empirically\nfind the method that increases the likelihood of achieving better results\nthrough the lens of various metrics, such as KL divergence and response length.\nWe also introduce LN-DPO, a simple length-normalized version of DPO that is\nmore stable across hyperparameters, effectively reduces the average response\nlength, and improves performance. Our analysis of state-of-the-art\nreference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO)\nmethods reveals that they perform similarly at their peak (i.e., best possible\nscenario). However, we uncover that the pattern of change in performance\ngreatly varies as we move away from the best possible scenario.", "AI": {"tldr": "This paper examines the robustness of human alignment methods under varying hyperparameters in out-of-distribution scenarios, introducing LN-DPO to enhance performance and stability.", "motivation": "To empirically investigate the robustness of state-of-the-art human alignment methods in real-world applications and identify strategies to improve their performance across varying hyperparameters.", "method": "The authors analyze existing preference optimization methods, particularly in OOD settings, using metrics like KL divergence and response length, and introduce LN-DPO for better stability and performance.", "result": "The introduction of LN-DPO leads to more stable performance across hyperparameters, reduces average response length, and maintains competitive results with other state-of-the-art methods.", "conclusion": "The study reveals that while top methods perform similarly at their best, their performance can vary significantly as conditions change, underlining the importance of robustness in human alignment methods.", "key_contributions": ["Identification of robustness issues in existing human alignment methods under varying hyperparameters.", "Introduction of LN-DPO, a length-normalized method that improves stability and response performance.", "Analysis of performance patterns in OOD scenarios for reference-free and reference-dependent methods."], "limitations": "The focus on certain metrics may not encompass all aspects of human alignment efficacy in practice.", "future_work": "Further exploration of other metrics and real-world applications to enhance the robustness and applicability of these methods.", "keywords": ["Human Alignment", "Preference Optimization", "Length-normalized DPO", "Out-of-distribution", "Performance Robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.00137", "pdf": "https://arxiv.org/pdf/2408.00137.pdf", "abs": "https://arxiv.org/abs/2408.00137", "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment", "authors": ["Sangwon Yu", "Jongyoon Song", "Bongkyu Hwang", "Hoyoung Kang", "Sooah Cho", "Junhwa Choi", "Seongho Joe", "Taehee Lee", "Youngjune L. Gwon", "Sungroh Yoon"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025 Oral", "summary": "A binary decision task, like yes-no questions or answer verification,\nreflects a significant real-world scenario such as where users look for\nconfirmation about the correctness of their decisions on specific issues. In\nthis work, we observe that language models exhibit a negative bias in the\nbinary decisions of complex reasoning tasks. Based on our observations and the\nrationale about attention-based model dynamics, we propose a negative attention\nscore (NAS) to systematically and quantitatively formulate negative bias. Based\non NAS, we identify attention heads that attend to negative tokens provided in\nthe instructions as answer candidate of binary decisions, regardless of the\nquestion in the prompt, and validate their association with the negative bias.\nAdditionally, we propose the negative attention score alignment (NASA) method,\nwhich is a parameter-efficient fine-tuning technique to address the extracted\nnegatively biased attention heads. Experimental results from various domains of\nreasoning tasks and large model search space demonstrate that NASA\nsignificantly reduces the gap between precision and recall caused by negative\nbias while preserving their generalization abilities.", "AI": {"tldr": "The paper investigates the negative bias in language models' binary decision making and proposes a method to address this issue.", "motivation": "Binary decision tasks are common in user interactions, and understanding negative biases in language models can improve their performance in real-world applications.", "method": "The authors propose a negative attention score (NAS) to quantify negative bias and use it to identify attention heads that focus on negative tokens. They also introduce the negative attention score alignment (NASA) method for fine-tuning these heads.", "result": "Experimental validation shows that NASA significantly improves precision and recall in reasoning tasks without sacrificing generalization capabilities.", "conclusion": "Addressing negative bias in language models is critical for enhancing their decision-making performance, and NAS and NASA provide effective solutions to this problem.", "key_contributions": ["Introduction of the negative attention score (NAS) for quantifying negative bias in language models.", "Development of the NASA fine-tuning method to mitigate negative bias in attention heads.", "Empirical demonstration of NASA's effectiveness in improving reasoning task performance."], "limitations": "", "future_work": "Further exploration of negative bias across different model architectures and extending the application of NAS and NASA to other types of tasks.", "keywords": ["negative bias", "language models", "attention heads", "fine-tuning", "reasoning tasks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2409.07394", "pdf": "https://arxiv.org/pdf/2409.07394.pdf", "abs": "https://arxiv.org/abs/2409.07394", "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL"], "comment": "NAACL 2025 (main conference), Code:\n  https://github.com/HanNight/AdaCAD", "summary": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Across four LLMs, six question-answering (QA) and three\nsummarization datasets, we demonstrate that ADACAD consistently outperforms\nother decoding baselines with average QA accuracy gains of 14.21% (absolute)\nover a static contrastive baseline, and improves the factuality of summaries by\n6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt\nperformance when conflict is absent, ADACAD mitigates these losses, making it\nmore applicable to real-world datasets in which some examples have conflict and\nothers do not.", "AI": {"tldr": "The paper introduces AdaCAD, a fine-grained instance-level approach that improves LLM performance in the presence of knowledge conflict by dynamically adjusting weights based on the degree of conflict detected.", "motivation": "To address the limitations of existing contrastive methods that fail to account for varying degrees of knowledge conflict in LLM outputs.", "method": "AdaCAD uses Jensen-Shannon divergence to measure the conflict between the contextual and parametric knowledge distributions, adjusting outputs accordingly.", "result": "AdaCAD achieved an average QA accuracy gain of 14.21% over static contrastive baselines and improved summarization factuality by 6.19 in AlignScore, outperforming previous methods on diverse datasets.", "conclusion": "The approach effectively mitigates performance drops in scenarios without conflict, making it well-suited for real-world applications with mixed conflict levels in LLM outputs.", "key_contributions": ["Introduction of AdaCAD for dynamic conflict adjustment in LLMs.", "Demonstrated substantial performance improvements on QA and summarization tasks.", "Mitigates performance issues in absence of knowledge conflict."], "limitations": "Performance may vary across different types of tasks and LLMs, necessitating further evaluation in broader contexts.", "future_work": "Exploring adaptations of AdaCAD for more LLM architectures and varied NLP tasks.", "keywords": ["Knowledge conflict", "AdaCAD", "LLM performance", "Dynamic adjustment", "Contrastive methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.02102", "pdf": "https://arxiv.org/pdf/2410.02102.pdf", "abs": "https://arxiv.org/abs/2410.02102", "title": "Racing Thoughts: Explaining Contextualization Errors in Large Language Models", "authors": ["Michael A. Lepori", "Michael C. Mozer", "Asma Ghandeharioun"], "categories": ["cs.CL"], "comment": null, "summary": "The profound success of transformer-based language models can largely be\nattributed to their ability to integrate relevant contextual information from\nan input sequence in order to generate a response or complete a task. However,\nwe know very little about the algorithms that a model employs to implement this\ncapability, nor do we understand their failure modes. For example, given the\nprompt \"John is going fishing, so he walks over to the bank. Can he make an ATM\ntransaction?\", a model may incorrectly respond \"Yes\" if it has not properly\ncontextualized \"bank\" as a geographical feature, rather than a financial\ninstitution. We propose the LLM Race Conditions Hypothesis as an explanation of\ncontextualization errors of this form. This hypothesis identifies dependencies\nbetween tokens (e.g., \"bank\" must be properly contextualized before the final\ntoken, \"?\", integrates information from \"bank\"), and claims that\ncontextualization errors are a result of violating these dependencies. Using a\nvariety of techniques from mechanistic intepretability, we provide\ncorrelational and causal evidence in support of the hypothesis, and suggest\ninference-time interventions to address it.", "AI": {"tldr": "The paper proposes the LLM Race Conditions Hypothesis to explain errors in contextualization within transformer-based language models, suggesting that these errors stem from violating token dependencies.", "motivation": "Understanding how transformer-based models manage contextual information is crucial for improving their performance and reliability.", "method": "The authors use mechanistic interpretability techniques to analyze token dependencies and provide both correlational and causal evidence for their hypothesis.", "result": "The study shows that contextualization errors arise when dependencies between tokens are not properly managed, leading to incorrect model responses.", "conclusion": "Inference-time interventions can be used to address the violations of token dependencies and enhance model accuracy.", "key_contributions": ["Introduction of the LLM Race Conditions Hypothesis for contextualization errors.", "Evidence supporting the role of token dependencies in model performance.", "Proposed interventions to mitigate errors during inference."], "limitations": "The hypothesis needs further empirical validation across various model architectures and tasks.", "future_work": "Further studies to explore the applicability of the hypothesis to different language models and contexts, as well as the development of more effective interventions.", "keywords": ["transformer-based models", "contextualization", "token dependencies", "mechanistic interpretability", "inference-time interventions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.07103", "pdf": "https://arxiv.org/pdf/2410.07103.pdf", "abs": "https://arxiv.org/abs/2410.07103", "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context", "authors": ["Sangwon Yu", "Ik-hwan Kim", "Jongyoon Song", "Saehyung Lee", "Junsung Park", "Sungroh Yoon"], "categories": ["cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the absolute position\nof supporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order,\nrelative position, in which the supporting documents are presented. We refer to\nthis as the misordered context problem. To address this issue, based on the\ntheoretical approach, we propose a simple yet effective method called context\nrepetition (CoRe), which involves prompting the model by repeatedly presenting\nthe context. This ensures that certain contiguous reasoning segments within\nsupporting documents are presented in the optimal order, effectively guiding\nthe model's reasoning in the appropriate direction. Applying CoRe, we improve\nthe F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to\n70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.", "AI": {"tldr": "This paper addresses the misordered context problem in multi-hop reasoning for LLMs by proposing a method called context repetition (CoRe), improving performance on QA tasks.", "motivation": "Multi-hop reasoning is challenging for LLMs, especially regarding the sensitivity to the order and position of supporting documents.", "method": "The authors propose CoRe, a method that involves repeatedly presenting context to ensure reasoning segments are optimally ordered.", "result": "CoRe improves the F1 score by up to 30% on multi-hop QA tasks and increases accuracy by up to 70% on a synthetic task.", "conclusion": "The CoRe method effectively guides a model's reasoning and mitigates the lost-in-the-middle problem while being compatible with retrieval-based Chain-of-Thought reasoning.", "key_contributions": ["Introduction of the misordered context problem in LLMs", "Development of the context repetition (CoRe) method", "Demonstrated significant performance improvements on multi-hop QA tasks"], "limitations": "The study may not address all scenarios in multi-hop reasoning or generalize across different domains.", "future_work": "Future research could explore the long-term impacts of CoRe across various tasks and deeper integration with retrieval approaches.", "keywords": ["multi-hop reasoning", "large language models", "context repetition"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.23463", "pdf": "https://arxiv.org/pdf/2410.23463.pdf", "abs": "https://arxiv.org/abs/2410.23463", "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following", "authors": ["Gabrielle Kaili-May Liu", "Bowen Shi", "Avi Caciularu", "Idan Szpektor", "Arman Cohan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nunique difficulties, including management of inter-document dependencies,\nredundancy, and incoherent structures. To address this challenge, we introduce\nMDCure, a scalable and effective instruction data generation framework to\nenhance the MD capabilities of LLMs without the computational cost of\npre-training or reliance on human-annotated data. MDCure generates high-quality\nsynthetic MD instruction data over sets of articles via targeted prompts. We\nalso introduce MDCureRM, a cost-effective, MD-specific reward model to score\nand filter generated data based on their training utility for MD settings.\nMDCure is compatible with open- and closed-source models in addition to policy\noptimization methods such as PPO, enabling even small open-source models to\nsurpass proprietary LLMs as strong generators of high-quality MD instruction\ndata without further data filtering. With MDCure, we fine-tune a wide variety\nof LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model\nfamilies. Extensive evaluations on a wide range of MD and long-context\nbenchmarks spanning various tasks and domains show MDCure consistently improves\nperformance over pre-trained baselines and base models by up to 75.1%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure.", "AI": {"tldr": "MDCure is an instruction data generation framework aimed at enhancing multi-document (MD) processing capabilities of LLMs through synthetic data generation.", "motivation": "To improve the ability of LLMs to handle multi-document contexts, addressing unique challenges like inter-document dependencies and redundancy.", "method": "MDCure generates high-quality synthetic MD instruction data using targeted prompts and incorporates a reward model called MDCureRM to score and filter the data based on its utility.", "result": "MDCure enables fine-tuning of various LLMs, with up to 75.1% improvement in performance on multi-document and long-context benchmarks compared to pre-trained baselines.", "conclusion": "MDCure significantly enhances the MD instruction data generation without the need for extensive pre-training or human annotations, making it accessible for both closed and open-source models.", "key_contributions": ["Introduction of MDCure framework for synthetic MD instruction data generation.", "Development of MDCureRM, a cost-effective MD-specific reward model.", "Demonstrated capability to improve LLM performance on various MD tasks by significant margins."], "limitations": "", "future_work": "", "keywords": ["multi-document processing", "instruction generation", "reward model"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.24175", "pdf": "https://arxiv.org/pdf/2410.24175.pdf", "abs": "https://arxiv.org/abs/2410.24175", "title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.", "AI": {"tldr": "This paper introduces a new method for generating high-quality instruction-response pairs for large language models (LLMs) using a technique called constraint back-translation, improving their ability to follow complex instructions.", "motivation": "LLMs struggle with complex instruction-following due to limitations in existing instruction-tuning methods and data generation techniques.", "method": "The authors propose constraint back-translation, where high-quality instruction-response pairs are modified by advanced LLMs to include complex constraints present in responses, thereby enhancing the dataset quality.", "result": "The resulting dataset, CRAB, was shown to improve the complex instruction-following abilities of multiple backbone LLMs when used for post-training, validated through extensive benchmarks.", "conclusion": "The study demonstrates that utilizing implicitly complex constraints in existing datasets can significantly enhance the training process for LLMs, suggesting a new avenue for research in dataset generation.", "key_contributions": ["Introduction of the constraint back-translation method for dataset generation", "Creation of the CRAB dataset that improves complex instruction-following", "Demonstration of the effectiveness of CRAB on various LLMs in extensive benchmarks"], "limitations": "The method assumes that existing datasets adequately represent complex constraints and may not account for all possible instruction complexities.", "future_work": "Future research could explore broader applications of constraint back-translation in other domains or investigate further improvements to dataset quality.", "keywords": ["large language models", "instruction-following", "constraint back-translation", "dataset generation", "natural language processing"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2411.07127", "pdf": "https://arxiv.org/pdf/2411.07127.pdf", "abs": "https://arxiv.org/abs/2411.07127", "title": "Benchmarking LLMs' Judgments with No Gold Standard", "authors": ["Shengwei Xu", "Yuxuan Lu", "Grant Schoenebeck", "Yuqing Kong"], "categories": ["cs.CL", "cs.LG"], "comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)", "summary": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset.", "AI": {"tldr": "The GEM metric evaluates language generation by LLMs without gold standard references, particularly in subjective tasks like academic peer review, demonstrating robust correlations with human scores and introducing GRE-bench for assessing LLM peer review quality.", "motivation": "To create a reliable metric for evaluating language generation by LLMs that can operate in subjective scenarios lacking clear gold standards.", "method": "GEM estimates mutual information between candidate and reference responses using a generative model, while GRE-bench evaluates LLMs based on high-quality peer review generation capabilities utilizing a dataset of continuously updated open-access research papers.", "result": "GEM shows competitive correlations with human evaluation scores and outperforms existing metrics. GRE-bench demonstrates effective assessment results of various LLMs on their peer review capabilities.", "conclusion": "GEM and GRE-bench provide robust tools for evaluating LLM performance in generation tasks without relying on traditional gold standards.", "key_contributions": ["Introduction of GEM as a new evaluation metric for LLMs.", "Development of GRE-bench for assessing LLMs in peer review generation.", "Demonstration of robustness against manipulation tactics in evaluating LLM performance."], "limitations": "", "future_work": "Further exploration of the applications of GEM in diverse generative tasks and enhancement of GRE-bench as more data becomes available.", "keywords": ["Generative Estimator for Mutual Information", "Large Language Models", "peer review", "evaluation metric", "GRE-bench"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2411.08243", "pdf": "https://arxiv.org/pdf/2411.08243.pdf", "abs": "https://arxiv.org/abs/2411.08243", "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset", "authors": ["Khaoula Chehbouni", "Jonathan Colaço Carr", "Yash More", "Jackie CK Cheung", "Golnoosh Farnadi"], "categories": ["cs.CL", "cs.CY"], "comment": "Prepared for conference submission", "summary": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs.", "AI": {"tldr": "This study audits the Helpful and Harmless (HH) dataset used in learning from human feedback (LHF) for large language models, revealing quality issues and potential safety disparities across demographic groups.", "motivation": "To assess the quality and effectiveness of human feedback in steering large language models towards safety.", "method": "Conducting manual and automated evaluations of the HH dataset, running experiments to assess its impact on model safety, and analyzing influential research citing this dataset.", "result": "Identified conceptualization failures and quality concerns in the HH dataset, which can exacerbate safety issues and lead to uneven model behaviors among different demographic groups.", "conclusion": "The findings emphasize the need for more refined and context-aware methods for ensuring safety in large language models.", "key_contributions": ["Audit of the HH dataset's content and its safety implications", "Experiments demonstrating the dataset's effect on model behavior", "Analysis of influential papers related to the dataset"], "limitations": "Focus is on a specific dataset and its immediate implications, may not cover other datasets or broader contexts.", "future_work": "Encourages the development of more nuanced approaches to LHF and safety in LLMs.", "keywords": ["Large Language Models", "Human Feedback", "Safety Mitigation", "Data Quality", "Demographic Disparities"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.09694", "pdf": "https://arxiv.org/pdf/2411.09694.pdf", "abs": "https://arxiv.org/abs/2411.09694", "title": "A Bayesian Optimization Approach to Machine Translation Reranking", "authors": ["Julius Cheng", "Maike Züfle", "Vilém Zouhar", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": "NAACL 2025 camera ready", "summary": "Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers.", "AI": {"tldr": "This paper presents a Bayesian optimization approach for reranking candidates in machine translation, significantly reducing the number of scoring evaluations needed while maintaining output quality.", "motivation": "The study addresses the growing computational cost associated with reranking in machine translation systems due to the increasing size of translation scoring models.", "method": "The authors frame reranking as a BayesOpt problem, emphasizing strategic candidate selection based on a trade-off between exploration and exploitation to achieve high-quality translations with fewer scoring evaluations.", "result": "The proposed method attains the same CometKiwi score using only 70 scoring evaluations, in contrast to 180 evaluations required by the baseline system.", "conclusion": "The use of a multi-fidelity BayesOpt setting allows for initial scoring with a less expensive, noisier proxy model, enhancing the cost-performance aspect when utilizing smaller, well-trained proxy scorers.", "key_contributions": ["Introduction of a Bayesian optimization framework for reranking in machine translation.", "Demonstration of significant reduction in required scoring evaluations while maintaining translation quality.", "Implementation of a multi-fidelity approach to optimize scoring efficiency."], "limitations": "", "future_work": "Further exploration of optimizing candidate selection strategies and enhancing the fidelity of proxy scoring models.", "keywords": ["machine translation", "Bayesian optimization", "reranking", "scoring model", "multi-fidelity"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.01563", "pdf": "https://arxiv.org/pdf/2502.01563.pdf", "abs": "https://arxiv.org/abs/2502.01563", "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding", "authors": ["Mingyu Jin", "Kai Mei", "Wujiang Xu", "Mingjie Sun", "Ruixiang Tang", "Mengnan Du", "Zirui Liu", "Yongfeng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in contextual\nknowledge understanding. In this paper, we show that these concentrated massive\nvalues consistently emerge in specific regions of attention queries (Q) and\nkeys (K) while not having such patterns in values (V) in various modern\ntransformer-based LLMs (Q, K, and V mean the representations output by the\nquery, key, and value layers respectively). Through extensive experiments, we\nfurther demonstrate that these massive values play a critical role in\ninterpreting contextual knowledge (knowledge obtained from the current context\nwindow) rather than in retrieving parametric knowledge stored within the\nmodel's parameters. Our further investigation of quantization strategies\nreveals that ignoring these massive values leads to a pronounced drop in\nperformance on tasks requiring rich contextual understanding, aligning with our\nanalysis. Finally, we trace the emergence of concentrated massive values and\nfind that such concentration is caused by Rotary Positional Encoding (RoPE),\nwhich has appeared since the first layers. These findings shed new light on how\nQ and K operate in LLMs and offer practical insights for model design and\noptimization. The Code is Available at\nhttps://github.com/MingyuJ666/Rope_with_LLM.", "AI": {"tldr": "This paper investigates the role of concentrated massive values in attention queries and keys of large language models and their impact on contextual knowledge understanding.", "motivation": "To understand how specific regions of attention in transformer-based large language models contribute to contextual knowledge interpretation.", "method": "The authors conducted extensive experiments comparing the behavior of attention queries, keys, and values in various modern LLMs, as well as analyzing the effects of quantization strategies.", "result": "The research shows that concentrated massive values are crucial for interpreting contextual knowledge, and their neglect leads to significant performance drops in understanding tasks.", "conclusion": "The study highlights that concentrated massive values, particularly influenced by Rotary Positional Encoding (RoPE), play a vital role in the functioning of attention mechanisms in LLMs and offers insights for future model optimizations.", "key_contributions": ["Identification of critical concentrated massive values in attention mechanisms.", "Insights into the influence of Rotary Positional Encoding on value concentration.", "Demonstration of the performance impact of quantization neglecting these values."], "limitations": "The study primarily focuses on attention mechanisms and may not cover other factors affecting LLM performance.", "future_work": "Further exploration into different encoding strategies and their influence on LLM performance.", "keywords": ["Large Language Models", "Attention Mechanisms", "Contextual Knowledge", "Rotary Positional Encoding", "Quantization Strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.12836", "pdf": "https://arxiv.org/pdf/2502.12836.pdf", "abs": "https://arxiv.org/abs/2502.12836", "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation", "authors": ["Mohammad Feli", "Iman Azimi", "Pasi Liljeberg", "Amir M. Rahmani"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub.", "AI": {"tldr": "This paper presents an LLM-powered agent for analyzing physiological time-series data to improve healthcare insights, particularly in heart rate estimation and decision-making support.", "motivation": "The aim is to enhance the integration of LLMs with analytical tools for better interpretation of physiological time-series data in healthcare contexts.", "method": "Developed an agent using OpenCHA and GPT-3.5-turbo, evaluating its effectiveness through a case study for heart rate estimation from PPG signals.", "result": "The agent outperformed models based on GPT-4o-mini and GPT-4o, boasting lower error rates and increased reliability in heart rate estimations.", "conclusion": "The LLM-powered agent shows significant promise in integrating advanced language models with classical data analysis tools for improved healthcare applications.", "key_contributions": ["Development of an LLM-powered agent tailored for physiological time-series analysis.", "Successful case study implementation demonstrating superior performance in heart rate estimation.", "Open-source availability of the agent to foster further research and application."], "limitations": "The study is focused on heart rate estimation, and further validation is needed across diverse physiological metrics and conditions.", "future_work": "Exploration of additional physiological measurements and further refinements of the LLM agent's analytical capabilities.", "keywords": ["Large Language Models", "Healthcare", "Physiological Time-Series", "Heart Rate Estimation", "Open Source"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2502.17163", "pdf": "https://arxiv.org/pdf/2502.17163.pdf", "abs": "https://arxiv.org/abs/2502.17163", "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "authors": ["María Andrea Cruz Blandón", "Jayasimha Talur", "Bruno Charron", "Dong Liu", "Saab Mansour", "Marcello Federico"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. Our\ndataset is available at https://github.com/amazon-science/MEMERAG", "AI": {"tldr": "Development of a multilingual meta-evaluation benchmark for retrieval augmented generation (RAG) systems addressing cultural nuances in user experience.", "motivation": "To improve automatic evaluation of RAG systems by integrating native-language perspectives that reflect cultural nuances overlooked by existing benchmarks.", "method": "Creation of the Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG) utilizing native-language questions and responses generated by diverse LLMs, assessed by expert annotators.", "result": "High inter-annotator agreement was achieved in the annotation process. The dataset enables reliable performance analysis of LLMs across languages and effective benchmarking of multilingual automatic evaluators.", "conclusion": "The MEMERAG benchmark demonstrates potential to enhance the evaluation of RAG systems, showcasing the impact of advanced prompting techniques on performance.", "key_contributions": ["Introduction of the MEMERAG benchmark for multilingual evaluation of RAG systems", "High-quality annotation process achieving strong inter-annotator agreement", "Application of the dataset for benchmarking LLMs as evaluators of RAG outputs"], "limitations": "The study may be limited by the scope of languages included and the generalizability of findings to all RAG systems.", "future_work": "Future research may explore expanding the benchmark to additional languages and further refining the evaluation criteria.", "keywords": ["retrieval augmented generation", "multilingual evaluation", "large language models", "automatic evaluators", "human judgement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358.pdf", "abs": "https://arxiv.org/abs/2503.15358", "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Marco Idiart"], "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.", "AI": {"tldr": "The paper discusses a challenge in NLP regarding the interpretation of idiomatic expressions using LLMs and presents datasets for SemEval-2025 Task 1.", "motivation": "Understanding idiomatic expressions is crucial for improving semantic representation in NLP, yet it remains a challenge even with advanced LLMs.", "method": "Participants in the SemEval-2025 Task 1 competed in subtasks involving ranking images based on their relevance to idiomatic meanings and predicting the next image in a sequence, utilizing pretrained LLMs and vision-language models.", "result": "Methods that combined pretrained models with a mixture of experts achieved human-level performance in understanding idiomaticity.", "conclusion": "Utilizing multimodal contexts and improved model architectures can significantly enhance the understanding of idiomatic expressions in NLP.", "key_contributions": ["Introduction of datasets for idiomaticity representation", "Implementation of multimodal tasks for idiomatic expressions", "Demonstration of human-level performance using advanced LLM techniques."], "limitations": "The effectiveness of idiomatic understanding may vary across different languages and cultural contexts.", "future_work": "Further exploration of the generalization of results to other languages and the integration of more diverse modalities.", "keywords": ["idiomatic expressions", "NLP", "multimodal", "LLM", "SemEval-2025"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.19878", "pdf": "https://arxiv.org/pdf/2503.19878.pdf", "abs": "https://arxiv.org/abs/2503.19878", "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation", "authors": ["Nengbo Wang", "Xiaotian Han", "Jagdip Singh", "Jing Ma", "Vipin Chaudhary"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks.", "AI": {"tldr": "CausalRAG introduces a novel retrieval framework that improves LLMs by integrating causal graphs, enhancing contextual integrity and retrieval precision.", "motivation": "To overcome limitations of traditional Retrieval-Augmented Generation (RAG) systems in maintaining contextual integrity and retrieval accuracy.", "method": "A new framework called CausalRAG is proposed, integrating causal graphs into the retrieval process to construct and trace causal relationships.", "result": "CausalRAG outperforms regular RAG and graph-based RAG methods across various metrics, showing improved contextual continuity and response accuracy.", "conclusion": "Integrating causal reasoning into retrieval processes offers a promising approach for enhancing performance in knowledge-intensive tasks.", "key_contributions": ["Introduction of CausalRAG framework", "Integration of causal graphs for improved context preservation", "Demonstrated superiority over traditional RAG and graph-based RAG approaches"], "limitations": "Specific limitations not detailed in the abstract.", "future_work": "To explore further applications of causal reasoning in various NLP and retrieval contexts.", "keywords": ["large language models", "retrieval-augmented generation", "causal reasoning", "natural language processing", "knowledge-intensive tasks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799.pdf", "abs": "https://arxiv.org/abs/2504.00799", "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "authors": ["Xi Wang", "Fanfei Meng", "Shiyang Zhang", "Lan Li"], "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries.", "AI": {"tldr": "This study investigates the reliability of electronic dictionaries, particularly Youdao, highlighting issues in definition accuracy and consultation habits among users.", "motivation": "To examine the reliability of electronic dictionaries for language learners and address the lack of scrutiny on their definitions.", "method": "The study employed an experimental approach, a user survey, and a critique of Youdao, analyzing the translation tasks performed by participants and their consultation behaviors.", "result": "Findings indicate that incomplete or misleading definitions lead to misunderstandings, and users demonstrate problematic consultation habits.", "conclusion": "The study calls for improved dictionary literacy among users and enhancements to AI models used in dictionary construction.", "key_contributions": ["Investigates the reliability of Youdao as an electronic dictionary.", "Analyzes user consultation habits and their impact on comprehension.", "Suggests improvements in AI model training for dictionary definitions."], "limitations": "Limited to one electronic dictionary (Youdao) and may not generalize to others.", "future_work": "Future research could address broader dictionary platforms and investigate user education in dictionary literacy.", "keywords": ["Electronic Dictionaries", "Youdao", "Dictionary Literacy", "AI in Dictionaries", "User Consultation Behavior"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2504.05239", "pdf": "https://arxiv.org/pdf/2504.05239.pdf", "abs": "https://arxiv.org/abs/2504.05239", "title": "LLM-based Automated Grading with Human-in-the-Loop", "authors": ["Hang Li", "Yucheng Chu", "Kaiqi Yang", "Yasemin Copur-Gencturk", "Jiliang Tang"], "categories": ["cs.CL"], "comment": null, "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation.", "AI": {"tldr": "This paper introduces GradeHITL, an interactive framework that utilizes large language models (LLMs) in automatic short answer grading (ASAG) by integrating human insights for enhanced accuracy.", "motivation": "Exploring the potential of LLMs in automatic short answer grading tasks and addressing the limitations of existing methods in achieving human-level grading performance.", "method": "The proposed GradeHITL framework employs a human-in-the-loop approach where LLMs interact with human experts to refine grading rubrics dynamically, improving performance.", "result": "The framework significantly improves grading accuracy, outperforming existing LLM-powered grading methods and getting closer to human-level evaluation.", "conclusion": "GradeHITL allows for sophisticated grading scenarios by effectively leveraging human expertise alongside AI technologies, enhancing the grading process in education.", "key_contributions": ["Introduction of the GradeHITL framework for ASAG using LLMs and human insights.", "Demonstrating substantial improvements in grading accuracy compared to traditional methods.", "Showcasing the potential of interactive LLM capabilities in educational assessment."], "limitations": "Challenges remain in achieving full automation and seamless integration of human feedback in ASAG processes.", "future_work": "Further exploration of automated elements in the grading process while maintaining the effectiveness of human feedback could be beneficial.", "keywords": ["automatic short answer grading", "human-in-the-loop", "large language models", "dynamic grading rubrics", "educational assessment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072.pdf", "abs": "https://arxiv.org/abs/2504.07072", "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Setayesh Heydari", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "categories": ["cs.CL", "cs.CV"], "comment": "v2: corrected the author list", "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "AI": {"tldr": "Kaleidoscope is a new comprehensive multilingual and multicultural benchmark for evaluating vision-language models, addressing gaps in current evaluation metrics.", "motivation": "To address the lack of comprehensive multilingual and multicultural evaluation benchmarks for vision-language models, which often rely on English and fail to capture cultural nuances.", "method": "Kaleidoscope, a large-scale multimodal benchmark, evaluates vision-language models across 18 languages and 14 subjects, comprising 20,911 multiple-choice questions, developed through global researcher collaboration.", "result": "Evaluation of leading multilingual vision-language models reveals poor performance on low-resource languages and in complex scenarios, indicating significant room for improvement in culturally inclusive frameworks.", "conclusion": "The study underscores the necessity for more inclusive multimodal evaluation frameworks that take into account linguistic and cultural diversity.", "key_contributions": ["Introduction of Kaleidoscope, a comprehensive multimodal benchmark for vision-language models.", "Focus on linguistic and cultural authenticity in evaluation benchmarks.", "Highlighting performance inadequacies of current models on low-resource languages."], "limitations": "The benchmark may not cover all languages and visual contexts; performance insights are based on current top models which may evolve.", "future_work": "Future research should concentrate on enhancing performance in low-resource languages and exploring additional cultural contexts in multimodal evaluation.", "keywords": ["vision-language models", "multilingual benchmarks", "cultural diversity"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914.pdf", "abs": "https://arxiv.org/abs/2504.13914", "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning", "authors": ["ByteDance Seed", ":", "Jiaze Chen", "Tiantian Fan", "Xin Liu", "Lingjun Liu", "Zhiqi Lin", "Mingxuan Wang", "Chengyi Wang", "Xiangpeng Wei", "Wenyuan Xu", "Yufeng Yuan", "Yu Yue", "Lin Yan", "Qiying Yu", "Xiaochen Zuo", "Chi Zhang", "Ruofei Zhu", "Zhecheng An", "Zhihao Bai", "Yu Bao", "Xingyan Bin", "Jiangjie Chen", "Feng Chen", "Hongmin Chen", "Riwei Chen", "Liangqiang Chen", "Zixin Chen", "Jinsong Chen", "Siyan Chen", "Kaiyuan Chen", "Zhi Chen", "Jin Chen", "Jiecao Chen", "Jinxin Chi", "Weinan Dai", "Ning Dai", "Jiahui Dai", "Shihan Dou", "Yantao Du", "Zhengyin Du", "Jianhui Duan", "Chen Dun", "Ting-Han Fan", "Jiazhan Feng", "Junda Feng", "Ziyuan Feng", "Yuwei Fu", "Wenqi Fu", "Hanjie Fu", "Hao Ge", "Hongyi Guo", "Mingji Han", "Li Han", "Wenhao Hao", "Xintong Hao", "Qianyu He", "Jerry He", "Feng He", "Wen Heng", "Zehua Hong", "Qi Hou", "Liang Hu", "Shengding Hu", "Nan Hu", "Kai Hua", "Qi Huang", "Ziyue Huang", "Hongzhi Huang", "Zihao Huang", "Ting Huang", "Wenhao Huang", "Wei Jia", "Bin Jia", "Xiaoying Jia", "Yuhua Jiang", "Haobin Jiang", "Ziheng Jiang", "Kaihua Jiang", "Chengquan Jiang", "Jianpeng Jiao", "Xiaoran Jin", "Xing Jin", "Xunhao Lai", "Zheng Li", "Xiang Li", "Liyi Li", "Hongkai Li", "Zheng Li", "Shengxian Wan", "Ya Wang", "Yunshui Li", "Chenggang Li", "Niuniu Li", "Siyu Li", "Xi Li", "Xiao Li", "Aoyan Li", "Yuntao Li", "Nianning Liang", "Xinnian Liang", "Haibin Lin", "Weijian Lin", "Ye Lin", "Zhicheng Liu", "Guanlin Liu", "Guanlin Liu", "Chenxiao Liu", "Yan Liu", "Gaohong Liu", "Juncai Liu", "Chundian Liu", "Deyi Liu", "Kaibo Liu", "Siyao Liu", "Qi Liu", "Yongfei Liu", "Kang Liu", "Gan Liu", "Boyi Liu", "Rui Long", "Weiqiang Lou", "Chenwei Lou", "Xiang Luo", "Yao Luo", "Caiping Lv", "Heyang Lv", "Bole Ma", "Qianli Ma", "Hongzhi Ma", "Yiyuan Ma", "Jin Ma", "Wenchang Ma", "Tingting Ma", "Chen Mao", "Qiyang Min", "Zhe Nan", "Guanghan Ning", "Jinxiang Ou", "Haojie Pan", "Renming Pang", "Yanghua Peng", "Tao Peng", "Lihua Qian", "Lihua Qian", "Mu Qiao", "Meng Qu", "Cheng Ren", "Hongbin Ren", "Yong Shan", "Wei Shen", "Ke Shen", "Kai Shen", "Guangming Sheng", "Jinlong Shi", "Wenlei Shi", "Guang Shi", "Shuai Shuai Cao", "Yuxin Song", "Zuquan Song", "Jing Su", "Yifan Sun", "Tao Sun", "Zewei Sun", "Borui Wan", "Zihan Wang", "Xiaohui Wang", "Xi Wang", "Shuguang Wang", "Jun Wang", "Qinlong Wang", "Chenyuan Wang", "Shuai Wang", "Zihan Wang", "Changbao Wang", "Jiaqiang Wang", "Shihang Wang", "Xuwu Wang", "Zaiyuan Wang", "Yuxuan Wang", "Wenqi Wang", "Taiqing Wang", "Chengzhi Wei", "Houmin Wei", "Ziyun Wei", "Shufa Wei", "Zheng Wu", "Yonghui Wu", "Yangjun Wu", "Bohong Wu", "Shuang Wu", "Jingqiao Wu", "Ning Wu", "Shuangzhi Wu", "Jianmin Wu", "Chenguang Xi", "Fan Xia", "Yuqiao Xian", "Liang Xiang", "Boren Xiang", "Bowen Xiao", "Zhen Xiao", "Xia Xiao", "Yongsheng Xiao", "Chao Xin", "Shulin Xin", "Yuwen Xiong", "Jingjing Xu", "Ziwen Xu", "Chenyin Xu", "Jiayi Xu", "Yifan Xu", "Wei Xu", "Yufei Xu", "Shikun Xu", "Shipeng Yan", "Shen Yan", "Qingping Yang", "Xi Yang", "Tianhao Yang", "Yuehang Yang", "Yuan Yang", "Ximing Yang", "Zeyu Yang", "Guang Yang", "Yifan Yang", "Xuesong Yao", "Bairen Yi", "Fan Yin", "Jianian Yin", "Ziqiang Ying", "Xiangyu Yu", "Hongli Yu", "Song Yu", "Menghan Yu", "Huan Yu", "Siyu Yuan", "Jun Yuan", "Yutao Zeng", "Tianyang Zhan", "Zheng Zhang", "Yun Zhang", "Mofan Zhang", "Wang Zhang", "Ru Zhang", "Zhi Zhang", "Tianqi Zhang", "Xinyi Zhang", "Zhexi Zhang", "Sijun Zhang", "Wenqiang Zhang", "Xiangxiang Zhang", "Yongtao Zhang", "Yuyu Zhang", "Ge Zhang", "He Zhang", "Yue Zhang", "Renjie Zheng", "Ningxin Zheng", "Zhuolin Zheng", "Yaowei Zheng", "Chen Zheng", "Xiaoyun Zhi", "Wanjun Zhong", "Cheng Zhong", "Zheng Zhong", "Baoquan Zhong", "Xun Zhou", "Na Zhou", "Huan Zhou", "Hang Zhu", "Defa Zhu", "Wenjia Zhu", "Lei Zuo"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed1.5-Thinking, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research. Model trial link:\nhttps://www.volcengine.com/experience/ark.", "AI": {"tldr": "Introduction of Seed1.5-Thinking, a reasoning model with strong performance on various benchmarks.", "motivation": "The aim is to enhance reasoning capabilities in AI models and demonstrate improved performance across diverse domains, particularly in STEM and coding tasks.", "method": "Seed1.5-Thinking is a Mixture-of-Experts model with 20B active parameters and 200B total parameters, designed to perform reasoning through a structured thought process before responding.", "result": "Achieves high scores on multiple benchmarks: 86.7 on AIME 2024, 55.0 on Codeforces, and 77.3 on GPQA, also surpassing DeepSeek R1 in non-reasoning tasks.", "conclusion": "Seed1.5-Thinking's performance showcases its effectiveness in reasoning tasks and generalization capabilities across different domains, supported by new benchmarks for future research.", "key_contributions": ["Introduction of a new reasoning methodology with notable performance improvements.", "Development of new benchmarks to assess generalized reasoning capabilities.", "Demonstration of applicability beyond reasoning tasks with higher win rates than existing models."], "limitations": "The paper does not address the implications of model size on efficiency or practical applications.", "future_work": "Further research will focus on improving the reasoning model and exploring its applications in various real-world scenarios.", "keywords": ["Seed1.5-Thinking", "reasoning models", "Mixture-of-Experts", "AI benchmarks", "generalization"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.15642", "pdf": "https://arxiv.org/pdf/2504.15642.pdf", "abs": "https://arxiv.org/abs/2504.15642", "title": "Computational Typology", "authors": ["Gerhard Jäger"], "categories": ["cs.CL", "q-bio.PE"], "comment": "19 pages, s5 figure", "summary": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology.", "AI": {"tldr": "This article discusses the role of computational methods in the study of language typology and illustrates the benefits of computational statistical modeling for analyzing large-scale linguistic data.", "motivation": "To explore the classification of languages based on structural features and the significance of computational methods in linguistic research.", "method": "The article illustrates the application of computational statistical modeling to linguistic data analysis and hypothesis testing in typology.", "result": "The use of computational methods enhances the analysis of linguistic structures and supports testing hypotheses about language diversity and evolution.", "conclusion": "Computational statistical modeling is beneficial for advancing research in language typology by enabling large-scale data analysis.", "key_contributions": ["Demonstrates the integration of computational techniques in linguistic typology.", "Illustrates the benefits of statistical modeling for understanding language universals and structures."], "limitations": "", "future_work": "Further exploration of computational methods in other areas of linguistics.", "keywords": ["typology", "computational methods", "linguistics", "statistical modeling", "language structures"], "importance_score": 2, "read_time_minutes": 10}}
