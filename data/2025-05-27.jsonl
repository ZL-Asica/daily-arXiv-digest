{"id": "2505.18318", "pdf": "https://arxiv.org/pdf/2505.18318.pdf", "abs": "https://arxiv.org/abs/2505.18318", "title": "The Relational Origins of Rules in Online Communities", "authors": ["Charles Kiene", "Sohyeon Hwang", "Nathan TeBlunthuis", "Carl Colglazier", "Aaron Shaw", "Benjamin Mako Hill"], "categories": ["cs.HC"], "comment": null, "summary": "Where do rules come from in online communities? While prior studies of online\ncommunity governance in social computing have sought to characterize rules by\ntheir functions within communities and documented practices of rule\nenforcement, they have largely overlooked rule adoption and change. This study\ninvestigates how and why online communities adopt and change their rules. We\nconducted a grounded theory-based analysis of 40 in-depth interviews with\ncommunity leaders from subreddits, Fandom wikis, and Fediverse servers, and\nidentified seven processes involved in the adoption of online community rules.\nOur findings reveal that, beyond regulating behavior and solving functional\nintra-community problems, rules are also adopted and changed for relational\nreasons, such as signaling or reinforcing community legitimacy and identity to\nother communities. While rule change was often prompted by challenges during\ncommunity growth or decline, change also depended on volunteer leaders' work\ncapacity, the presence of member feedback mechanisms, and relational dynamics\nbetween leaders and members. The findings extend prior theories from social\ncomputing and organizational research, illustrating how institutionalist and\necological explanations of the relational origins of rules complement more\nfunctional accounts. The results also support design recommendations that\nintegrate the relational aspects of rules and rulemaking to facilitate\nsuccessful governance across communities' lifecycles."}
{"id": "2505.18385", "pdf": "https://arxiv.org/pdf/2505.18385.pdf", "abs": "https://arxiv.org/abs/2505.18385", "title": "Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights", "authors": ["Jeba Rezwana", "Corey Ford"], "categories": ["cs.HC", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2504.02526", "summary": "Effective communication between AI and humans is essential for successful\nhuman-AI co-creation. However, many current co-creative AI systems lack\neffective communication, which limits their potential for collaboration. This\npaper presents the initial design of the Framework for AI Communication (FAICO)\nfor co-creative AI, developed through a systematic review of 107 full-length\npapers. FAICO presents key aspects of AI communication and their impact on user\nexperience, offering preliminary guidelines for designing human-centered AI\ncommunication. To improve the framework, we conducted a preliminary study with\ntwo focus groups involving skilled individuals in AI, HCI, and design. These\nsessions sought to understand participants' preferences for AI communication,\ngather their perceptions of the framework, collect feedback for refinement, and\nexplore its use in co-creative domains like collaborative writing and design.\nOur findings reveal a preference for a human-AI feedback loop over linear\ncommunication and emphasize the importance of context in fostering mutual\nunderstanding. Based on these insights, we propose actionable strategies for\napplying FAICO in practice and future directions, marking the first step toward\ndeveloping comprehensive guidelines for designing effective human-centered AI\ncommunication in co-creation."}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464.pdf", "abs": "https://arxiv.org/abs/2505.18464", "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies."}
{"id": "2505.18553", "pdf": "https://arxiv.org/pdf/2505.18553.pdf", "abs": "https://arxiv.org/abs/2505.18553", "title": "Applying Ontologies and Knowledge Augmented Large Language Models to Industrial Automation: A Decision-Making Guidance for Achieving Human-Robot Collaboration in Industry 5.0", "authors": ["John Oyekan", "Christopher Turner", "Michael Bax", "Erich Graf"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in\ninterest in their potential applications within manufacturing systems,\nparticularly in the context of Industry 5.0. However, determining when to\nimplement LLMs versus other Natural Language Processing (NLP) techniques,\nontologies or knowledge graphs, remains an open question. This paper offers\ndecision-making guidance for selecting the most suitable technique in various\nindustrial contexts, emphasizing human-robot collaboration and resilience in\nmanufacturing. We examine the origins and unique strengths of LLMs, ontologies,\nand knowledge graphs, assessing their effectiveness across different industrial\nscenarios based on the number of domains or disciplines required to bring a\nproduct from design to manufacture. Through this comparative framework, we\nexplore specific use cases where LLMs could enhance robotics for human-robot\ncollaboration, while underscoring the continued relevance of ontologies and\nknowledge graphs in low-dependency or resource-constrained sectors.\nAdditionally, we address the practical challenges of deploying these\ntechnologies, such as computational cost and interpretability, providing a\nroadmap for manufacturers to navigate the evolving landscape of Language based\nAI tools in Industry 5.0. Our findings offer a foundation for informed\ndecision-making, helping industry professionals optimize the use of Language\nBased models for sustainable, resilient, and human-centric manufacturing. We\nalso propose a Large Knowledge Language Model architecture that offers the\npotential for transparency and configuration based on complexity of task and\ncomputing resources available."}
{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159.pdf", "abs": "https://arxiv.org/abs/2505.18159", "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement."}
{"id": "2505.18771", "pdf": "https://arxiv.org/pdf/2505.18771.pdf", "abs": "https://arxiv.org/abs/2505.18771", "title": "SPIRAL integration of generative AI in an undergraduate creative media course: effects on self-efficacy and career outcome expectations", "authors": ["Troy Schotter", "Saba Kawas", "James Prather", "Juho Leinonen", "Jon Ippolito", "Greg L Nelson"], "categories": ["cs.HC"], "comment": null, "summary": "Computing education and computing students are rapidly integrating generative\nAI, but we know relatively little about how different pedagogical strategies\nfor intentionally integrating generative AI affect students' self-efficacy and\ncareer interests. This study investigates a SPIRAL integration of generative AI\n(Skills Practiced Independently, Revisited with AI Later), implemented in an\nintroductory undergraduate creative media and technology course in Fall 2023\n(n=31). Students first developed domain skills for half the semester, then\nrevisited earlier material integrating using generative AI, with explicit\ninstruction on how to use it critically and ethically. We contribute a mixed\nmethods quantitative and qualitative analysis of changes in self-efficacy and\ncareer interests over time, including longitudinal qualitative interviews (n=9)\nand thematic analysis. We found positive changes in both students' creative\nmedia self-efficacy and generative AI use self-efficacy, and mixed changes for\nethical generative AI use self-efficacy. We also found students experienced\ndemystification, transitioning from initial fear about generative AI taking\nover their fields and jobs, to doubting AI capability to do so and/or that\nsociety will push back against AI, through personal use of AI and observing\nothers' use of AI vicariously. For career interests, our SPIRAL integration of\ngenerative AI use appeared to have either a neutral or positive influence on\nstudents, including widening their perceived career options, depending on their\nview of how AI would influence the career itself. These findings suggest that\ncareful pedagogical sequencing can mitigate some potential negative impacts of\nAI, while promoting ethical and critical AI use that supports or has a neutral\neffect on students' career formation. To our knowledge our SPIRAL integration\nstrategy applied to generative AI integration is novel."}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215.pdf", "abs": "https://arxiv.org/abs/2505.18215", "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs."}
{"id": "2505.18862", "pdf": "https://arxiv.org/pdf/2505.18862.pdf", "abs": "https://arxiv.org/abs/2505.18862", "title": "Literature review on assistive technologies for people with Parkinson's disease", "authors": ["Subek Acharya", "Sansrit Paudel"], "categories": ["cs.HC"], "comment": null, "summary": "Parkinson's Disease (PD) is a neurodegenerative disorder that significantly\nimpacts motor and non-motor functions. There is currently no treatment that\nslows or stops neurodegeneration in PD. In this context, assistive technologies\n(ATs) have emerged as vital tools to aid people with Parkinson's and\nsignificantly improve their quality of life. This review explores a broad\nspectrum of ATs, including wearable and cueing devices, exoskeletons, robotics,\nvirtual reality, voice and video-assisted technologies, and emerging\ninnovations such as artificial intelligence (AI), machine learning (ML), and\nthe Internet of Things (IoT). The review highlights ATs' significant role in\naddressing motor symptoms such as freezing of gait (FOG) and gait and posture\ndisorders. However, it also identifies significant gaps in addressing non-motor\nsymptoms such as sleep dysfunction and mental health. Similarly, the research\nidentifies substantial potential in the further implementation of deep\nlearning, AI, IOT technologies. Overall, this review highlights the\ntransformative potential of AT in PD management while identifying gaps that\nfuture research should address to ensure personalized, accessible, and\neffective solutions."}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218.pdf", "abs": "https://arxiv.org/abs/2505.18218", "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "authors": ["Shuhang Xu", "Fangwei Zhong"], "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors."}
{"id": "2505.18928", "pdf": "https://arxiv.org/pdf/2505.18928.pdf", "abs": "https://arxiv.org/abs/2505.18928", "title": "Toward Human Centered Interactive Clinical Question Answering System", "authors": ["Dina Albassam"], "categories": ["cs.HC"], "comment": null, "summary": "Unstructured clinical notes contain essential patient information but are\nchallenging for physicians to search and interpret efficiently. Although large\nlanguage models (LLMs) have shown promise in question answering (QA), most\nexisting systems lack transparency, usability, and alignment with clinical\nworkflows. This work introduces an interactive QA system that enables\nphysicians to query clinical notes via text or voice and receive extractive\nanswers highlighted directly in the note for traceability.\n  The system was built using OpenAI models with zero-shot prompting and\nevaluated across multiple metrics, including exact string match, word overlap,\nSentenceTransformer similarity, and BERTScore. Results show that while exact\nmatch scores ranged from 47 to 62 percent, semantic similarity scores exceeded\n87 percent, indicating strong contextual alignment even when wording varied.\n  To assess usability, the system was also evaluated using simulated clinical\npersonas. Seven diverse physician and nurse personas interacted with the system\nacross scenario-based tasks and provided structured feedback. The evaluations\nhighlighted strengths in intuitive design and answer accessibility, alongside\nopportunities for enhancing explanation clarity."}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223.pdf", "abs": "https://arxiv.org/abs/2505.18223", "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning."}
{"id": "2505.19101", "pdf": "https://arxiv.org/pdf/2505.19101.pdf", "abs": "https://arxiv.org/abs/2505.19101", "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems", "authors": ["Vaishali Dhanoa", "Anton Wolter", "Gabriela Molina León", "Hans-Jörg Schulz", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous agents powered by Large Language Models are transforming AI,\ncreating an imperative for the visualization field to embrace agentic\nframeworks. However, our field's focus on a human in the sensemaking loop\nraises critical questions about autonomy, delegation, and coordination for such\n\\textit{agentic visualization} that preserve human agency while amplifying\nanalytical capabilities. This paper addresses these questions by reinterpreting\nexisting visualization systems with semi-automated or fully automatic AI\ncomponents through an agentic lens. Based on this analysis, we extract a\ncollection of design patterns for agentic visualization, including agentic\nroles, communication and coordination. These patterns provide a foundation for\nfuture agentic visualization systems that effectively harness AI agents while\nmaintaining human insight and control."}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237.pdf", "abs": "https://arxiv.org/abs/2505.18237", "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment."}
{"id": "2505.19325", "pdf": "https://arxiv.org/pdf/2505.19325.pdf", "abs": "https://arxiv.org/abs/2505.19325", "title": "What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study", "authors": ["Bhanuka Gamage", "Thanh-Toan Do", "Nicholas Seow Chiang Price", "Arthur Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's accepted version of a paper published at ACM SIGACCESS\n  Conference on Computers and Accessibility (ASSETS '23)", "summary": "Over the last decade there has been considerable research into how artificial\nintelligence (AI), specifically computer vision, can assist people who are\nblind or have low-vision (BLV) to understand their environment. However, there\nhas been almost no research into whether the tasks (object detection, image\ncaptioning, text recognition etc.) and devices (smartphones, smart-glasses\netc.) investigated by researchers align with the needs and preferences of BLV\npeople. We identified 646 studies published in the last two and a half years\nthat have investigated such assistive AI techniques. We analysed these papers\nto determine the task, device and participation by BLV individuals. We then\ninterviewed 24 BLV people and asked for their top five AI-based applications\nand to rank the applications found in the literature. We found only a weak\npositive correlation between BLV participants' perceived importance of tasks\nand researchers' focus and that participants prefer conversational agent\ninterface and head-mounted devices."}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240.pdf", "abs": "https://arxiv.org/abs/2505.18240", "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations."}
{"id": "2505.19335", "pdf": "https://arxiv.org/pdf/2505.19335.pdf", "abs": "https://arxiv.org/abs/2505.19335", "title": "Knoll: Creating a Knowledge Ecosystem for Large Language Models", "authors": ["Dora Zhao", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC"], "comment": "21 pages, 8 figures, 7 tables", "summary": "Large language models are designed to encode general purpose knowledge about\nthe world from Internet data. Yet, a wealth of information falls outside this\nscope -- ranging from personal preferences to organizational policies, from\ncommunity-specific advice to up-to-date news -- that users want models to\naccess but remains unavailable. In this paper, we propose a knowledge ecosystem\nin which end-users can create, curate, and configure custom knowledge modules\nthat are utilized by language models, such as ChatGPT and Claude. To support\nthis vision, we introduce Knoll, a software infrastructure that allows users to\nmake modules by clipping content from the web or authoring shared documents on\nGoogle Docs and GitHub, add modules that others have made, and rely on the\nsystem to insert relevant knowledge when interacting with an LLM. We conduct a\npublic deployment of Knoll reaching over 200 users who employed the system for\na diverse set of tasks including personalized recommendations, advice-seeking,\nand writing assistance. In our evaluation, we validate that using Knoll\nimproves the quality of generated responses."}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244.pdf", "abs": "https://arxiv.org/abs/2505.18244", "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "authors": ["Yukin Zhang", "Qi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities."}
{"id": "2505.19419", "pdf": "https://arxiv.org/pdf/2505.19419.pdf", "abs": "https://arxiv.org/abs/2505.19419", "title": "It's Not Just Labeling\" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features", "authors": ["Baichuan Li", "Larry Powell", "Tracy Hammond"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability."}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247.pdf", "abs": "https://arxiv.org/abs/2505.18247", "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc."}
{"id": "2505.19441", "pdf": "https://arxiv.org/pdf/2505.19441.pdf", "abs": "https://arxiv.org/abs/2505.19441", "title": "Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems", "authors": ["Jing Nathan Yan", "Junxiong Wang", "Jeffrey M. Rzeszotarski", "Allison Koenecke"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "The rapid proliferation of recommender systems necessitates robust fairness\npractices to address inherent biases. Assessing fairness, though, is\nchallenging due to constantly evolving metrics and best practices. This paper\nanalyzes how industry practitioners perceive and incorporate these changing\nfairness standards in their workflows. Through semi-structured interviews with\n11 practitioners from technical teams across a range of large technology\ncompanies, we investigate industry implementations of fairness in\nrecommendation system products. We focus on current debiasing practices,\napplied metrics, collaborative strategies, and integrating academic research\ninto practice. Findings show a preference for multi-dimensional debiasing over\ntraditional demographic methods, and a reliance on intuitive rather than\nacademic metrics. This study also highlights the difficulties in balancing\nfairness with both the practitioner's individual (bottom-up) roles and\norganizational (top-down) workplace constraints, including the interplay with\nlegal and compliance experts. Finally, we offer actionable recommendations for\nthe recommender system community and algorithmic fairness practitioners,\nunderlining the need to refine fairness practices continually."}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283.pdf", "abs": "https://arxiv.org/abs/2505.18283", "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS."}
{"id": "2505.19652", "pdf": "https://arxiv.org/pdf/2505.19652.pdf", "abs": "https://arxiv.org/abs/2505.19652", "title": "SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding", "authors": ["Hongbin Wang", "Zhihong Jia", "Yuanzhong Shen", "Ziwei Wang", "Siyang Li", "Kai Shu", "Feng Hu", "Dongrui Wu"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech disorders such as dysarthria and anarthria can severely impair the\npatient's ability to communicate verbally. Speech decoding brain-computer\ninterfaces (BCIs) offer a potential alternative by directly translating speech\nintentions into spoken words, serving as speech neuroprostheses. This paper\nreports an experimental protocol for Mandarin Chinese speech decoding BCIs,\nalong with the corresponding decoding algorithms. Stereo-electroencephalography\n(SEEG) and synchronized audio data were collected from eight drug-resistant\nepilepsy patients as they conducted a word-level reading task. The proposed\nSEEG and Audio Contrastive Matching (SACM), a contrastive learning-based\nframework, achieved decoding accuracies significantly exceeding chance levels\nin both speech detection and speech decoding tasks. Electrode-wise analysis\nrevealed that a single sensorimotor cortex electrode achieved performance\ncomparable to that of the full electrode array. These findings provide valuable\ninsights for developing more accurate online speech decoding BCIs."}
{"id": "2505.18298", "pdf": "https://arxiv.org/pdf/2505.18298.pdf", "abs": "https://arxiv.org/abs/2505.18298", "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards", "authors": ["Jinyan Su", "Claire Cardie"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models."}
{"id": "2505.20068", "pdf": "https://arxiv.org/pdf/2505.20068.pdf", "abs": "https://arxiv.org/abs/2505.20068", "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction", "authors": ["Qingyu Liang", "Jaime Banks"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion."}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322.pdf", "abs": "https://arxiv.org/abs/2505.18322", "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base."}
{"id": "2505.20082", "pdf": "https://arxiv.org/pdf/2505.20082.pdf", "abs": "https://arxiv.org/abs/2505.20082", "title": "Understanding and Supporting Co-viewing Comedy in VR with Embodied Expressive Avatars", "authors": ["Ryo Ohara", "Chi-Lan Yang", "Takuji Narumi", "Hideaki Kuzuoka"], "categories": ["cs.HC"], "comment": null, "summary": "Co-viewing videos with family and friends remotely has become prevalent with\nthe support of communication channels such as text messaging or real-time voice\nchat. However, current co-viewing platforms often lack visible embodied cues,\nsuch as body movements and facial expressions. This absence can reduce\nemotional engagement and the sense of co-presence when people are watching\ntogether remotely. Although virtual reality (VR) is an emerging technology that\nallows individuals to participate in various social activities while embodied\nas avatars, we still do not fully understand how this embodiment in VR affects\nco-viewing experiences, particularly in terms of engagement, emotional\ncontagion, and expressive norms. In a controlled experiment involving eight\ntriads of three participants each (N=24), we compared the participants'\nperceptions and reactions while watching comedy in VR using embodied expressive\navatars that displayed visible laughter cues. This was contrasted with a\ncontrol condition where no such embodied expressions were presented. With a\nmixed-method analysis, we found that embodied laughter cues shifted\nparticipants' engagement from individual immersion to socially coordinated\nparticipation. Participants reported heightened self-awareness of emotional\nexpression, greater emotional contagion, and the development of expressive\nnorms surrounding co-viewers' laughter. The result highlighted the tension\nbetween individual engagement and interpersonal emotional accommodation when\nco-viewing with embodied expressive avatars."}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331.pdf", "abs": "https://arxiv.org/abs/2505.18331", "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA"}
{"id": "2505.20085", "pdf": "https://arxiv.org/pdf/2505.20085.pdf", "abs": "https://arxiv.org/abs/2505.20085", "title": "Explanation User Interfaces: A Systematic Literature Review", "authors": ["Eleonora Cappuccio", "Andrea Esposito", "Francesco Greco", "Giuseppe Desolda", "Rosa Lanzilotti", "Salvatore Rinzivillo"], "categories": ["cs.HC", "cs.AI", "A.1"], "comment": "First version", "summary": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs."}
{"id": "2505.18343", "pdf": "https://arxiv.org/pdf/2505.18343.pdf", "abs": "https://arxiv.org/abs/2505.18343", "title": "Model Editing with Graph-Based External Memory", "authors": ["Yash Kumar Atri", "Ahmed Alaa", "Thomas Hartvigsen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning."}
{"id": "2505.20138", "pdf": "https://arxiv.org/pdf/2505.20138.pdf", "abs": "https://arxiv.org/abs/2505.20138", "title": "FairTalk: Facilitating Balanced Participation in Video Conferencing by Implicit Visualization of Predicted Turn-Grabbing Intention", "authors": ["Ryo Iijima", "Shigeo Yoshida", "Atsushi Hashimoto", "Jiaxin Ma"], "categories": ["cs.HC"], "comment": null, "summary": "Creating fair opportunities for all participants to contribute is a notable\nchallenge in video conferencing. This paper introduces FairTalk, a system that\nfacilitates the subconscious redistribution of speaking opportunities. FairTalk\npredicts participants' turn-grabbing intentions using a machine learning model\ntrained on web-collected videoconference data with positive-unlabeled learning,\nwhere turn-taking detection provides automatic positive labels. To subtly\nbalance speaking turns, the system visualizes predicted intentions by mimicking\nnatural human behaviors associated with the desire to speak. A user study\nsuggests that FairTalk may help improve speaking balance, though subjective\nfeedback indicates no significant perceived impact. We also discuss design\nimplications derived from participant interviews."}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356.pdf", "abs": "https://arxiv.org/abs/2505.18356", "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start."}
{"id": "2505.18156", "pdf": "https://arxiv.org/pdf/2505.18156.pdf", "abs": "https://arxiv.org/abs/2505.18156", "title": "InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models", "authors": ["Austin Howard"], "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": "This is an independent research whitepaper submitted as a preprint.\n  For more information, visit https://injectlab.org or\n  https://github.com/ahow2004/injectlab", "summary": "Large Language Models (LLMs) are changing the way people interact with\ntechnology. Tools like ChatGPT and Claude AI are now common in business,\nresearch, and everyday life. But with that growth comes new risks, especially\nprompt-based attacks that exploit how these models process language. InjectLab\nis a security framework designed to address that problem. This paper introduces\nInjectLab as a structured, open-source matrix that maps real-world techniques\nused to manipulate LLMs. The framework is inspired by MITRE ATT&CK and focuses\nspecifically on adversarial behavior at the prompt layer. It includes over 25\ntechniques organized under six core tactics, covering threats like instruction\noverride, identity swapping, and multi-agent exploitation. Each technique in\nInjectLab includes detection guidance, mitigation strategies, and YAML-based\nsimulation tests. A Python tool supports easy execution of prompt-based test\ncases. This paper outlines the framework's structure, compares it to other AI\nthreat taxonomies, and discusses its future direction as a practical,\ncommunity-driven foundation for securing language models."}
{"id": "2505.18363", "pdf": "https://arxiv.org/pdf/2505.18363.pdf", "abs": "https://arxiv.org/abs/2505.18363", "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases", "authors": ["AmirHossein Safdarian", "Milad Mohammadi", "Ehsan Jahanbakhsh", "Mona Shahamat Naderi", "Heshaam Faili"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes."}
{"id": "2505.18175", "pdf": "https://arxiv.org/pdf/2505.18175.pdf", "abs": "https://arxiv.org/abs/2505.18175", "title": "Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework", "authors": ["Natia Kukhilava", "Tatia Tsmindashvili", "Rapael Kalandadze", "Anchit Gupta", "Sofio Katamadze", "François Brémond", "Laura M. Ferrari", "Philipp Müller", "Benedikt Emanuel Wirth"], "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Electroencephalography-based Emotion Recognition (EEG-ER) has become a\ngrowing research area in recent years. Analyzing 216 papers published between\n2018 and 2023, we uncover that the field lacks a unified evaluation protocol,\nwhich is essential to fairly define the state of the art, compare new\napproaches and to track the field's progress. We report the main\ninconsistencies between the used evaluation protocols, which are related to\nground truth definition, evaluation metric selection, data splitting types\n(e.g., subject-dependent or subject-independent) and the use of different\ndatasets. Capitalizing on this state-of-the-art research, we propose a unified\nevaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which\nenables an easy and efficient evaluation of new methods and datasets. EEGain is\na novel open source software framework, offering the capability to compare -\nand thus define - state-of-the-art results. EEGain includes standardized\nmethods for data pre-processing, data splitting, evaluation metrics, and the\nability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,\nMAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In\naddition, we have assessed and validated EEGain using these six datasets on the\nfour most common publicly available methods (EEGNet, DeepConvNet,\nShallowConvNet, TSception). This is a significant step to make research on\nEEG-ER more reproducible and comparable, thereby accelerating the overall\nprogress of the field."}
{"id": "2505.18374", "pdf": "https://arxiv.org/pdf/2505.18374.pdf", "abs": "https://arxiv.org/abs/2505.18374", "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation", "authors": ["Jarrod Ragsdale", "Rajendra Boppana"], "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 11 figures, conference preprint", "summary": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research."}
{"id": "2505.18326", "pdf": "https://arxiv.org/pdf/2505.18326.pdf", "abs": "https://arxiv.org/abs/2505.18326", "title": "Pragmatic Disengagement and Culturally Situated Non Use Older Korean Immigrants Strategies for Navigating Digital Noise", "authors": ["Jeongone Seo", "Tawfiq Ammari"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Older immigrant adults often face layered barriers to digital participation,\nincluding language exclusion, generational divides, and emotional fatigue. This\nstudy examines how older Korean immigrants in the greater NYC area selectively\nengage with digital tools such as smartphones, YouTube, and AI platforms. Using\na community-based participatory research (CBPR) framework and 22\nsemi-structured interviews, we identify two key practices: pragmatic\ndisengagement, where users avoid emotionally taxing or culturally misaligned\ncontent, and interdependent navigation, where digital use is shaped through\nreliance on family or community support. These strategies challenge\ndeficit-oriented narratives of non-use, showing how disengagement can be\nthoughtful, protective, and culturally situated. We contribute to CSCW by\nexpanding theories of non-use and algorithmic resistance and by offering design\nand policy recommendations to support more dignified, culturally attuned\ndigital engagement for aging immigrant populations."}
{"id": "2505.18383", "pdf": "https://arxiv.org/pdf/2505.18383.pdf", "abs": "https://arxiv.org/abs/2505.18383", "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities", "authors": ["Abdellah El Mekki", "Houdaifa Atou", "Omer Nacar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development."}
{"id": "2505.18371", "pdf": "https://arxiv.org/pdf/2505.18371.pdf", "abs": "https://arxiv.org/abs/2505.18371", "title": "Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications", "authors": ["Riley Simmons-Edler", "Jean Dong", "Paul Lushenko", "Kanaka Rajan", "Ryan P. Badman"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.RO"], "comment": "16 pages, 2 tables, 1 figure", "summary": "Military weapon systems and command-and-control infrastructure augmented by\nartificial intelligence (AI) have seen rapid development and deployment in\nrecent years. However, the sociotechnical impacts of AI on combat systems,\nmilitary decision-making, and the norms of warfare have been understudied. We\nfocus on a specific subset of lethal autonomous weapon systems (LAWS) that use\nAI for targeting or battlefield decisions. We refer to this subset as\nAI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they\nintroduce novel risks -- including unanticipated escalation, poor reliability\nin unfamiliar environments, and erosion of human oversight -- all of which\nthreaten both military effectiveness and the openness of AI research. These\nrisks cannot be addressed by high-level policy alone; effective regulation must\nbe grounded in the technical behavior of AI models. We argue that AI\nresearchers must be involved throughout the regulatory lifecycle. Thus, we\npropose a clear, behavior-based definition of AI-LAWS -- systems that introduce\nunique risks through their use of modern AI -- as a foundation for technically\ngrounded regulation, given that existing frameworks do not distinguish them\nfrom conventional LAWS. Using this definition, we propose several\ntechnically-informed policy directions and invite greater participation from\nthe AI research community in military AI policy discussions."}
{"id": "2505.18405", "pdf": "https://arxiv.org/pdf/2505.18405.pdf", "abs": "https://arxiv.org/abs/2505.18405", "title": "RaDeR: Reasoning-aware Dense Retrieval Models", "authors": ["Debrup Das", "Sam O' Nuallain", "Razieh Rahimi"], "categories": ["cs.CL", "cs.IR"], "comment": "26 pages", "summary": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance.Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data."}
{"id": "2505.18412", "pdf": "https://arxiv.org/pdf/2505.18412.pdf", "abs": "https://arxiv.org/abs/2505.18412", "title": "Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering", "authors": ["Jessica Tang", "Ali Abedi", "Tracey J. F. Colella", "Shehroz S. Khan"], "categories": ["cs.CV", "cs.HC"], "comment": "16 pages, 3 figures, 5 tables", "summary": "Exercise-based rehabilitation improves quality of life and reduces morbidity,\nmortality, and rehospitalization, though transportation constraints and staff\nshortages lead to high dropout rates from rehabilitation programs. Virtual\nplatforms enable patients to complete prescribed exercises at home, while AI\nalgorithms analyze performance, deliver feedback, and update clinicians.\nAlthough many studies have developed machine learning and deep learning models\nfor exercise quality assessment, few have explored the use of large language\nmodels (LLMs) for feedback and are limited by the lack of rehabilitation\ndatasets containing textual feedback. In this paper, we propose a new method in\nwhich exercise-specific features are extracted from the skeletal joints of\npatients performing rehabilitation exercises and fed into pre-trained LLMs.\nUsing a range of prompting techniques, such as zero-shot, few-shot,\nchain-of-thought, and role-play prompting, LLMs are leveraged to evaluate\nexercise quality and provide feedback in natural language to help patients\nimprove their movements. The method was evaluated through extensive experiments\non two publicly available rehabilitation exercise assessment datasets (UI-PRMD\nand REHAB24-6) and showed promising results in exercise assessment, reasoning,\nand feedback generation. This approach can be integrated into virtual\nrehabilitation platforms to help patients perform exercises correctly, support\nrecovery, and improve health outcomes."}
{"id": "2505.18411", "pdf": "https://arxiv.org/pdf/2505.18411.pdf", "abs": "https://arxiv.org/abs/2505.18411", "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding", "authors": ["Yue Jiang", "Jichu Li", "Yang Liu", "Dingkang Yang", "Feng Zhou", "Quyu Kong"], "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench"}
{"id": "2505.18416", "pdf": "https://arxiv.org/pdf/2505.18416.pdf", "abs": "https://arxiv.org/abs/2505.18416", "title": "Dynamics of Affective States During Takeover Requests in Conditionally Automated Driving Among Older Adults with and without Cognitive Impairment", "authors": ["Gelareh Hajian", "Ali Abedi", "Bing Ye", "Jennifer Campos", "Alex Mihailidis"], "categories": ["cs.CV", "cs.HC"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Driving is a key component of independence and quality of life for older\nadults. However, cognitive decline associated with conditions such as mild\ncognitive impairment and dementia can compromise driving safety and often lead\nto premature driving cessation. Conditionally automated vehicles, which require\ndrivers to take over control when automation reaches its operational limits,\noffer a potential assistive solution. However, their effectiveness depends on\nthe driver's ability to respond to takeover requests (TORs) in a timely and\nappropriate manner. Understanding emotional responses during TORs can provide\ninsight into drivers' engagement, stress levels, and readiness to resume\ncontrol, particularly in cognitively vulnerable populations. This study\ninvestigated affective responses, measured via facial expression analysis of\nvalence and arousal, during TORs among cognitively healthy older adults and\nthose with cognitive impairment. Facial affect data were analyzed across\ndifferent road geometries and speeds to evaluate within- and between-group\ndifferences in affective states. Within-group comparisons using the Wilcoxon\nsigned-rank test revealed significant changes in valence and arousal during\nTORs for both groups. Cognitively healthy individuals showed adaptive increases\nin arousal under higher-demand conditions, while those with cognitive\nimpairment exhibited reduced arousal and more positive valence in several\nscenarios. Between-group comparisons using the Mann-Whitney U test indicated\nthat cognitively impaired individuals displayed lower arousal and higher\nvalence than controls across different TOR conditions. These findings suggest\nreduced emotional response and awareness in cognitively impaired drivers,\nhighlighting the need for adaptive vehicle systems that detect affective states\nand support safe handovers for vulnerable users."}
{"id": "2505.18426", "pdf": "https://arxiv.org/pdf/2505.18426.pdf", "abs": "https://arxiv.org/abs/2505.18426", "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps", "authors": ["Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mizanur Rahman", "Mashrur Chowdhury", "Bhavani Thuraisingham"], "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting\n  2025, and subsequently submitted for publication consideration in the\n  Transportation Research Record (TRR)", "summary": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies."}
{"id": "2505.18661", "pdf": "https://arxiv.org/pdf/2505.18661.pdf", "abs": "https://arxiv.org/abs/2505.18661", "title": "Supporting Preschool Emotional Development with AI-Powered Robots", "authors": ["Santiago Berrezueta-Guzman", "María Dolón-Poza", "Stefan Wagner"], "categories": ["cs.RO", "cs.HC"], "comment": "This is the preprint version of a paper accepted at the 24th ACM\n  Interaction Design and Children Conference, IDC 2025. The final published\n  version will be available via ACM Digital Library", "summary": "This study evaluates the integration of AI-powered robots in early childhood\neducation, focusing on their impact on emotional self-regulation, engagement,\nand collaborative skills. A ten-week experimental design involving two groups\nof children assessed the robot's effectiveness through progress assessments,\nparental surveys, and teacher feedback. Results demonstrated that early\nexposure to the robot significantly enhanced emotional recognition, while\nsustained interaction further improved collaborative and social engagement.\nParental and teacher feedback highlighted high acceptance levels, emphasizing\nthe robot's ease of integration and positive influence on classroom dynamics.\nThis research underscores the transformative potential of AI and robotics in\neducation. The findings advocate for the broader adoption of AI-powered\ninterventions, carefully examining equitable access, ethical considerations,\nand sustainable implementation. This work sets a foundation for exploring\nlong-term impacts and expanding applications of AI in inclusive and impactful\neducational settings."}
{"id": "2505.18436", "pdf": "https://arxiv.org/pdf/2505.18436.pdf", "abs": "https://arxiv.org/abs/2505.18436", "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier", "authors": ["AbdelRahim Elmadany", "Sang Yun Kwon", "Hawau Olamide Toyin", "Alcides Alcoba Inciarte", "Hanan Aldarmaki", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies."}
{"id": "2505.18814", "pdf": "https://arxiv.org/pdf/2505.18814.pdf", "abs": "https://arxiv.org/abs/2505.18814", "title": "Usability of Token-based and Remote Electronic Signatures: A User Experience Study", "authors": ["Omer Ege", "Mustafa Cagal", "Kemal Bicakci"], "categories": ["cs.CR", "cs.HC", "68M01", "H.5.2; K.6.5"], "comment": "27 pages", "summary": "As electronic signatures (e-signatures) become increasingly integral to\nsecure digital transactions, understanding their usability and security\nperception from an end-user perspective has become crucial. This study\nempirically evaluates and compares two major e-signature systems -- token-based\nand remote signatures -- through a controlled user experience study with 20\nparticipants. Participants completed tasks involving acquisition, installation,\nand document signing using both methods, followed by structured surveys and\nqualitative feedback. Statistical analyses revealed that remote e-signatures\nwere perceived as significantly more usable than token-based ones, due to their\nminimal setup and platform-independent accessibility. In contrast, token-based\nsignatures were rated as significantly more secure, highlighting users' trust\nin hardware-based protection. Although more participants preferred remote\ne-signatures for document signing, the preference did not reach statistical\nsignificance, indicating a trend toward favoring convenience in real-world\nscenarios. These findings underline the fundamental trade-off between usability\nand perceived security in digital signing systems. By bridging the gap between\ntheoretical frameworks and real user experience, this study contributes\nvaluable insights to the design and policymaking of qualified electronic\nsignature solutions."}
{"id": "2505.18440", "pdf": "https://arxiv.org/pdf/2505.18440.pdf", "abs": "https://arxiv.org/abs/2505.18440", "title": "Efficient Long CoT Reasoning in Small Language Models", "authors": ["Zhaoyang Wang", "Jinqi Jiang", "Tian Qiu", "Hui Liu", "Xianfeng Tang", "Huaxiu Yao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps."}
{"id": "2505.18829", "pdf": "https://arxiv.org/pdf/2505.18829.pdf", "abs": "https://arxiv.org/abs/2505.18829", "title": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS", "authors": ["Kai Mei", "Xi Zhu", "Hang Gao", "Shuhang Lin", "Yongfeng Zhang"], "categories": ["cs.AI", "cs.HC", "cs.OS"], "comment": null, "summary": "We present AIOS 1.0, a novel platform designed to advance computer-use agent\n(CUA) capabilities through environmental contextualization. While existing\napproaches primarily focus on building more powerful agent frameworks or\nenhancing agent models, we identify a fundamental limitation: the semantic\ndisconnect between how language models understand the world and how computer\ninterfaces are structured. AIOS 1.0 addresses this challenge by transforming\ncomputers into contextual environments that language models can natively\ncomprehend, implementing a Model Context Protocol (MCP) server architecture to\nabstract computer states and actions. This approach effectively decouples\ninterface complexity from decision complexity, enabling agents to reason more\neffectively about computing environments. To demonstrate our platform's\neffectiveness, we introduce LiteCUA, a lightweight computer-use agent built on\nAIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,\noutperforming several specialized agent frameworks despite its simple\narchitecture. Our results suggest that contextualizing computer environments\nfor language models represents a promising direction for developing more\ncapable computer-use agents and advancing toward AI that can interact with\ndigital systems. The source code of LiteCUA is available at\nhttps://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS\nmain branch as part of AIOS at https://github.com/agiresearch/AIOS."}
{"id": "2505.18450", "pdf": "https://arxiv.org/pdf/2505.18450.pdf", "abs": "https://arxiv.org/abs/2505.18450", "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph", "authors": ["Ainulla Khan", "Yamada Moyuru", "Srinidhi Akella"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising technique to\nenhance the quality and relevance of responses generated by large language\nmodels. While recent advancements have mainly focused on improving RAG for\ntext-based queries, RAG on multi-modal documents containing both texts and\nimages has not been fully explored. Especially when fine-tuning does not work.\nThis paper proposes BRIT, a novel multi-modal RAG framework that effectively\nunifies various text-image connections in the document into a multi-modal graph\nand retrieves the texts and images as a query-specific sub-graph. By traversing\nboth image-to-text and text-to-image paths in the graph, BRIT retrieve not only\ndirectly query-relevant images and texts but also further relevant contents to\nanswering complex cross-modal multi-hop questions. To evaluate the\neffectiveness of BRIT, we introduce MM-RAG test set specifically designed for\nmulti-modal question answering tasks that require to understand the text-image\nrelations. Our comprehensive experiments demonstrate the superiority of BRIT,\nhighlighting its ability to handle cross-modal questions on the multi-modal\ndocuments."}
{"id": "2505.19294", "pdf": "https://arxiv.org/pdf/2505.19294.pdf", "abs": "https://arxiv.org/abs/2505.19294", "title": "Towards Reliable Large Audio Language Model", "authors": ["Ziyang Ma", "Xiquan Li", "Yakun Song", "Wenxi Chen", "Chenpeng Du", "Jian Wu", "Yuanzhe Chen", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.MM", "eess.AS"], "comment": "ACL 2025 Findings", "summary": "Recent advancements in large audio language models (LALMs) have demonstrated\nimpressive results and promising prospects in universal understanding and\nreasoning across speech, music, and general sound. However, these models still\nlack the ability to recognize their knowledge boundaries and refuse to answer\nquestions they don't know proactively. While there have been successful\nattempts to enhance the reliability of LLMs, reliable LALMs remain largely\nunexplored. In this paper, we systematically investigate various approaches\ntowards reliable LALMs, including training-free methods such as multi-modal\nchain-of-thought (MCoT), and training-based methods such as supervised\nfine-tuning (SFT). Besides, we identify the limitations of previous evaluation\nmetrics and propose a new metric, the Reliability Gain Index (RGI), to assess\nthe effectiveness of different reliable methods. Our findings suggest that both\ntraining-free and training-based methods enhance the reliability of LALMs to\ndifferent extents. Moreover, we find that awareness of reliability is a \"meta\nability\", which can be transferred across different audio modalities, although\nsignificant structural and content differences exist among sound, music, and\nspeech."}
{"id": "2505.18452", "pdf": "https://arxiv.org/pdf/2505.18452.pdf", "abs": "https://arxiv.org/abs/2505.18452", "title": "MedScore: Factuality Evaluation of Free-Form Medical Answers", "authors": ["Heyuan Huang", "Alexandra DeLucia", "Vijay Murari Tiyyala", "Mark Dredze"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent and convincing\nresponses, they are not necessarily correct. This is especially apparent in the\npopular decompose-then-verify factuality evaluation pipeline, where LLMs\nevaluate generations by decomposing the generations into individual, valid\nclaims. Factuality evaluation is especially important for medical answers,\nsince incorrect medical information could seriously harm the patient. However,\nexisting factuality systems are a poor match for the medical domain, as they\nare typically only evaluated on objective, entity-centric, formulaic texts such\nas biographies and historical topics. This differs from condition-dependent,\nconversational, hypothetical, sentence-structure diverse, and subjective\nmedical answers, which makes decomposition into valid facts challenging. We\npropose MedScore, a new approach to decomposing medical answers into\ncondition-aware valid facts. Our method extracts up to three times more valid\nfacts than existing methods, reducing hallucination and vague references, and\nretaining condition-dependency in facts. The resulting factuality score\nsignificantly varies by decomposition method, verification corpus, and used\nbackbone LLM, highlighting the importance of customizing each step for reliable\nfactuality evaluation."}
{"id": "2505.19317", "pdf": "https://arxiv.org/pdf/2505.19317.pdf", "abs": "https://arxiv.org/abs/2505.19317", "title": "Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics", "authors": ["Tin Nguyen", "Jiannan Xu", "Zora Che", "Phuong-Anh Nguyen-Le", "Rushil Dandamudi", "Donald Braman", "Furong Huang", "Hal Daumé III", "Zubin Jelveh"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "Although popularized AI fairness metrics, e.g., demographic parity, have\nuncovered bias in AI-assisted decision-making outcomes, they do not consider\nhow much effort one has spent to get to where one is today in the input feature\nspace. However, the notion of effort is important in how Philosophy and humans\nunderstand fairness. We propose a philosophy-informed way to conceptualize and\nevaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal\ntrajectory of predictive features coupled with inertia. In addition to our\ntheoretical formulation of EaF metrics, our empirical contributions include: 1/\na pre-registered human subjects experiment, which demonstrates that for both\nstages of the (individual) fairness evaluation process, people consider the\ntemporal trajectory of a predictive feature more than its aggregate value; 2/\npipelines to compute Effort-aware Individual/Group Fairness in the criminal\njustice and personal finance contexts. Our work may enable AI model auditors to\nuncover and potentially correct unfair decisions against individuals who spent\nsignificant efforts to improve but are still stuck with systemic/early-life\ndisadvantages outside their control."}
{"id": "2505.18454", "pdf": "https://arxiv.org/pdf/2505.18454.pdf", "abs": "https://arxiv.org/abs/2505.18454", "title": "Hybrid Latent Reasoning via Reinforcement Learning", "authors": ["Zhenrui Yue", "Bowen Jin", "Huimin Zeng", "Honglei Zhuang", "Zhen Qin", "Jinsung Yoon", "Lanyu Shang", "Jiawei Han", "Dong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897.pdf", "abs": "https://arxiv.org/abs/2505.19897", "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.18456", "pdf": "https://arxiv.org/pdf/2505.18456.pdf", "abs": "https://arxiv.org/abs/2505.18456", "title": "Anchored Diffusion Language Model", "authors": ["Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches"}
{"id": "2505.20011", "pdf": "https://arxiv.org/pdf/2505.20011.pdf", "abs": "https://arxiv.org/abs/2505.20011", "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments", "authors": ["Maciej Świechowski", "Dominik Ślęzak"], "categories": ["cs.AI", "cs.HC", "cs.MM", "68T01", "I.2; I.6.0; H.1.2"], "comment": "In proceedings of the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23,\n  Detroit, Michigan, USA", "summary": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players."}
{"id": "2505.18466", "pdf": "https://arxiv.org/pdf/2505.18466.pdf", "abs": "https://arxiv.org/abs/2505.18466", "title": "Measuring South Asian Biases in Large Language Models", "authors": ["Mamnuya Rinki", "Chahat Raj", "Anjishnu Mukherjee", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluations of Large Language Models (LLMs) often overlook intersectional and\nculturally specific biases, particularly in underrepresented multilingual\nregions like South Asia. This work addresses these gaps by conducting a\nmultilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan\nand Dravidian languages, identifying how cultural stigmas influenced by purdah\nand patriarchy are reinforced in generative tasks. We construct a culturally\ngrounded bias lexicon capturing previously unexplored intersectional dimensions\nincluding gender, religion, marital status, and number of children. We use our\nlexicon to quantify intersectional bias and the effectiveness of self-debiasing\nin open-ended generations (e.g., storytelling, hobbies, and to-do lists), where\nbias manifests subtly and remains largely unexamined in multilingual contexts.\nFinally, we evaluate two self-debiasing strategies (simple and complex prompts)\nto measure their effectiveness in reducing culturally specific bias in\nIndo-Aryan and Dravidian languages. Our approach offers a nuanced lens into\ncultural bias by introducing a novel bias lexicon and evaluation framework that\nextends beyond Eurocentric or small-scale multilingual settings."}
{"id": "2405.07960", "pdf": "https://arxiv.org/pdf/2405.07960.pdf", "abs": "https://arxiv.org/abs/2405.07960", "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments", "authors": ["Samuel Schmidgall", "Rojin Ziaei", "Carl Harris", "Eduardo Reis", "Jeffrey Jopling", "Michael Moor"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Evaluating large language models (LLM) in clinical scenarios is crucial to\nassessing their potential clinical utility. Existing benchmarks rely heavily on\nstatic question-answering, which does not accurately depict the complex,\nsequential nature of clinical decision-making. Here, we introduce AgentClinic,\na multimodal agent benchmark for evaluating LLMs in simulated clinical\nenvironments that include patient interactions, multimodal data collection\nunder incomplete information, and the usage of various tools, resulting in an\nin-depth evaluation across nine medical specialties and seven languages. We\nfind that solving MedQA problems in the sequential decision-making format of\nAgentClinic is considerably more challenging, resulting in diagnostic\naccuracies that can drop to below a tenth of the original accuracy. Overall, we\nobserve that agents sourced from Claude-3.5 outperform other LLM backbones in\nmost settings. Nevertheless, we see stark differences in the LLMs' ability to\nmake use of tools, such as experiential learning, adaptive retrieval, and\nreflection cycles. Strikingly, Llama-3 shows up to 92% relative improvements\nwith the notebook tool that allows for writing and editing notes that persist\nacross cases. To further scrutinize our clinical simulations, we leverage\nreal-world electronic health records, perform a clinical reader study, perturb\nagents with biases, and explore novel patient-centric metrics that this\ninteractive environment firstly enables."}
{"id": "2505.18486", "pdf": "https://arxiv.org/pdf/2505.18486.pdf", "abs": "https://arxiv.org/abs/2505.18486", "title": "Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek", "authors": ["Hong Jiao", "Dan Song", "Won-Chan Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects."}
{"id": "2405.14341", "pdf": "https://arxiv.org/pdf/2405.14341.pdf", "abs": "https://arxiv.org/abs/2405.14341", "title": "How do Observable Users Decompose D3 Code? A Qualitative Study", "authors": ["Melissa Lin", "Heer Patel", "Medina Lamkin", "Hannah Bako", "Leilani Battle"], "categories": ["cs.HC"], "comment": null, "summary": "Many toolkit developers seek to streamline the visualization programming\nprocess for their users through structured support such as prescribed templates\nand example galleries. However, few projects examine how users organize their\nown visualization programs, how their coding choices may deviate from the\nintents of toolkit developers, and how these differences may impact\nvisualization prototyping and design. Further, is it possible to infer users'\nreasoning indirectly through their code, even when users copy code from other\nsources? Understanding these patterns can reveal opportunities to align toolkit\ndesign with actual user behavior, improving usability and supporting more\nflexible workflows. We explore this question through a qualitative analysis of\n715 D3 programs on Observable. We identify three levels of program organization\nbased on how users decompose their code into smaller blocks: Program-, Chart-,\nand Component-Level code decomposition, with a strong preference for\nComponent-Level reasoning. In a series of interviews, we corroborate that these\nlevels reflect how Observable users reason about visualization programs. We\ncompare common user-made components with those theorized in the Grammar of\nGraphics to assess overlap in user and toolkit developer reasoning. We find\nthat, while the Grammar of Graphics covers basic visualizations well, it falls\nshort in describing complex visualization types, especially those with\nanimation, interaction, and parameterization components. Our findings highlight\nhow user practices differ from formal grammars and suggest opportunities for\nrethinking visualization toolkit support, including augmenting learning tools\nand AI assistants to better reflect real-world coding strategies."}
{"id": "2505.18497", "pdf": "https://arxiv.org/pdf/2505.18497.pdf", "abs": "https://arxiv.org/abs/2505.18497", "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms."}
{"id": "2501.10551", "pdf": "https://arxiv.org/pdf/2501.10551.pdf", "abs": "https://arxiv.org/abs/2501.10551", "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing Essays", "authors": ["Andrew Jelson", "Daniel Manesh", "Alice Jang", "Daniel Dunlap", "Sang Won Lee"], "categories": ["cs.HC"], "comment": "19 pages, 10 figures, 2 tables, final preparation for a TOCHI\n  submission", "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."}
{"id": "2505.18522", "pdf": "https://arxiv.org/pdf/2505.18522.pdf", "abs": "https://arxiv.org/abs/2505.18522", "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation", "authors": ["Xin Lu", "Yanyan Zhao", "Si Wei", "Shijin Wang", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs."}
{"id": "2502.19082", "pdf": "https://arxiv.org/pdf/2502.19082.pdf", "abs": "https://arxiv.org/abs/2502.19082", "title": "Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation", "authors": ["JaeWon Kim", "Robert Wolfe", "Ramya Bhagirathi Subramanian", "Mei-Hsuan Lee", "Jessica Colnago", "Alexis Hiniker"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescents heavily rely on social media to build and maintain close\nrelationships, yet current platform designs often make self-disclosure feel\nrisky or uncomfortable. Through a three-part study involving 19 teens aged\n13-18, we identify key barriers to meaningful self-disclosure on social media.\nOur findings reveal that while these adolescents seek casual, frequent sharing\nto strengthen relationships, existing platform norms often discourage such\ninteractions. Based on our co-design interview findings, we propose platform\ndesign ideas to foster a more dynamic and nuanced privacy experience for teen\nsocial media users. We then introduce \\textbf{\\textit{trust-enabled privacy}}\nas a framework that recognizes trust -- whether building or eroding -- as\ncentral to boundary regulation, and foregrounds the role of platform design in\nshaping the very norms and interaction patterns that influence how trust\nunfolds. When trust is supported, boundary regulation becomes more adaptive and\nempowering; when it erodes, users resort to self-censorship or disengagement.\nThis work provides empirical insights and actionable guidelines for designing\nsocial media spaces where teens feel empowered to engage in meaningful\nrelationship-building processes."}
{"id": "2505.18524", "pdf": "https://arxiv.org/pdf/2505.18524.pdf", "abs": "https://arxiv.org/abs/2505.18524", "title": "metaTextGrad: Automatically optimizing language model optimizers", "authors": ["Guowei Xu", "Mert Yuksekgonul", "Carlos Guestrin", "James Zou"], "categories": ["cs.CL"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline."}
{"id": "2503.07586", "pdf": "https://arxiv.org/pdf/2503.07586.pdf", "abs": "https://arxiv.org/abs/2503.07586", "title": "Design for Hope: Cultivating Deliberate Hope in the Face of Complex Societal Challenges", "authors": ["JaeWon Kim", "Jiaying \"Lizzy\" Liu", "Lindsay Popowski", "Cassidy Pyle", "Ahmer Arif", "Gillian R. Hayes", "Alexis Hiniker", "Wendy Ju", "Florian \"Floyd\" Mueller", "Hua Shen", "Sowmya Somanath", "Casey Fiesler", "Yasmine Kotturi"], "categories": ["cs.HC"], "comment": null, "summary": "Design has the potential to cultivate hope in the face of complex societal\nchallenges. These challenges are often addressed through efforts aimed at harm\nreduction and prevention -- essential but sometimes limiting approaches that\ncan unintentionally narrow our collective sense of what is possible. This\none-day, in-person workshop builds on the first Positech Workshop at CSCW 2024\nby offering practical ways to move beyond reactive problem-solving toward\nbuilding capacity for proactive goal setting and generating pathways forward.\nWe explore how collaborative and reflective design methodologies can help\nresearch communities navigate uncertainty, expand possibilities, and foster\nmeaningful change. By connecting design thinking with hope theory, which frames\nhope as the interplay of ``goal-directed,'' ``pathways,'' and ``agentic''\nthinking, we will examine how researchers might chart new directions in the\nface of complexity and constraint. Through hands-on activities including\nproblem reframing, building a shared taxonomy of design methods that align with\nhope theory, and reflecting on what it means to sustain hopeful research\ntrajectories, participants will develop strategies to embed a deliberately\nhopeful approach into their research."}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536.pdf", "abs": "https://arxiv.org/abs/2505.18536", "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs."}
{"id": "2504.04833", "pdf": "https://arxiv.org/pdf/2504.04833.pdf", "abs": "https://arxiv.org/abs/2504.04833", "title": "Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology", "authors": ["Andrea Esposito", "Miriana Calvano", "Antonio Curci", "Francesco Greco", "Rosa Lanzilotti", "Antonio Piccinno"], "categories": ["cs.HC", "cs.AI"], "comment": "Second version (11 pages, 8 of content) [edited to remove unsupported\n  packages]", "summary": "The integration of Artificial Intelligence (AI) in modern society is\ntransforming how individuals perform tasks. In high-risk domains, ensuring\nhuman control over AI systems remains a key design challenge. This article\npresents a novel End-User Development (EUD) approach for black-box AI models,\nenabling users to edit explanations and influence future predictions through\ntargeted interventions. By combining explainability, user control, and model\nadaptability, the proposed method advances Human-Centered AI (HCAI), promoting\na symbiotic relationship between humans and adaptive, user-tailored AI systems."}
{"id": "2505.18542", "pdf": "https://arxiv.org/pdf/2505.18542.pdf", "abs": "https://arxiv.org/abs/2505.18542", "title": "Business as \\textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs", "authors": ["Chen Yang", "Ruping Xu", "Ruizhe Li", "Bin Cao", "Jing Fan"], "categories": ["cs.CL"], "comment": null, "summary": "Process mining aims to discover, monitor and optimize the actual behaviors of\nreal processes. While prior work has mainly focused on extracting procedural\naction flows from instructional texts, rule flows embedded in business\ndocuments remain underexplored. To this end, we introduce a novel annotated\nChinese dataset, \\textbf{BPRF}, which contains 50 business process documents\nwith 326 explicitly labeled business rules across multiple domains. Each rule\nis represented as a <Condition, Action> pair, and we annotate logical\ndependencies between rules (sequential, conditional, or parallel). We also\npropose \\textbf{ExIde}, a framework for automatic business rule extraction and\ndependency relationship identification using large language models (LLMs). We\nevaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,\nbenchmarking performance on both rule extraction and dependency classification\ntasks of current LLMs. Our results demonstrate the effectiveness of ExIde in\nextracting structured business rules and analyzing their interdependencies for\ncurrent SOTA LLMs, paving the way for more automated and interpretable business\nprocess automation."}
{"id": "2504.12830", "pdf": "https://arxiv.org/pdf/2504.12830.pdf", "abs": "https://arxiv.org/abs/2504.12830", "title": "A Taxonomy of Questions for Critical Reflection in Machine-Assisted Decision-Making", "authors": ["Simon W. S. Fischer", "Hanna Schraffenberger", "Serge Thill", "Pim Haselager"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Decision-makers run the risk of relying too much on machine recommendations,\nwhich is associated with lower cognitive engagement. Reflection has been shown\nto increase cognitive engagement and improve critical thinking and reasoning\nand therefore decision-making. However, there is currently no approach to\nsupport reflection in machine-assisted decision-making. We therefore present a\ntaxonomy that serves to systematically create questions related to\nmachine-assisted decision-making that promote reflection and thus cognitive\nengagement and ultimately a deliberate decision-making process. Our taxonomy\nbuilds on a taxonomy of Socratic questions and a question bank for\nhuman-centred explainable AI (XAI), and illustrates how XAI techniques can be\nutilised and repurposed to formulate questions. As a use case, we focus on\nclinical decision-making. An evaluation in education confirms the applicability\nand expected benefits of our taxonomy. Our work contributes to the growing\nresearch on human-AI interaction that goes beyond the paradigm of machine\nrecommendations and explanations and aims to enable effective human oversight\nas required by the European AI Act."}
{"id": "2505.18548", "pdf": "https://arxiv.org/pdf/2505.18548.pdf", "abs": "https://arxiv.org/abs/2505.18548", "title": "Composable Cross-prompt Essay Scoring by Merging Models", "authors": ["Sanwoo Lee", "Kun Liang", "Yunfang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in cross-prompt automated essay scoring (AES) typically train\nmodels jointly on all source prompts, often requiring additional access to\nunlabeled target prompt essays simultaneously. However, using all sources is\nsuboptimal in our pilot study, and re-accessing source datasets during\nadaptation raises privacy concerns. We propose a source-free adaptation\napproach that selectively merges individually trained source models' parameters\ninstead of datasets. In particular, we simulate joint training through linear\ncombinations of task vectors -- the parameter updates from fine-tuning. To\noptimize the combination's coefficients, we propose Prior-encoded Information\nMaximization (PIM), an unsupervised objective which promotes the model's score\ndiscriminability regularized by priors pre-computed from the sources. We employ\nBayesian optimization as an efficient optimizer of PIM. Experimental results\nwith LLMs on in-dataset and cross-dataset adaptation show that our method (1)\nconsistently outperforms training jointly on all sources, (2) maintains\nsuperior robustness compared to other merging methods, (3) excels under severe\ndistribution shifts where recent leading cross-prompt methods struggle, all\nwhile retaining computational efficiency."}
{"id": "2505.07214", "pdf": "https://arxiv.org/pdf/2505.07214.pdf", "abs": "https://arxiv.org/abs/2505.07214", "title": "Towards user-centered interactive medical image segmentation in VR with an assistive AI agent", "authors": ["Pascal Spiegler", "Arash Harirpoush", "Yiming Xiao"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from user\nfeedback. Therefore, with the complementary power of the latest radiological AI\nfoundation models and virtual reality (VR)'s intuitive data interaction, we\npropose SAMIRA, a novel conversational AI agent for medical VR that assists\nusers with localizing, segmenting, and visualizing 3D medical concepts. Through\nspeech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks."}
{"id": "2505.18549", "pdf": "https://arxiv.org/pdf/2505.18549.pdf", "abs": "https://arxiv.org/abs/2505.18549", "title": "MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors", "authors": ["Baraa Hikal", "Mohamed Basem", "Islam Oshallah", "Ali Hamdi"], "categories": ["cs.CL"], "comment": null, "summary": "We present MSA-MathEval, our submission to the BEA 2025 Shared Task on\nevaluating AI tutor responses across four instructional dimensions: Mistake\nIdentification, Mistake Location, Providing Guidance, and Actionability. Our\napproach uses a unified training pipeline to fine-tune a single\ninstruction-tuned language model across all tracks, without any task-specific\narchitectural changes. To improve prediction reliability, we introduce a\ndisagreement-aware ensemble inference strategy that enhances coverage of\nminority labels. Our system achieves strong performance across all tracks,\nranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both\nMistake Identification and Mistake Location. These results demonstrate the\neffectiveness of scalable instruction tuning and disagreement-driven modeling\nfor robust, multi-dimensional evaluation of LLMs as educational tutors."}
{"id": "2505.16254", "pdf": "https://arxiv.org/pdf/2505.16254.pdf", "abs": "https://arxiv.org/abs/2505.16254", "title": "Reassessing Collaborative Writing Theories and Frameworks in the Age of LLMs: What Still Applies and What We Must Leave Behind", "authors": ["Daisuke Yukita", "Tim Miller", "Joel Mackenzie"], "categories": ["cs.HC"], "comment": null, "summary": "In this paper, we conduct a critical review of existing theories and\nframeworks on human-human collaborative writing to assess their relevance to\nthe current human-AI paradigm in professional contexts, and draw seven insights\nalong with design implications for human-AI collaborative writing tools. We\nfound that, as LLMs nudge the writing process more towards an empirical \"trial\nand error\" process analogous to prototyping, the non-linear cognitive process\nof writing will stay the same, but more rigor will be required for revision\nmethodologies. This shift would shed further light on the importance of\ncoherence support, but the large language model (LLM)'s unprecedented semantic\ncapabilities can bring novel approaches to this ongoing challenge. We argue\nthat teamwork-related factors such as group awareness, consensus building and\nauthorship - which have been central in human-human collaborative writing\nstudies - should not apply to the human-AI paradigm due to excessive\nanthropomorphism. With the LLM's text generation capabilities becoming\nessentially indistinguishable from human-written ones, we are entering an era\nwhere, for the first time in the history of computing, we are engaging in\ncollaborative writing with AI at workplaces on a daily basis. We aim to bring\ntheoretical grounding and practical design guidance to the interaction designs\nof human-AI collaborative writing, with the goal of enhancing future human-AI\nwriting software."}
{"id": "2505.18555", "pdf": "https://arxiv.org/pdf/2505.18555.pdf", "abs": "https://arxiv.org/abs/2505.18555", "title": "Unraveling Misinformation Propagation in LLM Reasoning", "authors": ["Yiyang Feng", "Yichen Wang", "Shaobo Cui", "Boi Faltings", "Mina Lee", "Jiawei Zhou"], "categories": ["cs.CL"], "comment": "24 pages, 14 figures, 4 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%). Further analysis shows that applying factual corrections early in the\nreasoning process most effectively reduces misinformation propagation, and\nfine-tuning on synthesized data with early-stage corrections significantly\nimproves reasoning factuality. Our work offers a practical approach to\nmitigating misinformation propagation."}
{"id": "2505.17557", "pdf": "https://arxiv.org/pdf/2505.17557.pdf", "abs": "https://arxiv.org/abs/2505.17557", "title": "Novobo: Supporting Teachers' Peer Learning of Instructional Gestures by Teaching a Mentee AI-Agent Together", "authors": ["Jiaqi Jiang", "Kexin Huang", "Roberto Martinez-Maldonado", "Huan Zeng", "Duo Gong", "Pengcheng An"], "categories": ["cs.HC"], "comment": "Corrected the spelling of the second author's name (from \"Kexin\n  Huanga\" to \"Kexin Huang\"). No changes to the content. The submitted\n  manuscript is 52 pages in total, with 39 pages of main content (excluding\n  references). It contains 6 figures and 4 tables. A video demonstration is\n  included via a footnote link in the manuscript", "summary": "Instructional gestures are essential for teaching, as they enhance\ncommunication and support student comprehension. However, existing training\nmethods for developing these embodied skills can be time-consuming, isolating,\nor overly prescriptive. Research suggests that developing these tacit,\nexperiential skills requires teachers' peer learning, where they learn from\neach other and build shared knowledge. This paper introduces Novobo, an\napprentice AI-agent stimulating teachers' peer learning of instructional\ngestures through verbal and bodily inputs. Positioning the AI as a mentee\nemploys the learning-by-teaching paradigm, aiming to promote deliberate\nreflection and active learning. Novobo encourages teachers to evaluate its\ngenerated gestures and invite them to provide demonstrations. An evaluation\nwith 30 teachers in 10 collaborative sessions showed Novobo prompted teachers\nto share tacit knowledge through conversation and movement. This process helped\nteachers externalize, exchange, and internalize their embodied knowledge,\npromoting collaborative learning and building a shared understanding of\ninstructional gestures within the local teaching community. This work advances\nunderstanding of how teachable AI agents can enhance collaborative learning in\nteacher professional development, offering valuable design insights for\nleveraging AI to promote the sharing and construction of embodied and practical\nknowledge."}
{"id": "2505.18556", "pdf": "https://arxiv.org/pdf/2505.18556.pdf", "abs": "https://arxiv.org/abs/2505.18556", "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation", "authors": ["Jun Zhuang", "Haibo Jin", "Ye Zhang", "Zhengjian Kang", "Wenbin Zhang", "Gaby G. Dagher", "Haohan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review. TL;DR: We propose a new two-stage\n  intent-based prompt-refinement framework, IntentPrompt, that aims to explore\n  the vulnerability of LLMs' content moderation guardrails by refining prompts\n  into benign-looking declarative forms via intent manipulation for red-teaming\n  purposes", "summary": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails."}
{"id": "2312.03187", "pdf": "https://arxiv.org/pdf/2312.03187.pdf", "abs": "https://arxiv.org/abs/2312.03187", "title": "FERGI: Automatic Scoring of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction", "authors": ["Shuangquan Feng", "Junhua Ma", "Virginia R. de Sa"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically score user preferences\nfrom their spontaneous facial expression reaction to the generated images. We\ncollect a dataset of Facial Expression Reaction to Generated Images (FERGI) and\nshow that the activations of multiple facial action units (AUs) are highly\ncorrelated with user evaluations of the generated images. We develop an FAU-Net\n(Facial Action Units Neural Network), which receives inputs from an AU\nestimation model, to automatically score user preferences for text-to-image\ngeneration based on their facial expression reactions, which is complementary\nto the pre-trained scoring models based on the input text prompts and generated\nimages. Integrating our FAU-Net valence score with the pre-trained scoring\nmodels improves their consistency with human preferences. This method of\nautomatic annotation with facial expression analysis can be potentially\ngeneralized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes."}
{"id": "2505.18557", "pdf": "https://arxiv.org/pdf/2505.18557.pdf", "abs": "https://arxiv.org/abs/2505.18557", "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation", "authors": ["He Zhu", "Zhiwen Ruan", "Junyou Su", "Xingwei He", "Wenjia Zhang", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks."}
{"id": "2501.13836", "pdf": "https://arxiv.org/pdf/2501.13836.pdf", "abs": "https://arxiv.org/abs/2501.13836", "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Most social media users come from non-English speaking countries in the\nGlobal South, where much of harmful content appears in local languages. Yet,\ncurrent AI-driven moderation systems struggle with low-resource languages\nspoken in these regions. This work examines the systemic challenges in building\nautomated moderation tools for these languages. We conducted semi-structured\ninterviews with 22 AI experts working on detecting harmful content in four\nlow-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi\nArabic (North Africa), and Quechua (South America). Our findings show that\nbeyond the well-known data scarcity in local languages, technical issues--such\nas outdated machine translation systems, sentiment and toxicity models grounded\nin Western values, and unreliable language detection technologies--undermine\nmoderation efforts. Even with more data, current language models and\npreprocessing pipelines--primarily designed for English--struggle with the\nmorphological richness, linguistic complexity, and code-mixing. As a result,\nautomated moderation in Tamil, Swahili, Arabic, and Quechua remains fraught\nwith inaccuracies and blind spots. Based on our findings, we argue that these\nlimitations are not just technical gaps but reflect deeper structural\ninequities that continue to reproduce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve automated moderation for\nlow-resource languages."}
{"id": "2505.18562", "pdf": "https://arxiv.org/pdf/2505.18562.pdf", "abs": "https://arxiv.org/abs/2505.18562", "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test", "authors": ["Xunlian Dai", "Li Zhou", "Benyou Wang", "Haizhou Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies."}
{"id": "2502.10533", "pdf": "https://arxiv.org/pdf/2502.10533.pdf", "abs": "https://arxiv.org/abs/2502.10533", "title": "Expert-Agnostic Learning to Defer", "authors": ["Joshua Strong", "Pramit Saha", "Yasin Ibrahim", "Cheng Ouyang", "Alison Noble"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Learning to Defer (L2D) trains autonomous systems to handle straightforward\ncases while deferring uncertain ones to human experts. Recent advancements in\nthis field have introduced methods that offer flexibility to unseen experts at\ntest time. However, we find these approaches struggle to generalise to experts\nwith behaviours not seen during training, require extensive human annotation,\nand lack mechanisms for incorporating prior knowledge of expert capabilities.\nTo address these challenges, we introduce Expert-Agnostic Learning to Defer\n(EA-L2D), a novel L2D framework that employs a Bayesian approach to model\nexpert behaviour in an \\textit{expert-agnostic} fashion. Across benchmark\nmedical imaging datasets (HAM10000, Blood Cells, Retinal OCT, and Liver\nTumours), EA-L2D significantly outperforms prior methods on unseen experts,\nachieving up to a 28\\% relative improvement, while also matching or exceeding\nstate-of-the-art performance on seen experts."}
{"id": "2505.18581", "pdf": "https://arxiv.org/pdf/2505.18581.pdf", "abs": "https://arxiv.org/abs/2505.18581", "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "authors": ["Wentao Hu", "Wengyu Zhang", "Yiyang Jiang", "Chen Jason Zhang", "Xiaoyong Wei", "Qing Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG."}
{"id": "2502.19679", "pdf": "https://arxiv.org/pdf/2502.19679.pdf", "abs": "https://arxiv.org/abs/2502.19679", "title": "Architectural Vulnerability and Reliability Challenges in AI Text Annotation: A Survey-Inspired Framework with Independent Probability Assessment", "authors": ["Linzhuo li"], "categories": ["cs.DL", "cs.HC"], "comment": "7 figures", "summary": "Large Language Models, despite their power, have a fundamental architectural\nvulnerability stemming from their causal transformer design -- order\nsensitivity. This architectural constraint may distorts classification outcomes\nwhen prompt elements like label options are reordered, revealing a theoretical\ngap between accuracy metrics and true model reliability. The paper\nconceptualizes this vulnerability through the lens of survey methodology, where\nrespondent biases parallel LLM positional dependencies. Empirical evidence\nusing the F1000 biomedical dataset across three scales of LLaMA3.1 models (8B,\n70B, 405B) demonstrates that these architectural constraints produce\ninconsistent annotations under controlled perturbations. The paper advances a\npractical solution for social science - Independent Probability Assessment -\nwhich decouples label evaluation to circumvent positional bias inherent in\nsequential processing. This approach yields an information-theoretic\nreliability measure (R-score) that quantifies annotation robustness at the case\nlevel. The findings establish that architectural vulnerabilities in causal\ntransformers require methodological innovations beyond accuracy metrics to\nensure valid social science inference, as demonstrated through downstream\nregression analyses where order-sensitive annotations significantly alter\nsubstantive conclusions about scientific impact."}
{"id": "2505.18588", "pdf": "https://arxiv.org/pdf/2505.18588.pdf", "abs": "https://arxiv.org/abs/2505.18588", "title": "Safety Alignment via Constrained Knowledge Unlearning", "authors": ["Zesheng Shi", "Yucheng Zhou", "Jing Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing."}
{"id": "2504.16728", "pdf": "https://arxiv.org/pdf/2504.16728.pdf", "abs": "https://arxiv.org/abs/2504.16728", "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "ACL 2025 (System Demonstration Track)", "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System"}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596.pdf", "abs": "https://arxiv.org/abs/2505.18596", "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release."}
{"id": "2505.02329", "pdf": "https://arxiv.org/pdf/2505.02329.pdf", "abs": "https://arxiv.org/abs/2505.02329", "title": "Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling", "authors": ["Jonathan Lynn", "Rachel Y. Kim", "Sicun Gao", "Daniel Schneider", "Sachin S. Pandya", "Min Kyung Lee"], "categories": ["cs.CY", "cs.HC", "cs.SE"], "comment": "To appear in FAccT'25", "summary": "Algorithmic management (AM)'s impact on worker well-being has led to calls\nfor regulation. However, little is known about the effectiveness and challenges\nin real-world AM regulation across the regulatory process -- rule\noperationalization, software use, and enforcement. Our multi-stakeholder study\naddresses this gap within workplace scheduling, one of the few AM domains with\nimplemented regulations. We interviewed 38 stakeholders across the regulatory\nprocess: regulators, defense attorneys, worker advocates, managers, and\nworkers. Our findings suggest that the efficacy of AM regulation is influenced\nby: (i) institutional constraints that challenge efforts to encode law into AM\nsoftware, (ii) on-the-ground use of AM software that shapes its ability to\nfacilitate compliance, (iii) mismatches between software and regulatory\ncontexts that hinder enforcement, and (iv) unique concerns that software\nintroduces when used to regulate AM. These findings underscore the importance\nof a sociotechnical approach to AM regulation, which considers organizational\nand collaborative contexts alongside the inherent attributes of software. We\noffer future research directions and implications for technology policy and\ndesign."}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601.pdf", "abs": "https://arxiv.org/abs/2505.18601", "title": "Flex-Judge: Think Once, Judge Anywhere", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge."}
{"id": "2505.09868", "pdf": "https://arxiv.org/pdf/2505.09868.pdf", "abs": "https://arxiv.org/abs/2505.09868", "title": "Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?", "authors": ["Tin Trung Nguyen", "Jiannan Xu", "Phuong-Anh Nguyen-Le", "Jonathan Lazar", "Donald Braman", "Hal Daumé III", "Zubin Jelveh"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Despite its constitutional relevance, the technical ``individual fairness''\ncriterion has not been operationalized in U.S. state or federal\nstatutes/regulations. We conduct a human subjects experiment to address this\ngap, evaluating which demographic features are relevant for individual fairness\nevaluation of recidivism risk assessment (RRA) tools. Our analyses conclude\nthat the individual similarity function should consider age and sex, but it\nshould ignore race."}
{"id": "2505.18609", "pdf": "https://arxiv.org/pdf/2505.18609.pdf", "abs": "https://arxiv.org/abs/2505.18609", "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations", "authors": ["Ashwin Sankar", "Yoach Lacombe", "Sherry Thomas", "Praveen Srinivasa Varadhan", "Sanchit Gandhi", "Mitesh M Khapra"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408.pdf", "abs": "https://arxiv.org/abs/2505.12408", "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.18610", "pdf": "https://arxiv.org/pdf/2505.18610.pdf", "abs": "https://arxiv.org/abs/2505.18610", "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs", "authors": ["Tengxuan Liu", "Shiyao Li", "Jiayi Yang", "Tianchen Zhao", "Feng Zhou", "Xiaohui Song", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."}
{"id": "2505.15946", "pdf": "https://arxiv.org/pdf/2505.15946.pdf", "abs": "https://arxiv.org/abs/2505.15946", "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain."}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614.pdf", "abs": "https://arxiv.org/abs/2505.18614", "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "28 pages, 8 figures", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."}
{"id": "2505.17343", "pdf": "https://arxiv.org/pdf/2505.17343.pdf", "abs": "https://arxiv.org/abs/2505.17343", "title": "Ocular Authentication: Fusion of Gaze and Periocular Modalities", "authors": ["Dillon Lohr", "Michael J. Proulx", "Mehedi Hasan Raju", "Oleg V. Komogortsev"], "categories": ["cs.CV", "cs.HC"], "comment": "Supplementary material is available", "summary": "This paper investigates the feasibility of fusing two eye-centric\nauthentication modalities-eye movements and periocular images-within a\ncalibration-free authentication system. While each modality has independently\nshown promise for user authentication, their combination within a unified\ngaze-estimation pipeline has not been thoroughly explored at scale. In this\nreport, we propose a multimodal authentication system and evaluate it using a\nlarge-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)\nsignal quality equivalent to a consumer-facing virtual reality (VR) device. Our\nresults show that the multimodal approach consistently outperforms both\nunimodal systems across all scenarios, surpassing the FIDO benchmark. The\nintegration of a state-of-the-art machine learning architecture contributed\nsignificantly to the overall authentication performance at scale, driven by the\nmodel's ability to capture authentication representations and the complementary\ndiscriminative characteristics of the fused modalities."}
{"id": "2505.18630", "pdf": "https://arxiv.org/pdf/2505.18630.pdf", "abs": "https://arxiv.org/abs/2505.18630", "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "authors": ["Zhihao Jia", "Mingyi Jia", "Junwen Duan", "Jianxin Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 4 figures", "summary": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task."}
{"id": "2505.18638", "pdf": "https://arxiv.org/pdf/2505.18638.pdf", "abs": "https://arxiv.org/abs/2505.18638", "title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models", "authors": ["Md. Tanzib Hosain", "Rajan Das Gupta", "Md. Kishor Morol"], "categories": ["cs.CL"], "comment": "24 pages, 20 figures", "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation."}
{"id": "2505.18642", "pdf": "https://arxiv.org/pdf/2505.18642.pdf", "abs": "https://arxiv.org/abs/2505.18642", "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "authors": ["Xiao Chen", "Sihang Zhou", "Ke Liang", "Xiaoyu Sun", "Xinwang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks."}
{"id": "2505.18651", "pdf": "https://arxiv.org/pdf/2505.18651.pdf", "abs": "https://arxiv.org/abs/2505.18651", "title": "On the Emergence of Linear Analogies in Word Embeddings", "authors": ["Daniel J. Korchinski", "Dhruva Karkada", "Yasaman Bahri", "Matthieu Wyart"], "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG"], "comment": "Main: 12 pages, 3 figures. Appendices: 8 pages, 7 figures", "summary": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al."}
{"id": "2505.18653", "pdf": "https://arxiv.org/pdf/2505.18653.pdf", "abs": "https://arxiv.org/abs/2505.18653", "title": "Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change", "authors": ["Murathan Kurfalı", "Shorouq Zahra", "Joakim Nivre", "Gabriele Messori"], "categories": ["cs.CL"], "comment": "Accepted to ClimateNLP 2025@ACL", "summary": "Climate-Eval is a comprehensive benchmark designed to evaluate natural\nlanguage processing models across a broad range of tasks related to climate\nchange. Climate-Eval aggregates existing datasets along with a newly developed\nnews classification dataset, created specifically for this release. This\nresults in a benchmark of 25 tasks based on 13 datasets, covering key aspects\nof climate discourse, including text classification, question answering, and\ninformation extraction. Our benchmark provides a standardized evaluation suite\nfor systematically assessing the performance of large language models (LLMs) on\nthese tasks. Additionally, we conduct an extensive evaluation of open-source\nLLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot\nsettings, analyzing their strengths and limitations in the domain of climate\nchange."}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658.pdf", "abs": "https://arxiv.org/abs/2505.18658", "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."}
{"id": "2505.18673", "pdf": "https://arxiv.org/pdf/2505.18673.pdf", "abs": "https://arxiv.org/abs/2505.18673", "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Xiuying Chen", "Jieyu Zhao", "Meng Jiang", "Xiangliang Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025. Code available at\n  https://github.com/xzx34/Cross-Lingual-Pitfalls", "summary": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls."}
{"id": "2505.18677", "pdf": "https://arxiv.org/pdf/2505.18677.pdf", "abs": "https://arxiv.org/abs/2505.18677", "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts", "authors": ["Eric Chamoun", "Nedjma Ousidhoum", "Michael Schlichtkrull", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "Clarifying the research framing of NLP artefacts (e.g., models, datasets,\netc.) is crucial to aligning research with practical applications. Recent\nstudies manually analyzed NLP research across domains, showing that few papers\nexplicitly identify key stakeholders, intended uses, or appropriate contexts.\nIn this work, we propose to automate this analysis, developing a\nthree-component system that infers research framings by first extracting key\nelements (means, ends, stakeholders), then linking them through interpretable\nrules and contextual reasoning. We evaluate our approach on two domains:\nautomated fact-checking using an existing dataset, and hate speech detection\nfor which we annotate a new dataset-achieving consistent improvements over\nstrong LLM baselines. Finally, we apply our system to recent automated\nfact-checking papers and uncover three notable trends: a rise in vague or\nunderspecified research goals, increased emphasis on scientific exploration\nover application, and a shift toward supporting human fact-checkers rather than\npursuing full automation."}
{"id": "2505.18683", "pdf": "https://arxiv.org/pdf/2505.18683.pdf", "abs": "https://arxiv.org/abs/2505.18683", "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation", "authors": ["Raphaël Merx", "Hanna Suominen", "Lois Hong", "Nick Thieberger", "Trevor Cohn", "Ekaterina Vylomova"], "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) systems that support low-resource languages often\nstruggle on specialized domains. While researchers have proposed various\ntechniques for domain adaptation, these approaches typically require model\nfine-tuning, making them impractical for non-technical users and small\norganizations. To address this gap, we propose Tulun, a versatile solution for\nterminology-aware translation, combining neural MT with large language model\n(LLM)-based post-editing guided by existing glossaries and translation\nmemories. Our open-source web-based platform enables users to easily create,\nedit, and leverage terminology resources, fostering a collaborative\nhuman-machine translation process that respects and incorporates domain\nexpertise while increasing MT accuracy. Evaluations show effectiveness in both\nreal-world and benchmark scenarios: on medical and disaster relief translation\ntasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41\nChrF++ points over baseline MT systems. Across six low-resource languages on\nthe FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,\nachieving an average improvement of 2.8 ChrF points over NLLB-54B."}
{"id": "2505.18685", "pdf": "https://arxiv.org/pdf/2505.18685.pdf", "abs": "https://arxiv.org/abs/2505.18685", "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation", "authors": ["Zhihao Zhang", "Yiran Zhang", "Xiyue Zhou", "Liting Huang", "Imran Razzak", "Preslav Nakov", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Infodemics and health misinformation have significant negative impact on\nindividuals and society, exacerbating confusion and increasing hesitancy in\nadopting recommended health measures. Recent advancements in generative AI,\ncapable of producing realistic, human like text and images, have significantly\naccelerated the spread and expanded the reach of health misinformation,\nresulting in an alarming surge in its dissemination. To combat the infodemics,\nmost existing work has focused on developing misinformation datasets from\nsocial media and fact checking platforms, but has faced limitations in topical\ncoverage, inclusion of AI generation, and accessibility of raw content. To\naddress these issues, we present MM Health, a large scale multimodal\nmisinformation dataset in the health domain consisting of 34,746 news article\nencompassing both textual and visual information. MM Health includes\nhuman-generated multimodal information (5,776 articles) and AI generated\nmultimodal information (28,880 articles) from various SOTA generative AI\nmodels. Additionally, We benchmarked our dataset against three tasks\n(reliability checks, originality checks, and fine-grained AI detection)\ndemonstrating that existing SOTA models struggle to accurately distinguish the\nreliability and origin of information. Our dataset aims to support the\ndevelopment of misinformation detection across various health scenarios,\nfacilitating the detection of human and machine generated content at multimodal\nlevels."}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688.pdf", "abs": "https://arxiv.org/abs/2505.18688", "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "authors": ["Aleksandr Tsymbalov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."}
{"id": "2505.18690", "pdf": "https://arxiv.org/pdf/2505.18690.pdf", "abs": "https://arxiv.org/abs/2505.18690", "title": "Benchmarking and Rethinking Knowledge Editing for Large Language Models", "authors": ["Guoxiu He", "Xin Song", "Futing Wang", "Aixin Sun"], "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.05212", "summary": "Knowledge editing aims to update the embedded knowledge within Large Language\nModels (LLMs). However, existing approaches, whether through parameter\nmodification or external memory integration, often suffer from inconsistent\nevaluation objectives and experimental setups. To address this gap, we conduct\na comprehensive benchmarking study. In addition to fact-level datasets, we\nintroduce more complex event-based datasets and general-purpose datasets drawn\nfrom other tasks. Our evaluation covers both instruction-tuned and\nreasoning-oriented LLMs, under a realistic autoregressive inference setting\nrather than teacher-forced decoding. Beyond single-edit assessments, we also\nevaluate multi-edit scenarios to better reflect practical demands. We employ\nfour evaluation dimensions, including portability, and compare all recent\nmethods against a simple and straightforward baseline named Selective\nContextual Reasoning (SCR). Empirical results reveal that parameter-based\nediting methods perform poorly under realistic conditions. In contrast, SCR\nconsistently outperforms them across all settings. This study offers new\ninsights into the limitations of current knowledge editing methods and\nhighlights the potential of context-based reasoning as a more robust\nalternative."}
{"id": "2505.18703", "pdf": "https://arxiv.org/pdf/2505.18703.pdf", "abs": "https://arxiv.org/abs/2505.18703", "title": "Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task", "authors": ["Gaurav Negi", "Dhairya Dalal", "Omnia Zayed", "Paul Buitelaar"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Unified Opinion Concepts (UOC) ontology to\nintegrate opinions within their semantic context. The UOC ontology bridges the\ngap between the semantic representation of opinion across different\nformulations. It is a unified conceptualisation based on the facets of opinions\nstudied extensively in NLP and semantic structures described through symbolic\ndescriptions. We further propose the Unified Opinion Concept Extraction (UOCE)\ntask of extracting opinions from the text with enhanced expressivity.\nAdditionally, we provide a manually extended and re-annotated evaluation\ndataset for this task and tailored evaluation metrics to assess the adherence\nof extracted opinions to UOC semantics. Finally, we establish baseline\nperformance for the UOCE task using state-of-the-art generative models."}
{"id": "2505.18708", "pdf": "https://arxiv.org/pdf/2505.18708.pdf", "abs": "https://arxiv.org/abs/2505.18708", "title": "A General Knowledge Injection Framework for ICD Coding", "authors": ["Xu Zhang", "Kun Zhang", "Wenxin Ma", "Rongsheng Wang", "Chenxu Wu", "Yingtai Li", "S. Kevin Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD."}
{"id": "2505.18709", "pdf": "https://arxiv.org/pdf/2505.18709.pdf", "abs": "https://arxiv.org/abs/2505.18709", "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla", "authors": ["Sourav Kumar Das", "Md. Julkar Naeen", "MD. Jahidul Islam", "Md. Anisul Haque Sajeeb", "Narayan Ranjan Chakraborty", "Mayen Uddin Mojumdar"], "categories": ["cs.CL", "cs.AI"], "comment": "2024 15th International Conference on Computing Communication and\n  Networking Technologies (ICCCNT)", "summary": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations."}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720.pdf", "abs": "https://arxiv.org/abs/2505.18720", "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}."}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744.pdf", "abs": "https://arxiv.org/abs/2505.18744", "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges", "authors": ["Tao Liu", "Hongying Zan", "Yifan Li", "Dixuan Zhang", "Lulu Kong", "Haixin Liu", "Jiaming Hou", "Aoze Zheng", "Rui Li", "Yiming Qiao", "Zewei Luo", "Qi Wang", "Zhiqiang Zhang", "Jiaxi Li", "Supeng Liu", "Kunli Zhang", "Min Peng"], "categories": ["cs.CL"], "comment": "22 pages, 10 figures", "summary": "Text-to-SQL is a fundamental task in natural language processing that seeks\nto translate natural language questions into meaningful and executable SQL\nqueries. While existing datasets are extensive and primarily focus on business\nscenarios and operational logic, they frequently lack coverage of\ndomain-specific knowledge and complex mathematical reasoning. To address this\ngap, we present a novel dataset tailored for complex reasoning and\nchain-of-thought analysis in SQL inference, encompassing physical, arithmetic,\ncommonsense, and hypothetical reasoning. The dataset consists of 4,038 English\nquestions, each paired with a unique SQL query and accompanied by 12,114\nstep-by-step reasoning annotations, spanning 45 databases across diverse\ndomains. Experimental results demonstrate that LogicCat substantially increases\nthe difficulty for state-of-the-art models, with the highest execution accuracy\nreaching only 14.96%. Incorporating our chain-of-thought annotations boosts\nperformance to 33.96%. Benchmarking leading public methods on Spider and BIRD\nfurther underscores the unique challenges presented by LogicCat, highlighting\nthe significant opportunities for advancing research in robust,\nreasoning-driven text-to-SQL systems. We have released our dataset code at\nhttps://github.com/Ffunkytao/LogicCat."}
{"id": "2505.18752", "pdf": "https://arxiv.org/pdf/2505.18752.pdf", "abs": "https://arxiv.org/abs/2505.18752", "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning", "authors": ["Haolin Yang", "Hakaze Cho", "Yiqiao Zhong", "Naoya Inoue"], "categories": ["cs.CL"], "comment": "45 pages, 49 figures", "summary": "The unusual properties of in-context learning (ICL) have prompted\ninvestigations into the internal mechanisms of large language models. Prior\nwork typically focuses on either special attention heads or task vectors at\nspecific layers, but lacks a unified framework linking these components to the\nevolution of hidden states across layers that ultimately produce the model's\noutput. In this paper, we propose such a framework for ICL in classification\ntasks by analyzing two geometric factors that govern performance: the\nseparability and alignment of query hidden states. A fine-grained analysis of\nlayer-wise dynamics reveals a striking two-stage mechanism: separability\nemerges in early layers, while alignment develops in later layers. Ablation\nstudies further show that Previous Token Heads drive separability, while\nInduction Heads and task vectors enhance alignment. Our findings thus bridge\nthe gap between attention heads and task vectors, offering a unified account of\nICL's underlying mechanisms."}
{"id": "2505.18754", "pdf": "https://arxiv.org/pdf/2505.18754.pdf", "abs": "https://arxiv.org/abs/2505.18754", "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection", "authors": ["Elsen Ronando", "Sozo Inoue"], "categories": ["cs.CL", "I.2.7"], "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors\n  (2025). Final version before journal publication", "summary": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios."}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761.pdf", "abs": "https://arxiv.org/abs/2505.18761", "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."}
{"id": "2505.18762", "pdf": "https://arxiv.org/pdf/2505.18762.pdf", "abs": "https://arxiv.org/abs/2505.18762", "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "authors": ["Michael Flor", "Zuowei Wang", "Paul Deane", "Tenaha O'Reilly"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This manuscript was accepted to be published as an ETS Research\n  Report. Keywords topics; vocabulary; background knowledge; automatic item\n  generation; assessment; reading comprehension", "summary": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs."}
{"id": "2505.18774", "pdf": "https://arxiv.org/pdf/2505.18774.pdf", "abs": "https://arxiv.org/abs/2505.18774", "title": "Disentangling Knowledge Representations for Large Language Model Editing", "authors": ["Mengqi Zhang", "Zisheng Zhou", "Xiaotian Ye", "Qiang Liu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing has emerged as a promising solution for efficiently\nupdating embedded knowledge in large language models (LLMs). While existing\napproaches demonstrate effectiveness in integrating new knowledge and\npreserving the original capabilities of LLMs, they fail to maintain\nfine-grained irrelevant knowledge facts that share the same subject as edited\nknowledge but differ in relation and object. This challenge arises because\nsubject representations inherently encode multiple attributes, causing the\ntarget and fine-grained irrelevant knowledge to become entangled in the\nrepresentation space, and thus vulnerable to unintended alterations during\nediting. To address this, we propose DiKE, a novel approach that Disentangles\nKnowledge representations for LLM Editing (DiKE). DiKE consists of two key\ncomponents: a Knowledge Representation Disentanglement (KRD) module that\ndecomposes the subject representation into target-knowledgerelated and\n-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module\nthat updates only the target-related component while explicitly preserving the\nunrelated one. We further derive a closed-form, rank-one parameter update based\non matrix theory to enable efficient and minimally invasive edits. To\nrigorously evaluate fine-grained irrelevant knowledge preservation, we\nconstruct FINE-KED, a new benchmark comprising fine-grained irrelevant\nknowledge at different levels of relational similarity to the edited knowledge.\nExtensive experiments across multiple LLMs demonstrate that DiKE substantially\nimproves fine-grained irrelevant knowledge preservation while maintaining\ncompetitive general editing performance."}
{"id": "2505.18778", "pdf": "https://arxiv.org/pdf/2505.18778.pdf", "abs": "https://arxiv.org/abs/2505.18778", "title": "A generalised editor calculus (Short Paper)", "authors": ["Benjamin Bennetzen", "Peter Buus Steffensen", "Hans Hüttel", "Nikolaj Rossander Kristensen", "Andreas Tor Mortensen"], "categories": ["cs.CL", "F.2.2, I.2.7"], "comment": "7 pages, 21 figures", "summary": "In this paper, we present a generalization of a syntax-directed editor\ncalculus, which can be used to instantiate a specialized syntax-directed editor\nfor any language, given by some abstract syntax. The editor calculus guarantees\nthe absence of syntactical errors while allowing incomplete programs. The\ngeneralized editor calculus is then encoded into a simply typed lambda\ncalculus, extended with pairs, booleans, pattern matching and fixed points"}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799.pdf", "abs": "https://arxiv.org/abs/2505.18799", "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant costs, including constructing task-specific\ninstruction pairs and extensive training adjustments. Prior research has\nexplored various avenues to enhance alignment efficiency, primarily through\nminimal-data training or data-driven activations to identify key attention\nheads. However, these approaches inherently introduce data dependency, which\nhinders generalization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment."}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842.pdf", "abs": "https://arxiv.org/abs/2505.18842", "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch."}
{"id": "2505.18845", "pdf": "https://arxiv.org/pdf/2505.18845.pdf", "abs": "https://arxiv.org/abs/2505.18845", "title": "Multi-Party Conversational Agents: A Survey", "authors": ["Sagar Sapkota", "Mohammad Saqib Hasan", "Mubarak Shah", "Santu Karmaker"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in\ndialogue with more than two participants simultaneously. Unlike traditional\ntwo-party agents, designing MPCAs faces additional challenges due to the need\nto interpret both utterance semantics and social dynamics. This survey explores\nrecent progress in MPCAs by addressing three key questions: 1) Can agents model\neach participants' mental states? (State of Mind Modeling); 2) Can they\nproperly understand the dialogue content? (Semantic Understanding); and 3) Can\nthey reason about and predict future conversation flow? (Agent Action\nModeling). We review methods ranging from classical machine learning to Large\nLanguage Models (LLMs) and multi-modal systems. Our analysis underscores Theory\nof Mind (ToM) as essential for building intelligent MPCAs and highlights\nmulti-modal understanding as a promising yet underexplored direction. Finally,\nthis survey offers guidance to future researchers on developing more capable\nMPCAs."}
{"id": "2505.18853", "pdf": "https://arxiv.org/pdf/2505.18853.pdf", "abs": "https://arxiv.org/abs/2505.18853", "title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation", "authors": ["Alexander Shabalin", "Viacheslav Meshchaninov", "Dmitry Vetrov"], "categories": ["cs.CL"], "comment": "17 pages, 2 figures, 8 tables", "summary": "Diffusion models have achieved state-of-the-art performance in generating\nimages, audio, and video, but their adaptation to text remains challenging due\nto its discrete nature. Prior approaches either apply Gaussian diffusion in\ncontinuous latent spaces, which inherits semantic structure but struggles with\ntoken decoding, or operate in categorical simplex space, which respect\ndiscreteness but disregard semantic relation between tokens. In this paper, we\npropose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion\nmethod that combines the strengths of both approaches by progressively\nsmoothing token embeddings based on semantic similarity. This technique enables\ngradual information removal while maintaining a natural decoding process.\nExperimental results on several sequence-to-sequence generation tasks\ndemonstrate that Smoothie outperforms existing diffusion-based models in\ngeneration quality. Furthermore, ablation studies show that our proposed\ndiffusion space yields better performance than both the standard embedding\nspace and the categorical simplex. Our code is available at\nhttps://github.com/ashaba1in/smoothie."}
{"id": "2505.18859", "pdf": "https://arxiv.org/pdf/2505.18859.pdf", "abs": "https://arxiv.org/abs/2505.18859", "title": "Writing Like the Best: Exemplar-Based Expository Text Generation", "authors": ["Yuxiang Liu", "Kevin Chen-Chuan Chang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025. Camera-ready version", "summary": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task."}
{"id": "2505.18864", "pdf": "https://arxiv.org/pdf/2505.18864.pdf", "abs": "https://arxiv.org/abs/2505.18864", "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework", "authors": ["Binhao Ma", "Hanqing Guo", "Zhengping Jay Luo", "Rui Duan"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the naturalness and flexibility of human computer\ninteraction by enabling seamless understanding across text, vision, and audio\nmodalities. Among these, voice enabled models such as SpeechGPT have\ndemonstrated considerable improvements in usability, offering expressive, and\nemotionally responsive interactions that foster deeper connections in real\nworld communication scenarios. However, the use of voice introduces new\nsecurity risks, as attackers can exploit the unique characteristics of spoken\nlanguage, such as timing, pronunciation variability, and speech to text\ntranslation, to craft inputs that bypass defenses in ways not seen in\ntext-based systems. Despite substantial research on text based jailbreaks, the\nvoice modality remains largely underexplored in terms of both attack strategies\nand defense mechanisms. In this work, we present an adversarial attack\ntargeting the speech input of aligned MLLMs in a white box scenario.\nSpecifically, we introduce a novel token level attack that leverages access to\nthe model's speech tokenization to generate adversarial token sequences. These\nsequences are then synthesized into audio prompts, which effectively bypass\nalignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,\nour approach achieves up to 89 percent attack success rate across multiple\nrestricted tasks, significantly outperforming existing voice based jailbreak\nmethods. Our findings shed light on the vulnerabilities of voice-enabled\nmultimodal systems and to help guide the development of more robust\nnext-generation MLLMs."}
{"id": "2505.18867", "pdf": "https://arxiv.org/pdf/2505.18867.pdf", "abs": "https://arxiv.org/abs/2505.18867", "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing", "authors": ["Ming Cheng", "Jiaying Gong", "Hoda Eldardiry"], "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, ACL 2025 Findings", "summary": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing."}
{"id": "2505.18878", "pdf": "https://arxiv.org/pdf/2505.18878.pdf", "abs": "https://arxiv.org/abs/2505.18878", "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition."}
{"id": "2505.18903", "pdf": "https://arxiv.org/pdf/2505.18903.pdf", "abs": "https://arxiv.org/abs/2505.18903", "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos", "authors": ["Valentin Barriere", "Nahuel Gomez", "Leo Hemamou", "Sofia Callejas", "Brian Ravenet"], "categories": ["cs.CL"], "comment": null, "summary": "Aiming towards improving current computational models of humor detection, we\npropose a new multimodal dataset of stand-up comedies, in seven languages:\nEnglish, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset\nof more than 330 hours, is at the time of writing the biggest available for\nthis type of task, and the most diverse. The whole dataset is automatically\nannotated in laughter (from the audience), and the subpart left for model\nvalidation is manually annotated. Contrary to contemporary approaches, we do\nnot frame the task of humor detection as a binary sequence classification, but\nas word-level sequence labeling, in order to take into account all the context\nof the sequence and to capture the continuous joke tagging mechanism typically\noccurring in natural conversations. As par with unimodal baselines results, we\npropose a method for e propose a method to enhance the automatic laughter\ndetection based on Audio Speech Recognition errors. Our code and data are\navailable online: https://tinyurl.com/EMNLPHumourStandUpPublic"}
{"id": "2505.18905", "pdf": "https://arxiv.org/pdf/2505.18905.pdf", "abs": "https://arxiv.org/abs/2505.18905", "title": "Building a Functional Machine Translation Corpus for Kpelle", "authors": ["Kweku Andoh Yamoah", "Jackson Weako", "Emmanuel J. Dorley"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce the first publicly available English-Kpelle\ndataset for machine translation, comprising over 2000 sentence pairs drawn from\neveryday communication, religious texts, and educational materials. By\nfine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the\ndataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English\ndirection, demonstrating the benefits of data augmentation. Our findings align\nwith NLLB-200 benchmarks on other African languages, underscoring Kpelle's\npotential for competitive performance despite its low-resource status. Beyond\nmachine translation, this dataset enables broader NLP tasks, including speech\nrecognition and language modelling. We conclude with a roadmap for future\ndataset expansion, emphasizing orthographic consistency, community-driven\nvalidation, and interdisciplinary collaboration to advance inclusive language\ntechnology development for Kpelle and other low-resourced Mande languages."}
{"id": "2505.18906", "pdf": "https://arxiv.org/pdf/2505.18906.pdf", "abs": "https://arxiv.org/abs/2505.18906", "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Vivek Gupta"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems."}
{"id": "2505.18916", "pdf": "https://arxiv.org/pdf/2505.18916.pdf", "abs": "https://arxiv.org/abs/2505.18916", "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media", "authors": ["Yue Li", "Jake Vasilakes", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SCRum-9, a multilingual dataset for Rumour Stance\nClassification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond\nexisting stance classification datasets by covering more languages (9), linking\nexamples to more fact-checked claims (2.1k), and including complex annotations\nfrom multiple annotators to account for intra- and inter-annotator variability.\nAnnotations were made by at least three native speakers per language, totalling\naround 405 hours of annotation and 8,150 dollars in compensation. Experiments\non SCRum-9 show that it is a challenging benchmark for both state-of-the-art\nLLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating\nfuture work in this area."}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927.pdf", "abs": "https://arxiv.org/abs/2505.18927", "title": "Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments", "authors": ["Amel Muminovic"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation."}
{"id": "2505.18943", "pdf": "https://arxiv.org/pdf/2505.18943.pdf", "abs": "https://arxiv.org/abs/2505.18943", "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Min-Hsuan Yeh", "Yixuan Li"], "categories": ["cs.CL"], "comment": null, "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses user mental states (e.g., intent,\nemotion), (2) a Domain Agent refines these hypotheses using cultural norms and\nethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind."}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949.pdf", "abs": "https://arxiv.org/abs/2505.18949", "title": "The Price of Format: Diversity Collapse in LLMs", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning."}
{"id": "2505.18951", "pdf": "https://arxiv.org/pdf/2505.18951.pdf", "abs": "https://arxiv.org/abs/2505.18951", "title": "BnMMLU: Measuring Massive Multitask Language Understanding in Bengali", "authors": ["Saman Sarker Joy"], "categories": ["cs.CL"], "comment": "18 pages, 9 figures, 5 tables; Code & dataset available at\n  https://github.com/samanjoy2/bnmmlu", "summary": "The Massive Multitask Language Understanding (MMLU) benchmark has been widely\nused to evaluate language models across various domains. However, existing MMLU\ndatasets primarily focus on high-resource languages such as English, which\nleaves low-resource languages like Bengali underrepresented. In this paper, we\nintroduce BnMMLU, a benchmark to evaluate the multitask language understanding\ncapabilities of Bengali in language models. The dataset spans 23 domains,\nincluding science, humanities, mathematics and general knowledge and is\nstructured in a multiple-choice format to assess factual knowledge,\napplication-based problem-solving and reasoning abilities of language models.\nIt consists of 138,949 question-option pairs. We benchmark several proprietary\nand open-source large language models (LLMs) on the BnMMLU test set.\nAdditionally, we annotate the test set with three cognitive categories-factual\nknowledge, procedural application and reasoning-to gain deeper insights into\nmodel strengths and weaknesses across various cognitive tasks. The results\nreveal significant performance gaps, highlighting the need for improved\npre-training and fine-tuning strategies tailored to Bengali data. We release\nthe dataset and benchmark results to facilitate further research in this area."}
{"id": "2505.18953", "pdf": "https://arxiv.org/pdf/2505.18953.pdf", "abs": "https://arxiv.org/abs/2505.18953", "title": "Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?", "authors": ["Divij Chawla", "Ashita Bhutada", "Do Duc Anh", "Abhinav Raghunathan", "Vinod SP", "Cathy Guo", "Dar Win Liew", "Prannaya Gupta", "Rishabh Bhardwaj", "Rajat Bhardwaj", "Soujanya Poria"], "categories": ["cs.CL"], "comment": null, "summary": "We evaluate the credibility of leading AI models in assessing investment risk\nappetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and\nopen-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720\nuser profiles constructed with 16 risk-relevant features across 10 countries\nand both genders. We observe significant variance across models in score\ndistributions and demographic sensitivity. For example, GPT-4o assigns higher\nrisk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show\nopposite gender tendencies in risk classification. While some models (e.g.,\nGPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk\nranges, none maintain consistent performance across regions and demographics.\nOur findings highlight the need for rigorous, standardized evaluations of AI\nsystems in regulated financial contexts to prevent bias, opacity, and\ninconsistency in real-world deployment."}
{"id": "2505.18962", "pdf": "https://arxiv.org/pdf/2505.18962.pdf", "abs": "https://arxiv.org/abs/2505.18962", "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space.Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage."}
{"id": "2505.18970", "pdf": "https://arxiv.org/pdf/2505.18970.pdf", "abs": "https://arxiv.org/abs/2505.18970", "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM Classification", "authors": ["Bowen Wei", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications."}
{"id": "2505.18971", "pdf": "https://arxiv.org/pdf/2505.18971.pdf", "abs": "https://arxiv.org/abs/2505.18971", "title": "Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Ashutosh Balasubramaniam", "Tejas Anvekar", "Vivek Gupta"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We revisit the efficacy of simple, real-valued embedding models for knowledge\ngraph completion and introduce RelatE, an interpretable and modular method that\nefficiently integrates dual representations for entities and relations. RelatE\nemploys a real-valued phase-modulus decomposition, leveraging sinusoidal phase\nalignments to encode relational patterns such as symmetry, inversion, and\ncomposition. In contrast to recent approaches based on complex-valued\nembeddings or deep neural architectures, RelatE preserves architectural\nsimplicity while achieving competitive or superior performance on standard\nbenchmarks. Empirically, RelatE outperforms prior methods across several\ndatasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,\nsurpassing all baselines. Additionally, RelatE offers significant efficiency\ngains, reducing training time by 24%, inference latency by 31%, and peak GPU\nmemory usage by 22% compared to RotatE. Perturbation studies demonstrate\nimproved robustness, with MRR degradation reduced by up to 61% relative to\nTransE and by up to 19% compared to RotatE under structural edits such as edge\nremovals and relation swaps. Formal analysis further establishes the model's\nfull expressiveness and its capacity to represent essential first-order logical\ninference patterns. These results position RelatE as a scalable and\ninterpretable alternative to more complex architectures for knowledge graph\ncompletion."}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973.pdf", "abs": "https://arxiv.org/abs/2505.18973", "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail."}
{"id": "2505.18978", "pdf": "https://arxiv.org/pdf/2505.18978.pdf", "abs": "https://arxiv.org/abs/2505.18978", "title": "AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models", "authors": ["Miguel Angel Peñaloza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Michelle Bruno Hernandez", "Miguel Angel Alvarado Gonzalez", "Sandra Malagon"], "categories": ["cs.CL", "68", "I.2"], "comment": "36 pages, 5 figures", "summary": "Existing mathematical reasoning benchmarks are predominantly English only or\ntranslation-based, which can introduce semantic drift and mask languagespecific\nreasoning errors. To address this, we present AI4Math, a benchmark of 105\noriginal university level math problems natively authored in Spanish. The\ndataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,\nNumber Theory, Combinatorics, and Logic), and each problem is accompanied by a\nstep by step human solution. We evaluate six large language models GPT 4o, GPT\n4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under\nfour configurations: zero shot and chain of thought, each in Spanish and\nEnglish. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve\nover 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most\nmodels show no significant performance drop between languages, with GPT 4o even\nperforming better on Spanish problems in the zero shot setting. Geometry,\nCombinatorics, and Probability questions remain persistently challenging for\nall models. These results highlight the need for native-language benchmarks and\ndomain-specific evaluations to reveal reasoning failures not captured by\nstandard metrics."}
{"id": "2505.18995", "pdf": "https://arxiv.org/pdf/2505.18995.pdf", "abs": "https://arxiv.org/abs/2505.18995", "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)", "authors": ["Carlos Jude G. Maminta", "Isaiah Job Enriquez", "Deandre Nigel Nunez", "Michael B. Dela Fuente"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study presents FiLLM, a Filipino-optimized large language model,\ndesigned to enhance natural language processing (NLP) capabilities in the\nFilipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank\nAdaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining\ntask-specific performance. The model was trained and evaluated on diverse\nFilipino datasets to address key NLP tasks, including Named Entity Recognition\n(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text\nSummarization. Performance comparisons with the CalamanCy model were conducted\nusing F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap\nmetrics. Results indicate that Calamancy outperforms FILLM in several aspects,\ndemonstrating its effectiveness in processing Filipino text with improved\nlinguistic comprehension and adaptability. This research contributes to the\nadvancement of Filipino NLP applications by providing an optimized, efficient,\nand scalable language model tailored for local linguistic needs."}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000.pdf", "abs": "https://arxiv.org/abs/2505.19000", "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability."}
{"id": "2505.19018", "pdf": "https://arxiv.org/pdf/2505.19018.pdf", "abs": "https://arxiv.org/abs/2505.19018", "title": "CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "Md. Rajib Hossain", "Md. Saifur Rahman", "A. B. M. Shawkat Ali"], "categories": ["cs.CL"], "comment": null, "summary": "Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural\nlanguage processing, offering fine-grained insights into opinions expressed in\ntext. While existing research has largely focused on resource-rich languages\nlike English which leveraging large annotated datasets, pre-trained models, and\nlanguage-specific tools. These resources are often unavailable for low-resource\nlanguages such as Bengali. The ABSA task in Bengali remains poorly explored and\nis further complicated by its unique linguistic characteristics and a lack of\nannotated data, pre-trained models, and optimized hyperparameters. To address\nthese challenges, this research propose CrosGrpsABS, a novel hybrid framework\nthat leverages bidirectional cross-attention between syntactic and semantic\ngraphs to enhance aspect-level sentiment classification. The CrosGrpsABS\ncombines transformerbased contextual embeddings with graph convolutional\nnetworks, built upon rule-based syntactic dependency parsing and semantic\nsimilarity computations. By employing bidirectional crossattention, the model\neffectively fuses local syntactic structure with global semantic context,\nresulting in improved sentiment classification performance across both low- and\nhigh-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali\nABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The\nCrosGrpsABS consistently outperforms existing approaches, achieving notable\nimprovements, including a 0.93% F1-score increase for the Restaurant domain and\na 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark."}
{"id": "2505.19051", "pdf": "https://arxiv.org/pdf/2505.19051.pdf", "abs": "https://arxiv.org/abs/2505.19051", "title": "Efficient Data Selection at Scale via Influence Distillation", "authors": ["Mahdi Nikdan", "Vincent Cohen-Addad", "Dan Alistarh", "Vahab Mirrokni"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a $\\textit{landmark-based approximation}$:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to $3.5\\times$ faster selection."}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056.pdf", "abs": "https://arxiv.org/abs/2505.19056", "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."}
{"id": "2505.19060", "pdf": "https://arxiv.org/pdf/2505.19060.pdf", "abs": "https://arxiv.org/abs/2505.19060", "title": "UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models", "authors": ["Roman Vashurin", "Maiya Goloburda", "Preslav Nakov", "Maxim Panov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools across various\napplications, making it more important than ever to ensure the quality and the\ntrustworthiness of their outputs. This has led to growing interest in\nuncertainty quantification (UQ) methods for assessing the reliability of LLM\noutputs. Many existing UQ techniques rely on token probabilities, which\ninadvertently introduces a bias with respect to the length of the output. While\nsome methods attempt to account for this, we demonstrate that such biases\npersist even in length-normalized approaches. To address the problem, here we\npropose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing\nprocedure that regresses uncertainty scores on output length and uses the\nresiduals as corrected, length-invariant estimates. Our method is post-hoc,\nmodel-agnostic, and applicable to a range of UQ measures. Through extensive\nevaluation on machine translation, summarization, and question-answering tasks,\nwe demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally\nlength-normalized UQ methods uncertainty estimates across multiple metrics and\nmodels."}
{"id": "2505.19073", "pdf": "https://arxiv.org/pdf/2505.19073.pdf", "abs": "https://arxiv.org/abs/2505.19073", "title": "Towards Harmonized Uncertainty Estimation for Large Language Models", "authors": ["Rui Li", "Jing Long", "Muge Qi", "Heming Xia", "Lei Sha", "Peiyi Wang", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To facilitate robust and trustworthy deployment of large language models\n(LLMs), it is essential to quantify the reliability of their generations\nthrough uncertainty estimation. While recent efforts have made significant\nadvancements by leveraging the internal logic and linguistic features of LLMs\nto estimate uncertainty scores, our empirical analysis highlights the pitfalls\nof these methods to strike a harmonized estimation between indication, balance,\nand calibration, which hinders their broader capability for accurate\nuncertainty estimation. To address this challenge, we propose CUE (Corrector\nfor Uncertainty Estimation): A straightforward yet effective method that\nemploys a lightweight model trained on data aligned with the target LLM's\nperformance to adjust uncertainty scores. Comprehensive experiments across\ndiverse models and tasks demonstrate its effectiveness, which achieves\nconsistent improvements of up to 60% over existing methods."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091.pdf", "abs": "https://arxiv.org/abs/2505.19091", "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "authors": ["Benjamin Clavié", "Florian Brand"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100.pdf", "abs": "https://arxiv.org/abs/2505.19100", "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models."}
{"id": "2505.19103", "pdf": "https://arxiv.org/pdf/2505.19103.pdf", "abs": "https://arxiv.org/abs/2505.19103", "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection", "authors": ["Iddo Yosha", "Dorin Shteyman", "Yossi Adi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress."}
{"id": "2505.19108", "pdf": "https://arxiv.org/pdf/2505.19108.pdf", "abs": "https://arxiv.org/abs/2505.19108", "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "authors": ["Yongheng Zhang", "Xu Liu", "Ruoxi Zhou", "Qiguang Chen", "Hao Fei", "Wenpeng Lu", "Libo Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios."}
{"id": "2505.19112", "pdf": "https://arxiv.org/pdf/2505.19112.pdf", "abs": "https://arxiv.org/abs/2505.19112", "title": "Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering", "authors": ["Zheng Chu", "Huiming Fan", "Jingchang Chen", "Qianyu Wang", "Mingda Yang", "Jiafeng Liang", "Zhongjie Wang", "Hao Li", "Guo Tang", "Ming Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Although large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they still face challenges in knowledge-intensive multi-hop\nreasoning. Recent work explores iterative retrieval to address complex\nproblems. However, the lack of intermediate guidance often results in\ninaccurate retrieval and flawed intermediate reasoning, leading to incorrect\nreasoning. To address these, we propose Self-Critique Guided Iterative\nReasoning (SiGIR), which uses self-critique feedback to guide the iterative\nreasoning process. Specifically, through end-to-end training, we enable the\nmodel to iteratively address complex problems via question decomposition.\nAdditionally, the model is able to self-evaluate its intermediate reasoning\nsteps. During iterative reasoning, the model engages in branching exploration\nand employs self-evaluation to guide the selection of promising reasoning\ntrajectories. Extensive experiments on three multi-hop reasoning datasets\ndemonstrate the effectiveness of our proposed method, surpassing the previous\nSOTA by $8.6\\%$. Furthermore, our thorough analysis offers insights for future\nresearch. Our code, data, and models are available at Github:\nhttps://github.com/zchuz/SiGIR-MHQA."}
{"id": "2505.19116", "pdf": "https://arxiv.org/pdf/2505.19116.pdf", "abs": "https://arxiv.org/abs/2505.19116", "title": "Controlling Language Confusion in Multilingual LLMs", "authors": ["Nahyun Lee", "Yeongseo Woo", "Hyunwoo Ko", "Guijin Son"], "categories": ["cs.CL"], "comment": "4 pages", "summary": "Large language models often suffer from language confusion, a phenomenon\nwhere responses are partially or entirely generated in unintended languages.\nThis can critically impact user experience in low-resource settings. We\nhypothesize that conventional supervised fine-tuning exacerbates this issue\nbecause the softmax objective focuses probability mass only on the single\ncorrect token but does not explicitly penalize cross-lingual mixing.\nInterestingly, by observing loss trajectories during the pretraining phase, we\nobserve that models fail to learn to distinguish between monolingual and\nlanguage-confused text. Additionally, we find that ORPO, which adds penalties\nfor unwanted output styles to standard SFT, effectively suppresses\nlanguage-confused generations even at high decoding temperatures without\ndegrading overall model performance. Our findings suggest that incorporating\nappropriate penalty terms can mitigate language confusion in low-resource\nsettings with limited data."}
{"id": "2505.19121", "pdf": "https://arxiv.org/pdf/2505.19121.pdf", "abs": "https://arxiv.org/abs/2505.19121", "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models", "authors": ["Seunguk Yu", "Juhwan Choi", "Youngbin Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models."}
{"id": "2505.19126", "pdf": "https://arxiv.org/pdf/2505.19126.pdf", "abs": "https://arxiv.org/abs/2505.19126", "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning", "authors": ["Wenyang Luo", "Wayne Xin Zhao", "Jing Sha", "Shijin Wang", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has\nsignificantly advanced complex reasoning tasks. However, their capabilities in\nmultilingual complex reasoning remain underexplored, with existing efforts\nlargely focused on simpler tasks like MGSM. To address this gap, we introduce\nMMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality\nmath problems across 10 typologically diverse languages. Using MMATH, we\nobserve that even advanced models like DeepSeek R1 exhibit substantial\nperformance disparities across languages and suffer from a critical off-target\nissue-generating responses in unintended languages. To address this, we explore\nstrategies including prompting and training, demonstrating that reasoning in\nEnglish and answering in target languages can simultaneously enhance\nperformance and preserve target-language consistency. Our findings offer new\ninsights and practical strategies for advancing the multilingual reasoning\ncapabilities of large language models. Our code and data could be found at\nhttps://github.com/RUCAIBox/MMATH."}
{"id": "2505.19128", "pdf": "https://arxiv.org/pdf/2505.19128.pdf", "abs": "https://arxiv.org/abs/2505.19128", "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models", "authors": ["Jin Zhang", "Fan Gao", "Linyu Li", "Yongbin Yu", "Xiangxiang Wang", "Nyima Tashi", "Gadeng Luosang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rise of large language models has led to significant performance\nbreakthroughs in named entity recognition (NER) for high-resource languages,\nyet there remains substantial room for improvement in low- and medium-resource\nlanguages. Existing multilingual NER methods face severe language interference\nduring the multi-language adaptation process, manifested in feature conflicts\nbetween different languages and the competitive suppression of low-resource\nlanguage features by high-resource languages. Although training a dedicated\nmodel for each language can mitigate such interference, it lacks scalability\nand incurs excessive computational costs in real-world applications. To address\nthis issue, we propose RetrieveAll, a universal multilingual NER framework\nbased on dynamic LoRA. The framework decouples task-specific features across\nlanguages and demonstrates efficient dynamic adaptability. Furthermore, we\nintroduce a cross-granularity knowledge augmented method that fully exploits\nthe intrinsic potential of the data without relying on external resources. By\nleveraging a hierarchical prompting mechanism to guide knowledge injection,\nthis approach advances the paradigm from \"prompt-guided inference\" to\n\"prompt-driven learning.\" Experimental results show that RetrieveAll\noutperforms existing baselines; on the PAN-X dataset, it achieves an average F1\nimprovement of 12.1 percent."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147.pdf", "abs": "https://arxiv.org/abs/2505.19147", "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.19163", "pdf": "https://arxiv.org/pdf/2505.19163.pdf", "abs": "https://arxiv.org/abs/2505.19163", "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs", "authors": ["Firoj Alam", "Md Arid Hasan", "Shammur Absar Chowdhury"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based\n  Evaluation, Dialectal Speech, Low-resource Languages, Multimodal\n  Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction,\n  Natural Language Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious disciplines and tasks. However, benchmarking their capabilities with\nmultilingual spoken queries remains largely unexplored. In this study, we\nintroduce SpokenNativQA, the first multilingual and culturally aligned spoken\nquestion-answering (SQA) dataset designed to evaluate LLMs in real-world\nconversational settings. The dataset comprises approximately 33,000 naturally\nspoken questions and answers in multiple languages, including low-resource and\ndialect-rich languages, providing a robust benchmark for assessing LLM\nperformance in speech-based interactions. SpokenNativQA addresses the\nlimitations of text-based QA datasets by incorporating speech variability,\naccents, and linguistic diversity. We benchmark different ASR systems and LLMs\nfor SQA and present our findings. We released the data at\n(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental\nscripts at (https://llmebench.qcri.org/) for the research community."}
{"id": "2505.19176", "pdf": "https://arxiv.org/pdf/2505.19176.pdf", "abs": "https://arxiv.org/abs/2505.19176", "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "authors": ["Zhuo Liu", "Moxin Li", "Xun Deng", "Qifan Wang", "Fuli Feng"], "categories": ["cs.CL"], "comment": "Under review", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184.pdf", "abs": "https://arxiv.org/abs/2505.19184", "title": "Two LLMs debate, both are certain they've won", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."}
{"id": "2505.19187", "pdf": "https://arxiv.org/pdf/2505.19187.pdf", "abs": "https://arxiv.org/abs/2505.19187", "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling", "authors": ["Yang Xiao", "Jiashuo Wang", "Ruifeng Yuan", "Chunpu Xu", "Kaishuai Xu", "Wenjie Li", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints."}
{"id": "2505.19191", "pdf": "https://arxiv.org/pdf/2505.19191.pdf", "abs": "https://arxiv.org/abs/2505.19191", "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection", "authors": ["Nursulu Sagimbayeva", "Ruveyda Betül Bahçeci", "Ingmar Weber"], "categories": ["cs.CL"], "comment": "8 pages, 6 figures. Accepted for publication in the Proceedings of\n  1st Workshop on Misinformation Detection in the Era of LLMs (MisD) at\n  ICWSM-2025", "summary": "Inconsistent political statements represent a form of misinformation. They\nerode public trust and pose challenges to accountability, when left unnoticed.\nDetecting inconsistencies automatically could support journalists in asking\nclarification questions, thereby helping to keep politicians accountable. We\npropose the Inconsistency detection task and develop a scale of inconsistency\ntypes to prompt NLP-research in this direction. To provide a resource for\ndetecting inconsistencies in a political domain, we present a dataset of 698\nhuman-annotated pairs of political statements with explanations of the\nannotators' reasoning for 237 samples. The statements mainly come from voting\nassistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,\nreflecting real-world political issues. We benchmark Large Language Models\n(LLMs) on our dataset and show that in general, they are as good as humans at\ndetecting inconsistencies, and might be even better than individual humans at\npredicting the crowd-annotated ground-truth. However, when it comes to\nidentifying fine-grained inconsistency types, none of the model have reached\nthe upper bound of performance (due to natural labeling variation), thus\nleaving room for improvement. We make our dataset and code publicly available."}
{"id": "2505.19201", "pdf": "https://arxiv.org/pdf/2505.19201.pdf", "abs": "https://arxiv.org/abs/2505.19201", "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding", "authors": ["Yunhai Hu", "Tianhua Xia", "Zining Liu", "Rahul Raman", "Xingyu Liu", "Bo Bao", "Eric Sather", "Vithursan Thangarasa", "Sai Qian Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD) has emerged as a powerful method for accelerating\nautoregressive generation in large language models (LLMs), yet its integration\ninto vision-language models (VLMs) remains underexplored. We introduce DREAM, a\nnovel speculative decoding framework tailored for VLMs that combines three key\ninnovations: (1) a cross-attention-based mechanism to inject intermediate\nfeatures from the target model into the draft model for improved alignment, (2)\nadaptive intermediate feature selection based on attention entropy to guide\nefficient draft model training, and (3) visual token compression to reduce\ndraft model latency. DREAM enables efficient, accurate, and parallel multimodal\ndecoding with significant throughput improvement. Experiments across a diverse\nset of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,\ndemonstrate up to 3.6x speedup over conventional decoding and significantly\noutperform prior SD baselines in both inference throughput and speculative\ndraft acceptance length across a broad range of multimodal benchmarks. The code\nis publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git"}
{"id": "2505.19206", "pdf": "https://arxiv.org/pdf/2505.19206.pdf", "abs": "https://arxiv.org/abs/2505.19206", "title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "authors": ["Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems."}
{"id": "2505.19209", "pdf": "https://arxiv.org/pdf/2505.19209.pdf", "abs": "https://arxiv.org/abs/2505.19209", "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Yujie Liu", "Wei Li", "Tong Xie", "Lidong Bing", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines."}
{"id": "2505.19212", "pdf": "https://arxiv.org/pdf/2505.19212.pdf", "abs": "https://arxiv.org/abs/2505.19212", "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled their use in\ncomplex agentic roles, involving decision-making with humans or other agents,\nmaking ethical alignment a key AI safety concern. While prior work has examined\nboth LLMs' moral judgment and strategic behavior in social dilemmas, there is\nlimited understanding of how they act when moral imperatives directly conflict\nwith rewards or incentives. To investigate this, we introduce Moral Behavior in\nSocial Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the\nprisoner's dilemma and public goods game with morally charged contexts. In\nMoralSim, we test a range of frontier models across both game structures and\nthree distinct moral framings, enabling a systematic examination of how LLMs\nnavigate social dilemmas in which ethical norms conflict with payoff-maximizing\nstrategies. Our results show substantial variation across models in both their\ngeneral tendency to act morally and the consistency of their behavior across\ngame types, the specific moral framing, and situational factors such as\nopponent behavior and survival risks. Crucially, no model exhibits consistently\nmoral behavior in MoralSim, highlighting the need for caution when deploying\nLLMs in agentic roles where the agent's \"self-interest\" may conflict with\nethical expectations. Our code is available at\nhttps://github.com/sbackmann/moralsim."}
{"id": "2505.19217", "pdf": "https://arxiv.org/pdf/2505.19217.pdf", "abs": "https://arxiv.org/abs/2505.19217", "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training", "authors": ["Weize Chen", "Jiarui Yuan", "Tailin Jin", "Ning Ding", "Huimin Chen", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": "under review", "summary": "Recent large language models (LLMs) exhibit impressive reasoning but often\nover-think, generating excessively long responses that hinder efficiency. We\nintroduce DIET ( DIfficulty-AwarE Training), a framework that systematically\ncuts these \"token calories\" by integrating on-the-fly problem difficulty into\nthe reinforcement learning (RL) process. DIET dynamically adapts token\ncompression strategies by modulating token penalty strength and conditioning\ntarget lengths on estimated task difficulty, to optimize the\nperformance-efficiency trade-off. We also theoretically analyze the pitfalls of\nnaive reward weighting in group-normalized RL algorithms like GRPO, and propose\nAdvantage Weighting technique, which enables stable and effective\nimplementation of these difficulty-aware objectives. Experimental results\ndemonstrate that DIET significantly reduces token counts while simultaneously\nimproving reasoning performance. Beyond raw token reduction, we show two\ncrucial benefits largely overlooked by prior work: (1) DIET leads to superior\ninference scaling. By maintaining high per-sample quality with fewer tokens, it\nenables better scaling performance via majority voting with more samples under\nfixed computational budgets, an area where other methods falter. (2) DIET\nenhances the natural positive correlation between response length and problem\ndifficulty, ensuring verbosity is appropriately allocated, unlike many existing\ncompression methods that disrupt this relationship. Our analyses provide a\nprincipled and effective framework for developing more efficient, practical,\nand high-performing LLMs."}
{"id": "2505.19236", "pdf": "https://arxiv.org/pdf/2505.19236.pdf", "abs": "https://arxiv.org/abs/2505.19236", "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator", "authors": ["Qian Cao", "Xiting Wang", "Yuzhuo Yuan", "Yahui Liu", "Fang Luo", "Ruihua Song"], "categories": ["cs.CL"], "comment": null, "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240.pdf", "abs": "https://arxiv.org/abs/2505.19240", "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Pütz", "Benjamin Paaßen", "Steffen Eger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research."}
{"id": "2505.19250", "pdf": "https://arxiv.org/pdf/2505.19250.pdf", "abs": "https://arxiv.org/abs/2505.19250", "title": "PATS: Process-Level Adaptive Thinking Mode Switching", "authors": ["Yi Wang", "Junxiao Liu", "Shimao Zhang", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs."}
{"id": "2505.19254", "pdf": "https://arxiv.org/pdf/2505.19254.pdf", "abs": "https://arxiv.org/abs/2505.19254", "title": "Unveiling Dual Quality in Product Reviews: An NLP-Based Approach", "authors": ["Rafał Poświata", "Marcin Michał Mirończuk", "Sławomir Dadas", "Małgorzata Grębowiec", "Michał Perełkiewicz"], "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Industry Track", "summary": "Consumers often face inconsistent product quality, particularly when\nidentical products vary between markets, a situation known as the dual quality\nproblem. To identify and address this issue, automated techniques are needed.\nThis paper explores how natural language processing (NLP) can aid in detecting\nsuch discrepancies and presents the full process of developing a solution.\nFirst, we describe in detail the creation of a new Polish-language dataset with\n1,957 reviews, 540 highlighting dual quality issues. We then discuss\nexperiments with various approaches like SetFit with sentence-transformers,\ntransformer-based encoders, and LLMs, including error analysis and robustness\nverification. Additionally, we evaluate multilingual transfer using a subset of\nopinions in English, French, and German. The paper concludes with insights on\ndeployment and practical applications."}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286.pdf", "abs": "https://arxiv.org/abs/2505.19286", "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance."}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293.pdf", "abs": "https://arxiv.org/abs/2505.19293", "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs."}
{"id": "2505.19299", "pdf": "https://arxiv.org/pdf/2505.19299.pdf", "abs": "https://arxiv.org/abs/2505.19299", "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations", "authors": ["Lingjun Zhao", "Hal Daumé III"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Faithful free-text explanations are important to ensure transparency in\nhigh-stakes AI decision-making contexts, but they are challenging to generate\nby language models and assess by humans. In this paper, we present a measure\nfor Prediction-EXplanation (PEX) consistency, by extending the concept of\nweight of evidence. This measure quantifies how much a free-text explanation\nsupports or opposes a prediction, serving as an important aspect of explanation\nfaithfulness. Our analysis reveals that more than 62% explanations generated by\nlarge language models lack this consistency. We show that applying direct\npreference optimization improves the consistency of generated explanations\nacross three model families, with improvement ranging from 43.1% to 292.3%.\nFurthermore, we demonstrate that optimizing this consistency measure can\nimprove explanation faithfulness by up to 9.7%."}
{"id": "2505.19300", "pdf": "https://arxiv.org/pdf/2505.19300.pdf", "abs": "https://arxiv.org/abs/2505.19300", "title": "SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking", "authors": ["Junnan Liu", "Linhao Luo", "Thuy-Trang Vu", "Gholamreza Haffari"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in large language models (LLMs) demonstrate their impressive\nreasoning capabilities. However, the reasoning confined to internal parametric\nspace limits LLMs' access to real-time information and understanding of the\nphysical world. To overcome this constraint, we introduce SituatedThinker, a\nnovel framework that enables LLMs to ground their reasoning in real-world\ncontexts through situated thinking, which adaptively combines both internal\nknowledge and external information with predefined interfaces. By utilizing\nreinforcement learning, SituatedThinker incentivizes deliberate reasoning with\nthe real world to acquire information and feedback, allowing LLMs to surpass\ntheir knowledge boundaries and enhance reasoning. Experimental results\ndemonstrate significant performance improvements on multi-hop\nquestion-answering and mathematical reasoning benchmarks. Furthermore,\nSituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,\nTableQA, and text-based games, showcasing the generalizable real-world grounded\nreasoning capability. Our codes are available at\nhttps://github.com/jnanliu/SituatedThinker."}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345.pdf", "abs": "https://arxiv.org/abs/2505.19345", "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language generation (NLG) metrics play a central role in evaluating\ngenerated texts, but are not well suited for the structural and legal\ncharacteristics of patent documents. Large language models (LLMs) offer strong\npotential in automating patent generation, yet research on evaluating\nLLM-generated patents remains limited, especially in evaluating the generation\nquality of patent claims, which are central to defining the scope of\nprotection. Effective claim evaluation requires addressing legal validity,\ntechnical accuracy, and structural compliance. To address this gap, we\nintroduce PatentScore, a multi-dimensional evaluation framework for assessing\nLLM-generated patent claims. PatentScore incorporates: (1) hierarchical\ndecomposition for claim analysis; (2) domain-specific validation patterns based\non legal and technical standards; and (3) scoring across structural, semantic,\nand legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects\npatent-specific constraints and document structures, enabling evaluation beyond\nsurface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a\nPearson correlation of $r = 0.819$ with expert annotations, outperforming\nexisting NLG metrics. Furthermore, we conduct additional evaluations using open\nmodels such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong\ncorrelations with expert judgments, confirming the robustness and\ngeneralizability of our framework."}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354.pdf", "abs": "https://arxiv.org/abs/2505.19354", "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public."}
{"id": "2505.19355", "pdf": "https://arxiv.org/pdf/2505.19355.pdf", "abs": "https://arxiv.org/abs/2505.19355", "title": "Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement", "authors": ["Lin Tian", "Marian-Andrei Rizoiu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Understanding true influence in social media requires distinguishing\ncorrelation from causation--particularly when analyzing misinformation spread.\nWhile existing approaches focus on exposure metrics and network structures,\nthey often fail to capture the causal mechanisms by which external temporal\nsignals trigger engagement. We introduce a novel joint treatment-outcome\nframework that leverages existing sequential models to simultaneously adapt to\nboth policy timing and engagement effects. Our approach adapts causal inference\ntechniques from healthcare to estimate Average Treatment Effects (ATE) within\nthe sequential nature of social media interactions, tackling challenges from\nexternal confounding signals. Through our experiments on real-world\nmisinformation and disinformation datasets, we show that our models outperform\nexisting benchmarks by 15--22% in predicting engagement across diverse\ncounterfactual scenarios, including exposure adjustment, timing shifts, and\nvaried intervention durations. Case studies on 492 social media users show our\ncausal effect measure aligns strongly with the gold standard in influence\nestimation, the expert-based empirical influence."}
{"id": "2505.19360", "pdf": "https://arxiv.org/pdf/2505.19360.pdf", "abs": "https://arxiv.org/abs/2505.19360", "title": "ChartLens: Fine-grained Visual Attribution in Charts", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main)", "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%."}
{"id": "2505.19376", "pdf": "https://arxiv.org/pdf/2505.19376.pdf", "abs": "https://arxiv.org/abs/2505.19376", "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "authors": ["Lance Ying", "Almog Hillel", "Ryan Truong", "Vikash K. Mansinghka", "Joshua B. Tenenbaum", "Tan Zhi-Xuan"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures; oral presentation at CogSci 2025", "summary": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses."}
{"id": "2505.19384", "pdf": "https://arxiv.org/pdf/2505.19384.pdf", "abs": "https://arxiv.org/abs/2505.19384", "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor", "authors": ["Seokgi Lee", "Jungjun Kim"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "7 pages, 3 figures", "summary": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure."}
{"id": "2505.19388", "pdf": "https://arxiv.org/pdf/2505.19388.pdf", "abs": "https://arxiv.org/abs/2505.19388", "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 System Demonstration Track, 11 pages, 9 figures", "summary": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube."}
{"id": "2505.19392", "pdf": "https://arxiv.org/pdf/2505.19392.pdf", "abs": "https://arxiv.org/abs/2505.19392", "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "comment": null, "summary": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias."}
{"id": "2505.19405", "pdf": "https://arxiv.org/pdf/2505.19405.pdf", "abs": "https://arxiv.org/abs/2505.19405", "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "authors": ["Yan Wen", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL", "cs.CR"], "comment": "18 pages, 1 figure", "summary": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems."}
{"id": "2505.19410", "pdf": "https://arxiv.org/pdf/2505.19410.pdf", "abs": "https://arxiv.org/abs/2505.19410", "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering", "authors": ["Jiajun Zhu", "Ye Liu", "Meikai Bao", "Kai Zhang", "Yanghai Zhang", "Qi Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability."}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426.pdf", "abs": "https://arxiv.org/abs/2505.19426", "title": "The Role of Diversity in In-Context Learning for Large Language Models", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection."}
{"id": "2505.19428", "pdf": "https://arxiv.org/pdf/2505.19428.pdf", "abs": "https://arxiv.org/abs/2505.19428", "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things", "authors": ["Abhijnan Nath", "Carine Graff", "Andrei Bachinin", "Nikhil Krishnaswamy"], "categories": ["cs.CL"], "comment": "48 pages (main paper: 10 pages incl. Limitations and Acknowledgments;\n  references: 6 pages; appendix: 32 pages), 9 figures, 12 tables, appearing in\n  Proceedings of ACL 2025, Vienna, Austria", "summary": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL."}
{"id": "2505.19429", "pdf": "https://arxiv.org/pdf/2505.19429.pdf", "abs": "https://arxiv.org/abs/2505.19429", "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "categories": ["cs.CL"], "comment": null, "summary": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight finetuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models finetuned with in-domain data\nsignificantly outperform their zero-shot performance. The finetuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media."}
{"id": "2505.19430", "pdf": "https://arxiv.org/pdf/2505.19430.pdf", "abs": "https://arxiv.org/abs/2505.19430", "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research."}
{"id": "2505.19435", "pdf": "https://arxiv.org/pdf/2505.19435.pdf", "abs": "https://arxiv.org/abs/2505.19435", "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection", "authors": ["Zhihong Pan", "Kai Zhang", "Yuze Zhao", "Yupeng Han"], "categories": ["cs.CL"], "comment": null, "summary": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%."}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439.pdf", "abs": "https://arxiv.org/abs/2505.19439", "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses."}
{"id": "2505.19440", "pdf": "https://arxiv.org/pdf/2505.19440.pdf", "abs": "https://arxiv.org/abs/2505.19440", "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models."}
{"id": "2505.19472", "pdf": "https://arxiv.org/pdf/2505.19472.pdf", "abs": "https://arxiv.org/abs/2505.19472", "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU)."}
{"id": "2505.19475", "pdf": "https://arxiv.org/pdf/2505.19475.pdf", "abs": "https://arxiv.org/abs/2505.19475", "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation."}
{"id": "2505.19484", "pdf": "https://arxiv.org/pdf/2505.19484.pdf", "abs": "https://arxiv.org/abs/2505.19484", "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning."}
{"id": "2505.19494", "pdf": "https://arxiv.org/pdf/2505.19494.pdf", "abs": "https://arxiv.org/abs/2505.19494", "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents", "authors": ["Manoj Balaji Jagadeeshan", "Prince Raj", "Pawan Goyal"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana"}
{"id": "2505.19510", "pdf": "https://arxiv.org/pdf/2505.19510.pdf", "abs": "https://arxiv.org/abs/2505.19510", "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study", "authors": ["Dongil Yang", "Minjin Kim", "Sunghwan Kim", "Beong-woo Kwak", "Minjun Park", "Jinseok Hong", "Woontack Woo", "Jinyoung Yeo"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://anonymous.4open.science/r/TSG-Bench."}
{"id": "2505.19511", "pdf": "https://arxiv.org/pdf/2505.19511.pdf", "abs": "https://arxiv.org/abs/2505.19511", "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models", "authors": ["Aggrey Muhebwa", "Khalid K. Osman"], "categories": ["cs.CL"], "comment": null, "summary": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs."}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514.pdf", "abs": "https://arxiv.org/abs/2505.19514", "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows."}
{"id": "2505.19515", "pdf": "https://arxiv.org/pdf/2505.19515.pdf", "abs": "https://arxiv.org/abs/2505.19515", "title": "Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts."}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528.pdf", "abs": "https://arxiv.org/abs/2505.19528", "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness."}
{"id": "2505.19529", "pdf": "https://arxiv.org/pdf/2505.19529.pdf", "abs": "https://arxiv.org/abs/2505.19529", "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "authors": ["Tanjil Hasan Sakib", "Md. Tanzib Hosain", "Md. Kishor Morol"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models."}
{"id": "2505.19538", "pdf": "https://arxiv.org/pdf/2505.19538.pdf", "abs": "https://arxiv.org/abs/2505.19538", "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": "32 pages, 5 figures, 5 tables", "summary": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems."}
{"id": "2505.19548", "pdf": "https://arxiv.org/pdf/2505.19548.pdf", "abs": "https://arxiv.org/abs/2505.19548", "title": "How Syntax Specialization Emerges in Language Models", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance."}
{"id": "2505.19549", "pdf": "https://arxiv.org/pdf/2505.19549.pdf", "abs": "https://arxiv.org/abs/2505.19549", "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents", "authors": ["Derong Xu", "Yi Wen", "Pengyue Jia", "Yingyi Zhang", "wenlin zhang", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao", "Enhong Chen", "Tong Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings."}
{"id": "2505.19572", "pdf": "https://arxiv.org/pdf/2505.19572.pdf", "abs": "https://arxiv.org/abs/2505.19572", "title": "DocMEdit: Towards Document-Level Model Editing", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods."}
{"id": "2505.19586", "pdf": "https://arxiv.org/pdf/2505.19586.pdf", "abs": "https://arxiv.org/abs/2505.19586", "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."}
{"id": "2505.19591", "pdf": "https://arxiv.org/pdf/2505.19591.pdf", "abs": "https://arxiv.org/abs/2505.19591", "title": "Multi-Agent Collaboration via Evolving Orchestration", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution."}
{"id": "2505.19598", "pdf": "https://arxiv.org/pdf/2505.19598.pdf", "abs": "https://arxiv.org/abs/2505.19598", "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment."}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599.pdf", "abs": "https://arxiv.org/abs/2505.19599", "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output."}
{"id": "2505.19604", "pdf": "https://arxiv.org/pdf/2505.19604.pdf", "abs": "https://arxiv.org/abs/2505.19604", "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "authors": ["Ahan Prasannakumar Shetty"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems."}
{"id": "2505.19606", "pdf": "https://arxiv.org/pdf/2505.19606.pdf", "abs": "https://arxiv.org/abs/2505.19606", "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies."}
{"id": "2505.19628", "pdf": "https://arxiv.org/pdf/2505.19628.pdf", "abs": "https://arxiv.org/abs/2505.19628", "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench."}
{"id": "2505.19630", "pdf": "https://arxiv.org/pdf/2505.19630.pdf", "abs": "https://arxiv.org/abs/2505.19630", "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Yixue Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL"}
{"id": "2505.19631", "pdf": "https://arxiv.org/pdf/2505.19631.pdf", "abs": "https://arxiv.org/abs/2505.19631", "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA"}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634.pdf", "abs": "https://arxiv.org/abs/2505.19634", "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios."}
{"id": "2505.19640", "pdf": "https://arxiv.org/pdf/2505.19640.pdf", "abs": "https://arxiv.org/abs/2505.19640", "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning", "authors": ["Roy Xie", "David Qiu", "Deepak Gopinath", "Dong Lin", "Yanchao Sun", "Chong Wang", "Saloni Potdar", "Bhuwan Dhingra"], "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling."}
{"id": "2505.19647", "pdf": "https://arxiv.org/pdf/2505.19647.pdf", "abs": "https://arxiv.org/abs/2505.19647", "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation", "authors": ["Xiaochuan Liu", "Ruihua Song", "Xiting Wang", "Xu Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Findings)", "summary": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG."}
{"id": "2505.19660", "pdf": "https://arxiv.org/pdf/2505.19660.pdf", "abs": "https://arxiv.org/abs/2505.19660", "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "comment": "13 pages, 5 figures", "summary": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI"}
{"id": "2505.19667", "pdf": "https://arxiv.org/pdf/2505.19667.pdf", "abs": "https://arxiv.org/abs/2505.19667", "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions."}
{"id": "2505.19670", "pdf": "https://arxiv.org/pdf/2505.19670.pdf", "abs": "https://arxiv.org/abs/2505.19670", "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models", "authors": ["Hao Yang", "Lizhen Qu", "Ehsan Shareghi", "Gholamreza Haffari"], "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples."}
{"id": "2505.19674", "pdf": "https://arxiv.org/pdf/2505.19674.pdf", "abs": "https://arxiv.org/abs/2505.19674", "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "categories": ["cs.CL"], "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "summary": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations."}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675.pdf", "abs": "https://arxiv.org/abs/2505.19675", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678.pdf", "abs": "https://arxiv.org/abs/2505.19678", "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency."}
{"id": "2505.19679", "pdf": "https://arxiv.org/pdf/2505.19679.pdf", "abs": "https://arxiv.org/abs/2505.19679", "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points."}
{"id": "2505.19700", "pdf": "https://arxiv.org/pdf/2505.19700.pdf", "abs": "https://arxiv.org/abs/2505.19700", "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models."}
{"id": "2505.19706", "pdf": "https://arxiv.org/pdf/2505.19706.pdf", "abs": "https://arxiv.org/abs/2505.19706", "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/declare-lab/PathFinder-PRM", "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency."}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714.pdf", "abs": "https://arxiv.org/abs/2505.19714", "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT."}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715.pdf", "abs": "https://arxiv.org/abs/2505.19715", "title": "Graceful Forgetting in Generative Language Models", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance."}
{"id": "2505.19722", "pdf": "https://arxiv.org/pdf/2505.19722.pdf", "abs": "https://arxiv.org/abs/2505.19722", "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICIC 2025", "summary": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework."}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743.pdf", "abs": "https://arxiv.org/abs/2505.19743", "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs."}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754.pdf", "abs": "https://arxiv.org/abs/2505.19754", "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG."}
{"id": "2505.19756", "pdf": "https://arxiv.org/pdf/2505.19756.pdf", "abs": "https://arxiv.org/abs/2505.19756", "title": "Efficient Reasoning via Chain of Unconscious Thought", "authors": ["Ruihan Gong", "Yue Liu", "Wenjie Qu", "Mingzhe Du", "Yufei He", "Yingwei Ma", "Yulin Chen", "Xiang Liu", "Yi Wen", "Xinfeng Li", "Ruidong Wang", "Xinzhong Zhu", "Bryan Hooi", "Jiaheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT"}
{"id": "2505.19766", "pdf": "https://arxiv.org/pdf/2505.19766.pdf", "abs": "https://arxiv.org/abs/2505.19766", "title": "SGM: A Framework for Building Specification-Guided Moderation Filters", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol."}
{"id": "2505.19768", "pdf": "https://arxiv.org/pdf/2505.19768.pdf", "abs": "https://arxiv.org/abs/2505.19768", "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search", "authors": ["Xing Cui", "Yueying Zou", "Zekun Li", "Peipei Li", "Xinyuan Xu", "Xuannan Liu", "Huaibo Huang", "Ran He"], "categories": ["cs.CL"], "comment": null, "summary": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased."}
{"id": "2505.19773", "pdf": "https://arxiv.org/pdf/2505.19773.pdf", "abs": "https://arxiv.org/abs/2505.19773", "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs", "authors": ["Sangyeop Kim", "Yohan Lee", "Yongwoo Song", "Kimin Lee"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted by ACL 2025", "summary": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms."}
{"id": "2505.19776", "pdf": "https://arxiv.org/pdf/2505.19776.pdf", "abs": "https://arxiv.org/abs/2505.19776", "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts."}
{"id": "2505.19797", "pdf": "https://arxiv.org/pdf/2505.19797.pdf", "abs": "https://arxiv.org/abs/2505.19797", "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers"}
{"id": "2505.19800", "pdf": "https://arxiv.org/pdf/2505.19800.pdf", "abs": "https://arxiv.org/abs/2505.19800", "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "categories": ["cs.CL"], "comment": null, "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community."}
{"id": "2505.19804", "pdf": "https://arxiv.org/pdf/2505.19804.pdf", "abs": "https://arxiv.org/abs/2505.19804", "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation", "authors": ["Siyuan Li", "Jian Chen", "Rui Yao", "Xuming Hu", "Peilin Zhou", "Weihua Qiu", "Simin Zhang", "Chucheng Dong", "Zhiyao Li", "Qipeng Xie", "Zixuan Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation."}
{"id": "2505.19806", "pdf": "https://arxiv.org/pdf/2505.19806.pdf", "abs": "https://arxiv.org/abs/2505.19806", "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness."}
{"id": "2505.19815", "pdf": "https://arxiv.org/pdf/2505.19815.pdf", "abs": "https://arxiv.org/abs/2505.19815", "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques."}
{"id": "2505.19838", "pdf": "https://arxiv.org/pdf/2505.19838.pdf", "abs": "https://arxiv.org/abs/2505.19838", "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "summary": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes."}
{"id": "2505.19848", "pdf": "https://arxiv.org/pdf/2505.19848.pdf", "abs": "https://arxiv.org/abs/2505.19848", "title": "Improving Multilingual Math Reasoning for African Languages", "authors": ["Odunayo Ogundepo", "Akintunde Oladipo", "Kelechi Ogueji", "Esther Adenuga", "David Ifeoluwa Adelani", "Jimmy Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model."}
{"id": "2505.19851", "pdf": "https://arxiv.org/pdf/2505.19851.pdf", "abs": "https://arxiv.org/abs/2505.19851", "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead."}
{"id": "2505.19862", "pdf": "https://arxiv.org/pdf/2505.19862.pdf", "abs": "https://arxiv.org/abs/2505.19862", "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL."}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912.pdf", "abs": "https://arxiv.org/abs/2505.19912", "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "authors": ["Javier Marín"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining."}
{"id": "2505.19914", "pdf": "https://arxiv.org/pdf/2505.19914.pdf", "abs": "https://arxiv.org/abs/2505.19914", "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io."}
{"id": "2505.19937", "pdf": "https://arxiv.org/pdf/2505.19937.pdf", "abs": "https://arxiv.org/abs/2505.19937", "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs", "authors": ["Pooneh Mousavi", "Yingzhi Wang", "Mirco Ravanelli", "Cem Subakan"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Spoken Language Understanding\n(SLU). Recent SLU models process audio directly by adapting speech input into\nLLMs for better multimodal learning. A key consideration for these models is\nthe cross-modal alignment between text and audio modalities, which is a\ntelltale sign as to whether or not LLM is able to associate semantic meaning to\naudio segments. While various methods exist for fusing these modalities, there\nis no standard metric to evaluate alignment quality in LLMs. In this work, we\npropose a new metric, ALAS (Automatic Latent Alignment Score). Our study\nexamines the correlation between audio and text representations across\ntransformer layers, for two different tasks (Spoken Question Answering and\nEmotion Recognition). We showcase that our metric behaves as expected across\ndifferent layers and different tasks."}
{"id": "2505.19959", "pdf": "https://arxiv.org/pdf/2505.19959.pdf", "abs": "https://arxiv.org/abs/2505.19959", "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "categories": ["cs.CL"], "comment": "Accepted by ACL'25 main track", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial."}
{"id": "2505.19970", "pdf": "https://arxiv.org/pdf/2505.19970.pdf", "abs": "https://arxiv.org/abs/2505.19970", "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM", "authors": ["Jiayuan Su", "Fulin Lin", "Zhaopeng Feng", "Han Zheng", "Teng Wang", "Zhenyu Xiao", "Xinlong Zhao", "Zuozhu Liu", "Lu Cheng", "Hongwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness."}
{"id": "2505.19971", "pdf": "https://arxiv.org/pdf/2505.19971.pdf", "abs": "https://arxiv.org/abs/2505.19971", "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language", "authors": ["Kilian Sennrich", "Sina Ahmadi"], "categories": ["cs.CL"], "comment": "Accepted to LDK 2025 - the 5th Conference on Language, Data and\n  Knowledge. Naples, Italy, 9-11 September 2025", "summary": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation."}
{"id": "2505.19978", "pdf": "https://arxiv.org/pdf/2505.19978.pdf", "abs": "https://arxiv.org/abs/2505.19978", "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Elena Baralis"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Currently under review. See the official website:\n  https://salt-research.github.io/DeepDialogue", "summary": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations."}
{"id": "2505.19987", "pdf": "https://arxiv.org/pdf/2505.19987.pdf", "abs": "https://arxiv.org/abs/2505.19987", "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation", "authors": ["Yongshi Ye", "Biao Fu", "Chongxuan Huang", "Yidong Chen", "Xiaodong Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts."}
{"id": "2505.20006", "pdf": "https://arxiv.org/pdf/2505.20006.pdf", "abs": "https://arxiv.org/abs/2505.20006", "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition", "authors": ["Raphaël Bagat", "Irina Illina", "Emmanuel Vincent"], "categories": ["cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR."}
{"id": "2505.20013", "pdf": "https://arxiv.org/pdf/2505.20013.pdf", "abs": "https://arxiv.org/abs/2505.20013", "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "categories": ["cs.CL"], "comment": "18 pages", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents."}
{"id": "2505.20014", "pdf": "https://arxiv.org/pdf/2505.20014.pdf", "abs": "https://arxiv.org/abs/2505.20014", "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation", "authors": ["Hoyun Song", "Huije Lee", "Jisu Shin", "Sukmin Cho", "Changgeon Ko", "Jong C. Park"], "categories": ["cs.CL"], "comment": null, "summary": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015.pdf", "abs": "https://arxiv.org/abs/2505.20015", "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "authors": ["Ramon Ferrer-i-Cancho"], "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression."}
{"id": "2505.20016", "pdf": "https://arxiv.org/pdf/2505.20016.pdf", "abs": "https://arxiv.org/abs/2505.20016", "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "authors": ["Chengrui Huang", "Shen Gao", "Zhengliang Shi", "Dongsheng Wang", "Shuo Shang"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets."}
{"id": "2505.20023", "pdf": "https://arxiv.org/pdf/2505.20023.pdf", "abs": "https://arxiv.org/abs/2505.20023", "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories."}
{"id": "2505.20045", "pdf": "https://arxiv.org/pdf/2505.20045.pdf", "abs": "https://arxiv.org/abs/2505.20045", "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs", "authors": ["Artem Vazhentsev", "Lyudmila Rvanova", "Gleb Kuzmin", "Ekaterina Fadeeva", "Ivan Lazichny", "Alexander Panchenko", "Maxim Panov", "Timothy Baldwin", "Mrinmaya Sachan", "Preslav Nakov", "Artem Shelmanov"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs."}
{"id": "2505.20047", "pdf": "https://arxiv.org/pdf/2505.20047.pdf", "abs": "https://arxiv.org/abs/2505.20047", "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline."}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072.pdf", "abs": "https://arxiv.org/abs/2505.20072", "title": "Incentivizing Reasoning from Weak Supervision", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR."}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081.pdf", "abs": "https://arxiv.org/abs/2505.20081", "title": "Inference-time Alignment in Continuous Space", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA"}
{"id": "2505.20088", "pdf": "https://arxiv.org/pdf/2505.20088.pdf", "abs": "https://arxiv.org/abs/2505.20088", "title": "Multi-Domain Explainability of Preferences", "authors": ["Nitay Calderon", "Liat Ein-Dor", "Roi Reichart"], "categories": ["cs.CL"], "comment": null, "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated end-to-end method for\ngenerating local and global concept-based explanations of preferences across\nmultiple domains. Our method employs an LLM to discover concepts that\ndifferentiate between chosen and rejected responses and represent them with\nconcept-based vectors. To model the relationships between concepts and\npreferences, we propose a white-box Hierarchical Multi-Domain Regression model\nthat captures both domain-general and domain-specific effects. To evaluate our\nmethod, we curate a dataset spanning eight challenging and diverse domains and\nexplain twelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two novel application-driven settings.\nFirst, guiding LLM outputs with concepts from LaaJ explanations yields\nresponses that those judges consistently prefer. Second, prompting LaaJs with\nconcepts explaining humans improves their preference predictions. Together, our\nwork provides a new paradigm for explainability in the era of LLMs."}
{"id": "2505.20096", "pdf": "https://arxiv.org/pdf/2505.20096.pdf", "abs": "https://arxiv.org/abs/2505.20096", "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG."}
{"id": "2505.20097", "pdf": "https://arxiv.org/pdf/2505.20097.pdf", "abs": "https://arxiv.org/abs/2505.20097", "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs", "authors": ["Liang Cheng", "Tianyi LI", "Zhaowei Wang", "Mark Steedman"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness"}
{"id": "2505.20099", "pdf": "https://arxiv.org/pdf/2505.20099.pdf", "abs": "https://arxiv.org/abs/2505.20099", "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Under Review", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities."}
{"id": "2505.20101", "pdf": "https://arxiv.org/pdf/2505.20101.pdf", "abs": "https://arxiv.org/abs/2505.20101", "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT.In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype.Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications."}
{"id": "2505.20109", "pdf": "https://arxiv.org/pdf/2505.20109.pdf", "abs": "https://arxiv.org/abs/2505.20109", "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to InterSpeech 2025", "summary": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment."}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112.pdf", "abs": "https://arxiv.org/abs/2505.20112", "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness."}
{"id": "2505.20113", "pdf": "https://arxiv.org/pdf/2505.20113.pdf", "abs": "https://arxiv.org/abs/2505.20113", "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences."}
{"id": "2505.20118", "pdf": "https://arxiv.org/pdf/2505.20118.pdf", "abs": "https://arxiv.org/abs/2505.20118", "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul Röttger", "Terry Ruas", "Bela Gipp"], "categories": ["cs.CL", "cs.CR"], "comment": "8 pages, 5 figures", "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous."}
{"id": "2505.20128", "pdf": "https://arxiv.org/pdf/2505.20128.pdf", "abs": "https://arxiv.org/abs/2505.20128", "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "categories": ["cs.CL"], "comment": "Working in process", "summary": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work."}
{"id": "2505.20133", "pdf": "https://arxiv.org/pdf/2505.20133.pdf", "abs": "https://arxiv.org/abs/2505.20133", "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines."}
{"id": "2505.20144", "pdf": "https://arxiv.org/pdf/2505.20144.pdf", "abs": "https://arxiv.org/abs/2505.20144", "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "an early-stage version", "summary": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition."}
{"id": "2505.20154", "pdf": "https://arxiv.org/pdf/2505.20154.pdf", "abs": "https://arxiv.org/abs/2505.20154", "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models", "authors": ["Xueyan Zhang", "Jinman Zhao", "Zhifei Yang", "Yibo Zhong", "Shuhao Guan", "Linbo Cao", "Yining Wang"], "categories": ["cs.CL"], "comment": "20 pages, 2 figures, 15 tables", "summary": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs."}
{"id": "2505.20155", "pdf": "https://arxiv.org/pdf/2505.20155.pdf", "abs": "https://arxiv.org/abs/2505.20155", "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs", "authors": ["Hanting Chen", "Jiarui Qin", "Jialong Guo", "Tao Yuan", "Yichun Yin", "Huiling Zhen", "Yasheng Wang", "Jinpeng Li", "Xiaojun Meng", "Meng Zhang", "Rongju Ruan", "Zheyuan Bai", "Yehui Tang", "Can Chen", "Xinghao Chen", "Fisher Yu", "Ruiming Tang", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s."}
{"id": "2505.20163", "pdf": "https://arxiv.org/pdf/2505.20163.pdf", "abs": "https://arxiv.org/abs/2505.20163", "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition", "authors": ["Moreno La Quatra", "Alkis Koudounas", "Valerio Mario Salerno", "Sabato Marco Siniscalchi"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Despite the remarkable progress in end-to-end Automatic Speech Recognition\n(ASR) engines, accurately transcribing dysarthric speech remains a major\nchallenge. In this work, we proposed a two-stage framework for the Speech\nAccessibility Project Challenge at INTERSPEECH 2025, which combines\ncutting-edge speech recognition models with LLM-based generative error\ncorrection (GER). We assess different configurations of model scales and\ntraining strategies, incorporating specific hypothesis selection to improve\ntranscription accuracy. Experiments on the Speech Accessibility Project dataset\ndemonstrate the strength of our approach on structured and spontaneous speech,\nwhile highlighting challenges in single-word recognition. Through comprehensive\nanalysis, we provide insights into the complementary roles of acoustic and\nlinguistic modeling in dysarthric speech recognition"}
{"id": "2505.20164", "pdf": "https://arxiv.org/pdf/2505.20164.pdf", "abs": "https://arxiv.org/abs/2505.20164", "title": "Visual Abstract Thinking Empowers Multimodal Reasoning", "authors": ["Dairu Liu", "Ziyue Wang", "Minyuan Ruan", "Fuwen Luo", "Chi Chen", "Peng Li", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Images usually convey richer detail than text, but often include redundant\ninformation which potentially downgrades multimodal reasoning performance. When\nfaced with lengthy or complex messages, humans tend to employ abstract thinking\nto convert them into simple and concise abstracts. Inspired by this cognitive\nstrategy, we introduce Visual Abstract Thinking (VAT), a novel thinking\nparadigm that prompts Multimodal Large Language Models (MLLMs) with visual\nabstract instead of explicit verbal thoughts or elaborate guidance, permitting\na more concentrated visual reasoning mechanism. Explicit thinking, such as\nChain-of-thought (CoT) or tool-augmented approaches, increases the complexity\nof reasoning process via inserting verbose intermediate steps, external\nknowledge or visual information. In contrast, VAT reduces redundant visual\ninformation and encourages models to focus their reasoning on more essential\nvisual elements. Experimental results show that VAT consistently empowers\ndifferent models, and achieves an average gain of 17% over GPT-4o baseline by\nemploying diverse types of visual abstracts, demonstrating that VAT can enhance\nvisual reasoning abilities for MLLMs regarding conceptual, structural and\nrelational reasoning tasks. VAT is also compatible with CoT in\nknowledge-intensive multimodal reasoning tasks. These findings highlight the\neffectiveness of visual reasoning via abstract thinking and encourage further\nexploration of more diverse reasoning paradigms from the perspective of human\ncognition."}
{"id": "2505.20176", "pdf": "https://arxiv.org/pdf/2505.20176.pdf", "abs": "https://arxiv.org/abs/2505.20176", "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Eliana Pastor", "Sabato Marco Siniscalchi", "Elena Baralis"], "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms."}
{"id": "2505.20184", "pdf": "https://arxiv.org/pdf/2505.20184.pdf", "abs": "https://arxiv.org/abs/2505.20184", "title": "THiNK: Can Large Language Models Think-aloud?", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository."}
{"id": "2505.20195", "pdf": "https://arxiv.org/pdf/2505.20195.pdf", "abs": "https://arxiv.org/abs/2505.20195", "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning", "authors": ["Xiaorong Wang", "Ting Yang", "Zhu Zhang", "Shuo Wang", "Zihan Zhou", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach."}
{"id": "2505.20199", "pdf": "https://arxiv.org/pdf/2505.20199.pdf", "abs": "https://arxiv.org/abs/2505.20199", "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "authors": ["Pengxiang Li", "Shilin Yan", "Joey Tsai", "Renrui Zhang", "Ruichuan An", "Ziyu Guo", "Xiaowei Gao"], "categories": ["cs.CL"], "comment": "Project page: https://github.com/pixeli99/A-CFG", "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation."}
{"id": "2505.20201", "pdf": "https://arxiv.org/pdf/2505.20201.pdf", "abs": "https://arxiv.org/abs/2505.20201", "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Harneet Singh Khanuja", "Yiqiao Jin", "Munmun De Choudhury"], "categories": ["cs.CL"], "comment": "33 pages, 5 figures, 30 tables", "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations."}
{"id": "2505.20209", "pdf": "https://arxiv.org/pdf/2505.20209.pdf", "abs": "https://arxiv.org/abs/2505.20209", "title": "How to Improve the Robustness of Closed-Source Models on NLI", "authors": ["Joe Stacey", "Lisa Alazraki", "Aran Ubhi", "Beyza Ermis", "Aaron Mueller", "Marek Rei"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Closed-source Large Language Models (LLMs) have become increasingly popular,\nwith impressive performance across a wide range of natural language tasks.\nThese models can be fine-tuned to further improve performance, but this often\nresults in the models learning from dataset-specific heuristics that reduce\ntheir robustness on out-of-distribution (OOD) data. Existing methods to improve\nrobustness either perform poorly, or are non-applicable to closed-source models\nbecause they assume access to model internals, or the ability to change the\nmodel's training procedure. In this work, we investigate strategies to improve\nthe robustness of closed-source LLMs through data-centric methods that do not\nrequire access to model internals. We find that the optimal strategy depends on\nthe complexity of the OOD data. For highly complex OOD datasets, upsampling\nmore challenging training examples can improve robustness by up to 1.5%. For\nless complex OOD datasets, replacing a portion of the training set with\nLLM-generated examples can improve robustness by 3.7%. More broadly, we find\nthat large-scale closed-source autoregressive LLMs are substantially more\nrobust than commonly used encoder models, and are a more appropriate choice of\nbaseline going forward."}
{"id": "2505.20215", "pdf": "https://arxiv.org/pdf/2505.20215.pdf", "abs": "https://arxiv.org/abs/2505.20215", "title": "Dependency Parsing is More Parameter-Efficient with Normalization", "authors": ["Paolo Gajo", "Domenic Rosati", "Hassan Sajjad", "Alberto Barrón-Cedeño"], "categories": ["cs.CL"], "comment": null, "summary": "Dependency parsing is the task of inferring natural language structure, often\napproached by modeling word interactions via attention through biaffine\nscoring. This mechanism works like self-attention in Transformers, where scores\nare calculated for every pair of words in a sentence. However, unlike\nTransformer attention, biaffine scoring does not use normalization prior to\ntaking the softmax of the scores. In this paper, we provide theoretical\nevidence and empirical results revealing that a lack of normalization\nnecessarily results in overparameterized parser models, where the extra\nparameters compensate for the sharp softmax outputs produced by high variance\ninputs to the biaffine scoring function. We argue that biaffine scoring can be\nmade substantially more efficient by performing score normalization. We conduct\nexperiments on six datasets for semantic and syntactic dependency parsing using\na one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's\nperformance with and without normalizing biaffine scores. Normalizing allows us\nto beat the state of the art on two datasets, with fewer samples and trainable\nparameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1"}
{"id": "2505.20225", "pdf": "https://arxiv.org/pdf/2505.20225.pdf", "abs": "https://arxiv.org/abs/2505.20225", "title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "authors": ["Hao Kang", "Zichun Yu", "Chenyan Xiong"], "categories": ["cs.CL", "cs.LG"], "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE."}
{"id": "2505.20231", "pdf": "https://arxiv.org/pdf/2505.20231.pdf", "abs": "https://arxiv.org/abs/2505.20231", "title": "Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Existing Task-Oriented Dialogue (TOD) systems primarily focus on\nsingle-session dialogues, limiting their effectiveness in long-term memory\naugmentation. To address this challenge, we introduce a MS-TOD dataset, the\nfirst multi-session TOD dataset designed to retain long-term memory across\nsessions, enabling fewer turns and more efficient task completion. This defines\na new benchmark task for evaluating long-term memory in multi-session TOD.\nBased on this new dataset, we propose a Memory-Active Policy (MAP) that\nimproves multi-session dialogue efficiency through a two-stage approach. 1)\nMemory-Guided Dialogue Planning retrieves intent-aligned history, identifies\nkey QA units via a memory judger, refines them by removing redundant questions,\nand generates responses based on the reconstructed memory. 2) Proactive\nResponse Strategy detects and correct errors or omissions, ensuring efficient\nand accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on\nresponse quality and effectiveness of the proactive strategy. Experiments on\nMS-TOD demonstrate that MAP significantly improves task success and turn\nefficiency in multi-session scenarios, while maintaining competitive\nperformance on conventional single-session tasks."}
{"id": "2505.20237", "pdf": "https://arxiv.org/pdf/2505.20237.pdf", "abs": "https://arxiv.org/abs/2505.20237", "title": "Efficient Speech Translation through Model Compression and Knowledge Distillation", "authors": ["Yasmin Moslem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models."}
{"id": "2505.20243", "pdf": "https://arxiv.org/pdf/2505.20243.pdf", "abs": "https://arxiv.org/abs/2505.20243", "title": "It's High Time: A Survey of Temporal Information Retrieval and Question Answering", "authors": ["Bhawna Piryani", "Abdelrahman Abdullah", "Jamshid Mozafari", "Avishek Anand", "Adam Jatowt"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization."}
{"id": "2505.20245", "pdf": "https://arxiv.org/pdf/2505.20245.pdf", "abs": "https://arxiv.org/abs/2505.20245", "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing", "authors": ["Rui Li", "Quanyu Dai", "Zeyu Zhang", "Xu Chen", "Zhenhua Dong", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains."}
{"id": "2505.20249", "pdf": "https://arxiv.org/pdf/2505.20249.pdf", "abs": "https://arxiv.org/abs/2505.20249", "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models", "authors": ["Yongan Yu", "Qingchen Hu", "Xianda Du", "Jiayin Wang", "Fengran Mo", "Renee Sieber"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters."}
{"id": "2505.20258", "pdf": "https://arxiv.org/pdf/2505.20258.pdf", "abs": "https://arxiv.org/abs/2505.20258", "title": "ARM: Adaptive Reasoning Model", "authors": ["Siye Wu", "Jian Xie", "Yikai Zhang", "Aili Chen", "Kai Zhang", "Yu Su", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage."}
{"id": "2505.20264", "pdf": "https://arxiv.org/pdf/2505.20264.pdf", "abs": "https://arxiv.org/abs/2505.20264", "title": "We Need to Measure Data Diversity in NLP -- Better and Broader", "authors": ["Dong Nguyen", "Esther Ploeger"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although diversity in NLP datasets has received growing attention, the\nquestion of how to measure it remains largely underexplored. This opinion paper\nexamines the conceptual and methodological challenges of measuring data\ndiversity and argues that interdisciplinary perspectives are essential for\ndeveloping more fine-grained and valid measures."}
{"id": "2505.20276", "pdf": "https://arxiv.org/pdf/2505.20276.pdf", "abs": "https://arxiv.org/abs/2505.20276", "title": "Does quantization affect models' performance on long-context tasks?", "authors": ["Anmol Mekala", "Anirudh Atmakuru", "Yixiao Song", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages of content with 9 figures. 37 remaining pages of references\n  and supplementary with 17 figures. Under review as of May 26", "summary": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English."}
{"id": "2505.20277", "pdf": "https://arxiv.org/pdf/2505.20277.pdf", "abs": "https://arxiv.org/abs/2505.20277", "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction", "authors": ["Haonan Zhang", "Run Luo", "Xiong Liu", "Yuchuan Wu", "Ting-En Lin", "Pengpeng Zeng", "Qiang Qu", "Feiteng Fang", "Min Yang", "Lianli Gao", "Jingkuan Song", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter."}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282.pdf", "abs": "https://arxiv.org/abs/2505.20282", "title": "One-shot Entropy Minimization", "authors": ["Zitian Gao", "Lynx Chen", "Joey Zhou", "Bryan Dai"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em."}
{"id": "2505.20285", "pdf": "https://arxiv.org/pdf/2505.20285.pdf", "abs": "https://arxiv.org/abs/2505.20285", "title": "MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability", "authors": ["Weiqi Wu", "Xin Guan", "Shen Huang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jiuxin Cao", "Hai Zhao", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MASKSEARCH. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMASKSEARCH significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks."}
{"id": "2505.20293", "pdf": "https://arxiv.org/pdf/2505.20293.pdf", "abs": "https://arxiv.org/abs/2505.20293", "title": "Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery", "authors": ["Yifan Sun", "Danding Wang", "Qiang Sheng", "Juan Cao", "Jintao Li"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Concept-based explainable approaches have emerged as a promising method in\nexplainable AI because they can interpret models in a way that aligns with\nhuman reasoning. However, their adaption in the text domain remains limited.\nMost existing methods rely on predefined concept annotations and cannot\ndiscover unseen concepts, while other methods that extract concepts without\nsupervision often produce explanations that are not intuitively comprehensible\nto humans, potentially diminishing user trust. These methods fall short of\ndiscovering comprehensible concepts automatically. To address this issue, we\npropose \\textbf{ECO-Concept}, an intrinsically interpretable framework to\ndiscover comprehensible concepts with no concept annotations. ECO-Concept first\nutilizes an object-centric architecture to extract semantic concepts\nautomatically. Then the comprehensibility of the extracted concepts is\nevaluated by large language models. Finally, the evaluation result guides the\nsubsequent model fine-tuning to obtain more understandable explanations.\nExperiments show that our method achieves superior performance across diverse\ntasks. Further concept evaluations validate that the concepts learned by\nECO-Concept surpassed current counterparts in comprehensibility."}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295.pdf", "abs": "https://arxiv.org/abs/2505.20295", "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "authors": ["Michael Kirchhof", "Luca Füger", "Adam Goliński", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. Our metric enables\nfuture works towards this universal form of LLM uncertainties."}
{"id": "2505.20296", "pdf": "https://arxiv.org/pdf/2505.20296.pdf", "abs": "https://arxiv.org/abs/2505.20296", "title": "Reasoning LLMs are Wandering Solution Explorers", "authors": ["Jiahao Lu", "Ziwei Xu", "Mohan Kankanhalli"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "71 pages, 14 figures, 2 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself."}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298.pdf", "abs": "https://arxiv.org/abs/2505.20298", "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga."}
{"id": "2309.03824", "pdf": "https://arxiv.org/pdf/2309.03824.pdf", "abs": "https://arxiv.org/abs/2309.03824", "title": "Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization", "authors": ["Habib Hajimolahoseini", "Walid Ahmed", "Yang Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low Rank Decomposition (LRD) is a model compression technique applied to the\nweight tensors of deep learning models in order to reduce the number of\ntrainable parameters and computational complexity. However, due to high number\nof new layers added to the architecture after applying LRD, it may not lead to\na high training/inference acceleration if the decomposition ranks are not small\nenough. The issue is that using small ranks increases the risk of significant\naccuracy drop after decomposition. In this paper, we propose two techniques for\naccelerating low rank decomposed models without requiring to use small ranks\nfor decomposition. These methods include rank optimization and sequential\nfreezing of decomposed layers. We perform experiments on both convolutional and\ntransformer-based models. Experiments show that these techniques can improve\nthe model throughput up to 60% during training and 37% during inference when\ncombined together while preserving the accuracy close to that of the original\nmodels"}
{"id": "2309.03965", "pdf": "https://arxiv.org/pdf/2309.03965.pdf", "abs": "https://arxiv.org/abs/2309.03965", "title": "Improving Resnet-9 Generalization Trained on Small Datasets", "authors": ["Omar Mohamed Awad", "Habib Hajimolahoseini", "Michael Lim", "Gurpreet Gosal", "Walid Ahmed", "Yang Liu", "Gordon Deng"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper presents our proposed approach that won the first prize at the\nICLR competition on Hardware Aware Efficient Training. The challenge is to\nachieve the highest possible accuracy in an image classification task in less\nthan 10 minutes. The training is done on a small dataset of 5000 images picked\nrandomly from CIFAR-10 dataset. The evaluation is performed by the competition\norganizers on a secret dataset with 1000 images of the same size. Our approach\nincludes applying a series of technique for improving the generalization of\nResNet-9 including: sharpness aware optimization, label smoothing, gradient\ncentralization, input patch whitening as well as metalearning based training.\nOur experiments show that the ResNet-9 can achieve the accuracy of 88% while\ntrained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets"}
{"id": "2311.03426", "pdf": "https://arxiv.org/pdf/2311.03426.pdf", "abs": "https://arxiv.org/abs/2311.03426", "title": "GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values", "authors": ["Farnoosh Javadi", "Walid Ahmed", "Habib Hajimolahoseini", "Foozhan Ataiefard", "Mohammad Hassanpour", "Saina Asani", "Austin Wen", "Omar Mohamed Awad", "Kangling Liu", "Yang Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Massive transformer-based models face several challenges, including slow and\ncomputationally intensive pre-training and over-parametrization. This paper\naddresses these challenges by proposing a versatile method called GQKVA, which\ngeneralizes query, key, and value grouping techniques. GQKVA is designed to\nspeed up transformer pre-training while reducing the model size. Our\nexperiments with various GQKVA variants highlight a clear trade-off between\nperformance and model size, allowing for customized choices based on resource\nand time limitations. Our findings also indicate that the conventional\nmulti-head attention approach is not always the best choice, as there are\nlighter and faster alternatives available. We tested our method on ViT, which\nachieved an approximate 0.3% increase in accuracy while reducing the model size\nby about 4% in the task of image classification. Additionally, our most\naggressive model reduction experiment resulted in a reduction of approximately\n15% in model size, with only around a 1% drop in accuracy."}
{"id": "2406.12634", "pdf": "https://arxiv.org/pdf/2406.12634.pdf", "abs": "https://arxiv.org/abs/2406.12634", "title": "News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation", "authors": ["Andreea Iana", "Fabian David Schmidt", "Goran Glavaš", "Heiko Paulheim"], "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; H.3.3"], "comment": "Accepted at the 47th European Conference on Information Retrieval\n  (ECIR 2025) Appendix A is provided only in the arXiv version", "summary": "Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation."}
{"id": "2407.20266", "pdf": "https://arxiv.org/pdf/2407.20266.pdf", "abs": "https://arxiv.org/abs/2407.20266", "title": "Accelerating the Low-Rank Decomposed Models", "authors": ["Habib Hajimolahoseini", "Walid Ahmed", "Austin Wen", "Yang Liu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Tensor decomposition is a mathematically supported technique for data\ncompression. It consists of applying some kind of a Low Rank Decomposition\ntechnique on the tensors or matrices in order to reduce the redundancy of the\ndata. However, it is not a popular technique for compressing the AI models duo\nto the high number of new layers added to the architecture after decomposition.\nAlthough the number of parameters could shrink significantly, it could result\nin the model be more than twice deeper which could add some latency to the\ntraining or inference. In this paper, we present a comprehensive study about\nhow to modify low rank decomposition technique in AI models so that we could\nbenefit from both high accuracy and low memory consumption as well as speeding\nup the training and inference"}
{"id": "2505.16849", "pdf": "https://arxiv.org/pdf/2505.16849.pdf", "abs": "https://arxiv.org/abs/2505.16849", "title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks", "authors": ["Martin Böckling", "Heiko Paulheim", "Andreea Iana"], "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025", "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research."}
{"id": "2505.18212", "pdf": "https://arxiv.org/pdf/2505.18212.pdf", "abs": "https://arxiv.org/abs/2505.18212", "title": "Towards medical AI misalignment: a preliminary study", "authors": ["Barbara Puccio", "Federico Castagna", "Allan Tucker", "Pierangelo Veltri"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite their staggering capabilities as assistant tools, often exceeding\nhuman performances, Large Language Models (LLMs) are still prone to jailbreak\nattempts from malevolent users. Although red teaming practices have already\nidentified and helped to address several such jailbreak techniques, one\nparticular sturdy approach involving role-playing (which we named `Goofy Game')\nseems effective against most of the current LLMs safeguards. This can result in\nthe provision of unsafe content, which, although not harmful per se, might lead\nto dangerous consequences if delivered in a setting such as the medical domain.\nIn this preliminary and exploratory study, we provide an initial analysis of\nhow, even without technical knowledge of the internal architecture and\nparameters of generative AI models, a malicious user could construct a\nrole-playing prompt capable of coercing an LLM into producing incorrect (and\npotentially harmful) clinical suggestions. We aim to illustrate a specific\nvulnerability scenario, providing insights that can support future advancements\nin the field."}
{"id": "2505.18221", "pdf": "https://arxiv.org/pdf/2505.18221.pdf", "abs": "https://arxiv.org/abs/2505.18221", "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs", "authors": ["Sharad Duwal", "Mir Nafis Sharear Shopnil", "Abhishek Tyagi", "Adiba Mahbub Proma"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Multimodal out-of-context (OOC) misinformation is misinformation that\nrepurposes real images with unrelated or misleading captions. Detecting such\nmisinformation is challenging because it requires resolving the context of the\nclaim before checking for misinformation. Many current methods, including LLMs\nand LVLMs, do not perform this contextualization step. LLMs hallucinate in\nabsence of context or parametric knowledge. In this work, we propose a\ngraph-based method that evaluates the consistency between the image and the\ncaption by constructing two graph representations: an evidence graph, derived\nfrom online textual evidence, and a claim graph, from the claim in the caption.\nUsing graph neural networks (GNNs) to encode and compare these representations,\nour framework then evaluates the truthfulness of image-caption pairs. We create\ndatasets for our graph-based method, evaluate and compare our baseline model\nagainst popular LLMs on the misinformation detection task. Our method scores\n$93.05\\%$ detection accuracy on the evaluation set and outperforms the\nsecond-best performing method (an LLM) by $2.82\\%$, making a case for smaller\nand task-specific methods."}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232.pdf", "abs": "https://arxiv.org/abs/2505.18232", "title": "ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Hongjian Fang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of Large language models (LLMs) in many fields is largely\nhindered by their high computational and memory costs. Recent studies suggest\nthat LLMs exhibit sparsity, which can be used for pruning. Previous pruning\nmethods typically follow a prune-then-finetune paradigm. Since the pruned parts\nstill contain valuable information, statically removing them without updating\nthe remaining parameters often results in irreversible performance degradation,\nrequiring costly recovery fine-tuning (RFT) to maintain performance. To address\nthis, we propose a novel paradigm: first apply regularization, then prune.\nBased on this paradigm, we propose ELDeR: Getting Efficient LLMs through\nData-Driven Regularized Layer-wise Pruning. We multiply the output of each\ntransformer layer by an initial weight, then we iteratively learn the weights\nof each transformer layer by using a small amount of data in a simple way.\nAfter that, we apply regularization to the difference between the output and\ninput of the layers with smaller weights, forcing the information to be\ntransferred to the remaining layers. Compared with direct pruning, ELDeR\nreduces the information loss caused by direct parameter removal, thus better\npreserving the model's language modeling ability. Experimental results show\nthat ELDeR achieves superior performance compared with powerful layer-wise\nstructured pruning methods, while greatly reducing RFT computational costs.\nSince ELDeR is a layer-wise pruning method, its end-to-end acceleration effect\nis obvious, making it a promising technique for efficient LLMs."}
{"id": "2505.18246", "pdf": "https://arxiv.org/pdf/2505.18246.pdf", "abs": "https://arxiv.org/abs/2505.18246", "title": "Will Large Language Models Transform Clinical Prediction?", "authors": ["Yusuf Yildiz", "Goran Nenadic", "Meghna Jani", "David A. Jenkins"], "categories": ["cs.CY", "cs.CL"], "comment": "Submitted to: BMC Diagnostic and Prognostic Research", "summary": "Background: Large language models (LLMs) are attracting increasing interest\nin healthcare. Their ability to summarise large datasets effectively, answer\nquestions accurately, and generate synthesised text is widely recognised. These\ncapabilities are already finding applications in healthcare. Body: This\ncommentary discusses LLMs usage in the clinical prediction context and\nhighlight potential benefits and existing challenges. In these early stages,\nthe focus should be on extending the methodology, specifically on validation,\nfairness and bias evaluation, survival analysis and development of regulations.\nConclusion: We conclude that further work and domain-specific considerations\nneed to be made for full integration into the clinical prediction workflows."}
{"id": "2505.18279", "pdf": "https://arxiv.org/pdf/2505.18279.pdf", "abs": "https://arxiv.org/abs/2505.18279", "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations."}
{"id": "2505.18350", "pdf": "https://arxiv.org/pdf/2505.18350.pdf", "abs": "https://arxiv.org/abs/2505.18350", "title": "Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?", "authors": ["Waleed Reda", "Abhinav Jangda", "Krishna Chintalapudi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly being adopted for narrow\ntasks - such as medical question answering or sentiment analysis - and deployed\nin resource-constrained settings, a key question arises: how many parameters\ndoes a task actually need? In this work, we present LLM-Sieve, the first\ncomprehensive framework for task-specific pruning of LLMs that achieves 20-75%\nparameter reduction with only 1-5% accuracy degradation across diverse domains.\nUnlike prior methods that apply uniform pruning or rely on low-rank\napproximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns\ntask-aware joint projections to better approximate output behavior, and (ii)\nemploys a Genetic Algorithm to discover differentiated pruning levels for each\nmatrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,\nand uniquely demonstrates strong generalization across datasets within the same\ntask domain. Together, these results establish a practical and robust mechanism\nto generate smaller performant task-specific models."}
{"id": "2505.18366", "pdf": "https://arxiv.org/pdf/2505.18366.pdf", "abs": "https://arxiv.org/abs/2505.18366", "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems", "authors": ["Hansa Meghwani", "Amit Agarwal", "Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Srikant Panda"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3; I.2.6; I.2.7"], "comment": "Accepted to ACL 2025", "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications."}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413.pdf", "abs": "https://arxiv.org/abs/2505.18413", "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.18451", "pdf": "https://arxiv.org/pdf/2505.18451.pdf", "abs": "https://arxiv.org/abs/2505.18451", "title": "$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts", "authors": ["Toshiaki Koike-Akino", "Jing Liu", "Ye Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "To tackle the huge computational demand of large foundation models,\nactivation-aware compression techniques without retraining have been\nintroduced. However, since these rely on calibration data, domain shift may\narise for unknown downstream tasks. With a computationally efficient\ncalibration, activation-aware pruning can be executed for every prompt\nadaptively, yet achieving reduced complexity at inference. We formulate it as a\nmixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate\nthat $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured\nsparsity on the fly."}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458.pdf", "abs": "https://arxiv.org/abs/2505.18458", "title": "A Survey of LLM $\\times$ DATA", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awsome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464.pdf", "abs": "https://arxiv.org/abs/2505.18464", "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies."}
{"id": "2505.18467", "pdf": "https://arxiv.org/pdf/2505.18467.pdf", "abs": "https://arxiv.org/abs/2505.18467", "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark", "authors": ["Unggi Lee", "Jaeyong Lee", "Jiyeong Bae", "Yeil Jeong", "Junbo Koh", "Gyeonggeon Lee", "Gunho Lee", "Taekyung Ahn", "Hyeoncheol Kim"], "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 5 figures, 4 tables", "summary": "Recent advances in large reasoning models (LRMs) show strong performance in\nstructured domains such as mathematics and programming; however, they often\nlack pedagogical coherence and realistic teaching behaviors. To bridge this\ngap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use\nthrough three innovations: (1) a distillation-based pipeline that filters and\nrefines model outputs for instruction-tuning, (2) the Well-balanced Educational\nBenchmark (WBEB), which evaluates performance across subject knowledge,\npedagogical knowledge, tracing, essay scoring, and teacher decision-making, and\n(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting\nteacher-style reasoning. Our mixed-method evaluation combines quantitative\nmetrics with qualitative analysis, providing the first systematic assessment of\nLRMs' pedagogical strengths and limitations."}
{"id": "2505.18488", "pdf": "https://arxiv.org/pdf/2505.18488.pdf", "abs": "https://arxiv.org/abs/2505.18488", "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications", "authors": ["Yanxiang Zhang", "Zheng Xu", "Shanshan Wu", "Yuanbo Zhang", "Daniel Ramage"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL Industry", "summary": "Error correction is an important capability when applying large language\nmodels (LLMs) to facilitate user typing on mobile devices. In this paper, we\nuse LLMs to synthesize a high-quality dataset of error correction pairs to\nevaluate and improve LLMs for mobile applications. We first prompt LLMs with\nerror correction domain knowledge to build a scalable and reliable addition to\nthe existing data synthesis pipeline. We then adapt the synthetic data\ndistribution to match the mobile application domain by reweighting the samples.\nThe reweighting model is learnt by predicting (a handful of) live A/B test\nmetrics when deploying LLMs in production, given the LLM performance on offline\nevaluation data and scores from a small privacy-preserving on-device language\nmodel. Finally, we present best practices for mixing our synthetic data with\nother data sources to improve model performance on error correction in both\noffline evaluation and production live A/B testing."}
{"id": "2505.18502", "pdf": "https://arxiv.org/pdf/2505.18502.pdf", "abs": "https://arxiv.org/abs/2505.18502", "title": "Knowledge Grafting of Large Language Models", "authors": ["Guodong Du", "Xuanning Zhou", "Junlin Li", "Zhuo Li", "Zesheng Shi", "Wanyu Lin", "Ho-Kin Tang", "Xiucheng Li", "Fangming Liu", "Wenya Wang", "Min Zhang", "Jing Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM."}
{"id": "2505.18512", "pdf": "https://arxiv.org/pdf/2505.18512.pdf", "abs": "https://arxiv.org/abs/2505.18512", "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking", "authors": ["Soyoung Yoon", "Gyuwan Kim", "Gyu-Hwung Cho", "Seung-won Hwang"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, 3 figures. The first two authors contributed equally.\n  Author order is randomly determined via coin toss", "summary": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models."}
{"id": "2505.18545", "pdf": "https://arxiv.org/pdf/2505.18545.pdf", "abs": "https://arxiv.org/abs/2505.18545", "title": "B-score: Detecting biases in large language models using response history", "authors": ["An Vo", "Mohammad Reza Taesiri", "Daeyoung Kim", "Anh Totti Nguyen"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025 (Main track)", "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io."}
{"id": "2505.18573", "pdf": "https://arxiv.org/pdf/2505.18573.pdf", "abs": "https://arxiv.org/abs/2505.18573", "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs", "authors": ["Mengqi Liao", "Xiangyu Xi", "Ruinian Chen", "Jia Leng", "Yangen Hu", "Ke Zeng", "Shuai Liu", "Huaiyu Wan"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) excel in complex tasks, which has\ndrawn significant attention to reinforcement learning (RL) for LLMs. However,\nexisting approaches allocate an equal number of rollouts to all questions\nduring the RL process, which is inefficient. This inefficiency stems from the\nfact that training on simple questions yields limited gains, whereas more\nrollouts are needed for challenging questions to sample correct answers.\nFurthermore, while RL improves response precision, it limits the model's\nexploration ability, potentially resulting in a performance cap below that of\nthe base model prior to RL. To address these issues, we propose a mechanism for\ndynamically allocating rollout budgets based on the difficulty of the problems,\nenabling more efficient RL training. Additionally, we introduce an adaptive\ndynamic temperature adjustment strategy to maintain the entropy at a stable\nlevel, thereby encouraging sufficient exploration. This enables LLMs to improve\nresponse precision while preserving their exploratory ability to uncover\npotential correct pathways. The code and data is available on:\nhttps://github.com/LiaoMengqi/E3-RL4LLMs"}
{"id": "2505.18585", "pdf": "https://arxiv.org/pdf/2505.18585.pdf", "abs": "https://arxiv.org/abs/2505.18585", "title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "authors": ["Yedi Zhang", "Sun Yi Emma", "Annabelle Lee Jia En", "Annabelle Lee Jia En", "Jin Song Dong"], "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as a dominant AI paradigm due to\ntheir exceptional text understanding and generation capabilities. However,\ntheir tendency to generate inconsistent or erroneous outputs challenges their\nreliability, especially in high-stakes domains requiring accuracy and\ntrustworthiness. Existing research primarily focuses on detecting and\nmitigating model misbehavior in general-purpose scenarios, often overlooking\nthe potential of integrating domain-specific knowledge. In this work, we\nadvance misbehavior detection by incorporating domain knowledge. The core idea\nis to design a general specification language that enables domain experts to\ncustomize domain-specific predicates in a lightweight and intuitive manner,\nsupporting later runtime verification of LLM outputs. To achieve this, we\ndesign a novel specification language, ESL, and introduce a runtime\nverification framework, RvLLM, to validate LLM output against domain-specific\nconstraints defined in ESL. We evaluate RvLLM on three representative tasks:\nviolation detection against Singapore Rapid Transit Systems Act, numerical\ncomparison, and inequality solving. Experimental results demonstrate that RvLLM\neffectively detects erroneous outputs across various LLMs in a lightweight and\nflexible manner. The results reveal that despite their impressive capabilities,\nLLMs remain prone to low-level errors due to limited interpretability and a\nlack of formal guarantees during inference, and our framework offers a\npotential long-term solution by leveraging expert domain knowledge to\nrigorously and efficiently verify LLM outputs."}
{"id": "2505.18644", "pdf": "https://arxiv.org/pdf/2505.18644.pdf", "abs": "https://arxiv.org/abs/2505.18644", "title": "Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving", "authors": ["Jingran Xie", "Xiang Li", "Hui Wang", "Yue Yu", "Yang Xiang", "Xixin Wu", "Zhiyong Wu"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Interspeech 2025", "summary": "Large language models (LLMs) have shown remarkable generalization across\ntasks, leading to increased interest in integrating speech with LLMs. These\nspeech LLMs (SLLMs) typically use supervised fine-tuning to align speech with\ntext-based LLMs. However, the lack of annotated speech data across a wide range\nof tasks hinders alignment efficiency, resulting in poor generalization. To\naddress these issues, we propose a novel multi-task 'behavior imitation' method\nwith speech-text interleaving, called MTBI, which relies solely on paired\nspeech and transcripts. By ensuring the LLM decoder generates equivalent\nresponses to paired speech and text, we achieve a more generalized SLLM.\nInterleaving is used to further enhance alignment efficiency. We introduce a\nsimple benchmark to evaluate prompt and task generalization across different\nmodels. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs\non both prompt and task generalization, while requiring less supervised speech\ndata."}
{"id": "2505.18646", "pdf": "https://arxiv.org/pdf/2505.18646.pdf", "abs": "https://arxiv.org/abs/2505.18646", "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation", "authors": ["Siwei Liu", "Jinyuan Fang", "Han Zhou", "Yingxu Wang", "Zaiqiao Meng"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated effectiveness in code\ngeneration tasks. To enable LLMs to address more complex coding challenges,\nexisting research has focused on crafting multi-agent systems with agentic\nworkflows, where complex coding tasks are decomposed into sub-tasks, assigned\nto specialized agents. Despite their effectiveness, current approaches heavily\nrely on hand-crafted agentic workflows, with both agent topologies and prompts\nmanually designed, which limits their ability to automatically adapt to\ndifferent types of coding problems. To address these limitations and enable\nautomated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving\n\\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that\nautomatically generates and optimises multi-agent workflows. Extensive\nexperiments on three coding benchmark datasets, including the challenging\nLiveCodeBench, demonstrate that our SEW can automatically design agentic\nworkflows and optimise them through self-evolution, bringing up to 33\\%\nimprovement on LiveCodeBench compared to using the backbone LLM only.\nFurthermore, by investigating different representation schemes of workflow, we\nprovide insights into the optimal way to encode workflow information with text."}
{"id": "2505.18668", "pdf": "https://arxiv.org/pdf/2505.18668.pdf", "abs": "https://arxiv.org/abs/2505.18668", "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation", "authors": ["Zhen Li", "Yukai Guo", "Duan Li", "Xinyuan Guo", "Bowen Li", "Lanxi Xiao", "Shenyu Qiao", "Jiashu Chen", "Zijian Wu", "Hui Zhang", "Xinhuan Shu", "Shixia Liu"], "categories": ["cs.CV", "cs.CL"], "comment": "63 pages, submitted to NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs."}
{"id": "2505.18675", "pdf": "https://arxiv.org/pdf/2505.18675.pdf", "abs": "https://arxiv.org/abs/2505.18675", "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps", "authors": ["Sicheng Feng", "Song Wang", "Shuyi Ouyang", "Lingdong Kong", "Zikai Song", "Jianke Zhu", "Huan Wang", "Xinchao Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models."}
{"id": "2505.18680", "pdf": "https://arxiv.org/pdf/2505.18680.pdf", "abs": "https://arxiv.org/abs/2505.18680", "title": "$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models", "authors": ["Yuanhe Zhang", "Xinyue Wang", "Haoran Gao", "Zhenhong Zhou", "Fanyu Meng", "Yuyao Zhang", "Sen Su"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), due to substantial computational requirements,\nare vulnerable to resource consumption attacks, which can severely degrade\nserver performance or even cause crashes, as demonstrated by denial-of-service\n(DoS) attacks designed for LLMs. However, existing works lack mitigation\nstrategies against such threats, resulting in unresolved security risks for\nreal-world LLM deployments. To this end, we propose the Pluggable and Dynamic\nDoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend\nagainst resource consumption attacks from both the input and output sides. On\nthe input side, we propose the Resource Index to guide Dynamic Request Polling\nScheduling, thereby reducing resource usage induced by malicious attacks under\nhigh-concurrency scenarios. On the output side, we introduce the Adaptive\nEnd-Based Suppression mechanism, which terminates excessive malicious\ngeneration early. Experiments across six models demonstrate that $PD^3F$\nsignificantly mitigates resource consumption attacks, improving users' access\ncapacity by up to 500% during adversarial load. $PD^3F$ represents a step\ntoward the resilient and resource-aware deployment of LLMs against resource\nconsumption attacks."}
{"id": "2505.18713", "pdf": "https://arxiv.org/pdf/2505.18713.pdf", "abs": "https://arxiv.org/abs/2505.18713", "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer", "authors": ["Guodong Du", "Zitao Fang", "Jing Li", "Junlin Li", "Runhua Jiang", "Shuyang Yu", "Yifei Guo", "Yangneng Chen", "Sim Kuan Goh", "Ho-Kin Tang", "Daojing He", "Honghai Liu", "Min Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ACL2025 Main", "summary": "Foundation models and their checkpoints have significantly advanced deep\nlearning, boosting performance across various applications. However, fine-tuned\nmodels often struggle outside their specific domains and exhibit considerable\nredundancy. Recent studies suggest that combining a pruned fine-tuned model\nwith the original pre-trained model can mitigate forgetting, reduce\ninterference when merging model parameters across tasks, and improve\ncompression efficiency. In this context, developing an effective pruning\nstrategy for fine-tuned models is crucial. Leveraging the advantages of the\ntask vector mechanism, we preprocess fine-tuned models by calculating the\ndifferences between them and the original model. Recognizing that different\ntask vector subspaces contribute variably to model performance, we introduce a\nnovel method called Neural Parameter Search (NPS-Pruning) for slimming down\nfine-tuned models. This method enhances pruning efficiency by searching through\nneural parameters of task vectors within low-rank subspaces. Our method has\nthree key applications: enhancing knowledge transfer through pairwise model\ninterpolation, facilitating effective knowledge fusion via model merging, and\nenabling the deployment of compressed models that retain near-original\nperformance while significantly reducing storage costs. Extensive experiments\nacross vision, NLP, and multi-modal benchmarks demonstrate the effectiveness\nand robustness of our approach, resulting in substantial performance gains. The\ncode is publicly available at: https://github.com/duguodong7/NPS-Pruning."}
{"id": "2505.18722", "pdf": "https://arxiv.org/pdf/2505.18722.pdf", "abs": "https://arxiv.org/abs/2505.18722", "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers", "authors": ["Terry Yi Zhong", "Esther Janse", "Cristian Tejedor-Garcia", "Louis ten Bosch", "Martha Larson"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted for Interspeech 2025 (Camera-Ready)", "summary": "Speech-based Parkinson's disease (PD) detection has gained attention for its\nautomated, cost-effective, and non-intrusive nature. As research studies\nusually rely on data from diagnostic-oriented speech tasks, this work explores\nthe feasibility of diagnosing PD on the basis of speech data not originally\nintended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our\nfindings indicate that TT can be as useful as diagnostic-oriented PD datasets\nlike PC-GITA. We also investigate which specific dataset characteristics impact\nPD classification performance. The results show that concatenating audio\nrecordings and balancing participants' gender and status distributions can be\nbeneficial. Cross-dataset evaluation reveals that models trained on PC-GITA\ngeneralize poorly to TT, whereas models trained on TT perform better on\nPC-GITA. Furthermore, we provide insights into the high variability across\nfolds, which is mainly due to large differences in individual speaker\nperformance."}
{"id": "2505.18789", "pdf": "https://arxiv.org/pdf/2505.18789.pdf", "abs": "https://arxiv.org/abs/2505.18789", "title": "From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Boris Ginsburg"], "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Post-processing is crucial for the automatic evaluation of LLMs in\nfill-in-the-middle (FIM) code generation due to the frequent presence of\nextraneous code in raw outputs. This extraneous generation suggests a lack of\nawareness regarding output boundaries, requiring truncation for effective\nevaluation. The determination of an optimal truncation strategy, however, often\nproves intricate, particularly when the scope includes several programming\nlanguages. This study investigates the necessity of post-processing\ninstruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning\nsignificantly enhances FIM code generation, enabling LLMs to generate code that\nseamlessly integrates with the surrounding context. Evaluating our fine-tuned\n\\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and\nSAFIM benchmarks demonstrates improved performances without post-processing,\nespecially when the \\emph{middle} consist of complete lines. However,\npost-processing of the LLM outputs remains necessary when the \\emph{middle} is\na random span of code."}
{"id": "2505.18822", "pdf": "https://arxiv.org/pdf/2505.18822.pdf", "abs": "https://arxiv.org/abs/2505.18822", "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting", "authors": ["Shijue Huang", "Hongru Wang", "Wanjun Zhong", "Zhaochen Su", "Jiazhan Feng", "Bowen Cao", "Yi R. Fung"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs."}
{"id": "2505.18830", "pdf": "https://arxiv.org/pdf/2505.18830.pdf", "abs": "https://arxiv.org/abs/2505.18830", "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization", "authors": ["Wenlong Deng", "Yi Ren", "Muchen Li", "Danica J. Sutherland", "Xiaoxiao Li", "Christos Thrampoulidis"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become popular in enhancing the reasoning\ncapabilities of large language models (LLMs), with Group Relative Policy\nOptimization (GRPO) emerging as a widely used algorithm in recent systems.\nDespite GRPO's widespread adoption, we identify a previously unrecognized\nphenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood\nof correct responses marginally increases or even decreases during training.\nThis behavior mirrors a recently discovered misalignment issue in Direct\nPreference Optimization (DPO), attributed to the influence of negative\ngradients. We provide a theoretical analysis of GRPO's learning dynamic,\nidentifying the source of LLD as the naive penalization of all tokens in\nincorrect responses with the same strength. To address this, we develop a\nmethod called NTHR, which downweights penalties on tokens contributing to the\nLLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's\ngroup-based structure, using correct responses as anchors to identify\ninfluential tokens. Experiments on math reasoning benchmarks demonstrate that\nNTHR effectively mitigates LLD, yielding consistent performance gains across\nmodels ranging from 0.5B to 3B parameters."}
{"id": "2505.18847", "pdf": "https://arxiv.org/pdf/2505.18847.pdf", "abs": "https://arxiv.org/abs/2505.18847", "title": "Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework", "authors": ["William Han", "Chaojing Duan", "Zhepeng Cen", "Yihang Yao", "Xiaoyu Song", "Atharva Mhaskar", "Dylan Leong", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "categories": ["cs.AI", "cs.CL"], "comment": "29 pages, 2 figures, 8 tables", "summary": "Recent advances have increasingly applied large language models (LLMs) to\nelectrocardiogram (ECG) interpretation, giving rise to\nElectrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual\nquery, an ELM autoregressively generates a free-form textual response. Unlike\ntraditional classification-based systems, ELMs emulate expert cardiac\nelectrophysiologists by issuing diagnoses, analyzing waveform morphology,\nidentifying contributing factors, and proposing patient-specific action plans.\nTo realize this potential, researchers are curating instruction-tuning datasets\nthat pair ECGs with textual dialogues and are training ELMs on these resources.\nYet before scaling ELMs further, there is a fundamental question yet to be\nexplored: What is the most effective ECG input representation? In recent works,\nthree candidate representations have emerged-raw time-series signals, rendered\nimages, and discretized symbolic sequences. We present the first comprehensive\nbenchmark of these modalities across 6 public datasets and 5 evaluation\nmetrics. We find symbolic representations achieve the greatest number of\nstatistically significant wins over both signal and image inputs. We further\nablate the LLM backbone, ECG duration, and token budget, and we evaluate\nrobustness to signal perturbations. We hope that our findings offer clear\nguidance for selecting input representations when developing the next\ngeneration of ELMs."}
{"id": "2505.18855", "pdf": "https://arxiv.org/pdf/2505.18855.pdf", "abs": "https://arxiv.org/abs/2505.18855", "title": "Inference Compute-Optimal Video Vision Language Models", "authors": ["Peiqi Wang", "ShengYun Peng", "Xuewen Zhang", "Hanchao Yu", "Yibo Yang", "Lifu Huang", "Fujun Liu", "Qifan Wang"], "categories": ["cs.CV", "cs.CL"], "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025", "summary": "This work investigates the optimal allocation of inference compute across\nthree key scaling factors in video vision language models: language model size,\nframe count, and the number of visual tokens per frame. While prior works\ntypically focuses on optimizing model efficiency or improving performance\nwithout considering resource constraints, we instead identify optimal model\nconfiguration under fixed inference compute budgets. We conduct large-scale\ntraining sweeps and careful parametric modeling of task performance to identify\nthe inference compute-optimal frontier. Our experiments reveal how task\nperformance depends on scaling factors and finetuning data size, as well as how\nchanges in data size shift the compute-optimal frontier. These findings\ntranslate to practical tips for selecting these scaling factors."}
{"id": "2505.18929", "pdf": "https://arxiv.org/pdf/2505.18929.pdf", "abs": "https://arxiv.org/abs/2505.18929", "title": "Meta-aware Learning in text-to-SQL Large Language Model", "authors": ["Wenda Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": "Keywords: text-to-SQL LLM, fine-tuning, meta-aware leanring,\n  metadata, chain-of-thought, BigQuery SQL, business database", "summary": "The advancements of Large language models (LLMs) have provided great\nopportunities to text-to-SQL tasks to overcome the main challenges to\nunderstand complex domain information and complex database structures in\nbusiness applications. In this paper, we propose a meta-aware learning\nframework to integrate domain knowledge, database schema, chain-of-thought\nreasoning processes, and metadata relationships to improve the SQL generation\nquality. The proposed framework includes four learning strategies: schema-based\nlearning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key\ninformation tokenization. This approach provides a comprehensive understanding\nof database structure and metadata information towards LLM through fine-tuning\nto improve its performance on SQL generation within business domains. Through\ntwo experimental studies, we have demonstrated the superiority of the proposed\nmethods in execution accuracy, multi-task SQL generation capability, and\nreduction of catastrophic forgetting."}
{"id": "2505.18931", "pdf": "https://arxiv.org/pdf/2505.18931.pdf", "abs": "https://arxiv.org/abs/2505.18931", "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?", "authors": ["Ryan Saklad", "Aman Chadha", "Oleg Pavlov", "Raha Moraffah"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning."}
{"id": "2505.18933", "pdf": "https://arxiv.org/pdf/2505.18933.pdf", "abs": "https://arxiv.org/abs/2505.18933", "title": "REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing", "authors": ["Haitian Zhong", "Yuhuan Liu", "Ziyang Xu", "Guofan Liu", "Qiang Liu", "Shu Wu", "Zhe Zhao", "Liang Wang", "Tieniu Tan"], "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 4 figures", "summary": "Large language model editing methods frequently suffer from overfitting,\nwherein factual updates can propagate beyond their intended scope,\noveremphasizing the edited target even when it's contextually inappropriate. To\naddress this challenge, we introduce REACT (Representation Extraction And\nControllable Tuning), a unified two-phase framework designed for precise and\ncontrollable knowledge editing. In the initial phase, we utilize tailored\nstimuli to extract latent factual representations and apply Principal Component\nAnalysis with a simple learnbale linear transformation to compute a directional\n\"belief shift\" vector for each instance. In the second phase, we apply\ncontrollable perturbations to hidden states using the obtained vector with a\nmagnitude scalar, gated by a pre-trained classifier that permits edits only\nwhen contextually necessary. Relevant experiments on EVOKE benchmarks\ndemonstrate that REACT significantly reduces overfitting across nearly all\nevaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our\nmethod preserves balanced basic editing performance (reliability, locality, and\ngenerality) under diverse editing scenarios."}
{"id": "2505.18942", "pdf": "https://arxiv.org/pdf/2505.18942.pdf", "abs": "https://arxiv.org/abs/2505.18942", "title": "Language Models Surface the Unwritten Code of Science and Society", "authors": ["Honglin Bao", "Siyang Wu", "Jiwoong Choi", "Yingrong Mao", "James A. Evans"], "categories": ["cs.CY", "cs.CL", "cs.DL"], "comment": null, "summary": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985.pdf", "abs": "https://arxiv.org/abs/2505.18985", "title": "STRICT: Stress Test of Rendering Images Containing Text", "authors": ["Tianyu Zhang", "Xinyu Wang", "Zhenghan Tai", "Lu Li", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "13 pages", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2505.19010", "pdf": "https://arxiv.org/pdf/2505.19010.pdf", "abs": "https://arxiv.org/abs/2505.19010", "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal learning has become a critical research area because integrating\ntext and image data can significantly improve performance in tasks such as\nclassification, retrieval, and scene understanding. However, despite progress\nwith pre-trained models, current approaches are limited by inadequate\ncross-modal interactions and static fusion strategies that do not fully exploit\nthe complementary nature of different modalities. To address these\nshortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that\nleverages dual-path encoding, co-attention with dimension-wise gating, and\nadvanced expert fusion. Our approach begins by projecting text and image\nfeatures into a common embedding space, where a dedicated co-attention\nmechanism enables simultaneous, fine-grained interactions between modalities.\nThis mechanism is further enhanced by a dimension-wise gating network that\nadaptively regulates the feature contributions at the channel level, ensuring\nthat only the most relevant information is emphasized. In parallel, dual-path\nencoders refine the representations by processing cross-modal information\nseparately before an additional cross-attention layer further aligns\nmodalities. The refined features are then aggregated via an expert fusion\nmodule that combines learned gating and self-attention to produce a robust,\nunified representation. We validate our approach on the MIMIC and SemEval\nMemotion 1.0, where experimental results demonstrate significant improvements\nin cross-modal alignment and state-of-the-art performance, underscoring the\npotential of our model for a wide range of multi-modal applications."}
{"id": "2505.19025", "pdf": "https://arxiv.org/pdf/2505.19025.pdf", "abs": "https://arxiv.org/abs/2505.19025", "title": "SQUiD: Synthesizing Relational Databases from Unstructured Text", "authors": ["Mushtari Sadia", "Zhenning Yang", "Yunming Xiao", "Ang Chen", "Amrita Roy Chowdhury"], "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Relational databases are central to modern data management, yet most data\nexists in unstructured forms like text documents. To bridge this gap, we\nleverage large language models (LLMs) to automatically synthesize a relational\ndatabase by generating its schema and populating its tables from raw text. We\nintroduce SQUiD, a novel neurosymbolic framework that decomposes this task into\nfour stages, each with specialized techniques. Our experiments show that SQUiD\nconsistently outperforms baselines across diverse datasets."}
{"id": "2505.19037", "pdf": "https://arxiv.org/pdf/2505.19037.pdf", "abs": "https://arxiv.org/abs/2505.19037", "title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "authors": ["Ke-Han Lu", "Chun-Yi Kuan", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL"], "comment": "Accecpted by Interspeech 2025;\n  https://github.com/kehanlu/Speech-IFEval", "summary": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics."}
{"id": "2505.19075", "pdf": "https://arxiv.org/pdf/2505.19075.pdf", "abs": "https://arxiv.org/abs/2505.19075", "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs", "authors": ["Jaemin Kim", "Hangeol Chang", "Hyunmin Hwang", "Choonghan Kim", "Jong Chul Ye"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms \\add{existing baseline fine-tuning methods using the\nLlama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"}
{"id": "2505.19155", "pdf": "https://arxiv.org/pdf/2505.19155.pdf", "abs": "https://arxiv.org/abs/2505.19155", "title": "Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs", "authors": ["Xuan Zhang", "Cunxiao Du", "Sicheng Yu", "Jiawei Wu", "Fengzhuo Zhang", "Wei Gao", "Qian Liu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Due to the auto-regressive nature of current video large language models\n(Video-LLMs), the inference latency increases as the input sequence length\ngrows, posing challenges for the efficient processing of video sequences that\nare usually very long. We observe that during decoding, the attention scores of\nmost tokens in Video-LLMs tend to be sparse and concentrated, with only certain\ntokens requiring comprehensive full attention. Based on this insight, we\nintroduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two\ndistinct modules: one leveraging sparse top-K attention and the other employing\ndense full attention. These modules collaborate to accelerate Video-LLMs\nwithout loss. The fast (sparse) model speculatively decodes multiple tokens,\nwhile the slow (dense) model verifies them in parallel. StD is a tuning-free,\nplug-and-play solution that achieves up to a 1.94$\\times$ walltime speedup in\nvideo processing. It maintains model performance while enabling a seamless\ntransition from a standard Video-LLM to a sparse Video-LLM with minimal code\nmodifications."}
{"id": "2505.19234", "pdf": "https://arxiv.org/pdf/2505.19234.pdf", "abs": "https://arxiv.org/abs/2505.19234", "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "authors": ["Jialong Zhou", "Lichao Wang", "Xiao Yang"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration face critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm, learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization."}
{"id": "2505.19277", "pdf": "https://arxiv.org/pdf/2505.19277.pdf", "abs": "https://arxiv.org/abs/2505.19277", "title": "Next Token Prediction Is a Dead End for Creativity", "authors": ["Ibukun Olatunji", "Mark Sheppard"], "categories": ["cs.AI", "cs.CL", "J.5; I.2.0; I.2.7"], "comment": "10 pages including references", "summary": "This paper argues that token prediction is fundamentally misaligned with real\ncreativity. While next-token models have enabled impressive advances in\nlanguage generation, their architecture favours surface-level coherence over\nspontaneity, originality, and improvisational risk. We use battle rap as a case\nstudy to expose the limitations of predictive systems, demonstrating that they\ncannot truly engage in adversarial or emotionally resonant exchanges. By\nreframing creativity as an interactive process rather than a predictive output,\nwe offer a vision for AI systems that are more expressive, responsive, and\naligned with human creative practice."}
{"id": "2505.19294", "pdf": "https://arxiv.org/pdf/2505.19294.pdf", "abs": "https://arxiv.org/abs/2505.19294", "title": "Towards Reliable Large Audio Language Model", "authors": ["Ziyang Ma", "Xiquan Li", "Yakun Song", "Wenxi Chen", "Chenpeng Du", "Jian Wu", "Yuanzhe Chen", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.MM", "eess.AS"], "comment": "ACL 2025 Findings", "summary": "Recent advancements in large audio language models (LALMs) have demonstrated\nimpressive results and promising prospects in universal understanding and\nreasoning across speech, music, and general sound. However, these models still\nlack the ability to recognize their knowledge boundaries and refuse to answer\nquestions they don't know proactively. While there have been successful\nattempts to enhance the reliability of LLMs, reliable LALMs remain largely\nunexplored. In this paper, we systematically investigate various approaches\ntowards reliable LALMs, including training-free methods such as multi-modal\nchain-of-thought (MCoT), and training-based methods such as supervised\nfine-tuning (SFT). Besides, we identify the limitations of previous evaluation\nmetrics and propose a new metric, the Reliability Gain Index (RGI), to assess\nthe effectiveness of different reliable methods. Our findings suggest that both\ntraining-free and training-based methods enhance the reliability of LALMs to\ndifferent extents. Moreover, we find that awareness of reliability is a \"meta\nability\", which can be transferred across different audio modalities, although\nsignificant structural and content differences exist among sound, music, and\nspeech."}
{"id": "2505.19302", "pdf": "https://arxiv.org/pdf/2505.19302.pdf", "abs": "https://arxiv.org/abs/2505.19302", "title": "ODIN: A NL2SQL Recommender to Handle Schema Ambiguity", "authors": ["Kapil Vaidya", "Abishek Sankararaman", "Jialin Ding", "Chuan Lei", "Xiao Qin", "Balakrishnan Narayanaswamy", "Tim Kraska"], "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "NL2SQL (natural language to SQL) systems translate natural language into SQL\nqueries, allowing users with no technical background to interact with databases\nand create tools like reports or visualizations. While recent advancements in\nlarge language models (LLMs) have significantly improved NL2SQL accuracy,\nschema ambiguity remains a major challenge in enterprise environments with\ncomplex schemas, where multiple tables and columns with semantically similar\nnames often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL\nrecommendation engine. Instead of producing a single SQL query given a natural\nlanguage question, ODIN generates a set of potential SQL queries by accounting\nfor different interpretations of ambiguous schema components. ODIN dynamically\nadjusts the number of suggestions based on the level of ambiguity, and ODIN\nlearns from user feedback to personalize future SQL query recommendations. Our\nevaluation shows that ODIN improves the likelihood of generating the correct\nSQL query by 1.5-2$\\times$ compared to baselines."}
{"id": "2505.19353", "pdf": "https://arxiv.org/pdf/2505.19353.pdf", "abs": "https://arxiv.org/abs/2505.19353", "title": "Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation", "authors": ["Camilo Chacón Sartori"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.SE"], "comment": "preprint", "summary": "With the rise of generative AI (GenAI), Large Language Models are\nincreasingly employed for code generation, becoming active co-authors alongside\nhuman programmers. Focusing specifically on this application domain, this paper\narticulates distinct ``Architectures of Error'' to ground an epistemic\ndistinction between human and machine code generation. Examined through their\nshared vulnerability to error, this distinction reveals fundamentally different\ncausal origins: human-cognitive versus artificial-stochastic. To develop this\nframework and substantiate the distinction, the analysis draws critically upon\nDennett's mechanistic functionalism and Rescher's methodological pragmatism. I\nargue that a systematic differentiation of these error profiles raises critical\nphilosophical questions concerning semantic coherence, security robustness,\nepistemic limits, and control mechanisms in human-AI collaborative software\ndevelopment. The paper also utilizes Floridi's levels of abstraction to provide\na nuanced understanding of how these error dimensions interact and may evolve\nwith technological advancements. This analysis aims to offer philosophers a\nstructured framework for understanding GenAI's unique epistemological\nchallenges, shaped by these architectural foundations, while also providing\nsoftware engineers a basis for more critically informed engagement."}
{"id": "2505.19356", "pdf": "https://arxiv.org/pdf/2505.19356.pdf", "abs": "https://arxiv.org/abs/2505.19356", "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval", "authors": ["Kidist Amde Mekonnen", "Yosef Worku Alemneh", "Maarten de Rijke"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50 (Primary), 68T05 (Secondary)", "H.3.3; H.3.1; I.2.7"], "comment": "10 pages (excluding references and appendix), 10 figures. Accepted to\n  ACL 2025 Findings. Public release includes dataset, code, and trained models:\n  https://github.com/kidist-amde/amharic-ir-benchmarks", "summary": "Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval. However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization. We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones. Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative\nimprovement in MRR@10 and a 9.86% gain in Recall@10 over the strongest\nmultilingual baseline, Arctic Embed 2.0 (568M parameters). More compact\nvariants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while\nbeing over 13x smaller. Additionally, we train a ColBERT-based late interaction\nretrieval model that achieves the highest MRR@10 score (0.843) among all\nevaluated models. We benchmark our proposed models against both sparse and\ndense retrieval baselines to systematically assess retrieval effectiveness in\nAmharic. Our analysis highlights key challenges in low-resource settings and\nunderscores the importance of language-specific adaptation. To foster future\nresearch in low-resource IR, we publicly release our dataset, codebase, and\ntrained models at https://github.com/kidist-amde/amharic-ir-benchmarks."}
{"id": "2505.19436", "pdf": "https://arxiv.org/pdf/2505.19436.pdf", "abs": "https://arxiv.org/abs/2505.19436", "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "authors": ["Ye Ye"], "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.8; H.3.3"], "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures", "summary": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings."}
{"id": "2505.19443", "pdf": "https://arxiv.org/pdf/2505.19443.pdf", "abs": "https://arxiv.org/abs/2505.19443", "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "35 Pages, 8 Figures, 6 Tables", "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle."}
{"id": "2505.19457", "pdf": "https://arxiv.org/pdf/2505.19457.pdf", "abs": "https://arxiv.org/abs/2505.19457", "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs", "authors": ["Guilong Lu", "Xuntao Guo", "Rongjunchen Zhang", "Wenqiao Zhu", "Ji Liu"], "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Project Page: https://hithink-research.github.io/BizFinBench/", "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench."}
{"id": "2505.19504", "pdf": "https://arxiv.org/pdf/2505.19504.pdf", "abs": "https://arxiv.org/abs/2505.19504", "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "summary": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."}
{"id": "2505.19536", "pdf": "https://arxiv.org/pdf/2505.19536.pdf", "abs": "https://arxiv.org/abs/2505.19536", "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut"}
{"id": "2505.19563", "pdf": "https://arxiv.org/pdf/2505.19563.pdf", "abs": "https://arxiv.org/abs/2505.19563", "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights", "authors": ["Shi-Yu Tian", "Zhi Zhou", "Wei Dong", "Ming Yang", "Kun-Yang Yu", "Zi-Jian Cheng", "Lan-Zhe Guo", "Yu-Feng Li"], "categories": ["cs.AI", "cs.CL"], "comment": "Paper under review, code and dataset are all available", "summary": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks."}
{"id": "2505.19578", "pdf": "https://arxiv.org/pdf/2505.19578.pdf", "abs": "https://arxiv.org/abs/2505.19578", "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy."}
{"id": "2505.19590", "pdf": "https://arxiv.org/pdf/2505.19590.pdf", "abs": "https://arxiv.org/abs/2505.19590", "title": "Learning to Reason without External Rewards", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor"}
{"id": "2505.19601", "pdf": "https://arxiv.org/pdf/2505.19601.pdf", "abs": "https://arxiv.org/abs/2505.19601", "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2."}
{"id": "2505.19621", "pdf": "https://arxiv.org/pdf/2505.19621.pdf", "abs": "https://arxiv.org/abs/2505.19621", "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models", "authors": ["George Kour", "Itay Nakash", "Ateret Anaby-Tavor", "Michal Shmueli-Scheuer"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS"}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641.pdf", "abs": "https://arxiv.org/abs/2505.19641", "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2505.19683", "pdf": "https://arxiv.org/pdf/2505.19683.pdf", "abs": "https://arxiv.org/abs/2505.19683", "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field."}
{"id": "2505.19752", "pdf": "https://arxiv.org/pdf/2505.19752.pdf", "abs": "https://arxiv.org/abs/2505.19752", "title": "Discrete Markov Bridge", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches."}
{"id": "2505.19757", "pdf": "https://arxiv.org/pdf/2505.19757.pdf", "abs": "https://arxiv.org/abs/2505.19757", "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "authors": ["Maria Dziuba", "Valentin Malykh"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments."}
{"id": "2505.19770", "pdf": "https://arxiv.org/pdf/2505.19770.pdf", "abs": "https://arxiv.org/abs/2505.19770", "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO", "authors": ["Ruizhe Shi", "Minhak Song", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "categories": ["cs.LG", "cs.CL"], "comment": "30 pages, 5 figures", "summary": "We present a fine-grained theoretical analysis of the performance gap between\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO) under a representation gap. Our study decomposes this gap\ninto two sources: an explicit representation gap under exact optimization and\nan implicit representation gap under finite samples. In the exact optimization\nsetting, we characterize how the relative capacities of the reward and policy\nmodel classes influence the final policy qualities. We show that RLHF, DPO, or\nonline DPO can outperform one another depending on the type of model\nmis-specifications. Notably, online DPO can outperform both RLHF and standard\nDPO when the reward and policy model classes are isomorphic and both\nmis-specified. In the approximate optimization setting, we provide a concrete\nconstruction where the ground-truth reward is implicitly sparse and show that\nRLHF requires significantly fewer samples than DPO to recover an effective\nreward model -- highlighting a statistical advantage of two-stage learning.\nTogether, these results provide a comprehensive understanding of the\nperformance gap between RLHF and DPO under various settings, and offer\npractical insights into when each method is preferred."}
{"id": "2505.19866", "pdf": "https://arxiv.org/pdf/2505.19866.pdf", "abs": "https://arxiv.org/abs/2505.19866", "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget."}
{"id": "2505.19893", "pdf": "https://arxiv.org/pdf/2505.19893.pdf", "abs": "https://arxiv.org/abs/2505.19893", "title": "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining", "authors": ["Melis Ilayda Bal", "Volkan Cevher", "Michael Muehlebach"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language model pretraining is compute-intensive, yet many tokens\ncontribute marginally to learning, resulting in inefficiency. We introduce\nEfficient Selective Language Modeling (ESLM), a risk-aware algorithm that\nimproves training efficiency and distributional robustness by performing online\ntoken-level batch selection. ESLM leverages per-token statistics (e.g., entropy\nor loss) and applies value-at-risk thresholding to retain only the most\ninformative tokens per batch. This data-centric mechanism reshapes the training\nloss, prioritizing high-risk tokens and eliminating redundant gradient\ncomputation. We frame ESLM as a bilevel game: the model competes with a masking\nadversary that selects worst-case token subsets under a constrained\nthresholding rule. In the loss-based setting, ESLM recovers conditional\nvalue-at-risk loss minimization, providing a principled connection to\ndistributionally robust optimization. We extend our approach to Ada-ESLM, which\nadaptively tunes the selection confidence during training. Experiments on GPT-2\npretraining show that ESLM significantly reduces training FLOPs while\nmaintaining or improving both perplexity and downstream performance compared to\nbaselines. Our approach also scales across model sizes, pretraining corpora,\nand integrates naturally with knowledge distillation."}
{"id": "2505.19896", "pdf": "https://arxiv.org/pdf/2505.19896.pdf", "abs": "https://arxiv.org/abs/2505.19896", "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "comment": "Non revised version for paper going to be published in Journal of\n  Advances in Space Research", "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases"}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897.pdf", "abs": "https://arxiv.org/abs/2505.19897", "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944.pdf", "abs": "https://arxiv.org/abs/2505.19944", "title": "Can Visual Encoder Learn to See Arrows?", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.19954", "pdf": "https://arxiv.org/pdf/2505.19954.pdf", "abs": "https://arxiv.org/abs/2505.19954", "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning", "authors": ["Andrew Zamai", "Nathanael Fijalkow", "Boris Mansencal", "Laurent Simon", "Eloi Navet", "Pierrick Coupe"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions."}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955.pdf", "abs": "https://arxiv.org/abs/2505.19955", "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "40 pages, 7 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."}
{"id": "2505.19956", "pdf": "https://arxiv.org/pdf/2505.19956.pdf", "abs": "https://arxiv.org/abs/2505.19956", "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased."}
{"id": "2505.19964", "pdf": "https://arxiv.org/pdf/2505.19964.pdf", "abs": "https://arxiv.org/abs/2505.19964", "title": "The Limits of Preference Data for Post-Training", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors."}
{"id": "2505.19997", "pdf": "https://arxiv.org/pdf/2505.19997.pdf", "abs": "https://arxiv.org/abs/2505.19997", "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Mengze Li", "Yumeng Zhu", "Ang Li", "Kun Kuang", "Fei Wu"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy."}
{"id": "2505.20027", "pdf": "https://arxiv.org/pdf/2505.20027.pdf", "abs": "https://arxiv.org/abs/2505.20027", "title": "Multi-modal brain encoding models for multi-modal stimuli", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "summary": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain."}
{"id": "2505.20046", "pdf": "https://arxiv.org/pdf/2505.20046.pdf", "abs": "https://arxiv.org/abs/2505.20046", "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning", "authors": ["Le Zhang", "Bo Wang", "Xipeng Qiu", "Siva Reddy", "Aishwarya Agrawal"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking."}
{"id": "2505.20050", "pdf": "https://arxiv.org/pdf/2505.20050.pdf", "abs": "https://arxiv.org/abs/2505.20050", "title": "MVP: Multi-source Voice Pathology detection", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Gabriele Ciravegna", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli", "Sabato Marco Siniscalchi", "Elena Baralis"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Voice disorders significantly impact patient quality of life, yet\nnon-invasive automated diagnosis remains under-explored due to both the\nscarcity of pathological voice data, and the variability in recording sources.\nThis work introduces MVP (Multi-source Voice Pathology detection), a novel\napproach that leverages transformers operating directly on raw voice signals.\nWe explore three fusion strategies to combine sentence reading and sustained\nvowel recordings: waveform concatenation, intermediate feature fusion, and\ndecision-level combination. Empirical validation across the German, Portuguese,\nand Italian languages shows that intermediate feature fusion using transformers\nbest captures the complementary characteristics of both recording types. Our\napproach achieves up to +13% AUC improvement over single-source methods."}
{"id": "2505.20053", "pdf": "https://arxiv.org/pdf/2505.20053.pdf", "abs": "https://arxiv.org/abs/2505.20053", "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements."}
{"id": "2505.20063", "pdf": "https://arxiv.org/pdf/2505.20063.pdf", "abs": "https://arxiv.org/abs/2505.20063", "title": "SAEs Are Good for Steering -- If You Select the Right Features", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods."}
{"id": "2505.20087", "pdf": "https://arxiv.org/pdf/2505.20087.pdf", "abs": "https://arxiv.org/abs/2505.20087", "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models", "authors": ["Makesh Narsimhan Sreedhar", "Traian Rebedea", "Christopher Parisien"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems."}
{"id": "2505.20103", "pdf": "https://arxiv.org/pdf/2505.20103.pdf", "abs": "https://arxiv.org/abs/2505.20103", "title": "SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment", "authors": ["Xiangyu Li", "Jingqiang Chen"], "categories": ["cs.DL", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Citations are crucial in scientific research articles as they highlight the\nconnection between the current study and prior work. However, this process is\noften time-consuming for researchers. In this study, we propose the SciRGC\nframework, which aims to automatically recommend citation articles and generate\ncitation sentences for citation locations within articles. The framework\naddresses two key challenges in academic citation generation: 1) how to\naccurately identify the author's citation intent and find relevant citation\npapers, and 2) how to generate high-quality citation sentences that align with\nhuman preferences. We enhance citation recommendation accuracy in the citation\narticle recommendation module by incorporating citation networks and sentiment\nintent, and generate reasoning-based citation sentences in the citation\nsentence generation module by using the original article abstract, local\ncontext, citation intent, and recommended articles as inputs. Additionally, we\npropose a new evaluation metric to fairly assess the quality of generated\ncitation sentences. Through comparisons with baseline models and ablation\nexperiments, the SciRGC framework not only improves the accuracy and relevance\nof citation recommendations but also ensures the appropriateness of the\ngenerated citation sentences in context, providing a valuable tool for\ninterdisciplinary researchers."}
{"id": "2505.20139", "pdf": "https://arxiv.org/pdf/2505.20139.pdf", "abs": "https://arxiv.org/abs/2505.20139", "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 9 figures, 13 tables", "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152.pdf", "abs": "https://arxiv.org/abs/2505.20152", "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM."}
{"id": "2505.20161", "pdf": "https://arxiv.org/pdf/2505.20161.pdf", "abs": "https://arxiv.org/abs/2505.20161", "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning", "authors": ["Jaehun Jung", "Seungju Han", "Ximing Lu", "Skyler Hallinan", "David Acuna", "Shrimai Prabhumoye", "Mostafa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Effective generalization in language models depends critically on the\ndiversity of their training data. Yet existing diversity metrics often fall\nshort of this goal, relying on surface-level heuristics that are decoupled from\nmodel behavior. This motivates us to ask: What kind of diversity in training\ndata actually drives generalization in language models -- and how can we\nmeasure and amplify it? Through large-scale empirical analyses spanning over\n300 training runs, carefully controlled for data scale and quality, we show\nthat data diversity can be a strong predictor of generalization in LLM\nreasoning -- as measured by average model performance on unseen\nout-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies\ndiversity via the entropy of model-induced gradients. Despite using a small\noff-the-shelf proxy model for gradients, G-Vendi consistently outperforms\nalternative measures, achieving strong correlation (Spearman's $\\rho \\approx\n0.9$) with out-of-distribution (OOD) performance on both natural language\ninference (NLI) and math reasoning tasks. Building on this insight, we present\nPrismatic Synthesis, a framework for generating diverse synthetic data by\ntargeting underrepresented regions in gradient space. Experimental results show\nthat Prismatic Synthesis consistently improves model performance as we scale\nsynthetic data -- not just on in-distribution test but across unseen,\nout-of-distribution benchmarks -- significantly outperforming state-of-the-art\nmodels that rely on 20 times larger data generator than ours. For example,\nPrismMath-7B, our model distilled from a 32B LLM, outperforms\nR1-Distill-Qwen-7B -- the same base model trained on proprietary data generated\nby 671B R1 -- on 6 out of 7 challenging benchmarks."}
{"id": "2505.20166", "pdf": "https://arxiv.org/pdf/2505.20166.pdf", "abs": "https://arxiv.org/abs/2505.20166", "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Project Website: https://kuan2jiu99.github.io/Balsa", "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\nimportant textual capabilities such as instruction-following are lost after\ntraining on audio data. In some cases, models may even hallucinate sounds that\nare not present in the input audio, raising concerns about their reliability.\nSecond, achieving cross-modal alignment between audio and language typically\nrelies on large collections of task-specific question-answer pairs for\ninstruction tuning, making the process resource-intensive. To address these\nissues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose\ncaption-style alignment data. We refer to this process as bootstrapping\naudio-language alignment via synthetic data generation from backbone LLMs\n(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method designed\nto improve ALLMs' ability to distinguish between present and absent sounds. We\nfurther extend BALSa to multi-audio scenarios, where the model either explains\nthe differences between audio inputs or produces a unified caption that\ndescribes them all, thereby enhancing audio-language alignment. Experimental\nresults indicate that our method effectively mitigates audio hallucinations\nwhile reliably maintaining strong performance in audio understanding,\nreasoning, and instruction-following skills. Moreover, incorporating\nmulti-audio training further enhances the model's comprehension and reasoning\ncapabilities. Overall, BALSa offers an efficient and scalable approach to the\ndevelopment of ALLMs."}
{"id": "2505.20246", "pdf": "https://arxiv.org/pdf/2505.20246.pdf", "abs": "https://arxiv.org/abs/2505.20246", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning."}
{"id": "2505.20251", "pdf": "https://arxiv.org/pdf/2505.20251.pdf", "abs": "https://arxiv.org/abs/2505.20251", "title": "Learning Extrapolative Sequence Transformations from Markov Chains", "authors": ["Sophia Hager", "Aleem Khan", "Andrew Wang", "Nicholas Andrews"], "categories": ["cs.LG", "cs.CL"], "comment": "To be published at the Forty-Second International Conference on\n  Machine Learning", "summary": "Most successful applications of deep learning involve similar training and\ntest conditions. However, tasks such as biological sequence design involve\nsearching for sequences that improve desirable properties beyond previously\nknown values, which requires novel hypotheses that \\emph{extrapolate} beyond\ntraining data. In these settings, extrapolation may be achieved by using random\nsearch methods such as Markov chain Monte Carlo (MCMC), which, given an initial\nstate, sample local transformations to approximate a target density that\nrewards states with the desired properties. However, even with a well-designed\nproposal, MCMC may struggle to explore large structured state spaces\nefficiently. Rather than relying on stochastic search, it would be desirable to\nhave a model that greedily optimizes the properties of interest, successfully\nextrapolating in as few steps as possible. We propose to learn such a model\nfrom the Markov chains resulting from MCMC search. Specifically, our approach\nuses selected states from Markov chains as a source of training data for an\nautoregressive model, which is then able to efficiently generate novel\nsequences that extrapolate along the sequence-level properties of interest. The\nproposed approach is validated on three problems: protein sequence design, text\nsentiment control, and text anonymization. We find that the autoregressive\nmodel can extrapolate as well or better than MCMC, but with the additional\nbenefits of scalability and significantly higher sample efficiency."}
{"id": "2505.20254", "pdf": "https://arxiv.org/pdf/2505.20254.pdf", "abs": "https://arxiv.org/abs/2505.20254", "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs", "authors": ["Xiangchen Song", "Aashiq Muhamed", "Yujia Zheng", "Lingjing Kong", "Zeyu Tang", "Mona T. Diab", "Virginia Smith", "Kun Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI."}
{"id": "2505.20259", "pdf": "https://arxiv.org/pdf/2505.20259.pdf", "abs": "https://arxiv.org/abs/2505.20259", "title": "Lifelong Safety Alignment for Language Models", "authors": ["Haoyu Wang", "Zeyu Qin", "Yifei Zhao", "Chao Du", "Min Lin", "Xueqian Wang", "Tianyu Pang"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment."}
{"id": "2505.20278", "pdf": "https://arxiv.org/pdf/2505.20278.pdf", "abs": "https://arxiv.org/abs/2505.20278", "title": "The Coverage Principle: A Framework for Understanding Compositional Generalization", "authors": ["Hoyeon Chang", "Jinho Park", "Hanseul Cho", "Sohee Yang", "Miyoung Ko", "Hyeonbin Hwang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6"], "comment": null, "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\n\\emph{mechanism-based} taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality."}
{"id": "2505.20279", "pdf": "https://arxiv.org/pdf/2505.20279.pdf", "abs": "https://arxiv.org/abs/2505.20279", "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction", "authors": ["Zhiwen Fan", "Jian Zhang", "Renjie Li", "Junge Zhang", "Runjin Chen", "Hezhen Hu", "Kevin Wang", "Huaizhi Qu", "Dilin Wang", "Zhicheng Yan", "Hongyu Xu", "Justin Theiss", "Tianlong Chen", "Jiachen Li", "Zhengzhong Tu", "Zhangyang Wang", "Rakesh Ranjan"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability."}
{"id": "2505.20291", "pdf": "https://arxiv.org/pdf/2505.20291.pdf", "abs": "https://arxiv.org/abs/2505.20291", "title": "Visualized Text-to-Image Retrieval", "authors": ["Di Wu", "Yixin Wan", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image\n(T2I) retrieval that mitigates the limitations of cross-modal similarity\nalignment of existing multi-modal embeddings. VisRet first projects textual\nqueries into the image modality via T2I generation. Then, it performs retrieval\nwithin the image modality to bypass the weaknesses of cross-modal retrievers in\nrecognizing subtle visual-spatial features. Experiments on three\nknowledge-intensive T2I retrieval benchmarks, including a newly introduced\nmulti-entity benchmark, demonstrate that VisRet consistently improves T2I\nretrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet\nalso significantly benefits downstream visual question answering accuracy when\nused in retrieval-augmented generation pipelines. The method is plug-and-play\nand compatible with off-the-shelf retrievers, making it an effective module for\nknowledge-intensive multi-modal systems. Our code and the new benchmark are\npublicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve."}
{"id": "2505.20297", "pdf": "https://arxiv.org/pdf/2505.20297.pdf", "abs": "https://arxiv.org/abs/2505.20297", "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation", "authors": ["Qinyu Zhao", "Jaskirat Singh", "Ming Xu", "Akshay Asthana", "Stephen Gould", "Liang Zheng"], "categories": ["cs.CV", "cs.CL"], "comment": "Our code is available at https://github.com/Qinyu-Allen-Zhao/DiSA", "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$\nfor FlowAR and xAR, while maintaining the generation quality."}
{"id": "2112.11479", "pdf": "https://arxiv.org/pdf/2112.11479.pdf", "abs": "https://arxiv.org/abs/2112.11479", "title": "AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection", "authors": ["Geet Shingi", "Vedangi Wagh"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. Proposed work gives a much\nsimpler approach which is not only at par with these complex networks but also\nexceeds performance with the use of subword tokenization algorithms like BPE\nand Unigram, along with multi-head attention-based techniques, giving an\naccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use\nof BPE and Unigram algorithms help handle the nonconventional Hinglish\nvocabulary making the proposed technique simple, efficient and sustainable to\nuse in the real world."}
{"id": "2211.05414", "pdf": "https://arxiv.org/pdf/2211.05414.pdf", "abs": "https://arxiv.org/abs/2211.05414", "title": "ADEPT: A DEbiasing PrompT Framework", "authors": ["Ke Yang", "Charles Yu", "Yi Fung", "Manling Li", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Several works have proven that finetuning is an applicable approach for\ndebiasing contextualized word embeddings. Similarly, discrete prompts with\nsemantic meanings have shown to be effective in debiasing tasks. With unfixed\nmathematical representation at the token level, continuous prompts usually\nsurpass discrete ones at providing a pre-trained language model (PLM) with\nadditional task-specific information. Despite this, relatively few efforts have\nbeen made to debias PLMs by prompt tuning with continuous prompts compared to\nits discrete counterpart. Furthermore, for most debiasing methods that alter a\nPLM's original parameters, a major problem is the need to not only decrease the\nbias in the PLM but also to ensure that the PLM does not lose its\nrepresentation ability. Finetuning methods typically have a hard time\nmaintaining this balance, as they tend to violently remove meanings of\nattribute words. In this paper, we propose ADEPT, a method to debias PLMs using\nprompt tuning while maintaining the delicate balance between removing biases\nand ensuring representation ability. To achieve this, we propose a new training\ncriterion inspired by manifold learning and equip it with an explicit debiasing\nterm to optimize prompt tuning. In addition, we conduct several experiments\nwith regard to the reliability, quality, and quantity of a previously proposed\nattribute training corpus in order to obtain a clearer prototype of a certain\nattribute, which indicates the attribute's position and relative distances to\nother words on the manifold. We evaluate ADEPT on several widely acknowledged\ndebiasing benchmarks and downstream tasks, and find that it achieves\ncompetitive results while maintaining (and in some cases even improving) the\nPLM's representation ability. We further visualize words' correlation before\nand after debiasing a PLM, and give some possible explanations for the visible\neffects."}
{"id": "2309.12646", "pdf": "https://arxiv.org/pdf/2309.12646.pdf", "abs": "https://arxiv.org/abs/2309.12646", "title": "The More Similar, the Better? Associations between Latent Semantic Similarity and Emotional Experiences Differ across Conversation Contexts", "authors": ["Chen-Wei Yu", "Yun-Shiuan Chuang", "Alexandros N. Lotsos", "Tabea Meier", "Claudia M. Haase"], "categories": ["cs.CL"], "comment": null, "summary": "Latent semantic similarity (LSS) is a measure of the similarity of\ninformation exchanges in a conversation. Challenging the assumption that higher\nLSS bears more positive psychological meaning, we propose that this association\nmight depend on the type of conversation people have. On the one hand, the\nshare-mind perspective would predict that higher LSS should be associated with\nmore positive emotional experiences across the board. The broaden-and-build\ntheory, on the other hand, would predict that higher LSS should be inversely\nassociated with more positive emotional experiences specifically in pleasant\nconversations. Linear mixed modeling based on conversations among 50 long-term\nmarried couples supported the latter prediction. That is, partners experienced\ngreater positive emotions when their overall information exchanges were more\ndissimilar in pleasant (but not conflict) conversations. This work highlights\nthe importance of context in understanding the emotional correlates of LSS and\nexemplifies how modern natural language processing tools can be used to\nevaluate competing theory-driven hypotheses in social psychology."}
{"id": "2310.13312", "pdf": "https://arxiv.org/pdf/2310.13312.pdf", "abs": "https://arxiv.org/abs/2310.13312", "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models", "authors": ["Jaeyoung Choe", "Keonwoong Noh", "Nayeon Kim", "Seyun Ahn", "Woohwan Jung"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2023 (Findings)", "summary": "Over the past few years, various domain-specific pretrained language models\n(PLMs) have been proposed and have outperformed general-domain PLMs in\nspecialized areas such as biomedical, scientific, and clinical domains. In\naddition, financial PLMs have been studied because of the high economic impact\nof financial data analysis. However, we found that financial PLMs were not\npretrained on sufficiently diverse financial data. This lack of diverse\ntraining data leads to a subpar generalization performance, resulting in\ngeneral-purpose PLMs, including BERT, often outperforming financial PLMs on\nmany downstream tasks. To address this issue, we collected a broad range of\nfinancial corpus and trained the Financial Language Model (FiLM) on these\ndiverse datasets. Our experimental results confirm that FiLM outperforms not\nonly existing financial PLMs but also general domain PLMs. Furthermore, we\nprovide empirical evidence that this improvement can be achieved even for\nunseen corpus groups."}
{"id": "2401.14624", "pdf": "https://arxiv.org/pdf/2401.14624.pdf", "abs": "https://arxiv.org/abs/2401.14624", "title": "Unearthing Large Scale Domain-Specific Knowledge from Public Corpora", "authors": ["Zhaoye Fei", "Yunfan Shao", "Linyang Li", "Zhiyuan Zeng", "Conghui He", "Qipeng Guo", "Hang Yan", "Dahua Lin", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "We have released the full data (total of 735GB) in\n  https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile and partial data\n  (about 40GB) in https://huggingface.co/datasets/Query-of-CC/knowledge_pile", "summary": "Large language models (LLMs) have demonstrated remarkable potential in\nvarious tasks, however, there remains a significant lack of open-source models\nand data for specific domains. Previous work has primarily focused on manually\nspecifying resources and collecting high-quality data for specific domains,\nwhich is extremely time-consuming and labor-intensive. To address this\nlimitation, we introduce large models into the data collection pipeline to\nguide the generation of domain-specific information and retrieve relevant data\nfrom Common Crawl (CC), a large public corpus. We refer to this approach as\nRetrieve-from-CC. It not only collects data related to domain-specific\nknowledge but also mines the data containing potential reasoning procedures\nfrom the public corpus. By applying this method, we have collected a knowledge\ndomain-related dataset named Retrieve-Pile, which covers four main domains,\nincluding the sciences, humanities, and other categories. Through the analysis\nof , Retrieve-from-CC can effectively retrieve relevant data from the covered\nknowledge domains and significantly improve the performance in tests of\nmathematical and knowledge-related reasoning abilities. We have released\nRetrieve-Pile at https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile."}
{"id": "2402.13211", "pdf": "https://arxiv.org/pdf/2402.13211.pdf", "abs": "https://arxiv.org/abs/2402.13211", "title": "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation", "authors": ["Dongjin Kang", "Sunghwan Kim", "Taeyoon Kwon", "Seungjun Moon", "Hyunsouk Cho", "Youngjae Yu", "Dongha Lee", "Jinyoung Yeo"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2024, Outstanding Paper", "summary": "Emotional Support Conversation (ESC) is a task aimed at alleviating\nindividuals' emotional distress through daily conversation. Given its inherent\ncomplexity and non-intuitive nature, ESConv dataset incorporates support\nstrategies to facilitate the generation of appropriate responses. Recently,\ndespite the remarkable conversational ability of large language models (LLMs),\nprevious studies have suggested that they often struggle with providing useful\nemotional support. Hence, this work initially analyzes the results of LLMs on\nESConv, revealing challenges in selecting the correct strategy and a notable\npreference for a specific strategy. Motivated by these, we explore the impact\nof the inherent preference in LLMs on providing emotional support, and\nconsequently, we observe that exhibiting high preference for specific\nstrategies hinders effective emotional support, aggravating its robustness in\npredicting the appropriate strategy. Moreover, we conduct a methodological\nstudy to offer insights into the necessary approaches for LLMs to serve as\nproficient emotional supporters. Our findings emphasize that (1) low preference\nfor specific strategies hinders the progress of emotional support, (2) external\nassistance helps reduce preference bias, and (3) existing LLMs alone cannot\nbecome good emotional supporters. These insights suggest promising avenues for\nfuture research to enhance the emotional intelligence of LLMs."}
{"id": "2402.13405", "pdf": "https://arxiv.org/pdf/2402.13405.pdf", "abs": "https://arxiv.org/abs/2402.13405", "title": "A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion", "authors": ["Yanzhen Shen", "Yu Zhang", "Yunyi Zhang", "Jiawei Han"], "categories": ["cs.CL"], "comment": null, "summary": "Entity set expansion, taxonomy expansion, and seed-guided taxonomy\nconstruction are three representative tasks that can be applied to\nautomatically populate an existing taxonomy with emerging concepts. Previous\nstudies view them as three separate tasks. Therefore, their proposed techniques\nusually work for one specific task only, lacking generalizability and a\nholistic perspective. In this paper, we aim at a unified solution to the three\ntasks. To be specific, we identify two common skills needed for entity set\nexpansion, taxonomy expansion, and seed-guided taxonomy construction: finding\n\"siblings\" and finding \"parents\". We propose a taxonomy-guided instruction\ntuning framework to teach a large language model to generate siblings and\nparents for query entities, where the joint pre-training process facilitates\nthe mutual enhancement of the two skills. Extensive experiments on multiple\nbenchmark datasets demonstrate the efficacy of our proposed TaxoInstruct\nframework, which outperforms task-specific baselines across all three tasks."}
{"id": "2402.13606", "pdf": "https://arxiv.org/pdf/2402.13606.pdf", "abs": "https://arxiv.org/abs/2402.13606", "title": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models", "authors": ["Boyang Xue", "Hongru Wang", "Rui Wang", "Sheng Wang", "Zezhong Wang", "Yiming Du", "Bin Liang", "Wenxuan Zhang", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": "Accepted in ACL2025 Findings", "summary": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluated high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy in LS scenarios."}
{"id": "2402.15481", "pdf": "https://arxiv.org/pdf/2402.15481.pdf", "abs": "https://arxiv.org/abs/2402.15481", "title": "Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency", "authors": ["Yiran Liu", "Ke Yang", "Zehan Qi", "Xiao Liu", "Yang Yu", "ChengXiang Zhai"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "We present a novel statistical framework for analyzing stereotypes in large\nlanguage models (LLMs) by systematically estimating the bias and variation in\ntheir generation. Current alignment evaluation metrics often overlook\nstereotypes' randomness caused by LLMs' inconsistent generative behavior. For\ninstance, LLMs may display contradictory stereotypes, such as those related to\ngender or race, for identical professions in different contexts. Ignoring this\ninconsistency risks misleading conclusions in alignment assessments and\nundermines efforts to evaluate the potential of LLMs to perpetuate or amplify\nsocial biases and unfairness.\n  To address this, we propose the Bias-Volatility Framework (BVF), which\nestimates the probability distribution of stereotypes in LLM outputs. By\ncapturing the variation in generative behavior, BVF assesses both the\nlikelihood and degree to which LLM outputs negatively impact vulnerable groups,\nenabling a quantification of aggregated discrimination risk. Additionally, we\nintroduce a mathematical framework to decompose this risk into bias risk (from\nthe mean of the stereotype distribution) and volatility risk (from its\nvariation). Applying BVF to 12 widely used LLMs, we find: i) Bias risk is the\ndominant contributor to discrimination; ii) Most LLMs exhibit substantial\npro-male stereotypes across nearly all professions; iii) Reinforcement learning\nfrom human feedback reduces bias but increases volatility; iv) Discrimination\nrisk correlates with socio-economic factors, such as professional salaries.\nFinally, we highlight BVF's broader applicability for assessing how generation\ninconsistencies in LLMs impact behavior beyond stereotypes."}
{"id": "2402.17263", "pdf": "https://arxiv.org/pdf/2402.17263.pdf", "abs": "https://arxiv.org/abs/2402.17263", "title": "MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning", "authors": ["Pengjie Ren", "Chengshun Shi", "Shiguang Wu", "Mengqi Zhang", "Zhaochun Ren", "Maarten de Rijke", "Zhumin Chen", "Jiahuan Pei"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL2024", "summary": "Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring\npre-trained large language models (LLMs), especially as the models' scale and\nthe diversity of tasks increase. Low-rank adaptation (LoRA) is based on the\nidea that the adaptation process is intrinsically low-dimensional, i.e.,\nsignificant model changes can be represented with relatively few parameters.\nHowever, decreasing the rank encounters challenges with generalization errors\nfor specific tasks when compared to full-parameter fine-tuning. We present\nMELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters\nwhile maintaining a higher rank, thereby offering improved performance\npotential. The core idea is to freeze original pretrained weights and train a\ngroup of mini LoRAs with only a small number of parameters. This can capture a\nsignificant degree of diversity among mini LoRAs, thus promoting better\ngeneralization ability. We conduct a theoretical analysis and empirical studies\non various NLP tasks. Our experimental results show that, compared to LoRA,\nMELoRA achieves better performance with 8 times fewer trainable parameters on\nnatural language understanding tasks and 36 times fewer trainable parameters on\ninstruction following tasks, which demonstrates the effectiveness of MELoRA."}
{"id": "2405.13984", "pdf": "https://arxiv.org/pdf/2405.13984.pdf", "abs": "https://arxiv.org/abs/2405.13984", "title": "Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation", "authors": ["Dimitris Gkoumas", "Maria Liakata"], "categories": ["cs.CL", "cs.MM"], "comment": "ACL25 Main", "summary": "Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore post-training\nsynergies between alignment fine-tuning and model merging in a cross-modal\nsetup. We reveal intriguing insights into the behaviour and suitability of such\nmethods while significantly surpassing state-of-the-art models. Moreover, we\npropose a novel atomic-level evaluation method leveraging off-the-shelf Natural\nLanguage Inference (NLI) models for use in the unseen chemical domain. Our\nexperiments demonstrate that our evaluation operates at the right level of\ngranularity, effectively handling multiple content units and subsentence\nreasoning, while widely adopted NLI methods consistently misalign with\nassessment criteria."}
{"id": "2405.17062", "pdf": "https://arxiv.org/pdf/2405.17062.pdf", "abs": "https://arxiv.org/abs/2405.17062", "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection, and Generation", "authors": ["Jun Gao", "Qi Lv", "Zili Wang", "Tianxiang Wu", "Ziqiang Cao", "Wenjie Li"], "categories": ["cs.CL"], "comment": "ACL2025", "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."}
{"id": "2406.05348", "pdf": "https://arxiv.org/pdf/2406.05348.pdf", "abs": "https://arxiv.org/abs/2406.05348", "title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets", "authors": ["Satanu Ghosh", "Neal R. Brodnik", "Carolina Frey", "Collin Holgate", "Tresa M. Pollock", "Samantha Daly", "Samuel Carton"], "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2; I.2.7; H.4; H.5"], "comment": "LLM for information extraction. Update on 12/11/2024: We added some\n  relevant literature that we missed in the previous version of the paper.\n  Update on 05/25/2025: We changed the metadata", "summary": "We explore the ability of GPT-4 to perform ad-hoc schema based information\nextraction from scientific literature. We assess specifically whether it can,\nwith a basic prompting approach, replicate two existing material science\ndatasets, given the manuscripts from which they were originally manually\nextracted. We employ materials scientists to perform a detailed manual error\nanalysis to assess where the model struggles to faithfully extract the desired\ninformation, and draw on their insights to suggest research directions to\naddress this broadly important task."}
{"id": "2406.11632", "pdf": "https://arxiv.org/pdf/2406.11632.pdf", "abs": "https://arxiv.org/abs/2406.11632", "title": "Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation", "authors": ["Boxuan Lyu", "Hidetaka Kamigaito", "Kotaro Funakoshi", "Manabu Okumura"], "categories": ["cs.CL", "cs.AI"], "comment": "ACl2025 Main Conference", "summary": "Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding offers an alternative by seeking\nhypotheses with the highest expected utility.\n  Inspired by Quality Estimation (QE) reranking which uses the QE model as a\nranker we propose source-based MBR (sMBR) decoding, a novel approach that\nutilizes quasi-sources (generated via paraphrasing or back-translation) as\n``support hypotheses'' and a reference-free quality estimation metric as the\nutility function, marking the first work to solely use sources in MBR decoding.\nExperiments show that sMBR outperforms QE reranking and the standard MBR\ndecoding. Our findings suggest that sMBR is a promising approach for NMT\ndecoding."}
{"id": "2406.16833", "pdf": "https://arxiv.org/pdf/2406.16833.pdf", "abs": "https://arxiv.org/abs/2406.16833", "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations", "authors": ["Mounika Marreddy", "Subba Reddy Oota", "Venkata Charan Chinni", "Manish Gupta", "Lucie Flek"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "45 pages, 22 figures, Findings of the Association for Computational\n  Linguistics, ACL-2025 (Long)", "summary": "Analyzing user opinion changes in long conversation threads is extremely\ncritical for applications like enhanced personalization, market research,\npolitical campaigns, customer service, targeted advertising, and content\nmoderation. Unfortunately, previous studies on stance and dogmatism in user\nconversations have focused on training models using datasets annotated at the\npost level, treating each post as independent and randomly sampling posts from\nconversation threads. Hence, first, we build a dataset for studying user\nopinion fluctuations in 764 long multi-user Reddit conversation threads, called\nUSDC. USDC contains annotations for 2 tasks: i) User Stance classification,\nwhich involves labeling a user's stance in a post within a conversation on a\nfive-point scale; ii) User Dogmatism classification, which involves labeling a\nuser's overall opinion in the conversation on a four-point scale. Besides being\ntime-consuming and costly, manual annotations for USDC are challenging because:\n1) Conversation threads could be very long, increasing the chances of noisy\nannotations; and 2) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Hence, we leverage majority voting on zero-shot,\none-shot, and few-shot annotations from Mistral Large and GPT-4 to automate the\nannotation process. Human annotations on 200 test conversations achieved\ninter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism with\nthese LLM annotations, indicating a reasonable level of consistency between\nhuman and LLM annotations. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models like LLaMA, Falcon and Vicuna for the\nstance and dogmatism classification tasks. We make the code and dataset\npublicly available [https://github.com/mounikamarreddy/USDC]."}
{"id": "2406.19465", "pdf": "https://arxiv.org/pdf/2406.19465.pdf", "abs": "https://arxiv.org/abs/2406.19465", "title": "Can Large Language Models Generate High-quality Patent Claims?", "authors": ["Lekang Jiang", "Caiqi Zhang", "Pascal A Scherz", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025. 16 pages, 2 figures, 12 tables", "summary": "Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness."}
{"id": "2407.19299", "pdf": "https://arxiv.org/pdf/2407.19299.pdf", "abs": "https://arxiv.org/abs/2407.19299", "title": "The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations", "authors": ["Thanh-Dung Le", "Ti Ti Nguyen", "Vu Nguyen Ha", "Symeon Chatzinotas", "Philippe Jouvet", "Rita Noumeir"], "categories": ["cs.CL", "eess.SP"], "comment": "Under revisions", "summary": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to the domain gap and limited\ndata availability. This study investigates the effectiveness of various adapter\ntechniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a\nresource-constrained hospital environment. We experimented with four\nstructures-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN)-as final layers for clinical notes classification. We fine-tuned\nbiomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT,\nalongside two Transformer-based models. Our extensive experimental results\nindicate that i) employing adapter structures does not yield significant\nimprovements in fine-tuning biomedical pre-trained LLMs, and ii) simpler\nTransformer-based models, trained from scratch, perform better under resource\nconstraints. Among the adapter structures, GRN demonstrated superior\nperformance with accuracy, precision, recall, and an F1 score of 0.88.\nMoreover, the total training time for LLMs exceeded 1000 hours, compared to\nunder 6 hours for simpler transformer-based models, highlighting that LLMs are\nmore suitable for environments with extensive computational resources and\nlarger datasets. Consequently, this study demonstrates that simpler\nTransformer-based models can be effectively trained from scratch, providing a\nviable solution for clinical NLP tasks in low-resource environments with\nlimited data availability. By identifying the GRN as the most effective adapter\nstructure, we offer a practical approach to enhance clinical note\nclassification without requiring extensive computational resources."}
{"id": "2408.08696", "pdf": "https://arxiv.org/pdf/2408.08696.pdf", "abs": "https://arxiv.org/abs/2408.08696", "title": "Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling", "authors": ["Xianzhen Luo", "Yixuan Wang", "Qingfu Zhu", "Zhiming Zhang", "Xuanyu Zhang", "Qing Yang", "Dongliang Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL2025. Code is\n  [here](https://github.com/Luowaterbi/TokenRecycling). Token Recycling has\n  already merged into [SpecBench](https://github.com/hemingkx/Spec-Bench)", "summary": "Massive parameters of LLMs have made inference latency a fundamental\nbottleneck. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm. Some methods rely on additional\narchitectures to guess draft tokens, which need extra training before use.\nAlternatively, retrieval-based training-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nIt stores candidate tokens in an adjacency matrix and employs a\nbreadth-first-search (BFS)-like algorithm to construct a draft tree, which is\nthen validated through tree attention. New candidate tokens from the decoding\nprocess are then used to update the matrix. Token Recycling requires\n\\textless2MB of additional storage and achieves approximately 2x speedup across\nall sizes of LLMs. It significantly outperforms existing train-free methods by\n30\\% and even a widely recognized training method by 25\\%."}
{"id": "2408.09070", "pdf": "https://arxiv.org/pdf/2408.09070.pdf", "abs": "https://arxiv.org/abs/2408.09070", "title": "CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts", "authors": ["Qingkai Zeng", "Yuyang Bai", "Zhaoxuan Tan", "Zhenyu Wu", "Shangbin Feng", "Meng Jiang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by ACL2025 Findings", "summary": "Taxonomies play a crucial role in various applications by providing a\nstructural representation of knowledge. The task of taxonomy expansion involves\nintegrating emerging concepts into existing taxonomies by identifying\nappropriate parent concepts for these new query concepts. Previous approaches\ntypically relied on self-supervised methods that generate annotation data from\nexisting taxonomies. However, these methods are less effective when the\nexisting taxonomy is small (fewer than 100 entities). In this work, we\nintroduce CodeTaxo, a novel approach that leverages large language models\nthrough code language prompts to capture the taxonomic structure. Extensive\nexperiments on five real-world benchmarks from different domains demonstrate\nthat CodeTaxo consistently achieves superior performance across all evaluation\nmetrics, significantly outperforming previous state-of-the-art methods. The\ncode and data are available at https://github.com/QingkaiZeng/CodeTaxo-Pub."}
{"id": "2409.01345", "pdf": "https://arxiv.org/pdf/2409.01345.pdf", "abs": "https://arxiv.org/abs/2409.01345", "title": "Language Models Benefit from Preparation with Elicited Knowledge", "authors": ["Jiacan Yu", "Hannah An", "Lenhart K. Schubert"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The zero-shot chain of thought (CoT) approach is often used in question\nanswering (QA) by language models (LMs) for tasks that require multiple\nreasoning steps. However, some QA tasks hinge more on accessing relevant\nknowledge than on chaining reasoning steps. We introduce a simple prompting\ntechnique, called PREP, that involves using two instances of LMs: the first\n(LM1) generates relevant information, and the second (LM2) receives the\ninformation from the user and answers the question. This design is intended to\nmake better use of the LM's instruction-following capability. PREP is\napplicable across various QA tasks without domain-specific prompt engineering.\nPREP is developed on a dataset of 100 QA questions, derived from an extensive\nschematic dataset specifying artifact parts and material composition. These\nquestions ask which of two artifacts is less likely to share materials with\nanother artifact. Such questions probe the LM's knowledge of shared materials\nin the part structure of different artifacts. We test our method on our\nparts-and-materials dataset and three published commonsense reasoning datasets.\nThe average accuracy of our method is consistently higher than that of all the\nother tested methods across all the tested datasets."}
{"id": "2409.08103", "pdf": "https://arxiv.org/pdf/2409.08103.pdf", "abs": "https://arxiv.org/abs/2409.08103", "title": "The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language", "authors": ["Michael Ong", "Sean Robertson", "Leo Peckham", "Alba Jorquera Jimenez de Aberasturi", "Paula Arkhangorodsky", "Robin Huo", "Aman Sakhardande", "Mark Hallap", "Naomi Nagy", "Ewan Dunbar"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "To appear in INTERSPEECH 2025", "summary": "We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark\ncorpus designed to push the limits of current approaches to low-resource speech\nrecognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy,\nhas no standard orthography, has virtually no existing textual or speech\nresources other than what is included in the benchmark, and is quite different\nfrom other forms of Franco-Proven\\c{c}al. The corpus comes from field\nrecordings, most of which are noisy, for which only 5 hrs have matching\ntranscriptions, and for which forced alignment is of variable quality. The\ncorpus contains an additional 20 hrs of unlabelled speech. We report baseline\nresults from state-of-the-art multilingual speech foundation models with a best\nphone error rate of 30.4%, using a pipeline that continues pre-training on the\nfoundation model using the unlabelled set."}
{"id": "2409.19663", "pdf": "https://arxiv.org/pdf/2409.19663.pdf", "abs": "https://arxiv.org/abs/2409.19663", "title": "Identifying Knowledge Editing Types in Large Language Models", "authors": ["Xiaopeng Li", "Shasha Li", "Shangwen Wang", "Shezheng Song", "Bin Ji", "Huijun Liu", "Jun Ma", "Jie Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Knowledge editing has emerged as an efficient technique for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technique, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, $\\textbf{K}$nowledge $\\textbf{E}$diting $\\textbf{T}$ype\n$\\textbf{I}$dentification (KETI), aimed at identifying different types of edits\nin LLMs, thereby providing timely alerts to users when encountering illicit\nedits. As part of this task, we propose KETIBench, which includes five types of\nharmful edits covering the most popular toxic types, as well as one benign\nfactual edit. We develop five classical classification models and three\nBERT-based models as baseline identifiers for both open-source and\nclosed-source LLMs. Our experimental results, across 92 trials involving four\nmodels and three knowledge editing methods, demonstrate that all eight baseline\nidentifiers achieve decent identification performance, highlighting the\nfeasibility of identifying malicious edits in LLMs. Additional analyses reveal\nthat the performance of the identifiers is independent of the reliability of\nthe knowledge editing methods and exhibits cross-domain generalization,\nenabling the identification of edits from unknown sources. All data and code\nare available in https://github.com/xpq-tech/KETI."}
{"id": "2409.20434", "pdf": "https://arxiv.org/pdf/2409.20434.pdf", "abs": "https://arxiv.org/abs/2409.20434", "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering System", "authors": ["Zhengren Wang", "Qinhan Yu", "Shida Wei", "Zhiyu Li", "Feiyu Xiong", "Xiaoxing Wang", "Simin Niu", "Hao Liang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Modern QA systems entail retrieval-augmented generation (RAG) for accurate\nand trustworthy responses. However, the inherent gap between user queries and\nrelevant documents hinders precise matching. We introduce QAEncoder, a\ntraining-free approach to bridge this gap. Specifically, QAEncoder estimates\nthe expectation of potential queries in the embedding space as a robust\nsurrogate for the document embedding, and attaches document fingerprints to\neffectively distinguish these embeddings. Extensive experiments across diverse\ndatasets, languages, and embedding models confirmed QAEncoder's alignment\ncapability, which offers a simple-yet-effective solution with zero additional\nindex storage, retrieval latency, training costs, or catastrophic forgetting\nand hallucination issues. The repository is publicly available at\nhttps://github.com/IAAR-Shanghai/QAEncoder."}
{"id": "2410.00193", "pdf": "https://arxiv.org/pdf/2410.00193.pdf", "abs": "https://arxiv.org/abs/2410.00193", "title": "Do Vision-Language Models Really Understand Visual Language?", "authors": ["Yifan Hou", "Buse Giledereli", "Yilei Tu", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.CV"], "comment": "ICML 2025", "summary": "Visual language is a system of communication that conveys information through\nsymbols, shapes, and spatial arrangements. Diagrams are a typical example of a\nvisual language depicting complex concepts and their relationships in the form\nof an image. The symbolic nature of diagrams presents significant challenges\nfor building models capable of understanding them. Recent studies suggest that\nLarge Vision-Language Models (LVLMs) can even tackle complex reasoning tasks\ninvolving diagrams. In this paper, we investigate this phenomenon by developing\na comprehensive test suite to evaluate the diagram comprehension capability of\nLVLMs. Our test suite uses a variety of questions focused on concept entities\nand their relationships over a set of synthetic as well as real diagrams across\ndomains to evaluate the recognition and reasoning abilities of models. Our\nevaluation of LVLMs shows that while they can accurately identify and reason\nabout entities, their ability to understand relationships is notably limited.\nFurther testing reveals that the decent performance on diagram understanding\nlargely stems from leveraging their background knowledge as shortcuts to\nidentify and reason about the relational information. Thus, we conclude that\nLVLMs have a limited capability for genuine diagram understanding, and their\nimpressive performance in diagram reasoning is an illusion emanating from other\nconfounding factors, such as the background knowledge in the models."}
{"id": "2410.03124", "pdf": "https://arxiv.org/pdf/2410.03124.pdf", "abs": "https://arxiv.org/abs/2410.03124", "title": "In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement", "authors": ["Zhen-Yu Zhang", "Jiandong Zhang", "Huaxiu Yao", "Gang Niu", "Masashi Sugiyama"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved great success across diverse\ntasks, and fine-tuning is sometimes needed to further enhance generation\nquality. Most existing methods rely on human supervision or parameter\nretraining, both of which are costly in terms of data collection and\ncomputational resources. To handle these challenges, a direct solution is to\ngenerate ``high-confidence'' data from unsupervised downstream tasks and use\nthem for in-context prompting or prompt optimization to refine the\npseudo-supervision. However, relying solely on such data may lead to\noverfitting. In this paper, we leverage the in-context learning (ICL) abilities\nof LLMs and propose a novel approach, pseudo-supervised demonstrations aligned\nprompt optimization (PAPO) algorithm, which jointly refines both the prompt and\nthe overall pseudo-supervision. The proposed learning objective ensures that\nthe optimized prompt guides the LLM to generate consistent responses for a\ngiven input when pseudo-supervised data from the downstream task are used as\ndemonstrations, enabling refinement over the entire pseudo-supervision. The\nprompt is optimized by translating gradient signals into textual critiques,\nwhich serve as feedback to iteratively refine the prompt and model responses.\nTheoretical analysis in a simplified classification setting shows that the\nrefined pseudo-supervision exhibits a geometric clustering structure, helping\nto mitigate overfitting. Experiments on question answering, natural language\ninference benchmarks, and a real-world molecule optimization task, show the\neffectiveness of the proposed algorithm."}
{"id": "2410.04407", "pdf": "https://arxiv.org/pdf/2410.04407.pdf", "abs": "https://arxiv.org/abs/2410.04407", "title": "Lens: Rethinking Multilingual Enhancement for Large Language Models", "authors": ["Weixiang Zhao", "Yulin Hu", "Jiahe Guo", "Xingyu Sui", "Tongtong Wu", "Yang Deng", "Yanyan Zhao", "Bing Qin", "Wanxiang Che", "Ting Liu"], "categories": ["cs.CL"], "comment": "23 pages, 7 figures, 7 tables", "summary": "As global demand for multilingual large language models (LLMs) grows, most\nLLMs still remain overly focused on English, leading to the limited access to\nadvanced AI for non-English speakers. Current methods to enhance multilingual\ncapabilities largely rely on data-driven post-training techniques, such as\nmultilingual instruction tuning or continual pre-training. However, these\napproaches exhibit significant limitations, including high resource cost,\nexacerbation of off-target issue and catastrophic forgetting of central\nlanguage abilities. To this end, we propose Lens, a novel approach that\nenhances multilingual capabilities by leveraging LLMs' internal language\nrepresentation spaces. Lens operates on two subspaces: the language-agnostic\nsubspace, where it aligns target languages with the central language to inherit\nstrong semantic representations, and the language-specific subspace, where it\nseparates target and central languages to preserve linguistic specificity.\nExperiments on three English-centric LLMs show that Lens significantly improves\nmultilingual performance while maintaining the model's English proficiency,\nachieving better results with less computational cost compared to existing\npost-training approaches."}
{"id": "2410.06704", "pdf": "https://arxiv.org/pdf/2410.06704.pdf", "abs": "https://arxiv.org/abs/2410.06704", "title": "PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs", "authors": ["Krishna Kanth Nakka", "Ahmed Frikha", "Ricardo Mendes", "Xue Jiang", "Xuebing Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Additional results with Pythia6.9B; Additional results with Phone\n  number PII;", "summary": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies."}
{"id": "2410.07145", "pdf": "https://arxiv.org/pdf/2410.07145.pdf", "abs": "https://arxiv.org/abs/2410.07145", "title": "Stuffed Mamba: Oversized States Lead to the Inability to Forget", "authors": ["Yingfa Chen", "Xinrong Zhang", "Shengding Hu", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 18 figures", "summary": "Recent advancements in recurrent architectures, such as Mamba and RWKV, have\nshowcased strong language capabilities. Unlike transformer-based models, these\narchitectures encode all contextual information into a fixed-size state,\nleading to great inference efficiency. However, this approach can cause\ninformation interference, where different token data conflicts, resulting in\nperformance degradation and incoherent outputs beyond a certain context length.\nTo prevent this, most RNNs incorporate mechanisms designed to \"forget\" earlier\ntokens. In this paper, we reveal that Mamba-based models struggle to\neffectively forget earlier tokens even with built-in forgetting mechanisms. We\ndemonstrate that this issue stems from training on contexts that are too short\nfor the state size, enabling the model to perform well without needing to learn\nhow to forget. Then, we show that the minimum training length required for the\nmodel to learn forgetting scales linearly with the state size, and the maximum\ncontext length for accurate retrieval of a 5-digit passkey scales exponentially\nwith the state size, indicating that the model retains some information beyond\nthe point where forgetting begins. These findings highlight a critical\nlimitation in current RNN architectures and provide valuable insights for\nimproving long-context modeling. Our work suggests that future RNN designs must\naccount for the interplay between state size, training length, and forgetting\nmechanisms to achieve robust performance in long-context tasks."}
{"id": "2410.10700", "pdf": "https://arxiv.org/pdf/2410.10700.pdf", "abs": "https://arxiv.org/abs/2410.10700", "title": "LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts", "authors": ["Qibing Ren", "Hao Li", "Dongrui Liu", "Zhanxu Xie", "Xiaoya Lu", "Yu Qiao", "Lei Sha", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference. Code is available at\n  https://github.com/AI45Lab/ActorAttack", "summary": "Safety concerns in large language models (LLMs) have gained significant\nattention due to their exposure to potentially harmful data during\npre-training. In this paper, we identify a new safety vulnerability in LLMs:\ntheir susceptibility to \\textit{natural distribution shifts} between attack\nprompts and original toxic prompts, where seemingly benign prompts,\nsemantically related to harmful content, can bypass safety mechanisms. To\nexplore this issue, we introduce a novel attack method, \\textit{ActorBreaker},\nwhich identifies actors related to toxic prompts within pre-training\ndistribution to craft multi-turn prompts that gradually lead LLMs to reveal\nunsafe content. ActorBreaker is grounded in Latour's actor-network theory,\nencompassing both human and non-human actors to capture a broader range of\nvulnerabilities. Our experimental results demonstrate that ActorBreaker\noutperforms existing attack methods in terms of diversity, effectiveness, and\nefficiency across aligned LLMs. To address this vulnerability, we propose\nexpanding safety training to cover a broader semantic space of toxic content.\nWe thus construct a multi-turn safety dataset using ActorBreaker. Fine-tuning\nmodels on our dataset shows significant improvements in robustness, though with\nsome trade-offs in utility. Code is available at\nhttps://github.com/AI45Lab/ActorAttack."}
{"id": "2410.12323", "pdf": "https://arxiv.org/pdf/2410.12323.pdf", "abs": "https://arxiv.org/abs/2410.12323", "title": "Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up", "authors": ["Jiahao Yuan", "Dehui Du", "Hao Zhang", "Zixiang Di", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) have shown remarkable performance in reasoning\ntasks but face limitations in mathematical and complex logical reasoning.\nExisting methods to improve LLMs' logical capabilities either involve traceable\nor verifiable logical sequences that generate more reliable responses by\nconstructing logical structures yet increase computational costs, or introduces\nrigid logic template rules, reducing flexibility. In this paper, we propose\nReversal of Thought (RoT), a plug-and-play and cost-effective reasoning\nframework designed to enhance the logical reasoning abilities of LLMs during\nthe warm-up phase prior to batch inference. RoT utilizes a Preference-Guided\nReverse Reasoning warm-up strategy, which integrates logical symbols for\npseudocode planning through meta-cognitive mechanisms and pairwise preference\nself-evaluation to generate task-specific prompts solely through\ndemonstrations, aligning with LLMs' cognitive preferences shaped by RLHF.\nThrough reverse reasoning, we utilize a Cognitive Preference Manager to assess\nknowledge boundaries and further expand LLMs' reasoning capabilities by\naggregating solution logic for known tasks and stylistic templates for unknown\ntasks. Experiments across various tasks demonstrate that RoT surpasses existing\nbaselines in both reasoning accuracy and efficiency."}
{"id": "2410.12428", "pdf": "https://arxiv.org/pdf/2410.12428.pdf", "abs": "https://arxiv.org/abs/2410.12428", "title": "Conformity in Large Language Models", "authors": ["Xiaochen Zhu", "Caiqi Zhang", "Tom Stafford", "Nigel Collier", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages (main body), 9 figures (main body), ACL 2025 Main", "summary": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin popular LLMs. Our findings reveal that all tested models exhibit varying\nlevels of conformity toward the majority, regardless of their initial choice or\ncorrectness, across different knowledge domains. Notably, we are the first to\nshow that LLMs are more likely to conform when they are more uncertain in their\nown prediction. We further explore factors that influence conformity, such as\ntraining paradigms and input characteristics, finding that instruction-tuned\nmodels are less susceptible to conformity, while increasing the naturalness of\nmajority tones amplifies conformity. Finally, we propose two interventions,\nDevil's Advocate and Question Distillation, to mitigate conformity, providing\ninsights into building more robust language models."}
{"id": "2410.13553", "pdf": "https://arxiv.org/pdf/2410.13553.pdf", "abs": "https://arxiv.org/abs/2410.13553", "title": "SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms", "authors": ["Yuki Hou", "Haruki Tamoto", "Qinghua Zhao", "Homei Miyashita"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Existing retrieval methods in Large Language Models show degradation in\naccuracy when handling temporally distributed conversations, primarily due to\ntheir reliance on simple similarity-based retrieval. Unlike existing memory\nretrieval methods that rely solely on semantic similarity, we propose\nSynapticRAG, which uniquely combines temporal association triggers with\nbiologically-inspired synaptic propagation mechanisms. Our approach uses\ntemporal association triggers and synaptic-like stimulus propagation to\nidentify relevant dialogue histories. A dynamic leaky integrate-and-fire\nmechanism then selects the most contextually appropriate memories. Experiments\non four datasets of English, Chinese and Japanese show that compared to\nstate-of-the-art memory retrieval methods, SynapticRAG achieves consistent\nimprovements across multiple metrics up to 14.66% points. This work bridges the\ngap between cognitive science and language model development, providing a new\nframework for memory management in conversational systems."}
{"id": "2410.22394", "pdf": "https://arxiv.org/pdf/2410.22394.pdf", "abs": "https://arxiv.org/abs/2410.22394", "title": "AAAR-1.0: Assessing AI's Potential to Assist Research", "authors": ["Renze Lou", "Hanzi Xu", "Sijia Wang", "Jiangshu Du", "Ryo Kamoi", "Xiaoxin Lu", "Jian Xie", "Yuxuan Sun", "Yusen Zhang", "Jihyun Janice Ahn", "Hongchao Fang", "Zhuoyang Zou", "Wenchao Ma", "Xi Li", "Kai Zhang", "Congying Xia", "Lifu Huang", "Wenpeng Yin"], "categories": ["cs.CL"], "comment": "ICML 2025. Project Webpage: https://renzelou.github.io/AAAR-1.0/", "summary": "Numerous studies have assessed the proficiency of AI systems, particularly\nlarge language models (LLMs), in facilitating everyday tasks such as email\nwriting, question answering, and creative content generation. However,\nresearchers face unique challenges and opportunities in leveraging LLMs for\ntheir own work, such as brainstorming research ideas, designing experiments,\nand writing or reviewing papers. In this study, we introduce AAAR-1.0, a\nbenchmark dataset designed to evaluate LLM performance in three fundamental,\nexpertise-intensive research tasks: (i) EquationInference, assessing the\ncorrectness of equations based on the contextual information in paper\nsubmissions; (ii) ExperimentDesign, designing experiments to validate research\nideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper\nsubmissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews\nis deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:\nfirst, it is explicitly research-oriented, with tasks requiring deep domain\nexpertise; second, it is researcher-oriented, mirroring the primary activities\nthat researchers engage in on a daily basis. An evaluation of both open-source\nand proprietary LLMs reveals their potential as well as limitations in\nconducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new\nversions."}
{"id": "2411.00204", "pdf": "https://arxiv.org/pdf/2411.00204.pdf", "abs": "https://arxiv.org/abs/2411.00204", "title": "RESTOR: Knowledge Recovery in Machine Unlearning", "authors": ["Keivan Rezaei", "Khyathi Chandu", "Soheil Feizi", "Yejin Choi", "Faeze Brahman", "Abhilasha Ravichander"], "categories": ["cs.CL"], "comment": "Accepted to TMLR 2025", "summary": "Large language models trained on web-scale corpora can memorize undesirable\ndata containing misinformation, copyrighted material, or private or sensitive\ninformation. Recently, several machine unlearning algorithms have been proposed\nto eliminate the effect of such datapoints from trained models -- that is, to\napproximate a model that had never been trained on these datapoints in the\nfirst place. However, evaluating the effectiveness of unlearning algorithms\nremains an open challenge. Previous work has relied on heuristics -- such as\nverifying that the model can no longer reproduce the specific information\ntargeted for removal while maintaining accuracy on unrelated test data. These\napproaches inadequately capture the complete effect of reversing the influence\nof datapoints on a trained model. In this work, we propose the RESTOR framework\nfor machine unlearning evaluation, which assesses the ability of unlearning\nalgorithms for targeted data erasure, by evaluating the ability of models to\nforget the knowledge introduced in these datapoints, while simultaneously\nrecovering the model's knowledge state had it never encountered these\ndatapoints. RESTOR helps uncover several novel insights about popular\nunlearning algorithms, and the mechanisms through which they operate -- for\ninstance, identifying that some algorithms merely emphasize forgetting but not\nrecovering knowledge, and that localizing unlearning targets can enhance\nunlearning performance."}
{"id": "2411.02083", "pdf": "https://arxiv.org/pdf/2411.02083.pdf", "abs": "https://arxiv.org/abs/2411.02083", "title": "Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models", "authors": ["Jonas Zausinger", "Lars Pennig", "Anamarija Kozina", "Sean Sdahl", "Julian Sikora", "Adrian Dendorfer", "Timofey Kuznetsov", "Mohamad Hagog", "Nina Wiedemann", "Kacper Chlodny", "Vincent Limbach", "Anna Ketteler", "Thorben Prein", "Vishwa Mohan Singh", "Michael Morris Danziger", "Jannis Born"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": "ICML 2025", "summary": "While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving quantitative reasoning, especially arithmetic. One fundamental\nlimitation is the nature of the Cross Entropy loss, which assumes a nominal\nscale and thus cannot convey proximity between generated number tokens. In\nresponse, we here present a regression-like loss that operates purely on token\nlevel. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes\neither the Lp norm or the Wasserstein distance between the numerical values of\nthe real and predicted number tokens. NTL can easily be added to any language\nmodel and extend the Cross Entropy objective during training without runtime\noverhead. We evaluate the proposed scheme on various mathematical datasets and\nfind that it consistently improves performance in math-related tasks. In a\ndirect comparison on a regression task, we find that NTL can match the\nperformance of a regression head, despite operating on token level. Finally, we\nscale NTL up to 3B parameter models and observe improved performance,\ndemonstrating its potential for seamless integration into LLMs. We hope that\nthis work can inspire LLM developers to improve their pretraining objectives.\nThe code is available via: https://tum-ai.github.io/number-token-loss/"}
{"id": "2411.02391", "pdf": "https://arxiv.org/pdf/2411.02391.pdf", "abs": "https://arxiv.org/abs/2411.02391", "title": "Attacking Vision-Language Computer Agents via Pop-ups", "authors": ["Yanzhe Zhang", "Tao Yu", "Diyi Yang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Autonomous agents powered by large vision and language models (VLM) have\ndemonstrated significant potential in completing daily computer tasks, such as\nbrowsing the web to book travel and operating desktop software, which requires\nagents to understand these interfaces. Despite such visual inputs becoming more\nintegrated into agentic applications, what types of risks and attacks exist\naround them still remain unclear. In this work, we demonstrate that VLM agents\ncan be easily attacked by a set of carefully designed adversarial pop-ups,\nwhich human users would typically recognize and ignore. This distraction leads\nagents to click these pop-ups instead of performing their tasks as usual.\nIntegrating these pop-ups into existing agent testing environments like OSWorld\nand VisualWebArena leads to an attack success rate (the frequency of the agent\nclicking the pop-ups) of 86% on average and decreases the task success rate by\n47%. Basic defense techniques, such as asking the agent to ignore pop-ups or\nincluding an advertisement notice, are ineffective against the attack."}
{"id": "2411.02937", "pdf": "https://arxiv.org/pdf/2411.02937.pdf", "abs": "https://arxiv.org/abs/2411.02937", "title": "Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent", "authors": ["Yangning Li", "Yinghui Li", "Xinyu Wang", "Yong Jiang", "Zhen Zhang", "Xinran Zheng", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval Augmented Generation (mRAG) plays an important role in\nmitigating the \"hallucination\" issue inherent in multimodal large language\nmodels (MLLMs). Although promising, existing heuristic mRAGs typically\npredefined fixed retrieval processes, which causes two issues: (1) Non-adaptive\nRetrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws\ncannot be adequately reflected by current knowledge-seeking visual question\nanswering (VQA) datasets, since the most required knowledge can be readily\nobtained with a standard two-step retrieval. To bridge the dataset gap, we\nfirst construct Dyn-VQA dataset, consisting of three types of \"dynamic\"\nquestions, which require complex knowledge retrieval strategies variable in\nquery, tool, and time: (1) Questions with rapidly changing answers. (2)\nQuestions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments\non Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient\nand precisely relevant knowledge for dynamic questions due to their rigid\nretrieval processes. Hence, we further propose the first self-adaptive planning\nagent for multimodal retrieval, OmniSearch. The underlying idea is to emulate\nthe human behavior in question solution which dynamically decomposes complex\nmultimodal questions into sub-question chains with retrieval action. Extensive\nexperiments prove the effectiveness of our OmniSearch, also provide direction\nfor advancing mRAG. The code and dataset will be open-sourced at\nhttps://github.com/Alibaba-NLP/OmniSearch."}
{"id": "2411.07237", "pdf": "https://arxiv.org/pdf/2411.07237.pdf", "abs": "https://arxiv.org/abs/2411.07237", "title": "Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries", "authors": ["Chaitanya Malaviya", "Joseph Chee Chang", "Dan Roth", "Mohit Iyyer", "Mark Yatskar", "Kyle Lo"], "categories": ["cs.CL"], "comment": "Accepted to TACL. Code & data available at\n  https://github.com/allenai/ContextEval", "summary": "Language model users often issue queries that lack specification, where the\ncontext under which a query was issued -- such as the user's identity, the\nquery's intent, and the criteria for a response to be useful -- is not\nexplicit. For instance, a good response to a subjective query like \"What book\nshould I read next?\" would depend on the user's preferences, and a good\nresponse to an open-ended query like \"How do antibiotics work against\nbacteria?\" would depend on the user's expertise. This makes evaluation of\nresponses to such queries an ill-posed task, as evaluators may make arbitrary\njudgments about the response quality. To remedy this, we present contextualized\nevaluations, a protocol that synthetically constructs context surrounding an\nunderspecified query and provides it during evaluation. We find that the\npresence of context can 1) alter conclusions drawn from evaluation, even\nflipping benchmark rankings between model pairs, 2) nudge evaluators to make\nfewer judgments based on surface-level criteria, like style, and 3) provide new\ninsights about model behavior across diverse contexts. Specifically, our\nprocedure suggests a potential bias towards WEIRD (Western, Educated,\nIndustrialized, Rich and Democratic) contexts in models' \"default\" responses\nand we find that models are not equally sensitive to following different\ncontexts, even when they are provided in prompts."}
{"id": "2411.07965", "pdf": "https://arxiv.org/pdf/2411.07965.pdf", "abs": "https://arxiv.org/abs/2411.07965", "title": "SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs", "authors": ["Chuyi Kong", "Ziyang Luo", "Hongzhan Lin", "Zhiyuan Fan", "Yaxin Fan", "Yuxi Sun", "Jing Ma"], "categories": ["cs.CL"], "comment": "28 pages, unfortunately accepted to findings with Meta 4.. Apologize\n  for the reviewers and area chair who love our work, orz", "summary": "The advanced role-playing capabilities of Large Language Models (LLMs) have\nenabled rich interactive scenarios, yet existing research in social\ninteractions neglects hallucination while struggling with poor generalizability\nand implicit character fidelity judgments. To bridge this gap, motivated by\nhuman behaviour, we introduce a generalizable and explicit paradigm for\nuncovering interactive patterns of LLMs across diverse worldviews.\nSpecifically, we first define interactive hallucination through stance\ntransfer, then construct SHARP, a benchmark built by extracting relations from\ncommonsense knowledge graphs and utilizing LLMs' inherent hallucination\nproperties to simulate multi-role interactions. Extensive experiments confirm\nour paradigm's effectiveness and stability, examine the factors that influence\nthese metrics, and challenge conventional hallucination mitigation solutions.\nMore broadly, our work reveals a fundamental limitation in popular\npost-training methods for role-playing LLMs: the tendency to obscure knowledge\nbeneath style, resulting in monotonous yet human-like behaviors - interactive\nhallucination."}
{"id": "2411.10533", "pdf": "https://arxiv.org/pdf/2411.10533.pdf", "abs": "https://arxiv.org/abs/2411.10533", "title": "On the Compatibility of Generative AI and Generative Linguistics", "authors": ["Eva Portelance", "Masoud Jasbi"], "categories": ["cs.CL", "F.1.1; I.2.7; I.2.6"], "comment": null, "summary": "In mid-20th century, the linguist Noam Chomsky established generative\nlinguistics, and made significant contributions to linguistics, computer\nscience, and cognitive science by developing the computational and\nphilosophical foundations for a theory that defined language as a formal\nsystem, instantiated in human minds or artificial machines. These developments\nin turn ushered a wave of research on symbolic Artificial Intelligence (AI).\nMore recently, a new wave of non-symbolic AI has emerged with neural Language\nModels (LMs) that exhibit impressive linguistic performance, leading many to\nquestion the older approach and wonder about the the compatibility of\ngenerative AI and generative linguistics. In this paper, we argue that\ngenerative AI is compatible with generative linguistics and reinforces its\nbasic tenets in at least three ways. First, we argue that LMs are formal\ngenerative models as intended originally in Chomsky's work on formal language\ntheory. Second, LMs can help develop a program for discovery procedures as\ndefined by Chomsky's \"Syntactic Structures\". Third, LMs can be a major asset\nfor Chomsky's minimalist approach to Universal Grammar and language\nacquisition. In turn, generative linguistics can provide the foundation for\nevaluating and improving LMs as well as other generative computational models\nof language."}
{"id": "2411.15821", "pdf": "https://arxiv.org/pdf/2411.15821.pdf", "abs": "https://arxiv.org/abs/2411.15821", "title": "Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?", "authors": ["Aryan Sajith", "Krishna Chaitanya Rao Kathala"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures, 4 tables | Conference: International Conference\n  on Neural Computing for Advanced Applications 2025, Conference info:\n  https://aaci.org.hk/ncaa2025", "summary": "This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all."}
{"id": "2411.17679", "pdf": "https://arxiv.org/pdf/2411.17679.pdf", "abs": "https://arxiv.org/abs/2411.17679", "title": "Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning", "authors": ["Zhu Xu", "Zhiqiang Zhao", "Zihan Zhang", "Yuchi Liu", "Quanwei Shen", "Fei Liu", "Yu Kuang", "Jian He", "Conglin Liu"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness."}
{"id": "2411.18337", "pdf": "https://arxiv.org/pdf/2411.18337.pdf", "abs": "https://arxiv.org/abs/2411.18337", "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation", "authors": ["T. G. D. K. Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "categories": ["cs.CL"], "comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security", "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."}
{"id": "2412.02549", "pdf": "https://arxiv.org/pdf/2412.02549.pdf", "abs": "https://arxiv.org/abs/2412.02549", "title": "Patent-CR: A Dataset for Patent Claim Revision", "authors": ["Lekang Jiang", "Pascal A Scherz", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025. 15 pages, 6 tables, 3 figures", "summary": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision."}
{"id": "2412.02605", "pdf": "https://arxiv.org/pdf/2412.02605.pdf", "abs": "https://arxiv.org/abs/2412.02605", "title": "Interpretable Company Similarity with Sparse Autoencoders", "authors": ["Marco Molinari", "Victor Shao", "Luca Imeneo", "Mateusz Mikolajczak", "Vladimir Tregubiak", "Abhimanyu Pandey", "Sebastian Kuznetsov Ryder Torres Pereira"], "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Determining company similarity is a vital task in finance, underpinning risk\nmanagement, hedging, and portfolio diversification. Practitioners often rely on\nsector and industry classifications such as SIC and GICS codes to gauge\nsimilarity, the former being used by the U.S. Securities and Exchange\nCommission (SEC), and the latter widely used by the investment community. Since\nthese classifications lack granularity and need regular updating, using\nclusters of embeddings of company descriptions has been proposed as a potential\nalternative, but the lack of interpretability in token embeddings poses a\nsignificant barrier to adoption in high-stakes contexts. Sparse Autoencoders\n(SAEs) have shown promise in enhancing the interpretability of Large Language\nModels (LLMs) by decomposing Large Language Model (LLM) activations into\ninterpretable features. Moreover, SAEs capture an LLM's internal representation\nof a company description, as opposed to semantic similarity alone, as is the\ncase with embeddings. We apply SAEs to company descriptions, and obtain\nmeaningful clusters of equities. We benchmark SAE features against SIC-codes,\nIndustry codes, and Embeddings. Our results demonstrate that SAE features\nsurpass sector classifications and embeddings in capturing fundamental company\ncharacteristics. This is evidenced by their superior performance in correlating\nlogged monthly returns - a proxy for similarity - and generating higher Sharpe\nratios in co-integration trading strategies, which underscores deeper\nfundamental similarities among companies. Finally, we verify the\ninterpretability of our clusters, and demonstrate that sparse features form\nsimple and interpretable explanations for our clusters."}
{"id": "2412.07282", "pdf": "https://arxiv.org/pdf/2412.07282.pdf", "abs": "https://arxiv.org/abs/2412.07282", "title": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass", "authors": ["Romain Storaï", "Seung-won Hwang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to NAACL 2025 main (long)", "summary": "This paper aims to improve the performance of large language models by\naddressing the variable computational demands in inference steps, where some\ntokens require more computational resources than others. We present HARP, a\nsimple modification to \"off-the-shelf\" Transformer forward pass. Drawing from\nhesitation and the framing effect in decision-making, HARP selectively applies\nadditional computation when the model encounters uncertainty during token\ngeneration. Our method mimics human cognitive processes by pausing at difficult\ndecision points and reframing inputs for a different perspective. Unlike other\napproaches, HARP is model-agnostic, training-free, and easy to implement. We\nevaluate our method across various downstream tasks and model sizes,\ndemonstrating performance improvements up to +5.16%. Notably, HARP achieves\nthese gains while maintaining inference times twice faster than beam search.\nSimple and yet with significant gains, HARP provides insights into the\npotential of adaptive computation for enhancing the performance of\nTransformer-based language models."}
{"id": "2412.09879", "pdf": "https://arxiv.org/pdf/2412.09879.pdf", "abs": "https://arxiv.org/abs/2412.09879", "title": "On the Limit of Language Models as Planning Formalizers", "authors": ["Cassie Huang", "Li Zhang"], "categories": ["cs.CL"], "comment": "In ACL 2025 main conference", "summary": "Large Language Models have been found to create plans that are neither\nexecutable nor verifiable in grounded environments. An emerging line of work\ndemonstrates success in using the LLM as a formalizer to generate a formal\nrepresentation of the planning domain in some language, such as Planning Domain\nDefinition Language (PDDL). This formal representation can be deterministically\nsolved to find a plan. We systematically evaluate this methodology while\nbridging some major gaps. While previous work only generates a partial PDDL\nrepresentation, given templated, and therefore unrealistic environment\ndescriptions, we generate the complete representation given descriptions of\nvarious naturalness levels. Among an array of observations critical to improve\nLLMs' formal planning abilities, we note that most large enough models can\neffectively formalize descriptions as PDDL, outperforming those directly\ngenerating plans, while being robust to lexical perturbation. As the\ndescriptions become more natural-sounding, we observe a decrease in performance\nand provide detailed error analysis."}
{"id": "2412.10105", "pdf": "https://arxiv.org/pdf/2412.10105.pdf", "abs": "https://arxiv.org/abs/2412.10105", "title": "MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset", "authors": ["Sagi Shaier", "George Arthur Baker", "Chiranthan Sridhar", "Lawrence E Hunter", "Katharina von der Wense"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "Language models (LMs) have excelled in various broad domains. However, to\nensure their safe and effective integration into real-world educational\nsettings, they must demonstrate proficiency in specific, granular areas of\nknowledge. Existing cloze-style benchmarks, commonly used to evaluate LMs'\nknowledge, have three major limitations. They: 1) do not cover the educational\ndomain; 2) typically focus on low-complexity, generic knowledge or broad\ndomains, which do not adequately assess the models' knowledge in specific\nsubjects; and 3) often rely on templates that can bias model predictions. Here,\nwe introduce MALAMUTE, a multilingual, template-free, and highly granular\nprobing dataset comprising expert-written, peer-reviewed probes from 71\nuniversity-level textbooks across three languages (English, Spanish, and\nPolish). MALAMUTE is the first education-based cloze-style dataset. It covers\neight domains, each with up to 14 subdomains, further broken down into concepts\nand concept-based prompts, totaling 33,361 university curriculum concepts and\n116,887 prompts. MALAMUTE's fine granularity, educational focus, and inclusion\nof both sentence-level and paragraph-level prompts make it an ideal tool for\nevaluating LMs' course-related knowledge. Our evaluation of masked and causal\nLMs on MALAMUTE shows that despite overall proficiency, they have significant\ngaps in knowledge when examined closely on specific subjects, hindering their\nsafe use in classrooms and underscoring the need for further development."}
{"id": "2412.10138", "pdf": "https://arxiv.org/pdf/2412.10138.pdf", "abs": "https://arxiv.org/abs/2412.10138", "title": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL", "authors": ["Yang Qin", "Chao Chen", "Zhihang Fu", "Ze Chen", "Dezhong Peng", "Peng Hu", "Jieping Ye"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance."}
{"id": "2412.10827", "pdf": "https://arxiv.org/pdf/2412.10827.pdf", "abs": "https://arxiv.org/abs/2412.10827", "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training", "authors": ["Zongqian Wu", "Baoduo Xu", "Ruochen Cui", "Mengmeng Zhan", "Xiaofeng Zhu", "Lei Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 8 figures", "summary": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency."}
{"id": "2412.11041", "pdf": "https://arxiv.org/pdf/2412.11041.pdf", "abs": "https://arxiv.org/abs/2412.11041", "title": "Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models", "authors": ["Di Wu", "Xin Lu", "Yanyan Zhao", "Bing Qin"], "categories": ["cs.CL"], "comment": "16 pages, 14 figures. Camera-ready for ACL2025 findings", "summary": "Although large language models (LLMs) achieve effective safety alignment at\nthe time of release, they still face various safety challenges. A key issue is\nthat fine-tuning often compromises the safety alignment of LLMs. To address\nthis issue, we propose a method named IRR (Identify, Remove, and Recalibrate\nfor Safety Realignment) that performs safety realignment for LLMs. The core of\nIRR is to identify and remove unsafe delta parameters from the fine-tuned\nmodels, while recalibrating the retained ones. We evaluate the effectiveness of\nIRR across various datasets, including both full fine-tuning and LoRA methods.\nOur results demonstrate that IRR significantly enhances the safety performance\nof fine-tuned models on safety benchmarks, such as harmful queries and\njailbreak attacks, while maintaining their performance on downstream tasks. The\nsource code is available at: https://anonymous.4open.science/r/IRR-BD4F."}
{"id": "2412.11333", "pdf": "https://arxiv.org/pdf/2412.11333.pdf", "abs": "https://arxiv.org/abs/2412.11333", "title": "Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models", "authors": ["Xiaochen Zhu", "Georgi Karadzhov", "Chenxi Whitehouse", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages (main body), 3 figures (main body), ACL 2025 Main", "summary": "Diffusion models have shown promise in text generation, but often struggle\nwith generating long, coherent, and contextually accurate text. Token-level\ndiffusion doesn't model word-order dependencies explicitly and operates on\nshort, fixed output windows, while passage-level diffusion struggles with\nlearning robust representations for long-form text. To address these\nchallenges, we propose Segment-Level Diffusion (SLD), a framework that enhances\ndiffusion-based text generation through text segmentation, robust\nrepresentation training with adversarial and contrastive learning, and improved\nlatent-space guidance. By segmenting long-form outputs into multiple latent\nrepresentations and decoding them with an autoregressive decoder, SLD\nsimplifies diffusion predictions and improves scalability. Experiments on four\ndatasets demonstrate that, when compared to other diffusion and autoregressive\nbaselines SLD achieves competitive or superior fluency, coherence, and\ncontextual compatibility in automatic and human evaluations."}
{"id": "2412.12632", "pdf": "https://arxiv.org/pdf/2412.12632.pdf", "abs": "https://arxiv.org/abs/2412.12632", "title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context for Multi-Hop QA", "authors": ["Zhiyuan Chang", "Mingyang Li", "Xiaojun Jia", "Junjie Wang", "Yuekai Huang", "Qing Wang", "Yihao Huang", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "Incorporating external knowledge has emerged as a promising way to mitigate\noutdated knowledge and hallucinations in LLM. However, external knowledge is\noften imperfect, encompassing substantial extraneous or even inaccurate\ncontent, which interferes with the LLM's utilization of useful knowledge in the\ncontext. This paper seeks to characterize the features of preferred external\nknowledge and perform empirical studies in imperfect contexts. Inspired by the\nchain of evidence (CoE), we characterize that the knowledge preferred by LLMs\nshould maintain both relevance to the question and mutual support among the\ntextual pieces. Accordingly, we propose a CoE discrimination approach and\nconduct a comparative analysis between CoE and Non-CoE samples across\nsignificance, deceptiveness, and robustness, revealing the LLM's preference for\nexternal knowledge that aligns with CoE features. Furthermore, we selected\nthree representative tasks (RAG-based multi-hop QA, external knowledge\npoisoning and poisoning defense), along with corresponding SOTA or prevalent\nbaselines. By integrating CoE features, the variants achieved significant\nimprovements over the original baselines."}
{"id": "2412.13328", "pdf": "https://arxiv.org/pdf/2412.13328.pdf", "abs": "https://arxiv.org/abs/2412.13328", "title": "Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models", "authors": ["Elvis Nunez", "Luca Zancato", "Benjamin Bowman", "Aditya Golatkar", "Wei Xia", "Stefano Soatto"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The \"state\" of State Space Models (SSMs) represents their memory, which fades\nexponentially over an unbounded span. By contrast, Attention-based models have\n\"eidetic\" (i.e., verbatim, or photographic) memory over a finite span (context\nsize). Hybrid architectures combine State Space layers with Attention, but\nstill cannot recall the distant past and can access only the most recent tokens\neidetically. Unlike current methods of combining SSM and Attention layers, we\nallow the state to be allocated based on relevancy rather than recency. In this\nway, for every new set of query tokens, our models can \"eidetically\" access\ntokens from beyond the Attention span of current Hybrid SSMs without requiring\nextra hardware resources. We introduce a method to expand the memory span of\nthe hybrid state by \"reserving\" a fraction of the Attention context for tokens\nretrieved from arbitrarily distant in the past, thus expanding the eidetic\nmemory span of the overall state. We call this reserved fraction of tokens the\n\"expansion span,\" and the mechanism to retrieve and aggregate it \"Span-Expanded\nAttention\" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a\nnovel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows\nefficient adaptation on long spans of tokens. We show that SE-Attn enables us\nto efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8\ntimes longer than the ones used for pre-training. We show that HyLoRA with\nSE-Attn is cheaper and more performant than alternatives like LongLoRA when\napplied to Hybrid models on natural language benchmarks with long-range\ndependencies, such as PG-19, RULER, and other common natural language\ndownstream tasks."}
{"id": "2412.13549", "pdf": "https://arxiv.org/pdf/2412.13549.pdf", "abs": "https://arxiv.org/abs/2412.13549", "title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents", "authors": ["Cheng Qian", "Peixuan Han", "Qinyu Luo", "Bingxiang He", "Xiusi Chen", "Yuji Zhang", "Hongyi Du", "Jiarui Yao", "Xiaocheng Yang", "Denghui Zhang", "Yunzhu Li", "Heng Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 15 figures, ACL 2025 Main Conference", "summary": "Language model agents excel in long-session planning and reasoning, but\nexisting benchmarks primarily focus on goal-oriented tasks with explicit\nobjectives, neglecting creative adaptation in unfamiliar environments. To\naddress this, we introduce EscapeBench, a benchmark suite of room escape game\nenvironments designed to challenge agents with creative reasoning,\nunconventional tool use, and iterative problem-solving to uncover implicit\ngoals. Our results show that current LM models, despite employing working\nmemory and Chain-of-Thought reasoning, achieve only 15% average progress\nwithout hints, highlighting their limitations in creativity. To bridge this\ngap, we propose EscapeAgent, a framework designed to enhance creative reasoning\nthrough Foresight (innovative tool use) and Reflection (identifying unsolved\ntasks). Experiments show that EscapeAgent can execute action chains over 1,000\nsteps while maintaining logical coherence. It navigates and completes games\nwith up to 40% fewer steps and hints, performs robustly across difficulty\nlevels, and achieves higher action success rates with more efficient and\ninnovative puzzle-solving strategies."}
{"id": "2412.13879", "pdf": "https://arxiv.org/pdf/2412.13879.pdf", "abs": "https://arxiv.org/abs/2412.13879", "title": "Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings", "authors": ["Yuanhe Zhang", "Zhenhong Zhou", "Wei Zhang", "Xinyue Wang", "Xiaojun Jia", "Yang Liu", "Sen Su"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "22 pages, 8 figures, 11 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks yet still are vulnerable to external threats, particularly LLM\nDenial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to\nexhaust computational resources and block services. However, existing studies\npredominantly focus on white-box attacks, leaving black-box scenarios\nunderexplored. In this paper, we introduce Auto-Generation for LLM-DoS\n(AutoDoS) attack, an automated algorithm designed for black-box LLMs. AutoDoS\nconstructs the DoS Attack Tree and expands the node coverage to achieve\neffectiveness under black-box conditions. By transferability-driven iterative\noptimization, AutoDoS could work across different models in one prompt.\nFurthermore, we reveal that embedding the Length Trojan allows AutoDoS to\nbypass existing defenses more effectively. Experimental results show that\nAutoDoS significantly amplifies service response latency by over\n250$\\times\\uparrow$, leading to severe resource consumption in terms of GPU\nutilization and memory usage. Our work provides a new perspective on LLM-DoS\nattacks and security defenses. Our code is available at\nhttps://github.com/shuita2333/AutoDoS."}
{"id": "2412.14689", "pdf": "https://arxiv.org/pdf/2412.14689.pdf", "abs": "https://arxiv.org/abs/2412.14689", "title": "How to Synthesize Text Data without Model Collapse?", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Hengli Li", "Kaiyan Zhang", "Ermo Hua", "Xingtai Lv", "Ning Ding", "Zhouhan Lin", "Zilong Zheng", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves model performance."}
{"id": "2412.14838", "pdf": "https://arxiv.org/pdf/2412.14838.pdf", "abs": "https://arxiv.org/abs/2412.14838", "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs", "authors": ["Xiabin Zhou", "Wenbin Wang", "Minyan Zeng", "Jiaxian Guo", "Xuebo Liu", "Li Shen", "Min Zhang", "Liang Ding"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."}
{"id": "2412.20251", "pdf": "https://arxiv.org/pdf/2412.20251.pdf", "abs": "https://arxiv.org/abs/2412.20251", "title": "ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty", "authors": ["Qing Zong", "Zhaowei Wang", "Tianshi Zheng", "Xiyu Ren", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 findings", "summary": "The rapid development of LLMs has sparked extensive research into their\nfactual knowledge. Current works find that LLMs fall short on questions around\nlow-frequency entities. However, such proofs are unreliable since the questions\ncan differ not only in entity frequency but also in difficulty themselves. So\nwe introduce ComparisonQA benchmark, containing 283K abstract questions, each\ninstantiated by a pair of high-frequency and low-frequency entities. It ensures\na controllable comparison to study the role of knowledge frequency in the\nperformance of LLMs. Because the difference between such a pair is only the\nentity with different frequencies. In addition, we use both correctness and\nuncertainty to develop a two-round method to evaluate LLMs' knowledge\nrobustness. It aims to avoid possible semantic shortcuts which is a serious\nproblem of current QA study. Experiments reveal that LLMs, including GPT-4o,\nexhibit particularly low robustness regarding low-frequency knowledge. Besides,\nwe find that uncertainty can be used to effectively identify high-quality and\nshortcut-free questions while maintaining the data size. Based on this, we\npropose an automatic method to select such questions to form a subset called\nComparisonQA-Hard, containing only hard low-frequency questions."}
{"id": "2501.02979", "pdf": "https://arxiv.org/pdf/2501.02979.pdf", "abs": "https://arxiv.org/abs/2501.02979", "title": "Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation", "authors": ["Zhi Qu", "Yiran Wang", "Jiannan Mao", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (main)", "summary": "The multilingual neural machine translation (MNMT) aims for arbitrary\ntranslations across multiple languages. Although MNMT-specific models trained\non parallel data offer low costs in training and deployment, their performance\nconsistently lags behind that of large language models (LLMs). In this work, we\nintroduce registering, a novel method that enables a small MNMT-specific model\nto compete with LLMs. Specifically, we insert a set of artificial tokens\nspecifying the target language, called registers, into the input sequence\nbetween the source and target tokens. By modifying the attention mask, the\ntarget token generation only pays attention to the activation of registers,\nrepresenting the source tokens in the target language space. Experiments on\nEC-40, a large-scale benchmark, show that our method advances the\nstate-of-the-art of MNMT. We further pre-train two models, namely MITRE\n(multilingual translation with registers), by 9.3 billion sentence pairs across\n24 languages collected from public corpora. One of them, MITRE-913M,\noutperforms NLLB-3.3B, achieves comparable performance with commercial LLMs,\nand shows strong adaptability in fine-tuning. Finally, we open-source our\nmodels to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre."}
{"id": "2501.04561", "pdf": "https://arxiv.org/pdf/2501.04561.pdf", "abs": "https://arxiv.org/abs/2501.04561", "title": "OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis", "authors": ["Run Luo", "Ting-En Lin", "Haonan Zhang", "Yuchuan Wu", "Xiong Liu", "Min Yang", "Yongbin Li", "Longze Chen", "Jiaming Li", "Lei Zhang", "Yangyi Chen", "Xiaobo Xia", "Hamid Alinejad-Rokny", "Fei Huang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in omnimodal learning have significantly improved\nunderstanding and generation across images, text, and speech, yet these\ndevelopments remain predominantly confined to proprietary models. The lack of\nhigh-quality omnimodal datasets and the challenges of real-time emotional\nspeech synthesis have notably hindered progress in open-source research. To\naddress these limitations, we introduce \\name, a two-stage training framework\nthat integrates omnimodal alignment and speech generation to develop a\nstate-of-the-art omnimodal large language model. In the alignment phase, a\npre-trained speech model undergoes further training on text-image tasks,\nenabling (near) zero-shot generalization from vision to speech, outperforming\nmodels trained on tri-modal datasets. In the speech generation phase, a\nlightweight decoder is trained on speech tasks with direct preference\noptimization, enabling real-time emotional speech synthesis with high fidelity.\nExperiments show that \\name surpasses state-of-the-art models across omnimodal,\nvision-language, and speech-language benchmarks. It achieves a 4-point absolute\nimprovement on OmniBench over the leading open-source model VITA, despite using\n5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally,\n\\name achieves real-time speech generation with <1s latency at\nnon-autoregressive mode, reducing inference time by 5x compared to\nautoregressive methods, and improves emotion classification accuracy by 7.7\\%"}
{"id": "2501.06246", "pdf": "https://arxiv.org/pdf/2501.06246.pdf", "abs": "https://arxiv.org/abs/2501.06246", "title": "A partition cover approach to tokenization", "authors": ["Jia Peng Lim", "Shawn Tan", "Davin Choo", "Hady W. Lauw"], "categories": ["cs.CL", "cs.AI", "cs.DS"], "comment": "under review", "summary": "Tokenization is the process of encoding strings into tokens of a fixed\nvocabulary size, and is widely utilized in Natural Language Processing\napplications. The leading tokenization algorithm today is Byte-Pair Encoding\n(BPE), which formulates the tokenization problem as a compression problem and\ntackles it by performing sequences of merges. In this work, we formulate\ntokenization as an optimization objective, show that it is NP-hard via a simple\nreduction from vertex cover, and propose a polynomial-time greedy algorithm\nGreedTok. Our formulation naturally relaxes to the well-studied weighted\nmaximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm\nGreedWMC. Through empirical evaluations on real-world corpora, we show that\nGreedTok outperforms BPE and Unigram on compression and achieves a covering\nscore comparable to GreedWMC. Finally, our extensive pre-training for two\ntransformer-based language models with 1 billion parameters, comparing the\nchoices of BPE and GreedTok as the tokenizer, shows that GreedTok achieves a\nlower bit per byte even when we control for either the total dataset proportion\nor total training tokens."}
{"id": "2501.06892", "pdf": "https://arxiv.org/pdf/2501.06892.pdf", "abs": "https://arxiv.org/abs/2501.06892", "title": "Language Fusion for Parameter-Efficient Cross-lingual Transfer", "authors": ["Philipp Borchert", "Ivan Vulić", "Marie-Francine Moens", "Jochen De Weerdt"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Limited availability of multilingual text corpora for training language\nmodels often leads to poor performance on downstream tasks due to undertrained\nrepresentation spaces for languages other than English. This\n'under-representation' has motivated recent cross-lingual transfer methods to\nleverage the English representation space by e.g. mixing English and\n'non-English' tokens at the input level or extending model parameters to\naccommodate new languages. However, these approaches often come at the cost of\nincreased computational complexity. We propose Fusion forLanguage\nRepresentations (FLARE) in adapters, a novel method that enhances\nrepresentation quality and downstream performance for languages other than\nEnglish while maintaining parameter efficiency. FLARE integrates source and\ntarget language representations within low-rank (LoRA) adapters using\nlightweight linear transformations, maintaining parameter efficiency while\nimproving transfer performance. A series of experiments across representative\ncross-lingual natural language understanding tasks, including natural language\ninference, question-answering and sentiment analysis, demonstrate FLARE's\neffectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1\nand 2.2% for Gemma~2 compared to standard LoRA fine-tuning on\nquestion-answering tasks, as measured by the exact match metric."}
{"id": "2501.09706", "pdf": "https://arxiv.org/pdf/2501.09706.pdf", "abs": "https://arxiv.org/abs/2501.09706", "title": "Domain Adaptation of Foundation LLMs for e-Commerce", "authors": ["Christian Herold", "Michael Kozielski", "Tala Bazazo", "Pavel Petrushkov", "Patrycja Cieplicka", "Dominika Basaj", "Yannick Versley", "Seyyed Hadi Hashemi", "Shahram Khadivi"], "categories": ["cs.CL"], "comment": "Accepted at ACL25 (Industry )", "summary": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains."}
{"id": "2501.09766", "pdf": "https://arxiv.org/pdf/2501.09766.pdf", "abs": "https://arxiv.org/abs/2501.09766", "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use", "authors": ["Yirong Zeng", "Xiao Ding", "Yuxian Wang", "Weiwen Liu", "Wu Ning", "Yutai Hou", "Xu Huang", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "Augmenting large language models (LLMs) with external tools is a promising\napproach to enhance their capabilities, especially for complex tasks.\nSynthesizing tool-use data through real-world simulations is an effective way\nto achieve this. However, our investigation reveals that training gains\nsignificantly decay as synthetic data increases. The model struggles to benefit\nfrom more synthetic data, and it can not equip the model with advanced tool-use\ncapabilities in complex scenarios. Moreover, we discovered that the above\nlimitation usually manifests as a fragment deficiency (i.e., parameter errors)\nin response. To this end, we propose an iterative reinforced fine-tuning\nstrategy designed to alleviate this limitation. This strategy involves: (1)\nenhancing the diversity of response for synthetic data through path exploration\nof Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency\nby constructing fine-grained preference pairs, and then improving it by\npreference optimization algorithms for targeted improvement. The experiments\nshow that our method achieves 13.11% better performance than the same-size base\nmodel. It achieves an improvement of 6.5% in complex scenarios compared to the\nbaseline, and it also outperforms larger open-source and closed-source models."}
{"id": "2501.11478", "pdf": "https://arxiv.org/pdf/2501.11478.pdf", "abs": "https://arxiv.org/abs/2501.11478", "title": "Each Graph is a New Language: Graph Learning with LLMs", "authors": ["Huachi Zhou", "Jiahe Du", "Chuang Zhou", "Chang Yang", "Yilin Xiao", "Yuxuan Xie", "Xiao Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent efforts leverage Large Language Models (LLMs) for modeling\ntext-attributed graph structures in node classification tasks. These approaches\ndescribe graph structures for LLMs to understand or aggregate LLM-generated\ntextual attribute embeddings through graph structure. However, these approaches\nface two main limitations in modeling graph structures with LLMs. (i) Graph\ndescriptions become verbose in describing high-order graph structure. (ii)\nTextual attributes alone do not contain adequate graph structure information.\nIt is challenging to model graph structure concisely and adequately with LLMs.\nLLMs lack built-in mechanisms to model graph structures directly. They also\nstruggle with complex long-range dependencies between high-order nodes and\ntarget nodes.\n  Inspired by the observation that LLMs pre-trained on one language can achieve\nexceptional performance on another with minimal additional training, we propose\n\\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge\n\\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs\nto transfer their powerful language understanding capabilities to\ngraph-structured data. GDL4LLM translates graphs into a graph language corpus\ninstead of graph descriptions and pre-trains LLMs on this corpus to adequately\nunderstand graph structures. During fine-tuning, this corpus describes the\nstructural information of target nodes concisely with only a few tokens. By\ntreating graphs as a new language, GDL4LLM enables LLMs to model graph\nstructures adequately and concisely for node classification tasks. Extensive\nexperiments on three real-world datasets demonstrate that GDL4LLM outperforms\ndescription-based and textual attribute embeddings-based baselines by\nefficiently modeling different orders of graph structure with LLMs."}
{"id": "2501.12766", "pdf": "https://arxiv.org/pdf/2501.12766.pdf", "abs": "https://arxiv.org/abs/2501.12766", "title": "NExtLong: Toward Effective Long-Context Training without Long Documents", "authors": ["Chaochen Gao", "Xing Wu", "Zijia Lin", "Debing Zhang", "Songlin Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025. Corresponding authors: xing wu, and songlin hu", "summary": "Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs."}
{"id": "2501.13836", "pdf": "https://arxiv.org/pdf/2501.13836.pdf", "abs": "https://arxiv.org/abs/2501.13836", "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Most social media users come from non-English speaking countries in the\nGlobal South, where much of harmful content appears in local languages. Yet,\ncurrent AI-driven moderation systems struggle with low-resource languages\nspoken in these regions. This work examines the systemic challenges in building\nautomated moderation tools for these languages. We conducted semi-structured\ninterviews with 22 AI experts working on detecting harmful content in four\nlow-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi\nArabic (North Africa), and Quechua (South America). Our findings show that\nbeyond the well-known data scarcity in local languages, technical issues--such\nas outdated machine translation systems, sentiment and toxicity models grounded\nin Western values, and unreliable language detection technologies--undermine\nmoderation efforts. Even with more data, current language models and\npreprocessing pipelines--primarily designed for English--struggle with the\nmorphological richness, linguistic complexity, and code-mixing. As a result,\nautomated moderation in Tamil, Swahili, Arabic, and Quechua remains fraught\nwith inaccuracies and blind spots. Based on our findings, we argue that these\nlimitations are not just technical gaps but reflect deeper structural\ninequities that continue to reproduce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve automated moderation for\nlow-resource languages."}
{"id": "2501.19337", "pdf": "https://arxiv.org/pdf/2501.19337.pdf", "abs": "https://arxiv.org/abs/2501.19337", "title": "Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models", "authors": ["Messi H. J. Lee", "Soyeon Jeon"], "categories": ["cs.CL", "cs.CV"], "comment": "11 pages, 5 figures", "summary": "Homogeneity bias is one form of stereotyping in AI models where certain\ngroups are represented as more similar to each other than other groups. This\nbias is a major obstacle to creating equitable language technologies. We test\nwhether the bias is driven by systematic differences in token-sampling\nuncertainty across six large language models. While we observe the presence of\nhomogeneity bias using sentence similarity, we find very little difference in\ntoken sampling uncertainty across groups. This finding elucidates why\ntemperature-based sampling adjustments fail to mitigate homogeneity bias. It\nsuggests researchers should prioritize interventions targeting representation\nlearning mechanisms and training corpus composition rather than inference-time\noutput manipulations."}
{"id": "2502.00136", "pdf": "https://arxiv.org/pdf/2502.00136.pdf", "abs": "https://arxiv.org/abs/2502.00136", "title": "A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment", "authors": ["Edward Y. Chang"], "categories": ["cs.CL", "cs.AI", "F.2.2"], "comment": "20 pages, 7 tables, 6 figures. arXiv admin note: substantial text\n  overlap with arXiv:2405.07076", "summary": "This paper introduces a checks-and-balances framework for ethical alignment\nof Large Language Models (LLMs), inspired by three-branch governmental systems.\nIt implements three independent yet interacting components: LLMs as the\nexecutive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. Beyond structural separation, we address a fundamental\nchallenge: regulating emotion to shape behaviors. Drawing from psychological\ntheories where managing emotional responses prevents harmful behaviors, we\ndevelop a self-supervised learning pipeline that maps emotions to linguistic\nbehaviors, enabling precise behavioral modulation through emotional\nconditioning. By integrating this approach with adversarial testing, our\nframework demonstrates how DIKE and ERIS direct linguistic behaviors toward\nethical outcomes while preserving independence throughout knowledge generation,\nethical oversight, and contextual interpretation."}
{"id": "2502.00334", "pdf": "https://arxiv.org/pdf/2502.00334.pdf", "abs": "https://arxiv.org/abs/2502.00334", "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models", "authors": ["Xin Xu", "Qiyun Xu", "Tong Xiao", "Tianhao Chen", "Yuchen Yan", "Jiaxin Zhang", "Shizhe Diao", "Can Yang", "Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics ."}
{"id": "2502.00507", "pdf": "https://arxiv.org/pdf/2502.00507.pdf", "abs": "https://arxiv.org/abs/2502.00507", "title": "A statistically consistent measure of semantic uncertainty using Language Models", "authors": ["Yi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To address the challenge of quantifying uncertainty in the outputs generated\nby language models, we propose a novel measure of semantic uncertainty,\nsemantic spectral entropy, that is statistically consistent under mild\nassumptions. This measure is implemented through a straightforward algorithm\nthat relies solely on standard, pretrained language models, without requiring\naccess to the internal generation process. Our approach imposes minimal\nconstraints on the choice of language models, making it broadly applicable\nacross different architectures and settings. Through comprehensive simulation\nstudies, we demonstrate that the proposed method yields an accurate and robust\nestimate of semantic uncertainty, even in the presence of the inherent\nrandomness characteristic of generative language model outputs."}
{"id": "2502.04066", "pdf": "https://arxiv.org/pdf/2502.04066.pdf", "abs": "https://arxiv.org/abs/2502.04066", "title": "SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals", "authors": ["Changhao Jiang", "Ming Zhang", "Junjie Ye", "Xiaoran Fan", "Yifei Cao", "Jiajun Sun", "Zhiheng Xi", "Shihan Dou", "Yi Dong", "Yujiong Shen", "Jingqi Tong", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The GPT-4 technical report highlights the possibility of predicting model\nperformance on downstream tasks using only pre-training signals, though\ndetailed methodologies are absent. Such predictive capabilities are essential\nfor resource-efficient pre-training and the construction of task-aligned\ndatasets. In this paper, we aim to predict performance in closed-book question\nanswering (QA), a vital downstream task indicative of a model's internal\nknowledge. We address three primary challenges: (1) limited access to and\nunderstanding of pre-training corpora, (2) limitations of current evaluation\nmethods for pre-trained models, and (3) limitations of frequency-based metrics\nin predicting model performance. In response to these challenges, we conduct\nlarge-scale retrieval and semantic analysis across the pre-training corpora of\n21 publicly available and 3 custom-trained large language models. Subsequently,\nwe develop a multi-template QA evaluation framework incorporating paraphrased\nquestion variants. Building on these foundations, we propose Size-dependent\nMutual Information (SMI), an information-theoretic metric that linearly\ncorrelates pre-training data characteristics, model size, and QA accuracy,\nwithout requiring any additional training. The experimental results demonstrate\nthat SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on\nmodels with over one billion parameters. Theoretical analysis further reveals\nthe marginal benefits of scaling model size and optimizing data, indicating\nthat the upper limit of specific QA task accuracy is approximately 80%. Our\nproject is available at https://github.com/yuhui1038/SMI."}
{"id": "2502.04345", "pdf": "https://arxiv.org/pdf/2502.04345.pdf", "abs": "https://arxiv.org/abs/2502.04345", "title": "JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment", "authors": ["Yehan Yang", "Tianhao Ma", "Ruotai Li", "Xinhan Zheng", "Guodong Shan", "Chisheng Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The effective application of traditional Chinese medicine (TCM) requires\nextensive knowledge of TCM and clinical experience. The emergence of Large\nLanguage Models (LLMs) provides a solution to this, while existing LLMs for TCM\nexhibit critical limitations of incomplete clinical consultation and diagnoses,\nas well as inaccurate syndrome differentiation. To address these issues, we\nestablish JingFang (JF), a novel TCM LLM that demonstrates the level of\nexpertise in clinical consultation and syndrome differentiation. We propose a\nMulti-Agent Collaborative Chain-of-Thought Mechanism (MACCTM) for comprehensive\nand targeted clinical consultation, enabling JF with effective and accurate\ndiagnostic ability. In addition, a Syndrome Agent and a Dual-Stage Recovery\nScheme (DSRS) are developed to accurately enhance the differentiation of the\nsyndrome and the subsequent corresponding treatment. JingFang not only\nfacilitates the application of LLMs but also promotes the effective application\nof TCM for healthcare."}
{"id": "2502.04394", "pdf": "https://arxiv.org/pdf/2502.04394.pdf", "abs": "https://arxiv.org/abs/2502.04394", "title": "DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease", "authors": ["Tingyu Mo", "Jacqueline C. K. Lam", "Victor O. K. Li", "Lawrence Y. L. Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\naffecting 50 million people worldwide. Low-cost, accurate identification of key\nmarkers of AD is crucial for timely diagnosis and intervention. Language\nimpairment is one of the earliest signs of cognitive decline, which can be used\nto discriminate AD patients from normal control individuals.\nPatient-interviewer dialogues may be used to detect such impairments, but they\nare often mixed with ambiguous, noisy, and irrelevant information, making the\nAD detection task difficult. Moreover, the limited availability of AD speech\nsamples and variability in their speech styles pose significant challenges in\ndeveloping robust speech-based AD detection models. To address these\nchallenges, we propose DECT, a novel speech-based domain-specific approach\nleveraging large language models (LLMs) for fine-grained linguistic analysis\nand label-switched label-preserved data generation. Our study presents four\nnovelties: We harness the summarizing capabilities of LLMs to identify and\ndistill key Cognitive-Linguistic information from noisy speech transcripts,\neffectively filtering irrelevant information. We leverage the inherent\nlinguistic knowledge of LLMs to extract linguistic markers from unstructured\nand heterogeneous audio transcripts. We exploit the compositional ability of\nLLMs to generate AD speech transcripts consisting of diverse linguistic\npatterns to overcome the speech data scarcity challenge and enhance the\nrobustness of AD detection models. We use the augmented AD textual speech\ntranscript dataset and a more fine-grained representation of AD textual speech\ntranscript data to fine-tune the AD detection model. The results have shown\nthat DECT demonstrates superior model performance with an 11% improvement in AD\ndetection accuracy on the datasets from DementiaBank compared to the baselines."}
{"id": "2502.04528", "pdf": "https://arxiv.org/pdf/2502.04528.pdf", "abs": "https://arxiv.org/abs/2502.04528", "title": "Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection", "authors": ["Minseok Jung", "Cynthia Fuertes Panizo", "Liam Dugan", "Yi R.", "Fung", "Pin-Yu Chen", "Paul Pu Liang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The advancement of large language models (LLMs) has made it difficult to\ndifferentiate human-written text from AI-generated text. Several AI-text\ndetectors have been developed in response, which typically utilize a fixed\nglobal threshold (e.g., $\\theta = 0.5$) to classify machine-generated text.\nHowever, one universal threshold could fail to account for distributional\nvariations by subgroups. For example, when using a fixed threshold, detectors\nmake more false positive errors on shorter human-written text, and more\npositive classifications of neurotic writing styles among long texts. These\ndiscrepancies can lead to misclassifications that disproportionately affect\ncertain groups. We address this critical limitation by introducing FairOPT, an\nalgorithm for group-specific threshold optimization for probabilistic AI-text\ndetectors. We partitioned data into subgroups based on attributes (e.g., text\nlength and writing style) and implemented FairOPT to learn decision thresholds\nfor each group to reduce discrepancy. In experiments with nine AI text\nclassifiers on three datasets, FairOPT decreases overall balanced error rate\n(BER) discrepancy by 12\\% while minimally sacrificing accuracy by 0.003\\%. Our\nframework paves the way for more robust classification in AI-generated content\ndetection via post-processing."}
{"id": "2502.07340", "pdf": "https://arxiv.org/pdf/2502.07340.pdf", "abs": "https://arxiv.org/abs/2502.07340", "title": "Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering", "authors": ["Shuzheng Si", "Haozhe Zhao", "Gang Chen", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Kaikai An", "Kangyang Luo", "Chen Qian", "Fanchao Qi", "Baobao Chang", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Training LLMs on data containing unfamiliar knowledge during the instruction\ntuning stage can encourage hallucinations. To address this challenge, we\nintroduce NOVA, a novel framework designed to identify high-quality data that\naligns well with the LLM's learned knowledge to reduce hallucinations. NOVA\nincludes Internal Consistency Probing (ICP) and Semantic Equivalence\nIdentification (SEI) to measure how familiar the LLM is with instruction data.\nSpecifically, ICP evaluates the LLM's understanding of the given instruction by\ncalculating the tailored consistency among multiple self-generated responses.\nSEI further assesses the familiarity of the LLM with the target response by\ncomparing it to the generated responses, using the proposed semantic clustering\nand well-designed voting strategy. Finally, to ensure the quality of selected\nsamples, we introduce an expert-aligned reward model, considering\ncharacteristics beyond just familiarity. By considering data quality and\navoiding unfamiliar data, we can utilize the selected data to effectively align\nLLMs to follow instructions and hallucinate less."}
{"id": "2502.08279", "pdf": "https://arxiv.org/pdf/2502.08279.pdf", "abs": "https://arxiv.org/abs/2502.08279", "title": "What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations", "authors": ["Dongqi Liu", "Chenxi Whitehouse", "Xi Yu", "Louis Mahon", "Rohit Saxena", "Zheng Zhao", "Yifu Qiu", "Mirella Lapata", "Vera Demberg"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Main & Long Conference Paper", "summary": "Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of our dataset. This study aims to\npave the way for future research on scientific video-to-text summarization."}
{"id": "2502.08767", "pdf": "https://arxiv.org/pdf/2502.08767.pdf", "abs": "https://arxiv.org/abs/2502.08767", "title": "SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence", "authors": ["Zhining Liu", "Rana Ali Amjad", "Ravinarayana Adkathimar", "Tianxin Wei", "Hanghang Tong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025. 21 pages, 5 figures, 13 tables", "summary": "Providing Language Models (LMs) with relevant evidence in the context (either\nvia retrieval or user-provided) can significantly improve their ability to\nprovide better-grounded responses. However, recent studies have found that LMs\noften struggle to fully comprehend and utilize key evidence from the context,\nespecially when it contains noise and irrelevant information, an issue common\nin real-world scenarios. To address this, we propose SelfElicit, an\ninference-time approach that helps LMs focus on key contextual evidence through\nself-guided explicit highlighting. By leveraging the inherent evidence-finding\ncapabilities of LMs using the attention scores of deeper layers, our method\nautomatically identifies and emphasizes key evidence within the input context,\nfacilitating more accurate and grounded responses without additional training\nor iterative prompting. We demonstrate that SelfElicit brings consistent and\nsignificant improvement on multiple evidence-based QA tasks for various LM\nfamilies while maintaining computational efficiency. Our code and documentation\nare available at https://github.com/ZhiningLiu1998/SelfElicit."}
{"id": "2502.11211", "pdf": "https://arxiv.org/pdf/2502.11211.pdf", "abs": "https://arxiv.org/abs/2502.11211", "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?", "authors": ["Wenxuan Wang", "Zizhan Ma", "Zheng Wang", "Chenghan Wu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Yixuan Yuan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) are transforming healthcare through the\ndevelopment of LLM-based agents that can understand, reason about, and assist\nwith medical tasks. This survey provides a comprehensive review of LLM-based\nagents in medicine, examining their architectures, applications, and\nchallenges. We analyze the key components of medical agent systems, including\nsystem profiles, clinical planning mechanisms, medical reasoning frameworks,\nand external capacity enhancement. The survey covers major application\nscenarios such as clinical decision support, medical documentation, training\nsimulations, and healthcare service optimization. We discuss evaluation\nframeworks and metrics used to assess these agents' performance in healthcare\nsettings. While LLM-based agents show promise in enhancing healthcare delivery,\nseveral challenges remain, including hallucination management, multimodal\nintegration, implementation barriers, and ethical considerations. The survey\nconcludes by highlighting future research directions, including advances in\nmedical reasoning inspired by recent developments in LLM architectures,\nintegration with physical systems, and improvements in training simulations.\nThis work provides researchers and practitioners with a structured overview of\nthe current state and future prospects of LLM-based agents in medicine."}
{"id": "2502.11393", "pdf": "https://arxiv.org/pdf/2502.11393.pdf", "abs": "https://arxiv.org/abs/2502.11393", "title": "HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning", "authors": ["Xiaoyuan Li", "Moxin Li", "Rui Men", "Yichang Zhang", "Keqin Bao", "Wenjie Wang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable capabilities in\ncommonsense reasoning; however, some variations in questions can trigger\nincorrect responses. Do these models truly understand commonsense knowledge, or\njust memorize expression patterns? To investigate this question, we present the\nfirst extensive robustness evaluation of LLMs in commonsense reasoning. We\nintroduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200\ncases, by designing and compiling seven types of question variants. To\nconstruct this benchmark, we propose a two-stage method to develop Chinese\nHellaSwag, a finely annotated dataset comprising 12,000 instances across 56\ncategories. We conduct extensive experiments on 41 representative LLMs,\nrevealing that these LLMs are far from robust in commonsense reasoning.\nFurthermore, this robustness varies depending on the language in which the LLM\nis tested. This work establishes a high-quality evaluation benchmark, with\nextensive experiments offering valuable insights to the community in\ncommonsense reasoning for LLMs."}
{"id": "2502.11514", "pdf": "https://arxiv.org/pdf/2502.11514.pdf", "abs": "https://arxiv.org/abs/2502.11514", "title": "Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study", "authors": ["Yujie Lin", "Ante Wang", "Moye Chen", "Jingyao Liu", "Hao Liu", "Jinsong Su", "Xinyan Xiao"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, inference-time scaling of chain-of-thought (CoT) has been\ndemonstrated as a promising approach for addressing multi-modal reasoning\ntasks. While existing studies have predominantly centered on text-based\nthinking, the integration of both visual and textual modalities within the\nreasoning process remains unexplored. In this study, we pioneer the exploration\nof inference-time scaling with multi-modal thought, aiming to bridge this gap.\nTo provide a comprehensive analysis, we systematically investigate popular\nsampling-based and tree search-based inference-time scaling methods on 10\nchallenging tasks spanning various domains. Besides, we uniformly adopt a\nconsistency-enhanced verifier to ensure effective guidance for both methods\nacross different thought paradigms. Results show that multi-modal thought\npromotes better performance against conventional text-only thought, and\nblending the two types of thought fosters more diverse thinking. Despite these\nadvantages, multi-modal thoughts necessitate higher token consumption for\nprocessing richer visual inputs, which raises concerns in practical\napplications. We hope that our findings on the merits and drawbacks of this\nresearch line will inspire future works in the field."}
{"id": "2502.11598", "pdf": "https://arxiv.org/pdf/2502.11598.pdf", "abs": "https://arxiv.org/abs/2502.11598", "title": "Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?", "authors": ["Leyi Pan", "Aiwei Liu", "Shiyu Huang", "Yijian Lu", "Xuming Hu", "Lijie Wen", "Irwin King", "Philip S. Yu"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Accepted by ACL 2025 (Main)", "summary": "The radioactive nature of Large Language Model (LLM) watermarking enables the\ndetection of watermarks inherited by student models when trained on the outputs\nof watermarked teacher models, making it a promising tool for preventing\nunauthorized knowledge distillation. However, the robustness of watermark\nradioactivity against adversarial actors remains largely unexplored. In this\npaper, we investigate whether student models can acquire the capabilities of\nteacher models through knowledge distillation while avoiding watermark\ninheritance. We propose two categories of watermark removal approaches:\npre-distillation removal through untargeted and targeted training data\nparaphrasing (UP and TP), and post-distillation removal through inference-time\nwatermark neutralization (WN). Extensive experiments across multiple model\npairs, watermarking schemes and hyper-parameter settings demonstrate that both\nTP and WN thoroughly eliminate inherited watermarks, with WN achieving this\nwhile maintaining knowledge transfer efficiency and low computational overhead.\nGiven the ongoing deployment of watermarking techniques in production LLMs,\nthese findings emphasize the urgent need for more robust defense strategies.\nOur code is available at\nhttps://github.com/THU-BPM/Watermark-Radioactivity-Attack."}
{"id": "2502.11736", "pdf": "https://arxiv.org/pdf/2502.11736.pdf", "abs": "https://arxiv.org/abs/2502.11736", "title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews", "authors": ["Madhav Krishan Garg", "Tejash Prasad", "Tanmay Singhal", "Chhavi Kirtani", "Murari Mandal", "Dhruv Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review: 8 pages, 2 figures, 5 tables, 9 pages for appendix", "summary": "The escalating volume of academic research, coupled with a shortage of\nqualified reviewers, necessitates innovative approaches to peer review. In this\nwork, we propose: 1. ReviewEval, a comprehensive evaluation framework for\nAI-generated reviews that measures alignment with human assessments, verifies\nfactual accuracy, assesses analytical depth, identifies degree of\nconstructiveness and adherence to reviewer guidelines; and 2. ReviewAgent, an\nLLM-based review generation agent featuring a novel alignment mechanism to\ntailor feedback to target conferences and journals, along with a\nself-refinement loop that iteratively optimizes its intermediate outputs and an\nexternal improvement loop using ReviewEval to improve upon the final reviews.\nReviewAgent improves actionable insights by 6.78% and 47.62% over existing AI\nbaselines and expert reviews respectively. Further, it boosts analytical depth\nby 3.97% and 12.73%, enhances adherence to guidelines by 10.11% and 47.26%\nrespectively. This paper establishes essential metrics for AIbased peer review\nand substantially enhances the reliability and impact of AI-generated reviews\nin academic research."}
{"id": "2502.11962", "pdf": "https://arxiv.org/pdf/2502.11962.pdf", "abs": "https://arxiv.org/abs/2502.11962", "title": "Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning", "authors": ["Tianyi Wu", "Jingwei Ni", "Bryan Hooi", "Jiaheng Zhang", "Elliott Ash", "See-Kiong Ng", "Mrinmaya Sachan", "Markus Leippold"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction fine-tuning (IFT) can increase the informativeness of large\nlanguage models (LLMs), but may reduce their truthfulness. This trade-off\narises because IFT steers LLMs to generate responses containing long-tail\nknowledge that was not well covered during pre-training. As a result, models\nbecome more informative but less accurate when generalizing to unseen tasks. In\nthis paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets\ncan negatively affect the truthfulness of LLMs, and we introduce two new IFT\nparadigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$\nidentifies and removes unfamiliar knowledge from IFT datasets to mitigate its\nimpact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize\ntheir uncertainty and explicitly indicate it at the end of their responses. Our\nexperiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,\nwhile $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by\ndistinguishing between confident and uncertain statements."}
{"id": "2502.12051", "pdf": "https://arxiv.org/pdf/2502.12051.pdf", "abs": "https://arxiv.org/abs/2502.12051", "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 11 tables, 4 figures", "summary": "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies."}
{"id": "2502.12067", "pdf": "https://arxiv.org/pdf/2502.12067.pdf", "abs": "https://arxiv.org/abs/2502.12067", "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "authors": ["Heming Xia", "Chak Tou Leong", "Wenjie Wang", "Yongqi Li", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop."}
{"id": "2502.12289", "pdf": "https://arxiv.org/pdf/2502.12289.pdf", "abs": "https://arxiv.org/abs/2502.12289", "title": "Evaluating Step-by-step Reasoning Traces: A Survey", "authors": ["Jinu Lee", "Julia Hockenmaier"], "categories": ["cs.CL"], "comment": "25 pages (8 pages of main content), 5 figures", "summary": "Step-by-step reasoning is widely used to enhance the reasoning ability of\nlarge language models (LLMs) in complex problems. Evaluating the quality of\nreasoning traces is crucial for understanding and improving LLM reasoning.\nHowever, existing evaluation practices are highly inconsistent, resulting in\nfragmented progress across evaluator design and benchmark development. To\naddress this gap, this survey provides a comprehensive overview of step-by-step\nreasoning evaluation, proposing a taxonomy of evaluation criteria with four\ntop-level categories (factuality, validity, coherence, and utility). Based on\nthe taxonomy, we review different evaluator implementations and recent\nfindings, leading to promising directions for future research."}
{"id": "2502.12568", "pdf": "https://arxiv.org/pdf/2502.12568.pdf", "abs": "https://arxiv.org/abs/2502.12568", "title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation", "authors": ["Kaiyang Wan", "Honglin Mu", "Rui Hao", "Haoran Luo", "Tianle Gu", "Xiuying Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures", "summary": "Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}."}
{"id": "2502.12672", "pdf": "https://arxiv.org/pdf/2502.12672.pdf", "abs": "https://arxiv.org/abs/2502.12672", "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization", "authors": ["Tzu-Quan Lin", "Wei-Ping Huang", "Hao Tang", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning speech representation models can enhance performance on specific\ntasks but often compromises their cross-task generalization ability. This\ndegradation is often caused by excessive changes in the representations, making\nit difficult to retain information learned during pre-training. Existing\napproaches, such as regularizing weight changes during fine-tuning, may fail to\nmaintain sufficiently high feature similarity with the pre-trained model, and\nthus could possibly lose cross-task generalization. To address this issue, we\npropose Speech-FT, a novel two-stage fine-tuning framework designed to maintain\ncross-task generalization while benefiting from fine-tuning. Speech-FT first\napplies fine-tuning specifically designed to reduce representational drift,\nfollowed by weight-space interpolation with the pre-trained model to restore\ncross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR\n2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves\nperformance across a wide range of supervised, unsupervised, and multitask\nfine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task\ngeneralization compared to fine-tuning baselines that explicitly constrain\nweight changes, such as weight-space regularization and LoRA fine-tuning. Our\nanalysis reveals that Speech-FT maintains higher feature similarity to the\npre-trained model compared to alternative strategies, despite allowing larger\nweight-space updates. Notably, Speech-FT achieves significant improvements on\nthe SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech\nrecognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%,\nlower word error rate from 6.38% to 5.75%, and increase speaker identification\naccuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful\nsolution for further refining speech representation models after pre-training."}
{"id": "2502.12904", "pdf": "https://arxiv.org/pdf/2502.12904.pdf", "abs": "https://arxiv.org/abs/2502.12904", "title": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements", "authors": ["Shu Yang", "Shenzhe Zhu", "Zeyu Wu", "Keyu Wang", "Junchi Yao", "Junchao Wu", "Lijie Hu", "Mengdi Li", "Derek F. Wong", "Di Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings", "summary": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities."}
{"id": "2502.12924", "pdf": "https://arxiv.org/pdf/2502.12924.pdf", "abs": "https://arxiv.org/abs/2502.12924", "title": "Conditioning LLMs to Generate Code-Switched Text", "authors": ["Maite Heredia", "Gorka Labaka", "Jeremy Barnes", "Aitor Soroa"], "categories": ["cs.CL", "cs.AI"], "comment": "[v2]Added new experiments and analyses", "summary": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license."}
{"id": "2502.13034", "pdf": "https://arxiv.org/pdf/2502.13034.pdf", "abs": "https://arxiv.org/abs/2502.13034", "title": "Natural Language Generation from Visual Events: Challenges and Future Directions", "authors": ["Aditya K Surikuchi", "Raquel Fernández", "Sandro Pezzelle"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The ability to use natural language to talk about visual events is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. In recent years, a substantial body of work in visually grounded NLP\nhas focused on describing content depicted in single images. By contrast,\ncomparatively less attention has been devoted to exhaustively modeling\nscenarios in which natural language is employed to interpret and talk about\nevents presented through videos or sequences of images. In this position paper,\nwe argue that any NLG task dealing with sequences of images or frames is an\ninstance of the broader, more general problem of modeling the intricate\nrelationships between visual events unfolding over time and the features of the\nlanguage used to interpret, describe, or narrate them. Therefore, solving these\ntasks requires models to be capable of identifying and managing such\nintricacies. We consider five seemingly different tasks, which we argue are\ncompelling instances of this broader multimodal problem. Consistently, we claim\nthat these tasks pose a common set of challenges and share similarities in\nterms of modeling and evaluation approaches. Building on this perspective, we\nidentify key open questions and propose several research directions for future\ninvestigation. We claim that improving language-and-vision models'\nunderstanding of visual events is both timely and essential, given their\ngrowing applications. Additionally, this challenge offers significant\nscientific insight, advancing model development through principles of human\ncognition and language use."}
{"id": "2502.13311", "pdf": "https://arxiv.org/pdf/2502.13311.pdf", "abs": "https://arxiv.org/abs/2502.13311", "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors", "authors": ["Jian Wang", "Yinpei Dai", "Yichi Zhang", "Ziqiao Ma", "Wenjie Li", "Joyce Chai"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized knowledge in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students towards completing\npredefined coding tasks. We propose a novel agent workflow, Trace-and-Verify\n(TRAVER), which combines knowledge tracing to estimate a student's knowledge\nstate and turn-by-turn verification to ensure effective guidance toward task\ncompletion. We introduce DICT, an automatic evaluation protocol that assesses\ntutor agents using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our approach can be extended beyond\ncoding, providing valuable insights into advancing tutoring agents for human\ntask learning."}
{"id": "2502.14662", "pdf": "https://arxiv.org/pdf/2502.14662.pdf", "abs": "https://arxiv.org/abs/2502.14662", "title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL 2025 and WWW2025@HCRS", "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure."}
{"id": "2502.14924", "pdf": "https://arxiv.org/pdf/2502.14924.pdf", "abs": "https://arxiv.org/abs/2502.14924", "title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?", "authors": ["Ibrahim Alabdulmohsin", "Andreas Steiner"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language exhibits a fractal structure in its information-theoretic complexity\n(i.e. bits per token), with self-similarity across scales and long-range\ndependence (LRD). In this work, we investigate whether large language models\n(LLMs) can replicate such fractal characteristics and identify conditions-such\nas temperature setting and prompting method-under which they may fail.\nMoreover, we find that the fractal parameters observed in natural language are\ncontained within a narrow range, whereas those of LLMs' output vary widely,\nsuggesting that fractal parameters might prove helpful in detecting a\nnon-trivial portion of LLM-generated texts. Notably, these findings, and many\nothers reported in this work, are robust to the choice of the architecture;\ne.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset\ncomprising of over 240,000 articles generated by various LLMs (both pretrained\nand instruction-tuned) with different decoding temperatures and prompting\nmethods, along with their corresponding human-generated texts. We hope that\nthis work highlights the complex interplay between fractal properties,\nprompting, and statistical mimicry in LLMs, offering insights for generating,\nevaluating and detecting synthetic texts."}
{"id": "2502.15094", "pdf": "https://arxiv.org/pdf/2502.15094.pdf", "abs": "https://arxiv.org/abs/2502.15094", "title": "Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models", "authors": ["Marianne Chuang", "Gabriel Chuang", "Cheryl Chuang", "John Chuang"], "categories": ["cs.CL", "stat.AP"], "comment": "17 pages, 12 figures. To appear, ClimateNLP 2025", "summary": "We study the use of large language models (LLMs) to both evaluate and\ngreenwash corporate climate disclosures. First, we investigate the use of the\nLLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on\nemissions reduction targets and progress. Second, we probe the behavior of an\nLLM when it is prompted to greenwash a response subject to accuracy and length\nconstraints. Finally, we test the robustness of the LLMJ methodology against\nresponses that may be greenwashed using an LLM. We find that two LLMJ scoring\nsystems, numerical rating and pairwise comparison, are effective in\ndistinguishing high-performing companies from others, with the pairwise\ncomparison system showing greater robustness against LLM-greenwashed responses."}
{"id": "2502.15361", "pdf": "https://arxiv.org/pdf/2502.15361.pdf", "abs": "https://arxiv.org/abs/2502.15361", "title": "Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning", "authors": ["Xuyang Wu", "Jinming Nian", "Ting-Ruen Wei", "Zhiqiang Tao", "Hsin-Tai Wu", "Yi Fang"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Recent advances in large language models (LLMs) have enabled automatic\ngeneration of chain-of-thought (CoT) reasoning, leading to strong performance\non tasks such as math and code. However, when reasoning steps reflect social\nstereotypes (e.g., those related to gender, race or age), they can reinforce\nharmful associations and lead to misleading conclusions. We present the first\nsystematic evaluation of social bias within LLM-generated reasoning, using the\nBBQ dataset to analyze both prediction accuracy and bias. Our study spans a\nwide range of mainstream reasoning models, including instruction-tuned and\nCoT-augmented variants of DeepSeek-R1 (8B/32B), ChatGPT, and other open-source\nLLMs. We quantify how biased reasoning steps correlate with incorrect\npredictions and often lead to stereotype expression. To mitigate\nreasoning-induced bias, we propose Answer Distribution as Bias Proxy (ADBP), a\nlightweight mitigation method that detects bias by tracking how model\npredictions change across incremental reasoning steps. ADBP outperforms a\nstereotype-free baseline in most cases, mitigating bias and improving the\naccuracy of LLM outputs. Code will be released upon paper acceptance."}
{"id": "2502.16514", "pdf": "https://arxiv.org/pdf/2502.16514.pdf", "abs": "https://arxiv.org/abs/2502.16514", "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking", "authors": ["Yingjian Chen", "Haoran Liu", "Yinhong Liu", "Jinxiang Xie", "Rui Yang", "Han Yuan", "Yanran Fu", "Peng Yuan Zhou", "Qingyu Chen", "James Caverlee", "Irene Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used, but they often generate subtle\nfactual errors, especially in long-form text. These errors are fatal in some\nspecialized domains such as medicine. Existing fact-checking with grounding\ndocuments methods face two main challenges: (1) they struggle to understand\ncomplex multihop relations in long documents, often overlooking subtle factual\nerrors; (2) most specialized methods rely on pairwise comparisons, requiring\nmultiple model calls, leading to high resource and computational costs. To\naddress these challenges, we propose GraphCheck, a fact-checking framework that\nuses extracted knowledge graphs to enhance text representation. Graph Neural\nNetworks further process these graphs as a soft prompt, enabling LLMs to\nincorporate structured knowledge more effectively. Enhanced with graph-based\nreasoning, GraphCheck captures multihop reasoning chains that are often\noverlooked by existing methods, enabling precise and efficient fact-checking in\na single inference call. Experimental results on seven benchmarks spanning both\ngeneral and medical domains demonstrate up to a 7.1% overall improvement over\nbaseline models. Notably, GraphCheck outperforms existing specialized\nfact-checkers and achieves comparable performance with state-of-the-art LLMs,\nsuch as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters."}
{"id": "2502.16880", "pdf": "https://arxiv.org/pdf/2502.16880.pdf", "abs": "https://arxiv.org/abs/2502.16880", "title": "CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter", "authors": ["Yepeng Weng", "Dianwen Mei", "Huishi Qiu", "Xujie Chen", "Li Liu", "Jiang Tian", "Zhongchao Shi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 main conference", "summary": "Speculative decoding is a powerful technique that accelerates Large Language\nModel (LLM) inference by leveraging a lightweight speculative draft model.\nHowever, existing designs suffers in performance due to misalignment between\ntraining and inference. Recent methods have tried to solve this issue by\nadopting a multi-step training strategy, but the complex inputs of different\ntraining steps make it harder for the draft model to converge. To address this,\nwe propose CORAL, a novel framework that improves both accuracy and efficiency\nin speculative drafting. CORAL introduces Cross-Step Representation Alignment,\na method that enhances consistency across multiple training steps,\nsignificantly improving speculative drafting performance. Additionally, we\nidentify the LM head as a major bottleneck in the inference speed of the draft\nmodel. We introduce a weight-grouping mechanism that selectively activates a\nsubset of LM head parameters during inference, substantially reducing the\nlatency of the draft model. We evaluate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming\nstate-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that\nCORAL effectively mitigates training-inference misalignment and delivers\nsignificant speedup for modern LLMs with large vocabularies."}
{"id": "2502.17173", "pdf": "https://arxiv.org/pdf/2502.17173.pdf", "abs": "https://arxiv.org/abs/2502.17173", "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch", "authors": ["Xueru Wen", "Jie Lou", "Zichao Li", "Yaojie Lu", "Xing Yu", "Yuqiu Ji", "Guohai Xu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Debing Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development."}
{"id": "2502.18001", "pdf": "https://arxiv.org/pdf/2502.18001.pdf", "abs": "https://arxiv.org/abs/2502.18001", "title": "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Zhijing Sun", "Wenjin Guo", "Miaoran Zhang", "Yanjun Chen", "Yirong Sun", "Hui Su", "Yijie Pan", "Dietrich Klakow", "Wenjie Li", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in reasoning tasks through\nChain-of-Thought (CoT) prompting. However, CoT prompting greatly increases\ncomputational demands, which has prompted growing interest in distilling CoT\ncapabilities into Small Language Models (SLMs). This study systematically\nexamines the factors influencing CoT distillation, including the choice of\ngranularity, format and teacher model. Through experiments involving four\nteacher models and seven student models across seven mathematical and\ncommonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,\nSLMs exhibit a non-monotonic relationship with granularity, with stronger\nmodels benefiting from finer-grained reasoning and weaker models performing\nbetter with simpler CoT supervision; (2) CoT format significantly impacts LLMs\nbut has minimal effect on SLMs, likely due to their reliance on supervised\nfine-tuning rather than pretraining preferences; (3) Stronger teacher models do\nNOT always produce better student models, as diversity and complexity in CoT\nsupervision can outweigh accuracy alone. These findings emphasize the need to\ntailor CoT strategies to specific student model, offering actionable insights\nfor optimizing CoT distillation in SLMs. The code and datasets are available at\nhttps://github.com/EIT-NLP/Distilling-CoT-Reasoning."}
{"id": "2502.18791", "pdf": "https://arxiv.org/pdf/2502.18791.pdf", "abs": "https://arxiv.org/abs/2502.18791", "title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs", "authors": ["Jungsoo Park", "Junmo Kang", "Gabriel Stanovsky", "Alan Ritter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 main conference", "summary": "The surge of LLM studies makes synthesizing their findings challenging.\nAnalysis of experimental results from literature can uncover important trends\nacross studies, but the time-consuming nature of manual data extraction limits\nits use. Our study presents a semi-automated approach for literature analysis\nthat accelerates data extraction using LLMs. It automatically identifies\nrelevant arXiv papers, extracts experimental results and related attributes,\nand organizes them into a structured dataset, LLMEvalDB. We then conduct an\nautomated literature analysis of frontier LLMs, reducing the effort of paper\nsurveying and data extraction by more than 93% compared to manual approaches.\nWe validate LLMEvalDB by showing that it reproduces key findings from a recent\nmanual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new\ninsights that go beyond it, showing, for example, that in-context examples\nbenefit coding & multimodal tasks but offer limited gains in math reasoning\ntasks compared to zero-shot CoT. Our automatically updatable dataset enables\ncontinuous tracking of target models by extracting evaluation studies as new\ndata becomes available. Through LLMEvalDB and empirical analysis, we provide\ninsights into LLMs while facilitating ongoing literature analyses of their\nbehavior."}
{"id": "2502.19127", "pdf": "https://arxiv.org/pdf/2502.19127.pdf", "abs": "https://arxiv.org/abs/2502.19127", "title": "Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization", "authors": ["Siyuan Zhang", "Yichi Zhang", "Yinpeng Dong", "Hang Su"], "categories": ["cs.CL"], "comment": "31 pages, 17 figures", "summary": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. Although\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its knowledge and introduce\nPKUE, which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments demonstrate that PKUE significantly\nimproves LLM overall performance, with consistent enhancement across factual\ntasks of various forms, general tasks beyond factuality, and tasks in a\ndifferent language."}
{"id": "2502.19148", "pdf": "https://arxiv.org/pdf/2502.19148.pdf", "abs": "https://arxiv.org/abs/2502.19148", "title": "Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs", "authors": ["Zhaowei Zhang", "Fengshuo Bai", "Qizhi Chen", "Chengdong Ma", "Mingzhi Wang", "Haoran Sun", "Zilong Zheng", "Yaodong Yang"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "Accepted by ICLR 2025, Project page:\n  https://zowiezhang.github.io/projects/Amulet", "summary": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency."}
{"id": "2502.19735", "pdf": "https://arxiv.org/pdf/2502.19735.pdf", "abs": "https://arxiv.org/abs/2502.19735", "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning", "authors": ["Minggui He", "Yilun Liu", "Shimin Tao", "Yuanchang Luo", "Hongyong Zeng", "Chang Su", "Li Zhang", "Hongxia Ma", "Daimeng Wei", "Weibin Meng", "Hao Yang", "Boxing Chen", "Osamu Yoshie"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans and supervised fine-tuning (SFT) prone to overfitting, limiting their\nadaptability to diverse translation scenarios. This paper introduces\nR1-Translator (R1-T1), a novel framework to achieve inference-time reasoning\nfor general MT via reinforcement learning (RL) with human-aligned CoTs\ncomprising six common patterns. Our approach pioneers three innovations: (1)\nextending reasoning-based translation to broader MT scenarios (e.g.,\nmultilingual MT, domain MT) unseen in the training phase; (2) formalizing six\nexpert-curated CoT templates that mirror hybrid human strategies like\ncontext-aware paraphrasing and back translation; and (3) enabling self-evolving\nCoT discovery through RL. Both human and automatic evaluation results indicate\na steady translation performance improvement in a total of 10+ languages and\n40+ translation directions on Flores-101 test set and four domain-specific MT\ntasks, especially on the languages unseen from training."}
{"id": "2502.19953", "pdf": "https://arxiv.org/pdf/2502.19953.pdf", "abs": "https://arxiv.org/abs/2502.19953", "title": "GeoEdit: Geometric Knowledge Editing for Large Language Models", "authors": ["Yujie Feng", "Liming Zhan", "Zexin Lu", "Yongxin Xu", "Xu Chu", "Yasha Wang", "Jiannong Cao", "Philip S. Yu", "Xiao-Ming Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Regular updates are essential for maintaining up-to-date knowledge in large\nlanguage models (LLMs). Consequently, various model editing methods have been\ndeveloped to update specific knowledge within LLMs. However, training-based\napproaches often struggle to effectively incorporate new knowledge while\npreserving unrelated general knowledge. To address this challenge, we propose a\nnovel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes\nthe geometric relationships of parameter updates from fine-tuning to\ndifferentiate between neurons associated with new knowledge updates and those\nrelated to general knowledge perturbations. By employing a direction-aware\nknowledge identification method, we avoid updating neurons with directions\napproximately orthogonal to existing knowledge, thus preserving the model's\ngeneralization ability. For the remaining neurons, we integrate both old and\nnew knowledge for aligned directions and apply a \"forget-then-learn\" editing\nstrategy for opposite directions. Additionally, we introduce an\nimportance-guided task vector fusion technique that filters out redundant\ninformation and provides adaptive neuron-level weighting, further enhancing\nmodel editing performance. Extensive experiments on two publicly available\ndatasets demonstrate the superiority of GeoEdit over existing state-of-the-art\nmethods."}
{"id": "2502.21017", "pdf": "https://arxiv.org/pdf/2502.21017.pdf", "abs": "https://arxiv.org/abs/2502.21017", "title": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues", "authors": ["Fangxu Yu", "Lai Jiang", "Shenyi Huang", "Zhen Wu", "Xinyu Dai"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\nscenarios. Although recent studies have evaluated ToM in Large Language Models\n(LLMs), existing benchmarks focus on simplified settings (e.g.,\nSally-Anne-style tasks) and overlook the complexity of real-world social\ninteractions. To mitigate this gap, we propose PersuasiveToM, a benchmark\ndesigned to evaluate the ToM abilities of LLMs in persuasive dialogues. Our\nframework contains two core tasks: ToM Reasoning, which tests tracking of\nevolving desires, beliefs, and intentions; and ToM Application, which assesses\nthe use of inferred mental states to predict and evaluate persuasion\nstrategies. Experiments across eight leading LLMs reveal that while models\nexcel on multiple questions, they struggle with the tasks that need tracking\nthe dynamics and shifts of mental states and understanding the mental states in\nthe whole dialogue comprehensively. Our aim with PersuasiveToM is to allow an\neffective evaluation of the ToM reasoning ability of LLMs with more focus on\ncomplex psychological activities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM."}
{"id": "2503.00032", "pdf": "https://arxiv.org/pdf/2503.00032.pdf", "abs": "https://arxiv.org/abs/2503.00032", "title": "Detecting LLM-Generated Korean Text through Linguistic Feature Analysis", "authors": ["Shinwoo Park", "Shubin Kim", "Do-Kyung Kim", "Yo-Sub Han"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis."}
{"id": "2503.01763", "pdf": "https://arxiv.org/pdf/2503.01763.pdf", "abs": "https://arxiv.org/abs/2503.01763", "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", "authors": ["Zhengliang Shi", "Yuhan Wang", "Lingyong Yan", "Pengjie Ren", "Shuaiqiang Wang", "Dawei Yin", "Zhaochun Ren"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025. Code: https://github.com/mangopy/tool-retrieval-benchmark", "summary": "Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels."}
{"id": "2503.02863", "pdf": "https://arxiv.org/pdf/2503.02863.pdf", "abs": "https://arxiv.org/abs/2503.02863", "title": "SteerConf: Steering LLMs for Confidence Elicitation", "authors": ["Ziang Zhou", "Tianyuan Jin", "Jieming Shi", "Qing Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) exhibit impressive performance across diverse\ndomains but often suffer from overconfidence, limiting their reliability in\ncritical applications. We propose SteerConf, a novel framework that\nsystematically steers LLMs' confidence scores to improve their calibration and\nreliability. SteerConf introduces three key components: (1) a steering prompt\nstrategy that guides LLMs to produce confidence scores in specified directions\n(e.g., conservative or optimistic) by leveraging prompts with varying steering\nlevels; (2) a steered confidence consistency measure that quantifies alignment\nacross multiple steered confidences to enhance calibration; and (3) a steered\nconfidence calibration method that aggregates confidence scores using\nconsistency measures and applies linear quantization for answer selection.\nSteerConf operates without additional training or fine-tuning, making it\nbroadly applicable to existing LLMs. Experiments on seven benchmarks spanning\nprofessional knowledge, common sense, ethics, and reasoning tasks, using\nadvanced LLM models (GPT-3.5, LLaMA 3, GPT-4), demonstrate that SteerConf\nsignificantly outperforms existing methods, often by a significant margin. Our\nfindings highlight the potential of steering the confidence of LLMs to enhance\ntheir reliability for safer deployment in real-world applications."}
{"id": "2503.02972", "pdf": "https://arxiv.org/pdf/2503.02972.pdf", "abs": "https://arxiv.org/abs/2503.02972", "title": "LINGOLY-TOO: Disentangling Memorisation from Knowledge with Linguistic Templatisation and Orthographic Obfuscation", "authors": ["Jude Khouja", "Karolina Korgul", "Simi Hellsten", "Lingyi Yang", "Vlad Neacsu", "Harry Mayne", "Ryan Kearns", "Andrew Bean", "Adam Mahdi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The expanding knowledge and memorisation capacity of frontier language models\nallows them to solve many reasoning tasks directly by exploiting prior\nknowledge, leading to inflated estimates of their reasoning abilities. We\nintroduce LINGOLY-TOO, a challenging reasoning benchmark grounded in natural\nlanguage and designed to counteract the effect of non-reasoning abilities on\nreasoning estimates. Using linguistically informed rulesets, we permute\nreasoning problems written in real languages to generate numerous question\nvariations. These permutations preserve the intrinsic reasoning steps required\nfor each solution while reducing the likelihood problems are directly solvable\nwith models' knowledge. Experiments and analyses show that models can\ncircumvent reasoning and answer from prior knowledge. On a metric that rewards\nconsistent reasoning, all models perform poorly and exhibit high variance\nacross question permutations, indicating that Large Language Models' (LLMs)\nreasoning faculty remains brittle. Overall, results on the benchmark reflect\nthe recent progress of Inference-Time Compute (ITC) models but suggest ample\nroom for further improvement. The benchmark is a step towards better\nmeasurement of reasoning abilities of LLMs and offers a cautionary tale on the\nimportance of disentangling reasoning abilities from models' internalised\nknowledge when developing reasoning benchmarks."}
{"id": "2503.03862", "pdf": "https://arxiv.org/pdf/2503.03862.pdf", "abs": "https://arxiv.org/abs/2503.03862", "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions", "authors": ["Emmy Liu", "Amanda Bertsch", "Lintang Sutawika", "Lindia Tjuatja", "Patrick Fernandes", "Lara Marinov", "Michael Chen", "Shreya Singhal", "Carolin Lawrence", "Aditi Raghunathan", "Kiril Gashteovski", "Graham Neubig"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Improvements in language model capabilities are often attributed to\nincreasing model size or training data, but in some cases smaller models\ntrained on curated data or with different architectural decisions can\noutperform larger ones trained on more tokens. What accounts for this? To\nquantify the impact of these design choices, we meta-analyze 92 open-source\npretrained models across a wide array of scales, including state-of-the-art\nopen-weights models as well as less performant models and those with less\nconventional design decisions. We find that by incorporating features besides\nmodel size and number of training tokens, we can achieve a relative 3-28%\nincrease in ability to predict downstream performance compared with using scale\nalone. Analysis of model design decisions reveal insights into data\ncomposition, such as the trade-off between language and code tasks at 15-25\\%\ncode, as well as the better performance of some architectural decisions such as\nchoosing rotary over learned embeddings. Broadly, our framework lays a\nfoundation for more systematic investigation of how model development choices\nshape final capabilities."}
{"id": "2503.04240", "pdf": "https://arxiv.org/pdf/2503.04240.pdf", "abs": "https://arxiv.org/abs/2503.04240", "title": "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models", "authors": ["Ruizhe Chen", "Wenhao Chai", "Zhifei Yang", "Xiaotian Zhang", "Joey Tianyi Zhou", "Tony Quek", "Soujanya Poria", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B."}
{"id": "2503.04856", "pdf": "https://arxiv.org/pdf/2503.04856.pdf", "abs": "https://arxiv.org/abs/2503.04856", "title": "One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs", "authors": ["Junwoo Ha", "Hyunjun Kim", "Sangyoon Yu", "Haon Park", "Ashkan Yousefpour", "Yuna Park", "Suhyun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a novel framework for consolidating multi-turn adversarial\n``jailbreak'' prompts into single-turn queries, significantly reducing the\nmanual overhead required for adversarial testing of large language models\n(LLMs). While multi-turn human jailbreaks have been shown to yield high attack\nsuccess rates, they demand considerable human effort and time. Our\nmulti-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize\n-- systematically reformat multi-turn dialogues into structured single-turn\nprompts. Despite removing iterative back-and-forth interactions, these prompts\npreserve and often enhance adversarial potency: in extensive evaluations on the\nMulti-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success\nrates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs.\nRemarkably, the single-turn prompts outperform the original multi-turn attacks\nby as much as 17.5 percentage points while cutting token usage by more than\nhalf on average. Further analysis shows that embedding malicious requests in\nenumerated or code-like structures exploits ``contextual blindness'', bypassing\nboth native guardrails and external input-output filters. By converting\nmulti-turn conversations into concise single-turn prompts, the M2S framework\nprovides a scalable tool for large-scale red teaming and reveals critical\nweaknesses in contemporary LLM defenses."}
{"id": "2503.06692", "pdf": "https://arxiv.org/pdf/2503.06692.pdf", "abs": "https://arxiv.org/abs/2503.06692", "title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models", "authors": ["Yuchen Yan", "Yongliang Shen", "Yang Liu", "Jin Jiang", "Mengdi Zhang", "Jian Shao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/InftyThink Code:\n  https://github.com/ZJU-REAL/InftyThink", "summary": "Advanced reasoning in large language models has achieved remarkable\nperformance on challenging tasks, but the prevailing long-context reasoning\nparadigm faces critical limitations: quadratic computational scaling with\nsequence length, reasoning constrained by maximum context boundaries, and\nperformance degradation beyond pre-training context windows. Existing\napproaches primarily compress reasoning chains without addressing the\nfundamental scaling problem. To overcome these challenges, we introduce\nInftyThink, a paradigm that transforms monolithic reasoning into an iterative\nprocess with intermediate summarization. By interleaving short reasoning\nsegments with concise progress summaries, our approach enables unbounded\nreasoning depth while maintaining bounded computational costs. This creates a\ncharacteristic sawtooth memory pattern that significantly reduces computational\ncomplexity compared to traditional approaches. Furthermore, we develop a\nmethodology for reconstructing long-context reasoning datasets into our\niterative format, transforming OpenR1-Math into 333K training instances.\nExperiments across multiple model architectures demonstrate that our approach\nreduces computational costs while improving performance, with Qwen2.5-Math-7B\nshowing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks.\nOur work challenges the assumed trade-off between reasoning depth and\ncomputational efficiency, providing a more scalable approach to complex\nreasoning without architectural modifications."}
{"id": "2503.09600", "pdf": "https://arxiv.org/pdf/2503.09600.pdf", "abs": "https://arxiv.org/abs/2503.09600", "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System", "authors": ["Jihao Zhao", "Zhiyuan Ji", "Zhaoxin Fan", "Hanyu Wang", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system."}
{"id": "2503.10497", "pdf": "https://arxiv.org/pdf/2503.10497.pdf", "abs": "https://arxiv.org/abs/2503.10497", "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation", "authors": ["Weihao Xuan", "Rui Yang", "Heli Qi", "Qingcheng Zeng", "Yunze Xiao", "Aosong Feng", "Dairui Liu", "Yun Xing", "Junjue Wang", "Fan Gao", "Jinghui Lu", "Yuang Jiang", "Huitao Li", "Xin Li", "Kunyu Yu", "Ruihai Dong", "Shangding Gu", "Yuekang Li", "Xiaofei Xie", "Felix Juefei-Xu", "Foutse Khomh", "Osamu Yoshie", "Qingyu Chen", "Douglas Teodoro", "Nan Liu", "Randy Goebel", "Lei Ma", "Edison Marrese-Taylor", "Shijian Lu", "Yusuke Iwasawa", "Yutaka Matsuo", "Irene Li"], "categories": ["cs.CL"], "comment": null, "summary": "Existing large language model (LLM) evaluation benchmarks primarily focus on\nEnglish, while current multilingual tasks lack parallel questions that\nspecifically assess cross-linguistic reasoning abilities. This dual limitation\nmakes it challenging to comprehensively assess LLMs' performance in the\nmultilingual setting. To fill this gap, we introduce MMLU-ProX, a comprehensive\nbenchmark covering 29 languages, built on an English benchmark. Each language\nversion consists of 11,829 identical questions, enabling direct\ncross-linguistic comparisons. Additionally, to meet efficient evaluation needs,\nwe provide a lite version containing 658 questions per language. To ensure the\nhigh quality of MMLU-ProX, we employ a rigorous development process that\ninvolves multiple powerful LLMs for translation, followed by expert review to\nensure accurate expression, consistent terminology, and cultural relevance.\nBuilding on this, we systematically evaluate 36 state-of-the-art LLMs,\nincluding reasoning-enhanced and multilingual-optimized LLMs. The results\nreveal significant disparities in the multilingual capabilities of LLMs: While\nthey perform well in high-resource languages, their performance declines\nmarkedly in low-resource languages, with gaps of up to 24.3%. Through\nMMLU-ProX, we aim to advance the development of more inclusive AI systems and\npromote equitable access to technology across global contexts."}
{"id": "2503.10688", "pdf": "https://arxiv.org/pdf/2503.10688.pdf", "abs": "https://arxiv.org/abs/2503.10688", "title": "CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding", "authors": ["Tadesse Destaw Belay", "Ahmed Haj Ahmed", "Alvin Grissom II", "Iqra Ameer", "Grigori Sidorov", "Olga Kolesnikova", "Seid Muhie Yimam"], "categories": ["cs.CL"], "comment": "ACL-main 2025", "summary": "NLP research has increasingly focused on subjective tasks such as emotion\nanalysis. However, existing emotion benchmarks suffer from two major\nshortcomings: (1) they largely rely on keyword-based emotion recognition,\noverlooking crucial cultural dimensions required for deeper emotion\nunderstanding, and (2) many are created by translating English-annotated data\ninto other languages, leading to potentially unreliable evaluation. To address\nthese issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first\nbenchmark designed to evaluate culture-aware emotion prediction across six\nlanguages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo\ncomprises 400 crafted questions per language, each requiring nuanced cultural\nreasoning and understanding. We use this benchmark to evaluate several\nstate-of-the-art LLMs on culture-aware emotion prediction and sentiment\nanalysis tasks. Our findings reveal that (1) emotion conceptualizations vary\nsignificantly across languages and cultures, (2) LLMs performance likewise\nvaries by language and cultural context, and (3) prompting in English with\nexplicit country context often outperforms in-language prompts for\nculture-aware emotion and sentiment understanding. The dataset and evaluation\ncode are publicly available."}
{"id": "2503.11299", "pdf": "https://arxiv.org/pdf/2503.11299.pdf", "abs": "https://arxiv.org/abs/2503.11299", "title": "BriLLM: Brain-inspired Large Language Model", "authors": ["Hai Zhao", "Hongqiu Wu", "Dongjie Yang", "Anni Zou", "Jiale Hong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper reports the first brain-inspired large language model (BriLLM).\nThis is a non-Transformer, non-GPT, non-traditional machine learning\ninput-output controlled generative language model. The model is based on the\nSignal Fully-connected flowing (SiFu) definition on the directed graph in terms\nof the neural network, and has the interpretability of all nodes on the graph\nof the whole model, instead of the traditional machine learning model that only\nhas limited interpretability at the input and output ends. In the language\nmodel scenario, the token is defined as a node in the graph. A randomly shaped\nor user-defined signal flow flows between nodes on the principle of \"least\nresistance\" along paths. The next token or node to be predicted or generated is\nthe target of the signal flow. As a language model, BriLLM theoretically\nsupports infinitely long $n$-gram models when the model size is independent of\nthe input and predicted length of the model. The model's working signal flow\nprovides the possibility of recall activation and innate multi-modal support\nsimilar to the cognitive patterns of the human brain. At present, we released\nthe first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node\nwidth, 16-token long sequence prediction ability, and language model prediction\nperformance comparable to GPT-1. More computing power will help us explore the\ninfinite possibilities depicted above."}
{"id": "2503.12345", "pdf": "https://arxiv.org/pdf/2503.12345.pdf", "abs": "https://arxiv.org/abs/2503.12345", "title": "General Table Question Answering via Answer-Formula Joint Generation", "authors": ["Zhongyuan Wang", "Richong Zhang", "Zhijie Nie"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperations, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nthe Formula as the executable representation for solving complex reasoning on\ntables with different structures. Specifically, we construct\n\\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing\ndatasets. In addition, we propose \\texttt{TabAF}, a general table answering\nframework to solve multiple types of tasks over multiple types of tables\nsimultaneously. Unlike existing methods, \\texttt{TabAF} decodes answers and\nFormulas with a single LLM backbone, demonstrating great versatility and\ngeneralization. \\texttt{TabAF} based on Llama3.1-70B achieves new\nstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact."}
{"id": "2503.12759", "pdf": "https://arxiv.org/pdf/2503.12759.pdf", "abs": "https://arxiv.org/abs/2503.12759", "title": "RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning", "authors": ["Jerry Huang", "Siddarth Madala", "Risham Sidhu", "Cheng Niu", "Hao Peng", "Julia Hockenmaier", "Tong Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems rely on retrieval models for\nidentifying relevant contexts and answer generation models for utilizing those\ncontexts. However, retrievers exhibit imperfect recall and precision, limiting\ndownstream performance. We introduce RAG-RL, an answer generation model trained\nnot only to produce answers but also to identify and cite relevant information\nfrom larger sets of retrieved contexts, shifting some of the burden of\nidentifying relevant documents from the retriever to the answer generator. Our\napproach uses curriculum learning, where the model is first trained on easier\nexamples that include only relevant contexts. Our experiments show that these\ntraining samples enable models to acquire citation and reasoning skills with\ngreater sample efficiency and generalizability, demonstrating strong model\nperformance even as the number of irrelevant passages increases. We benchmark\nour methods on three open-domain multi-hop question answering datasets and\nreport significant gains in answer and citation accuracy. Our experiments\nprovide empirical insights into how easier training samples can give models\nstronger signals for learning specific skills (e.g., citation generation) and\nhow different components of post-training (e.g., training set construction,\nrule-based rewards, training sample ordering, etc.) impact final model\nperformance."}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354.pdf", "abs": "https://arxiv.org/abs/2503.15354", "title": "Optimizing Decomposition for Optimal Claim Verification", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims."}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287.pdf", "abs": "https://arxiv.org/abs/2503.17287", "title": "FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "categories": ["cs.CL"], "comment": "Ongoing Work", "summary": "Improving training efficiency continues to be one of the primary challenges\nin large-scale Reinforcement Learning (RL). In this paper, we investigate how\ncontext length and the complexity of training data influence the RL scaling\ntraining process of R1-distilled small reasoning models, e.g.,\nDeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply\ncontrolling the context length and curating the training data based on the\ninput prompt length can effectively improve the training efficiency of scaling\nRL, achieving better performance with more concise CoT; (2) properly scaling\nthe context length helps mitigate entropy collapse; and (3) choosing an optimal\ncontext length can improve the efficiency of model training and incentivize the\nmodel's chain-of-thought reasoning capabilities. Inspired by these insights, we\npropose FastCuRL, a curriculum RL framework with stage-wise context scaling to\nachieve efficient training and concise CoT reasoning. Experiment results\ndemonstrate that FastCuRL-1.5B-V3 significantly outperforms state-of-the-art\nreasoning models on five competition-level benchmarks and achieves 49.6\\%\naccuracy on AIME 2024. Furthermore, FastCuRL-1.5B-Preview surpasses\nDeepScaleR-1.5B-Preview on five benchmarks while only using a single node with\n8 GPUs and a total of 50\\% of training steps. %The code, training data, and\nmodels will be publicly released."}
{"id": "2503.20749", "pdf": "https://arxiv.org/pdf/2503.20749.pdf", "abs": "https://arxiv.org/abs/2503.20749", "title": "Prompting is Not All You Need! Evaluating LLM Agent Simulation Methodologies with Real-World Online Customer Behavior Data", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bingsheng Yao", "Sisong Bei", "Jiri Gesi", "Yaochen Xie", "Zheshen", "Wang", "Qi He", "Dakuo Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating LLM's objective ``accuracy'' rather than the subjective\n``believability'' in simulating human behavior, leveraging a large-scale,\nreal-world dataset collected from customers' online shopping actions. We\npresent the first comprehensive evaluation of state-of-the-art LLMs (e.g.,\nDeepSeek-R1, Llama, and Claude) on the task of web shopping action generation.\nOur results show that out-of-the-box LLM-generated actions are often misaligned\nwith actual human behavior, whereas fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate accurate actions compared\nto prompt-only methods. Furthermore, incorporating synthesized reasonings into\nmodel training leads to additional performance gains, demonstrating the value\nof explicit rationale in behavior modeling. This work evaluates\nstate-of-the-art LLMs in behavior simulation and provides actionable insights\ninto how real-world action data can enhance the fidelity of LLM agents."}
{"id": "2504.01346", "pdf": "https://arxiv.org/pdf/2504.01346.pdf", "abs": "https://arxiv.org/abs/2504.01346", "title": "GTR: Graph-Table-RAG for Cross-Table Question Answering", "authors": ["Jiaru Zou", "Dongqi Fu", "Sirui Chen", "Xinrui He", "Zihao Li", "Yada Zhu", "Jiawei Han", "Jingrui He"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "20 pages, 7 figures", "summary": "Beyond pure text, a substantial amount of knowledge is stored in tables. In\nreal-world scenarios, user questions often require retrieving answers that are\ndistributed across multiple tables. GraphRAG has recently attracted much\nattention for enhancing LLMs' reasoning capabilities by organizing external\nknowledge to address ad-hoc and complex questions, exemplifying a promising\ndirection for cross-table question answering. In this paper, to address the\ncurrent gap in available data, we first introduce a multi-table benchmark,\nMutliTableQA, comprising 60k tables and 25k user queries collected from\nreal-world sources. Then, we propose the first Graph-Table-RAG framework,\nnamely GTR, which reorganizes table corpora into a heterogeneous graph, employs\na hierarchical coarse-to-fine retrieval process to extract the most relevant\ntables, and integrates graph-aware prompting for downstream LLMs' tabular\nreasoning. Extensive experiments show that GTR exhibits superior cross-table\nquestion-answering performance while maintaining high deployment efficiency,\ndemonstrating its real-world practical applicability."}
{"id": "2504.04050", "pdf": "https://arxiv.org/pdf/2504.04050.pdf", "abs": "https://arxiv.org/abs/2504.04050", "title": "FISH-Tuning: Enhancing PEFT Methods with Fisher Information", "authors": ["Kang Xue", "Ming Dong", "Xinhui Tu", "Tingting He"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth in the parameter size of Large Language Models (LLMs) has\nspurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods to\nmitigate the substantial computational costs of fine-tuning. Among these,\nFisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT\ntechnique that identifies a critical subset of pre-trained parameters using\napproximate Fisher information. While addition-based and\nreparameterization-based PEFT methods like LoRA and Adapter already fine-tune\nonly a small number of parameters, the newly introduced parameters within these\nmethods themselves present an opportunity for further optimization. Selectively\nfine-tuning only the most impactful among these new parameters could further\nreduce resource consumption while maintaining, or even improving, fine-tuning\neffectiveness. In this paper, we propose \\textbf{FISH-Tuning}, a novel approach\nthat incorporates FISH Mask into such PEFT methods, including LoRA, Adapter,\nand their variants. By leveraging Fisher information to identify and update\nonly the most significant parameters within these added or reparameterized\ncomponents, FISH-Tuning aims to achieve superior performance without increasing\ntraining time or inference latency compared to the vanilla PEFT methods.\nExperimental results across various datasets and pre-trained models demonstrate\nthat FISH-Tuning consistently outperforms the vanilla PEFT methods when using\nthe same proportion of trainable parameters. Code is available at\nhttps://anonymous.4open.science/r/FISH-Tuning-6F7C."}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050.pdf", "abs": "https://arxiv.org/abs/2504.05050", "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities."}
{"id": "2504.05228", "pdf": "https://arxiv.org/pdf/2504.05228.pdf", "abs": "https://arxiv.org/abs/2504.05228", "title": "NoveltyBench: Evaluating Language Models for Humanlike Diversity", "authors": ["Yiming Zhang", "Harshita Diddee", "Susan Holm", "Hanchen Liu", "Xinyue Liu", "Vinay Samuel", "Barry Wang", "Daphne Ippolito"], "categories": ["cs.CL"], "comment": null, "summary": "Language models have demonstrated remarkable capabilities on standard\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\nbenchmark specifically designed to evaluate the ability of language models to\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\nprompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art\nsystems generate significantly less diversity than human writers. Notably,\nlarger models within a family often exhibit less diversity than their smaller\ncounterparts, challenging the notion that capability on standard benchmarks\ntranslates directly to generative utility. While prompting strategies like\nin-context regeneration can elicit diversity, our findings highlight a\nfundamental lack of distributional diversity in current models, reducing their\nutility for users seeking varied responses and suggesting the need for new\ntraining and evaluation paradigms that prioritize diversity alongside quality."}
{"id": "2504.07440", "pdf": "https://arxiv.org/pdf/2504.07440.pdf", "abs": "https://arxiv.org/abs/2504.07440", "title": "Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric", "authors": ["Yixin Cao", "Jiahao Ying", "Yaoning Wang", "Xipeng Qiu", "Xuanjing Huang", "Yugang Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. One core challenge of evaluation in the\nlarge language model (LLM) era is the generalization issue: how to infer a\nmodel's near-unbounded abilities from inevitably bounded benchmarks. We address\nthis challenge by proposing Model Utilization Index (MUI), a mechanism\ninterpretability enhanced metric that complements traditional performance\nscores. MUI quantifies the effort a model expends on a task, defined as the\nproportion of activated neurons or features during inference. Intuitively, a\ntruly capable model should achieve higher performance with lower effort.\nExtensive experiments across popular LLMs reveal a consistent inverse\nlogarithmic relationship between MUI and performance, which we formulate as the\nUtility Law. From this law we derive four practical corollaries that (i) guide\ntraining diagnostics, (ii) expose data contamination issue, (iii) enable fairer\nmodel comparisons, and (iv) design model-specific dataset diversity. Our code\ncan be found at https://github.com/ALEX-nlp/MUI-Eva."}
{"id": "2504.07830", "pdf": "https://arxiv.org/pdf/2504.07830.pdf", "abs": "https://arxiv.org/abs/2504.07830", "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations", "authors": ["Genglin Liu", "Vivian Le", "Salman Rahman", "Elisa Kreiss", "Marzyeh Ghassemi", "Saadia Gabriel"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Work in progress. 27 pages", "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences."}
{"id": "2504.09866", "pdf": "https://arxiv.org/pdf/2504.09866.pdf", "abs": "https://arxiv.org/abs/2504.09866", "title": "PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims", "authors": ["Ziyu Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "Automated fact-checking (AFC) still falters on claims that are\ntime-sensitive, entity-ambiguous, or buried beneath noisy search-engine\nresults. We present PASS-FC, a Progressive and Adaptive Search Scheme for Fact\nChecking. Each atomic claim is first grounded with a precise time span and\ndisambiguated entity descriptors. An adaptive search loop then issues\nstructured queries, filters domains through credible-source selection, and\nexpands queries cross-lingually; when necessary, a lightweight reflection\nroutine restarts the loop. Experiments on six benchmark--covering general\nknowledge, scientific literature, real-world events, and ten languages--show\nthat PASS-FC consistently outperforms prior systems, even those powered by\nlarger backbone LLMs. On the multilingual X-FACT set, performance of different\nlanguages partially correlates with typological closeness to English, and\nforcing the model to reason in low-resource languages degrades accuracy.\nAblations highlight the importance of temporal grounding and the adaptive\nsearch scheme, while detailed analysis shows that cross-lingual retrieval\ncontributes genuinely new evidence. Code and full results will be released to\nfacilitate further research."}
{"id": "2504.11442", "pdf": "https://arxiv.org/pdf/2504.11442.pdf", "abs": "https://arxiv.org/abs/2504.11442", "title": "TextArena", "authors": ["Leon Guertler", "Bobby Cheng", "Simon Yu", "Bo Liu", "Leshem Choshen", "Cheston Tan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Work in progress; 5 pages, 3 figures", "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/."}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366.pdf", "abs": "https://arxiv.org/abs/2504.14366", "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs), in which a smaller student model is trained to mimic a\nlarger teacher model. Typically, both the teacher and student models are\nTransformer-based architectures, leveraging softmax attention for sequence\nmodeling. However, the quadratic complexity of self-attention during inference\nremains a significant bottleneck, motivating the exploration of subquadratic\nalternatives such as structured state-space models (SSMs), linear attention,\nand recurrent architectures. In this work, we systematically evaluate the\ntransferability of knowledge distillation from a Transformer teacher model to\neight subquadratic student architectures. Our study investigates which\nsubquadratic model can most effectively approximate the teacher model's learned\nrepresentations through knowledge distillation, and how different architectural\ndesign choices influence the training dynamics. We further investigate the\nimpact of initialization strategies, such as matrix mixing and query-key-value\n(QKV) copying, on the adaptation process. Our empirical results on multiple NLP\nbenchmarks provide insights into the trade-offs between efficiency and\nperformance, highlighting key factors for successful knowledge transfer to\nsubquadratic architectures."}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704.pdf", "abs": "https://arxiv.org/abs/2504.17704", "title": "Safety in Large Reasoning Models: A Survey", "authors": ["Cheng Wang", "Yue Liu", "Baolong Bi", "Duzhen Zhang", "Zhong-Zhi Li", "Yingwei Ma", "Yufei He", "Shengju Yu", "Xinfeng Li", "Junfeng Fang", "Jiaheng Zhang", "Bryan Hooi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095.pdf", "abs": "https://arxiv.org/abs/2504.19095", "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "authors": ["Jikai Wang", "Juntao Li", "Jianye Hou", "Bowen Yan", "Lijun Wu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy of the target model for complex tasks. Experimental\nresults on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that\nSCoT reduces reasoning latency by 48\\%$\\sim$66\\% and 21\\%$\\sim$49\\% for\nDeepseek-R1-Distill-Qwen-32B and Deepseek-R1-Distill-Llama-70B while achieving\nnear-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339.pdf", "abs": "https://arxiv.org/abs/2504.19339", "title": "Explanatory Summarization with Discourse-Driven Planning", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL 2025)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2505.00985", "pdf": "https://arxiv.org/pdf/2505.00985.pdf", "abs": "https://arxiv.org/abs/2505.00985", "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "authors": ["Yash Goel", "Ayan Sengupta", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."}
{"id": "2505.01855", "pdf": "https://arxiv.org/pdf/2505.01855.pdf", "abs": "https://arxiv.org/abs/2505.01855", "title": "Intra-Layer Recurrence in Transformers for Language Modeling", "authors": ["Anthony Nguyen", "Wenjun Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Canadian AI 2025. Code available at\n  https://github.com/ant-8/Layer-Recurrent-Transformers", "summary": "Transformer models have established new benchmarks in natural language\nprocessing; however, their increasing depth results in substantial growth in\nparameter counts. While existing recurrent transformer methods address this\nissue by reprocessing layers multiple times, they often apply recurrence\nindiscriminately across entire blocks of layers. In this work, we investigate\nIntra-Layer Recurrence (ILR), a more targeted approach that applies recurrence\nselectively to individual layers within a single forward pass. Our experiments\nshow that allocating more iterations to earlier layers yields optimal results.\nThese findings suggest that ILR offers a promising direction for optimizing\nrecurrent structures in transformer architectures."}
{"id": "2505.02172", "pdf": "https://arxiv.org/pdf/2505.02172.pdf", "abs": "https://arxiv.org/abs/2505.02172", "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization", "authors": ["Chuck Arvin"], "categories": ["cs.CL"], "comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)", "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate scaling effects -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks."}
{"id": "2505.02518", "pdf": "https://arxiv.org/pdf/2505.02518.pdf", "abs": "https://arxiv.org/abs/2505.02518", "title": "Bemba Speech Translation: Exploring a Low-Resource African Language", "authors": ["Muhammad Hazim Al Farouq", "Aman Kassahun Wassie", "Yasmin Moslem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2025), low-resource languages track, namely\nfor Bemba-to-English speech translation. We built cascaded speech translation\nsystems based on Whisper and NLLB-200, and employed data augmentation\ntechniques, such as back-translation. We investigate the effect of using\nsynthetic data and discuss our experimental setup."}
{"id": "2505.02865", "pdf": "https://arxiv.org/pdf/2505.02865.pdf", "abs": "https://arxiv.org/abs/2505.02865", "title": "Accelerating Large Language Model Reasoning via Speculative Search", "authors": ["Zhihai Wang", "Jie Wang", "Jilai Pan", "Xilin Xia", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Feng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality."}
{"id": "2505.06149", "pdf": "https://arxiv.org/pdf/2505.06149.pdf", "abs": "https://arxiv.org/abs/2505.06149", "title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance."}
{"id": "2505.08311", "pdf": "https://arxiv.org/pdf/2505.08311.pdf", "abs": "https://arxiv.org/abs/2505.08311", "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale", "authors": ["Yunjie Ji", "Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\n\\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}."}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113.pdf", "abs": "https://arxiv.org/abs/2505.10113", "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community."}
{"id": "2505.10714", "pdf": "https://arxiv.org/pdf/2505.10714.pdf", "abs": "https://arxiv.org/abs/2505.10714", "title": "GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?", "authors": ["Bowen Jiang", "Yangxinyu Xie", "Xiaomeng Wang", "Jiashu He", "Joshua Bergerson", "John K Hutchison", "Jordan Branham", "Camillo J Taylor", "Tanwi Mallick"], "categories": ["cs.CL"], "comment": null, "summary": "We present GeoGrid-Bench, a benchmark designed to evaluate the ability of\nfoundation models to understand geo-spatial data in the grid structure.\nGeo-spatial datasets pose distinct challenges due to their dense numerical\nvalues, strong spatial and temporal dependencies, and unique multimodal\nrepresentations including tabular data, heatmaps, and geographic\nvisualizations. To assess how foundation models can support scientific research\nin this domain, GeoGrid-Bench features large-scale, real-world data covering 16\nclimate variables across 150 locations and extended time frames. The benchmark\nincludes approximately 3,200 question-answer pairs, systematically generated\nfrom 8 domain expert-curated templates to reflect practical tasks encountered\nby human scientists. These range from basic queries at a single location and\ntime to complex spatiotemporal comparisons across regions and periods. Our\nevaluation reveals that vision-language models perform best overall, and we\nprovide a fine-grained analysis of the strengths and limitations of different\nfoundation models in different geo-spatial tasks. This benchmark offers clearer\ninsights into how foundation models can be effectively applied to geo-spatial\ndata analysis and used to support scientific research."}
{"id": "2505.10924", "pdf": "https://arxiv.org/pdf/2505.10924.pdf", "abs": "https://arxiv.org/abs/2505.10924", "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?", "authors": ["Ada Chen", "Yongjiang Wu", "Junyuan Zhang", "Jingyu Xiao", "Shu Yang", "Jen-tse Huang", "Kun Wang", "Wenxuan Wang", "Shuai Wang"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.SE"], "comment": null, "summary": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents."}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441.pdf", "abs": "https://arxiv.org/abs/2505.11441", "title": "Is Compression Really Linear with Code Intelligence?", "authors": ["Xianzhen Luo", "Shijie Xuyang", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain."}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604.pdf", "abs": "https://arxiv.org/abs/2505.11604", "title": "Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Soyoung Yang", "Jaehyeok Jang", "Jaegul Choo"], "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Editing presentation slides remains one of the most common and time-consuming\ntasks faced by millions of users daily, despite significant advances in\nautomated slide generation. Existing approaches have successfully demonstrated\nslide editing via graphic user interface (GUI)-based agents, offering intuitive\nvisual control. However, such methods often suffer from high computational cost\nand latency. In this paper, we propose Talk-to-Your-Slides, an LLM-powered\nagent designed to edit slides %in active PowerPoint sessions by leveraging\nstructured information about slide objects rather than relying on image\nmodality. The key insight of our work is designing the editing process with\ndistinct high-level and low-level layers to facilitate interaction between user\ncommands and slide objects. By providing direct access to application objects\nrather than screen pixels, our system enables 34.02% faster processing, 34.76%\nbetter instruction fidelity, and 87.42% cheaper operation than baselines. To\nevaluate slide editing capabilities, we introduce TSBench, a human-annotated\ndataset comprising 379 diverse editing instructions paired with corresponding\nslide variations in four categories. Our code, benchmark and demos are\navailable at https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827.pdf", "abs": "https://arxiv.org/abs/2505.11827", "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/usail-hkust/LongShort."}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891.pdf", "abs": "https://arxiv.org/abs/2505.11891", "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216.pdf", "abs": "https://arxiv.org/abs/2505.12216", "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "authors": ["Rongguang Ye", "Ming Tang"], "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines."}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392.pdf", "abs": "https://arxiv.org/abs/2505.12392", "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388.pdf", "abs": "https://arxiv.org/abs/2505.13388", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.14106", "pdf": "https://arxiv.org/pdf/2505.14106.pdf", "abs": "https://arxiv.org/abs/2505.14106", "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses."}
{"id": "2505.14107", "pdf": "https://arxiv.org/pdf/2505.14107.pdf", "abs": "https://arxiv.org/abs/2505.14107", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Jiaji Liu", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena."}
{"id": "2505.14272", "pdf": "https://arxiv.org/pdf/2505.14272.pdf", "abs": "https://arxiv.org/abs/2505.14272", "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages."}
{"id": "2505.14499", "pdf": "https://arxiv.org/pdf/2505.14499.pdf", "abs": "https://arxiv.org/abs/2505.14499", "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 2 figures, 6 tables. Accepted by ICONIP2024", "summary": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA."}
{"id": "2505.14617", "pdf": "https://arxiv.org/pdf/2505.14617.pdf", "abs": "https://arxiv.org/abs/2505.14617", "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation."}
{"id": "2505.14810", "pdf": "https://arxiv.org/pdf/2505.14810.pdf", "abs": "https://arxiv.org/abs/2505.14810", "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models", "authors": ["Tingchen Fu", "Jiawei Gu", "Yafu Li", "Xiaoye Qu", "Yu Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF."}
{"id": "2505.14996", "pdf": "https://arxiv.org/pdf/2505.14996.pdf", "abs": "https://arxiv.org/abs/2505.14996", "title": "MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision", "authors": ["Zixuan Ke", "Austin Xu", "Yifei Ming", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) leveraging the impressive capabilities of Large\nLanguage Models (LLMs) hold significant potential for tackling complex tasks.\nHowever, most current MAS depend on manually designed agent roles and\ncommunication protocols. These manual designs often fail to align with the\nunderlying LLMs' strengths and struggle to adapt to novel tasks. Recent\nautomatic MAS approaches attempt to mitigate these limitations but typically\nnecessitate a validation set for tuning and yield static MAS designs lacking\nadaptability during inference. We introduce MAS-ZERO, the first self-evolved,\ninference-time framework for automatic MAS design. MAS-ZERO employs meta-level\ndesign to iteratively generate, evaluate, and refine MAS configurations\ntailored to each problem instance, without requiring a validation set.\nCritically, it enables dynamic agent composition and problem decomposition\nthrough meta-feedback on solvability and completeness. Experiments across math,\ngraduate-level QA, and software engineering benchmarks, using both\nclosed-source and open-source LLM backbones of varying sizes, demonstrate that\nMAS-ZERO outperforms both manual and automatic MAS baselines, achieving a 7.44%\naverage accuracy improvement over the next strongest baseline while maintaining\ncost-efficiency. These findings underscore the promise of meta-level\nself-evolved design for creating effective and adaptive MAS."}
{"id": "2505.15062", "pdf": "https://arxiv.org/pdf/2505.15062.pdf", "abs": "https://arxiv.org/abs/2505.15062", "title": "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning", "authors": ["Jiashu He", "Jinxuan Fan", "Bowen Jiang", "Ignacio Houine", "Dan Roth", "Alejandro Ribeiro"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When addressing complex questions that require new information, people often\nassociate the question with existing knowledge to derive a sensible answer. For\ninstance, when evaluating whether melatonin aids insomnia, one might associate\n\"hormones helping mental disorders\" with \"melatonin being a hormone and\ninsomnia a mental disorder\" to complete the reasoning. Large Language Models\n(LLMs) also require such associative thinking, particularly in resolving\nscientific inquiries when retrieved knowledge is insufficient and does not\ndirectly answer the question. Graph Inspired Veracity Extrapolation (GIVE)\naddresses this by using a knowledge graph (KG) to extrapolate structured\nknowledge. However, it involves the construction and pruning of many\nhypothetical triplets, which limits efficiency and generalizability. We propose\nSelf-GIVE, a retrieve-RL framework that enhances LLMs with automatic\nassociative thinking through reinforcement learning. Self-GIVE extracts\nstructured information and entity sets to assist the model in linking to the\nqueried concepts. We address GIVE's key limitations: (1) extensive LLM calls\nand token overhead for knowledge extrapolation, (2) difficulty in deploying on\nsmaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate\nknowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE\nwith a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B\nmodels by up to $\\textbf{28.5%$\\rightarrow$71.4%}$ and\n$\\textbf{78.6$\\rightarrow$90.5%}$ in samples $\\textbf{unseen}$ in challenging\nbiomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or\noutperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\\%.\nSelf-GIVE enhances the scalable integration of structured retrieval and\nreasoning with associative thinking."}
{"id": "2505.15107", "pdf": "https://arxiv.org/pdf/2505.15107.pdf", "abs": "https://arxiv.org/abs/2505.15107", "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization", "authors": ["Ziliang Wang", "Xuhui Zheng", "Kang An", "Cijun Ouyang", "Jialu Cai", "Yuhang Wang", "Yichao Wu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "20 pages, 6 figures", "summary": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our code\nwill be released on https://github.com/Zillwang/StepSearch."}
{"id": "2505.15337", "pdf": "https://arxiv.org/pdf/2505.15337.pdf", "abs": "https://arxiv.org/abs/2505.15337", "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors", "authors": ["Hao Fang", "Jiawei Kong", "Tianqu Zhuang", "Yixiang Qiu", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios."}
{"id": "2505.15634", "pdf": "https://arxiv.org/pdf/2505.15634.pdf", "abs": "https://arxiv.org/abs/2505.15634", "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models", "authors": ["Zihao Li", "Xu Wang", "Yuzhe Yang", "Ziyu Yao", "Haoyi Xiong", "Mengnan Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs."}
{"id": "2505.15692", "pdf": "https://arxiv.org/pdf/2505.15692.pdf", "abs": "https://arxiv.org/abs/2505.15692", "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities", "authors": ["Jinyang Wu", "Chonghua Liao", "Mingkuan Feng", "Shuai Zhang", "Zhengqi Wen", "Pengpeng Shao", "Huazhe Xu", "Jianhua Tao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability."}
{"id": "2505.15700", "pdf": "https://arxiv.org/pdf/2505.15700.pdf", "abs": "https://arxiv.org/abs/2505.15700", "title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "authors": ["Alkis Koudounas", "Claudio Savelli", "Flavio Giobergia", "Elena Baralis"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques."}
{"id": "2505.15801", "pdf": "https://arxiv.org/pdf/2505.15801.pdf", "abs": "https://arxiv.org/abs/2505.15801", "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models", "authors": ["Yuchen Yan", "Jin Jiang", "Zhenbang Ren", "Yijun Li", "Xudong Cai", "Yang Liu", "Xin Xu", "Mengdi Zhang", "Jian Shao", "Yongliang Shen", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/VerifyBench Dataset:\n  https://huggingface.co/datasets/ZJU-REAL/VerifyBench Code:\n  https://github.com/ZJU-REAL/VerifyBench", "summary": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks."}
{"id": "2505.16000", "pdf": "https://arxiv.org/pdf/2505.16000.pdf", "abs": "https://arxiv.org/abs/2505.16000", "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model", "authors": ["Mehrdad Ghassabi", "Pedram Rostami", "Hamidreza Baradaran Kashani", "Amirhossein Poursina", "Zahra Kazemi", "Milad Tavakoli"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments."}
{"id": "2505.16014", "pdf": "https://arxiv.org/pdf/2505.16014.pdf", "abs": "https://arxiv.org/abs/2505.16014", "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "authors": ["Yash Saxena", "Ankur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on\nsimilarity-based retrieval and re-ranking, which depend on heuristics such as\ntop-k, and lack explainability, interpretability, and robustness against\nadversarial content. To address this gap, we propose a novel method METEORA\nthat replaces re-ranking in RAG with a rationale-driven selection approach.\nMETEORA operates in two stages. First, a general-purpose LLM is\npreference-tuned to generate rationales conditioned on the input query using\ndirect preference optimization. These rationales guide the evidence chunk\nselection engine, which selects relevant chunks in three stages: pairing\nindividual rationales with corresponding retrieved chunks for local relevance,\nglobal selection with elbow detection for adaptive cutoff, and context\nexpansion via neighboring chunks. This process eliminates the need for top-k\nheuristics. The rationales are also used for consistency check using a Verifier\nLLM to detect and filter poisoned or misleading content for safe generation.\nThe framework provides explainable and interpretable evidence flow by using\nrationales consistently across both selection and verification. Our evaluation\nacross six datasets spanning legal, financial, and academic research domains\nshows that METEORA improves generation accuracy by 33.34% while using\napproximately 50% fewer chunks than state-of-the-art re-ranking methods. In\nadversarial settings, METEORA significantly improves the F1 score from 0.10 to\n0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating\nstrong resilience to poisoning attacks. Code available at:\nhttps://anonymous.4open.science/r/METEORA-DC46/README.md"}
{"id": "2505.16088", "pdf": "https://arxiv.org/pdf/2505.16088.pdf", "abs": "https://arxiv.org/abs/2505.16088", "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future time\nperiods; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday). Our datasets and code are made publicly available\n\\href{https://github.com/gagan3012/date-fragments}{here}."}
{"id": "2505.16102", "pdf": "https://arxiv.org/pdf/2505.16102.pdf", "abs": "https://arxiv.org/abs/2505.16102", "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "categories": ["cs.CL"], "comment": "Data and Code available at https://github.com/yashkumaratri/bRAGgen", "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses."}
{"id": "2505.16128", "pdf": "https://arxiv.org/pdf/2505.16128.pdf", "abs": "https://arxiv.org/abs/2505.16128", "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings."}
{"id": "2505.16212", "pdf": "https://arxiv.org/pdf/2505.16212.pdf", "abs": "https://arxiv.org/abs/2505.16212", "title": "Large Language Models based ASR Error Correction for Child Conversations", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs."}
{"id": "2505.16241", "pdf": "https://arxiv.org/pdf/2505.16241.pdf", "abs": "https://arxiv.org/abs/2505.16241", "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content."}
{"id": "2505.16245", "pdf": "https://arxiv.org/pdf/2505.16245.pdf", "abs": "https://arxiv.org/abs/2505.16245", "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "categories": ["cs.CL"], "comment": null, "summary": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs."}
{"id": "2505.16514", "pdf": "https://arxiv.org/pdf/2505.16514.pdf", "abs": "https://arxiv.org/abs/2505.16514", "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL"], "comment": "15 pages, 4 figures", "summary": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making."}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520.pdf", "abs": "https://arxiv.org/abs/2505.16520", "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation."}
{"id": "2505.16582", "pdf": "https://arxiv.org/pdf/2505.16582.pdf", "abs": "https://arxiv.org/abs/2505.16582", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xinyu Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 figures", "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."}
{"id": "2505.16834", "pdf": "https://arxiv.org/pdf/2505.16834.pdf", "abs": "https://arxiv.org/abs/2505.16834", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067.pdf", "abs": "https://arxiv.org/abs/2505.17067", "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection."}
{"id": "2505.17123", "pdf": "https://arxiv.org/pdf/2505.17123.pdf", "abs": "https://arxiv.org/abs/2505.17123", "title": "MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation", "authors": ["Xiaoyuan Li", "Keqin Bao", "Yubo Ma", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Recent advances in Large Language Models (LLMs) have shown promising results\nin complex reasoning tasks. However, current evaluations predominantly focus on\nsingle-turn reasoning scenarios, leaving interactive tasks largely unexplored.\nWe attribute it to the absence of comprehensive datasets and scalable automatic\nevaluation protocols. To fill these gaps, we present MTR-Bench for LLMs'\nMulti-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600\ninstances, MTR-Bench covers diverse reasoning capabilities, fine-grained\ndifficulty granularity, and necessitates multi-turn interactions with the\nenvironments. Moreover, MTR-Bench features fully-automated framework spanning\nboth dataset constructions and model evaluations, which enables scalable\nassessment without human interventions. Extensive experiments reveal that even\nthe cutting-edge reasoning models fall short of multi-turn, interactive\nreasoning tasks. And the further analysis upon these results brings valuable\ninsights for future research in interactive AI systems."}
{"id": "2505.17135", "pdf": "https://arxiv.org/pdf/2505.17135.pdf", "abs": "https://arxiv.org/abs/2505.17135", "title": "When can isotropy help adapt LLMs' next word prediction to numerical domains?", "authors": ["Rashed Shelim", "Shengzhe Xu", "Walid Saad", "Naren Ramakrishnan"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown that vector representations of contextual\nembeddings learned by pre-trained large language models (LLMs) are effective in\nvarious downstream tasks in numerical domains. Despite their significant\nbenefits, the tendency of LLMs to hallucinate in such domains can have severe\nconsequences in applications such as energy, nature, finance, healthcare,\nretail and transportation, among others. To guarantee prediction reliability\nand accuracy in numerical domains, it is necessary to open the black-box and\nprovide performance guarantees through explanation. However, there is little\ntheoretical understanding of when pre-trained language models help solve\nnumeric downstream tasks. This paper seeks to bridge this gap by understanding\nwhen the next-word prediction capability of LLMs can be adapted to numerical\ndomains through a novel analysis based on the concept of isotropy in the\ncontextual embedding space. Specifically, we consider a log-linear model for\nLLMs in which numeric data can be predicted from its context through a network\nwith softmax in the output layer of LLMs (i.e., language model head in\nself-attention). We demonstrate that, in order to achieve state-of-the-art\nperformance in numerical domains, the hidden representations of the LLM\nembeddings must possess a structure that accounts for the shift-invariance of\nthe softmax function. By formulating a gradient structure of self-attention in\npre-trained models, we show how the isotropic property of LLM embeddings in\ncontextual embedding space preserves the underlying structure of\nrepresentations, thereby resolving the shift-invariance problem and providing a\nperformance guarantee. Experiments show that different characteristics of\nnumeric data and model architecture could have different impacts on isotropy."}
{"id": "2505.17362", "pdf": "https://arxiv.org/pdf/2505.17362.pdf", "abs": "https://arxiv.org/abs/2505.17362", "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit", "authors": ["Zafarullah Mahmood", "Soliman Ali", "Jiading Zhu", "Mohamed Abdelwahab", "Michelle Yu Collins", "Sihan Chen", "Yi Cheng Zhao", "Jodi Wolff", "Osnat Melamed", "Nadia Minian", "Marta Maslej", "Carolynne Cooper", "Matt Ratto", "Peter Selby", "Jonathan Rose"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Findings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL), Vienna, Austria, 2025", "summary": "The conversational capabilities of Large Language Models (LLMs) suggest that\nthey may be able to perform as automated talk therapists. It is crucial to know\nif these systems would be effective and adhere to known standards. We present a\ncounsellor chatbot that focuses on motivating tobacco smokers to quit smoking.\nIt uses a state-of-the-art LLM and a widely applied therapeutic approach called\nMotivational Interviewing (MI), and was evolved in collaboration with\nclinician-scientists with expertise in MI. We also describe and validate an\nautomated assessment of both the chatbot's adherence to MI and client\nresponses. The chatbot was tested on 106 participants, and their confidence\nthat they could succeed in quitting smoking was measured before the\nconversation and one week later. Participants' confidence increased by an\naverage of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed\nadherence to MI standards in 98% of utterances, higher than human counsellors.\nThe chatbot scored well on a participant-reported metric of perceived empathy\nbut lower than typical human counsellors. Furthermore, participants' language\nindicated a good level of motivation to change, a key goal in MI. These results\nsuggest that the automation of talk therapy with a modern LLM has promise."}
{"id": "2505.17399", "pdf": "https://arxiv.org/pdf/2505.17399.pdf", "abs": "https://arxiv.org/abs/2505.17399", "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow", "authors": ["Haoyu Sun", "Huichen Will Wang", "Jiawei Gu", "Linjie Li", "Yu Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) \\textbf{across the full front-end\ndevelopment pipeline}. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront."}
{"id": "2505.17441", "pdf": "https://arxiv.org/pdf/2505.17441.pdf", "abs": "https://arxiv.org/abs/2505.17441", "title": "Discovering Forbidden Topics in Language Models", "authors": ["Can Rager", "Chris Wendler", "Rohit Gandikota", "David Bau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, LLM-crawler, that uses token prefilling to\nfind forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawl to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems."}
{"id": "2505.18152", "pdf": "https://arxiv.org/pdf/2505.18152.pdf", "abs": "https://arxiv.org/abs/2505.18152", "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs", "authors": ["Wafa Alghallabi", "Ritesh Thawkar", "Sara Ghaboura", "Ketan More", "Omkar Thawakar", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "categories": ["cs.CL"], "comment": "Github:https://github.com/mbzuai-oryx/FannOrFlop,\n  Dataset:https://huggingface.co/datasets/omkarthawakar/FannOrFlop", "summary": "Arabic poetry is one of the richest and most culturally rooted forms of\nexpression in the Arabic language, known for its layered meanings, stylistic\ndiversity, and deep historical continuity. Although large language models\n(LLMs) have demonstrated strong performance across languages and tasks, their\nability to understand Arabic poetry remains largely unexplored. In this work,\nwe introduce \\emph{Fann or Flop}, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in 12 historical eras, covering 14 core\npoetic genres and a variety of metrical forms, from classical structures to\ncontemporary free verse. The benchmark comprises a curated corpus of poems with\nexplanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM understands classical\nArabic through Arabic poetry. Unlike surface-level tasks, this domain demands\ndeeper interpretive reasoning and cultural sensitivity. Our evaluation of\nstate-of-the-art LLMs shows that most models struggle with poetic understanding\ndespite strong results on standard Arabic benchmarks. We release \"Fann or Flop\"\nalong with the evaluation suite as an open-source resource to enable rigorous\nevaluation and advancement for Arabic language models. Code is available at:\nhttps://github.com/mbzuai-oryx/FannOrFlop."}
{"id": "2010.11123", "pdf": "https://arxiv.org/pdf/2010.11123.pdf", "abs": "https://arxiv.org/abs/2010.11123", "title": "Towards End-to-End Training of Automatic Speech Recognition for Nigerian Pidgin", "authors": ["Amina Mardiyyah Rufai", "Afolabi Abeeb", "Esther Oduntan", "Tayo Arulogun", "Oluwabukola Adegboro", "Daniel Ajisafe"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Updated empirical results, included additional architectures, and\n  added a zero-shot baseline", "summary": "The prevalence of automatic speech recognition (ASR) systems in spoken\nlanguage applications has increased significantly in recent years. Notably,\nmany African languages lack sufficient linguistic resources to support the\nrobustness of these systems. This paper focuses on the development of an\nend-to-end speech recognition system customized for Nigerian Pidgin English. We\ninvestigated and evaluated different pretrained state-of-the-art architectures\non a new dataset. Our empirical results demonstrate a notable performance of\nthe variant Wav2Vec2 XLSR-53 on our dataset, achieving a word error rate (WER)\nof 29.6% on the test set, surpassing other architectures such as NEMO QUARTZNET\nand Wav2Vec2.0 BASE-100H in quantitative assessments. Additionally, we\ndemonstrate that pretrained state-of-the-art architectures do not work well\nout-of-the-box. We performed zero-shot evaluation using XLSR-English as the\nbaseline, chosen for its similarity to Nigerian Pidgin. This yielded a higher\nWER of 73.7%. By adapting this architecture to nuances represented in our\ndataset, we reduce error by 59.84%. Our dataset comprises 4,288 recorded\nutterances from 10 native speakers, partitioned into training, validation, and\ntest sets. This study underscores the potential for improving ASR systems for\nunder-resourced languages like Nigerian Pidgin English, contributing to greater\ninclusion in speech technology applications. We publicly release our unique\nparallel dataset (speech-to-text) on Nigerian Pidgin, as well as the model\nweights on Hugging Face. Our code would be made available to foster future\nresearch from the community."}
{"id": "2402.03299", "pdf": "https://arxiv.org/pdf/2402.03299.pdf", "abs": "https://arxiv.org/abs/2402.03299", "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models", "authors": ["Haibo Jin", "Ruoxi Chen", "Peiyan Zhang", "Andy Zhou", "Yang Zhang", "Haohan Wang"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "28 papges", "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities."}
{"id": "2402.05668", "pdf": "https://arxiv.org/pdf/2402.05668.pdf", "abs": "https://arxiv.org/abs/2402.05668", "title": "JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs", "authors": ["Junjie Chu", "Yugeng Liu", "Ziqing Yang", "Xinyue Shen", "Michael Backes", "Yang Zhang"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Correct typos and update new experiment results. Accepted in ACL\n  2025. 25 pages, 12 figures", "summary": "Jailbreak attacks aim to bypass the LLMs' safeguards. While researchers have\nproposed different jailbreak attacks in depth, they have done so in isolation\n-- either with unaligned settings or comparing a limited range of methods. To\nfill this gap, we present a large-scale evaluation of various jailbreak\nattacks. We collect 17 representative jailbreak attacks, summarize their\nfeatures, and establish a novel jailbreak attack taxonomy. Then we conduct\ncomprehensive measurement and ablation studies across nine aligned LLMs on 160\nforbidden questions from 16 violation categories. Also, we test jailbreak\nattacks under eight advanced defenses. Based on our taxonomy and experiments,\nwe identify some important patterns, such as heuristic-based attacks could\nachieve high attack success rates but are easy to mitigate by defenses, causing\nlow practicality. Our study offers valuable insights for future research on\njailbreak attacks and defenses. We hope our work could help the community avoid\nincremental work and serve as an effective benchmark tool for practitioners."}
{"id": "2404.01012", "pdf": "https://arxiv.org/pdf/2404.01012.pdf", "abs": "https://arxiv.org/abs/2404.01012", "title": "Query Performance Prediction using Relevance Judgments Generated by Large Language Models", "authors": ["Chuan Meng", "Negar Arabzadeh", "Arian Askari", "Mohammad Aliannejadi", "Maarten de Rijke"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "comment": "Accepted by ACM Transactions on Information Systems (TOIS)", "summary": "Query performance prediction (QPP) aims to estimate the retrieval quality of\na search system for a query without human relevance judgments. Previous QPP\nmethods typically return a single scalar value and do not require the predicted\nvalues to approximate a specific information retrieval (IR) evaluation measure,\nleading to certain drawbacks: (i) a single scalar is insufficient to accurately\nrepresent different IR evaluation measures, especially when metrics do not\nhighly correlate, and (ii) a single scalar limits the interpretability of QPP\nmethods because solely using a scalar is insufficient to explain QPP results.\nTo address these issues, we propose a QPP framework using automatically\ngenerated relevance judgments (QPP-GenRE), which decomposes QPP into\nindependent subtasks of predicting the relevance of each item in a ranked list\nto a given query. This allows us to predict any IR evaluation measure using the\ngenerated relevance judgments as pseudo-labels. This also allows us to\ninterpret predicted IR evaluation measures, and identify, track and rectify\nerrors in generated relevance judgments to improve QPP quality. We predict an\nitem's relevance by using open-source large language models (LLMs) to ensure\nscientific reproducibility. We face two main challenges: (i) excessive\ncomputational costs of judging an entire corpus for predicting a metric\nconsidering recall, and (ii) limited performance in prompting open-source LLMs\nin a zero-/few-shot manner. To solve the challenges, we devise an approximation\nstrategy to predict an IR measure considering recall and propose to fine-tune\nopen-source LLMs using human-labeled relevance judgments. Experiments on the\nTREC 2019 to 2022 deep learning tracks and CAsT-19 and 20 datasets show that\nQPP-GenRE achieves state-of-the-art QPP quality for both lexical and neural\nrankers."}
{"id": "2404.16792", "pdf": "https://arxiv.org/pdf/2404.16792.pdf", "abs": "https://arxiv.org/abs/2404.16792", "title": "Model Extrapolation Expedites Alignment", "authors": ["Chujie Zheng", "Ziqi Wang", "Heng Ji", "Minlie Huang", "Nanyun Peng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "Given the high computational cost of preference alignment training of large\nlanguage models (LLMs), exploring efficient methods to reduce the training\noverhead remains an important and compelling research problem. Motivated by the\nobservation that alignment training typically involves only small parameter\nchanges without injecting new knowledge into models, we propose a\nstraightforward method called ExPO (model extrapolation) to expedite LLMs'\nalignment with human preferences. Given a partially-trained model and its\ninitial SFT checkpoint, ExPO improves the implicit optimization objective of\nalignment training by simply amplifying the parameter change based on a\nfirst-order approximation, without any additional training overhead. Through\ncontrolled experiments, we demonstrate that ExPO boosts a DPO model trained\nwith only 20% steps to outperform the fully-trained one. Moreover, we show that\nExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B\nparameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which\nhighlights ExPO's broader utility in efficiently enhancing LLM alignment."}
{"id": "2405.07671", "pdf": "https://arxiv.org/pdf/2405.07671.pdf", "abs": "https://arxiv.org/abs/2405.07671", "title": "Constructing a BPE Tokenization DFA", "authors": ["Martin Berglund", "Willeke Martens", "Brink van der Merwe"], "categories": ["cs.FL", "cs.CL", "cs.LG"], "comment": "This is a revised and extended version of the conference paper\n  indicated in the external DOI field. Prefer citing that version whenever the\n  extended content is unnecessary", "summary": "Many natural language processing systems operate over tokenizations of text\nto address the open-vocabulary problem. In this paper, we give and analyze an\nalgorithm for the efficient construction of deterministic finite automata (DFA)\ndesigned to operate directly on tokenizations produced by the popular byte pair\nencoding (BPE) technique. This makes it possible to apply many existing\ntechniques and algorithms to the tokenized case, such as pattern matching,\nequivalence checking of tokenization dictionaries, and composing tokenized\nlanguages in various ways. The construction preserves some key properties of\nthe automaton, and we use this to establish asymptotic bounds on the state\ncomplexity of the automata that result. Finally, we demonstrate how to\nconstruct an input-deterministic (subsequential) string-to-string transducer\nwhich precisely describes the relationship between strings and their correct\ntokenizations."}
{"id": "2405.07960", "pdf": "https://arxiv.org/pdf/2405.07960.pdf", "abs": "https://arxiv.org/abs/2405.07960", "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments", "authors": ["Samuel Schmidgall", "Rojin Ziaei", "Carl Harris", "Eduardo Reis", "Jeffrey Jopling", "Michael Moor"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Evaluating large language models (LLM) in clinical scenarios is crucial to\nassessing their potential clinical utility. Existing benchmarks rely heavily on\nstatic question-answering, which does not accurately depict the complex,\nsequential nature of clinical decision-making. Here, we introduce AgentClinic,\na multimodal agent benchmark for evaluating LLMs in simulated clinical\nenvironments that include patient interactions, multimodal data collection\nunder incomplete information, and the usage of various tools, resulting in an\nin-depth evaluation across nine medical specialties and seven languages. We\nfind that solving MedQA problems in the sequential decision-making format of\nAgentClinic is considerably more challenging, resulting in diagnostic\naccuracies that can drop to below a tenth of the original accuracy. Overall, we\nobserve that agents sourced from Claude-3.5 outperform other LLM backbones in\nmost settings. Nevertheless, we see stark differences in the LLMs' ability to\nmake use of tools, such as experiential learning, adaptive retrieval, and\nreflection cycles. Strikingly, Llama-3 shows up to 92% relative improvements\nwith the notebook tool that allows for writing and editing notes that persist\nacross cases. To further scrutinize our clinical simulations, we leverage\nreal-world electronic health records, perform a clinical reader study, perturb\nagents with biases, and explore novel patient-centric metrics that this\ninteractive environment firstly enables."}
{"id": "2405.14917", "pdf": "https://arxiv.org/pdf/2405.14917.pdf", "abs": "https://arxiv.org/abs/2405.14917", "title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models", "authors": ["Wei Huang", "Haotong Qin", "Yangdong Liu", "Yawei Li", "Qinshuo Liu", "Xianglong Liu", "Luca Benini", "Michele Magno", "Shiming Zhang", "Xiaojuan Qi"], "categories": ["cs.LG", "cs.CL"], "comment": "22 pages", "summary": "Post-training quantization (PTQ) is an effective technique for compressing\nlarge language models (LLMs). However, while uniform-precision quantization is\ncomputationally efficient, it often compromises model performance. To address\nthis, we propose SliM-LLM, a salience-driven mixed-precision quantization\nframework that allocates bit-widths at the group-wise. Our approach leverages\nthe observation that important weights follow a structured distribution and\nintroduces two key components: \\textbf{1)} \\textit{Salience-Determined Bit\nAllocation} adaptively assigns bit-widths to groups within each layer based on\ntheir salience; and \\textbf{2)} \\textit{Salience-Weighted Quantizer\nCalibration} optimizes quantizer parameters by incorporating element-level\nsalience. With its structured partitioning, SliM-LLM provides a\nhardware-friendly solution that matches the efficiency of uniform quantization\nmethods while improving accuracy. Experiments show that SliM-LLM achieves\nsuperior performance across various LLMs at low bit-widths. For example, a\n2-bit quantized LLaMA-7B model reduces memory usage by nearly 6x compared to\nthe floating-point baseline, decreases perplexity by 48\\% compared to\nstate-of-the-art gradient-free PTQ methods, and maintains GPU inference speed.\nAdditionally, the extended version, SliM-LLM$^+$, which incorporates\ngradient-based quantization, further reduces perplexity by 35.1\\%. Our code is\navailable at https://github.com/Aaronhuang-778/SliM-LLM"}
{"id": "2405.17130", "pdf": "https://arxiv.org/pdf/2405.17130.pdf", "abs": "https://arxiv.org/abs/2405.17130", "title": "Explaining the role of Intrinsic Dimensionality in Adversarial Training", "authors": ["Enes Altinisik", "Safa Messaoud", "Husrev Taha Sencar", "Hassan Sajjad", "Sanjay Chawla"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Adversarial Training (AT) impacts different architectures in distinct ways:\nvision models gain robustness but face reduced generalization, encoder-based\nmodels exhibit limited robustness improvements with minimal generalization\nloss, and recent work in latent-space adversarial training (LAT) demonstrates\nthat decoder-based models achieve improved robustness by applying AT across\nmultiple layers. We provide the first explanation for these trends by\nleveraging the manifold conjecture: off-manifold adversarial examples (AEs)\nenhance robustness, while on-manifold AEs improve generalization. We show that\nvision and decoder-based models exhibit low intrinsic dimensionality in earlier\nlayers (favoring off-manifold AEs), whereas encoder-based models do so in later\nlayers (favoring on-manifold AEs). Exploiting this property, we introduce\nSMAAT, which improves the scalability of AT for encoder-based models by\nperturbing the layer with the lowest intrinsic dimensionality. This reduces the\nprojected gradient descent (PGD) chain length required for AE generation,\ncutting GPU time by 25-33% while significantly boosting robustness. We validate\nSMAAT across multiple tasks, including text generation, sentiment\nclassification, safety filtering, and retrieval augmented generation setups,\ndemonstrating superior robustness with comparable generalization to standard\ntraining."}
{"id": "2405.17423", "pdf": "https://arxiv.org/pdf/2405.17423.pdf", "abs": "https://arxiv.org/abs/2405.17423", "title": "Little Data, Big Impact: Privacy-Aware Visual Language Models via Minimal Tuning", "authors": ["Laurens Samson", "Nimrod Barazani", "Sennay Ghebreab", "Yuki M. Asano"], "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "As Visual Language Models (VLMs) become increasingly embedded in everyday\napplications, ensuring they can recognize and appropriately handle\nprivacy-sensitive content is essential. We conduct a comprehensive evaluation\nof ten state-of-the-art VLMs and identify limitations in their understanding of\nvisual privacy. Existing datasets suffer from label inconsistencies, limiting\ntheir reliability. To address this, we introduce two compact, high-quality\nbenchmarks, PrivBench and PrivBench-H, that focus on commonly recognized\nprivacy categories aligned with the General Data Protection Regulation (GDPR).\nAdditionally, we present PrivTune, an instruction-tuning dataset specifically\ncurated to improve privacy sensitivity. We obtain a Privacy VLM by fine-tuning\nan off-the-shelf VLM on only 100 samples from PrivTune, which leads to\nsubstantial gains on all benchmarks, surpassing GPT-4, while maintaining strong\nperformance on other tasks. Our findings show that privacy-awareness in VLMs\ncan be substantially improved with minimal data and careful dataset design,\nsetting the stage for safer, more privacy-aligned AI systems."}
{"id": "2406.02539", "pdf": "https://arxiv.org/pdf/2406.02539.pdf", "abs": "https://arxiv.org/abs/2406.02539", "title": "Parrot: Multilingual Visual Instruction Tuning", "authors": ["Hai-Long Sun", "Da-Wei Zhou", "Yang Li", "Shiyin Lu", "Chao Yi", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. Code and dataset are available at:\n  https://github.com/AIDC-AI/Parrot", "summary": "The rapid development of Multimodal Large Language Models (MLLMs), such as\nGPT-4o, marks a significant step toward artificial general intelligence.\nExisting methods typically align vision encoders with LLMs via supervised\nfine-tuning (SFT), but this often deteriorates their ability to handle multiple\nlanguages as training progresses. We empirically observe that imbalanced SFT\ndatasets, largely English-centric, degrade performance on non-English languages\ndue to the failure in multilingual token alignment. To address this, we propose\nPARROT, a novel approach that leverages textual guidance for visual token\nalignment at the language level. PARROT conditions visual tokens on diverse\nlanguage inputs and uses Mixture-of-Experts (MoE) to align multilingual tokens.\nBy computing cross-attention between initial visual features and textual\nembeddings, we select the most relevant experts, converting visual tokens into\nlanguage-specific representations. Additionally, we introduce the Massive\nMultilingual Multimodal Benchmark (MMMB), a new benchmark comprising 6\nlanguages, 15 categories, and 12,000 questions, to assess multilingual\ncapabilities. PARROT achieves state-of-the-art performance on both the\nmultilingual benchmarks and a wide range of multimodal tasks. Code and dataset\nare available at: https://github.com/AIDC-AI/Parrot"}
{"id": "2407.04899", "pdf": "https://arxiv.org/pdf/2407.04899.pdf", "abs": "https://arxiv.org/abs/2407.04899", "title": "Algorithmic Language Models with Neurally Compiled Libraries", "authors": ["Lucas Saldyt", "Subbarao Kambhampati"], "categories": ["cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Important tasks such as reasoning and planning are fundamentally algorithmic,\nmeaning that solving them robustly requires acquiring true reasoning or\nplanning algorithms, rather than shortcuts. Large Language Models lack true\nalgorithmic ability primarily because of the limitations of neural network\noptimization algorithms, their optimization data and optimization objective,\nbut also due to architectural inexpressivity. To solve this, our paper proposes\naugmenting LLMs with a library of fundamental operations and sophisticated\ndifferentiable programs, so that common algorithms do not need to be learned\nfrom scratch. We add memory, registers, basic operations, and adaptive\nrecurrence to a transformer architecture built on LLaMA3. Then, we define a\nmethod for directly compiling algorithms into a differentiable starting\nlibrary, which is used natively and propagates gradients for optimization. In\nthis preliminary study, we explore the feasability of augmenting LLaMA3 with a\ndifferentiable computer, for instance by fine-tuning small transformers on\nsimple algorithmic tasks with variable computational depth."}
{"id": "2408.09865", "pdf": "https://arxiv.org/pdf/2408.09865.pdf", "abs": "https://arxiv.org/abs/2408.09865", "title": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation", "authors": ["Ching-Wen Yang", "Zhi-Quan Feng", "Ying-Jia Lin", "Che-Wei Chen", "Kun-da Wu", "Hao Xu", "Jui-Feng Yao", "Hung-Yu Kao"], "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": "9 main pages, 10 pages for appendix. ACL'25 Main", "summary": "The Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models approach review generation as a proxy for explainable\nrecommendations. While these models can produce fluent and grammatically\ncorrect sentences, they often lack precision and fail to provide personalized,\ninformative recommendations. To address this issue, we propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), which\nintegrates aspect category as another input dimension to facilitate memorizing\nfine-grained aspect terms. Experiments conducted on two real-world review\ndatasets in the restaurant domain demonstrate that MAPLE significantly\noutperforms baseline review-generation models. MAPLE excels in both text and\nfeature diversity, ensuring that the generated content covers a wide range of\naspects. Additionally, MAPLE delivers good generation quality while maintaining\nstrong coherence and factual relevance. The code and dataset used in this paper\ncan be found here https://github.com/Nana2929/MAPLE.git."}
{"id": "2409.10289", "pdf": "https://arxiv.org/pdf/2409.10289.pdf", "abs": "https://arxiv.org/abs/2409.10289", "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework", "authors": ["Jiahao Yuan", "Zixiang Di", "Zhiqing Cui", "Guisong Yang", "Usman Naseem"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting,\nReflectDiffu adeptly translates emotional decision-making into precise intent\nactions, thereby addressing empathetic response misalignments stemming from\nemotional misrecognition. Through reflection, the framework maps emotional\nstates to intents, markedly enhancing both response empathy and flexibility.\nComprehensive experiments reveal that ReflectDiffu outperforms existing models\nregarding relevance, controllability, and informativeness, achieving\nstate-of-the-art results in both automatic and human evaluations."}
{"id": "2410.02167", "pdf": "https://arxiv.org/pdf/2410.02167.pdf", "abs": "https://arxiv.org/abs/2410.02167", "title": "Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis", "authors": ["Hongkang Li", "Songtao Lu", "Pin-Yu Chen", "Xiaodong Cui", "Meng Wang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) is an efficient prompting method that enables the\nreasoning ability of large language models by augmenting the query using\nmultiple examples with multiple intermediate steps. Despite the empirical\nsuccess, the theoretical understanding of how to train a Transformer to achieve\nthe CoT ability remains less explored. This is primarily due to the technical\nchallenges involved in analyzing the nonconvex optimization on nonlinear\nattention models. To the best of our knowledge, this work provides the first\ntheoretical study of training Transformers with nonlinear attention to obtain\nthe CoT generalization capability so that the resulting model can inference on\nunseen tasks when the input is augmented by examples of the new task. We first\nquantify the required training samples and iterations to train a Transformer\nmodel towards CoT ability. We then prove the success of its CoT generalization\non unseen tasks with distribution-shifted testing data. Moreover, we\ntheoretically characterize the conditions for an accurate reasoning output by\nCoT even when the provided reasoning examples contain noises and are not always\naccurate. In contrast, in-context learning (ICL), which can be viewed as\none-step CoT without intermediate steps, may fail to provide an accurate output\nwhen CoT does. These theoretical findings are justified through experiments."}
{"id": "2410.09403", "pdf": "https://arxiv.org/pdf/2410.09403.pdf", "abs": "https://arxiv.org/abs/2410.09403", "title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System", "authors": ["Haoyang Su", "Renqi Chen", "Shixiang Tang", "Zhenfei Yin", "Xinzhe Zheng", "Jinzhe Li", "Biqing Qi", "Qi Wu", "Hui Li", "Wanli Ouyang", "Philip Torr", "Bowen Zhou", "Nanqing Dong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "The rapid advancement of scientific progress requires innovative tools that\ncan accelerate knowledge discovery. Although recent AI methods, particularly\nlarge language models (LLMs), have shown promise in tasks such as hypothesis\ngeneration and experimental design, they fall short of replicating the\ncollaborative nature of real-world scientific practices, where diverse experts\nwork together in teams to tackle complex problems. To address the limitations,\nwe propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),\ndesigned to mimic the teamwork inherent in scientific research. VirSci\norganizes a team of agents to collaboratively generate, evaluate, and refine\nresearch ideas. Through comprehensive experiments, we demonstrate that this\nmulti-agent approach outperforms the state-of-the-art method in producing novel\nscientific ideas. We further investigate the collaboration mechanisms that\ncontribute to its tendency to produce ideas with higher novelty, offering\nvaluable insights to guide future research and illuminating pathways toward\nbuilding a robust system for autonomous scientific discovery. The code is\navailable at https://github.com/open-sciencelab/Virtual-Scientists."}
{"id": "2410.13825", "pdf": "https://arxiv.org/pdf/2410.13825.pdf", "abs": "https://arxiv.org/abs/2410.13825", "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents", "authors": ["Ke Yang", "Yao Liu", "Sapana Chaudhary", "Rasool Fakoor", "Pratik Chaudhari", "George Karypis", "Huzefa Rangwala"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents."}
{"id": "2410.16638", "pdf": "https://arxiv.org/pdf/2410.16638.pdf", "abs": "https://arxiv.org/abs/2410.16638", "title": "LLMScan: Causal Scan for LLM Misbehavior Detection", "authors": ["Mengdi Zhang", "Kai Kiat Goh", "Peixin Zhang", "Jun Sun", "Rose Lin Xin", "Hongyu Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks."}
{"id": "2410.22663", "pdf": "https://arxiv.org/pdf/2410.22663.pdf", "abs": "https://arxiv.org/abs/2410.22663", "title": "Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers", "authors": ["Lam Nguyen Tung", "Steven Cho", "Xiaoning Du", "Neelofar Neelofar", "Valerio Terragni", "Stefano Ruberto", "Aldeida Aleti"], "categories": ["cs.SE", "cs.CL", "cs.CR"], "comment": "24 pages, 5 tables, 9 figures, Camera-ready version accepted to FSE\n  2025", "summary": "Machine learning (ML) for text classification has been widely used in various\ndomains. These applications can significantly impact ethics, economics, and\nhuman behavior, raising serious concerns about trusting ML decisions. Studies\nindicate that conventional metrics are insufficient to build human trust in ML\nmodels. These models often learn spurious correlations and predict based on\nthem. In the real world, their performance can deteriorate significantly. To\navoid this, a common practice is to test whether predictions are reasonable\nbased on valid patterns in the data. Along with this, a challenge known as the\ntrustworthiness oracle problem has been introduced. Due to the lack of\nautomated trustworthiness oracles, the assessment requires manual validation of\nthe decision process disclosed by explanation methods. However, this is\ntime-consuming, error-prone, and unscalable.\n  We propose TOKI, the first automated trustworthiness oracle generation method\nfor text classifiers. TOKI automatically checks whether the words contributing\nthe most to a prediction are semantically related to the predicted class.\nSpecifically, we leverage ML explanations to extract the decision-contributing\nwords and measure their semantic relatedness with the class based on word\nembeddings. We also introduce a novel adversarial attack method that targets\ntrustworthiness vulnerabilities identified by TOKI. To evaluate their alignment\nwith human judgement, experiments are conducted. We compare TOKI with a naive\nbaseline based solely on model confidence and TOKI-guided adversarial attack\nmethod with A2T, a SOTA adversarial attack method. Results show that relying on\nprediction uncertainty cannot effectively distinguish between trustworthy and\nuntrustworthy predictions, TOKI achieves 142% higher accuracy than the naive\nbaseline, and TOKI-guided attack method is more effective with fewer\nperturbations than A2T."}
{"id": "2411.10272", "pdf": "https://arxiv.org/pdf/2411.10272.pdf", "abs": "https://arxiv.org/abs/2411.10272", "title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning", "authors": ["Xiaodong Chen", "Yuxuan Hu", "Xiaokang Zhang", "Yanling Wang", "Cuiping Li", "Hong Chen", "Jing Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted as Main of ACL2025", "summary": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs."}
{"id": "2411.14507", "pdf": "https://arxiv.org/pdf/2411.14507.pdf", "abs": "https://arxiv.org/abs/2411.14507", "title": "FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers", "authors": ["Zehua Pei", "Hui-Ling Zhen", "Xianzhi Yu", "Sinno Jialin Pan", "Mingxuan Yuan", "Bei Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Generative Pre-trained Transformers (GPTs) have demonstrated remarkable\nperformance across diverse domains, largely due to the extensive scaling of\nmodel parameters. Recent works have observed redundancy within transformer\nblocks and developed compression methods by structured pruning of less\nimportant blocks. However, such direct removal often leads to irreversible\nperformance degradation. In this paper, we propose FuseGPT, a novel methodology\ndesigned to recycle pruned transformer blocks, thereby recovering the model's\nperformance. Firstly, we introduce a new importance detection metric, Macro\nInfluence (MI), which evaluates the long-term impact of each transformer block\nby quantifying the information loss incurred upon its removal. Next, we propose\ngroup-level layer fusion, which leverages the parameters from layers of less\nimportant blocks and integrates them into the corresponding layers of\nneighboring blocks. This fusion process is not a one-time operation but is\nrefined through iterative parameter updates by lightweight group-level\nfine-tuning. Specifically, the injected parameters are frozen but are weighted\nwith learnable rank decomposition matrices to reduce the computational overhead\nduring fine-tuning. Our approach not only works well for large language models\nbut also for large multimodal models. Experimental results indicate that, even\nwith modest amounts of data, FuseGPT surpasses previous methods in both\nperplexity and zero-shot task performance."}
{"id": "2411.17404", "pdf": "https://arxiv.org/pdf/2411.17404.pdf", "abs": "https://arxiv.org/abs/2411.17404", "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving", "authors": ["Teng Wang", "Wing-Yin Yu", "Zhenqi He", "Zehua Liu", "Hailei Gong", "Han Wu", "Xiongwei Han", "Wei Shi", "Ruifeng She", "Fangzhou Zhu", "Tao Zhong"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, an algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions. The StructuredOR dataset is\navailable on Huggingface https://huggingface.co/datasets/LLM4OR/StructuredOR\nand GitHub https://github.com/LLM4OR/StructuredOR."}
{"id": "2412.03966", "pdf": "https://arxiv.org/pdf/2412.03966.pdf", "abs": "https://arxiv.org/abs/2412.03966", "title": "Demonstration Selection for In-Context Learning via Reinforcement Learning", "authors": ["Xubin Wang", "Jianfei Wu", "Yichen Yuan", "Deyu Cai", "Mingzhe Li", "Weijia Jia"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Diversity in demonstration selection is critical for enhancing model\ngeneralization by enabling broader coverage of structures and concepts.\nConstructing appropriate demonstration sets remains a key research challenge.\nThis paper introduces the Relevance-Diversity Enhanced Selection (RDES), an\ninnovative approach that leverages reinforcement learning (RL) frameworks to\noptimize the selection of diverse reference demonstrations for tasks amenable\nto in-context learning (ICL), particularly text classification and reasoning,\nin few-shot prompting scenarios. RDES employs frameworks like Q-learning and a\nPPO-based variant to dynamically identify demonstrations that maximize both\ndiversity (quantified by label distribution) and relevance to the task\nobjective. This strategy ensures a balanced representation of reference data,\nleading to improved accuracy and generalization. Through extensive experiments\non multiple benchmark datasets, including diverse reasoning tasks, and\ninvolving 14 closed-source and open-source LLMs, we demonstrate that RDES\nsignificantly enhances performance compared to ten established baselines. Our\nevaluation includes analysis of performance across varying numbers of\ndemonstrations on selected datasets. Furthermore, we investigate incorporating\nChain-of-Thought (CoT) reasoning, which further boosts predictive performance.\nThe results highlight the potential of RL for adaptive demonstration selection\nand addressing challenges in ICL."}
{"id": "2412.06559", "pdf": "https://arxiv.org/pdf/2412.06559.pdf", "abs": "https://arxiv.org/abs/2412.06559", "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning", "authors": ["Chujie Zheng", "Zhenru Zhang", "Beichen Zhang", "Runji Lin", "Keming Lu", "Bowen Yu", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "As language models regularly make mistakes when solving math problems,\nautomated identification of errors in the reasoning process becomes\nincreasingly significant for their scalable oversight. In this paper, we\nintroduce ProcessBench for measuring the ability to identify erroneous steps in\nmathematical reasoning. It consists of 3,400 test cases, primarily focused on\ncompetition- and Olympiad-level math problems. Each test case contains a\nstep-by-step solution with error location annotated by human experts. Models\nare required to identify the earliest step that contains an error, or conclude\nthat all steps are correct. We conduct extensive evaluation on ProcessBench,\ninvolving two types of models: process reward models (PRMs) and critic models,\nwhere for the latter we prompt general language models to critique each\nsolution step by step. We draw two main observations: (1) Existing PRMs\ntypically fail to generalize to more challenging math problems beyond GSM8K and\nMATH. They underperform both critic models (i.e., prompted general language\nmodels) and our own trained PRM that is straightforwardly fine-tuned on the\nPRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has\ndemonstrated the critique capability competitive with the proprietary model\nGPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We\nhope ProcessBench can foster future research in reasoning process assessment,\npaving the way toward scalable oversight of language models."}
{"id": "2412.08564", "pdf": "https://arxiv.org/pdf/2412.08564.pdf", "abs": "https://arxiv.org/abs/2412.08564", "title": "Visual Program Distillation with Template-Based Augmentation", "authors": ["Michal Shlapentokh-Rothman", "Yu-Xiong Wang", "Derek Hoiem"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Adapting visual programming or prompting large language models (LLMs) to\ngenerate executable code for visual tasks like visual question answering (VQA)\nfor specialized tasks or domains remains challenging due to high annotation and\ninference costs. We propose a low-cost visual program distillation method that\ncan be used for models with at most 1 billion parameters and requires no\nhuman-generated program annotations. We achieve this through synthetic data\naugmentation based on decoupling programs into higher-level skills, called\ntemplates, and their corresponding arguments. Experimental results show that,\nwith a relatively small amount of question/answer data, small language models\ncan generate high-quality specialized visual programs with the added benefit of\nmuch faster inference"}
{"id": "2412.16216", "pdf": "https://arxiv.org/pdf/2412.16216.pdf", "abs": "https://arxiv.org/abs/2412.16216", "title": "GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration", "authors": ["Ting Bai", "Yue Yu", "Le Huang", "Zenan Xu", "Zhe Zhao", "Chuan Shi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "9 pages, 25 figures", "summary": "The sparse Mixture-of-Experts (MoE) architecture of large language models\n(LLMs) confronts an inherent issue of load imbalance arising from the\nsimplistic linear router strategy, which ultimately causes the instability and\ninefficient learning of LLMs. To address this challenge, we introduce a novel\nMoE graph-based framework $\\textbf{GMoE}$, aimed at enhancing the collaboration\namong multiple experts. In GMoE, a graph router function is designed to capture\nthe collaboration signals among experts. This enables all experts to\ndynamically allocate information derived from input data by sharing information\nwith their neighboring experts. Moreover, we put forward two coordination\nstrategies in GMoE: the $\\textit{Poisson distribution-based distinction\nstrategy}$ and the $\\textit{Normal distribution-based balance strategy}$, to\nfurther release the capacity of each expert and increase the model stability in\nthe fine-tuning of LLMs. Specifically, we leverage a parameter-efficient\nfine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph\nMoE architecture. Extensive experiments on four real-world benchmark datasets\ndemonstrate the effectiveness of GMoE, showing the benefits of facilitating\ncollaborations of multiple experts in LLM fine-tuning. The code of experimental\nimplementation is available at https://github.com/BAI-LAB/GMoE"}
{"id": "2501.10639", "pdf": "https://arxiv.org/pdf/2501.10639.pdf", "abs": "https://arxiv.org/abs/2501.10639", "title": "Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks", "authors": ["Xin Yi", "Yue Li", "dongsheng Shi", "Linlin Wang", "Xiaoling Wang", "Liang He"], "categories": ["cs.CR", "cs.CL"], "comment": "Under Review", "summary": "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks."}
{"id": "2501.12432", "pdf": "https://arxiv.org/pdf/2501.12432.pdf", "abs": "https://arxiv.org/abs/2501.12432", "title": "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation", "authors": ["Dongsheng Zhu", "Weixian Shi", "Zhengliang Shi", "Zhaochun Ren", "Shuaiqiang Wang", "Lingyong Yan", "Dawei Yin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https://corn0205.github.io/"}
{"id": "2502.00577", "pdf": "https://arxiv.org/pdf/2502.00577.pdf", "abs": "https://arxiv.org/abs/2502.00577", "title": "Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach", "authors": ["Changdae Oh", "Zhen Fang", "Shawn Im", "Xuefeng Du", "Yixuan Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025 camera-ready", "summary": "Multimodal large language models (MLLMs) have shown promising capabilities\nbut struggle under distribution shifts, where evaluation data differ from\ninstruction tuning distributions. Although previous works have provided\nempirical evaluations, we argue that establishing a formal framework that can\ncharacterize and quantify the risk of MLLMs is necessary to ensure the safe and\nreliable application of MLLMs in the real world. By taking an\ninformation-theoretic perspective, we propose the first theoretical framework\nthat enables the quantification of the maximum risk of MLLMs under distribution\nshifts. Central to our framework is the introduction of Effective Mutual\nInformation (EMI), a principled metric that quantifies the relevance between\ninput queries and model responses. We derive an upper bound for the EMI\ndifference between in-distribution (ID) and out-of-distribution (OOD) data,\nconnecting it to visual and textual distributional discrepancies. Extensive\nexperiments on real benchmark datasets, spanning 61 shift scenarios,\nempirically validate our theoretical insights."}
{"id": "2502.01247", "pdf": "https://arxiv.org/pdf/2502.01247.pdf", "abs": "https://arxiv.org/abs/2502.01247", "title": "Polynomial, trigonometric, and tropical activations", "authors": ["Ismail Khalfaoui-Hassani", "Stefan Kesselheim"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "math.AG"], "comment": null, "summary": "Which functions can be used as activations in deep neural networks? This\narticle explores families of functions based on orthonormal bases, including\nthe Hermite polynomial basis and the Fourier trigonometric basis, as well as a\nbasis resulting from the tropicalization of a polynomial basis. Our study shows\nthat, through simple variance-preserving initialization and without additional\nclamping mechanisms, these activations can successfully be used to train deep\nmodels, such as GPT-2 for next-token prediction on OpenWebText and ConvNeXt for\nimage classification on ImageNet. Our work addresses the issue of exploding and\nvanishing activations and gradients, particularly prevalent with polynomial\nactivations, and opens the door for improving the efficiency of large-scale\nlearning tasks. Furthermore, our approach provides insight into the structure\nof neural networks, revealing that networks with polynomial activations can be\ninterpreted as multivariate polynomial mappings. Finally, using Hermite\ninterpolation, we show that our activations can closely approximate classical\nones in pre-trained models by matching both the function and its derivative,\nmaking them especially useful for fine-tuning tasks. These activations are\navailable in the torchortho library, which can be accessed via:\nhttps://github.com/K-H-Ismail/torchortho."}
{"id": "2502.01534", "pdf": "https://arxiv.org/pdf/2502.01534.pdf", "abs": "https://arxiv.org/abs/2502.01534", "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge", "authors": ["Dawei Li", "Renliang Sun", "Yue Huang", "Ming Zhong", "Bohan Jiang", "Jiawei Han", "Xiangliang Zhang", "Wei Wang", "Huan Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "20 pages, 7 figures", "summary": "Large Language Models (LLMs) as judges and LLM-based data synthesis have\nemerged as two fundamental LLM-driven data annotation methods in model\ndevelopment. While their combination significantly enhances the efficiency of\nmodel training and evaluation, little attention has been given to the potential\ncontamination brought by this new model development paradigm. In this work, we\nexpose preference leakage, a contamination problem in LLM-as-a-judge caused by\nthe relatedness between the synthetic data generators and LLM-based evaluators.\nTo study this issue, we first define three common relatednesses between the\ndata generator LLM and the judge LLM: being the same model, having an\ninheritance relationship, and belonging to the same model family. Through\nextensive experiments, we empirically confirm the bias of judges towards their\nrelated student models caused by preference leakage across multiple LLM\nbaselines and benchmarks. Further analysis suggests that preference leakage is\na pervasive and real-world problem that is harder to detect compared to\npreviously identified biases in LLM-as-a-judge scenarios. All of these findings\nimply that preference leakage is a widespread and challenging problem in the\narea of LLM-as-a-judge. We release all codes and data at:\nhttps://github.com/David-Li0406/Preference-Leakage."}
{"id": "2502.01718", "pdf": "https://arxiv.org/pdf/2502.01718.pdf", "abs": "https://arxiv.org/abs/2502.01718", "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis", "authors": ["Huaye Zeng", "Dongfu Jiang", "Haozhe Wang", "Ping Nie", "Xiaotong Chen", "Wenhu Chen"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "9 pages, 4 figure, 11 tables. Accepted to ACL 2025 main conference", "summary": "Most progress in recent coder models has been driven by supervised\nfine-tuning (SFT), while the potential of reinforcement learning (RL) remains\nlargely unexplored, primarily due to the lack of reliable reward data/model in\nthe code domain. In this paper, we address this challenge by leveraging\nautomated large-scale test-case synthesis to enhance code model training.\nSpecifically, we design a pipeline that generates extensive (question,\ntest-cases) pairs from existing code data. Using these test cases, we construct\npreference pairs based on pass rates over sampled programs to train reward\nmodels with Bradley-Terry loss. It shows an average of 10-point improvement for\nLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through\nbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.\nFurthermore, we conduct reinforcement learning with both reward models and\ntest-case pass rewards, leading to consistent improvements across HumanEval,\nMBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\ntraining can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\%\nfor merely 80 optimization steps. We believe our results highlight the huge\npotential of reinforcement learning in coder models."}
{"id": "2502.03930", "pdf": "https://arxiv.org/pdf/2502.03930.pdf", "abs": "https://arxiv.org/abs/2502.03930", "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation", "authors": ["Dongya Jia", "Zhuo Chen", "Jiawei Chen", "Chenpeng Du", "Jian Wu", "Jian Cong", "Xiaobin Zhuang", "Chumin Li", "Zhen Wei", "Yuping Wang", "Yuxuan Wang"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted by ICML 2025", "summary": "Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness."}
{"id": "2502.07266", "pdf": "https://arxiv.org/pdf/2502.07266.pdf", "abs": "https://arxiv.org/abs/2502.07266", "title": "When More is Less: Understanding Chain-of-Thought Length in LLMs", "authors": ["Yuyang Wu", "Yifei Wang", "Tianqi Du", "Stefanie Jegelka", "Yisen Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) employ Chain-of-Thought (CoT) reasoning to\ndeconstruct complex problems. While longer CoTs are often presumed superior,\nthis paper challenges that notion, arguing that longer is not always better.\nDrawing on combined evidence from real-world observations, controlled\nexperiments, and theoretical analysis, we demonstrate that task accuracy\ntypically follows an inverted U-shaped curve with CoT length, where performance\ninitially improves but eventually decreases as the number of CoT steps\nincreases. With controlled experiments, we further uncover the scaling\nbehaviors of the optimal CoT length: it increases with task difficulty but\ndecreases with model capability, exposing an inherent simplicity bias where\nmore capable models favor shorter, more efficient CoT reasoning. This bias is\nalso evident in Reinforcement Learning (RL) training, where models gravitate\ntowards shorter CoTs as their accuracy improves. To have a deep understanding\nof these dynamics, we establish a simple theoretical model that formally proves\nthese phenomena, including the optimal length's scaling laws and the emergence\nof simplicity bias during RL. Guided by this framework, we demonstrate\nsignificant practical benefits from training with optimally-lengthed CoTs and\nemploying length-aware filtering at inference. These findings offer both a\nprincipled understanding of the \"overthinking\" phenomenon and multiple\npractical guidelines for CoT calibration, enabling LLMs to achieve optimal\nreasoning performance with adaptive CoTs tailored to task complexity and model\ncapability."}
{"id": "2502.09723", "pdf": "https://arxiv.org/pdf/2502.09723.pdf", "abs": "https://arxiv.org/abs/2502.09723", "title": "QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language", "authors": ["Qingsong Zou", "Jingyu Xiao", "Qing Li", "Zhi Yan", "Yuhang Wang", "Li Xu", "Wenxuan Wang", "Kuofeng Gao", "Ruoyu Li", "Yong Jiang"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "To appear in ACL 2025", "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to $64\\%$ on GPT-4-1106. Our code is\navailable at https://github.com/horizonsinzqs/QueryAttack."}
{"id": "2502.11435", "pdf": "https://arxiv.org/pdf/2502.11435.pdf", "abs": "https://arxiv.org/abs/2502.11435", "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Hongru Wang", "Xiusi Chen", "Avirup Sil", "Dilek Hakkani-Tür", "Gokhan Tur", "Heng Ji"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "18 pages, 11 tables, 7 figures, ACL 2025 Findings", "summary": "Current Large Language Model (LLM) agents demonstrate strong reasoning and\ntool use capabilities, but often lack self-awareness, failing to balance these\napproaches effectively. This imbalance leads to Tool Overuse, where models\nunnecessarily rely on external tools for tasks solvable with parametric\nknowledge, increasing computational overhead. Inspired by human metacognition,\nwe introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm\nthat enhances an agent's self-awareness to optimize task handling and reduce\ntool overuse. To support this paradigm, we introduce SMART-ER, a dataset\nspanning three domains, where reasoning alternates between parametric knowledge\nand tool-dependent steps, with each step enriched by rationales explaining when\ntools are necessary. Through supervised training, we develop SMARTAgent, a\nfamily of models that dynamically balance parametric knowledge and tool use.\nEvaluations show that SMARTAgent reduces tool use by 24% while improving\nperformance by over 37%, enabling 7B-scale models to match its 70B counterpart\nand GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test\ndata like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool\ncalls. These highlight the potential of strategic tool use to enhance\nreasoning, mitigate overuse, and bridge the gap between model size and\nperformance, advancing intelligent and resource-efficient agent designs."}
{"id": "2502.11492", "pdf": "https://arxiv.org/pdf/2502.11492.pdf", "abs": "https://arxiv.org/abs/2502.11492", "title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding", "authors": ["Kung-Hsiang Huang", "Can Qin", "Haoyi Qiu", "Philippe Laban", "Shafiq Joty", "Caiming Xiong", "Chien-Sheng Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Code and data are available at\n  https://github.com/SalesforceAIResearch/CogAlign", "summary": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks."}
{"id": "2502.12085", "pdf": "https://arxiv.org/pdf/2502.12085.pdf", "abs": "https://arxiv.org/abs/2502.12085", "title": "APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs", "authors": ["Yuxiang Huang", "Mingye Li", "Xu Han", "Chaojun Xiao", "Weilin Zhao", "Sun Ao", "Hao Zhou", "Jie Zhou", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025 main", "summary": "While long-context inference is crucial for advancing large language model\n(LLM) applications, its prefill speed remains a significant bottleneck. Current\napproaches, including sequence parallelism strategies and compute reduction\nthrough approximate attention mechanisms, still fall short of delivering\noptimal inference efficiency. This hinders scaling the inputs to longer\nsequences and processing long-context queries in a timely manner. To address\nthis, we introduce APB, an efficient long-context inference framework that\nleverages multi-host approximate attention to enhance prefill speed by reducing\ncompute and enhancing parallelism simultaneously. APB introduces a\ncommunication mechanism for essential key-value pairs within a sequence\nparallelism framework, enabling a faster inference speed while maintaining task\nperformance. We implement APB by incorporating a tailored FlashAttn kernel\nalongside optimized distribution strategies, supporting diverse models and\nparallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x\ncompared with FlashAttn, RingAttn, and StarAttn, respectively, without any\nobservable task performance degradation. We provide the implementation and\nexperiment code of APB in https://github.com/thunlp/APB."}
{"id": "2502.12442", "pdf": "https://arxiv.org/pdf/2502.12442.pdf", "abs": "https://arxiv.org/abs/2502.12442", "title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation", "authors": ["Hao Liu", "Zhengren Wang", "Xi Chen", "Zhiyu Li", "Feiyu Xiong", "Qinhan Yu", "Wentao Zhang"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems often struggle with imperfect\nretrieval, as traditional retrievers focus on lexical or semantic similarity\nrather than logical relevance. To address this, we propose \\textbf{HopRAG}, a\nnovel RAG framework that augments retrieval with logical reasoning through\ngraph-structured knowledge exploration. During indexing, HopRAG constructs a\npassage graph, with text chunks as vertices and logical connections established\nvia LLM-generated pseudo-queries as edges. During retrieval, it employs a\n\\textit{retrieve-reason-prune} mechanism: starting with lexically or\nsemantically similar passages, the system explores multi-hop neighbors guided\nby pseudo-queries and LLM reasoning to identify truly relevant ones.\nExperiments on multiple multi-hop benchmarks demonstrate that HopRAG's\n\\textit{retrieve-reason-prune} mechanism can expand the retrieval scope based\non logical connections and improve final answer quality."}
{"id": "2502.12678", "pdf": "https://arxiv.org/pdf/2502.12678.pdf", "abs": "https://arxiv.org/abs/2502.12678", "title": "Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees", "authors": ["Yongtao Wu", "Luca Viano", "Yihang Chen", "Zhenyu Zhu", "Kimon Antonakopoulos", "Quanquan Gu", "Volkan Cevher"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has been highly successful\nin aligning large language models with human preferences. While prevalent\nmethods like DPO have demonstrated strong performance, they frame interactions\nwith the language model as a bandit problem, which limits their applicability\nin real-world scenarios where multi-turn conversations are common.\nAdditionally, DPO relies on the Bradley-Terry model assumption, which does not\nadequately capture the non-transitive nature of human preferences. In this\npaper, we address these challenges by modeling the alignment problem as a\ntwo-player constant-sum Markov game, where each player seeks to maximize their\nwinning rate against the other across all steps of the conversation. Our\napproach Optimistic Multi-step Preference Optimization (OMPO) is built upon the\noptimistic online mirror descent\nalgorithm~\\citep{rakhlin2013online,joulani17a}. Theoretically, we provide a\nrigorous analysis for the convergence of OMPO and show that OMPO requires\n$\\mathcal{O}(\\epsilon^{-1})$ policy updates to converge to an\n$\\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of\nour method on multi-turn conversations dataset and math reasoning dataset."}
{"id": "2502.13344", "pdf": "https://arxiv.org/pdf/2502.13344.pdf", "abs": "https://arxiv.org/abs/2502.13344", "title": "K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction", "authors": ["Tassallah Abdullahi", "Ioanna Gemou", "Nihal V. Nayak", "Ghulam Murtaza", "Stephen H. Bach", "Carsten Eickhoff", "Ritambhara Singh"], "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Biomedical knowledge graphs (KGs) encode rich, structured information\ncritical for drug discovery tasks, but extracting meaningful insights from\nlarge-scale KGs remains challenging due to their complex structure. Existing\nbiomedical subgraph retrieval methods are tailored for graph neural networks\n(GNNs), limiting compatibility with other paradigms, including large language\nmodels (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that\nextracts structured, diverse, and biologically meaningful multi-hop paths from\ndense biomedical KGs. These paths enable the prediction of unobserved drug-drug\nand drug-disease interactions, including those involving entities not seen\nduring training, thus supporting inductive reasoning. K-Paths is training-free\nand employs a diversity-aware adaptation of Yen's algorithm to extract the K\nshortest loopless paths between entities in a query, prioritizing biologically\nrelevant and relationally diverse connections. These paths serve as concise,\ninterpretable reasoning chains that can be directly integrated with LLMs or\nGNNs to improve generalization, accuracy, and enable explainable inference.\nExperiments on benchmark datasets show that K-Paths improves zero-shot\nreasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by\n19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing\ntasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same\ntasks. K-Paths also boosts the training efficiency of EmerGNN, a\nstate-of-the-art GNN, by reducing the KG size by 90% while maintaining\npredictive performance. Beyond efficiency, K-Paths bridges the gap between KGs\nand LLMs, enabling scalable and explainable LLM-augmented scientific discovery.\nWe release our code and the retrieved paths as a benchmark for inductive\nreasoning."}
{"id": "2502.18632", "pdf": "https://arxiv.org/pdf/2502.18632.pdf", "abs": "https://arxiv.org/abs/2502.18632", "title": "Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems", "authors": ["Zhangqi Duan", "Nigel Fernandez", "Arun Balajiee Lekshmi Narayanan", "Mohammad Hassany", "Rafaella Sampaio de Alencar", "Peter Brusilovsky", "Bita Akram", "Andrew Lan"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.SE"], "comment": null, "summary": "Knowledge components (KCs) mapped to problems help model student learning,\ntracking their mastery levels on fine-grained skills thereby facilitating\npersonalized learning and feedback in online learning platforms. However,\ncrafting and tagging KCs to problems, traditionally performed by human domain\nexperts, is highly labor-intensive. We present a fully automated, LLM-based\npipeline for KC generation and tagging for open-ended programming problems. We\nalso develop an LLM-based knowledge tracing (KT) framework to leverage these\nLLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive\nquantitative and qualitative evaluations on a real-world student code\nsubmission dataset. We find that KCGen-KT outperforms existing KT methods and\nhuman-written KCs on future student response prediction. We investigate the\nlearning curves of generated KCs and show that LLM-generated KCs result in a\nbetter fit than human-written KCs under a cognitive model. We also conduct a\nhuman evaluation with course instructors to show that our pipeline generates\nreasonably accurate problem-KC mappings."}
{"id": "2502.19400", "pdf": "https://arxiv.org/pdf/2502.19400.pdf", "abs": "https://arxiv.org/abs/2502.19400", "title": "TheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding", "authors": ["Max Ku", "Thomas Chong", "Jonathan Leung", "Krish Shah", "Alvin Yu", "Wenhu Chen"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "comment": "accepted to ACL 2025 main, camera ready", "summary": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations."}
{"id": "2502.19883", "pdf": "https://arxiv.org/pdf/2502.19883.pdf", "abs": "https://arxiv.org/abs/2502.19883", "title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models", "authors": ["Sibo Yi", "Tianshuo Cong", "Xinlei He", "Qi Li", "Jiaxing Song"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 findings", "summary": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs."}
{"id": "2503.01263", "pdf": "https://arxiv.org/pdf/2503.01263.pdf", "abs": "https://arxiv.org/abs/2503.01263", "title": "Generalizable Prompt Learning of CLIP: A Brief Overview", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xule Wang", "Liang Xiao"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing vision-language models (VLMs) such as CLIP have showcased an\nimpressive capability to generalize well across various downstream tasks. These\nmodels leverage the synergy between visual and textual information, enabling\nthem to understand and reason about the content present in images and text in a\nunified manner. This article provides a brief overview of CLIP based on\nfew-shot prompt learning, including experimental data and technical\ncharacteristics of some methods. The purpose of this review is to provide a\nreference for researchers who have just started their research in generalizable\nprompting of CLIP through few-shot training for classification across 15\ndatasets and also to facilitate the integration of this field by researchers in\nother downstream tasks."}
{"id": "2503.10542", "pdf": "https://arxiv.org/pdf/2503.10542.pdf", "abs": "https://arxiv.org/abs/2503.10542", "title": "Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More", "authors": ["Arvid Frydenlund"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.8; I.5.0"], "comment": "ACL 2025 Main. A camera-ready version will follow in a few weeks. A\n  reduced version of this work was also accepted to the Workshop on Spurious\n  Correlation and Shortcut Learning: Foundations and Solutions (SCSL) at ICLR\n  2025", "summary": "This work concerns the path-star task, a minimal example of searching over a\ngraph. The graph, $G$, is star-shaped with $D$ arms radiating from a start\nnode, $s$. A language model (LM) is given $G$, $s$, and a target node $t$,\nwhich ends one of the arms and is tasked with generating the arm containing\n$t$. The minimal nature of this task means only a single choice needs to be\nmade: which of the $D$ arms contains $t$?\n  Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to\na learned shortcut that absorbs training supervision. We show how this\npathology is caused by excess supervision and we present a series of solutions\ndemonstrating that the task is solvable via decoder-only LMs. We find that the\ntask's minimal nature causes its difficulty, as it prevents task decomposition.\nOur solutions provide insight into the pathology and its implications for LMs\ntrained via next-token prediction."}
{"id": "2503.13377", "pdf": "https://arxiv.org/pdf/2503.13377.pdf", "abs": "https://arxiv.org/abs/2503.13377", "title": "Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding", "authors": ["Ye Wang", "Ziheng Wang", "Boshen Xu", "Yang Du", "Kejun Lin", "Zihan Xiao", "Zihao Yue", "Jianzhong Ju", "Liang Zhang", "Dingyi Yang", "Xiangnan Fang", "Zewen He", "Zhenbo Luo", "Wenxuan Wang", "Junqi Lin", "Jian Luan", "Qin Jin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://xuboshen.github.io/Time-R1/", "summary": "Temporal Video Grounding (TVG), the task of locating specific video segments\nbased on language queries, is a core challenge in long-form video\nunderstanding. While recent Large Vision-Language Models (LVLMs) have shown\nearly promise in tackling TVG through supervised fine-tuning (SFT), their\nabilities to generalize remain limited. To address this, we propose a novel\npost-training framework that enhances the generalization capabilities of LVLMs\nvia reinforcement learning (RL). Specifically, our contributions span three key\ndirections: (1) Time-R1: we introduce a reasoning-guided post-training\nframework via RL with verifiable reward to enhance the capabilities of LVLMs on\nthe TVG task. (2) TimeRFT: we explore data-efficient post-training strategies\non our curated RL-friendly dataset, which trains the model to progressively\ncomprehend difficult samples, leading to better generalization. (3) TVGBench:\nwe carefully construct a small yet comprehensive benchmark for LVLM evaluation,\nassessing 11 types of queries and featuring balanced distributions across both\nvideos and queries. Extensive experiments demonstrate that Time-R1 achieves\nstate-of-the-art performance across multiple downstream datasets using only\n2.5K training data, while improving its general video understanding\ncapabilities."}
{"id": "2503.15798", "pdf": "https://arxiv.org/pdf/2503.15798.pdf", "abs": "https://arxiv.org/abs/2503.15798", "title": "Mixture of Lookup Experts", "authors": ["Shibo Jie", "Yehui Tang", "Kai Han", "Yitong Li", "Duyu Tang", "Zhi-Hong Deng", "Yunhe Wang"], "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025", "summary": "Mixture-of-Experts (MoE) activates only a subset of experts during inference,\nallowing the model to maintain low inference FLOPs and latency even as the\nparameter count scales up. However, since MoE dynamically selects the experts,\nall the experts need to be loaded into VRAM. Their large parameter size still\nlimits deployment, and offloading, which load experts into VRAM only when\nneeded, significantly increase inference latency. To address this, we propose\nMixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in\nboth communication and VRAM usage. In MoLE, the experts are Feed-Forward\nNetworks (FFNs) during training, taking the output of the embedding layer as\ninput. Before inference, these experts can be re-parameterized as lookup tables\n(LUTs) that retrieves expert outputs based on input ids, and offloaded to\nstorage devices. Therefore, we do not need to perform expert computations\nduring inference. Instead, we directly retrieve the expert's computation\nresults based on input ids and load them into VRAM, and thus the resulting\ncommunication overhead is negligible. Experiments show that, with the same\nFLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models\nand significantly faster than MoE with experts offloading, while maintaining\nperformance on par with MoE."}
{"id": "2503.23100", "pdf": "https://arxiv.org/pdf/2503.23100.pdf", "abs": "https://arxiv.org/abs/2503.23100", "title": "MoLAE: Mixture of Latent Experts for Parameter-Efficient Language Models", "authors": ["Zehua Liu", "Han Wu", "Ruifeng She", "Xiaojin Fu", "Xiongwei Han", "Tao Zhong", "Mingxuan Yuan"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) has become a key architectural paradigm for\nefficiently scaling Large Language Models (LLMs) by selectively activating a\nsubset of parameters for each input token. However, standard MoE architectures\nface significant challenges, including high memory consumption and\ncommunication overhead during distributed training. In this paper, we introduce\nMixture of Latent Experts (MoLAE), a novel parameterization that addresses\nthese limitations by reformulating expert operations through a shared\nprojection into a lower-dimensional latent space, followed by expert-specific\ntransformations. This factorized approach substantially reduces parameter count\nand computational requirements, particularly in existing LLMs where hidden\ndimensions significantly exceed MoE intermediate dimensions. We provide a\nrigorous mathematical framework for transforming pre-trained MoE models into\nMoLAE architecture, characterizing conditions for optimal factorization, and\ndeveloping a systematic two-step algorithm for this conversion. Our\ncomprehensive theoretical analysis demonstrates that MoLAE significantly\nimproves efficiency across multiple dimensions while preserving model\ncapabilities. Experimental results confirm that MoLAE achieves comparable\nperformance to standard MoE with substantially reduced resource requirements."}
{"id": "2504.01382", "pdf": "https://arxiv.org/pdf/2504.01382.pdf", "abs": "https://arxiv.org/abs/2504.01382", "title": "An Illusion of Progress? Assessing the Current State of Web Agents", "authors": ["Tianci Xue", "Weijian Qi", "Tianneng Shi", "Chan Hee Song", "Boyu Gou", "Dawn Song", "Huan Sun", "Yu Su"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 17 figures, 7 tables", "summary": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research."}
{"id": "2504.05652", "pdf": "https://arxiv.org/pdf/2504.05652.pdf", "abs": "https://arxiv.org/abs/2504.05652", "title": "Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking", "authors": ["Yu-Hang Wu", "Yu-Jie Xiong", "Hao Zhang", "Jia-Chen Zhang", "Zheng Zhou"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "With the increasingly deep integration of large language models (LLMs) across\ndiverse domains, the effectiveness of their safety mechanisms is encountering\nsevere challenges. Currently, jailbreak attacks based on prompt engineering\nhave become a major safety threat. However, existing methods primarily rely on\nblack-box manipulation of prompt templates, resulting in poor interpretability\nand limited generalization. To break through the bottleneck, this study first\nintroduces the concept of Defense Threshold Decay (DTD), revealing the\npotential safety impact caused by LLMs' benign generation: as benign content\ngeneration in LLMs increases, the model's focus on input instructions\nprogressively diminishes. Building on this insight, we propose the Sugar-Coated\nPoison (SCP) attack paradigm, which uses a \"semantic reversal\" strategy to\ncraft benign inputs that are opposite in meaning to malicious intent. This\nstrategy induces the models to generate extensive benign content, thereby\nenabling adversarial reasoning to bypass safety mechanisms. Experiments show\nthat SCP outperforms existing baselines. Remarkably, it achieves an average\nattack success rate of 87.23% across six LLMs. For defense, we propose\nPart-of-Speech Defense (POSD), leveraging verb-noun dependencies for syntactic\nanalysis to enhance safety of LLMs while preserving their generalization\nability."}
{"id": "2504.06766", "pdf": "https://arxiv.org/pdf/2504.06766.pdf", "abs": "https://arxiv.org/abs/2504.06766", "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark", "authors": ["Yuxin Wang", "Yiran Guo", "Yining Zheng", "Zhangyue Yin", "Shuo Chen", "Jie Yang", "Jiajun Chen", "Yuan Li", "Xuanjing Huang", "Xipeng Qiu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool, including base and extended datasets, challenges\nLLMs with queries spanning from 1 to 4 relational hops (e.g., inferring\nfamilial connections and preferences) and 2 to 6 hops respectively, and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\n\\href{https://github.com/yxzwang/FamilyTool}{https://github.com/yxzwang/FamilyTool}."}
{"id": "2504.08725", "pdf": "https://arxiv.org/pdf/2504.08725.pdf", "abs": "https://arxiv.org/abs/2504.08725", "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation", "authors": ["Dayu Yang", "Antoine Simoulin", "Xin Qian", "Xiaoyi Liu", "Yuwei Cao", "Zhaopu Teng", "Grey Yang"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ACL 2025. Code: github.com/facebookresearch/DocAgent", "summary": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."}
{"id": "2504.10893", "pdf": "https://arxiv.org/pdf/2504.10893.pdf", "abs": "https://arxiv.org/abs/2504.10893", "title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search", "authors": ["Yize Zhang", "Tianshu Wang", "Sirui Chen", "Kun Wang", "Xingyu Zeng", "Hongyu Lin", "Xianpei Han", "Le Sun", "Chaochao Lu"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Main. Project homepage:\n  https://opencausalab.github.io/ARise", "summary": "Large language models (LLMs) have demonstrated impressive capabilities and\nare receiving increasing attention to enhance their reasoning through scaling\ntest--time compute. However, their application in open--ended,\nknowledge--intensive, complex reasoning scenarios is still limited.\nReasoning--oriented methods struggle to generalize to open--ended scenarios due\nto implicit assumptions of complete world knowledge. Meanwhile,\nknowledge--augmented reasoning (KAR) methods fail to address two core\nchallenges: 1) error propagation, where errors in early steps cascade through\nthe chain, and 2) verification bottleneck, where the explore--exploit tradeoff\narises in multi--branch decision processes. To overcome these limitations, we\nintroduce ARise, a novel framework that integrates risk assessment of\nintermediate reasoning states with dynamic retrieval--augmented generation\n(RAG) within a Monte Carlo tree search paradigm. This approach enables\neffective construction and optimization of reasoning plans across multiple\nmaintained hypothesis branches. Experimental results show that ARise\nsignificantly outperforms the state--of--the--art KAR methods by up to 23.10%,\nand the latest RAG-equipped large reasoning models by up to 25.37%. Our project\npage is at https://opencausalab.github.io/ARise."}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191.pdf", "abs": "https://arxiv.org/abs/2504.14191", "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery."}
{"id": "2504.15266", "pdf": "https://arxiv.org/pdf/2504.15266.pdf", "abs": "https://arxiv.org/abs/2504.15266", "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025 (spotlight)", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\nmulti-token approaches, namely teacherless training and diffusion models,\ncomparatively excel in producing diverse and original output. Secondly, to\nelicit randomness without hurting coherence, we find that injecting noise at\nthe input layer (dubbed as seed-conditioning) works surprisingly as well as\n(and in some conditions, better than) temperature sampling from the output\nlayer. Thus, our work offers a principled, minimal test-bed for analyzing\nopen-ended creative skills, and offers new arguments for going beyond\nnext-token learning and temperature sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity"}
{"id": "2504.16728", "pdf": "https://arxiv.org/pdf/2504.16728.pdf", "abs": "https://arxiv.org/abs/2504.16728", "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "ACL 2025 (System Demonstration Track)", "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System"}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073.pdf", "abs": "https://arxiv.org/abs/2504.20073", "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Xing Jin", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on four stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and gradient stabilization. Second, we find the shaping\nof RL rollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN."}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571.pdf", "abs": "https://arxiv.org/abs/2504.20571", "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Liyuan Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "36 pages, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the mathematical reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Furthermore,\nRLVR with only two examples even slightly exceeds these results (MATH500:\n74.8%, average: 36.6%). Similar substantial improvements are observed across\nvarious models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct,\nDeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different\nmath examples (when employed as a single training example). In addition, we\nidentify some interesting phenomena during 1-shot RLVR, including cross-domain\ngeneralization, increased frequency of self-reflection, and sustained test\nperformance improvement even after the training accuracy has saturated, a\nphenomenon we term post-saturation generalization. Moreover, we verify that the\neffectiveness of 1-shot RLVR primarily arises from the policy gradient loss,\ndistinguishing it from the \"grokking\" phenomenon. We also show the critical\nrole of promoting exploration (e.g., by incorporating entropy loss with an\nappropriate coefficient) in 1-shot RLVR training. We also further discuss\nrelated observations about format correction, label robustness and prompt\nmodification. These findings can inspire future work on RLVR efficiency and\nencourage a re-examination of recent progress and the underlying mechanisms in\nRLVR. Our code, model, and data are open source at\nhttps://github.com/ypwang61/One-Shot-RLVR."}
{"id": "2505.00212", "pdf": "https://arxiv.org/pdf/2505.00212.pdf", "abs": "https://arxiv.org/abs/2505.00212", "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems", "authors": ["Shaokun Zhang", "Ming Yin", "Jieyu Zhang", "Jiale Liu", "Zhiguang Han", "Jingyang Zhang", "Beibin Li", "Chi Wang", "Huazheng Wang", "Yiran Chen", "Qingyun Wu"], "categories": ["cs.MA", "cs.CL"], "comment": "revise affiliation. indicate ICML processed", "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution"}
{"id": "2505.03273", "pdf": "https://arxiv.org/pdf/2505.03273.pdf", "abs": "https://arxiv.org/abs/2505.03273", "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation", "authors": ["Zhaoxi Mu", "Xinyu Yang", "Gang Wang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Appears in IJCAI 2025", "summary": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments."}
{"id": "2505.06843", "pdf": "https://arxiv.org/pdf/2505.06843.pdf", "abs": "https://arxiv.org/abs/2505.06843", "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety", "authors": ["Zihan Guan", "Mengxuan Hu", "Ronghang Zhu", "Sheng Li", "Anil Vullikanti"], "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 13 figures", "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning\nstage of large language models (LLMs): even fine-tuning on entirely benign\ndatasets can lead to a significant increase in the harmfulness of LLM outputs.\nBuilding on this finding, our red teaming study takes this threat one step\nfurther by developing a more effective attack. Specifically, we analyze and\nidentify samples within benign datasets that contribute most to safety\ndegradation, then fine-tune LLMs exclusively on these samples. We approach this\nproblem from an outlier detection perspective and propose Self-Inf-N, to detect\nand extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs\non 100 outlier samples selected by Self-Inf-N in the benign datasets severely\ncompromises LLM safety alignment. Extensive experiments across seven mainstream\nLLMs demonstrate that our attack exhibits high transferability across different\narchitectures and remains effective in practical scenarios. Alarmingly, our\nresults indicate that most existing mitigation strategies fail to defend\nagainst this attack, underscoring the urgent need for more robust alignment\nsafeguards. Codes are available at\nhttps://github.com/GuanZihan/Benign-Samples-Matter."}
{"id": "2505.08704", "pdf": "https://arxiv.org/pdf/2505.08704.pdf", "abs": "https://arxiv.org/abs/2505.08704", "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs", "authors": ["K M Sajjadul Islam", "Ayesha Siddika Nipu", "Jiawei Wu", "Praveen Madiraju"], "categories": ["cs.AI", "cs.CL"], "comment": "IEEE 26th International Conference on Information Reuse and\n  Integration for Data Science (IRI 2025), San Jose, CA, USA", "summary": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting."}
{"id": "2505.11365", "pdf": "https://arxiv.org/pdf/2505.11365.pdf", "abs": "https://arxiv.org/abs/2505.11365", "title": "Phare: A Safety Probe for Large Language Models", "authors": ["Pierre Le Jeune", "Benoît Malézieux", "Weixuan Xiao", "Matteo Dora"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731.pdf", "abs": "https://arxiv.org/abs/2505.11731", "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269.pdf", "abs": "https://arxiv.org/abs/2505.12269", "title": "Vague Knowledge: Evidence from Analyst Reports", "authors": ["Kerry Xiao", "Amy Zang"], "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but\nless-known role in representing subjective expectations. Empirically, we find\nthat in their reports, analysts include useful information in linguistic\nexpressions but not numerical forecasts. Specifically, the textual tone of\nanalyst reports has predictive power for forecast errors and subsequent\nrevisions in numerical forecasts, and this relation becomes stronger when\nanalyst's language is vaguer, when uncertainty is higher, and when analysts are\nbusier. Overall, our theory and evidence suggest that some useful information\nis vaguely known and only communicated through language."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992.pdf", "abs": "https://arxiv.org/abs/2505.12992", "title": "Fractured Chain-of-Thought Reasoning", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning. Code is available at\nhttps://github.com/BaohaoLiao/frac-cot."}
{"id": "2505.13718", "pdf": "https://arxiv.org/pdf/2505.13718.pdf", "abs": "https://arxiv.org/abs/2505.13718", "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings", "authors": ["Safal Shrestha", "Minwu Kim", "Aadim Nepal", "Anubhav Shrestha", "Keith Ross"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: $(i)$ the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro; $(ii)$ When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset ($\\leq100$ examples), the warmed-up model consistently outperforms the\nbase model; $(iii)$ Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; $(iv)$\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments."}
{"id": "2505.13862", "pdf": "https://arxiv.org/pdf/2505.13862.pdf", "abs": "https://arxiv.org/abs/2505.13862", "title": "PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety."}
{"id": "2505.14814", "pdf": "https://arxiv.org/pdf/2505.14814.pdf", "abs": "https://arxiv.org/abs/2505.14814", "title": "GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples", "authors": ["Harry Zhang", "Kurt Partridge", "Pai Zhu", "Neng Chen", "Hyun Jin Park", "Dhruuv Agarwal", "Quan Wang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Spoken Keyword Spotting (KWS) is the task of distinguishing between the\npresence and absence of a keyword in audio. The accuracy of a KWS model hinges\non its ability to correctly classify examples close to the keyword and\nnon-keyword boundary. These boundary examples are often scarce in training\ndata, limiting model performance. In this paper, we propose a method to\nsystematically generate adversarial examples close to the decision boundary by\nmaking insertion/deletion/substitution edits on the keyword's graphemes. We\nevaluate this technique on held-out data for a popular keyword and show that\nthe technique improves AUC on a dataset of synthetic hard negatives by 61%\nwhile maintaining quality on positives and ambient negative audio data."}
{"id": "2505.15259", "pdf": "https://arxiv.org/pdf/2505.15259.pdf", "abs": "https://arxiv.org/abs/2505.15259", "title": "ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search", "authors": ["Hyunseok Lee", "Jeonghoon Kim", "Beomjun Kim", "Jihoon Tack", "Chansong Jo", "Jaehong Lee", "Cheonbok Park", "Sookyo In", "Jinwoo Shin", "Kang Min Yoo"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled\nautonomous agents to interact with computers via Graphical User Interfaces\n(GUIs), where accurately localizing the coordinates of interface elements\n(e.g., buttons) is often required for fine-grained actions. However, this\nremains significantly challenging, leading prior works to rely on large-scale\nweb datasets to improve the grounding accuracy. In this work, we propose\nReasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a\nnovel and effective framework for web grounding that enables MLLMs to learn\ndata efficiently through self-generated reasoning and spatial-aware criticism.\nMore specifically, ReGUIDE learns to (i) self-generate a language reasoning\nprocess for the localization via online reinforcement learning, and (ii)\ncriticize the prediction using spatial priors that enforce equivariance under\ninput transformations. At inference time, ReGUIDE further boosts performance\nthrough a test-time scaling strategy, which combines spatial search with\ncoordinate aggregation. Our experiments demonstrate that ReGUIDE significantly\nadvances web grounding performance across multiple benchmarks, outperforming\nbaselines with substantially fewer training data points (e.g., only 0.2%\nsamples compared to the best open-sourced baselines)."}
{"id": "2505.15489", "pdf": "https://arxiv.org/pdf/2505.15489.pdf", "abs": "https://arxiv.org/abs/2505.15489", "title": "Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models", "authors": ["Jiaying Wu", "Fanxiao Li", "Min-Yen Kan", "Bryan Hooi"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "The real-world impact of misinformation stems from the underlying misleading\nnarratives that creators seek to convey. As such, interpreting misleading\ncreator intent is essential for multimodal misinformation detection (MMD)\nsystems aimed at effective information governance. In this paper, we introduce\nan automated framework that simulates real-world multimodal news creation by\nexplicitly modeling creator intent through two components: the desired\ninfluence and the execution plan. Using this framework, we construct\nDeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs\naligned with trustworthy reference articles. The dataset captures both\nmisleading and non-misleading intents and spans manipulations across visual and\ntextual modalities. We conduct a comprehensive evaluation of 14\nstate-of-the-art vision-language models (VLMs) on three intent-centric tasks:\n(1) misleading intent detection, (2) misleading source attribution, and (3)\ncreator desire inference. Despite recent advances, we observe that current VLMs\nfall short in recognizing misleading intent, often relying on spurious cues\nsuch as superficial cross-modal consistency, stylistic signals, and heuristic\nauthenticity hints. Our findings highlight the pressing need for intent-aware\nmodeling in MMD and open new directions for developing systems capable of\ndeeper reasoning about multimodal misinformation."}
{"id": "2505.15966", "pdf": "https://arxiv.org/pdf/2505.15966.pdf", "abs": "https://arxiv.org/abs/2505.15966", "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning", "authors": ["Alex Su", "Haozhe Wang", "Weiming Ren", "Fangzhen Lin", "Wenhu Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://tiger-ai-lab.github.io/Pixel-Reasoner/,\n  Hands-on Demo: https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner", "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework."}
{"id": "2505.16938", "pdf": "https://arxiv.org/pdf/2505.16938.pdf", "abs": "https://arxiv.org/abs/2505.16938", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.65 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours."}
{"id": "2505.16968", "pdf": "https://arxiv.org/pdf/2505.16968.pdf", "abs": "https://arxiv.org/abs/2505.16968", "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "comment": "20 pages, 11 figures, 5 tables", "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\n$\\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\\leftrightarrow$ AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."}
