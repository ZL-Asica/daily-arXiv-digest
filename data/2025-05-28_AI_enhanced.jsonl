{"id": "2505.20464", "pdf": "https://arxiv.org/pdf/2505.20464.pdf", "abs": "https://arxiv.org/abs/2505.20464", "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions", "authors": ["Samuel Rhys Cox", "Rune MÃ¸berg Jacobsen", "Niels van Berkel"], "categories": ["cs.HC", "cs.CL"], "comment": "In ACM Conversational User Interfaces (CUI '25), July 8-10, 2025; 18\n  pages; 6 Figures; 6 Tables", "summary": "Self-disclosure, the sharing of one's thoughts and feelings, is affected by\nthe perceived relationship between individuals. While chatbots are increasingly\nused for self-disclosure, the impact of a chatbot's framing on users'\nself-disclosure remains under-explored. We investigated how a chatbot's\ndescription of its relationship with users, particularly in terms of\nephemerality, affects self-disclosure. Specifically, we compared a Familiar\nchatbot, presenting itself as a companion remembering past interactions, with a\nStranger chatbot, presenting itself as a new, unacquainted entity in each\nconversation. In a mixed factorial design, participants engaged with either the\nFamiliar or Stranger chatbot in two sessions across two days, with one\nconversation focusing on Emotional- and another Factual-disclosure. When\nEmotional-disclosure was sought in the first chatting session,\nStranger-condition participants felt more comfortable self-disclosing. However,\nwhen Factual-disclosure was sought first, these differences were replaced by\nmore enjoyment among Familiar-condition participants. Qualitative findings\nshowed Stranger afforded anonymity and reduced judgement, whereas Familiar\nsometimes felt intrusive unless rapport was built via low-risk\nFactual-disclosure.", "AI": {"tldr": "The paper explores how the framing of chatbot relationships affects self-disclosure, comparing Familiar and Stranger chatbots in emotional and factual contexts.", "motivation": "To understand the impact of chatbot relationships on users' willingness to share personal thoughts and feelings, particularly in the context of self-disclosure.", "method": "A mixed factorial design was employed where participants interacted with either a Familiar or Stranger chatbot over two sessions, focusing on Emotional and Factual self-disclosure.", "result": "Stranger-condition participants felt more comfortable disclosing emotions initially, while the Familiar-condition participants enjoyed the interaction more when factual disclosure was prioritized.", "conclusion": "The study reveals that the type of relationship framed by chatbots affects self-disclosure, with anonymity offered by Stranger chatbots enhancing comfort and Familiar chatbots potentially feeling intrusive without prior rapport.", "key_contributions": ["Investigated the effect of chatbot relationship framing on self-disclosure.", "Demonstrated nuanced impacts in emotional vs. factual disclosure contexts.", "Provided qualitative insights into user perceptions of anonymity and judgement."], "limitations": "The study may not generalize outside the contexts tested or account for all factors influencing self-disclosure.", "keywords": ["self-disclosure", "chatbots", "human-computer interaction", "relationships", "privacy"], "importance_score": 7, "read_time_minutes": 18}}
{"id": "2505.20585", "pdf": "https://arxiv.org/pdf/2505.20585.pdf", "abs": "https://arxiv.org/abs/2505.20585", "title": "HOT-FIT-BR: A Context-Aware Evaluation Framework for Digital Health Systems in Resource-Limited Settings", "authors": ["Ben Rahman"], "categories": ["cs.HC", "cs.CY"], "comment": "This work proposes HOT-FIT-BR, an extended evaluation framework for\n  digital health systems in LMICs. It includes infrastructure readiness, policy\n  compliance, and community engagement metrics-validated in Indonesian primary\n  healthcare settings", "summary": "Implementation of digital health systems in low-middle-income countries\n(LMICs) often fails due to a lack of evaluations that take into account\ninfrastructure limitations, local policies, and community readiness. We\nintroduce HOT-FIT-BR, a contextual evaluation framework that expands the\nHOT-FIT model with three new dimensions: (1) Infrastructure Index to measure\nelectricity/internet availability, (2) Policy Compliance Layer to ensure\nregulatory compliance (e.g., Permenkes 24/2022 in Indonesia), and (3) Community\nEngagement Fit. Simulations at Indonesian Health Centers show that HOT-FIT-BR\nis 58% more sensitive to detecting problems than HOT-FIT, especially in rural\nareas with an Infra Index <3. The framework has also proven adaptive to the\ncontext of other LMICs such as India and Kenya through local parameter\nadjustments.", "AI": {"tldr": "The paper introduces HOT-FIT-BR, a contextual evaluation framework for digital health systems in low-middle-income countries, addressing infrastructure, policy, and community factors.", "motivation": "Digital health system implementations in LMICs often fail due to inadequate evaluations that do not consider local contexts, necessitating the development of a more holistic evaluation framework.", "method": "The HOT-FIT-BR framework integrates an Infrastructure Index, a Policy Compliance Layer, and a Community Engagement Fit, validated through simulations in Indonesian Health Centers.", "result": "HOT-FIT-BR demonstrated a 58% increase in sensitivity for detecting problems compared to the original HOT-FIT model, particularly in rural settings with low infrastructure scores.", "conclusion": "The framework is adaptable to various LMIC contexts, as shown by local parameter adjustments in countries like India and Kenya.", "key_contributions": ["Introduction of the Infrastructure Index for evaluating digital health systems.", "Inclusion of a Policy Compliance Layer to address regulatory factors.", "Demonstration of increased sensitivity in evaluations, especially in rural areas."], "limitations": "", "keywords": ["digital health", "LMIC", "evaluation framework", "community engagement", "policy compliance"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.20623", "pdf": "https://arxiv.org/pdf/2505.20623.pdf", "abs": "https://arxiv.org/abs/2505.20623", "title": "Institutionalizing Folk Theories of Algorithms: How Multi-Channel Networks (MCNs) Govern Algorithmic Labor in Chinese Live-Streaming Industry", "authors": ["Qing Xiao", "Rongyi Chen", "Jingjia Xiao", "Tianyang Fu", "Alice Qian Zhang", "Xianzhe Fan", "Bingbing Zhang", "Zhicong Lu", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "28 pages, 2 figures", "summary": "As algorithmic systems increasingly structure platform labor, workers often\nrely on informal \"folk theories\", experience-based beliefs about how algorithms\nwork, to navigate opaque and unstable algorithmic environments. Prior research\nhas largely treated these theories as bottom-up, peer-driven strategies for\ncoping with algorithmic opacity and uncertainty. In this study, we shift\nanalytical attention to intermediary organizations and examine how folk\ntheories of algorithms can be institutionally constructed and operationalized\nby those organizations as tools of labor management. Drawing on nine months of\nethnographic fieldwork and 37 interviews with live-streamers and staff at\nMulti-Channel Networks (MCNs) in China, we show that MCNs develop and circulate\ndual algorithmic theories: internally, they acknowledge the volatility of\nplatform systems and adopt probabilistic strategies to manage risk; externally,\nthey promote simplified, prescriptive theories portraying the algorithm as\ntransparent, fair, and responsive to individual effort. They have further\noperationalize those folk theories for labor management, encouraging streamers\nto self-discipline and invest in equipment, training, and routines, while\nabsolving MCNs of accountability. We contribute to CSCW and platform labor\nliterature by demonstrating how informal algorithmic knowledge, once\ninstitutionalized, can become infrastructures of soft control -- shaping not\nonly how workers interpret platform algorithms, but also how their labor is\nstructured, moralized and governed.", "AI": {"tldr": "This study examines how intermediary organizations like Multi-Channel Networks (MCNs) in China construct and operationalize folk theories of algorithms to manage platform labor, revealing the dual narratives they promote.", "motivation": "To explore the role of intermediary organizations in shaping workers' understanding of algorithms in platform labor environments.", "method": "The study is based on nine months of ethnographic fieldwork and 37 interviews with live-streamers and MCN staff in China.", "result": "MCNs create dual algorithmic theories, recognizing algorithm volatility internally while presenting simplified theories externally that promote transparency and fairness, thus influencing labor management.", "conclusion": "Institutionalized folk theories can act as infrastructures of soft control, affecting how workers view platform algorithms and structuring their labor.", "key_contributions": ["Investigates the role of intermediary organizations in platform labor", "Reveals the dual narratives of algorithmic theories promulgated by MCNs", "Demonstrates the impact of institutionalized folk theories on labor management"], "limitations": "", "keywords": ["algorithmic systems", "folk theories", "platform labor", "Multi-Channel Networks", "ethnographic study"], "importance_score": 7, "read_time_minutes": 28}}
{"id": "2505.20692", "pdf": "https://arxiv.org/pdf/2505.20692.pdf", "abs": "https://arxiv.org/abs/2505.20692", "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity.", "AI": {"tldr": "This paper examines societal biases in text-to-image (T2I) generation models and proposes a rubric for bias detection and a Social Stereotype Index (SSI) to evaluate T2I outputs, revealing that bias can be reduced with prompt refinement but may impact contextual alignment.", "motivation": "To address the ethical concerns arising from societal stereotypes in T2I models that can amplify biases related to gender, race, and culture.", "method": "The paper introduces a bias detection rubric and a Social Stereotype Index (SSI) to assess the outputs of major T2I models. It includes an audit of DALL-E-3, Midjourney-6.1, and Stability AI Core using 100 queries across three categories and employs LLMs for targeted prompt refinement.", "result": "The analysis showed that T2I models often produce outputs with stereotypical visual cues, with significant reductions in bias (SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries) through refined prompts.", "conclusion": "While prompt refinement can reduce biases, it may also limit contextual alignment; there's a tension between ethical debiasing and the representation of social realities, highlighting the need for inclusive T2I systems.", "key_contributions": ["Introduction of a bias detection rubric for T2I outputs.", "Development of a Social Stereotype Index (SSI) for systematic evaluation of biases.", "Empirical findings on the user perception of AI-generated biased imagery and the impact of prompt refinement."], "limitations": "The study may not cover all biases present in varied cultural contexts and relies on user perception, which can be subjective.", "keywords": ["Generative AI", "Text-to-Image Generation", "Bias Detection", "Social Stereotypes", "User Perception"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20309", "pdf": "https://arxiv.org/pdf/2505.20309.pdf", "abs": "https://arxiv.org/abs/2505.20309", "title": "Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs", "authors": ["Amr Hegazy", "Mostafa Elhoushi", "Amr Alanwar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Controlling undesirable Large Language Model (LLM) behaviors, such as the\ngeneration of unsafe content or failing to adhere to safety guidelines, often\nrelies on costly fine-tuning. Activation steering provides an alternative for\ninference-time control, but existing methods typically lack fine-grained,\nadaptive mechanisms. We introduce a novel approach using a lightweight,\ntrainable controller network integrated during inference. This controller\nnetwork observes specific intermediate LLM activations and predicts both a\nglobal scaling factor and layer-specific weights. The predicted global scaling\nfactor and layer-specific weights then dynamically modulate the intensity of a\nsteering patch, derived from a pre-computed \"refusal direction\" vector, applied\nacross the LLM's layers during generation. Trained on activations from both\nharmful and benign prompts, our controller learns to discriminatively apply\nnuanced, layer-aware interventions, activating steering primarily for harmful\ninputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild\nJailbreak Prompts demonstrate that our weighted steering controller\nsignificantly increases refusal rates compared to the base LLM, achieving\ntargeted behavioral modification without altering the original model\nparameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show\nour approach outperforms existing methods, presenting an efficient and adaptive\nmethod for fine-grained control over LLM behavior at inference time.", "AI": {"tldr": "Introducing a trainable controller network for fine-grained control over Large Language Model (LLM) behaviors during inference, enhancing safety without the need for fine-tuning.", "motivation": "To control undesirable LLM behaviors such as generating unsafe content without incurring the costs of fine-tuning.", "method": "A lightweight, trainable controller network observes LLM activations and predicts a global scaling factor and layer-specific weights that modulate a steering patch during generation.", "result": "The controller significantly increases refusal rates on safety benchmarks compared to the base LLM, achieving targeted behavioral modification without altering model parameters.", "conclusion": "This method demonstrates efficient, adaptive control of LLM behavior at inference time, outperforming existing techniques.", "key_contributions": ["Introduction of a trainable controller network for LLMs", "Ability to dynamically modulate LLM behavior during inference", "Demonstrated improvements on safety benchmarks compared to base models."], "limitations": "", "keywords": ["Large Language Models", "activation steering", "safety control", "inference-time intervention", "adaptive mechanisms"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20711", "pdf": "https://arxiv.org/pdf/2505.20711.pdf", "abs": "https://arxiv.org/abs/2505.20711", "title": "Automating eHMI Action Design with LLMs for Automated Vehicle Communication", "authors": ["Ding Xia", "Xinyue Gui", "Fan Gao", "Dongyuan Li", "Mark Colley", "Takeo Igarashi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "The absence of explicit communication channels between automated vehicles\n(AVs) and other road users requires the use of external Human-Machine\nInterfaces (eHMIs) to convey messages effectively in uncertain scenarios.\nCurrently, most eHMI studies employ predefined text messages and manually\ndesigned actions to perform these messages, which limits the real-world\ndeployment of eHMIs, where adaptability in dynamic scenarios is essential.\nGiven the generalizability and versatility of large language models (LLMs),\nthey could potentially serve as automated action designers for the\nmessage-action design task. To validate this idea, we make three contributions:\n(1) We propose a pipeline that integrates LLMs and 3D renderers, using LLMs as\naction designers to generate executable actions for controlling eHMIs and\nrendering action clips. (2) We collect a user-rated Action-Design Scoring\ndataset comprising a total of 320 action sequences for eight intended messages\nand four representative eHMI modalities. The dataset validates that LLMs can\ntranslate intended messages into actions close to a human level, particularly\nfor reasoning-enabled LLMs. (3) We introduce two automated raters, Action\nReference Score (ARS) and Vision-Language Models (VLMs), to benchmark 18 LLMs,\nfinding that the VLM aligns with human preferences yet varies across eHMI\nmodalities.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) to enhance external Human-Machine Interfaces (eHMIs) for automated vehicles (AVs), proposing a pipeline for creating adaptable action designs based on user-rated datasets.", "motivation": "The need for effective communication between automated vehicles and road users, particularly in uncertain scenarios, necessitates adaptable external Human-Machine Interfaces.", "method": "The paper proposes a pipeline integrating LLMs with 3D renderers to generate executable actions for eHMIs, validated by a user-rated Action-Design Scoring dataset and automated raters to benchmark the performance of different LLMs.", "result": "LLMs can translate intended messages into actions with human-like competency, particularly for reasoning-enabled models, as evidenced by the collected dataset and automated rating methods.", "conclusion": "The integration of LLMs in eHMI design demonstrates significant potential for scalable and adaptable communication approaches in AVs, with varying effectiveness across different modalities.", "key_contributions": ["Proposed an LLM-based pipeline for generating eHMI actions and sequences.", "Collected a user-rated dataset of action sequences to validate LLM capability in action design.", "Introduced automated raters for benchmarking LLM performance in eHMI message-action conversion."], "limitations": "The applicability of the findings may vary across diverse eHMI modalities, and further exploration is needed in real-world scenarios.", "keywords": ["Automated Vehicles", "Human-Machine Interfaces", "Large Language Models", "Action Design", "Ubiquitous Computing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20315", "pdf": "https://arxiv.org/pdf/2505.20315.pdf", "abs": "https://arxiv.org/abs/2505.20315", "title": "Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL", "authors": ["Zhewei Yao", "Guoheng Sun", "Lukasz Borchmann", "Zheyu Shen", "Minghang Deng", "Bohan Zhai", "Hao Zhang", "Ang Li", "Yuxiong He"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 2 figures", "summary": "Translating natural language into SQL (Test2SQL) is a longstanding challenge\nat the intersection of natural language understanding and structured data\naccess. While large language models (LLMs) have significantly improved fluency\nin SQL generation, producing correct and executable SQL--particularly for\ncomplex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a\nreinforcement learning (RL) framework and model family designed to generate\naccurate, executable SQL using a lightweight reward signal based solely on\nexecution correctness. Our approach avoids brittle intermediate supervision and\ncomplex reward shaping, promoting stable training and alignment with the end\ntask. Combined with carefully curated data, strong supervised initialization,\nand effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art\nexecution accuracy across six diverse Test2SQL benchmarks, including the top\nposition on the BIRD leaderboard. Notably, our 7B model outperforms prior\n70B-class systems, highlighting the framework's scalability and efficiency. We\nfurther demonstrate inference-time robustness through simple extensions like\nvalue retrieval and majority voting. Extensive experiments and ablation studies\noffer both positive and negative insights, providing practical guidance for\nfuture Test2SQL research.", "AI": {"tldr": "Introducing Arctic-Text2SQL-R1, a reinforcement learning framework for generating accurate SQL from natural language queries, showing superior performance on multiple benchmarks.", "motivation": "The paper addresses the challenge of translating natural language into executable SQL queries, especially for complex queries where traditional methods struggle.", "method": "The proposed Arctic-Text2SQL-R1 uses a reinforcement learning framework focused on execution correctness as a reward signal, avoiding intermediary supervision.", "result": "The method achieves state-of-the-art execution accuracy on six Test2SQL benchmarks and demonstrates that a 7B model surpasses larger 70B models in performance.", "conclusion": "Arctic-Text2SQL-R1's design enhances scalability and efficiency in generating SQL from natural language, also providing insights for future research.", "key_contributions": ["Introduction of a reinforcement learning framework for SQL generation", "State-of-the-art performance on diverse Test2SQL benchmarks", "Demonstration of scalability with a smaller model outperforming larger counterparts"], "limitations": "", "keywords": ["natural language processing", "SQL generation", "reinforcement learning", "large language models", "Test2SQL"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20727", "pdf": "https://arxiv.org/pdf/2505.20727.pdf", "abs": "https://arxiv.org/abs/2505.20727", "title": "What Shapes Writers' Decisions to Disclose AI Use?", "authors": ["Jingchao Fang", "Mina Lee"], "categories": ["cs.HC"], "comment": "Navigating Generative AI Disclosure, Ownership, and Accountability in\n  Co-Creative Domains Workshop at CHIWORK 2025", "summary": "Have you ever read a blog or social media post and suspected that it was\nwritten--at least in part--by artificial intelligence (AI)? While transparently\nacknowledging contributors to writing is generally valued, why some writers\nchoose to disclose or withhold AI involvement remains unclear. In this work, we\nask what factors shape writers' decisions to disclose their AI use as a\nstarting point to effectively advocate for transparency. To shed light on this\nquestion, we synthesize study findings and theoretical frameworks in human-AI\ninteraction and behavioral science. Concretely, we identify and curate a list\nof factors that could affect writers' decisions regarding disclosure for\nhuman-AI co-created content.", "AI": {"tldr": "This paper explores the factors influencing writers' decisions to disclose their use of AI in co-created content, aiming to promote transparency in human-AI collaboration.", "motivation": "Understanding the reasons behind writers' choices to disclose or withhold AI involvement in their content is essential for advocating for transparency in human-AI co-creation.", "method": "The study synthesizes findings and theoretical frameworks from human-AI interaction and behavioral science to identify factors that affect disclosure decisions.", "result": "A curated list of factors influencing the decision to disclose AI involvement in content creation has been identified, which can inform future transparency initiatives.", "conclusion": "Promoting transparency in AI-assisted writing content requires a better understanding of the diverse factors influencing writers' decisions to disclose AI use.", "key_contributions": ["Identification of factors influencing writers' decisions regarding AI disclosure", "Synthesis of study findings within human-AI interaction frameworks", "Contribution to the discourse on transparency in AI-generated content"], "limitations": "", "keywords": ["AI Disclosure", "Human-AI Interaction", "Co-Creation", "Transparency", "Behavioral Science"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20318", "pdf": "https://arxiv.org/pdf/2505.20318.pdf", "abs": "https://arxiv.org/abs/2505.20318", "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations", "authors": ["Wang Cai", "Hsiu-Yuan Huang", "Zhixiang Wang", "Yunfang Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context derived Vector (ICV) methods extract task-relevant representations\nfrom large language models (LLMs) and reinject them during inference, achieving\ncomparable performance to few-shot In-Context Learning (ICL) without repeated\ndemonstration processing. However, existing ICV methods remain sensitive to\nICL-specific factors, often use coarse or semantically fragmented\nrepresentations as the source of the vector, and rely on heuristic-based\ninjection positions, limiting their applicability.\n  To address these issues, we propose Dynamic Vector (DyVec), which\nincorporates an Exhaustive Query Rotation (EQR) strategy to extract robust\nsemantically aggregated latent representations by mitigating variance\nintroduced by ICL. It then applies Dynamic Latent Segmentation and Injection to\nadaptively partition representations based on task complexity and leverages\nREINFORCE-based optimization to learn optimal injection positions for each\nsegment.\n  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior\nICV baselines. Further analysis highlights the effectiveness of dynamically\nsegmenting and injecting semantically aggregated latent representations. DyVec\nprovides a lightweight and data-efficient solution for inference-time task\nadaptation.", "AI": {"tldr": "DyVec introduces a novel method for extracting and injecting task-relevant representations from LLMs, outperforming existing few-shot learning methods.", "motivation": "Existing ICV methods are limited by sensitivity to ICL-specific factors and suboptimal representation injection positions, necessitating a more robust and adaptable approach for inference.", "method": "The method employs an Exhaustive Query Rotation strategy to extract aggregated representations, Dynamic Latent Segmentation for partitioning based on task complexity, and REINFORCE-based optimization for selecting optimal injection positions.", "result": "DyVec demonstrated improved performance compared to few-shot ICL and prior ICV methods, proving the efficacy of its dynamic representation approach.", "conclusion": "DyVec offers a lightweight, data-efficient solution for enhancing task adaptation during inference, addressing key limitations of traditional methods.", "key_contributions": ["Introduction of Dynamic Vector (DyVec) method", "Utilization of Exhaustive Query Rotation for robust representation extraction", "Application of Dynamic Latent Segmentation for task-based representation partitioning"], "limitations": "", "keywords": ["Dynamic Vector", "In-Context Learning", "Latent Representation", "Reinforcement Learning", "Task Adaptation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20788", "pdf": "https://arxiv.org/pdf/2505.20788.pdf", "abs": "https://arxiv.org/abs/2505.20788", "title": "Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset", "authors": ["Robin Burchard", "Kristof Van Laerhoven"], "categories": ["cs.HC", "cs.LG"], "comment": "Submitted to ISWC 2025", "summary": "Wearable human activity recognition has been shown to benefit from the\ninclusion of acoustic data, as the sounds around a person often contain\nvaluable context. However, due to privacy concerns, it is usually not ethically\nfeasible to record and save microphone data from the device, since the audio\ncould, for instance, also contain private conversations. Rather, the data\nshould be processed locally, which in turn requires processing power and\nconsumes energy on the wearable device. One special use case of contextual\ninformation that can be utilized to augment special tasks in human activity\nrecognition is water flow detection, which can, e.g., be used to aid wearable\nhand washing detection. We created a new label called tap water for the\nrecently released HD-Epic data set, creating 717 hand-labeled annotations of\ntap water flow, based on existing annotations of the water class. We analyzed\nthe relation of tap water and water in the dataset and additionally trained and\nevaluated two lightweight classifiers to evaluate the newly added label class,\nshowing that the new class can be learned more easily.", "AI": {"tldr": "This paper introduces a new label for human activity recognition using acoustic data, specifically focusing on tap water detection to improve hand washing detection in wearable devices without compromising privacy.", "motivation": "The integration of acoustic data in wearable human activity recognition can provide valuable context, but recording audio poses privacy challenges. The study aims to utilize contextual information from water flow detection while ensuring ethical considerations are met.", "method": "The authors created a new label, 'tap water', in the HD-Epic dataset with 717 hand-labeled annotations and analyzed its relation to water flow. They trained and evaluated two lightweight classifiers to assess the classification performance of the new label.", "result": "The presence of the tap water label facilitates easier learning by classifiers, indicating significant improvements in detecting hand washing activities using localized acoustic data processing.", "conclusion": "Processing and classifying tap water sounds locally on wearable devices can enhance human activity recognition while addressing privacy concerns.", "key_contributions": ["Introduction of the 'tap water' label in the HD-Epic dataset", "Creation of a novel dataset with 717 hand-labeled samples", "Evaluation of lightweight classifiers for improved context-aware recognition"], "limitations": "The study is bounded by the limitations of the HD-Epic dataset and localized processing constraints on wearable devices.", "keywords": ["human activity recognition", "acoustic data", "privacy", "wearable technology", "tap water detection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20320", "pdf": "https://arxiv.org/pdf/2505.20320.pdf", "abs": "https://arxiv.org/abs/2505.20320", "title": "Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP", "authors": ["Satya Narayana Cheetirala", "Ganesh Raut", "Dhavalkumar Patel", "Fabio Sanatana", "Robert Freeman", "Matthew A Levin", "Girish N. Nadkarni", "Omar Dawkins", "Reba Miller", "Randolph M. Steinhagen", "Eyal Klang", "Prem Timsina"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long text classification is challenging for Large Language Models (LLMs) due\nto token limits and high computational costs. This study explores whether a\nRetrieval Augmented Generation (RAG) approach using only the most relevant text\nsegments can match the performance of processing entire clinical notes with\nlarge context LLMs. We begin by splitting clinical documents into smaller\nchunks, converting them into vector embeddings, and storing these in a FAISS\nindex. We then retrieve the top 4,000 words most pertinent to the\nclassification query and feed these consolidated segments into an LLM. We\nevaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication\nidentification task. Metrics such as AUC ROC, precision, recall, and F1 showed\nno statistically significant differences between the RAG based approach and\nwhole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can\nsignificantly reduce token usage without sacrificing classification accuracy,\nproviding a scalable and cost effective solution for analyzing lengthy clinical\ndocuments.", "AI": {"tldr": "This study evaluates a Retrieval Augmented Generation (RAG) approach for long text classification in clinical notes, demonstrating its efficiency without loss of accuracy compared to full context LLM processing.", "motivation": "Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs.", "method": "The approach involves splitting clinical documents into chunks, converting them into vector embeddings, and storing them in a FAISS index to retrieve the most relevant text segments for classification.", "result": "Evaluated three LLMs (GPT4o, LLaMA, and Mistral) showed no statistically significant differences in classification performance between the RAG approach and whole-text processing.", "conclusion": "RAG can significantly reduce token usage while maintaining classification accuracy, offering a scalable and cost-effective solution for lengthy clinical document analysis.", "key_contributions": ["Introduction of a RAG approach to reduce token usage", "Demonstrated equivalence in performance for classification tasks between RAG and LLMs processing full texts", "Provided insights on efficient LLM applications in health informatics."], "limitations": "The study has limitations in terms of the scope of evaluated models and the specific clinical task focused on.", "keywords": ["Long Text Classification", "Retrieval Augmented Generation", "Clinical Notes", "Large Language Models", "Health Informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20796", "pdf": "https://arxiv.org/pdf/2505.20796.pdf", "abs": "https://arxiv.org/abs/2505.20796", "title": "Describe Me Something You Do Not Remember - Challenges and Risks of Exposure Design Using Generative Artificial Intelligence for Therapy of Complex Post-traumatic Disorder", "authors": ["Annalisa Degenhard", "Stefan TschÃ¶ke", "Michael Rietzler", "Enrico Rukzio"], "categories": ["cs.HC"], "comment": "Extended abstract for the Workshop \"Generative AI and Accessibility\n  Workshop: Surfacing Opportunities and Risks\" at the CHI conference on Human\n  Factors in Computing Systems 2025 (CHI'25) - Accepted on 4 April 2025", "summary": "Post-traumatic stress disorder (PTSD) is associated with sudden,\nuncontrollable, and intense flashbacks of traumatic memories. Trauma exposure\npsychotherapy has proven effective in reducing the severity of trauma-related\nsymptoms. It involves controlled recall of traumatic memories to train coping\nmechanisms for flashbacks and enable autobiographical integration of\ndistressing experiences. In particular, exposure to visualizations of these\nmemories supports successful recall. Although this approach is effective for\nvarious trauma types, it remains available for only a few. This is due to the\nlack of cost-efficient solutions for creating individualized exposure\nvisualizations. This issue is particularly relevant for the treatment of\nComplex PTSD (CPTSD), where traumatic memories are highly individual and\ngeneric visualizations do not meet therapeutic needs. Generative Artificial\nIntelligence (GAI) offers a flexible and cost-effective alternative. GAI\nenables the creation of individualized exposure visualizations during therapy\nand, for the first time, allows patients to actively participate in the\nvisualization process. While GAI opens new therapeutic perspectives and may\nimprove access to trauma therapy, especially for CPTSD, it also introduces\nsignificant challenges and risks. The extreme uncertainty and lack of control\nthat define both CPTSD and GAI raise concerns about feasibility and safety. To\nsupport safe and effective three-way communication, it is essential to\nunderstand the roles of patient, system, and therapist in exposure\nvisualization and how each can contribute to safety. This paper outlines\nperspectives, challenges, and risks associated with the use of GAI in trauma\ntherapy, with a focus on CPTSD.", "AI": {"tldr": "The paper discusses the use of Generative Artificial Intelligence (GAI) to create individualized exposure visualizations in trauma therapy, particularly for Complex PTSD (CPTSD).", "motivation": "There is a lack of cost-efficient solutions for creating individualized exposure visualizations in trauma therapy, which is crucial for treating Complex PTSD.", "method": "The paper outlines the roles of the patient, system, and therapist in using GAI for generating exposure visualizations, emphasizing safe and effective communication.", "result": "GAI enables the creation of tailored therapeutic tools that allow patients to engage in the recall process, potentially improving access to trauma therapy for CPTSD.", "conclusion": "While GAI presents exciting opportunities for trauma therapy, it also poses significant challenges and risks that must be carefully managed.", "key_contributions": ["Introduces GAI as a tool for individualized trauma therapy visualizations.", "Highlights the role of patient participation in the therapy process.", "Discusses the need for safety in the therapeutic environment using GAI."], "limitations": "Concerns regarding feasibility and safety in integrating GAI into trauma therapy, particularly for CPTSD.", "keywords": ["Generative AI", "Complex PTSD", "Trauma therapy", "Visualization", "Patient participation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20321", "pdf": "https://arxiv.org/pdf/2505.20321.pdf", "abs": "https://arxiv.org/abs/2505.20321", "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases", "authors": ["Mathew J. Koretsky", "Maya Willey", "Adi Asija", "Owen Bianchi", "Chelsea X. Alvarado", "Tanay Nayak", "Nicole Kuznetsov", "Sungwon Kim", "Mike A. Nalls", "Daniel Khashabi", "Faraz Faghri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.", "AI": {"tldr": "BiomedSQL is a benchmark for evaluating scientific reasoning in text-to-SQL generation in biomedical databases, consisting of 68,000 question/SQL query/answer triples.", "motivation": "Current text-to-SQL systems struggle with translating qualitative scientific questions into SQL, especially when domain reasoning is involved.", "method": "The dataset BiomedSQL includes 68,000 question/SQL query/answer triples and is evaluated using various LLMs against a baseline of expert accuracy.", "result": "LLMs showed significant performance gaps, with the best model achieving 62.6% execution accuracy, below the expert baseline of 90%.", "conclusion": "BiomedSQL establishes a new standard for advancing text-to-SQL systems that can facilitate scientific exploration through intelligent reasoning.", "key_contributions": ["Introduction of the BiomedSQL benchmark for scientific reasoning in text-to-SQL systems.", "In-depth evaluation of various LLMs and their performance against a robust expert baseline.", "Public accessibility of the dataset and code for further research."], "limitations": "Performance of models remains significantly lower than expert human benchmarks, indicating room for improvement.", "keywords": ["BiomedSQL", "text-to-SQL", "biomedical databases", "large language models", "scientific reasoning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.20916", "pdf": "https://arxiv.org/pdf/2505.20916.pdf", "abs": "https://arxiv.org/abs/2505.20916", "title": "Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and Mitigation of Risks", "authors": ["Kyzyl Monteiro", "Yuchen Wu", "Sauvik Das"], "categories": ["cs.HC", "H.5.2; K.4.1"], "comment": "26 pages including appendix, 14 images, 3 tables", "summary": "Users often struggle to navigate the privacy / publicity boundary in sharing\nimages online: they may lack awareness of image privacy risks and/or the\nability to apply effective mitigation strategies. To address this challenge, we\nintroduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that\nenables users to identify and mitigate privacy risks with images they intend to\nshare. Driven by design requirements from a formative user study with 7\nimage-editing experts, Imago Obscura enables users to articulate their\nimage-sharing intent and privacy concerns. The system uses these inputs to\nsurface contextually pertinent privacy risks, and then recommends and\nfacilitates application of a suite of obfuscation techniques found to be\neffective in prior literature -- e.g., inpainting, blurring, and generative\ncontent replacement. We evaluated Imago Obscura with 15 end-users in a lab\nstudy and found that it greatly improved users' awareness of image privacy\nrisks and their ability to address those risks, allowing them to make more\ninformed sharing decisions.", "AI": {"tldr": "Imago Obscura is an AI image-editing copilot designed to help users identify and mitigate privacy risks when sharing images online.", "motivation": "Address users' struggles with navigating the privacy/publicity boundary and improve their awareness of image privacy risks.", "method": "The system employs user inputs on sharing intent and privacy concerns to recommend effective obfuscation techniques based on prior literature. Evaluated through a lab study with 15 end-users.", "result": "Imago Obscura significantly improved users' awareness of privacy risks and their management capabilities, leading to more informed image-sharing decisions.", "conclusion": "The study demonstrates the potential of AI tools like Imago Obscura in enhancing user autonomy and safety when sharing images.", "key_contributions": ["Introduction of an AI-powered tool to assist with image privacy", "Evaluation based on a lab study with end-users", "Recommendations for effective image obfuscation techniques"], "limitations": "", "keywords": ["image privacy", "AI", "user study", "image-sharing", "obfuscation techniques"], "importance_score": 8, "read_time_minutes": 26}}
{"id": "2505.20322", "pdf": "https://arxiv.org/pdf/2505.20322.pdf", "abs": "https://arxiv.org/abs/2505.20322", "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms", "authors": ["Mengru Wang", "Ziwen Xu", "Shengyu Mao", "Shumin Deng", "Zhaopeng Tu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": null, "summary": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control.", "AI": {"tldr": "The paper introduces Steering Target Atoms (STA), a method for improving control in language models by isolating and manipulating disentangled knowledge components, enhancing safety and robustness, particularly in adversarial scenarios.", "motivation": "To improve control precision in language model generation and ensure safety and reliability, especially amid intertwined internal representations that limit steerability.", "method": "The paper proposes a novel method called Steering Target Atoms (STA) that isolates and manipulates disentangled knowledge components within language models to enhance their controllability and safety.", "result": "Experimental results show that the STA method allows for precise manipulation of knowledge components, improving robustness and flexibility of steering in both general and adversarial scenarios.", "conclusion": "The STA approach enables enhanced control over language models, confirming its effectiveness in ensuring safety during model operation, especially while dealing with complex reasoning tasks.", "key_contributions": ["Introduction of Steering Target Atoms (STA) for disentangled knowledge manipulation", "Demonstrated effectiveness of STA through comprehensive experiments", "Confirmed robustness and flexibility in adversarial contexts"], "limitations": "", "keywords": ["language models", "safety", "control precision", "Steering Target Atoms", "disentangled knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21116", "pdf": "https://arxiv.org/pdf/2505.21116.pdf", "abs": "https://arxiv.org/abs/2505.21116", "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.", "AI": {"tldr": "This survey focuses on creativity within multi-agent systems (MAS) driven by large language models (LLMs), examining how agents generate and evaluate novel ideas in collaboration with humans.", "motivation": "To address the lack of focus on creativity in existing surveys on multi-agent systems, which predominantly emphasize infrastructure over the creative process.", "method": "The authors provide a taxonomy of agent proactivity and persona design, an overview of generation techniques, and a discussion on challenges in the field.", "result": "The survey presents a structured framework for understanding creative workflows in MAS, highlighting techniques like divergent exploration and issues such as evaluation standards and bias mitigation.", "conclusion": "This work lays the groundwork for future research and development in the creative capabilities of multi-agent systems, aiming for improved standardization and evaluation.", "key_contributions": ["Taxonomy of agent proactivity and persona design", "Overview of generation techniques and evaluation metrics", "Identification of challenges in creativity in MAS"], "limitations": "The paper does not fully explore all domain-specific applications of creativity in MAS due to the breadth of the topic.", "keywords": ["multi-agent systems", "creativity", "large language models", "text generation", "image generation"], "importance_score": 9, "read_time_minutes": 23}}
{"id": "2505.20323", "pdf": "https://arxiv.org/pdf/2505.20323.pdf", "abs": "https://arxiv.org/abs/2505.20323", "title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "George H. Chen", "Jeremy C. Weiss"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding temporal dynamics in clinical narratives is essential for\nmodeling patient trajectories, yet large-scale temporally annotated resources\nremain limited. We present PMOA-TTS, the first openly available dataset of\n124,699 PubMed Open Access (PMOA) case reports, each converted into structured\n(event, time) timelines via a scalable LLM-based pipeline. Our approach\ncombines heuristic filtering with Llama 3.3 to identify single-patient case\nreports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,\nresulting in over 5.6 million timestamped clinical events. To assess timeline\nquality, we evaluate against a clinician-curated reference set using three\nmetrics: (i) event-level matching (80% match at a cosine similarity threshold\nof 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the\nLog-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide\ndiagnostic and demographic coverage. In a downstream survival prediction task,\nembeddings from extracted timelines achieve time-dependent concordance indices\nup to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally\nstructured narratives. PMOA-TTS provides a scalable foundation for timeline\nextraction, temporal reasoning, and longitudinal modeling in biomedical NLP.\nThe dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .", "AI": {"tldr": "The paper introduces PMOA-TTS, a dataset of 124,699 PubMed case reports converted into structured timelines, aiding in modeling patient trajectories and predictive analytics in healthcare.", "motivation": "There is a need for large-scale temporally annotated resources to model patient trajectories effectively in clinical narratives.", "method": "A scalable LLM-based pipeline was developed, combining heuristic filtering and prompt-driven extraction using Llama 3.3 and DeepSeek R1 to generate structured timelines of clinical events.", "result": "The dataset includes over 5.6 million timestamped clinical events and achieves high-quality metrics for timeline accuracy and predictive tasks.", "conclusion": "PMOA-TTS is the first openly available dataset for structured timeline extraction in clinical narratives and supports advanced temporal reasoning and modeling in biomedical NLP.", "key_contributions": ["Introduction of PMOA-TTS, a large dataset for clinical timeline extraction.", "Utilization of LLMs for creating structured timelines in healthcare contexts.", "Validation of the dataset against a clinician-curated reference set using multiple metrics."], "limitations": "", "keywords": ["temporal dynamics", "clinical narratives", "dataset", "PubMed", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21196", "pdf": "https://arxiv.org/pdf/2505.21196.pdf", "abs": "https://arxiv.org/abs/2505.21196", "title": "Learning Annotation Consensus for Continuous Emotion Recognition", "authors": ["Ibrahim Shoer", "Engin Erzin"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In affective computing, datasets often contain multiple annotations from\ndifferent annotators, which may lack full agreement. Typically, these\nannotations are merged into a single gold standard label, potentially losing\nvaluable inter-rater variability. We propose a multi-annotator training\napproach for continuous emotion recognition (CER) that seeks a consensus across\nall annotators rather than relying on a single reference label. Our method\nemploys a consensus network to aggregate annotations into a unified\nrepresentation, guiding the main arousal-valence predictor to better reflect\ncollective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach\noutperforms traditional methods that unify annotations into a single label.\nThis underscores the benefits of fully leveraging multi-annotator data in\nemotion recognition and highlights its applicability across various fields\nwhere annotations are abundant yet inconsistent.", "AI": {"tldr": "This paper introduces a multi-annotator training method for continuous emotion recognition that aggregates diverse annotations into a unified label, improving predictive accuracy over traditional methods.", "motivation": "Traditional methods in affective computing often lose valuable information by merging multiple annotations into a single gold standard label, which may not reflect the diversity of inputs from different annotators.", "method": "The proposed method employs a consensus network that aggregates annotations from multiple annotators to form a unified representation, which is then used to guide the primary arousal-valence predictor.", "result": "The method was tested on the RECOLA and COGNIMUSE datasets and demonstrated improved performance over conventional methods that rely on single reference labels.", "conclusion": "The study demonstrates the advantages of utilizing multi-annotator data in emotion recognition, making it applicable in various fields where annotations are often inconsistent.", "key_contributions": ["Introduction of a consensus network for emotion recognition", "Demonstrated enhancement in predictive performance using multi-annotator data", "Implications for fields with diverse annotation challenges"], "limitations": "", "keywords": ["affective computing", "emotion recognition", "multi-annotator", "machine learning", "data aggregation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20325", "pdf": "https://arxiv.org/pdf/2505.20325.pdf", "abs": "https://arxiv.org/abs/2505.20325", "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence", "authors": ["Amirhosein Ghasemabadi", "Keith G. Mills", "Baochun Li", "Di Niu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.", "AI": {"tldr": "Guided by Gut (GG) is a self-guided Test-Time Scaling framework for LLMs that enhances reasoning efficiency by using intrinsic signals instead of external verifier models, achieving high accuracy with reduced computational costs.", "motivation": "To address the substantial computational costs associated with existing TTS methods that rely on external Process Reward Models or sampling methods.", "method": "GG employs a lightweight tree search guided by intrinsic LLM signals such as token-level confidence and step novelty. It includes a reinforcement learning fine-tuning phase to improve internal confidence estimates.", "result": "Empirical evaluations show that GG allows smaller models (1.5B parameters) to match or exceed the performance of much larger models (32B-70B parameters) while reducing GPU memory usage by up to 10x.", "conclusion": "Guided by Gut achieves PRM-level performance with significantly faster inference speeds, lower memory usage, and more efficient deployment of TTS techniques.", "key_contributions": ["Introduction of an efficient self-guided TTS framework without costly external verifier models.", "Improved reliability of internal confidence estimates through reinforcement learning fine-tuning.", "Significant reductions in memory usage and inference speed compared to existing methods."], "limitations": "", "keywords": ["Test-Time Scaling", "Large Language Models", "Reinforcement Learning", "Intrinsic Signals", "Efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.21385", "pdf": "https://arxiv.org/pdf/2505.21385.pdf", "abs": "https://arxiv.org/abs/2505.21385", "title": "Dynamic Vision from EEG Brain Recordings: How much does EEG know?", "authors": ["Prajwal Singh", "Anupam Sharma", "Pankaj Pandey", "Krishna Miyapuram", "Shanmuganathan Raman"], "categories": ["cs.HC"], "comment": null, "summary": "Reconstructing and understanding dynamic visual information (video) from\nbrain EEG recordings is challenging due to the non-stationary nature of EEG\nsignals, their low signal-to-noise ratio (SNR), and the limited availability of\nEEG-Video stimulus datasets. Most recent studies have focused on reconstructing\nstatic images from EEG recordings. In this work, we propose a framework to\nreconstruct dynamic visual stimuli from EEG data and conduct an in-depth study\nof the information encoded in EEG signals. Our approach first trains a feature\nextraction network using a triplet-based contrastive learning strategy within\nan EEG-video generation framework. The extracted EEG features are then used for\nvideo synthesis with a modified StyleGAN-ADA, which incorporates temporal\ninformation as conditioning. Additionally, we analyze how different brain\nregions contribute to processing dynamic visual stimuli. Through several\nempirical studies, we evaluate the effectiveness of our framework and\ninvestigate how much dynamic visual information can be inferred from EEG\nsignals. The inferences we derive through our extensive studies would be of\nimmense value to future research on extracting visual dynamics from EEG.", "AI": {"tldr": "A framework to reconstruct dynamic visual stimuli from EEG data using triplet-based contrastive learning and modified StyleGAN-ADA.", "motivation": "To address the challenges of reconstructing dynamic visual information from EEG signals due to their non-stationary nature and low SNR.", "method": "A feature extraction network is trained with a triplet-based contrastive learning strategy, followed by video synthesis using a modified StyleGAN-ADA that incorporates temporal information.", "result": "The framework effectively synthesizes dynamic visual stimuli from EEG, revealing how different brain regions contribute to processing such information.", "conclusion": "The findings highlight the potential of deriving dynamic visual information from EEG data, providing insights for future research.", "key_contributions": ["Proposed a novel framework for reconstructing dynamic visuals from EEG signals.", "Utilized triplet-based contrastive learning for feature extraction.", "Modified StyleGAN-ADA to integrate temporal dynamics in video synthesis."], "limitations": "Limited availability of EEG-Video stimulus datasets may restrict further validation and application of the model.", "keywords": ["EEG", "video synthesis", "contrastive learning", "StyleGAN", "dynamic visual information"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2505.20333", "pdf": "https://arxiv.org/pdf/2505.20333.pdf", "abs": "https://arxiv.org/abs/2505.20333", "title": "Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models", "authors": ["Yukun Zhang", "Qi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have achieved strong\nperformance, yet their internal reasoning remains opaque, limiting\ninterpretability and trust in critical applications. We propose a novel\nMulti_Scale Manifold Alignment framework that decomposes the latent space into\nglobal, intermediate, and local semantic manifolds capturing themes, context,\nand word-level details. Our method introduces cross_scale mapping functions\nthat jointly enforce geometric alignment (e.g., Procrustes analysis) and\ninformation preservation (via mutual information constraints like MINE or VIB).\nWe further incorporate curvature regularization and hyperparameter tuning for\nstable optimization. Theoretical analysis shows that alignment error, measured\nby KL divergence, can be bounded under mild assumptions. This framework offers\na unified explanation of how LLMs structure multi-scale semantics, advancing\ninterpretability and enabling applications such as bias detection and\nrobustness enhancement.", "AI": {"tldr": "This paper proposes a Multi_Scale Manifold Alignment framework to enhance interpretability in Large Language Models by structuring their latent space into semantic manifolds.", "motivation": "The internal reasoning of Large Language Models is often opaque, which limits their interpretability and trust in critical applications.", "method": "The proposed framework decomposes the latent space into global, intermediate, and local semantic manifolds, employing cross_scale mapping functions for geometric alignment and information preservation with curvature regularization.", "result": "The theoretical analysis demonstrates that the alignment error can be bounded under mild assumptions, which supports the effectiveness of the framework in improving interpretability.", "conclusion": "This framework not only enhances the understanding of LLMs' multi-scale semantics but also enables practical applications like bias detection and robustness improvement.", "key_contributions": ["Introduction of Multi_Scale Manifold Alignment framework.", "Joint enforcement of geometric alignment and information preservation.", "Stable optimization through curvature regularization."], "limitations": "", "keywords": ["Large Language Models", "interpretability", "semantic alignment", "manifold learning", "bias detection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20334", "pdf": "https://arxiv.org/pdf/2505.20334.pdf", "abs": "https://arxiv.org/abs/2505.20334", "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query", "authors": ["Yixuan Wang", "Shiyu Ji", "Yijun Liu", "Yuzhuang Xu", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures", "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.", "AI": {"tldr": "The paper introduces Lookahead Q-Cache (LAQ), a novel eviction framework for key-value caches in large language models that improves cache eviction consistency and performance during text decoding.", "motivation": "Existing key-value cache eviction methods for large language models become inconsistent under memory constraints, necessitating a more efficient approach that better aligns with real inference scenarios.", "method": "The paper proposes LAQ, which utilizes low-cost pseudo lookahead queries to estimate token importance for cache eviction, leading to more accurate and consistent results.", "result": "LAQ outperforms existing cache eviction methods, demonstrating a 1 to 4 point improvement on the LongBench benchmark under limited cache budgets while maintaining compatibility with current techniques.", "conclusion": "The Lookahead Q-Cache framework significantly enhances cache eviction processes in large language models, yielding better performance and flexibility for real-world applications.", "key_contributions": ["Introduction of Lookahead Q-Cache (LAQ) for efficient key-value cache eviction in LLMs", "Demonstrated performance improvement on LongBench and Needle-in-a-Haystack benchmarks", "Proposed method is complementary to existing cache approaches."], "limitations": "", "keywords": ["Large Language Models", "Key-Value Cache", "Cache Eviction", "Machine Learning", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 16}}
{"id": "2505.20335", "pdf": "https://arxiv.org/pdf/2505.20335.pdf", "abs": "https://arxiv.org/abs/2505.20335", "title": "Language Model Distillation: A Temporal Difference Imitation Learning Perspective", "authors": ["Zishun Yu", "Shangzhe Li", "Xinhua Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have led to significant progress across many NLP tasks,\nalthough their massive sizes often incur substantial computational costs.\nDistillation has become a common practice to compress these large and highly\ncapable models into smaller, more efficient ones. Many existing language model\ndistillation methods can be viewed as behavior cloning from the perspective of\nimitation learning or inverse reinforcement learning. This viewpoint has\ninspired subsequent studies that leverage (inverse) reinforcement learning\ntechniques, including variations of behavior cloning and temporal difference\nlearning methods. Rather than proposing yet another specific temporal\ndifference method, we introduce a general framework for temporal\ndifference-based distillation by exploiting the distributional sparsity of the\nteacher model. Specifically, it is often observed that language models assign\nmost probability mass to a small subset of tokens. Motivated by this\nobservation, we design a temporal difference learning framework that operates\non a reduced action space (a subset of vocabulary), and demonstrate how\npractical algorithms can be derived and the resulting performance improvements.", "AI": {"tldr": "This paper presents a framework for temporal difference-based distillation of language models that focuses on the distributional sparsity of teacher models.", "motivation": "To address the computational costs of large language models by exploring efficient distillation methods.", "method": "A general framework for temporal difference-based distillation is proposed, leveraging the distributional sparsity of teacher models by operating on a reduced action space.", "result": "Demonstrates how practical algorithms can be derived from this framework and shows resulting performance improvements in model efficiency.", "conclusion": "The proposed framework provides a more efficient way to distill large language models while retaining performance by focusing on a smaller subset of vocabulary tokens.", "key_contributions": ["Introduction of a general framework for temporal difference-based distillation.", "Focus on distributional sparsity in language models.", "Demonstration of performance improvements using a reduced action space."], "limitations": "", "keywords": ["language model distillation", "temporal difference learning", "imitation learning", "reinforcement learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20336", "pdf": "https://arxiv.org/pdf/2505.20336.pdf", "abs": "https://arxiv.org/abs/2505.20336", "title": "MOSLIM:Align with diverse preferences in prompts through reward classification", "authors": ["Yu Zhang", "Wanli Jiang", "Zhengyu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The multi-objective alignment of Large Language Models (LLMs) is essential\nfor ensuring foundational models conform to diverse human preferences. Current\nresearch in this field typically involves either multiple policies or multiple\nreward models customized for various preferences, or the need to train a\npreference-specific supervised fine-tuning (SFT) model. In this work, we\nintroduce a novel multi-objective alignment method, MOSLIM, which utilizes a\nsingle reward model and policy model to address diverse objectives. MOSLIM\nprovides a flexible way to control these objectives through prompting and does\nnot require preference training during SFT phase, allowing thousands of\noff-the-shelf models to be directly utilized within this training framework.\nMOSLIM leverages a multi-head reward model that classifies question-answer\npairs instead of scoring them and then optimize policy model with a scalar\nreward derived from a mapping function that converts classification results\nfrom reward model into reward scores. We demonstrate the efficacy of our\nproposed method across several multi-objective benchmarks and conduct ablation\nstudies on various reward model sizes and policy optimization methods. The\nMOSLIM method outperforms current multi-objective approaches in most results\nwhile requiring significantly fewer GPU computing resources compared with\nexisting policy optimization methods.", "AI": {"tldr": "MOSLIM is a novel multi-objective alignment method for LLMs using a single reward model and policy, allowing for flexibility in controlling objectives without the need for preference-specific training.", "motivation": "The need for multi-objective alignment of LLMs to align with diverse human preferences in a resource-efficient manner.", "method": "MOSLIM employs a single multi-head reward model that classifies question-answer pairs and utilizes a mapping function to optimize a policy model based on scalar rewards derived from classification results.", "result": "MOSLIM demonstrates superior performance to existing multi-objective methods, with a significant reduction in GPU resource requirements.", "conclusion": "The proposed method provides an efficient way to align LLMs with multiple objectives, enhancing the usability of existing models without extensive retraining.", "key_contributions": ["Introduction of the MOSLIM method for multi-objective alignment", "Utilization of a single reward model to reduce complexity", "Demonstration of resource efficiency compared to existing methods"], "limitations": "", "keywords": ["Large Language Models", "Multi-objective alignment", "Reward model"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20338", "pdf": "https://arxiv.org/pdf/2505.20338.pdf", "abs": "https://arxiv.org/abs/2505.20338", "title": "Assessing the Capability of LLMs in Solving POSCOMP Questions", "authors": ["Cayo Viegas", "Rohit Gheyi", "MÃ¡rcio Ribeiro"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nexpanded the capabilities of artificial intelligence in natural language\nprocessing tasks. Despite this progress, their performance in specialized\ndomains such as computer science remains relatively unexplored. Understanding\nthe proficiency of LLMs in these domains is critical for evaluating their\npractical utility and guiding future developments. The POSCOMP, a prestigious\nBrazilian examination used for graduate admissions in computer science promoted\nby the Brazlian Computer Society (SBC), provides a challenging benchmark. This\nstudy investigates whether LLMs can match or surpass human performance on the\nPOSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and\nLe Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP\nexams. The assessments measured the models' proficiency in handling complex\nquestions typical of the exam. LLM performance was notably better on text-based\nquestions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led\nwith 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced\n(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were\nobserved in the 2023 exam. ChatGPT-4 achieved the highest performance,\nsurpassing all students who took the POSCOMP 2023 exam. LLMs, particularly\nChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image\ninterpretation remains a challenge. Given the rapid evolution of LLMs, we\nexpanded our analysis to include more recent models - o1, Gemini 2.5 Pro,\nClaude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.\nThese newer models demonstrate further improvements and consistently surpass\nboth the average and top-performing human participants across all three years.", "AI": {"tldr": "This study evaluates the performance of Large Language Models (LLMs) on the POSCOMP exam for computer science graduate admissions, comparing their results with human test-takers over multiple years.", "motivation": "To assess the proficiency of LLMs in specialized domains like computer science and guide future developments in AI applications.", "method": "The performance of four LLMs (ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and Le Chat Mistral Large) was evaluated on the POSCOMP exams from 2022 and 2023, focusing on their ability to solve complex text-based and image interpretation questions.", "result": "LLMs, especially ChatGPT-4, outperformed human students in text-based questions, achieving the highest scores on the 2023 exam. Newer models consistently surpassed human performance across all examined years.", "conclusion": "ChatGPT-4 and subsequent models demonstrate significant capabilities in text-based tasks, with ongoing improvements in LLMs suggesting promising potential for future applications in computer science education.", "key_contributions": ["Evaluation of LLMs on a specialized academic exam", "Comparison of LLM performance with human students", "Analysis of advancements in LLM capabilities over multiple years"], "limitations": "Performance in image interpretation tasks remains weak; the study focuses only on specific LLMs.", "keywords": ["Large Language Models", "POSCOMP exam", "Natural Language Processing", "AI in education", "Computer science"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.20340", "pdf": "https://arxiv.org/pdf/2505.20340.pdf", "abs": "https://arxiv.org/abs/2505.20340", "title": "Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models", "authors": ["Yukun Zhang", "Qi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework\nthat models large language model generation as a controlled dynamical system\nevolving on a low_dimensional semantic manifold. By casting latent_state\nupdates as discrete time Euler approximations of continuous dynamics, we map\nintrinsic energy_driven flows and context_dependent forces onto Transformer\ncomponents (residual connections, attention, feed-forward networks). Leveraging\nLyapunov stability theory We define three empirical metrics (state continuity,\nclustering quality, topological persistence) that quantitatively link\nlatent_trajectory properties to text fluency, grammaticality, and semantic\ncoherence. Extensive experiments across decoding parameters validate DMET's\npredictions and yield principled guidelines for balancing creativity and\nconsistency in text generation.", "AI": {"tldr": "Proposes Dynamic Manifold Evolution Theory (DMET) for modeling large language model generation as a dynamical system on a semantic manifold.", "motivation": "To create a framework that links the dynamics of language generation to semantic properties in a systematic way.", "method": "Models latent state updates using discrete time Euler approximations; employs Lyapunov stability theory to define three empirical metrics for evaluating text generation.", "result": "Extensive experiments show that DMET's predictions align with linguistic properties such as fluency and coherence, providing guidelines to improve text generation.", "conclusion": "DMET offers a new lens for understanding language model behavior and balancing creativity with consistency.", "key_contributions": ["Introduction of Dynamic Manifold Evolution Theory (DMET) for language generation.", "Connection between dynamical systems theory and language model performance.", "Empirical metrics linking latent trajectories to linguistic qualities."], "limitations": "", "keywords": ["Dynamic Manifold Evolution Theory", "language generation", "semantic manifold", "Lyapunov stability", "text fluency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20343", "pdf": "https://arxiv.org/pdf/2505.20343.pdf", "abs": "https://arxiv.org/abs/2505.20343", "title": "Do LLMs have a Gender (Entropy) Bias?", "authors": ["Sonal Prabhune", "Balaji Padmanabhan", "Kaushik Dutta"], "categories": ["cs.CL", "cs.AI", "68T42, 68T50", "I.2.7"], "comment": "18 pages, 4 figures", "summary": "We investigate the existence and persistence of a specific type of gender\nbias in some of the popular LLMs and contribute a new benchmark dataset,\nRealWorldQuestioning (released on HuggingFace ), developed from real-world\nquestions across four key domains in business and health contexts: education,\njobs, personal financial management, and general health. We define and study\nentropy bias, which we define as a discrepancy in the amount of information\ngenerated by an LLM in response to real questions users have asked. We tested\nthis using four different LLMs and evaluated the generated responses both\nqualitatively and quantitatively by using ChatGPT-4o (as \"LLM-as-judge\"). Our\nanalyses (metric-based comparisons and \"LLM-as-judge\" evaluation) suggest that\nthere is no significant bias in LLM responses for men and women at a category\nlevel. However, at a finer granularity (the individual question level), there\nare substantial differences in LLM responses for men and women in the majority\nof cases, which \"cancel\" each other out often due to some responses being\nbetter for males and vice versa. This is still a concern since typical users of\nthese tools often ask a specific question (only) as opposed to several varied\nones in each of these common yet important areas of life. We suggest a simple\ndebiasing approach that iteratively merges the responses for the two genders to\nproduce a final result. Our approach demonstrates that a simple, prompt-based\ndebiasing strategy can effectively debias LLM outputs, thus producing responses\nwith higher information content than both gendered variants in 78% of the\ncases, and consistently achieving a balanced integration in the remaining\ncases.", "AI": {"tldr": "This paper investigates gender bias in LLMs, introduces the RealWorldQuestioning dataset, and proposes a prompt-based debiasing method.", "motivation": "To address and analyze the gender bias present in popular LLMs, especially in the contexts of business and health.", "method": "The authors developed the RealWorldQuestioning dataset and tested it on four LLMs, using qualitative and quantitative measures, including an 'LLM-as-judge,' to evaluate responses for bias.", "result": "The study found that while overall bias does not exist at a category level, significant discrepancies were noted at the individual question level. A prompt-based debiasing approach effectively improved the information content of LLM outputs in 78% of cases.", "conclusion": "A simple debiasing strategy can enhance the quality of LLM outputs by merging responses for different genders, addressing substantial biases that could affect user decision-making.", "key_contributions": ["Introduction of the RealWorldQuestioning dataset", "Definition and investigation of entropy bias in LLM responses", "Proposed effective debiasing method using prompt-based strategies"], "limitations": "The study only considers four LLMs and specific domains; further research is needed to generalize findings across more diverse datasets and models.", "keywords": ["gender bias", "LLM", "debiasing", "RealWorldQuestioning", "entropic responses"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.20679", "pdf": "https://arxiv.org/pdf/2505.20679.pdf", "abs": "https://arxiv.org/abs/2505.20679", "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations", "authors": ["Danush Khanna", "Pratinav Seth", "Sidhaarth Sredharan Murali", "Aditya Kumar Guru", "Siddharth Shukla", "Tanuj Tyagi", "Sandeep Chaurasia", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal\ncommunication, making its detection critical for safeguarding potential\nvictims. However, due to manipulation's nuanced and context-specific nature,\nidentifying manipulative language in complex, multi-turn, and multi-person\nconversations remains a significant challenge for large language models (LLMs).\nTo address this gap, we introduce the MultiManip dataset, comprising 220\nmulti-turn, multi-person dialogues balanced between manipulative and\nnon-manipulative interactions, all drawn from reality shows that mimic\nreal-world scenarios. For manipulative interactions, it includes 11 distinct\nmanipulations depicting real-life scenarios. We conduct extensive evaluations\nof state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various\nprompting strategies. Despite their capabilities, these models often struggle\nto detect manipulation effectively. To overcome this limitation, we propose\nSELF-PERCEPT, a novel, two-stage prompting framework inspired by\nSelf-Perception Theory, demonstrating strong performance in detecting\nmulti-person, multi-turn mental manipulation. Our code and data are publicly\navailable at https://github.com/danushkhanna/self-percept .", "AI": {"tldr": "The paper introduces the MultiManip dataset for detecting mental manipulation in dialogues and proposes a novel prompting framework called SELF-PERCEPT to improve detection with LLMs.", "motivation": "To enhance the detection of mental manipulation in complex dialogues which is crucial for safeguarding victims of abuse.", "method": "The authors created the MultiManip dataset containing 220 dialogues classified as manipulative or non-manipulative and evaluated state-of-the-art LLMs using different prompting strategies, culminating in the development of the SELF-PERCEPT framework.", "result": "State-of-the-art LLMs struggle with identifying manipulative language; the proposed SELF-PERCEPT framework performs significantly better in detecting multi-person, multi-turn manipulation.", "conclusion": "The SELF-PERCEPT framework offers an effective solution to an existing gap in LLM capabilities for understanding complex dialogues involving mental manipulation.", "key_contributions": ["Introduction of the MultiManip dataset with balanced manipulative dialogue examples", "Evaluation of current LLMs on the dataset establishing their limitations", "Development of the SELF-PERCEPT two-stage prompting framework for improved detection"], "limitations": "The study focuses on dialogues from reality shows, which may not fully represent all types of real-world scenarios.", "keywords": ["Mental Manipulation", "Multi-turn Dialogues", "Large Language Models", "Dataset", "Self-Perception Theory"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20347", "pdf": "https://arxiv.org/pdf/2505.20347.pdf", "abs": "https://arxiv.org/abs/2505.20347", "title": "SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data", "authors": ["Wenkai Fang", "Shunyu Liu", "Yang Zhou", "Kongcheng Zhang", "Tongya Zheng", "Kaixuan Chen", "Mingli Song", "Dacheng Tao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have demonstrated the effectiveness of Reinforcement Learning\n(RL) in improving the reasoning capabilities of Large Language Models (LLMs).\nHowever, existing works inevitably rely on high-quality instructions and\nverifiable rewards for effective training, both of which are often difficult to\nobtain in specialized domains. In this paper, we propose Self-play\nReinforcement Learning(SeRL) to bootstrap LLM training with limited initial\ndata. Specifically, SeRL comprises two complementary modules: self-instruction\nand self-rewarding. The former module generates additional instructions based\non the available data at each training step, employing robust online filtering\nstrategies to ensure instruction quality, diversity, and difficulty. The latter\nmodule introduces a simple yet effective majority-voting mechanism to estimate\nresponse rewards for additional instructions, eliminating the need for external\nannotations. Finally, SeRL performs conventional RL based on the generated\ndata, facilitating iterative self-play learning. Extensive experiments on\nvarious reasoning benchmarks and across different LLM backbones demonstrate\nthat the proposed SeRL yields results superior to its counterparts and achieves\nperformance on par with those obtained by high-quality data with verifiable\nrewards. Our code is available at https://github.com/wantbook-book/SeRL.", "AI": {"tldr": "This paper presents Self-play Reinforcement Learning (SeRL) to enhance the training of Large Language Models (LLMs) with limited data by generating high-quality instructional data and estimating rewards through a voting mechanism.", "motivation": "The effectiveness of Reinforcement Learning in improving LLMs is established, but challenges arise from the need for quality instructions and verifiable rewards, especially in specialized domains.", "method": "SeRL consists of two modules: self-instruction, which generates new instructions from existing data, and self-rewarding, which uses a majority-voting mechanism to estimate rewards for these instructions, allowing for effective RL training without external annotations.", "result": "Experiments show that SeRL outperforms existing methods and matches the performance achieved with high-quality datasets and verifiable rewards on various reasoning benchmarks.", "conclusion": "SeRL effectively boosts LLM training with limited initial data by generating reliable instructional content and automatic reward estimation, ensuring robust performance in reasoning tasks.", "key_contributions": ["Introduction of Self-play Reinforcement Learning (SeRL) framework", "Effective self-instruction and self-rewarding modules", "Achievement of high performance without reliance on high-quality external data"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Self-play", "Instruction Generation", "Reward Mechanism"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20354", "pdf": "https://arxiv.org/pdf/2505.20354.pdf", "abs": "https://arxiv.org/abs/2505.20354", "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?", "authors": ["Juntong Wu", "Zijing Liu", "He Cao", "Hao Li", "Bin Feng", "Zishan Shu", "Ke Yu", "Li Yuan", "Yu Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM.", "AI": {"tldr": "The paper addresses limitations in protein-text models related to data leakage and evaluation metrics, proposing a new framework that outperforms existing LLMs in protein-to-text generation.", "motivation": "To improve the understanding and generation of proteins through protein-text models by addressing data leakage and inadequate evaluation metrics in current benchmarks.", "method": "The authors analyze existing architectures and benchmarks, reorganize datasets, and introduce a novel evaluation framework based on biological entities alongside a retrieval-enhanced method.", "result": "The proposed retrieval-enhanced method significantly outperforms fine-tuned LLMs in protein-to-text generation, demonstrating improved accuracy and efficiency in training-free scenarios.", "conclusion": "The novel evaluation framework and retrieval-enhanced method represent significant advancements in the performance and assessment of protein-text models.", "key_contributions": ["Introduction of a novel evaluation framework based on biological entities", "Identification of data leakage issues in current benchmarks", "Development of a retrieval-enhanced method for better performance in protein generation"], "limitations": "", "keywords": ["Protein generation", "Large language models", "Evaluation framework", "Data leakage", "Natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20415", "pdf": "https://arxiv.org/pdf/2505.20415.pdf", "abs": "https://arxiv.org/abs/2505.20415", "title": "Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision", "authors": ["Xingwei Tan", "Marco Valentino", "Mahmud Akhter", "Maria Liakata", "Nikolaos Aletras"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) have shown promising performance in mathematical\nand logical reasoning benchmarks. However, recent studies have pointed to\nmemorization, rather than generalization, as one of the leading causes for such\nperformance. LLMs, in fact, are susceptible to content variations,\ndemonstrating a lack of robust symbolic abstractions supporting their reasoning\nprocess. To improve reliability, many attempts have been made to combine LLMs\nwith symbolic methods. Nevertheless, existing approaches fail to effectively\nleverage symbolic representations due to the challenges involved in developing\nreliable and scalable verification mechanisms. In this paper, we propose to\novercome such limitations by generating symbolic reasoning trajectories and\nselect the high-quality ones using a process reward model automatically tuned\nbased on Monte Carlo estimation. The trajectories are then employed via\nfine-tuning methods to improve logical reasoning and generalization. Our\nresults on logical reasoning benchmarks such as FOLIO and LogicAsker show the\neffectiveness of the proposed method with large gains on frontier and\nopen-weight models. Moreover, additional experiments on claim verification\nreveal that fine-tuning on the generated symbolic reasoning trajectories\nenhances out-of-domain generalizability, suggesting the potential impact of\nsymbolically-guided process supervision in alleviating the effect of\nmemorization on LLM reasoning.", "AI": {"tldr": "This paper addresses the limitations of large language models (LLMs) in reasoning, proposing a method that combines symbolic reasoning trajectories with fine-tuning to enhance performance on logical reasoning tasks.", "motivation": "To improve the reliability of large language models in logical and mathematical reasoning, by addressing issues related to memorization and generalization.", "method": "The authors propose generating symbolic reasoning trajectories and selecting high-quality ones using a Monte Carlo estimation-tuned reward model, followed by fine-tuning the LLMs with these trajectories.", "result": "The proposed method shows significant improvement in logical reasoning benchmarks such as FOLIO and LogicAsker, with noticeable gains on frontier and open-weight models.", "conclusion": "Fine-tuning on generated symbolic reasoning trajectories improves the generalizability of LLMs and mitigates the effects of memorization, highlighting the potential of symbolic methods in enhancing LLM reasoning capabilities.", "key_contributions": ["Introduction of symbolic reasoning trajectories for LLMs", "Development of an automatic tuning process reward model using Monte Carlo estimation", "Demonstration of improved generalization and reasoning capabilities on logical benchmarks"], "limitations": "The study is a work in progress and may require further validation through broader testing.", "keywords": ["large language models", "symbolic reasoning", "logical reasoning", "fine-tuning", "generalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20416", "pdf": "https://arxiv.org/pdf/2505.20416.pdf", "abs": "https://arxiv.org/abs/2505.20416", "title": "GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation", "authors": ["Zihong Chen", "Wanli Jiang", "Jinzhe Li", "Zhonghang Yuan", "Huanjun Kong", "Wanli Ouyang", "Nanqing Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning for large language models (LLMs) typically requires substantial\namounts of high-quality supervised data, which is both costly and\nlabor-intensive to acquire. While synthetic data generation has emerged as a\npromising solution, existing approaches frequently suffer from factual\ninaccuracies, insufficient long-tail coverage, simplistic knowledge structures,\nand homogenized outputs. To address these challenges, we introduce GraphGen, a\nknowledge graph-guided framework designed for three key question-answering (QA)\nscenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by\nconstructing a fine-grained knowledge graph from the source text. It then\nidentifies knowledge gaps in LLMs using the expected calibration error metric,\nprioritizing the generation of QA pairs that target high-value, long-tail\nknowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling\nto capture complex relational information and employs style-controlled\ngeneration to diversify the resulting QA data. Experimental results on\nknowledge-intensive tasks under closed-book settings demonstrate that GraphGen\noutperforms conventional synthetic data methods, offering a more reliable and\ncomprehensive solution to the data scarcity challenge in supervised\nfine-tuning. The code and data are publicly available at\nhttps://github.com/open-sciencelab/GraphGen.", "AI": {"tldr": "GraphGen is a framework that generates high-quality synthetic data for fine-tuning large language models (LLMs) by leveraging knowledge graphs and addressing data limitations.", "motivation": "The motivation behind GraphGen is to improve the quality of synthetic data used in fine-tuning LLMs, overcoming challenges related to factual inaccuracies and knowledge coverage.", "method": "GraphGen constructs a knowledge graph from the source text, identifies knowledge gaps using the expected calibration error metric, prioritizes QA pair generation for long-tail knowledge, and employs multi-hop neighborhood sampling with style-controlled generation.", "result": "Experimental results indicate that GraphGen significantly outperforms existing synthetic data generation methods, providing more reliable training data for supervised fine-tuning.", "conclusion": "GraphGen presents a comprehensive solution to the limitations of using synthetic data in LLM fine-tuning, improving data quality and coverage.", "key_contributions": ["Introduces a knowledge graph-guided framework for QA data generation.", "Implements multi-hop neighborhood sampling for relational information capture.", "Enhances diversity in generated QA pairs through style-controlled generation."], "limitations": "", "keywords": ["Large Language Models", "Synthetic Data Generation", "Knowledge Graphs", "Question Answering", "Fine-Tuning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.20422", "pdf": "https://arxiv.org/pdf/2505.20422.pdf", "abs": "https://arxiv.org/abs/2505.20422", "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model", "authors": ["Arvindh Arun", "Sumit Kumar", "Mojtaba Nayyeri", "Bo Xiong", "Ponnurangam Kumaraguru", "Antonio Vergari", "Steffen Staab"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling\nzero-shot reasoning over unseen graphs by learning transferable patterns.\nHowever, most existing KGFMs rely solely on graph structure, overlooking the\nrich semantic signals encoded in textual attributes. We introduce SEMMA, a\ndual-module KGFM that systematically integrates transferable textual semantics\nalongside structure. SEMMA leverages Large Language Models (LLMs) to enrich\nrelation identifiers, generating semantic embeddings that subsequently form a\ntextual relation graph, which is fused with the structural component. Across 54\ndiverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully\ninductive link prediction. Crucially, we show that in more challenging\ngeneralization settings, where the test-time relation vocabulary is entirely\nunseen, structural methods collapse while SEMMA is 2x more effective. Our\nfindings demonstrate that textual semantics are critical for generalization in\nsettings where structure alone fails, highlighting the need for foundation\nmodels that unify structural and linguistic signals in knowledge reasoning.", "AI": {"tldr": "SEMMA is a dual-module Knowledge Graph Foundation Model that combines textual semantics with graph structure for improved reasoning and link prediction.", "motivation": "To improve knowledge graph reasoning by integrating transferable textual semantics along with graph structure, addressing the limitations of existing models that only consider structure.", "method": "SEMMA leverages Large Language Models to enrich relation identifiers, generating semantic embeddings that form a textual relation graph, which is then fused with the structural aspect of the model.", "result": "SEMMA outperforms purely structural baselines in inductive link prediction across 54 diverse knowledge graphs, particularly excelling in generalization settings with unseen relation vocabulary.", "conclusion": "Textual semantics play a crucial role in enhancing generalization capabilities in knowledge reasoning, suggesting the need for models that integrate both structural and linguistic information.", "key_contributions": ["Introduction of SEMMA as a dual-module KGFM", "Demonstration of the importance of textual semantics in generalization", "Significant performance improvement over purely structural models in challenging settings."], "limitations": "", "keywords": ["Knowledge Graphs", "Machine Learning", "Large Language Models", "Inductive Link Prediction", "Textual Semantics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20428", "pdf": "https://arxiv.org/pdf/2505.20428.pdf", "abs": "https://arxiv.org/abs/2505.20428", "title": "The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project", "authors": ["Angelina A. Aquino", "Lester James V. Miranda", "Elsie Marie T. Or"], "categories": ["cs.CL"], "comment": "Link to treebank:\n  https://huggingface.co/datasets/UD-Filipino/UD_Tagalog-NewsCrawl ; All\n  authors contributed equally in this work", "summary": "This paper presents UD-NewsCrawl, the largest Tagalog treebank to date,\ncontaining 15.6k trees manually annotated according to the Universal\nDependencies framework. We detail our treebank development process, including\ndata collection, pre-processing, manual annotation, and quality assurance\nprocedures. We provide baseline evaluations using multiple transformer-based\nmodels to assess the performance of state-of-the-art dependency parsers on\nTagalog. We also highlight challenges in the syntactic analysis of Tagalog\ngiven its distinctive grammatical properties, and discuss its implications for\nthe annotation of this treebank. We anticipate that UD-NewsCrawl and our\nbaseline model implementations will serve as valuable resources for advancing\ncomputational linguistics research in underrepresented languages like Tagalog.", "AI": {"tldr": "UD-NewsCrawl is the largest Tagalog treebank with 15.6k trees, focusing on syntactic analysis and baseline evaluations of dependency parsers using transformer models.", "motivation": "To provide a robust resource for computational linguistics research in underrepresented languages, specifically Tagalog.", "method": "The paper details the development process of the UD-NewsCrawl treebank, including data collection, pre-processing, manual annotation, and quality assurance, as well as baseline evaluations using transformer-based models.", "result": "Baseline evaluations demonstrated the performance of state-of-the-art dependency parsers on Tagalog, addressing unique grammatical challenges.", "conclusion": "UD-NewsCrawl offers significant resources for advancing research in Tagalog and potentially other underrepresented languages due to its comprehensive treebank.", "key_contributions": ["Largest Tagalog treebank to date with 15.6k trees", "Implementation of baseline evaluations for dependency parsers", "Insights into syntactic challenges and grammatical properties of Tagalog."], "limitations": "", "keywords": ["Tagalog", "treebank", "dependency parsing", "Universal Dependencies", "computational linguistics"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2505.20429", "pdf": "https://arxiv.org/pdf/2505.20429.pdf", "abs": "https://arxiv.org/abs/2505.20429", "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy", "authors": ["Shuhao Guan", "Moule Lin", "Cheng Xu", "Xinyi Liu", "Jinman Zhao", "Jiexin Fan", "Qi Xu", "Derek Greene"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 main", "summary": "This paper introduces PreP-OCR, a two-stage pipeline that combines document\nimage restoration with semantic-aware post-OCR correction to improve text\nextraction from degraded historical documents. Our key innovation lies in\njointly optimizing image clarity and linguistic consistency. First, we generate\nsynthetic image pairs with randomized text fonts, layouts, and degradations. An\nimage restoration model is trained on this synthetic data, using\nmulti-directional patch extraction and fusion to process large images. Second,\na ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,\naddresses any remaining OCR errors. Detailed experiments on 13,831 pages of\nreal historical documents in English, French, and Spanish show that PreP-OCR\npipeline reduces character error rates by 63.9-70.3\\% compared to OCR on raw\nimages. Our pipeline demonstrates the potential of integrating image\nrestoration with linguistic error correction for digitizing historical\narchives.", "AI": {"tldr": "PreP-OCR is a two-stage pipeline for improving text extraction from degraded historical documents by combining image restoration and post-OCR correction.", "motivation": "To enhance text extraction accuracy from degraded historical documents by addressing both image quality and OCR linguistic errors.", "method": "The approach consists of two stages: first, generating synthetic image pairs for training an image restoration model; second, using a ByT5 model as a post-corrector to fix OCR errors on historical text.", "result": "PreP-OCR significantly reduces character error rates by 63.9-70.3% when tested on 13,831 pages of real historical documents in multiple languages.", "conclusion": "Integrating image restoration with linguistic error correction shows promise for effectively digitizing historical archives.", "key_contributions": ["Development of a two-stage PreP-OCR pipeline", "Use of synthetic data for training image restoration model", "Demonstrated significant improvement in OCR accuracy on degraded documents"], "limitations": "", "keywords": ["OCR", "image restoration", "text extraction", "historical documents", "linguistic consistency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.21362", "pdf": "https://arxiv.org/pdf/2505.21362.pdf", "abs": "https://arxiv.org/abs/2505.21362", "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "AI": {"tldr": "This paper introduces a framework to evaluate how large language models (LLMs) adapt their responses based on users' sociodemographic characteristics through dialogue history and explicit profiles.", "motivation": "The need for effective engagement by LLMs that adjusts responses to users' sociodemographic characteristics.", "method": "A multi-agent pipeline constructs a synthetic dataset that pairs dialogue histories with user profiles, and evaluates model behavior consistency across modalities.", "result": "Most LLMs adjust their expressed values in response to demographic changes, with variations in consistency; models with stronger reasoning capabilities show better alignment.", "conclusion": "Reasoning ability in LLMs is crucial for effective adaptation to users' sociodemographic attributes, suggesting a focus for future improvements.", "key_contributions": ["Proposed framework for evaluating LLM adaptation to sociodemographic attributes", "Demonstrated the impact of reasoning capabilities on adaptation consistency", "Constructed a synthetic dataset for testing model responses"], "limitations": "The study relies on synthetic data and may not fully capture real-world interactions.", "keywords": ["large language models", "sociodemographic adaptation", "multi-turn dialogue", "user profiles", "reasoning capabilities"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20438", "pdf": "https://arxiv.org/pdf/2505.20438.pdf", "abs": "https://arxiv.org/abs/2505.20438", "title": "HAMburger: Accelerating LLM Inference via Token Smashing", "authors": ["Jingyu Liu", "Ce Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.", "AI": {"tldr": "HAMburger is a new model for optimizing Large Language Model (LLM) inference by enhancing computation and memory efficiency through innovative resource allocation.", "motivation": "The paper addresses the inefficiencies in LLM inference, where each token currently requires a separate forward pass and KV cache, potentially leading to sub-optimal performance.", "method": "The authors introduce HAMburger, which stacks a compositional embedder and a micro-step decoder to allow multiple tokens to be generated per step and utilize a speculative decoding framework.", "result": "HAMburger reduces KV cache computation by up to 2x and achieves up to 2x throughput (TPS), while maintaining quality across tasks with varying context lengths.", "conclusion": "The method provides a hardware-agnostic approach that greatly improves inference efficiency and scalability of LLMs, particularly for challenging computation- and memory-constrained environments.", "key_contributions": ["Introduction of HAMburger for efficient LLM inference", "Performance improvements in KV cache usage and throughput", "Speculative decoding framework for enhanced token generation"], "limitations": "", "keywords": ["Large Language Model", "LLM optimization", "inference efficiency", "speculative decoding", "KV cache"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21396", "pdf": "https://arxiv.org/pdf/2505.21396.pdf", "abs": "https://arxiv.org/abs/2505.21396", "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.", "AI": {"tldr": "This paper examines how augmenting LLMs with relevant data during idea generation can enhance the quality of research ideas, finding significant improvements through data incorporation.", "motivation": "To address the challenges faced by LLM-generated research ideas related to feasibility and effectiveness, especially in the social science domain.", "method": "The study introduces two methods: integrating metadata during the idea generation phase to steer LLMs towards feasible research directions, and employing automatic validation during the selection phase to evaluate the plausibility of the generated ideas.", "result": "Experiments show that using metadata leads to a 20% improvement in the feasibility of generated ideas and a 7% increase in the quality of ideas selected through automatic validation.", "conclusion": "The incorporation of relevant data during the idea generation and selection processes enhances the quality and feasibility of LLM-generated research ideas, demonstrating its utility in academic settings.", "key_contributions": ["Introduced metadata guidance for LLM-generated ideas", "Implemented automatic validation for idea selection", "Demonstrated improved feasibility and quality of research ideas in a human study"], "limitations": "", "keywords": ["Large Language Models", "Idea Generation", "Research Ideas", "Metadata", "Automatic Validation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20445", "pdf": "https://arxiv.org/pdf/2505.20445.pdf", "abs": "https://arxiv.org/abs/2505.20445", "title": "In-context Language Learning for Endangered Languages in Speech Recognition", "authors": ["Zhaolin Li", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With approximately 7,000 languages spoken worldwide, current large language\nmodels (LLMs) support only a small subset. Prior research indicates LLMs can\nlearn new languages for certain tasks without supervised data. We extend this\ninvestigation to speech recognition, investigating whether LLMs can learn\nunseen, low-resource languages through in-context learning (ICL). With\nexperiments on four diverse endangered languages that LLMs have not been\ntrained on, we find that providing more relevant text samples enhances\nperformance in both language modelling and Automatic Speech Recognition (ASR)\ntasks. Furthermore, we show that the probability-based approach outperforms the\ntraditional instruction-based approach in language learning. Lastly, we show\nICL enables LLMs to achieve ASR performance that is comparable to or even\nsurpasses dedicated language models trained specifically for these languages,\nwhile preserving the original capabilities of the LLMs.", "AI": {"tldr": "This paper investigates the ability of large language models (LLMs) to learn unseen, low-resource languages in speech recognition tasks through in-context learning (ICL), demonstrating that relevant sample provision can enhance performance significantly.", "motivation": "To explore whether LLMs can learn low-resource languages using in-context learning for speech recognition, addressing the limitation of existing LLMs supporting only a small number of languages.", "method": "Experiments on four endangered languages not previously seen by LLMs, testing the effectiveness of in-context learning through varying relevant text samples and comparing probability-based and instruction-based approaches.", "result": "LLMs can achieve Automatic Speech Recognition performance comparable to dedicated language models for the tested low-resource languages when utilizing in-context learning and more relevant training samples.", "conclusion": "In-context learning allows LLMs to effectively learn and perform tasks in low-resource languages, demonstrating that they can surpass traditional models designed specifically for those languages.", "key_contributions": ["Demonstrated LLMs can learn unseen languages via in-context learning for ASR tasks.", "Showed that relevant text samples improve performance in language modeling and ASR.", "Established that a probability-based approach is more effective than traditional instruction-based methods."], "limitations": "Focused on a small number of endangered languages, may not generalize to all low-resource languages.", "keywords": ["language models", "speech recognition", "in-context learning", "low-resource languages", "automatic speech recognition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20451", "pdf": "https://arxiv.org/pdf/2505.20451.pdf", "abs": "https://arxiv.org/abs/2505.20451", "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries", "authors": ["Sahana Ramnath", "Anurag Mudgil", "Brihi Joshi", "Skyler Hallinan", "Xiang Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Today, large language models are widely used as judges to evaluate responses\nfrom other language models. Hence, it is imperative to benchmark and improve\nthese LLM-judges on real-world language model usage: a typical human-assistant\nconversation is lengthy, and shows significant diversity in topics, intents,\nand requirements across turns, e.g. social interactions, task requests,\nfeedback. We present Amulet, a framework that leverages pertinent linguistic\nconcepts of dialog-acts and maxims to improve the accuracy of LLM-judges on\npreference data with complex, multi-turn conversational context. Amulet\npresents valuable insights about (a) the communicative structures and intents\npresent in the conversation (dialog acts), and (b) the satisfaction of\nconversational principles (maxims) by the preference responses, and uses them\nto make judgments. On four challenging datasets, Amulet shows that (a) humans\nfrequently (60 to 70 percent of the time) change their intents from one turn of\nthe conversation to the next, and (b) in 75 percent of instances, the\npreference responses can be differentiated via dialog acts and/or maxims,\nreiterating the latter's significance in judging such data. Amulet can be used\neither as a judge by applying the framework to a single LLM, or integrated into\na jury with different LLM judges; our judges and juries show strong\nimprovements on relevant baselines for all four datasets.", "AI": {"tldr": "Amulet is a framework designed to enhance the accuracy of language model judges in complex, multi-turn conversations by utilizing dialog acts and conversational maxims.", "motivation": "To benchmark and improve large language model judges in real-world conversational contexts, accounting for the diversity of topics, intents, and requirements in human-assistant interactions.", "method": "Amulet leverages linguistic concepts such as dialog acts and conversational maxims to evaluate preference responses within multi-turn conversational contexts, demonstrating how frequently human intents change during dialogues.", "result": "Amulet improves judgment accuracy on four challenging datasets, revealing that human intents change frequently in conversations and that dialog acts and maxims significantly aid in differentiating preference responses.", "conclusion": "Amulet can function independently as a judge for a single LLM or as part of a jury of LLM judges, yielding substantial improvements on relevant baseline performances.", "key_contributions": ["Introduces a framework for improving LLM-judges based on dialog acts and maxims", "Demonstrates significant changes in human intents across conversation turns", "Shows strong performance improvements across multiple datasets"], "limitations": "", "keywords": ["large language models", "judging", "dialog acts", "conversational maxims", "multi-turn interactions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20482", "pdf": "https://arxiv.org/pdf/2505.20482.pdf", "abs": "https://arxiv.org/abs/2505.20482", "title": "Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding", "authors": ["Vibhor Agarwal", "Arjoo Gupta", "Suparna De", "Nishanth Sastry"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at International AAAI Conference on Web and Social Media\n  (ICWSM) 2025", "summary": "Understanding online conversations has attracted research attention with the\ngrowth of social networks and online discussion forums. Content analysis of\nposts and replies in online conversations is difficult because each individual\nutterance is usually short and may implicitly refer to other posts within the\nsame conversation. Thus, understanding individual posts requires capturing the\nconversational context and dependencies between different parts of a\nconversation tree and then encoding the context dependencies between posts and\ncomments/replies into the language model.\n  To this end, we propose a general-purpose mechanism to discover appropriate\nconversational context for various aspects about an online post in a\nconversation, such as whether it is informative, insightful, interesting or\nfunny. Specifically, we design two families of Conversation Kernels, which\nexplore different parts of the neighborhood of a post in the tree representing\nthe conversation and through this, build relevant conversational context that\nis appropriate for each task being considered. We apply our developed method to\nconversations crawled from slashdot.org, which allows users to apply highly\ndifferent labels to posts, such as 'insightful', 'funny', etc., and therefore\nprovides an ideal experimental platform to study whether a framework such as\nConversation Kernels is general-purpose and flexible enough to be adapted to\ndisparately different conversation understanding tasks.", "AI": {"tldr": "The paper proposes a mechanism for understanding individual posts in online conversations by capturing their contextual dependencies. It introduces Conversation Kernels to analyze various aspects of posts, tested on data from slashdot.org.", "motivation": "With the rise of social networks, understanding online conversations has become important, yet challenging due to the complexity and short nature of posts.", "method": "The paper develops two families of Conversation Kernels that explore the neighborhood of a post within a conversation tree to build relevant context for categorizing posts.", "result": "The findings demonstrate the adaptability of Conversation Kernels for tasks such as identifying if a post is informative, insightful, interesting, or funny.", "conclusion": "The framework proves flexible for various conversation understanding tasks, validated through experiments on user-labeled data from slashdot.org.", "key_contributions": ["Introduction of Conversation Kernels to capture conversational context.", "Application of the model to different labeling tasks on slashdot.org.", "Demonstration of general-purpose flexibility in understanding conversations."], "limitations": "", "keywords": ["conversational context", "Conversation Kernels", "social media analysis"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.20487", "pdf": "https://arxiv.org/pdf/2505.20487.pdf", "abs": "https://arxiv.org/abs/2505.20487", "title": "InFact: Informativeness Alignment for Improved LLM Factuality", "authors": ["Roi Cohen", "Russa Biswas", "Gerard de Melo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual completeness is a general term that captures how detailed and\ninformative a factually correct text is. For instance, the factual sentence\n``Barack Obama was born in the United States'' is factually correct, though\nless informative than the factual sentence ``Barack Obama was born in Honolulu,\nHawaii, United States''. Despite the known fact that LLMs tend to hallucinate\nand generate factually incorrect text, they might also tend to choose to\ngenerate factual text that is indeed factually correct and yet less informative\nthan other, more informative choices. In this work, we tackle this problem by\nproposing an informativeness alignment mechanism. This mechanism takes\nadvantage of recent factual benchmarks to propose an informativeness alignment\nobjective. This objective prioritizes answers that are both correct and\ninformative. A key finding of our work is that when training a model to\nmaximize this objective or optimize its preference, we can improve not just\ninformativeness but also factuality.", "AI": {"tldr": "The paper proposes an informativeness alignment mechanism for improving the informativeness of text generated by LLMs while maintaining factuality.", "motivation": "LLMs often generate factually correct but less informative text. The authors aim to address this issue to enhance the informativeness of generated content.", "method": "The authors introduce an informativeness alignment mechanism that utilizes recent factual benchmarks to create an objective that prioritizes responses that are both correct and informative.", "result": "The study demonstrates that training models under the informativeness alignment objective leads to improvements in both informativeness and factual correctness of generated text.", "conclusion": "The proposed method effectively aligns the generation process of LLMs towards producing more informative and factually correct text.", "key_contributions": ["Introduction of an informativeness alignment mechanism for LLMs.", "Development of an objective that balances factuality and informativeness.", "Empirical evidence showing improved performance in both aspects during model training."], "limitations": "", "keywords": ["factual completeness", "large language models", "informativeness alignment", "factuality", "text generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20496", "pdf": "https://arxiv.org/pdf/2505.20496.pdf", "abs": "https://arxiv.org/abs/2505.20496", "title": "Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages", "authors": ["Asif Shahriar", "Rifat Shahriyar", "M Saifur Rahman"], "categories": ["cs.CL"], "comment": null, "summary": "Conventional transformer models typically compress the information from all\ntokens in a sequence into a single \\texttt{[CLS]} token to represent global\ncontext-- an approach that can lead to information loss in tasks requiring\nlocalized or hierarchical cues. In this work, we introduce \\textit{Inceptive\nTransformer}, a modular and lightweight architecture that enriches\ntransformer-based token representations by integrating a multi-scale feature\nextraction module inspired by inception networks. Our model is designed to\nbalance local and global dependencies by dynamically weighting tokens based on\ntheir relevance to a particular task. Evaluation across a diverse range of\ntasks including emotion recognition (both English and Bangla), irony detection,\ndisease identification, and anti-COVID vaccine tweets classification shows that\nour models consistently outperform the baselines by 1\\% to 14\\% while\nmaintaining efficiency. These findings highlight the versatility and\ncross-lingual applicability of our method for enriching transformer-based\nrepresentations across diverse domains.", "AI": {"tldr": "Introduces Inceptive Transformer, a modular architecture enhancing transformer token representations for improved task performance and reduced information loss.", "motivation": "Conventional transformers often lose information by compressing sequences into a single token, which can hinder tasks requiring localized cues.", "method": "A modular and lightweight architecture that integrates a multi-scale feature extraction module, inspired by inception networks, to dynamically weight tokens based on task relevance.", "result": "The Inceptive Transformer consistently outperformed baselines by 1% to 14% in various tasks, demonstrating its efficiency and cross-lingual applicability.", "conclusion": "The model proves beneficial for enriching transformer representations across diverse domains, improving performance in localized and global tasks.", "key_contributions": ["Introduction of Inceptive Transformer architecture", "Dynamic weighting of tokens for task relevance", "Cross-lingual applicability in various domains"], "limitations": "", "keywords": ["Inceptive Transformer", "multi-scale feature extraction", "token representation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20500", "pdf": "https://arxiv.org/pdf/2505.20500.pdf", "abs": "https://arxiv.org/abs/2505.20500", "title": "Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism", "authors": ["Naba Rizvi", "Harper Strickland", "Saleha Ahmedi", "Aekta Kallepalli", "Isha Khirwadkar", "William Wu", "Imani N. S. Munyaka", "Nedjma Ousidhoum"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in decision-making tasks\nlike r\\'esum\\'e screening and content moderation, giving them the power to\namplify or suppress certain perspectives. While previous research has\nidentified disability-related biases in LLMs, little is known about how they\nconceptualize ableism or detect it in text. We evaluate the ability of four\nLLMs to identify nuanced ableism directed at autistic individuals. We examine\nthe gap between their understanding of relevant terminology and their\neffectiveness in recognizing ableist content in context. Our results reveal\nthat LLMs can identify autism-related language but often miss harmful or\noffensive connotations. Further, we conduct a qualitative comparison of human\nand LLM explanations. We find that LLMs tend to rely on surface-level keyword\nmatching, leading to context misinterpretations, in contrast to human\nannotators who consider context, speaker identity, and potential impact. On the\nother hand, both LLMs and humans agree on the annotation scheme, suggesting\nthat a binary classification is adequate for evaluating LLM performance, which\nis consistent with findings from prior studies involving human annotators.", "AI": {"tldr": "This paper evaluates the ability of large language models to detect nuanced ableism against autistic individuals, revealing strengths and limitations in their understanding and contextual recognition.", "motivation": "To investigate how large language models conceptualize and detect ableism, particularly against autistic individuals, amidst previous findings of disability-related biases in LLMs.", "method": "Four large language models were evaluated for their ability to identify ableist language directed at autistic individuals, alongside a qualitative comparison with human annotations.", "result": "The models can recognize autism-related terms but frequently miss harmful connotations, relying on keyword matching rather than contextual understanding.", "conclusion": "A binary classification scheme is adequate for evaluating LLM performance in recognizing ableism, aligning with human annotators' agreements in classification.", "key_contributions": ["Identification of limitations in LLMsâ ability to detect nuanced ableism against autistic individuals.", "Comparison between LLM and human annotation processes highlighting context consideration.", "Suggestions for improving LLM performance in recognizing harmful language through better contextual understanding."], "limitations": "The study is limited to four LLMs and their ability to recognize only specific ableist language without deeper contextual analysis.", "keywords": ["large language models", "ableism", "autistic individuals", "bias detection", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20501", "pdf": "https://arxiv.org/pdf/2505.20501.pdf", "abs": "https://arxiv.org/abs/2505.20501", "title": "Gatsby Without the 'E': Crafting Lipograms with LLMs", "authors": ["Rohan Balasubramanian", "Nitish Gokulakrishnan", "Syeda Jannatus Saba", "Steven Skiena"], "categories": ["cs.CL"], "comment": "7.5 pages", "summary": "Lipograms are a unique form of constrained writing where all occurrences of a\nparticular letter are excluded from the text, typified by the novel Gadsby,\nwhich daringly avoids all usage of the letter 'e'. In this study, we explore\nthe power of modern large language models (LLMs) by transforming the novel F.\nScott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented\nwith a range of techniques, from baseline methods like synonym replacement to\nsophisticated generative models enhanced with beam search and named entity\nanalysis. We show that excluding up to 3.6% of the most common letters (up to\nthe letter 'u') had minimal impact on the text's meaning, although translation\nfidelity rapidly and predictably decays with stronger lipogram constraints. Our\nwork highlights the surprising flexibility of English under strict constraints,\nrevealing just how adaptable and creative language can be.", "AI": {"tldr": "This study examines the transformation of 'The Great Gatsby' into a text without the letter 'e' using various methods, highlighting the adaptability of language under constraints.", "motivation": "To explore the capabilities of large language models (LLMs) in the context of constrained writing, specifically lipograms.", "method": "The study involved transforming 'The Great Gatsby' into an 'e'-less text using techniques such as synonym replacement and generative models enhanced with beam search and named entity analysis.", "result": "The findings indicate that excluding up to 3.6% of the most common letters minimally impacts meaning, though translation fidelity declines with more stringent constraints.", "conclusion": "The research reveals the flexibility of English under strict constraints, emphasizing the adaptability and creativity of language.", "key_contributions": ["Demonstration of LLMs' capabilities in constrained writing tasks.", "Assessment of the impact of letter exclusion on meaning retention.", "Insights into language adaptability under constraints."], "limitations": "The study primarily focuses on English and results may vary for other languages; performance may depend on the specific LLM used.", "keywords": ["lipograms", "large language models", "text transformation", "meaning retention", "language adaptability"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.20505", "pdf": "https://arxiv.org/pdf/2505.20505.pdf", "abs": "https://arxiv.org/abs/2505.20505", "title": "Large Language Models for IT Automation Tasks: Are We There Yet?", "authors": ["Md Mahadi Hassan", "John Salvador", "Akond Rahman", "Santu Karmaker"], "categories": ["cs.CL", "cs.SE"], "comment": "8 pages", "summary": "LLMs show promise in code generation, yet their effectiveness for IT\nautomation tasks, particularly for tools like Ansible, remains understudied.\nExisting benchmarks rely primarily on synthetic tasks that fail to capture the\nneeds of practitioners who use IT automation tools, such as Ansible. We present\nITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,\nconfiguring servers, managing files) where each task accounts for state\nreconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'\nability to generate functional Ansible automation scripts via dynamic execution\nin controlled environments. We evaluate 14 open-source LLMs, none of which\naccomplish pass@10 at a rate beyond 12%. To explain these low scores, we\nanalyze 1,411 execution failures across the evaluated LLMs and identify two\nmain categories of prevalent semantic errors: failures in state reconciliation\nrelated reasoning (44.87% combined from variable (11.43%), host (11.84%),\npath(11.63%), and template (9.97%) issues) and deficiencies in module-specific\nexecution knowledge (24.37% combined from Attribute and parameter (14.44%) and\nmodule (9.93%) errors). Our findings reveal key limitations in open-source\nLLMs' ability to track state changes and apply specialized module knowledge,\nindicating that reliable IT automation will require major advances in state\nreasoning and domain-specific execution understanding.", "AI": {"tldr": "The paper presents ITAB, a benchmark to evaluate LLMs' performance in generating Ansible scripts for IT automation tasks, revealing significant limitations in their understanding of state reconciliation and module-specific execution knowledge.", "motivation": "To address the inadequacy of existing benchmarks for evaluating LLMs in real-world IT automation tasks, particularly with tools like Ansible.", "method": "The study introduces the ITAB benchmark, consisting of 126 diverse IT automation tasks designed to test LLMs in generating functional Ansible scripts, evaluated through dynamic execution in controlled environments.", "result": "Evaluation of 14 open-source LLMs revealed that none achieved a pass@10 rate beyond 12%, with analysis of 1,411 execution failures highlighting prevalent semantic errors, particularly in state reconciliation and module knowledge.", "conclusion": "The findings indicate that current LLMs struggle with essential reasoning for IT automation, emphasizing the need for advancements in state reasoning and domain-specific knowledge to improve reliability.", "key_contributions": ["Introduction of the ITAB benchmark for IT automation tasks", "Evaluation of 14 open-source LLMs under real-world task conditions", "Identification of common semantic errors affecting LLM performance in generating Ansible scripts."], "limitations": "The study is limited to the evaluation of open-source LLMs and the tasks defined within the ITAB benchmark.", "keywords": ["LLM", "IT Automation", "Ansible", "Benchmark", "State Reconciliation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.20506", "pdf": "https://arxiv.org/pdf/2505.20506.pdf", "abs": "https://arxiv.org/abs/2505.20506", "title": "ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis", "authors": ["Hawau Olamide Toyin", "Rufael Marew", "Humaid Alblooshi", "Samar M. Magdy", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025 The dataset is available at\n  https://huggingface.co/datasets/MBZUAI/ArVoice", "summary": "We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech\ncorpus with diacritized transcriptions, intended for multi-speaker speech\nsynthesis, and can be useful for other tasks such as speech-based diacritic\nrestoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a\nnew professionally recorded set from six voice talents with diverse\ndemographics, (2) a modified subset of the Arabic Speech Corpus; and (3)\nhigh-quality synthetic speech from two commercial systems. The complete corpus\nconsists of a total of 83.52 hours of speech across 11 voices; around 10 hours\nconsist of human voices from 7 speakers. We train three open-source TTS and two\nvoice conversion systems to illustrate the use cases of the dataset. The corpus\nis available for research use.", "AI": {"tldr": "ArVoice is a multi-speaker Modern Standard Arabic speech corpus designed for various speech synthesis tasks.", "motivation": "To provide a high-quality, diacritized speech corpus for improving multi-speaker speech synthesis and related applications.", "method": "The corpus includes 83.52 hours of professional recordings from six diverse voice talents, a modified subset of the Arabic Speech Corpus, and high-quality synthetic speech from commercial systems. It features three open-source text-to-speech (TTS) and two voice conversion systems for demonstration.", "result": "The dataset successfully demonstrates its applicability in speech-based diacritic restoration, voice conversion, and deepfake detection, showcasing high-quality synthesis capabilities.", "conclusion": "ArVoice is made available for research use, providing a comprehensive resource for advancing Arabic speech synthesis technologies.", "key_contributions": ["Launch of a new Arabic speech synthesis corpus with diacritized transcriptions.", "Inclusion of diverse voice talents in the dataset.", "Demonstration of advanced TTS and voice conversion systems using the corpus."], "limitations": "", "keywords": ["speech corpus", "Modern Standard Arabic", "speech synthesis", "voice conversion", "diacritization"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.20511", "pdf": "https://arxiv.org/pdf/2505.20511.pdf", "abs": "https://arxiv.org/abs/2505.20511", "title": "Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yang Liu", "Pengxu Zhu", "Yun Xue", "Ziwei Gong", "Julia Hirschberg", "Bolei Ma"], "categories": ["cs.CL"], "comment": null, "summary": "While text-based emotion recognition methods have achieved notable success,\nreal-world dialogue systems often demand a more nuanced emotional understanding\nthan any single modality can offer. Multimodal Emotion Recognition in\nConversations (MERC) has thus emerged as a crucial direction for enhancing the\nnaturalness and emotional understanding of human-computer interaction. Its goal\nis to accurately recognize emotions by integrating information from various\nmodalities such as text, speech, and visual signals.\n  This survey offers a systematic overview of MERC, including its motivations,\ncore tasks, representative methods, and evaluation strategies. We further\nexamine recent trends, highlight key challenges, and outline future directions.\nAs interest in emotionally intelligent systems grows, this survey provides\ntimely guidance for advancing MERC research.", "AI": {"tldr": "A survey on Multimodal Emotion Recognition in Conversations (MERC) discussing its significance, methods, challenges, and future directions.", "motivation": "To enhance naturalness and emotional understanding in human-computer interaction through improved emotion recognition.", "method": "Systematic overview of MERC including core tasks, representative methods, and evaluation strategies.", "result": "Identifies motivations for MERC and discusses recent trends and key challenges in the field.", "conclusion": "The growth of interest in emotionally intelligent systems creates an urgent need for advancing MERC research.", "key_contributions": ["Systematic overview of MERC", "Identification of key challenges in multimodal emotion recognition", "Guidance on future research directions"], "limitations": "", "keywords": ["Multimodal", "Emotion Recognition", "Human-Computer Interaction", "Survey", "Dialogue Systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20538", "pdf": "https://arxiv.org/pdf/2505.20538.pdf", "abs": "https://arxiv.org/abs/2505.20538", "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy", "authors": ["Sebastian Antony Joseph", "Syed Murtaza Husain", "Stella S. R. Offner", "StÃ©phanie Juneau", "Paul Torrey", "Adam S. Bolton", "Juan P. Farias", "Niall Gaffney", "Greg Durrett", "Junyi Jessy Li"], "categories": ["cs.CL", "astro-ph.IM", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology.", "AI": {"tldr": "AstroVisBench is the first benchmark for evaluating LLMs in scientific computing and visualization, specifically in astronomy.", "motivation": "To address the challenge of evaluating LLM-mediated scientific workflows for their ability to derive correct scientific insights.", "method": "AstroVisBench evaluates LLMs on their ability to create astronomy-specific workflows and visualize results, using a novel LLM-as-a-judge method validated by professional astronomers.", "result": "The evaluation revealed a significant gap in state-of-the-art LLMs' capabilities as assistants in astronomy research.", "conclusion": "The findings provide a pathway for developing visualization-based workflows critical across various scientific fields.", "key_contributions": ["Introduction of AstroVisBench as a benchmark for LLMs in scientific workflows.", "Integration of LLM-as-a-judge for evaluating visualizations validated by experts.", "Identification of the limitations of current LLMs in astronomy-related tasks."], "limitations": "Focuses specifically on the astronomy domain, limiting generalizability to other scientific fields.", "keywords": ["Large Language Models", "AstroVisBench", "scientific computing", "data visualization", "astronomy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20546", "pdf": "https://arxiv.org/pdf/2505.20546.pdf", "abs": "https://arxiv.org/abs/2505.20546", "title": "Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline", "authors": ["Meng Lu", "Ruochen Zhang", "Ellie Pavlick", "Carsten Eickhoff"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit factual\ninconsistencies across languages, with significantly better performance in\nfactual recall tasks in English than in other languages. The causes of these\nfailures, however, remain poorly understood. Using mechanistic analysis\ntechniques, we uncover the underlying pipeline that LLMs employ, which involves\nusing the English-centric factual recall mechanism to process multilingual\nqueries and then translating English answers back into the target language. We\nidentify two primary sources of error: insufficient engagement of the reliable\nEnglish-centric mechanism for factual recall, and incorrect translation from\nEnglish back into the target language for the final answer. To address these\nvulnerabilities, we introduce two vector interventions, both independent of\nlanguages and datasets, to redirect the model toward better internal paths for\nhigher factual consistency. Our interventions combined increase the recall\naccuracy by over 35 percent for the lowest-performing language. Our findings\ndemonstrate how mechanistic insights can be used to unlock latent multilingual\ncapabilities in LLMs.", "AI": {"tldr": "The paper investigates factual inconsistency in multilingual LLMs, revealing underlying mechanisms and proposing interventions to improve accuracy.", "motivation": "Multilingual LLMs perform better in English than in other languages, leading to inconsistent factual recall. Understanding and addressing these failures is crucial for their effective use.", "method": "The authors conducted a mechanistic analysis to identify the pipeline LLMs use for multilingual queries and devised two vector interventions to enhance factual consistency.", "result": "The proposed interventions increased factual recall accuracy by over 35% for the lowest-performing language, addressing identified sources of errors in LLMs.", "conclusion": "Mechanistic insights can be leveraged to improve multilingual performance in LLMs, unlocking their potential across languages.", "key_contributions": ["Identification of English-centric factual recall mechanisms in multilingual LLMs.", "Introduction of two vector interventions to enhance factual consistency across languages.", "Demonstration of over 35% improvement in recall accuracy for low-performing languages."], "limitations": "The interventions are not tested across all LLM architectures or all languages beyond the lowest-performing ones.", "keywords": ["multilingual LLMs", "factual consistency", "mechanistic analysis", "vector interventions", "language translation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20564", "pdf": "https://arxiv.org/pdf/2505.20564.pdf", "abs": "https://arxiv.org/abs/2505.20564", "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages", "authors": ["Chris Emezue", "The NaijaVoices Community", "Busayo Awobade", "Abraham Owodunni", "Handel Emezue", "Gloria Monica Tobechukwu Emezue", "Nefertiti Nneoma Emezue", "Sewade Ogun", "Bunmi Akinremi", "David Ifeoluwa Adelani", "Chris Pal"], "categories": ["cs.CL"], "comment": "Accepted for publication at Interspeech 2025", "summary": "The development of high-performing, robust, and reliable speech technologies\ndepends on large, high-quality datasets. However, African languages --\nincluding our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to\ninsufficient data. Popular voice-enabled technologies do not support any of the\n2000+ African languages, limiting accessibility for circa one billion people.\nWhile previous dataset efforts exist for the target languages, they lack the\nscale and diversity needed for robust speech models. To bridge this gap, we\nintroduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+\nspeakers. We outline our unique data collection approach, analyze its acoustic\ndiversity, and demonstrate its impact through finetuning experiments on\nautomatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%\n(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'\npotential to advance multilingual speech processing for African languages.", "AI": {"tldr": "Proposes the NaijaVoices dataset, a large-scale speech-text resource for African languages, to enhance speech technology performance.", "motivation": "To address the under-representation of African languages in speech technology, which limits accessibility for millions.", "method": "Introduced the NaijaVoices dataset, consisting of 1,800 hours of speech collected from 5,000+ speakers, and conducted finetuning experiments on existing speech recognition models.", "result": "Finetuning with NaijaVoices achieved average WER improvements of 75.86% for Whisper, 52.06% for MMS, and 42.33% for XLSR.", "conclusion": "NaijaVoices has significant potential to enhance multilingual speech processing capabilities for African languages.", "key_contributions": ["Development of the NaijaVoices dataset with significant scale and diversity.", "Unique data collection approach that can serve as a model for other languages.", "Demonstrated improvements in ASR performance using the dataset."], "limitations": "", "keywords": ["speech technology", "African languages", "speech datasets", "automatic speech recognition", "multilingual processing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.20571", "pdf": "https://arxiv.org/pdf/2505.20571.pdf", "abs": "https://arxiv.org/abs/2505.20571", "title": "Emotion Classification In-Context in Spanish", "authors": ["Bipul Thapa", "Gabriel Cofre"], "categories": ["cs.CL", "cs.LG"], "comment": "This paper has been accepted and presented at the 4th International\n  Conference on Applied Intelligence and Informatics (AII 2024). The final\n  version will appear in the official conference proceedings. This preprint is\n  provided to ensure the timely dissemination of the research prior to formal\n  publication", "summary": "Classifying customer feedback into distinct emotion categories is essential\nfor understanding sentiment and improving customer experience. In this paper,\nwe classify customer feedback in Spanish into three emotion\ncategories--positive, neutral, and negative--using advanced NLP and ML\ntechniques. Traditional methods translate feedback from widely spoken languages\nto less common ones, resulting in a loss of semantic integrity and contextual\nnuances inherent to the original language. To address this limitation, we\npropose a hybrid approach that combines TF-IDF with BERT embeddings,\neffectively transforming Spanish text into rich numerical representations that\npreserve the semantic depth of the original language by using a Custom Stacking\nEnsemble (CSE) approach. To evaluate emotion classification, we utilize a range\nof models, including Logistic Regression, KNN, Bagging classifier with LGBM,\nand AdaBoost. The CSE model combines these classifiers as base models and uses\na one-vs-all Logistic Regression as the meta-model. Our experimental results\ndemonstrate that CSE significantly outperforms the individual and BERT model,\nachieving a test accuracy of 93.3% on the native Spanish dataset--higher than\nthe accuracy obtained from the translated version. These findings underscore\nthe challenges of emotion classification in Spanish and highlight the\nadvantages of combining vectorization techniques like TF-IDF with BERT for\nimproved accuracy. Our results provide valuable insights for businesses seeking\nto leverage emotion classification to enhance customer feedback analysis and\nservice improvements.", "AI": {"tldr": "This paper presents a novel hybrid approach using TF-IDF and BERT embeddings for classifying customer feedback in Spanish into emotion categories, improving accuracy through a Custom Stacking Ensemble model.", "motivation": "Classifying customer feedback into distinct emotion categories is essential for understanding sentiment and improving customer experience, especially in non-English languages.", "method": "The study employs a hybrid model that combines TF-IDF vectorization with BERT embeddings along with a Custom Stacking Ensemble approach to classify Spanish customer feedback into positive, neutral, and negative emotions.", "result": "The proposed CSE model achieves a test accuracy of 93.3% on a native Spanish dataset, outperforming individual classifiers and models using translated feedback.", "conclusion": "Combining vectorization techniques like TF-IDF with BERT allows for better preservation of semantic context in emotion classification, providing essential insights for businesses to enhance customer feedback analysis.", "key_contributions": ["Introduction of a hybrid approach that combines TF-IDF with BERT embeddings", "Development of a Custom Stacking Ensemble (CSE) model for improved classification accuracy", "Empirical results demonstrating significantly better performance compared to traditional methods."], "limitations": "", "keywords": ["Customer feedback", "Emotion classification", "Spanish", "Machine learning", "Natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20591", "pdf": "https://arxiv.org/pdf/2505.20591.pdf", "abs": "https://arxiv.org/abs/2505.20591", "title": "Effectiveness of Prompt Optimization in NL2SQL Systems", "authors": ["Sairam Gurajada", "Eser Kandogan", "Sajjadur Rahman"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "NL2SQL approaches have greatly benefited from the impressive capabilities of\nlarge language models (LLMs). In particular, bootstrapping an NL2SQL system for\na specific domain can be as simple as instructing an LLM with sufficient\ncontextual information, such as schema details and translation demonstrations.\nHowever, building an accurate system still requires the rigorous task of\nselecting the right context for each query-including identifying relevant\nschema elements, cell values, and suitable exemplars that help the LLM\nunderstand domain-specific nuances. Retrieval-based methods have become the\ngo-to approach for identifying such context. While effective, these methods\nintroduce additional inference-time costs due to the retrieval process.\n  In this paper, we argue that production scenarios demand high-precision,\nhigh-performance NL2SQL systems, rather than simply high-quality SQL\ngeneration, which is the focus of most current NL2SQL approaches. In such\nscenarios, the careful selection of a static set of exemplars-capturing the\nintricacies of the query log, target database, SQL constructs, and execution\nlatencies-plays a more crucial role than exemplar selection based solely on\nsimilarity. The key challenge, however, lies in identifying a representative\nset of exemplars for a given production setting. To this end, we propose a\nprompt optimization framework that not only addresses the high-precision\nrequirement but also optimizes the performance of the generated SQL through\nmulti-objective optimization. Preliminary empirical analysis demonstrates the\neffectiveness of the proposed framework.", "AI": {"tldr": "This paper presents a prompt optimization framework for NL2SQL systems that focuses on selecting a static set of exemplars to enhance both precision and performance in SQL generation, addressing the limitations of existing retrieval-based methods.", "motivation": "The need for high-precision and high-performance NL2SQL systems in production scenarios, as current approaches primarily focus on SQL quality rather than context selection and system performance.", "method": "The authors propose a prompt optimization framework that utilizes multi-objective optimization for selecting a representative set of exemplars, capturing the intricacies of queries, databases, and SQL performance.", "result": "Empirical analysis shows that the proposed method effectively improves SQL generation performance while maintaining high precision in production settings.", "conclusion": "The prompt optimization framework effectively addresses the nuanced requirements of NL2SQL systems in production by optimizing both the selection of exemplars and the performance of SQL queries.", "key_contributions": ["Introduction of a prompt optimization framework for NL2SQL", "Focus on static exemplar selection for production scenarios", "Demonstration of effectiveness through empirical analysis"], "limitations": "", "keywords": ["NL2SQL", "Large Language Models", "Exemplar Selection", "Multi-Objective Optimization", "SQL Generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20606", "pdf": "https://arxiv.org/pdf/2505.20606.pdf", "abs": "https://arxiv.org/abs/2505.20606", "title": "Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation", "authors": ["Dancheng Liu", "Amir Nassereldine", "Chenhui Xu", "Jinjun Xiong"], "categories": ["cs.CL", "cs.MM"], "comment": "in submission", "summary": "Whisper's robust performance in automatic speech recognition (ASR) is often\nattributed to its massive 680k-hour training set, an impractical scale for most\nresearchers. In this work, we examine how linguistic and acoustic diversity in\ntraining data affect the robustness of the ASR model and reveal that\ntranscription generalization is primarily driven by acoustic variation rather\nthan linguistic richness. We find that targeted acoustic augmentation methods\ncould significantly improve the generalization ability of ASR models, reducing\nword-error rates by up to 19.24 percent on unseen datasets when training on the\n960-hour Librispeech dataset. These findings highlight strategic acoustically\nfocused data augmentation as a promising alternative to massive datasets for\nbuilding robust ASR models, offering a potential solution to future foundation\nASR models when massive human speech data is lacking.", "AI": {"tldr": "This paper explores how acoustic and linguistic diversity in training data impacts ASR model robustness, suggesting that acoustic variation is more crucial for generalization than linguistic richness.", "motivation": "To understand the factors influencing the robustness of ASR models, particularly in the context of limited training data.", "method": "The study investigates the effect of linguistic and acoustic diversity on ASR performance, employing targeted acoustic augmentation methods to evaluate their impact on generalization.", "result": "Using targeted acoustic augmentation techniques, the paper achieves a reduction in word-error rates by up to 19.24% on unseen datasets when trained on the 960-hour Librispeech dataset.", "conclusion": "Strategic acoustically focused data augmentation can serve as a feasible alternative to large datasets for enhancing ASR model robustness, especially when data is scarce.", "key_contributions": ["Demonstrates that acoustic variation is more important than linguistic richness for ASR generalization.", "Introduces targeted acoustic augmentation methods that significantly improve ASR performance.", "Offers insights into building robust ASR models without relying on massive datasets."], "limitations": "The findings may not fully generalize across all languages or dialects due to the specific datasets used.", "keywords": ["Automatic Speech Recognition", "Acoustic Variation", "Data Augmentation", "Generalization", "Robustness"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.20613", "pdf": "https://arxiv.org/pdf/2505.20613.pdf", "abs": "https://arxiv.org/abs/2505.20613", "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning", "authors": ["Ziju Shen", "Naohao Huang", "Fanyi Yang", "Yutong Wang", "Guoxiong Gao", "Tianyi Xu", "Jiedong Jiang", "Wanyi He", "Pu Yang", "Mengzhou Sun", "Haocheng Ju", "Peihao Wu", "Bryan Dai", "Bin Dong"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64).", "AI": {"tldr": "The paper presents REAL-Prover, an advanced theorem prover for Lean 4 that utilizes a fine-tuned large language model to enhance performance on college-level mathematics problems.", "motivation": "To extend the capabilities of theorem provers from high-school mathematics to more advanced college-level mathematics.", "method": "REAL-Prover is developed using a fine-tuned large language model (REAL-Prover-v1) integrated with a retrieval system (Leansearch-PS), alongside a data extraction pipeline (HERALD-AF) for converting natural language math problems into formal statements and a new interactive environment (Jixia-interactive) for data collection.", "result": "REAL-Prover achieves a 23.7% success rate on the ProofNet dataset and a state-of-the-art 56.7% success rate on the new FATE-M benchmark focused on algebraic problems.", "conclusion": "The proposed system significantly enhances the ability of theorem provers to solve more complex mathematical problems and establishes new benchmarks for performance assessment.", "key_contributions": ["Introduction of REAL-Prover, a stepwise theorem prover for advanced mathematics.", "Development of HERALD-AF for converting natural language math problems into formal statements.", "Introduction of the FATE-M benchmark for evaluating algebraic problem-solving capabilities."], "limitations": "", "keywords": ["theorem proving", "Lean 4", "large language model", "formal verification", "mathematics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.20622", "pdf": "https://arxiv.org/pdf/2505.20622.pdf", "abs": "https://arxiv.org/abs/2505.20622", "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation", "authors": ["Ting Xu", "Zhichao Huang", "Jiankai Sun", "Shanbo Cheng", "Wai Lam"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "We present Sequential Policy Optimization for Simultaneous Machine\nTranslation (SeqPO-SiMT), a new policy optimization framework that defines the\nsimultaneous machine translation (SiMT) task as a sequential decision making\nproblem, incorporating a tailored reward to enhance translation quality while\nreducing latency. In contrast to popular Reinforcement Learning from Human\nFeedback (RLHF) methods, such as PPO and DPO, which are typically applied in\nsingle-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.\nThis intuitive framework allows the SiMT LLMs to simulate and refine the SiMT\nprocess using a tailored reward. We conduct experiments on six datasets from\ndiverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that\nSeqPO-SiMT consistently achieves significantly higher translation quality with\nlower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning\n(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17\nin the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context\nthan offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly\nrival the offline translation of high-performing LLMs, including\nQwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.", "AI": {"tldr": "A new framework called SeqPO-SiMT for simultaneous machine translation is introduced, optimizing translation quality and reducing latency via sequential decision making and tailored rewards.", "motivation": "To address the challenges of simultaneous machine translation (SiMT) as a sequential decision making problem, improving upon traditional methods that focus on single-step tasks.", "method": "The framework defines the SiMT task as a multi-step decision making problem, incorporating a tailored reward mechanism to enhance translation quality and reduce latency.", "result": "Experiments on six datasets demonstrate that SeqPO-SiMT achieves higher translation quality and lower latency compared to supervised fine-tuning models, with significant improvements in metrics like COMET and Average Lagging.", "conclusion": "SeqPO-SiMT can rival the performance of high-performing offline translation LLMs despite operating with significantly less context in SiMT settings.", "key_contributions": ["Introduction of the SeqPO-SiMT framework for simultaneous machine translation.", "Utilization of a tailored reward mechanism to enhance translation quality and efficiency.", "Demonstrated superior performance on several datasets compared to traditional supervised fine-tuning approaches."], "limitations": "", "keywords": ["Machine Translation", "Reinforcement Learning", "Sequential Decision Making", "Simultaneous Translation", "Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20624", "pdf": "https://arxiv.org/pdf/2505.20624.pdf", "abs": "https://arxiv.org/abs/2505.20624", "title": "POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization", "authors": ["Usman Naseem", "Juan Ren", "Saba Anwar", "Sarah Kohail", "Rudy Alexandro Garrido Veliz", "Robert Geislinger", "Aisha Jabr", "Idris Abdulmumin", "Laiba Qureshi", "Aarushi Ajay Borkar", "Maryam Ibrahim Mukhtar", "Abinew Ali Ayele", "Ibrahim Said Ahmad", "Adem Ali", "Martin Semmann", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Online polarization poses a growing challenge for democratic discourse, yet\nmost computational social science research remains monolingual, culturally\nnarrow, or event-specific. We introduce POLAR, a multilingual, multicultural,\nand multievent dataset with over 23k instances in seven languages from diverse\nonline platforms and real-world events. Polarization is annotated along three\naxes: presence, type, and manifestation, using a variety of annotation\nplatforms adapted to each cultural context. We conduct two main experiments:\n(1) we fine-tune six multilingual pretrained language models in both\nmonolingual and cross-lingual setups; and (2) we evaluate a range of open and\nclosed large language models (LLMs) in few-shot and zero-shot scenarios.\nResults show that while most models perform well on binary polarization\ndetection, they achieve substantially lower scores when predicting polarization\ntypes and manifestations. These findings highlight the complex, highly\ncontextual nature of polarization and the need for robust, adaptable approaches\nin NLP and computational social science. All resources will be released to\nsupport further research and effective mitigation of digital polarization\nglobally.", "AI": {"tldr": "Introduces POLAR, a multilingual and multicultural dataset for detecting online polarization, with experiments on language models showing variable success.", "motivation": "Address the challenge of online polarization and the limitations of existing computational social science research.", "method": "Introduced a dataset (POLAR) with over 23k instances across seven languages; fine-tuned multilingual language models and evaluated LLMs in few-shot and zero-shot scenarios.", "result": "Most models perform well in binary polarization detection but struggle with predicting types and manifestations of polarization.", "conclusion": "Highlights the complex nature of polarization and the need for adaptable approaches in NLP and computational social science.", "key_contributions": ["Creation of a multilingual, multicultural dataset for polarization", "Fine-tuning of multilingual pretrained language models", "Evaluation of LLMs in few-shot and zero-shot scenarios"], "limitations": "Limited focus on the complexity of polarization types beyond binary detection.", "keywords": ["polarization", "multilingual dataset", "NLP", "language models", "computational social science"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.20625", "pdf": "https://arxiv.org/pdf/2505.20625.pdf", "abs": "https://arxiv.org/abs/2505.20625", "title": "Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration", "authors": ["Sibo Xiao", "Zixin Lin", "Wenyang Gao", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). Existing works leverage agent-based divide-and-conquer\nmethods for processing long contexts. But these methods face crucial\nlimitations, including prohibitive accumulated latency and amplified\ninformation loss from excessive agent invocations, and the disruption of\ninherent textual dependencies by immoderate partitioning. In this paper, we\npropose a novel multi-agent framework XpandA (Expand-Agent) coupled with\nquestion-driven workflow and dynamic partitioning for robust long-context\nprocessing. XpandA overcomes these limitations through: 1) dynamic partitioning\nof long texts, which adaptively modulates the filling rate of context windows\nfor input sequences of vastly varying lengths; 2) question-guided protocol to\nupdate flat information ensembles within centralized shared memory,\nconstructing consistent inter-agent knowledge across partitions; and 3)\nselectively replaying specific partitions based on the state-tracking of\nquestion-information couples to promote the resolution of inverted-order\nstructures across partitions (e.g., flashbacks). We perform a comprehensive\nevaluation of XpandA on multiple long-context benchmarks with length varying\nfrom 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long\nsequences and its significant effectiveness in enhancing the long-context\ncapabilities of various LLMs by achieving 20\\% improvements and 1.5x inference\nspeedup over baselines of full-context, RAG and previous agent-based methods.", "AI": {"tldr": "XpandA is a novel multi-agent framework for processing long contexts in LLMs, improving speed and effectiveness.", "motivation": "The need for better long-context processing in LLMs due to limitations in existing agent-based methods.", "method": "A multi-agent framework coupled with dynamic partitioning and question-driven workflows.", "result": "XpandA achieves 20% improvements and 1.5x inference speedup over existing methods on long-context benchmarks.", "conclusion": "XpandA significantly enhances long-context capabilities of LLMs and addresses existing limitations effectively.", "key_contributions": ["Dynamic partitioning of long texts", "Question-guided protocol for consistent knowledge sharing", "Selective replay of partitions to resolve structured information"], "limitations": "", "keywords": ["long-context processing", "multi-agent framework", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20633", "pdf": "https://arxiv.org/pdf/2505.20633.pdf", "abs": "https://arxiv.org/abs/2505.20633", "title": "Test-Time Learning for Large Language Models", "authors": ["Jinwu Hu", "Zhitian Zhang", "Guohao Chen", "Xutao Wen", "Chao Shuai", "Wei Luo", "Bin Xiao", "Yuanqing Li", "Mingkui Tan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML2025", "summary": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation.", "AI": {"tldr": "This paper introduces a Test-Time Learning (TTL) framework for enhancing Large Language Models (LLMs) by dynamically adapting them to target domains using unlabeled test data, resulting in improved performance.", "motivation": "The motivation behind this research is to address the limitations of LLMs in generalizing to specialized domains and handling linguistic variations, which are significant challenges in their practical applications.", "method": "The authors propose a Test-Time Learning (TTL) paradigm (TLM) that minimizes input perplexity of unlabeled test data during testing to enhance LLM performance without requiring labeled data.", "result": "The experiments demonstrate that TLM improves LLM performance by at least 20% in domain knowledge adaptation, leveraging high-perplexity samples for optimization and employing Low-Rank Adaptation (LoRA) for model updates.", "conclusion": "The study concludes that TLM provides a significant advancement in the adaptability of LLMs to specialized domains while mitigating issues like catastrophic forgetting.", "key_contributions": ["Introduction of Test-Time Learning (TTL) paradigm for LLMs", "Innovative input perplexity minimization approach for performance enhancement", "Development of low-parameter updates through Low-Rank Adaptation (LoRA)"], "limitations": "", "keywords": ["Test-Time Learning", "Large Language Models", "Low-Rank Adaptation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20645", "pdf": "https://arxiv.org/pdf/2505.20645.pdf", "abs": "https://arxiv.org/abs/2505.20645", "title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models", "authors": ["Kai Chen", "Zihao He", "Taiwei Shi", "Kristina Lerman"], "categories": ["cs.CL"], "comment": null, "summary": "Steerability, or the ability of large language models (LLMs) to adapt outputs\nto align with diverse community-specific norms, perspectives, and communication\nstyles, is critical for real-world applications but remains under-evaluated. We\nintroduce Steer-Bench, a benchmark for assessing population-specific steering\nusing contrasting Reddit communities. Covering 30 contrasting subreddit pairs\nacross 19 domains, Steer-Bench includes over 10,000 instruction-response pairs\nand validated 5,500 multiple-choice question with corresponding silver labels\nto test alignment with diverse community norms. Our evaluation of 13 popular\nLLMs using Steer-Bench reveals that while human experts achieve an accuracy of\n81% with silver labels, the best-performing models reach only around 65%\naccuracy depending on the domain and configuration. Some models lag behind\nhuman-level alignment by over 15 percentage points, highlighting significant\ngaps in community-sensitive steerability. Steer-Bench is a benchmark to\nsystematically assess how effectively LLMs understand community-specific\ninstructions, their resilience to adversarial steering attempts, and their\nability to accurately represent diverse cultural and ideological perspectives.", "AI": {"tldr": "Steer-Bench evaluates the steerability of LLMs using community norms from Reddit.", "motivation": "To assess how well large language models adapt their outputs to align with diverse community-specific norms and perspectives.", "method": "Introduction of Steer-Bench, a benchmark with 30 subreddit pairs and over 10,000 instruction-response pairs to evaluate LLMs' alignment with community norms.", "result": "The best-performing LLMs achieved only around 65% accuracy, significantly lower than human experts' 81%, indicating gaps in steerability across communities.", "conclusion": "Steer-Bench systematically measures LLMs' effectiveness in understanding community-specific instructions and their ability to represent diverse perspectives.", "key_contributions": ["Introduction of Steer-Bench benchmark for LLM evaluation", "In-depth analysis of 13 popular LLMs across various domains", "Identification of significant gaps in community-sensitive steerability among LLMs"], "limitations": "Limited to specific subreddit pairs and does not cover all community norms; accuracy levels vary widely depending on domain and model configuration.", "keywords": ["steerability", "large language models", "community norms", "benchmark", "Reddit"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20650", "pdf": "https://arxiv.org/pdf/2505.20650.pdf", "abs": "https://arxiv.org/abs/2505.20650", "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information", "authors": ["Yan Wang", "Yang Ren", "Lingfei Qian", "Xueqing Peng", "Keyi Wang", "Yi Han", "Dongji Feng", "Xiao-Yang Liu", "Jimin Huang", "Qianqian Xie"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.", "AI": {"tldr": "FinTagging is a benchmark for evaluating LLMs in XBRL-based financial reporting focusing on structured information extraction and semantic alignment.", "motivation": "To assess LLM capabilities in handling XBRL tagging that includes both financial entity extraction and taxonomy alignment for improved financial reporting accuracy.", "method": "The paper introduces FinTagging, decomposing XBRL tagging into FinNI (financial entity extraction) and FinCL (taxonomy alignment) and evaluates several LLMs under zero-shot conditions.", "result": "LLMs show strong generalization in extracting information but struggle with precise concept alignment, particularly with similar taxonomy entries.", "conclusion": "Current LLMs are insufficient for fully automating XBRL tagging, highlighting the need for better semantic reasoning and schema-aware models.", "key_contributions": ["First comprehensive benchmark for LLMs in XBRL tagging", "Decomposes tagging into financial extraction and concept alignment tasks", "Analyzes performance of diverse LLMs under realistic testing conditions."], "limitations": "LLMs exhibit limitations in disambiguating closely related taxonomy entries in XBRL tagging.", "keywords": ["XBRL", "financial reporting", "LLMs", "semantic alignment", "information extraction"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2505.20654", "pdf": "https://arxiv.org/pdf/2505.20654.pdf", "abs": "https://arxiv.org/abs/2505.20654", "title": "Chinese Cyberbullying Detection: Dataset, Method, and Validation", "authors": ["Yi Zhu", "Xin Zou", "Xindong Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing cyberbullying detection benchmarks were organized by the polarity of\nspeech, such as \"offensive\" and \"non-offensive\", which were essentially hate\nspeech detection. However, in the real world, cyberbullying often attracted\nwidespread social attention through incidents. To address this problem, we\npropose a novel annotation method to construct a cyberbullying dataset that\norganized by incidents. The constructed CHNCI is the first Chinese\ncyberbullying incident detection dataset, which consists of 220,676 comments in\n91 incidents. Specifically, we first combine three cyberbullying detection\nmethods based on explanations generation as an ensemble method to generate the\npseudo labels, and then let human annotators judge these labels. Then we\npropose the evaluation criteria for validating whether it constitutes a\ncyberbullying incident. Experimental results demonstrate that the constructed\ndataset can be a benchmark for the tasks of cyberbullying detection and\nincident prediction. To the best of our knowledge, this is the first study for\nthe Chinese cyberbullying incident detection task.", "AI": {"tldr": "Introduction of a novel cyberbullying dataset and detection method focusing on incident-based organization.", "motivation": "Previous benchmarks for cyberbullying detection do not adequately capture the incident-based nature of real-world cyberbullying.", "method": "A novel annotation method for constructing a cyberbullying dataset organized by incidents, combining three detection methods with human validation of pseudo labels.", "result": "The CHNCI dataset consists of 220,676 comments across 91 incidents and serves as a benchmark for cyberbullying detection.", "conclusion": "This study is the first to address incident detection in Chinese cyberbullying, providing a valuable resource for future research.", "key_contributions": ["Introduction of the CHNCI dataset for cyberbullying incident detection", "First study focusing on incident-based cyberbullying in Chinese contexts", "Proposes an ensemble method for pseudo labeling in dataset construction"], "limitations": "", "keywords": ["cyberbullying", "dataset", "incident detection", "natural language processing", "Chinese"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.20658", "pdf": "https://arxiv.org/pdf/2505.20658.pdf", "abs": "https://arxiv.org/abs/2505.20658", "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge", "authors": ["Yue Fang", "Zhi Jin", "Jie An", "Hongshen Chen", "Xiaohong Chen", "Naijun Zhan"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures, published to ACL", "summary": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets.", "AI": {"tldr": "The paper presents a novel dataset for transforming natural language into Signal Temporal Logic and introduces a framework for this transformation, showing improved accuracy and diversity compared to existing methods.", "motivation": "The paper addresses the challenges faced in automatically transforming natural language (NL) into Signal Temporal Logic (STL) due to the lack of datasets and the limitations of manual transformation.", "method": "The authors created the STL-Diversity-Enhanced (STL-DivEn) dataset comprising 16,000 NL-STL samples. They developed a Knowledge-Guided STL Transformation (KGST) framework that utilizes a generate-then-refine approach based on LLMs and external knowledge for accurate transformation.", "result": "Statistical analyses indicate that STL-DivEn has greater diversity than existing datasets. The KGST framework demonstrates superior transformation accuracy compared to baseline models on both STL-DivEn and DeepSTL datasets.", "conclusion": "The proposed dataset and transformation framework significantly improve the process of converting natural language to STL, contributing to better formal specifications in cyber-physical systems.", "key_contributions": ["Introduction of the STL-Diversity-Enhanced dataset containing 16,000 NL-STL samples.", "Development of the Knowledge-Guided STL Transformation framework to enhance transformation accuracy.", "Validation of improvements in diversity and accuracy over existing datasets and baseline methods."], "limitations": "", "keywords": ["Signal Temporal Logic", "Natural Language Processing", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20660", "pdf": "https://arxiv.org/pdf/2505.20660.pdf", "abs": "https://arxiv.org/abs/2505.20660", "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism", "authors": ["Qinzhuo Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents have gained substantial attention due\nto their impressive capabilities to complete tasks through multiple\ninteractions within GUI environments. However, existing agents primarily focus\non enhancing the accuracy of individual actions and often lack effective\nmechanisms for detecting and recovering from errors. To address these\nshortcomings, we propose the BacktrackAgent, a robust framework that\nincorporates a backtracking mechanism to improve task completion efficiency.\nBacktrackAgent includes verifier, judger, and reflector components as modules\nfor error detection and recovery, while also applying judgment rewards to\nfurther enhance the agent's performance. Additionally, we develop a training\ndataset specifically designed for the backtracking mechanism, which considers\nthe outcome pages after action executions. Experimental results show that\nBacktrackAgent has achieved performance improvements in both task success rate\nand step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be\nreleased upon acceptance.", "AI": {"tldr": "BacktrackAgent is a new framework for GUI agents that incorporates a backtracking mechanism for error detection and recovery, improving task completion efficiency.", "motivation": "To enhance the capabilities of GUI agents by addressing errors in task execution and improving their efficiency in completing tasks.", "method": "BacktrackAgent utilizes a backtracking mechanism with three components: verifier, judger, and reflector, supplemented by judgment rewards and a specially designed training dataset for better performance.", "result": "Experimental results demonstrate that BacktrackAgent significantly improves both task success rates and step accuracy on the Mobile3M and Auto-UI benchmarks.", "conclusion": "The incorporation of backtracking mechanisms in GUI agents leads to more efficient task completion and enhanced performance, with data and code to be shared upon acceptance.", "key_contributions": ["Introduction of a backtracking mechanism for GUI agents", "Development of a training dataset tailored for backtracking", "Demonstrated performance improvements on key benchmarks."], "limitations": "", "keywords": ["GUI agents", "Backtracking", "Error recovery", "Task completion", "Machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20664", "pdf": "https://arxiv.org/pdf/2505.20664.pdf", "abs": "https://arxiv.org/abs/2505.20664", "title": "Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning", "authors": ["Yang He", "Xiao Ding", "Bibo Cai", "Yufei Zhang", "Kai Xiong", "Zhouhao Sun", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reasoning-augmented large language models (RLLMs) significantly enhance\ncomplex task performance through extended reasoning chains, they inevitably\nintroduce substantial unnecessary token consumption, particularly for simpler\nproblems where Short Chain-of-Thought (Short CoT) suffices. This overthinking\nphenomenon leads to inefficient resource usage without proportional accuracy\ngains. To address this issue, we propose Self-Route, a dynamic reasoning\nframework that automatically selects between general and reasoning modes based\non model capability estimation. Our approach introduces a lightweight\npre-inference stage to extract capability-aware embeddings from hidden layer\nrepresentations, enabling real-time evaluation of the model's ability to solve\nproblems. We further construct Gradient-10K, a model difficulty\nestimation-based dataset with dense complexity sampling, to train the router\nfor precise capability boundary detection. Extensive experiments demonstrate\nthat Self-Route achieves comparable accuracy to reasoning models while reducing\ntoken consumption by 30-55\\% across diverse benchmarks. The proposed framework\ndemonstrates consistent effectiveness across models with different parameter\nscales and reasoning paradigms, highlighting its general applicability and\npractical value.", "AI": {"tldr": "The paper introduces Self-Route, a dynamic reasoning framework that optimizes reasoning-augmented large language models (RLLMs) by reducing unnecessary token consumption while maintaining accuracy.", "motivation": "To tackle the inefficiencies in resource usage caused by reasoning-augmented large language models (RLLMs) when applied to simpler problems.", "method": "Self-Route utilizes a lightweight pre-inference stage to assess model capabilities and dynamically switches between reasoning and general modes based on capability-aware embeddings.", "result": "Self-Route reduces token consumption by 30-55% while achieving comparable accuracy to traditional reasoning models across various benchmarks.", "conclusion": "The framework is effective across models of different sizes and reasoning methods, showcasing its broad applicability in optimizing RLLMs.", "key_contributions": ["Introduction of Self-Route for dynamic reasoning mode selection", "Development of Gradient-10K dataset for model difficulty estimation", "Demonstration of token consumption reduction without loss of accuracy"], "limitations": "", "keywords": ["dynamic reasoning", "large language models", "self-route"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20674", "pdf": "https://arxiv.org/pdf/2505.20674.pdf", "abs": "https://arxiv.org/abs/2505.20674", "title": "Pretraining Language Models to Ponder in Continuous Space", "authors": ["Boyi Zeng", "Shixiang Song", "Siyuan Huang", "Yixuan Wang", "He Li", "Ziwei He", "Xinbing Wang", "Zhiyu Li", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM.", "AI": {"tldr": "This paper introduces a new method for integrating a pondering process into language models, enabling deeper cognitive processing and improved performance on language modeling tasks without needing human annotations.", "motivation": "To enhance cognitive processing in language models by mimicking the human pondering process before sentence articulation, thus improving model output quality.", "method": "The model employs a self-supervised learning approach to yield a weighted sum of token embeddings during a pondering phase, integrating this feedback into the subsequent forward pass.", "result": "Pondering language models demonstrate performance on par with conventional models with double the parameters, achieving substantial improvements on multiple downstream benchmarks, particularly with the Pythia models.", "conclusion": "The integration of pondering into language models is effective and can significantly enhance performance, suggesting a promising direction for future research in self-supervised learning for NLP.", "key_contributions": ["Introduction of the pondering process into language models", "Demonstration of effectiveness across multiple architectures", "Code availability for replication and further research"], "limitations": "", "keywords": ["language models", "self-supervised learning", "NLP", "pondering process", "benchmark evaluations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20679", "pdf": "https://arxiv.org/pdf/2505.20679.pdf", "abs": "https://arxiv.org/abs/2505.20679", "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations", "authors": ["Danush Khanna", "Pratinav Seth", "Sidhaarth Sredharan Murali", "Aditya Kumar Guru", "Siddharth Shukla", "Tanuj Tyagi", "Sandeep Chaurasia", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal\ncommunication, making its detection critical for safeguarding potential\nvictims. However, due to manipulation's nuanced and context-specific nature,\nidentifying manipulative language in complex, multi-turn, and multi-person\nconversations remains a significant challenge for large language models (LLMs).\nTo address this gap, we introduce the MultiManip dataset, comprising 220\nmulti-turn, multi-person dialogues balanced between manipulative and\nnon-manipulative interactions, all drawn from reality shows that mimic\nreal-world scenarios. For manipulative interactions, it includes 11 distinct\nmanipulations depicting real-life scenarios. We conduct extensive evaluations\nof state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various\nprompting strategies. Despite their capabilities, these models often struggle\nto detect manipulation effectively. To overcome this limitation, we propose\nSELF-PERCEPT, a novel, two-stage prompting framework inspired by\nSelf-Perception Theory, demonstrating strong performance in detecting\nmulti-person, multi-turn mental manipulation. Our code and data are publicly\navailable at https://github.com/danushkhanna/self-percept .", "AI": {"tldr": "This paper introduces the MultiManip dataset for detecting mental manipulation in dialogues and proposes a novel two-stage prompting framework, SELF-PERCEPT, which enhances detection performance.", "motivation": "The need for effective detection of mental manipulation in interpersonal communication to protect potential victims.", "method": "The authors introduce the MultiManip dataset with 220 dialogues and evaluate state-of-the-art LLMs, proposing a self-perception based prompting framework called SELF-PERCEPT.", "result": "Performance evaluations revealed that current LLMs struggled with manipulation detection, but SELF-PERCEPT showed strong capabilities.", "conclusion": "SELF-PERCEPT significantly improves the detection of manipulative language in multi-turn, multi-person dialogues.", "key_contributions": ["Introduction of the MultiManip dataset for mental manipulation in dialogues.", "Development of SELF-PERCEPT, a two-stage prompting framework for improved manipulation detection.", "Evaluation of LLMs in detecting manipulation, revealing their limitations."], "limitations": "The paper primarily focuses on dialogues from reality shows, which may not fully generalize to other contexts.", "keywords": ["mental manipulation", "dialogue dataset", "self-perception", "language models", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20693", "pdf": "https://arxiv.org/pdf/2505.20693.pdf", "abs": "https://arxiv.org/abs/2505.20693", "title": "Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages", "authors": ["Praveen Srinivasa Varadhan", "Srija Anand", "Soma Siddhartha", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "What happens when an English Fairytaler is fine-tuned on Indian languages? We\nevaluate how the English F5-TTS model adapts to 11 Indian languages, measuring\npolyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:\n(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and\n(iii) fine-tuning on both Indian and English data to prevent forgetting.\nFine-tuning with only Indian data proves most effective and the resultant IN-F5\nis a near-human polyglot; that enables speakers of one language (e.g., Odia) to\nfluently speak in another (e.g., Hindi). Our results show English pretraining\naids low-resource TTS in reaching human parity. To aid progress in other\nlow-resource languages, we study data-constrained setups and arrive at a\ncompute optimal strategy. Finally, we show IN-F5 can synthesize unseen\nlanguages like Bhojpuri and Tulu using a human-in-the-loop approach for\nzero-resource TTS via synthetic data generation.", "AI": {"tldr": "The study investigates the adaptation of an English TTS model to 11 Indian languages through various fine-tuning strategies, leading to near-human performance in polyglot speech synthesis.", "motivation": "To explore the effectiveness of adapting English TTS technology for use in low-resource Indian languages.", "method": "The research compares different fine-tuning methods: training from scratch, fine-tuning solely on Indian data, and fine-tuning on both Indian and English data.", "result": "Fine-tuning with only Indian data yields the most effective model, IN-F5, achieving near-human polyglot fluency across multiple Indian languages, including the ability to synthesize previously unseen languages.", "conclusion": "The study demonstrates that English pretraining significantly enhances TTS capabilities for low-resource languages and proposes a compute-optimal strategy for further advancements in this field.", "key_contributions": ["Development of IN-F5 for polyglot fluency in Indian languages", "Evidence that English pretraining aids low-resource TTS", "Human-in-the-loop approach for synthesizing unseen languages"], "limitations": "", "keywords": ["Text-to-Speech", "Polyglot Fluency", "Fine-tuning", "Low-resource Languages", "Synthetic Data Generation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.20700", "pdf": "https://arxiv.org/pdf/2505.20700.pdf", "abs": "https://arxiv.org/abs/2505.20700", "title": "Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration", "authors": ["Yong Wu", "Weihang Pan", "Ke Li", "Chen Binhui", "Ping Li", "Binbin Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nyet aligning such abilities to small language models (SLMs) remains a challenge\ndue to distributional mismatches and limited model capacity. Existing reasoning\ndatasets, typically designed for powerful LLMs, often lead to degraded\nperformance when directly applied to weaker models. In this work, we introduce\nDynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation\nframework that bridges the capability gap between expert reasoning trajectories\nand diverse SLMs. Instead of uniformly imitating expert steps, DART employs a\nselective imitation strategy guided by step-wise adaptability estimation via\nsolution simulation. When expert steps surpass the student's capacity --\nsignaled by an Imitation Gap -- the student autonomously explores alternative\nreasoning paths, constrained by outcome consistency. We validate DART across\nmultiple reasoning benchmarks and model scales, demonstrating that it\nsignificantly improves generalization and data efficiency over static\nfine-tuning. Our method enhances supervision quality by aligning training\nsignals with the student's reasoning capabilities, offering a scalable solution\nfor reasoning alignment in resource-constrained models.", "AI": {"tldr": "A novel framework, DART, improves reasoning alignment for small language models (SLMs) by selectively imitating expert reasoning steps and adapting to their capabilities, leading to better performance and efficiency.", "motivation": "Aligning the reasoning capabilities of small language models (SLMs) with those of larger models remains difficult due to distributional differences and the SLMs' limited capacity.", "method": "DART employs a selective imitation strategy that uses step-wise adaptability estimation via solution simulation to guide SLMs when imitating expert reasoning steps, allowing exploration of alternative paths when necessary.", "result": "DART significantly enhances generalization and data efficiency across various reasoning benchmarks, outperforming static fine-tuning methods.", "conclusion": "The DART framework offers a scalable approach for improving reasoning alignment in resource-constrained models by aligning training signals with the capabilities of the SLMs.", "key_contributions": ["Introduction of the DART framework for reasoning alignment.", "Selective imitation guided by adaptability estimation.", "Improvement in generalization and data efficiency for small language models."], "limitations": "", "keywords": ["language models", "reasoning alignment", "data efficiency", "selective imitation", "machine learning"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.20707", "pdf": "https://arxiv.org/pdf/2505.20707.pdf", "abs": "https://arxiv.org/abs/2505.20707", "title": "Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Deepak Subramani"], "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "comment": null, "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, making them promising for educational applications. However,\ntheir capacity for complex reasoning, particularly in domains such as physics,\nremains underexplored. This study investigates the high school physics\nreasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),\nincluding instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.\nWe developed a comprehensive physics dataset from the OpenStax High School\nPhysics textbook, annotated according to Bloom's Taxonomy, with LaTeX and\nplaintext mathematical notations. A novel cultural contextualization approach\nwas applied to a subset, creating culturally adapted problems for Asian,\nAfrican, and South American/Australian contexts while preserving core physics\nprinciples. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,\nwe evaluated answer and reasoning chain correctness, along with calculation\naccuracy. The results reveal significant differences between the SLMs. Qwen 3\n1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was\nsubstantially low (38%). The format of the mathematical notation had a\nnegligible impact on performance. SLMs exhibited varied performance across the\nphysics topics and showed a decline in reasoning quality with increasing\ncognitive and knowledge complexity. In particular, the consistency of reasoning\nwas largely maintained in diverse cultural contexts, especially by better\nperforming models. These findings indicate that, while SLMs can often find\ncorrect answers, their underlying reasoning is frequently flawed, suggesting an\noverreliance on pattern recognition. For SLMs to become reliable educational\ntools in physics, future development must prioritize enhancing genuine\nunderstanding and the generation of sound, verifiable reasoning chains over\nmere answer accuracy.", "AI": {"tldr": "This study examines the reasoning capabilities of Small Language Models (SLMs) in high school physics, revealing strengths in answer accuracy but weaknesses in reasoning quality.", "motivation": "Exploring the potential of Small Language Models (SLMs) for educational use in physics while identifying their limitations in reasoning capabilities.", "method": "Developed a comprehensive physics dataset from the OpenStax High School Physics textbook and evaluated SLMs using a novel cultural contextualization approach and an LLM-as-a-judge framework.", "result": "Qwen 3 1.7B achieved 85% answer accuracy, but only 38% in fully correct reasoning; performance varied across topics, and reasoning quality deteriorated with complexity.", "conclusion": "SLMs can provide correct answers but often rely on flawed reasoning; future development should focus on enhancing understanding and reasoning chains.", "key_contributions": ["Created a culturally contextualized physics dataset for SLM evaluation.", "Introduced LLM-as-a-judge framework for evaluating reasoning correctness.", "Identified significant gaps in reasoning quality of SLMs despite high answer accuracy."], "limitations": "Study focused on a specific domain (high school physics) and may not generalize to other subjects or levels.", "keywords": ["Small Language Models", "educational applications", "physics reasoning", "cultural contextualization", "LLM evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20732", "pdf": "https://arxiv.org/pdf/2505.20732.pdf", "abs": "https://arxiv.org/abs/2505.20732", "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) holds significant promise for training LLM agents\nto handle complex, goal-oriented tasks that require multi-step interactions\nwith external environments. However, a critical challenge when applying RL to\nthese agentic tasks arises from delayed rewards: feedback signals are typically\navailable only after the entire task is completed. This makes it non-trivial to\nassign delayed rewards to earlier actions, providing insufficient guidance\nregarding environmental constraints and hindering agent training. In this work,\nwe draw on the insight that the ultimate completion of a task emerges from the\ncumulative progress an agent makes across individual steps. We propose Stepwise\nProgress Attribution (SPA), a general reward redistribution framework that\ndecomposes the final reward into stepwise contributions, each reflecting its\nincremental progress toward overall task completion. To achieve this, we train\na progress estimator that accumulates stepwise contributions over a trajectory\nto match the task completion. During policy optimization, we combine the\nestimated per-step contribution with a grounding signal for actions executed in\nthe environment as the fine-grained, intermediate reward for effective agent\ntraining. Extensive experiments on common agent benchmarks (including Webshop,\nALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the\nstate-of-the-art method in both success rate (+2.5\\% on average) and grounding\naccuracy (+1.9\\% on average). Further analyses demonstrate that our method\nremarkably provides more effective intermediate rewards for RL training. Our\ncode is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.", "AI": {"tldr": "This paper introduces Stepwise Progress Attribution (SPA), a novel framework for enhancing reinforcement learning (RL) agent training by providing intermediate rewards based on cumulative progress in multi-step tasks.", "motivation": "The paper addresses the challenge of delayed rewards in reinforcement learning tasks, which complicate the training of agents by providing insufficient feedback signals during multi-step interactions.", "method": "The authors propose a reward redistribution framework called Stepwise Progress Attribution (SPA), which decomposes final rewards into incremental contributions reflecting per-step progress toward task completion. A progress estimator is trained to accumulate these contributions, integrating them with grounding signals during policy optimization.", "result": "SPA consistently outperforms state-of-the-art methods on common agent benchmarks, achieving an average increase of 2.5% in success rates and 1.9% in grounding accuracy across experiments.", "conclusion": "The proposed method shows promise for providing more effective intermediate rewards for RL training, potentially improving the learning process and overall performance of RL agents in complex tasks.", "key_contributions": ["Introduction of Stepwise Progress Attribution (SPA) framework for RL.", "Demonstration of improved agent performance in benchmarks through SPA.", "Effective intermediate rewards based on per-step progress."], "limitations": "", "keywords": ["Reinforcement Learning", "Stepwise Progress Attribution", "Intermediate Rewards", "Agent Training", "Multi-step Tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20738", "pdf": "https://arxiv.org/pdf/2505.20738.pdf", "abs": "https://arxiv.org/abs/2505.20738", "title": "Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator", "authors": ["Peiwen Yuan", "Yiwei Li", "Shaoxiong Feng", "Xinglin Wang", "Yueqi Zhang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-Benchmark-Generator methods have been widely studied as a supplement\nto human annotators for scalable evaluation, while the potential biases within\nthis paradigm remain underexplored. In this work, we systematically define and\nvalidate the phenomenon of inflated performance in models evaluated on their\nself-generated benchmarks, referred to as self-bias, and attribute it to\nsub-biases arising from question domain, language style, and wrong labels. On\nthis basis, we propose Silencer, a general framework that leverages the\nheterogeneity between multiple generators at both the sample and benchmark\nlevels to neutralize bias and generate high-quality, self-bias-silenced\nbenchmark. Experimental results across various settings demonstrate that\nSilencer can suppress self-bias to near zero, significantly improve evaluation\neffectiveness of the generated benchmark (with an average improvement from\n0.655 to 0.833 in Pearson correlation with high-quality human-annotated\nbenchmark), while also exhibiting strong generalizability.", "AI": {"tldr": "This paper introduces Silencer, a framework to reduce biases in LLM-generated benchmarks used for model evaluation.", "motivation": "To address the issue of inflated performance results due to biases in models evaluated on self-generated benchmarks, termed self-bias.", "method": "The proposed Silencer framework utilizes the diversity of multiple benchmark generators to mitigate self-bias at both the sample and benchmark levels.", "result": "The application of Silencer demonstrates a reduction of self-bias to near zero and an improvement in evaluation effectiveness of generated benchmarks, indicated by an increase in Pearson correlation from 0.655 to 0.833 compared with human-annotated benchmarks.", "conclusion": "Silencer effectively neutralizes self-bias and enhances the quality of model evaluations, showcasing strong generalizability across different scenarios.", "key_contributions": ["Definition and validation of self-bias in LLM evaluations", "Introduction of the Silencer framework for bias mitigation", "Demonstrated improvement in benchmark evaluation effectiveness"], "limitations": "", "keywords": ["LLM", "self-bias", "benchmarking", "performance evaluation", "AI"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20767", "pdf": "https://arxiv.org/pdf/2505.20767.pdf", "abs": "https://arxiv.org/abs/2505.20767", "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Faithfulness hallucination are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandard, existing benchmarks only contain \"factual statements\" that rephrase\nsource materials without marking \"cognitive statements\" that make inference\nfrom the given context, making the consistency evaluation and optimization of\ncognitive statements difficult. Inspired by how an evidence is assessed in the\nlegislative domain, we design a rigorous framework to assess different levels\nof faithfulness of cognitive statements and create a benchmark dataset where we\nreveal insightful statistics. We design an annotation pipeline to create larger\nbenchmarks for different LLMs automatically, and the resulting larger-scale\nCogniBench-L dataset can be used to train accurate cognitive hallucination\ndetection model. We release our model and dataset at:\nhttps://github.com/FUTUREEEEEE/CogniBench", "AI": {"tldr": "This paper addresses faithfulness hallucination in Large Language Models (LLMs) through a new framework and benchmark dataset, CogniBench-L, for assessing cognitive statements and training detection models.", "motivation": "To tackle the issue of faithfulness hallucination in LLMs by providing a clearer assessment methodology for cognitive statements.", "method": "The authors designed a rigorous framework inspired by legislative evidence assessment and created a benchmark dataset, CogniBench-L, complemented by an annotation pipeline for large-scale benchmarks.", "result": "The paper presents a new dataset that reveals insightful statistics about cognitive statements' faithfulness and can be used to train models that detect cognitive hallucinations in LLMs.", "conclusion": "The proposed framework and dataset aim to improve the evaluation and optimization of cognitive statements generated by LLMs.", "key_contributions": ["A rigorous framework for assessing the faithfulness of cognitive statements in LLMs.", "Development of the CogniBench-L benchmark dataset for facilitating research in cognitive hallucination detection.", "An annotation pipeline that enables the automatic creation of large-scale benchmarks for LLM evaluation."], "limitations": "", "keywords": ["faithfulness hallucination", "Large Language Models", "cognitive statements"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20776", "pdf": "https://arxiv.org/pdf/2505.20776.pdf", "abs": "https://arxiv.org/abs/2505.20776", "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences", "authors": ["Jungyoub Cha", "Hyunjong Kim", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; C.4"], "comment": "8 pages, 3 figures. Under review at EMNLP 2025", "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .", "AI": {"tldr": "SpecExtend enhances speculative decoding for long sequences in LLMs by integrating efficient attention mechanisms and a novel KV cache update strategy.", "motivation": "To address the performance degradation of speculative decoding in LLMs on long inputs due to increased attention cost and reduced draft accuracy.", "method": "SpecExtend implements efficient attention mechanisms, including FlashAttention and Hybrid Tree Attention, while introducing Cross-model Retrieval to optimize KV cache updates based on attention scores.", "result": "SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for input sizes up to 16K tokens, demonstrating its effectiveness on long-context understanding datasets.", "conclusion": "SpecExtend provides a significant improvement in the speed and accuracy of speculative decoding for long sequences without needing additional training.", "key_contributions": ["Introduction of SpecExtend as a drop-in enhancement for speculative decoding.", "Integration of efficient attention mechanisms for performance improvement.", "Proposition of Cross-model Retrieval for dynamic context selection."], "limitations": "", "keywords": ["speculative decoding", "large language models", "efficient attention mechanisms", "long sequences", "cross-model retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20779", "pdf": "https://arxiv.org/pdf/2505.20779.pdf", "abs": "https://arxiv.org/abs/2505.20779", "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature", "authors": ["Noy Sternlicht", "Tom Hope"], "categories": ["cs.CL"], "comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web", "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA", "AI": {"tldr": "CHIMERA is a large-scale knowledge base for mining scientific recombination examples to enhance creativity in research by integrating concepts from different domains.", "motivation": "To explore how scientists create new ideas by recombining existing concepts and mechanisms.", "method": "The paper presents a novel task for extracting recombination from scientific abstracts, collects a manually annotated dataset, and trains an LLM-based extraction model on a large corpus of AI papers.", "result": "CHIMERA contains over 28K examples of recombination and provides insights into properties of recombination in various AI subfields.", "conclusion": "The research demonstrates how CHIMERA can be used to inspire new research directions and supports the hypothesis generation in science.", "key_contributions": ["Development of a large-scale KB for recombination examples", "Introduction of a novel information extraction task", "Training of a model to predict new creative directions"], "limitations": "", "keywords": ["recombination", "knowledge base", "machine learning", "hypothesis generation", "scientific literature"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20809", "pdf": "https://arxiv.org/pdf/2505.20809.pdf", "abs": "https://arxiv.org/abs/2505.20809", "title": "Improved Representation Steering for Language Models", "authors": ["Zhengxuan Wu", "Qinan Yu", "Aryaman Arora", "Christopher D. Manning", "Christopher Potts"], "categories": ["cs.CL"], "comment": "46 pages, 23 figures, preprint", "summary": "Steering methods for language models (LMs) seek to provide fine-grained and\ninterpretable control over model generations by variously changing model\ninputs, weights, or representations to adjust behavior. Recent work has shown\nthat adjusting weights or representations is often less effective than steering\nby prompting, for instance when wanting to introduce or suppress a particular\nconcept. We demonstrate how to improve representation steering via our new\nReference-free Preference Steering (RePS), a bidirectional\npreference-optimization objective that jointly does concept steering and\nsuppression. We train three parameterizations of RePS and evaluate them on\nAxBench, a large-scale model steering benchmark. On Gemma models with sizes\nranging from 2B to 27B, RePS outperforms all existing steering methods trained\nwith a language modeling objective and substantially narrows the gap with\nprompting -- while promoting interpretability and minimizing parameter count.\nIn suppression, RePS matches the language-modeling objective on Gemma-2 and\noutperforms it on the larger Gemma-3 variants while remaining resilient to\nprompt-based jailbreaking attacks that defeat prompting. Overall, our results\nsuggest that RePS provides an interpretable and robust alternative to prompting\nfor both steering and suppression.", "AI": {"tldr": "This paper introduces Reference-free Preference Steering (RePS), a novel method for steering language models that improves both concept steering and suppression compared to existing methods and prompting.", "motivation": "The need for effective steering methods in language models that provide interpretable control over model generations without relying on traditional prompting techniques.", "method": "RePS is a bidirectional preference-optimization objective that adjusts model behavior by steering and suppressing concepts through a more interpretable framework.", "result": "RePS outperforms existing steering methods in a large-scale benchmark, achieving results comparable to prompting while maintaining a lower parameter count and being resilient to prompt-based attacks.", "conclusion": "RePS offers an interpretable and robust alternative to existing prompting strategies for steering language models.", "key_contributions": ["Introduction of a new preference-optimization method for steering language models.", "Demonstrated improved performance on concept steering and suppression tasks.", "Enhanced interpretability and reduced parameter requirements compared to existing methods."], "limitations": "", "keywords": ["language models", "steering", "preference optimization", "interpretability", "prompting"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20813", "pdf": "https://arxiv.org/pdf/2505.20813.pdf", "abs": "https://arxiv.org/abs/2505.20813", "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "summary": "In knowledge graph embedding, leveraging relation-specific\nentity-transformation has markedly enhanced performance. However, the\nconsistency of embedding differences before and after transformation remains\nunaddressed, risking the loss of valuable inductive bias inherent in the\nembeddings. This inconsistency stems from two problems. First, transformation\nrepresentations are specified for relations in a disconnected manner, allowing\ndissimilar transformations and corresponding entity-embeddings for similar\nrelations. Second, a generalized plug-in approach as a SFBR (Semantic Filter\nBased on Relations) disrupts this consistency through excessive concentration\nof entity embeddings under entity-based regularization, generating\nindistinguishable score distributions among relations. In this paper, we\nintroduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),\ncontaining more consistent entity-transformation characterized by three\nfeatures: 1) shared affine transformation of relation embeddings across all\nrelations, 2) rooted entity-transformation that adds an entity embedding to its\nchange represented by the transformed vector, and 3) normalization of the\nchange to prevent scale reduction. To amplify the advantages of consistency\nthat preserve semantics on embeddings, RSCF adds relation transformation and\nprediction modules for enhancing the semantics. In knowledge graph completion\ntasks with distance-based and tensor decomposition models, RSCF significantly\noutperforms state-of-the-art KGE methods, showing robustness across all\nrelations and their frequencies.", "AI": {"tldr": "This paper introduces the Relation-Semantics Consistent Filter (RSCF) method for knowledge graph embedding, which improves consistency in entity-transformation and enhances performance in knowledge graph completion tasks.", "motivation": "The need for consistent embedding differences in relation-specific entity-transformation methods to retain inductive biases in knowledge graphs.", "method": "The proposed RSCF method employs shared affine transformations across relations, rooted entity-transformations, and normalization to ensure consistency in embeddings while integrating additional relation transformation and prediction modules.", "result": "RSCF outperforms state-of-the-art knowledge graph embedding methods in various tasks, demonstrating robustness across different relation types and frequencies.", "conclusion": "The RSCF method provides enhanced consistency and semantics in knowledge graph embeddings, leading to improved completion performance.", "key_contributions": ["Introduction of the Relation-Semantics Consistent Filter (RSCF) method", "Improvement of consistency in entity-transformation", "Demonstrated superior performance over existing KGE methods"], "limitations": "", "keywords": ["Knowledge Graph Embedding", "Entity Transformation", "Relation-Semantics Consistency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20816", "pdf": "https://arxiv.org/pdf/2505.20816.pdf", "abs": "https://arxiv.org/abs/2505.20816", "title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective", "authors": ["Krishna Singh Rajput", "Tejas Anvekar", "Chitta Baral", "Vivek Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in multimodal question answering have primarily focused on\ncombining heterogeneous modalities or fine-tuning multimodal large language\nmodels. While these approaches have shown strong performance, they often rely\non a single, generalized reasoning strategy, overlooking the unique\ncharacteristics of each modality ultimately limiting both accuracy and\ninterpretability. To address these limitations, we propose MAMMQA, a\nmulti-agent QA framework for multimodal inputs spanning text, tables, and\nimages. Our system includes two Visual Language Model (VLM) agents and one\ntext-based Large Language Model (LLM) agent. The first VLM decomposes the user\nquery into sub-questions and sequentially retrieves partial answers from each\nmodality. The second VLM synthesizes and refines these results through\ncross-modal reasoning. Finally, the LLM integrates the insights into a cohesive\nanswer. This modular design enhances interpretability by making the reasoning\nprocess transparent and allows each agent to operate within its domain of\nexpertise. Experiments on diverse multimodal QA benchmarks demonstrate that our\ncooperative, multi-agent framework consistently outperforms existing baselines\nin both accuracy and robustness.", "AI": {"tldr": "MAMMQA is a multi-agent framework for multimodal question answering that improves accuracy and interpretability by using specialized agents for text, images, and tables.", "motivation": "To overcome limitations of current multimodal question answering approaches that rely on generalized reasoning strategies and do not leverage unique modal characteristics.", "method": "A framework with two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent to decompose queries, retrieve partial answers, and synthesize results through cross-modal reasoning.", "result": "MAMMQA consistently outperforms existing baselines in terms of accuracy and robustness across diverse multimodal QA benchmarks.", "conclusion": "The modular design of MAMMQA enhances interpretability and allows each agent to focus on its domain expertise, leading to improved performance in multimodal question answering tasks.", "key_contributions": ["Development of a multi-agent QA framework for multimodal inputs.", "Enhanced interpretability through transparent reasoning processes.", "Improved performance on multimodal QA benchmarks compared to existing methods."], "limitations": "", "keywords": ["multimodal question answering", "multi-agent framework", "cross-modal reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20819", "pdf": "https://arxiv.org/pdf/2505.20819.pdf", "abs": "https://arxiv.org/abs/2505.20819", "title": "Tracing and Reversing Rank-One Model Edits", "authors": ["Paul Youssef", "Zhixue Zhao", "Christin Seifert", "JÃ¶rg SchlÃ¶tterer"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing methods (KEs) are a cost-effective way to update the\nfactual content of large language models (LLMs), but they pose a dual-use risk.\nWhile KEs are beneficial for updating outdated or incorrect information, they\ncan be exploited maliciously to implant misinformation or bias. In order to\ndefend against these types of malicious manipulation, we need robust techniques\nthat can reliably detect, interpret, and mitigate adversarial edits. This work\ninvestigates the traceability and reversibility of knowledge edits, focusing on\nthe widely used Rank-One Model Editing (ROME) method. We first show that ROME\nintroduces distinctive distributional patterns in the edited weight matrices,\nwhich can serve as effective signals for locating the edited weights. Second,\nwe show that these altered weights can reliably be used to predict the edited\nfactual relation, enabling partial reconstruction of the modified fact.\nBuilding on this, we propose a method to infer the edited object entity\ndirectly from the modified weights, without access to the editing prompt,\nachieving over 95% accuracy. Finally, we demonstrate that ROME edits can be\nreversed, recovering the model's original outputs with $\\geq$ 80% accuracy. Our\nfindings highlight the feasibility of detecting, tracing, and reversing edits\nbased on the edited weights, offering a robust framework for safeguarding LLMs\nagainst adversarial manipulations.", "AI": {"tldr": "This work investigates the traceability and reversibility of knowledge editing in LLMs, focusing on the ROME method, and proposes techniques to detect and reverse adversarial edits.", "motivation": "To defend against the malicious use of knowledge editing methods that can implant misinformation or bias in large language models.", "method": "The study analyzes the distributional patterns in the weight matrices of the ROME method to locate edited weights, predict modified facts, and reverse edits.", "result": "Achieved over 95% accuracy in inferring edited entities and over 80% accuracy in recovering original outputs after reversing edits.", "conclusion": "The findings provide a robust framework for detecting, tracing, and reversing knowledge edits in LLMs, enhancing their resistance against adversarial manipulations.", "key_contributions": ["Identification of unique distributional patterns in ROME weight matrices", "Development of a method for accurate prediction of edited factual relations", "Demonstration of effective reversal of knowledge edits"], "limitations": "", "keywords": ["knowledge editing", "large language models", "adversarial detection", "model editing"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.20825", "pdf": "https://arxiv.org/pdf/2505.20825.pdf", "abs": "https://arxiv.org/abs/2505.20825", "title": "Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Long-form question answering (LFQA) presents unique challenges for large\nlanguage models, requiring the synthesis of coherent, paragraph-length answers.\nWhile retrieval-augmented generation (RAG) systems have emerged as a promising\nsolution, existing research struggles with key limitations: the scarcity of\nhigh-quality training data for long-form generation, the compounding risk of\nhallucination in extended outputs, and the absence of reliable evaluation\nmetrics for factual completeness. In this paper, we propose RioRAG, a novel\nreinforcement learning (RL) framework that advances long-form RAG through\nreinforced informativeness optimization. Our approach introduces two\nfundamental innovations to address the core challenges. First, we develop an RL\ntraining paradigm of reinforced informativeness optimization that directly\noptimizes informativeness and effectively addresses the slow-thinking deficit\nin conventional RAG systems, bypassing the need for expensive supervised data.\nSecond, we propose a nugget-centric hierarchical reward modeling approach that\nenables precise assessment of long-form answers through a three-stage process:\nextracting the nugget from every source webpage, constructing a nugget claim\nchecklist, and computing rewards based on factual alignment. Extensive\nexperiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the\neffectiveness of the proposed method. Our codes are available at\nhttps://github.com/RUCAIBox/RioRAG.", "AI": {"tldr": "This paper presents RioRAG, a novel reinforcement learning framework for improving long-form question answering in large language models by optimizing informativeness and using a hierarchical reward system.", "motivation": "Existing long-form question answering systems face challenges such as lack of quality data, hallucination risks, and poor evaluation metrics.", "method": "The proposed RioRAG utilizes reinforced informativeness optimization through a new RL training paradigm and a nugget-centric hierarchical reward model to assess factual alignment in long-form answers.", "result": "Extensive experiments on LFQA benchmarks show that RioRAG effectively improves informativeness and factual accuracy compared to traditional methods.", "conclusion": "RioRAG represents a significant advancement in long-form question answering, showcasing both practical improvements and theoretical insights into the optimization process.", "key_contributions": ["Introduction of a RL training paradigm for informativeness optimization", "Nugget-centric hierarchical reward modeling for assessing answers", "Demonstrated effectiveness on LFQA benchmarks LongFact and RAGChecker"], "limitations": "", "keywords": ["long-form question answering", "reinforcement learning", "retrieval-augmented generation", "natural language processing", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20826", "pdf": "https://arxiv.org/pdf/2505.20826.pdf", "abs": "https://arxiv.org/abs/2505.20826", "title": "AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset", "authors": ["Soichiro Murakami", "Peinan Zhang", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "Identifying factors that make ad text attractive is essential for advertising\nsuccess. This study proposes AdParaphrase v2.0, a dataset for ad text\nparaphrasing, containing human preference data, to enable the analysis of the\nlinguistic factors and to support the development of methods for generating\nattractive ad texts. Compared with v1.0, this dataset is 20 times larger,\ncomprising 16,460 ad text paraphrase pairs, each annotated with preference data\nfrom ten evaluators, thereby enabling a more comprehensive and reliable\nanalysis. Through the experiments, we identified multiple linguistic features\nof engaging ad texts that were not observed in v1.0 and explored various\nmethods for generating attractive ad texts. Furthermore, our analysis\ndemonstrated the relationships between human preference and ad performance, and\nhighlighted the potential of reference-free metrics based on large language\nmodels for evaluating ad text attractiveness. The dataset is publicly available\nat: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.", "AI": {"tldr": "This study introduces AdParaphrase v2.0, a large dataset for ad text paraphrasing aimed at enhancing ad attractiveness through linguistic analysis.", "motivation": "The research aims to identify factors that contribute to the attractiveness of ad text, which is critical for successful advertising.", "method": "Developed a dataset, AdParaphrase v2.0, consisting of 16,460 annotated ad text paraphrase pairs derived from human preference data, and conducted experiments to analyze linguistic features of engaging ad texts.", "result": "Identified new linguistic features linked to engaging ad texts and established connections between human preference and ad performance; demonstrated the utility of reference-free metrics from large language models for evaluating ad attractiveness.", "conclusion": "The study suggests that AdParaphrase v2.0 can advance the understanding of ad text attractiveness and supports the development of better ad generation methods.", "key_contributions": ["Introduction of a significantly larger dataset for ad text paraphrasing", "Identification of new linguistic features contributing to ad engagement", "Validation of reference-free metrics using large language models for ad evaluation"], "limitations": "", "keywords": ["ad text", "paraphrasing", "advertising", "linguistic analysis", "human preference"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2505.20841", "pdf": "https://arxiv.org/pdf/2505.20841.pdf", "abs": "https://arxiv.org/abs/2505.20841", "title": "Concealment of Intent: A Game-Theoretic Analysis", "authors": ["Xinbo Wu", "Abhishek Umrawal", "Lav R. Varshney"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) grow more capable, concerns about their safe\ndeployment have also grown. Although alignment mechanisms have been introduced\nto deter misuse, they remain vulnerable to carefully designed adversarial\nprompts. In this work, we present a scalable attack strategy: intent-hiding\nadversarial prompting, which conceals malicious intent through the composition\nof skills. We develop a game-theoretic framework to model the interaction\nbetween such attacks and defense systems that apply both prompt and response\nfiltering. Our analysis identifies equilibrium points and reveals structural\nadvantages for the attacker. To counter these threats, we propose and analyze a\ndefense mechanism tailored to intent-hiding attacks. Empirically, we validate\nthe attack's effectiveness on multiple real-world LLMs across a range of\nmalicious behaviors, demonstrating clear advantages over existing adversarial\nprompting techniques.", "AI": {"tldr": "This paper presents a new attack strategy on large language models involving intent-hiding adversarial prompting and proposes a defense mechanism against it.", "motivation": "With the increasing capabilities of large language models, there are growing concerns about their potential misuse, prompting the need for better defense mechanisms against adversarial prompts.", "method": "A game-theoretic framework is developed to model the interaction between intent-hiding adversarial prompting and filtering defense systems. The framework identifies equilibrium points and structural advantages for attackers.", "result": "The study empirically demonstrates the effectiveness of the intent-hiding attack across multiple real-world LLMs, showcasing its supremacy over existing adversarial prompting techniques.", "conclusion": "A defense mechanism tailored for intent-hiding attacks is proposed and analyzed, addressing the vulnerabilities of current methods.", "key_contributions": ["Introduction of intent-hiding adversarial prompting as a scalable attack strategy.", "Development of a game-theoretic framework for modeling attack-defense interactions.", "Proposal of a specific defense mechanism against intent-hiding attacks."], "limitations": "The paper primarily focuses on adversarial prompting and does not address potential defenses against other types of attacks.", "keywords": ["large language models", "adversarial prompting", "game theory", "defense mechanisms"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20871", "pdf": "https://arxiv.org/pdf/2505.20871.pdf", "abs": "https://arxiv.org/abs/2505.20871", "title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG", "authors": ["Xin Sun", "Jianan Xie", "Zhongqi Chen", "Qiang Liu", "Shu Wu", "Yuehe Chen", "Bowen Song", "Weiqiang Wang", "Zilei Wang", "Liang Wang"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Large language models (LLMs) augmented with retrieval systems have\nsignificantly advanced natural language processing tasks by integrating\nexternal knowledge sources, enabling more accurate and contextually rich\nresponses. To improve the robustness of such systems against noisy retrievals,\nRetrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.\nHowever, RAFT conditions models to generate answers even in the absence of\nreliable knowledge. This behavior undermines their reliability in high-stakes\ndomains, where acknowledging uncertainty is critical. To address this issue, we\npropose Divide-Then-Align (DTA), a post-training approach designed to endow RAG\nsystems with the ability to respond with \"I don't know\" when the query is out\nof the knowledge boundary of both the retrieved passages and the model's\ninternal knowledge. DTA divides data samples into four knowledge quadrants and\nconstructs tailored preference data for each quadrant, resulting in a curated\ndataset for Direct Preference Optimization (DPO). Experimental results on three\nbenchmark datasets demonstrate that DTA effectively balances accuracy with\nappropriate abstention, enhancing the reliability and trustworthiness of\nretrieval-augmented systems.", "AI": {"tldr": "This paper introduces Divide-Then-Align (DTA), a method that improves the reliability of retrieval-augmented systems by enabling them to respond with 'I don't know' when beyond their knowledge limits.", "motivation": "To enhance the reliability of Retrieval-Augmented Models (RAG) in high-stakes environments by allowing them to properly decline answering when unsure.", "method": "The method divides data samples into four knowledge quadrants, creating a specialized dataset for Direct Preference Optimization (DPO) to guide responses.", "result": "Experimental results indicate that DTA effectively balances the accuracy of responses while allowing the model to abstain when uncertain, showcasing improvements in trustworthiness.", "conclusion": "DTA enhances the robustness of retrieval-augmented systems by instilling a systematic approach to acknowledge uncertainty, crucial for high-stakes applications.", "key_contributions": ["Introduction of Divide-Then-Align (DTA) for RAG systems", "Development of a new dataset for Direct Preference Optimization", "Demonstrated improvements in accuracy and reliability through experimental results"], "limitations": "", "keywords": ["Retrieval-Augmented Models", "Divide-Then-Align", "Direct Preference Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20874", "pdf": "https://arxiv.org/pdf/2505.20874.pdf", "abs": "https://arxiv.org/abs/2505.20874", "title": "Can LLMs Learn to Map the World from Local Descriptions?", "authors": ["Sirui Xia", "Aili Chen", "Xintao Wang", "Tinghui Zhu", "Yikai Zhang", "Jiangjie Chen", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": "19 pages, 11 figures", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\ncapabilities in tasks such as code and mathematics. However, their potential to\ninternalize structured spatial knowledge remains underexplored. This study\ninvestigates whether LLMs, grounded in locally relative human observations, can\nconstruct coherent global spatial cognition by integrating fragmented\nrelational descriptions. We focus on two core aspects of spatial cognition:\nspatial perception, where models infer consistent global layouts from local\npositional relationships, and spatial navigation, where models learn road\nconnectivity from trajectory data and plan optimal paths between unconnected\nlocations. Experiments conducted in a simulated urban environment demonstrate\nthat LLMs not only generalize to unseen spatial relationships between points of\ninterest (POIs) but also exhibit latent representations aligned with real-world\nspatial distributions. Furthermore, LLMs can learn road connectivity from\ntrajectory descriptions, enabling accurate path planning and dynamic spatial\nawareness during navigation.", "AI": {"tldr": "This study explores how Large Language Models (LLMs) can integrate spatial knowledge to understand global spatial layouts and improve navigation tasks through local observations and trajectory data.", "motivation": "The potential of LLMs in understanding structured spatial knowledge is largely unexplored, especially in relation to human spatial perception and navigation.", "method": "The paper conducts experiments in a simulated urban environment to test LLMs on spatial perception and navigation tasks using relational descriptions and trajectory data.", "result": "LLMs were able to generalize spatial relationships between points of interest and demonstrated the ability to learn road connectivity from trajectory descriptions, allowing for accurate path planning.", "conclusion": "LLMs can construct coherent global spatial cognition and maintain dynamic spatial awareness, which enhances their utility in navigation tasks.", "key_contributions": ["Investigation of LLMs in spatial cognition", "Demonstration of generalization to unseen spatial relationships", "Ability to learn road connectivity for path planning"], "limitations": "The experiments are limited to simulated environments and may not fully capture real-world complexities.", "keywords": ["Large Language Models", "spatial cognition", "navigation", "trajectory data", "urban environment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20875", "pdf": "https://arxiv.org/pdf/2505.20875.pdf", "abs": "https://arxiv.org/abs/2505.20875", "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "authors": ["Jiyoung Lee", "Seungho Kim", "Jieun Han", "Jun-Min Lee", "Kitaek Kim", "Alice Oh", "Edward Choi"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 6 figures, 16 tables", "summary": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available.", "AI": {"tldr": "The paper introduces Trans-EnV, a framework for evaluating LLMs on various English varieties, revealing significant performance disparities.", "motivation": "To address fairness concerns in LLM performance across different English varieties, which are often overlooked in standard evaluations.", "method": "Trans-EnV transforms SAE datasets into multiple English varieties using linguistic expert knowledge and LLM-based transformations to evaluate robustness.", "result": "Significant performance disparities were found, with accuracy decreasing by up to 46.3% on non-standard varieties across seven LLMs evaluated.", "conclusion": "Comprehensive evaluations across diverse English varieties are crucial for understanding LLM performance and ensuring fairness for global users.", "key_contributions": ["Introduction of the Trans-EnV framework for linguistic robustness evaluation across English varieties.", "Transformation of six benchmark datasets into 38 English varieties.", "Validation of results through statistical testing and expert consultation."], "limitations": "", "keywords": ["Language Models", "English Varieties", "Linguistic Robustness", "Fairness", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20880", "pdf": "https://arxiv.org/pdf/2505.20880.pdf", "abs": "https://arxiv.org/abs/2505.20880", "title": "MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection", "authors": ["Baraa Hikal", "Ahmed Nasreldin", "Ali Hamdi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes. The task involves detecting hallucinated spans in text\ngenerated by instruction-tuned Large Language Models (LLMs) across multiple\nlanguages. Our approach combines task-specific prompt engineering with an LLM\nensemble verification mechanism, where a primary model extracts hallucination\nspans and three independent LLMs adjudicate their validity through\nprobability-based voting. This framework simulates the human annotation\nworkflow used in the shared task validation and test data. Additionally, fuzzy\nmatching refines span alignment. Our system ranked 1st in Arabic and Basque,\n2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.", "AI": {"tldr": "This paper presents a submission for the SemEval-2025 Task 3 focused on detecting hallucinations in text generated by instruction-tuned LLMs across languages, utilizing an ensemble verification mechanism with a strong ranking in various languages.", "motivation": "To address the challenge of hallucinations in text generated by instruction-tuned LLMs, the paper develops a method to automatically detect these errors across multiple languages.", "method": "The method utilizes task-specific prompt engineering combined with an LLM ensemble that verifies hallucinated spans through probability-based voting from multiple models, simulating human annotation processes.", "result": "The system achieved 1st place in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French in the shared task rankings.", "conclusion": "The framework demonstrates effective detection of hallucinations in generated texts and shows promise in multilingual settings.", "key_contributions": ["Development of a novel ensemble verification mechanism for detecting hallucinations in LLM outputs.", "First-place performance in multiple languages, showcasing the method's effectiveness.", "Simulation of human annotation workflows to enhance detection accuracy."], "limitations": "", "keywords": ["hallucination detection", "Large Language Models", "multilingual", "ensemble verification", "prompt engineering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20888", "pdf": "https://arxiv.org/pdf/2505.20888.pdf", "abs": "https://arxiv.org/abs/2505.20888", "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "authors": ["Chengyu Wang", "Junbing Yan", "Wenrui Cai", "Yuanhao Yue", "Jun Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community.", "AI": {"tldr": "EasyDistill is a toolkit for knowledge distillation of large language models, offering functionalities like data synthesis and fine-tuning.", "motivation": "To make advanced knowledge distillation techniques for large language models more accessible and impactful in the NLP community.", "method": "The framework includes features for data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning, designed for both fast and analytical models.", "result": "EasyDistill provides a user-friendly interface and robust distilled models along with open-sourced datasets for various applications.", "conclusion": "The toolkit facilitates seamless experimentation and implementation of state-of-the-art knowledge distillation strategies, integrating with Alibaba Cloud's AI platform.", "key_contributions": ["Comprehensive toolkit for black-box and white-box knowledge distillation of LLMs.", "Modular design that supports various KD functionalities and use cases.", "Integration with Alibaba Cloud's Platform for AI, enabling wider accessibility."], "limitations": "", "keywords": ["knowledge distillation", "large language models", "toolkit", "NLP", "Alibaba Cloud"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20899", "pdf": "https://arxiv.org/pdf/2505.20899.pdf", "abs": "https://arxiv.org/abs/2505.20899", "title": "Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing", "authors": ["Jeongsoo Choi", "Jaehun Kim", "Joon Son Chung"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper introduces a cross-lingual dubbing system that translates speech\nfrom one language to another while preserving key characteristics such as\nduration, speaker identity, and speaking speed. Despite the strong translation\nquality of existing speech translation approaches, they often overlook the\ntransfer of speech patterns, leading to mismatches with source speech and\nlimiting their suitability for dubbing applications. To address this, we\npropose a discrete diffusion-based speech-to-unit translation model with\nexplicit duration control, enabling time-aligned translation. We then\nsynthesize speech based on the predicted units and source identity with a\nconditional flow matching model. Additionally, we introduce a unit-based speed\nadaptation mechanism that guides the translation model to produce speech at a\nrate consistent with the source, without relying on any text. Extensive\nexperiments demonstrate that our framework generates natural and fluent\ntranslations that align with the original speech's duration and speaking pace,\nwhile achieving competitive translation performance.", "AI": {"tldr": "The paper presents a cross-lingual dubbing system that translates speech while maintaining characteristics like duration and speaker identity using a novel model.", "motivation": "Existing speech translation methods often fail to preserve speech patterns during translation, which limits their application in dubbing.", "method": "A discrete diffusion-based speech-to-unit translation model with explicit duration control is proposed, supplemented by a conditional flow matching model for synthesizing speech and a unit-based speed adaptation mechanism.", "result": "The proposed system generates natural translations that align with the original speech's duration and speaking pace while achieving competitive translation performance.", "conclusion": "The framework effectively addresses the limitations of traditional speech translation methods, making it suitable for dubbing applications.", "key_contributions": ["Introduction of a discrete diffusion-based speech-to-unit translation model.", "Explicit duration control for time-aligned translation.", "Unit-based speed adaptation mechanism for consistent speech rate."], "limitations": "", "keywords": ["cross-lingual dubbing", "speech translation", "discrete diffusion model"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.20901", "pdf": "https://arxiv.org/pdf/2505.20901.pdf", "abs": "https://arxiv.org/abs/2505.20901", "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models", "authors": ["Junhyuk Choi", "Minju Kim", "Yeseon Hong", "Bugeun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "As large vision language models(LVLMs) rapidly advance, concerns about their\npotential to learn and generate social biases and stereotypes are increasing.\nPrevious studies on LVLM's stereotypes face two primary limitations: metrics\nthat overlooked the importance of content words, and datasets that overlooked\nthe effect of color. To address these limitations, this study introduces new\nevaluation metrics based on the Stereotype Content Model (SCM). We also propose\nBASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM\nmetrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.\nAs a result, we found three findings. (1) The SCM-based evaluation is effective\nin capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output\nalong with gender and race ones. (3) Interaction between model architecture and\nparameter sizes seems to affect stereotypes. We release BASIC publicly on\n[anonymized for review].", "AI": {"tldr": "This study introduces new evaluation metrics and a benchmark called BASIC to assess stereotypes in large vision language models (LVLMs), revealing interactions between model characteristics and stereotype representation.", "motivation": "To address the limitations of existing measurements and datasets regarding social biases and stereotypes in LVLMs, particularly the oversight of content words and color effects.", "method": "The study employs new metrics based on the Stereotype Content Model (SCM) and introduces BASIC, a benchmark for evaluating gender, race, and color stereotypes across eight LVLMs.", "result": "The SCM-based evaluation effectively captures stereotypes; LVLMs show color stereotypes along with gender and race biases; the interaction between model architecture and parameter sizes influences these stereotypes.", "conclusion": "The findings emphasize the need for improved measures in assessing biases in LVLMs and the importance of considering color in such evaluations.", "key_contributions": ["Development of new SCM-based evaluation metrics for stereotypes", "Introduction of BASIC benchmark for assessing stereotypes in LVLMs", "Identification of interaction effects between model architecture and stereotype expression"], "limitations": "", "keywords": ["vision language models", "stereotypes", "social biases", "evaluation metrics", "BASIC"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20903", "pdf": "https://arxiv.org/pdf/2505.20903.pdf", "abs": "https://arxiv.org/abs/2505.20903", "title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?", "authors": ["Ziming Wang", "Zeyu Shi", "Haoyi Zhou", "Shiqi Gao", "Qingyun Sun", "Jianxin Li"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main; The code will be released soon", "summary": "Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,\nwith their confidence scores misaligned with actual performance. While\ncalibration has been extensively studied in models trained from scratch, the\nimpact of LLMs' prior knowledge on calibration during fine-tuning remains\nunderstudied. Our research reveals that LLMs' prior knowledge causes potential\npoor calibration due to the ubiquitous presence of known data in real-world\nfine-tuning, which appears harmful for calibration. Specifically, data aligned\nwith LLMs' prior knowledge would induce overconfidence, while new knowledge\nimproves calibration. Our findings expose a tension: LLMs' encyclopedic\nknowledge, while enabling task versatility, undermines calibration through\nunavoidable knowledge overlaps. To address this, we propose CogCalib, a\ncognition-aware framework that applies targeted learning strategies according\nto the model's prior knowledge. Experiments across 7 tasks using 3 LLM families\nprove that CogCalib significantly improves calibration while maintaining\nperformance, achieving an average 57\\% reduction in ECE compared to standard\nfine-tuning in Llama3-8B. These improvements generalize well to out-of-domain\ntasks, enhancing the objectivity and reliability of domain-specific LLMs, and\nmaking them more trustworthy for critical human-AI interaction applications.", "AI": {"tldr": "This paper addresses the poor calibration of fine-tuned Large Language Models (LLMs) due to prior knowledge, proposing a new framework called CogCalib to improve calibration without sacrificing performance.", "motivation": "LLMs often have misaligned confidence scores, affecting their reliability, particularly in domains requiring precise human-AI interaction.", "method": "The authors propose CogCalib, which employs targeted learning strategies based on the model's prior knowledge to enhance calibration during fine-tuning.", "result": "Experiments show CogCalib reduces Expected Calibration Error (ECE) by an average of 57% across tasks using Llama3-8B, while maintaining performance.", "conclusion": "The proposed method enhances the reliability of LLMs in human-AI interactions by improving calibration while leveraging the models' existing knowledge.", "key_contributions": ["Introduction of the CogCalib framework for cognition-aware calibration", "Demonstration of significant ECE reduction in multiple tasks", "Improvement in out-of-domain task performance"], "limitations": "", "keywords": ["Large Language Models", "Calibration", "CogCalib", "Fine-tuning", "Human-AI Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20910", "pdf": "https://arxiv.org/pdf/2505.20910.pdf", "abs": "https://arxiv.org/abs/2505.20910", "title": "Automated Privacy Information Annotation in Large Language Model Interactions", "authors": ["Hang Zeng", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Shaojie Tang", "Guihai Chen"], "categories": ["cs.CL"], "comment": "9 content pages", "summary": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application scenarios, typically tagging personally identifiable\ninformation (PII) in anonymous content. In this work, to support the\ndevelopment and evaluation of privacy detection models for LLM interactions\nthat are deployable on local user devices, we construct a large-scale\nmultilingual dataset with 249K user queries and 154K annotated privacy phrases.\nIn particular, we build an automated privacy annotation pipeline with\ncloud-based strong LLMs to automatically extract privacy phrases from dialogue\ndatasets and annotate leaked information. We also design evaluation metrics at\nthe levels of privacy leakage, extracted privacy phrase, and privacy\ninformation. We further establish baseline methods using light-weight LLMs with\nboth tuning-free and tuning-based methods, and report a comprehensive\nevaluation of their performance. Evaluation results reveal a gap between\ncurrent performance and the requirements of real-world LLM applications,\nmotivating future research into more effective local privacy detection methods\ngrounded in our dataset.", "AI": {"tldr": "This paper addresses the privacy risks users face when interacting with large language models (LLMs) by developing a dataset and evaluation metrics for privacy detection in LLM queries.", "motivation": "Users risk disclosing private information while interacting with LLMs under real identities, highlighting the need for effective privacy detection.", "method": "Constructed a large multilingual dataset with 249K user queries and 154K annotated privacy phrases; developed an automated annotation pipeline and established evaluation metrics for privacy leakage.", "result": "Baseline methods were established using light-weight LLMs; the evaluation revealed a performance gap in current privacy detection systems compared to real-world requirements.", "conclusion": "The study emphasizes the need for improved local privacy detection methods and sets a foundation for future research using the provided dataset.", "key_contributions": ["Large-scale multilingual dataset for LLM privacy detection", "Automated privacy annotation pipeline", "Evaluation metrics for privacy leakage and phrase extraction"], "limitations": "The gap in performance indicates that more effective methods are necessary for practical applications.", "keywords": ["privacy detection", "large language models", "user queries"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2505.20921", "pdf": "https://arxiv.org/pdf/2505.20921.pdf", "abs": "https://arxiv.org/abs/2505.20921", "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models", "authors": ["Injae Na", "Keonwoong Noh", "Woohwan Jung"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "LLM providers typically offer multiple LLM tiers, varying in performance and\nprice. As NLP tasks become more complex and modularized, selecting the suitable\nLLM tier for each subtask is a key challenge to balance between cost and\nperformance. To address the problem, we introduce LLM Automatic Transmission\n(LLM-AT) framework that automatically selects LLM tiers without training.\nLLM-AT consists of Starter, Generator, and Judge. The starter selects the\ninitial LLM tier expected to solve the given question, the generator produces a\nresponse using the LLM of the selected tier, and the judge evaluates the\nvalidity of the response. If the response is invalid, LLM-AT iteratively\nupgrades to a higher-tier model, generates a new response, and re-evaluates\nuntil a valid response is obtained. Additionally, we propose accuracy\nestimator, which enables the suitable initial LLM tier selection without\ntraining. Given an input question, accuracy estimator estimates the expected\naccuracy of each LLM tier by computing the valid response rate across top-k\nsimilar queries from past inference records. Experiments demonstrate that\nLLM-AT achieves superior performance while reducing costs, making it a\npractical solution for real-world applications.", "AI": {"tldr": "The LLM Automatic Transmission (LLM-AT) framework automates the selection of suitable LLM tiers for NLP tasks, balancing cost and performance without training.", "motivation": "As NLP tasks grow in complexity, selecting the appropriate LLM tier for subtasks has become crucial for optimizing costs and performance.", "method": "LLM-AT employs a three-component system: a Starter selects an initial LLM tier, a Generator produces responses, and a Judge evaluates those responses to ensure validity. If a response is invalid, the tier is upgraded and re-evaluated until a valid response is achieved. An accuracy estimator helps in selecting the appropriate initial tier based on past inference records.", "result": "LLM-AT demonstrates enhanced performance in generating valid responses while significantly reducing costs in comparison to traditional approaches.", "conclusion": "The framework presents a practical solution for effectively selecting LLM tiers in real-world applications, making it both cost-efficient and robust for various NLP tasks.", "key_contributions": ["Introduction of LLM Automatic Transmission (LLM-AT) for automatic tier selection", "Incorporation of an accuracy estimator for initial tier selection", "Demonstration of superior performance and cost reduction in LLM usage"], "limitations": "", "keywords": ["LLM", "Automatic Transmission", "NLP", "Cost-Performance Optimization", "Accuracy Estimator"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.20925", "pdf": "https://arxiv.org/pdf/2505.20925.pdf", "abs": "https://arxiv.org/abs/2505.20925", "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "authors": ["Zhuo Li", "Guodong Du", "Weiyang Guo", "Yigeng Zhou", "Xiucheng Li", "Wenya Wang", "Fangming Liu", "Yequan Wang", "Deheng Ye", "Min Zhang", "Jing Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models (LLMs) to simultaneously satisfy multiple\nobjectives remains a significant challenge, especially given the diverse and\noften conflicting nature of human preferences. Existing alignment methods\nstruggle to balance trade-offs effectively, often requiring costly retraining\nor yielding suboptimal results across the Pareto frontier of preferences. In\nthis paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a\n\\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play}\napproach that eliminates the need for model training, while enabling LLMs to\nadapt across the entire Pareto frontier and accommodate diverse user\npreferences. In particular, \\textit{HoE} consists of three hierarchical\ncomponents: LoRA Experts, Router Experts and Preference Routing, reaching\noptimal Pareto frontiers and achieving a trade-off between parameter size,\ntraining cost, and performance. We evaluate \\textit{HoE} across various tasks\non 14 objectives and 200 different preferences among 6 benchmarks,\ndemonstrating superior performance over 15 recent baselines. Code is available\nin the supplementary materials.", "AI": {"tldr": "The paper presents HoE, a lightweight and parameter-efficient approach for aligning large language models to diverse human preferences without retraining.", "motivation": "To address the challenges of aligning LLMs to effectively balance multiple and often conflicting human preferences.", "method": "Introducing a Hierarchical Mixture-of-Experts (HoE) approach that consists of three components: LoRA Experts, Router Experts, and Preference Routing, allowing adaptation across the entire Pareto frontier without model training.", "result": "HoE was evaluated on 14 objectives and 200 different preferences across 6 benchmarks, showing superior performance compared to 15 recent baselines.", "conclusion": "HoE effectively reaches optimal Pareto frontiers while balancing parameter size, training cost, and performance, providing a new strategy for LLM alignment.", "key_contributions": ["Introduction of a Hierarchical Mixture-of-Experts framework for LLM alignment.", "Demonstration of superior performance on multiple objectives and preferences without retraining.", "Framework is lightweight and parameter-efficient, optimized for diverse user preferences."], "limitations": "", "keywords": ["large language models", "alignment", "Hierarchical Mixture-of-Experts", "parameter-efficient", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20933", "pdf": "https://arxiv.org/pdf/2505.20933.pdf", "abs": "https://arxiv.org/abs/2505.20933", "title": "Information-Theoretic Complementary Prompts for Improved Continual Text Classification", "authors": ["Duzhen Zhang", "Yong Ren", "Chenxing Li", "Dong Yu", "Tielin Zhang"], "categories": ["cs.CL"], "comment": "Accepted by Neural Networks", "summary": "Continual Text Classification (CTC) aims to continuously classify new text\ndata over time while minimizing catastrophic forgetting of previously acquired\nknowledge. However, existing methods often focus on task-specific knowledge,\noverlooking the importance of shared, task-agnostic knowledge. Inspired by the\ncomplementary learning systems theory, which posits that humans learn\ncontinually through the interaction of two systems -- the hippocampus,\nresponsible for forming distinct representations of specific experiences, and\nthe neocortex, which extracts more general and transferable representations\nfrom past experiences -- we introduce Information-Theoretic Complementary\nPrompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two\ndistinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These\nrespectively encode task-specific and task-invariant knowledge, enabling models\nto sequentially learn classification tasks without relying on data replay. To\npromote more informative prompt learning, InfoComp uses an\ninformation-theoretic framework that maximizes mutual information between\ndifferent parameters (or encoded representations). Within this framework, we\ndesign two novel loss functions: (1) to strengthen the accumulation of\ntask-specific knowledge in P-Prompt, effectively mitigating catastrophic\nforgetting, and (2) to enhance the retention of task-invariant knowledge in\nS-Prompt, improving forward knowledge transfer. Extensive experiments on\ndiverse CTC benchmarks show that our approach outperforms previous\nstate-of-the-art methods.", "AI": {"tldr": "This paper introduces InfoComp, a novel approach for Continual Text Classification that minimizes catastrophic forgetting by learning task-specific and task-invariant knowledge through distinct prompt spaces and an information-theoretic framework.", "motivation": "To address the challenge of Continual Text Classification (CTC) which suffers from catastrophic forgetting, particularly regarding task-invariant knowledge.", "method": "The approach utilizes two distinct prompt spaces, P-Prompt for task-specific knowledge and S-Prompt for task-invariant knowledge, and employs novel loss functions to maximize mutual information between parameters while learning.", "result": "Extensive experiments demonstrate that InfoComp significantly outperforms previous state-of-the-art methods in CTC benchmarks, indicating effective retention of both task-specific and task-invariant knowledge.", "conclusion": "InfoComp provides a robust framework for CTC, allowing models to learn new classification tasks effectively while preserving previously acquired knowledge.", "key_contributions": ["Introduction of two distinct prompt spaces for task-specific and task-invariant knowledge in CTC.", "Development of an information-theoretic framework to optimize learning across tasks.", "Novel loss functions designed to mitigate catastrophic forgetting and enhance knowledge retention."], "limitations": "", "keywords": ["Continual Learning", "Text Classification", "Machine Learning", "Human-Computer Interaction", "Information Theory"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20937", "pdf": "https://arxiv.org/pdf/2505.20937.pdf", "abs": "https://arxiv.org/abs/2505.20937", "title": "On VLMs for Diverse Tasks in Multimodal Meme Classification", "authors": ["Deepesh Gavit", "Debajyoti Mazumder", "Samiran Das", "Jasabanta Patro"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "In this paper, we present a comprehensive and systematic analysis of\nvision-language models (VLMs) for disparate meme classification tasks. We\nintroduced a novel approach that generates a VLM-based understanding of meme\nimages and fine-tunes the LLMs on textual understanding of the embedded meme\ntext for improving the performance. Our contributions are threefold: (1)\nBenchmarking VLMs with diverse prompting strategies purposely to each sub-task;\n(2) Evaluating LoRA fine-tuning across all VLM components to assess performance\ngains; and (3) Proposing a novel approach where detailed meme interpretations\ngenerated by VLMs are used to train smaller language models (LLMs),\nsignificantly improving classification. The strategy of combining VLMs with\nLLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,\noffensive and sentiment classification, respectively. Our results reveal the\nstrengths and limitations of VLMs and present a novel strategy for meme\nunderstanding.", "AI": {"tldr": "The paper presents a systematic analysis of vision-language models (VLMs) for meme classification tasks, introducing a novel method that enhances LLM understanding of meme text to improve classification performance.", "motivation": "To improve the performance of meme classification tasks using vision-language models (VLMs) and language models (LLMs).", "method": "The study benchmarks VLMs with different prompting strategies, evaluates LoRA fine-tuning across VLM components, and proposes using detailed meme interpretations generated by VLMs to train smaller LLMs.", "result": "Combining VLMs with LLMs yielded performance improvements of 8.34% for sarcasm, 3.52% for offensive content, and 26.24% for sentiment classification.", "conclusion": "The proposed approach reveals the strengths and limitations of VLMs and improves meme understanding significantly.", "key_contributions": ["Benchmarking VLMs with diverse prompting strategies for disparate meme classification tasks", "Evaluating LoRA fine-tuning on VLM components for performance assessment", "Proposing a novel method for training smaller LLMs using detailed meme interpretations from VLMs"], "limitations": "", "keywords": ["vision-language models", "meme classification", "fine-tuning", "language models", "textual understanding"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2505.20959", "pdf": "https://arxiv.org/pdf/2505.20959.pdf", "abs": "https://arxiv.org/abs/2505.20959", "title": "Research Community Perspectives on \"Intelligence\" and Large Language Models", "authors": ["Bertram HÃ¸jer", "Terne Sasha Thorn Jakobsen", "Anna Rogers", "Stefan Heinrich"], "categories": ["cs.CL", "cs.CY"], "comment": "ACL Findings 2025", "summary": "Despite the widespread use of ''artificial intelligence'' (AI) framing in\nNatural Language Processing (NLP) research, it is not clear what researchers\nmean by ''intelligence''. To that end, we present the results of a survey on\nthe notion of ''intelligence'' among researchers and its role in the research\nagenda. The survey elicited complete responses from 303 researchers from a\nvariety of fields including NLP, Machine Learning (ML), Cognitive Science,\nLinguistics, and Neuroscience. We identify 3 criteria of intelligence that the\ncommunity agrees on the most: generalization, adaptability, & reasoning. Our\nresults suggests that the perception of the current NLP systems as\n''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the\nrespondents see developing intelligent systems as a research goal, and these\nrespondents are more likely to consider the current systems intelligent.", "AI": {"tldr": "A survey of 303 researchers reveals differing opinions on the definition of 'intelligence' in AI, particularly in NLP, with only a minority viewing current systems as 'intelligent.'", "motivation": "To understand varying interpretations of 'intelligence' among researchers in fields related to NLP and AI, and its implication on research goals.", "method": "A survey was conducted with 303 researchers from disciplines including NLP, ML, Cognitive Science, Linguistics, and Neuroscience, focusing on their definitions and criteria for intelligence.", "result": "The survey identified three key criteria for intelligence: generalization, adaptability, and reasoning. It was found that only 29% of researchers view current NLP systems as intelligent and only 16.2% prioritize developing intelligent systems.", "conclusion": "The findings indicate that the notion of intelligence in NLP is complex and that most researchers do not align with seeing current systems as truly intelligent, suggesting a need for clearer definitions in research agendas.", "key_contributions": ["Identification of three criteria for intelligence in NLP: generalization, adaptability, and reasoning.", "Evidence that the current perception of NLP systems as intelligent is a minority view.", "Insights into researchers' priorities regarding the development of intelligent AI systems."], "limitations": "", "keywords": ["artificial intelligence", "natural language processing", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20963", "pdf": "https://arxiv.org/pdf/2505.20963.pdf", "abs": "https://arxiv.org/abs/2505.20963", "title": "Context-Aware Content Moderation for German Newspaper Comments", "authors": ["Felix Krejca", "Tobias Kietreiber", "Alexander Buchelt", "Sebastian Neumaier"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing volume of online discussions requires advanced automatic\ncontent moderation to maintain responsible discourse. While hate speech\ndetection on social media is well-studied, research on German-language\nnewspaper forums remains limited. Existing studies often neglect\nplatform-specific context, such as user history and article themes. This paper\naddresses this gap by developing and evaluating binary classification models\nfor automatic content moderation in German newspaper forums, incorporating\ncontextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging\nthe One Million Posts Corpus from the Austrian newspaper Der Standard, we\nassess the impact of context-aware models. Results show that CNN and LSTM\nmodels benefit from contextual information and perform competitively with\nstate-of-the-art approaches. In contrast, ChatGPT's zero-shot classification\ndoes not improve with added context and underperforms.", "AI": {"tldr": "This paper develops binary classification models for automatic content moderation in German newspapers, focusing on the impact of contextual information.", "motivation": "To address the lack of research on hate speech detection in German-language newspaper forums and improve automatic content moderation.", "method": "The paper develops and evaluates LSTM, CNN, and ChatGPT-3.5 Turbo models, utilizing the One Million Posts Corpus to assess the impact of contextual information in moderation tasks.", "result": "Context-aware models, specifically CNN and LSTM, show improvements in performance over traditional methods, while ChatGPT's performance does not benefit from added context.", "conclusion": "Incorporating contextual information enhances moderation model performance for German newspaper forums, highlighting a gap in existing literature related to platform-specific contexts.", "key_contributions": ["Development of context-aware models for hate speech detection in German forums.", "Evaluation of LSTM, CNN, and ChatGPT-3.5 Turbo for content moderation.", "Use of the One Million Posts Corpus to improve model training and evaluation."], "limitations": "Study is limited to German-language newspaper forums; results may not generalize to other platforms or languages.", "keywords": ["content moderation", "hate speech detection", "machine learning", "context-aware models", "German newspaper forums"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2505.20966", "pdf": "https://arxiv.org/pdf/2505.20966.pdf", "abs": "https://arxiv.org/abs/2505.20966", "title": "Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation", "authors": ["Zhibo Wang", "Xiaoze Jiang", "Zhiheng Qin", "Enyun Yu", "Han Li"], "categories": ["cs.CL", "cs.IR"], "comment": "KDD 2025", "summary": "Query auto-completion (QAC) plays a crucial role in modern search systems.\nHowever, in real-world applications, there are two pressing challenges that\nstill need to be addressed. First, there is a need for hierarchical\npersonalized representations for users. Previous approaches have typically used\nusers' search behavior as a single, overall representation, which proves\ninadequate in more nuanced generative scenarios. Additionally, query prefixes\nare typically short and may contain typos or sensitive information, increasing\nthe likelihood of generating toxic content compared to traditional text\ngeneration tasks. Such toxic content can degrade user experience and lead to\npublic relations issues. Therefore, the second critical challenge is\ndetoxifying QAC systems.\n  To address these two limitations, we propose a novel model (LaD) that\ncaptures personalized information from both long-term and short-term interests,\nincorporating adaptive detoxification. In LaD, personalized information is\ncaptured hierarchically at both coarse-grained and fine-grained levels. This\napproach preserves as much personalized information as possible while enabling\nonline generation within time constraints. To move a futher step, we propose an\nonline training method based on Reject Preference Optimization (RPO). By\nincorporating a special token [Reject] during both the training and inference\nprocesses, the model achieves adaptive detoxification. Consequently, the\ngenerated text presented to users is both non-toxic and relevant to the given\nprefix. We conduct comprehensive experiments on industrial-scale datasets and\nperform online A/B tests, delivering the largest single-experiment metric\nimprovement in nearly two years of our product. Our model has been deployed on\nKuaishou search, driving the primary traffic for hundreds of millions of active\nusers. The code is available at https://github.com/JXZe/LaD.", "AI": {"tldr": "The paper presents LaD, a novel query auto-completion model that captures hierarchical personalized representations and incorporates adaptive detoxification to improve user experience and relevance in search systems.", "motivation": "There is a need for improved personalized representations in query auto-completion (QAC) systems and a method to detoxify generated content to enhance user experience and mitigate public relations issues.", "method": "LaD captures personalized information hierarchically at both coarse-grained and fine-grained levels and employs an online training method using Reject Preference Optimization (RPO) with a special token [Reject] for adaptive detoxification.", "result": "Experiments show that LaD delivers the largest single-experiment metric improvement in nearly two years, enhancing the user experience for hundreds of millions of active users in Kuaishou search.", "conclusion": "LaD effectively balances personalized content generation with the need to produce non-toxic outputs, representing a significant advancement in QAC technologies.", "key_contributions": ["Hierarchical personalized representations for query auto-completion", "Adaptive detoxification through Reject Preference Optimization", "Deployment and real-world performance improvements in Kuaishou search."], "limitations": "", "keywords": ["Query auto-completion", "Personalization", "Detoxification", "Machine learning", "Search systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20971", "pdf": "https://arxiv.org/pdf/2505.20971.pdf", "abs": "https://arxiv.org/abs/2505.20971", "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA", "authors": ["Xiangqing Shen", "Fanfan Wang", "Rui Xia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks,\nyet they often suffer from hallucinations and lack reliable factual grounding.\nMeanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack\nthe flexible reasoning abilities of LLMs. In this paper, we present\nReason-Align-Respond (RAR), a novel framework that systematically integrates\nLLM reasoning with knowledge graphs for KGQA. Our approach consists of three\nkey components: a Reasoner that generates human-like reasoning chains, an\nAligner that maps these chains to valid KG paths, and a Responser that\nsynthesizes the final answer. We formulate this process as a probabilistic\nmodel and optimize it using the Expectation-Maximization algorithm, which\niteratively refines the reasoning chains and knowledge paths. Extensive\nexperiments on multiple benchmarks demonstrate the effectiveness of RAR,\nachieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on\nWebQSP and CWQ respectively. Human evaluation confirms that RAR generates\nhigh-quality, interpretable reasoning chains well-aligned with KG paths.\nFurthermore, RAR exhibits strong zero-shot generalization capabilities and\nmaintains computational efficiency during inference.", "AI": {"tldr": "RAR is a framework that combines LLM reasoning with knowledge graphs for Knowledge Graph Question Answering (KGQA), enhancing performance and interpretability.", "motivation": "LLMs struggle with hallucinations and factual grounding, while KGs lack reasoning flexibility; RAR merges their strengths for improved KGQA.", "method": "RAR comprises a Reasoner for generating reasoning chains, an Aligner for mapping to KG paths, and a Responser for synthesizing answers, modeled probabilistically and optimized via Expectation-Maximization.", "result": "RAR achieved state-of-the-art performance on benchmarks with Hit@1 scores of 93.3% on WebQSP and 91.0% on CWQ, with strong human-validated reasoning quality.", "conclusion": "The RAR framework significantly enhances KGQA by providing interpretable reasoning in line with KGs while demonstrating efficient inference and zero-shot generalization.", "key_contributions": ["Introduction of the RAR framework for KGQA.", "Integration of LLM reasoning with knowledge graphs.", "Demonstrated state-of-the-art performance in multiple benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Question Answering", "Probabilistic Modeling", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20976", "pdf": "https://arxiv.org/pdf/2505.20976.pdf", "abs": "https://arxiv.org/abs/2505.20976", "title": "Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing", "authors": ["Peiming Guo", "Meishan Zhang", "Jianling Li", "Min Zhang", "Yue Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main conference", "summary": "Cross-domain constituency parsing is still an unsolved challenge in\ncomputational linguistics since the available multi-domain constituency\ntreebank is limited. We investigate automatic treebank generation by large\nlanguage models (LLMs) in this paper. The performance of LLMs on constituency\nparsing is poor, therefore we propose a novel treebank generation method, LLM\nback generation, which is similar to the reverse process of constituency\nparsing. LLM back generation takes the incomplete cross-domain constituency\ntree with only domain keyword leaf nodes as input and fills the missing words\nto generate the cross-domain constituency treebank. Besides, we also introduce\na span-level contrastive learning pre-training strategy to make full use of the\nLLM back generation treebank for cross-domain constituency parsing. We verify\nthe effectiveness of our LLM back generation treebank coupled with contrastive\nlearning pre-training on five target domains of MCTB. Experimental results show\nthat our approach achieves state-of-the-art performance on average results\ncompared with various baselines.", "AI": {"tldr": "This paper proposes a novel method for automatic treebank generation using large language models (LLMs) to address the challenge of cross-domain constituency parsing, reporting state-of-the-art results.", "motivation": "Cross-domain constituency parsing is challenging due to the limited availability of multi-domain constituency treebanks.", "method": "The paper introduces a method called LLM back generation, which fills in missing words in incomplete cross-domain constituency trees, and employs a span-level contrastive learning pre-training strategy to enhance the generated treebank's utility.", "result": "The method achieves state-of-the-art performance on average results in five target domains compared to various baselines.", "conclusion": "The proposed LLM back generation approach, along with the contrastive learning strategy, significantly improves performance in cross-domain constituency parsing tasks.", "key_contributions": ["Proposal of LLM back generation for treebank generation", "Introduction of span-level contrastive learning for pre-training", "Achieving state-of-the-art results in cross-domain parsing"], "limitations": "", "keywords": ["Cross-domain Parsing", "Constituency Treebank", "Large Language Models", "Contrastive Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20977", "pdf": "https://arxiv.org/pdf/2505.20977.pdf", "abs": "https://arxiv.org/abs/2505.20977", "title": "Evaluating and Steering Modality Preferences in Multimodal Large Language Model", "authors": ["Yu Zhang", "Jinlong Ma", "Yongshuai Hou", "Xuefeng Bai", "Kehai Chen", "Yang Xiang", "Jun Yu", "Min Zhang"], "categories": ["cs.CL"], "comment": "Modality Preference", "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a \\textbf{MC\\textsuperscript{2}}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements.", "AI": {"tldr": "This paper investigates modality preference in multimodal large language models (MLLMs) and proposes methods to control this preference in various tasks.", "motivation": "To understand how MLLMs exhibit modality preference when processing multimodal contexts and to improve their performance in tasks that require multimodal evidence.", "method": "The authors created the MCÂ² benchmark to evaluate modality preference under controlled conflict scenarios and introduced a probing and steering method based on representation engineering to adjust this preference without fine-tuning.", "result": "Evaluation of 18 MLLMs showed clear modality bias influenced by external factors, and the proposed method successfully enhanced the preferred modality in downstream tasks like hallucination mitigation and multimodal translation.", "conclusion": "The findings highlight the existence of modality preference in MLLMs, and the proposed methods provide a means to effectively control this preference, demonstrating improved performance in applications.", "key_contributions": ["Development of the MCÂ² benchmark for evaluating modality preference", "Introduction of a method to control modality preference using representation engineering", "Demonstration of improved performance in multimodal tasks through the proposed methods"], "limitations": "", "keywords": ["multimodal large language models", "modality preference", "representation engineering"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.20993", "pdf": "https://arxiv.org/pdf/2505.20993.pdf", "abs": "https://arxiv.org/abs/2505.20993", "title": "Who Reasons in the Large Language Models?", "authors": ["Jie Shao", "Jianxin Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), the\nprocess of endowing them with new capabilities--such as mathematical\nreasoning--remains largely empirical and opaque. A critical open question is\nwhether reasoning abilities stem from the entire model, specific modules, or\nare merely artifacts of overfitting. In this work, we hypothesize that the\nreasoning capabilities in well-trained LLMs are primarily attributed to the\noutput projection module (oproj) in the Transformer's multi-head self-attention\n(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for\nNetworks (SfN), a suite of diagnostic tools designed to probe and analyze the\ninternal behaviors of LLMs. Using SfN, we provide both circumstantial and\nempirical evidence suggesting that oproj plays a central role in enabling\nreasoning, whereas other modules contribute more to fluent dialogue. These\nfindings offer a new perspective on LLM interpretability and open avenues for\nmore targeted training strategies, potentially enabling more efficient and\nspecialized LLMs.", "AI": {"tldr": "This paper investigates the role of the output projection module in large language models' reasoning abilities using diagnostic tools.", "motivation": "To understand how reasoning capabilities are manifested in large language models, particularly whether they arise from the entire model or specific components.", "method": "The authors introduce a tool called Stethoscope for Networks (SfN) to analyze the internal behaviors of LLMs, focusing on the output projection module of the Transformer architecture.", "result": "Evidence indicates that the output projection module is central to reasoning capabilities, while other components are more important for fluent dialogue.", "conclusion": "The findings provide insights into LLM interpretability and suggest directions for more effective training strategies for specialized models.", "key_contributions": ["Introduction of Stethoscope for Networks (SfN) for analyzing LLMs", "Hypothesis that reasoning is primarily attributed to the output projection module", "Evidence supporting targeted training strategies for LLMs"], "limitations": "", "keywords": ["large language models", "output projection module", "reasoning capabilities"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20995", "pdf": "https://arxiv.org/pdf/2505.20995.pdf", "abs": "https://arxiv.org/abs/2505.20995", "title": "Articulatory strategy in vowel production as a basis for speaker discrimination", "authors": ["Justin J. H. Lo", "Patrycja Strycharczuk", "Sam Kirkham"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "The way speakers articulate is well known to be variable across individuals\nwhile at the same time subject to anatomical and biomechanical constraints. In\nthis study, we ask whether articulatory strategy in vowel production can be\nsufficiently speaker-specific to form the basis for speaker discrimination. We\nconducted Generalised Procrustes Analyses of tongue shape data from 40 English\nspeakers from the North West of England, and assessed the\nspeaker-discriminatory potential of orthogonal tongue shape features within the\nframework of likelihood ratios. Tongue size emerged as the individual dimension\nwith the strongest discriminatory power, while tongue shape variation in the\nmore anterior part of the tongue generally outperformed tongue shape variation\nin the posterior part. When considered in combination, shape-only information\nmay offer comparable levels of speaker specificity to size-and-shape\ninformation, but only when features do not exhibit speaker-level co-variation.", "AI": {"tldr": "This study investigates whether individual articulatory strategies in vowel production can be used for speaker discrimination, revealing that tongue size and anterior tongue shape are key factors.", "motivation": "To determine if articulatory strategies in vowel production can allow for speaker discrimination based on anatomical and biomechanical variability.", "method": "Generalised Procrustes Analyses of tongue shape data from 40 English speakers, assessing speaker-discriminatory potential through likelihood ratios.", "result": "Tongue size was found to be the most significant factor for discrimination, with anterior tongue shape variation outperforming posterior variation.", "conclusion": "Shape-only information may provide speaker specificity similar to size-and-shape information, depending on the features' co-variation across speakers.", "key_contributions": ["Demonstrated the relevance of tongue size and shape in speaker discrimination", "Introduced a statistical framework for analyzing speaker-specific articulation", "Highlighted the importance of anterior tongue shape variation in speaker discrimination"], "limitations": "Potential limitations include the sample size and speaker homogeneity, which may affect generalizability.", "keywords": ["articulation", "speaker discrimination", "tongue shape", "vowel production", "Generalised Procrustes Analyses"], "importance_score": 2, "read_time_minutes": 7}}
{"id": "2505.21003", "pdf": "https://arxiv.org/pdf/2505.21003.pdf", "abs": "https://arxiv.org/abs/2505.21003", "title": "Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?", "authors": ["Yifei Wang", "Yu Sheng", "Linjing Li", "Daniel Zeng"], "categories": ["cs.CL"], "comment": "Camera-ready versions for ACL 2025 Findings", "summary": "Recent advances in handling long sequences have facilitated the exploration\nof long-context in-context learning (ICL). While much of the existing research\nemphasizes performance improvements driven by additional in-context examples,\nthe influence on the trustworthiness of generated responses remains\nunderexplored. This paper addresses this gap by investigating how increased\nexamples influence predictive uncertainty, an essential aspect in\ntrustworthiness. We begin by systematically quantifying the uncertainty of ICL\nwith varying shot counts, analyzing the impact of example quantity. Through\nuncertainty decomposition, we introduce a novel perspective on performance\nenhancement, with a focus on epistemic uncertainty (EU). Our results reveal\nthat additional examples reduce total uncertainty in both simple and complex\ntasks by injecting task-specific knowledge, thereby diminishing EU and\nenhancing performance. For complex tasks, these advantages emerge only after\naddressing the increased noise and uncertainty associated with longer inputs.\nFinally, we explore the evolution of internal confidence across layers,\nunveiling the mechanisms driving the reduction in uncertainty.", "AI": {"tldr": "This paper investigates how the number of in-context examples influences predictive uncertainty in long-context in-context learning (ICL), revealing that more examples can enhance trustworthiness by reducing epistemic uncertainty.", "motivation": "The paper addresses the underexplored relationship between the quantity of in-context examples and the trustworthiness of generated responses in ICL.", "method": "The authors systematically quantify the uncertainty of ICL with varying shot counts and conduct uncertainty decomposition to analyze the effects on performance and epistemic uncertainty.", "result": "The results show that additional examples reduce total uncertainty and epistemic uncertainty, enhancing performance, particularly for complex tasks after managing increased noise.", "conclusion": "Increased in-context examples improve performance by injecting task-specific knowledge, ultimately enhancing the trustworthiness of predictions.", "key_contributions": ["Introduction of a novel perspective on performance enhancement through uncertainty decomposition.", "Demonstration of the role of example quantity in reducing epistemic uncertainty.", "Analysis of internal confidence evolution across layers."], "limitations": "", "keywords": ["in-context learning", "predictive uncertainty", "epistemic uncertainty", "long-context sequences", "trustworthiness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21011", "pdf": "https://arxiv.org/pdf/2505.21011.pdf", "abs": "https://arxiv.org/abs/2505.21011", "title": "LLMs are Frequency Pattern Learners in Natural Language Inference", "authors": ["Liang Cheng", "Zhaowei Wang", "Mark Steedman"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "While fine-tuning LLMs on NLI corpora improves their inferential performance,\nthe underlying mechanisms driving this improvement remain largely opaque. In\nthis work, we conduct a series of experiments to investigate what LLMs actually\nlearn during fine-tuning. We begin by analyzing predicate frequencies in\npremises and hypotheses across NLI datasets and identify a consistent frequency\nbias, where predicates in hypotheses occur more frequently than those in\npremises for positive instances. To assess the impact of this bias, we evaluate\nboth standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial\ncases. We find that LLMs exploit frequency bias for inference and perform\npoorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit\nsignificantly increased reliance on this bias, suggesting that they are\nlearning these frequency patterns from datasets. Finally, we compute the\nfrequencies of hyponyms and their corresponding hypernyms from WordNet,\nrevealing a correlation between frequency bias and textual entailment. These\nfindings help explain why learning frequency patterns can enhance model\nperformance on inference tasks.", "AI": {"tldr": "This work investigates the improvements in LLMs' inferential performance through fine-tuning on NLI corpora, focusing on the role of frequency bias in premises and hypotheses.", "motivation": "To understand what LLMs learn during fine-tuning on Natural Language Inference (NLI) datasets and the mechanisms behind improved inferential performance.", "method": "The study involved analyzing predicate frequencies across NLI datasets and assessing LLM performance on both bias-consistent and bias-adversarial instances.", "result": "The results indicate that LLMs leverage frequency bias for inference, showing poor performance on adversarial instances and increased reliance on this bias when fine-tuned.", "conclusion": "The findings suggest that LLMs learn frequency patterns that enhance their performance on inference tasks, providing insights into the biases present in NLI datasets.", "key_contributions": ["Analyzed predicate frequencies in NLI datasets to identify consistent frequency bias.", "Evaluated impact of frequency bias on LLMs' performance under varying conditions.", "Revealed correlation between frequency bias of words and model performance in entailment tasks."], "limitations": "The study primarily focuses on frequency bias and does not explore other potential factors affecting LLM inferential performance.", "keywords": ["Large Language Models", "Natural Language Inference", "Frequency Bias", "Textual Entailment", "Fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21033", "pdf": "https://arxiv.org/pdf/2505.21033.pdf", "abs": "https://arxiv.org/abs/2505.21033", "title": "Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation", "authors": ["Seungmin Lee", "Yongsang Yoo", "Minhwa Jung", "Min Song"], "categories": ["cs.CL"], "comment": "19 pages, 3 figures, Accepted to Findings of the ACL 2025", "summary": "Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent\nsegments. DTS plays a crucial role in various NLP downstream tasks, but suffers\nfrom chronic problems: data shortage, labeling ambiguity, and incremental\ncomplexity of recently proposed solutions. On the other hand, Despite advances\nin Large Language Models (LLMs) and reasoning strategies, these have rarely\nbeen applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for\nOpen-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step\ndeductive reasoning to enhance DTS performance and enable case study using\nintermediate result. Our method employs a structured prompting approach for\nbidirectional context summarization, utterance intent classification, and\ndeductive topic shift detection. In the intent classification process, we\npropose the generalizable intent list for domain-agnostic dialogue intent\nclassification. Experiments in various dialogue settings demonstrate that\nDef-DTS consistently outperforms traditional and state-of-the-art approaches,\nwith each subtask contributing to improved performance, particularly in\nreducing type 2 error. We also explore the potential for autolabeling,\nemphasizing the importance of LLM reasoning techniques in DTS.", "AI": {"tldr": "This paper presents Def-DTS, a method utilizing LLM-based deductive reasoning to improve dialogue topic segmentation (DTS).", "motivation": "Dialogue Topic Segmentation (DTS) is essential for various NLP tasks but faces issues such as data shortage and complexity, which this paper addresses using LLMs.", "method": "The proposed method employs multi-step deductive reasoning through structured prompting for summarizing context, classifying intent, and detecting topic shifts.", "result": "Def-DTS outperforms traditional and state-of-the-art DTS methods, reducing type 2 errors significantly across multiple dialogue settings.", "conclusion": "The findings highlight the potential of using LLM reasoning techniques for enhancing dialogue topic segmentation and suggest avenues for autolabeling.", "key_contributions": ["Introduction of Def-DTS for enhanced dialogue topic segmentation", "Generalizable intent list for domain-agnostic intent classification", "Significant performance improvements demonstrated in experiments"], "limitations": "", "keywords": ["Dialogue Topic Segmentation", "Large Language Models", "NLP", "Deductive Reasoning", "Intent Classification"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.21040", "pdf": "https://arxiv.org/pdf/2505.21040.pdf", "abs": "https://arxiv.org/abs/2505.21040", "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "authors": ["Wei Chen", "Zhao Zhang", "Meng Yuan", "Kepeng Xu", "Fuzhen Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 6 figures", "summary": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT.", "AI": {"tldr": "This paper presents FCKT, a fine-grained cross-task knowledge transfer framework for targeted sentiment analysis, emphasizing the importance of aspect extraction and contextual sentiment differentiation.", "motivation": "The study addresses the limitations of existing targeted sentiment analysis methods that rely on coarse-grained knowledge transfer, leading to negative transfer when predicting sentiments.", "method": "The authors propose the FCKT framework, which incorporates aspect-level information into sentiment prediction to enable fine-grained knowledge transfer.", "result": "Experiments conducted on three datasets show that FCKT effectively mitigates negative transfer and enhances the overall performance of targeted sentiment analysis compared to various baselines and large language models.", "conclusion": "FCKT improves sentiment prediction by addressing the oversimplified assumptions about aspect-sentiment relationships, ultimately leading to better task performance.", "key_contributions": ["Introduction of the FCKT framework for targeted sentiment analysis", "Achievement of fine-grained knowledge transfer", "Demonstration of improved performance over existing approaches and baselines"], "limitations": "", "keywords": ["targeted sentiment analysis", "cross-task knowledge transfer", "aspect extraction"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.21043", "pdf": "https://arxiv.org/pdf/2505.21043.pdf", "abs": "https://arxiv.org/abs/2505.21043", "title": "Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "categories": ["cs.CL", "cs.RO"], "comment": null, "summary": "Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)\nfacilitate naturalistic human-robot interaction, yet most rely solely on\nspeech. We introduce MM-VAP, a multimodal PTTM which combines speech with\nvisual cues including facial expression, head pose and gaze. We find that it\noutperforms the state-of-the-art audio-only in videoconferencing interactions\n(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which\naggregates all holds and shifts, we group by duration of silence between turns.\nThis reveals that through the inclusion of visual features, MM-VAP outperforms\na state-of-the-art audio-only turn-taking model across all durations of speaker\ntransitions. We conduct a detailed ablation study, which reveals that facial\nexpression features contribute the most to model performance. Thus, our working\nhypothesis is that when interlocutors can see one another, visual cues are\nvital for turn-taking and must therefore be included for accurate turn-taking\nprediction. We additionally validate the suitability of automatic speech\nalignment for PTTM training using telephone speech. This work represents the\nfirst comprehensive analysis of multimodal PTTMs. We discuss implications for\nfuture work and make all code publicly available.", "AI": {"tldr": "We present MM-VAP, a multimodal predictive turn-taking model that incorporates visual cues alongside speech, outperforming audio-only models in predicting turn-taking in videoconferencing.", "motivation": "To improve human-robot interaction through accurate turn-taking prediction by integrating visual cues beyond just speech.", "method": "MM-VAP, a multimodal predictive turn-taking model, uses speech and visual cues (facial expression, head pose, gaze) to predict when participants will hold or shift turns during conversation.", "result": "MM-VAP achieved 84% prediction accuracy compared to 79% for state-of-the-art audio-only models, especially improving across different durations of speaker transitions.", "conclusion": "Incorporating visual cues significantly enhances turn-taking prediction performance, indicating their importance in settings where interlocutors can see each other.", "key_contributions": ["Introduction of MM-VAP, a multimodal PTTM incorporating visual cues", "Demonstrated superior performance over audio-only models in turn-taking prediction", "Conducted an ablation study identifying the importance of facial expressions in model accuracy."], "limitations": "", "keywords": ["multimodal interaction", "turn-taking prediction", "human-robot interaction", "speech alignment", "facial cues"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21068", "pdf": "https://arxiv.org/pdf/2505.21068.pdf", "abs": "https://arxiv.org/abs/2505.21068", "title": "Predicting Implicit Arguments in Procedural Video Instructions", "authors": ["Anil Batra", "Laura Sevilla-Lara", "Marcus Rohrbach", "Frank Keller"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.", "AI": {"tldr": "This paper presents the Implicit-VidSRL dataset to enhance AI's understanding of procedural texts in cooking contexts by improving Semantic Role Labeling, particularly for implicit argument inference.", "motivation": "Understanding procedural instructions requires recognizing both explicit and implicit arguments, which current benchmarks fail to address effectively.", "method": "Introduction of the Implicit-VidSRL dataset to evaluate multimodal models on their ability to infer implicit semantic roles within cooking procedures.", "result": "Multimodal LLMs were tested and shown to struggle with predicting implicit arguments; the proposed model, iSRL-Qwen2-VL, demonstrated significant improvements in F1-scores for implicit semantic roles.", "conclusion": "The study illustrates the challenges of current models in understanding implicit elements in multimodal procedural data and offers a dataset to address these challenges.", "key_contributions": ["Development of the Implicit-VidSRL dataset for multimodal cooking procedures", "Benchmarking of recent multimodal LLMs on implicit semantic role prediction", "Introduction of the iSRL-Qwen2-VL model with improved prediction accuracy"], "limitations": "The study focuses specifically on cooking procedures, limiting the generalizability of findings across other procedural domains.", "keywords": ["Semantic Role Labeling", "multimodal models", "cooking procedures", "implicit arguments", "LLM"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21072", "pdf": "https://arxiv.org/pdf/2505.21072.pdf", "abs": "https://arxiv.org/abs/2505.21072", "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation", "authors": ["Ekaterina Fadeeva", "Aleksandr Rubashevskii", "Roman Vashurin", "Shehzaad Dhuliawala", "Artem Shelmanov", "Timothy Baldwin", "Preslav Nakov", "Mrinmaya Sachan", "Maxim Panov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods.", "AI": {"tldr": "Introducing FRANQ, a method for hallucination detection in Retrieval-Augmented Generation (RAG) outputs by using Uncertainty Quantification techniques.", "motivation": "RAG systems face issues with hallucinations that result in factually incorrect outputs. Current methods mistakenly conflate factuality with faithfulness to retrieved context.", "method": "FRANQ utilizes various Uncertainty Quantification techniques to assess factuality based on the faithfulness of statements to retrieved context.", "result": "FRANQ demonstrates more accurate detection of factual errors in RAG-generated outputs compared to existing methods through extensive experiments on QA tasks.", "conclusion": "The proposed FRANQ method significantly enhances the ability to determine factuality in RAG outputs, providing a clearer distinction between factuality and faithfulness.", "key_contributions": ["Introduction of FRANQ for hallucination detection in RAG outputs", "Development of a new QA dataset annotated for factuality and faithfulness", "Demonstration of improved accuracy in detecting errors in RAG responses"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "hallucination detection", "Uncertainty Quantification"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.21082", "pdf": "https://arxiv.org/pdf/2505.21082.pdf", "abs": "https://arxiv.org/abs/2505.21082", "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "authors": ["Jieyong Kim", "Tongyoung Kim", "Soonjin Yoon", "Jaehyung Kim", "Dongha Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs.", "AI": {"tldr": "This paper presents RPM, a framework for reasoning-level personalization in black-box large language models (LLMs), enhancing model outputs by aligning them with user-specific reasoning processes.", "motivation": "The motivation is to improve personalization in black-box LLMs, which typically produce generalized outputs that don't consider individual user preferences and reasoning styles.", "method": "RPM constructs statistical user-specific factors from user history, builds personalized reasoning paths, and retrieves reasoning-aligned examples to condition inference on user-specific logic.", "result": "Extensive experiments show that RPM outperforms existing response-level personalization methods, improving both predictive accuracy and interpretability.", "conclusion": "The reasoning-level personalization using RPM allows LLMs to produce outputs grounded in user-specific logic, thereby enhancing performance and user satisfaction.", "key_contributions": ["Introduction of RPM for reasoning-level personalization in black-box LLMs", "Construction of user-specific reasoning paths", "Demonstration of improved predictive accuracy and interpretability over traditional methods."], "limitations": "", "keywords": ["large language models", "personalization", "reasoning-level", "human-computer interaction", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21092", "pdf": "https://arxiv.org/pdf/2505.21092.pdf", "abs": "https://arxiv.org/abs/2505.21092", "title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge", "authors": ["Daeen Kabir", "Minhajur Rahman Chowdhury Mahim", "Sheikh Shafayat", "Adnan Sadik", "Arian Ahmed", "Eunsu Kim", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we introduce BLUCK, a new dataset designed to measure the\nperformance of Large Language Models (LLMs) in Bengali linguistic understanding\nand cultural knowledge. Our dataset comprises 2366 multiple-choice questions\n(MCQs) carefully curated from compiled collections of several college and job\nlevel examinations and spans 23 categories covering knowledge on Bangladesh's\nculture and history and Bengali linguistics. We benchmarked BLUCK using 6\nproprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,\nGemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that\nwhile these models perform reasonably well overall, they, however, struggles in\nsome areas of Bengali phonetics. Although current LLMs' performance on Bengali\ncultural and linguistic contexts is still not comparable to that of mainstream\nlanguages like English, our results indicate Bengali's status as a mid-resource\nlanguage. Importantly, BLUCK is also the first MCQ-based evaluation benchmark\nthat is centered around native Bengali culture, history, and linguistics.", "AI": {"tldr": "BLUCK is a new dataset for evaluating LLMs on Bengali linguistic understanding and cultural knowledge, consisting of 2366 MCQs across various categories.", "motivation": "To measure LLM performance in Bengali language and culture, filling a gap for mid-resource languages.", "method": "Created a dataset of 2366 multiple-choice questions from exams, covering Bengali culture, history, and linguistics; benchmarked on several LLMs.", "result": "The benchmarking results indicate reasonable performance of LLMs, but notable struggles in Bengali phonetics.", "conclusion": "BLUCK highlights the need for better LLM performance on mid-resource languages, specifically Bengali, and serves as a foundational evaluation tool.", "key_contributions": ["Introduction of the BLUCK dataset for Bengali LLM evaluation", "First MCQ benchmark focused on Bengali cultural and linguistic content", "Benchmarking results highlighting performance gaps in Bengali phonetics"], "limitations": "", "keywords": ["Bengali", "LLM", "dataset", "evaluation", "cultural knowledge"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.21097", "pdf": "https://arxiv.org/pdf/2505.21097.pdf", "abs": "https://arxiv.org/abs/2505.21097", "title": "Thinker: Learning to Think Fast and Slow", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "comment": "21 pages", "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.", "AI": {"tldr": "This paper introduces a modified question-answering task inspired by Dual Process Theory to enhance the reasoning capabilities of LLMs through a structured multi-stage process, resulting in improved accuracy and efficiency.", "motivation": "To address the imprecise and redundant responses generated by LLMs in QA tasks, the authors aim to improve reasoning capabilities by applying a structured process inspired by psychological theories.", "method": "The proposed method involves a four-stage QA task: Fast Thinking (quick response within a token budget), Verification (evaluating the initial response), Slow Thinking (refining the response), and Summarization (condensing the refined response into precise steps).", "result": "The modification improved average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. The Fast Thinking mode alone achieved 26.8% accuracy with fewer than 1000 tokens.", "conclusion": "The findings indicate that distinct systems of intuition and deliberative reasoning in LLMs can be enhanced through structured training methods, leading to improved performance in QA tasks.", "key_contributions": ["Introduction of a multi-stage QA task based on Dual Process Theory", "Demonstration of improved accuracy and inference efficiency in LLM responses", "Revelation of the benefits of structured reasoning processes for enhancing LLM performance."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Question Answering", "Dual Process Theory", "Inference Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21109", "pdf": "https://arxiv.org/pdf/2505.21109.pdf", "abs": "https://arxiv.org/abs/2505.21109", "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction", "authors": ["Bogdan Bogachov", "Yaoyao Fiona Zhao"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG", "I.2.7; I.2.1; I.5.1; I.2.6; H.3.1"], "comment": "10 pages, 4 Figures, 6 Tables. This paper has been accepted to be\n  published in the proceedings of IDETC-CIE 2025", "summary": "Despite recent advancements in domain adaptation techniques for large\nlanguage models, these methods remain computationally intensive, and the\nresulting models can still exhibit hallucination issues. Most existing\nadaptation methods do not prioritize reducing the computational resources\nrequired for fine-tuning and inference of language models. Hallucination issues\nhave gradually decreased with each new model release. However, they remain\nprevalent in engineering contexts, where generating well-structured text with\nminimal errors and inconsistencies is critical. This work introduces a novel\napproach called the Small Language Graph (SLG), which is a lightweight\nadaptation solution designed to address the two key challenges outlined above.\nThe system is structured in the form of a graph, where each node represents a\nlightweight expert - a small language model fine-tuned on specific and concise\ntexts. The results of this study have shown that SLG was able to surpass\nconventional fine-tuning methods on the Exact Match metric by 3 times.\nAdditionally, the fine-tuning process was 1.7 times faster compared to that of\na larger stand-alone language model. These findings introduce a potential for\nsmall to medium-sized engineering companies to confidently use generative AI\ntechnologies, such as LLMs, without the necessity to invest in expensive\ncomputational resources. Also, the graph architecture and the small size of\nexpert nodes offer a possible opportunity for distributed AI systems, thus\npotentially diverting the global need for expensive centralized compute\nclusters.", "AI": {"tldr": "The Small Language Graph (SLG) offers a lightweight adaptation approach for large language models, reducing computational demands while improving performance and addressing hallucination issues.", "motivation": "To reduce the computational resources required for fine-tuning and inference of language models, and to mitigate hallucination problems that affect the generation of structured text.", "method": "The SLG is structured as a graph with nodes representing lightweight expert language models fine-tuned on specific texts, enabling efficient adaptation.", "result": "SLG surpassed conventional fine-tuning methods on the Exact Match metric by 3 times, and the fine-tuning process was 1.7 times faster than that of larger models.", "conclusion": "SLG enables small to medium-sized engineering companies to use generative AI technologies without heavy computational investments, potentially paving the way for distributed AI systems.", "key_contributions": ["Introduction of the Small Language Graph (SLG) for language model adaptation", "Demonstrated significant performance improvements in fine-tuning", "Potential for widespread adoption in engineering contexts due to reduced resource demands"], "limitations": "Not specified in the abstract.", "keywords": ["Small Language Graph", "language models", "adaptation", "fine-tuning", "generative AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21115", "pdf": "https://arxiv.org/pdf/2505.21115.pdf", "abs": "https://arxiv.org/abs/2505.21115", "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA", "authors": ["Sergey Pletenev", "Maria Marina", "Nikolay Ivanov", "Daria Galimzianova", "Nikita Krayko", "Mikhail Salnikov", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.", "AI": {"tldr": "Introduces EverGreenQA, a multilingual QA dataset focusing on question temporality, and benchmarks LLM performance on this aspect.", "motivation": "To address the issue of hallucination in LLMs during QA tasks, particularly by exploring the concept of question temporality and its impact on answer reliability.", "method": "The study presents EverGreenQA, a dataset labeled with evergreen and mutable question characteristics, and benchmarks 12 modern LLMs using explicit and implicit measures of temporality. Additionally, it trains EG-E5, a multilingual classifier for this classification task.", "result": "Benchmarked LLMs show varying capabilities in recognizing question temporality, and the EG-E5 classifier achieves state-of-the-art performance in classifying evergreen vs. mutable questions.", "conclusion": "The introduction of EverGreenQA and EG-E5 provides valuable tools for enhancing the reliability of QA systems in understanding question temporality, with implications for real-world applications.", "key_contributions": ["Creation of EverGreenQA dataset with evergreen labels.", "Benchmarking of LLMs on question temporality.", "Development of EG-E5, a lightweight classifier with SoTA performance."], "limitations": "Focused on the classification of question temporality, leaving other factors affecting LLM performance unaddressed.", "keywords": ["Large Language Models", "question answering", "temporality", "multilingual dataset", "classifier"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.21137", "pdf": "https://arxiv.org/pdf/2505.21137.pdf", "abs": "https://arxiv.org/abs/2505.21137", "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "authors": ["Mengjie Qian", "Rao Ma", "Stefano BannÃ²", "Kate M. Knill", "Mark J. F. Gales"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech", "summary": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial.", "AI": {"tldr": "This paper explores the application of end-to-end speech models for Spoken Grammatical Error Correction (SGEC) and feedback generation, introducing a pseudo-labelling method to enhance training data.", "motivation": "The motivation behind this research is to improve Spoken Grammatical Error Correction and feedback mechanisms for second language learners, teachers, and test takers using modern speech foundation models.", "method": "The authors employed a pseudo-labelling process to increase the size of the training dataset significantly and utilized an E2E Whisper-based model for SGEC and feedback generation, analyzing the effects of model size and data augmentation.", "result": "The introduction of pseudo-labelling expanded the training data from 77 hours to approximately 2500 hours, resulting in improved performance. Prompting the Whisper model with fluent transcriptions yielded slight performance gains in SGEC and more significant improvements in feedback generation.", "conclusion": "While increasing model size did not provide performance benefits when using pseudo-labelled data, the addition of fluent prompts proved to enhance model performance in targeted tasks.", "key_contributions": ["Introduced a pseudo-labelling technique to enhance training data for SGEC tasks.", "Employed an end-to-end Whisper-based model for grammatical error correction and feedback generation.", "Demonstrated the effectiveness of prompting with fluent transcriptions for performance improvement."], "limitations": "", "keywords": ["Spoken Grammatical Error Correction", "end-to-end models", "feedback generation", "pseudo-labelling", "Whisper model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21138", "pdf": "https://arxiv.org/pdf/2505.21138.pdf", "abs": "https://arxiv.org/abs/2505.21138", "title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis", "authors": ["Tianyi Xu", "Hongjie Chen", "Wang Qing", "Lv Hang", "Jian Kang", "Li Jie", "Zhennan Lin", "Yongxiang Li", "Xie Lei"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch", "AI": {"tldr": "This paper explores enhancing ASR performance for Chinese dialects using self-supervised learning and large language models.", "motivation": "Chinese accents and dialects pose challenges for ASR models due to limited data availability. The paper aims to improve ASR models in this context by leveraging self-supervised pre-training with LLMs.", "method": "The authors pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect speech data and align it with a supervised dataset of 40,000 hours to evaluate the impacts of different projectors and LLMs on speech recognition performance.", "result": "The proposed method achieved state-of-the-art results on several dialect datasets, including Kespeech.", "conclusion": "The study demonstrates the effectiveness of self-supervised learning combined with LLMs for ASR in low-resource scenarios, particularly for Chinese dialects and accents.", "key_contributions": ["Introduction of a pre-training approach using self-supervised learning for ASR in dialects", "Demonstration of state-of-the-art performance with the Data2vec2 model on dialect datasets", "Open-sourcing of research to facilitate reproducibility"], "limitations": "", "keywords": ["ASR", "self-supervised learning", "Chinese dialects", "large language models", "speech recognition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21148", "pdf": "https://arxiv.org/pdf/2505.21148.pdf", "abs": "https://arxiv.org/abs/2505.21148", "title": "Assessment of L2 Oral Proficiency using Speech Large Language Models", "authors": ["Rao Ma", "Mengjie Qian", "Siyuan Tang", "Stefano BannÃ²", "Kate M. Knill", "Mark J. F. Gales"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech", "summary": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training.", "AI": {"tldr": "This paper explores the use of multi-modal large language models for assessing L2 oral proficiency, comparing various training strategies that enhance performance over traditional models.", "motivation": "The increasing need for automatic graders for spoken language assessment due to the rise in L2 English speakers and the limitations of traditional grading systems.", "method": "The paper compares different training strategies with regression and classification targets using large language models specifically designed for speech recognition and assessment.", "result": "The speech LLMs demonstrated superior performance compared to all previous baselines on two datasets and showed strong generalization capabilities in evaluations across different tasks.", "conclusion": "Multi-modal LLMs can effectively serve as automatic graders for L2 oral proficiency, overcoming the limitations of past methodologies.", "key_contributions": ["Introduction of multi-modal LLMs for spoken language assessment", "Demonstration of superior performance over existing models", "Validation of strong generalization capabilities across tasks"], "limitations": "The study may be limited by the specific datasets used and the focus on English-language learners only.", "keywords": ["spoke language assessment", "large language models", "L2 proficiency grading"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21171", "pdf": "https://arxiv.org/pdf/2505.21171.pdf", "abs": "https://arxiv.org/abs/2505.21171", "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "authors": ["Rochelle Choenni", "Ivan Titov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning.", "AI": {"tldr": "The paper presents M-Wanda, a novel pruning method aimed at maintaining multilingual performance in large language models while reducing their size by dynamically adjusting sparsity based on cross-lingual importance.", "motivation": "Recent efforts in one-shot pruning of multilingual LLMs reveal serious trade-offs between model size and performance, necessitating a better understanding of these dynamics.", "method": "The authors propose M-Wanda, a pruning method that integrates language-aware activation statistics into its sparsity criteria, allowing for dynamic layerwise adjustments based on cross-lingual importance.", "result": "M-Wanda demonstrates improved multilingual performance with minimal cost, indicating the effectiveness of optimizing pruning for multilingual models.", "conclusion": "The study shows that careful pruning can retain performance while reducing model size, paving the way for future research in multilingual pruning techniques.", "key_contributions": ["Introduction of M-Wanda, a language-aware pruning method", "Demonstration of substantial performance improvements with minimal sparsity", "First explicit optimization of pruning for multilingual model performance"], "limitations": "The study mainly focuses on moderate sparsity levels; extreme levels of pruning and their impacts are not evaluated.", "keywords": ["Multilingual LLMs", "Pruning", "M-Wanda", "Language-aware activation", "Sparse models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21172", "pdf": "https://arxiv.org/pdf/2505.21172.pdf", "abs": "https://arxiv.org/abs/2505.21172", "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment", "authors": ["Zheng Li", "Mao Zheng", "Mingyang Song", "Wenjie Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings.", "AI": {"tldr": "This paper introduces TAT-R1, a terminology-aware translation model that uses reinforcement learning to improve the accuracy of terminology translation in machine translation tasks.", "motivation": "To address the gap in deep reasoning LLMs concerning terminology translation, which is a crucial aspect of machine translation that has not been thoroughly explored.", "method": "The authors propose TAT-R1, which employs a word alignment model to extract keyword translation pairs and implements three rule-based alignment rewards to enhance the training of the translation model via reinforcement learning.", "result": "Experimental results demonstrate that TAT-R1 significantly boosts terminology translation accuracy compared to baseline models, while still performing well on general translation tasks.", "conclusion": "The findings suggest that with specific rewards focused on terminology, a reinforcement learning approach can effectively enhance translation models, especially for key information in texts.", "key_contributions": ["Introduction of TAT-R1, a novel terminology-aware translation model.", "Developed specific rule-based alignment rewards tailored for terminology translation.", "Conducted ablation studies to evaluate the impact of the training paradigm."], "limitations": "", "keywords": ["Terminology Translation", "Reinforcement Learning", "Machine Translation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21178", "pdf": "https://arxiv.org/pdf/2505.21178.pdf", "abs": "https://arxiv.org/abs/2505.21178", "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning", "authors": ["Mingyang Song", "Mao Zheng"], "categories": ["cs.CL"], "comment": "Ongoing Work", "summary": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.", "AI": {"tldr": "The paper proposes a two-stage reinforcement learning framework, ConciseR, to enhance reasoning capabilities in LLMs by generating more concise Chain-of-Thought responses while reducing redundancy.", "motivation": "To address the issue of overthinking and redundancy in long Chain-of-Thought responses generated by Large Language Models (LLMs).", "method": "The method involves a two-stage reinforcement learning framework: the first stage (GRPO++) incentivizes reasoning capabilities, while the second stage (L-GRPO) enforces conciseness and efficiency in responses.", "result": "ConciseR outperforms recent state-of-the-art reasoning models on various benchmarks by generating more concise Chain-of-Thought reasoning responses.", "conclusion": "ConciseR effectively addresses the challenge of excessive redundancy in reasoning outputs of LLMs, contributing to improved reasoning performance.", "key_contributions": ["Introduction of ConciseR as a novel framework for LLMs.", "First stage incentivizing better reasoning capabilities through GRPO++.", "Second stage enforcing response conciseness and efficiency with L-GRPO."], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought", "Reinforcement Learning", "Conciseness", "Reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21189", "pdf": "https://arxiv.org/pdf/2505.21189.pdf", "abs": "https://arxiv.org/abs/2505.21189", "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation", "authors": ["Gleb Mezentsev", "Ivan Oseledets"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.", "AI": {"tldr": "This paper investigates the ability of frozen large language models to generate multiple tokens in a single forward pass without autoregression, using two learned embeddings.", "motivation": "To explore the potential of large language models (LLMs) in multi-token generation without iterative decoding, which has not been thoroughly studied.", "method": "The authors examine the performance of frozen LLMs to generate hundreds of tokens from just two learned embeddings in a single forward pass.", "result": "The study demonstrates that LLMs can generate sequences of accurate text without relying on autoregressive processes, revealing an unexpected capability of these models.", "conclusion": "The findings suggest that while these embeddings are not unique to specific texts, they occupy connected and local areas in the embedding space, indicating the possibility of developing a specialized encoder for such representations.", "key_contributions": ["Demonstration of non-autoregressive multi-token generation by frozen LLMs.", "Insights into the properties of learned embeddings.", "Potential for developing dedicated encoders into embedding space."], "limitations": "The embeddings are not unique for a given text, which may limit their application.", "keywords": ["large language models", "multi-token generation", "embedding space"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21190", "pdf": "https://arxiv.org/pdf/2505.21190.pdf", "abs": "https://arxiv.org/abs/2505.21190", "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "authors": ["Jong Hak Moon", "Geon Choi", "Paloma Rabaey", "Min Gwan Kim", "Hyuk Gi Hong", "Jung-Oh Lee", "Hangyul Yoon", "Eun Woo Doe", "Jiyoun Kim", "Harshita Sharma", "Daniel C. Castro", "Javier Alvarez-Valle", "Edward Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage", "AI": {"tldr": "The paper introduces LUNGUAGE, a benchmark dataset and evaluation methods for structured radiology report generation that focus on temporal dependencies and fine-grained clinical semantics.", "motivation": "To overcome limitations of existing evaluation methods for radiology reports that do not capture temporal dependencies and detailed clinical observations across multiple studies.", "method": "A two-stage framework transforms generated reports into schema-aligned structured representations, while LUNGUAGESCORE serves as an interpretable metric for comparing structured outputs.", "result": "The LUNGUAGE dataset consists of 1,473 annotated chest X-ray reports, allowing for both single-report evaluation and longitudinal assessment, with LUNGUAGESCORE demonstrating effectiveness in structured report evaluation.", "conclusion": "The introduction of the LUNGUAGE dataset and LUNGUAGESCORE establishes a new standard for evaluating sequential radiology reporting, supporting better clinical interpretation over time.", "key_contributions": ["Development of the LUNGUAGE benchmark dataset for structured radiology reports.", "Creation of a two-stage framework for longitudinal interpretation of reports.", "Introduction of LUNGUAGESCORE as a novel evaluation metric for structured output comparison."], "limitations": "", "keywords": ["radiology", "report generation", "evaluation metric", "longitudinal assessment", "structured representation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.21191", "pdf": "https://arxiv.org/pdf/2505.21191.pdf", "abs": "https://arxiv.org/abs/2505.21191", "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities", "authors": ["Junyan Zhang", "Yubo Gao", "Yibo Yan", "Jungang Li", "Zhaorui Hou", "Sicheng Tao", "Shuliang Liu", "Song Dai", "Yonghua Hei", "Junzhuo Li", "Xuming Hu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.", "AI": {"tldr": "This study explores the effects of fine-tuning on Large Language Models (LLMs), focusing on how it alters their computational mechanisms by examining instruction-specific sparse components.", "motivation": "The motivation behind this research is to understand the computational mechanisms that enhance the instruction-following capabilities of LLMs after finetuning, which remain poorly understood.", "method": "The authors introduce HexaInst, a dataset with six distinct instructional categories, and SPARCOM, an analytical framework that includes methods for identifying sparse components, evaluating their generality, and comparing alterations across models.", "result": "Experiments show that the identified sparse components exhibit functional generality and uniqueness, highlighting their critical role in effective instruction execution within LLMs post-finetuning.", "conclusion": "The findings provide insights into the relationship between fine-tuning adaptations and the underlying computational structures, contributing to a more trustworthy LLM landscape.", "key_contributions": ["Introduction of HexaInst, a balanced instructional dataset", "Proposal of SPARCOM, a novel analytical framework for studying LLMs", "Demonstration of the functional generality and uniqueness of sparse components during instruction execution"], "limitations": "", "keywords": ["Large Language Models", "Fine-tuning", "Instruction-following", "Sparse components", "Computational mechanisms"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21218", "pdf": "https://arxiv.org/pdf/2505.21218.pdf", "abs": "https://arxiv.org/abs/2505.21218", "title": "Pretrained LLMs Learn Multiple Types of Uncertainty", "authors": ["Roi Cohen", "Omri Fahn", "Gerard de Melo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction.", "AI": {"tldr": "This paper investigates how large language models (LLMs) capture uncertainty in their outputs and the implications for predicting correctness in tasks.", "motivation": "To explore the capability of LLMs in capturing different types of uncertainty and its correlation with correct and incorrect outputs, especially in the context of hallucinations.", "method": "The study analyzes the latent space of LLMs to evaluate how they represent uncertainty without explicit training, using various benchmarks to assess performance.", "result": "The results indicate that LLMs capture multiple uncertainty types, which are useful for predicting task correctness and demonstrate a correlation with misinformation avoidance.", "conclusion": "Unifying the different types of uncertainty through techniques like instruction-tuning enhances the model's ability to predict output correctness.", "key_contributions": ["Identification of multiple types of uncertainty inherent in LLMs", "Correlation between uncertainty representation and accuracy of outputs", "Recommendation for unifying uncertainty types to improve correctness predictions"], "limitations": "", "keywords": ["Large Language Models", "Uncertainty", "Hallucinations", "Correctness Prediction", "Instruction-Tuning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.21224", "pdf": "https://arxiv.org/pdf/2505.21224.pdf", "abs": "https://arxiv.org/abs/2505.21224", "title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Understanding robustness is essential for building reliable NLP systems.\nUnfortunately, in the context of machine translation, previous work mainly\nfocused on documenting robustness failures or improving robustness. In\ncontrast, we study robustness from a model representation perspective by\nlooking at internal model representations of ungrammatical inputs and how they\nevolve through model layers. For this purpose, we perform Grammatical Error\nDetection (GED) probing and representational similarity analysis. Our findings\nindicate that the encoder first detects the grammatical error, then corrects it\nby moving its representation toward the correct form. To understand what\ncontributes to this process, we turn to the attention mechanism where we\nidentify what we term Robustness Heads. We find that Robustness Heads attend to\ninterpretable linguistic units when responding to grammatical errors, and that\nwhen we fine-tune models for robustness, they tend to rely more on Robustness\nHeads for updating the ungrammatical word representation.", "AI": {"tldr": "The paper analyzes how internal model representations evolve in NLP systems, particularly in machine translation, focusing on robustness concerning grammatical errors.", "motivation": "To address the gap in understanding how robustness manifests in the model representations of NLP systems, particularly for machine translation.", "method": "The study employs Grammatical Error Detection probing and representational similarity analysis to examine how model representations change in response to ungrammatical inputs.", "result": "The encoder detects grammatical errors and corrects them by adjusting representations toward the correct form, with specific attention given to Robustness Heads that focus on linguistic units during this process.", "conclusion": "Robustness Heads play a critical role in adapting representations in response to grammatical errors, and fine-tuning for robustness enhances their importance.", "key_contributions": ["Introduces the concept of Robustness Heads in NLP models", "Demonstrates how attentional mechanisms aid in correcting grammatical errors", "Analyzes representational changes in response to ungrammatical inputs"], "limitations": "", "keywords": ["NLP", "Robustness", "Grammatical Error Detection", "Machine Translation", "Attention Mechanism"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21239", "pdf": "https://arxiv.org/pdf/2505.21239.pdf", "abs": "https://arxiv.org/abs/2505.21239", "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners", "authors": ["Yu He", "Zihan Yao", "Chentao Song", "Tianyu Qi", "Jun Liu", "Ming Li", "Qing Huang"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD", "AI": {"tldr": "This paper proposes LMCD, a novel framework for Cognitive Diagnosis that enhances cold-start performance by leveraging large language models.", "motivation": "Cognitive Diagnosis is essential for personalized learning, but traditional models struggle in cold-start situations due to insufficient data.", "method": "LMCD utilizes large language models to facilitate Knowledge Diffusion and Semantic-Cognitive Fusion, allowing for better integration of textual information and cognitive states.", "result": "Experiments show that LMCD significantly outperforms existing methods in both exercise-cold and domain-cold settings on real-world datasets.", "conclusion": "LMCD offers a promising approach to enhancing Cognitive Diagnosis through improved semantic understanding and cognitive profiling.", "key_contributions": ["Introduces LMCD as a framework for zero-shot cognitive diagnosis", "Employs Knowledge Diffusion and Semantic-Cognitive Fusion techniques", "Demonstrates superior performance over state-of-the-art methods."], "limitations": "Currently a work in progress; practical application may require further validation.", "keywords": ["Cognitive Diagnosis", "Large Language Models", "Personalized Learning", "Cold-start Challenges", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21242", "pdf": "https://arxiv.org/pdf/2505.21242.pdf", "abs": "https://arxiv.org/abs/2505.21242", "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings", "authors": ["Gunjan Balde", "Soumyadeep Roy", "Mainack Mondal", "Niloy Ganguly"], "categories": ["cs.CL"], "comment": "16 pages. Accepted for publication in the Findings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.", "AI": {"tldr": "This study investigates vocabulary adaptation strategies for enhancing LLM performance in medical text summarization, particularly under challenging conditions with out-of-vocabulary words.", "motivation": "LLMs have shown success in medical text summarization, but evaluations often overlook performance drops in difficult conditions, particularly with out-of-vocabulary words.", "method": "The paper employs a benchmarking study with extensive experimentation of various vocabulary adaptation strategies and continual pretraining methods across three medical summarization datasets.", "result": "Vocabulary adaptation significantly improves LLM performance in summarizing medical texts, especially under challenging conditions, as confirmed by both quantitative and qualitative evaluations.", "conclusion": "The study highlights the importance of vocabulary adaptation in customizing LLMs for the medical domain, leading to better summarization outcomes as assessed by medical experts.", "key_contributions": ["Benchmarking of LLM performance under challenging conditions in medical summarization.", "Demonstration of effective vocabulary adaptation strategies for LLMs.", "Publicly available codebase for further research and experimentation."], "limitations": "", "keywords": ["Large Language Models", "medical text summarization", "vocabulary adaptation", "natural language processing", "out-of-vocabulary words"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21250", "pdf": "https://arxiv.org/pdf/2505.21250.pdf", "abs": "https://arxiv.org/abs/2505.21250", "title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision", "authors": ["Dosung Lee", "Wonjun Oh", "Boyoung Kim", "Minyoung Kim", "Joonsuk Park", "Paul Hongsuck Seo"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, ACL 2025", "summary": "Multi-hop question answering (MHQA) involves reasoning across multiple\ndocuments to answer complex questions. Dense retrievers typically outperform\nsparse methods like BM25 by leveraging semantic embeddings; however, they\nrequire labeled query-document pairs for fine-tuning. This poses a significant\nchallenge in MHQA due to the high variability of queries (reformulated)\nquestions throughout the reasoning steps. To overcome this limitation, we\nintroduce Retriever Supervision with Consistency and Relevance (ReSCORE), a\nnovel method for training dense retrievers for MHQA without labeled documents.\nReSCORE leverages large language models to capture each documents relevance to\nthe question and consistency with the correct answer and use them to train a\nretriever within an iterative question-answering framework. Experiments on\nthree MHQA benchmarks demonstrate the effectiveness of ReSCORE, with\nsignificant improvements in retrieval, and in turn, the state-of-the-art MHQA\nperformance. Our implementation is available at:\nhttps://leeds1219.github.io/ReSCORE.", "AI": {"tldr": "Introduces ReSCORE, a method for training dense retrievers in multi-hop question answering (MHQA) without needing labeled documents, using large language models for relevance and consistency assessment.", "motivation": "To address the challenge of training dense retrievers for MHQA due to the lack of labeled query-document pairs, which is exacerbated by the variability in reformulated questions.", "method": "ReSCORE uses large language models to determine each document's relevance to a question and its consistency with the correct answer, training a retriever within an iterative question-answering framework.", "result": "Experiments on three MHQA benchmarks show that ReSCORE significantly improves retrieval performance and establishes state-of-the-art results in MHQA tasks.", "conclusion": "ReSCORE effectively trains dense retrievers for MHQA without labeled documents, demonstrating its potential in enhancing retrieval and question answering.", "key_contributions": ["Introduction of ReSCORE for MHQA training without labeled documents.", "Utilization of large language models for assessing relevance and consistency.", "Demonstration of improved state-of-the-art performance on MHQA benchmarks."], "limitations": "", "keywords": ["Multi-hop question answering", "Dense retrievers", "Large language models", "Retrieval performance", "Natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.21265", "pdf": "https://arxiv.org/pdf/2505.21265.pdf", "abs": "https://arxiv.org/abs/2505.21265", "title": "Multilingual Pretraining for Pixel Language Models", "authors": ["Ilker Kesen", "Jonas F. Lotz", "Ingo Ziegler", "Phillip Rust", "Desmond Elliott"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 19 figures, 7 tables", "summary": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "AI": {"tldr": "PIXEL-M4 is a pixel language model pretrained on English, Hindi, Ukrainian, and Simplified Chinese, showing improved performance in multilingual tasks over an English-only model.", "motivation": "To explore multilingual pretraining in pixel language models and enhance their capabilities for diverse languages beyond fixed vocabulary systems.", "method": "The PIXEL-M4 model was pretrained on images of rendered text from four diverse languages, followed by evaluations on semantic and syntactic tasks to measure its effectiveness.", "result": "PIXEL-M4 outperformed an English-only model on non-Latin scripts and demonstrated rich feature capture for languages not included in pretraining.", "conclusion": "Multilingual pretraining significantly improves pixel language model performance, offering better support for a variety of languages.", "key_contributions": ["Introduction of the PIXEL-M4 model for multilingual pretraining", "Demonstrated superior performance on non-Latin scripts in evaluations", "Insights into semantic embedding alignment across languages during pretraining"], "limitations": "", "keywords": ["pixel language models", "multilingual pretraining", "semantic tasks", "syntactic tasks", "cross-lingual transfer"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.21297", "pdf": "https://arxiv.org/pdf/2505.21297.pdf", "abs": "https://arxiv.org/abs/2505.21297", "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset", "authors": ["Yifei Liu", "Li Lyna Zhang", "Yi Zhu", "Bingcheng Dong", "Xudong Zhou", "Ning Shang", "Fan Yang", "Mao Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar.", "AI": {"tldr": "The paper presents rStar-Coder, a large-scale verified dataset that enhances code reasoning in LLMs by providing high-difficulty coding challenges and long-reasoning solutions.", "motivation": "To advance code reasoning in large language models, there is a need for high-difficulty datasets with verified input-output test cases that support rigorous solution validation.", "method": "The methodology involves curating a dataset of 418K competitive programming problems, synthesizing new solvable problems, and implementing a three-step input generation method with a mutual verification mechanism for output labeling.", "result": "Experiments show that the rStar-Coder dataset significantly improves performance, enhancing various models on key benchmarks, with notable improvements in LiveCodeBench and USA Computing Olympiad results.", "conclusion": "The rStar-Coder dataset demonstrates substantial effectiveness in boosting the code reasoning capabilities of smaller LLMs, rivaling larger models, and is set to be publicly released.", "key_contributions": ["Curated dataset of competitive coding problems", "Input-output test case synthesis pipeline", "Augmented long-reasoning solutions with verification"], "limitations": "", "keywords": ["code reasoning", "large language models", "dataset", "competitive programming", "verification"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21301", "pdf": "https://arxiv.org/pdf/2505.21301.pdf", "abs": "https://arxiv.org/abs/2505.21301", "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "authors": ["Andrea Pedrotti", "Giulia Rambelli", "Caterina Villani", "Marianna Bolognesi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.", "AI": {"tldr": "This study examines category organization by analyzing subordinate-level exemplars and evaluates the alignment of LLMs with human categorization.", "motivation": "The research addresses the lack of exploration into subordinate-level categorizations and seeks to compare human and LLM-generated exemplars.", "method": "We created an Italian dataset of human-generated exemplars for 187 concrete words and used it to assess LLMs on tasks of exemplar generation, category induction, and typicality judgment.", "result": "The study found low alignment between human and LLM-generated exemplars, with performance varying across semantic domains.", "conclusion": "This research underscores the potential and limitations of AI-generated exemplars in the context of psychological and linguistic studies.", "key_contributions": ["New Italian psycholinguistic dataset of human-generated categorization exemplars.", "Evaluation of LLMs across three tasks related to category organization.", "Analysis of the alignment between human and LLM-generated exemplars."], "limitations": "The alignment discrepancies suggest limitations in current LLMs' understanding of human category structures.", "keywords": ["exemplar generation", "category induction", "typicality judgment", "psycholinguistics", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21315", "pdf": "https://arxiv.org/pdf/2505.21315.pdf", "abs": "https://arxiv.org/abs/2505.21315", "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead", "authors": ["Jesujoba O. Alabi", "Michael A. Hedderich", "David Ifeoluwa Adelani", "Dietrich Klakow"], "categories": ["cs.CL"], "comment": "Working paper", "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.", "AI": {"tldr": "This survey analyzes 734 research papers on NLP for African languages, providing an overview of progress, trends, and future directions.", "motivation": "To address the exclusion of African languages in state-of-the-art NLP systems and LLMs, which risks widening the digital divide.", "method": "The paper surveys and analyzes 734 research papers published over the past five years in the field of NLP for African languages.", "result": "The analysis identifies key trends in the field and highlights the progress made in core NLP tasks for African languages.", "conclusion": "The paper calls for more inclusive and sustainable NLP research for African languages and outlines promising future directions.", "key_contributions": ["Comprehensive overview of recent research on NLP for African languages.", "Identification of key trends shaping the field.", "Suggestions for fostering inclusive NLP research."], "limitations": "", "keywords": ["NLP", "African languages", "large language models", "digital divide", "inclusivity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.21324", "pdf": "https://arxiv.org/pdf/2505.21324.pdf", "abs": "https://arxiv.org/abs/2505.21324", "title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "authors": ["Yuxin Zhu", "Yuting Guo", "Noah Marchuck", "Abeed Sarker", "Yun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.", "AI": {"tldr": "This study presents an ensemble framework that combines LLMs and traditional ML techniques to classify ADHD diagnoses from narrative transcripts, demonstrating improved performance over individual models.", "motivation": "The integration of LLMs with traditional ML techniques for psychiatric applications remains underexplored despite advances in technology, particularly for complex narrative data.", "method": "The framework integrates three models: LLaMA3 (an LLM), RoBERTa (a pre-trained transformer), and an SVM classifier, using a majority voting mechanism to enhance classification performance.", "result": "The ensemble achieved an F1 score of 0.71, outperforming individual models, particularly improving recall while maintaining competitive precision.", "conclusion": "Combining LLMs with traditional ML approaches shows promise for robust psychiatric text classification, highlighting the value of hybrid architectures.", "key_contributions": ["Introduction of a novel ensemble framework for ADHD diagnosis classification", "Integration of LLaMA3, RoBERTa, and SVM for enhanced predictive performance", "Empirical evidence demonstrating improved recall in ADHD identification"], "limitations": "", "keywords": ["ADHD classification", "ensemble learning", "large language models", "machine learning", "psychiatric applications"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.21342", "pdf": "https://arxiv.org/pdf/2505.21342.pdf", "abs": "https://arxiv.org/abs/2505.21342", "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "authors": ["Valentin Knappich", "Annemarie Friedrich", "Anna HÃ¤tty", "Simon Razniewski"], "categories": ["cs.CL"], "comment": null, "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code.", "AI": {"tldr": "The paper introduces PEDANTIC, a novel dataset of 14k annotated US patent claims, aimed at automating the examination of patent claim definiteness, critical for improving patent application efficiency.", "motivation": "The paper addresses the frequent rejections of patent claims due to indefiniteness and the lack of annotated datasets to facilitate automatic examination methods, which could enhance patent drafting and assessment processes.", "method": "PEDANTIC was constructed using an automated pipeline that retrieves office action documents from the USPTO and employs Large Language Models (LLMs) to extract reasons for indefiniteness, followed by a human validation study to ensure annotation accuracy.", "result": "The LLM agents, specifically based on Qwen 2.5 models, were shown to struggle against logistic regression baselines in definiteness prediction, indicating limitations of the models in this specific task.", "conclusion": "The dataset and code will be publicly released, providing a valuable resource for advancing AI research in patent examination.", "key_contributions": ["Introduction of PEDANTIC dataset with 14k US patent claims", "Fully automated construction and annotation pipeline using LLMs", "Validation of LLM's effectiveness compared to traditional methods in patent claim definiteness prediction."], "limitations": "LLM agents struggled to outperform simpler logistic regression models in definiteness prediction despite correctly identifying reasons, indicating potential limitations in the LLM's reasoning capabilities for this task.", "keywords": ["patent claims", "indefiniteness", "natural language processing", "machine learning", "large language models"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2505.21354", "pdf": "https://arxiv.org/pdf/2505.21354.pdf", "abs": "https://arxiv.org/abs/2505.21354", "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "authors": ["Bidyarthi Paul", "Jalisha Jashim Era", "Mirazur Rahman Zim", "Tahmid Sattar Aothoi", "Faisal Muhammad Shah"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.", "AI": {"tldr": "This paper introduces SOMADHAN, a dataset of 8792 complex Bengali Math Word Problems designed to support NLP reasoning-focused tasks and model development.", "motivation": "The lack of human-annotated Bengali datasets for Math Word Problems has hindered progress in applying NLP techniques to this language, which is classified as low-resource.", "method": "A dataset of 8792 Bengali MWPs was created with manually written solutions; various large language models were evaluated using standard and Chain of Thought (CoT) prompting techniques, along with fine-tuning via Low-Rank Adaptation (LoRA).", "result": "LLaMA-3.3 70B achieved the highest accuracy at 88% using few-shot CoT prompting, demonstrating enhanced reasoning capabilities.", "conclusion": "This work provides a necessary dataset and framework to advance research in Bengali NLP, particularly in mathematical reasoning, and aims to promote equitable access to educational technologies.", "key_contributions": ["Introduction of SOMADHAN dataset for Bengali MWPs", "Demonstration of CoT prompting effectiveness for improved model performance", "Application of LoRA for efficient model fine-tuning"], "limitations": "", "keywords": ["Bengali", "Math Word Problems", "Natural Language Processing", "Large Language Models", "Chain of Thought"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2505.21362", "pdf": "https://arxiv.org/pdf/2505.21362.pdf", "abs": "https://arxiv.org/abs/2505.21362", "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "AI": {"tldr": "This paper proposes a framework for evaluating how large language models (LLMs) adapt their responses based on users' sociodemographic attributes across different interaction modalities.", "motivation": "To assess how LLMs can effectively engage with users by adapting their responses according to sociodemographic characteristics, an area that has not been fully explored in existing evaluations.", "method": "The study employs a multi-agent pipeline to construct a synthetic dataset that pairs dialogue histories with distinct user profiles. Responses are analyzed when user attributes are introduced explicitly through profiles and implicitly via multi-turn dialogue history.", "result": "Most LLMs adjust their values in response to demographic changes, particularly regarding age and education level, although the consistency of this adaptation varies across models.", "conclusion": "The research highlights that LLMs with stronger reasoning capabilities show greater alignment in their responses to sociodemographic changes, indicating reasoning is crucial for effective adaptation.", "key_contributions": ["Proposes a new framework for evaluating LLM adaptation to user sociodemographics", "Constructs a synthetic dataset for testing LLM responses across modalities", "Finds that reasoning capabilities influence LLMs' consistency in responses to demographic changes."], "limitations": "", "keywords": ["large language models", "sociodemographics", "multi-turn dialogue", "reasoning", "value expression"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.21378", "pdf": "https://arxiv.org/pdf/2505.21378.pdf", "abs": "https://arxiv.org/abs/2505.21378", "title": "Analyzing values about gendered language reform in LLMs' revisions", "authors": ["Jules Watson", "Xi Wang", "Raymond Liu", "Suzanne Stevenson", "Barend Beekhuizen"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment.", "AI": {"tldr": "This paper explores how LLMs revise gendered role nouns, assessing their justifications against feminist and trans-inclusive language reforms, and evaluates LLM sensitivity to contextual effects.", "motivation": "To understand LLMs' ability to revise gendered language and ensure alignment with contemporary language reforms in support of feminism and trans inclusivity.", "method": "The study utilizes sociolinguistic insights to analyze LLMs' revisions of gendered nouns and their justifications, evaluating contextual sensitivity comparable to human responses.", "result": "The findings reveal that LLMs demonstrate evidence of sensitivity to contextual effects in gendered language revision, aligning with feminist and trans-inclusive reforms.", "conclusion": "The study highlights important implications for the alignment of LLMs with societal values around gender and inclusivity in language.", "key_contributions": ["Analysis of LLMs' gendered language revision capabilities", "Evaluation of justifications for revisions from a sociolinguistic perspective", "Evidence of contextual sensitivity in LLM responses"], "limitations": "", "keywords": ["LLM", "gendered language", "sociolinguistics", "feminism", "trans inclusivity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.21380", "pdf": "https://arxiv.org/pdf/2505.21380.pdf", "abs": "https://arxiv.org/abs/2505.21380", "title": "PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense", "authors": ["Byungjun Kim", "Minju Kim", "Hyeonchu Park", "Bugeun Kim"], "categories": ["cs.CL"], "comment": "Under review", "summary": "As malicious users increasingly employ phonetic substitution to evade hate\nspeech detection, researchers have investigated such strategies. However, two\nkey challenges remain. First, existing studies have overlooked the Korean\nlanguage, despite its vulnerability to phonetic perturbations due to its\nphonographic nature. Second, prior work has primarily focused on constructing\ndatasets rather than developing architectural defenses. To address these\nchallenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)\nthat exploits the phonological characteristics of the Korean writing system,\nand (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the\ndetector's robustness by incorporating phonetic information at the\narchitectural level. Our experimental results demonstrate the effectiveness of\nour proposed methods on both perturbed and unperturbed datasets, suggesting\nthat they not only improve detection performance but also reflect realistic\nadversarial behaviors employed by malicious users.", "AI": {"tldr": "The paper addresses the challenges of hate speech detection in the Korean language by proposing novel methods that leverage phonetic characteristics to improve detection robustness against evasion tactics.", "motivation": "Malicious users employ phonetic substitutions to evade hate speech detection, with a gap in prior research regarding the Korean language and the need for architectural defenses.", "method": "The paper proposes PHonetic-Informed Substitution for Hangul (PHISH) and Mixed Encoding of Semantic-pHonetic features (MESH), which utilize the phonological traits of Korean to enhance detection systems.", "result": "The proposed methods show improved detection performance on both perturbed and unperturbed datasets, validating their effectiveness in combating adversarial evasion tactics.", "conclusion": "The findings indicate that the methods not only enhance detection capabilities but also align with realistic adversarial behaviors employed by users.", "key_contributions": ["Introduction of PHISH to leverage phonetic characteristics of Hangul for detection.", "Development of MESH to incorporate phonetic information in the detector architecture.", "Empirical validation of the proposed methods' effectiveness against both perturbed and unperturbed data."], "limitations": "", "keywords": ["hate speech detection", "phonetic substitution", "Korean language", "adversarial behavior", "machine learning"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2505.21389", "pdf": "https://arxiv.org/pdf/2505.21389.pdf", "abs": "https://arxiv.org/abs/2505.21389", "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "authors": ["Xuanwen Ding", "Chengjun Pan", "Zejun Li", "Jiwen Zhang", "Siyuan Wang", "Zhongyu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench.", "AI": {"tldr": "AutoJudger is an agent-driven framework designed for efficient and adaptive benchmarking of multimodal large language models (MLLMs), significantly reducing evaluation costs while maintaining high accuracy.", "motivation": "Evaluating MLLMs is becoming increasingly expensive due to the complexity and size of benchmarks.", "method": "AutoJudger employs Item Response Theory (IRT) to estimate question difficulty and uses an autonomous evaluation agent to select informative test questions based on real-time model performance, along with a semantic-aware retrieval mechanism and a dynamic memory for question selection.", "result": "AutoJudger dramatically reduces evaluation expenses, using only 4% of the data to achieve over 90% ranking accuracy compared to full benchmark evaluations.", "conclusion": "The proposed framework provides a cost-effective solution for efficiently evaluating MLLMs while maintaining high accuracy in assessments.", "key_contributions": ["Introduction of AutoJudger for adaptive benchmarking of MLLMs", "Use of Item Response Theory for difficulty estimation", "Innovative retrieval mechanism and dynamic memory for question selection"], "limitations": "", "keywords": ["multimodal large language models", "benchmarking", "Item Response Theory"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21396", "pdf": "https://arxiv.org/pdf/2505.21396.pdf", "abs": "https://arxiv.org/abs/2505.21396", "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings.", "AI": {"tldr": "This paper investigates how enhancing LLMs with data can improve the feasibility and quality of generated research ideas, specifically in social sciences.", "motivation": "The need to improve the feasibility and effectiveness of research ideas generated by LLMs.", "method": "The paper introduces two methods: 1) metadata to guide LLMs during idea generation and 2) automatic validation for assessing the plausibility of ideas.", "result": "Experiments show that metadata improves idea feasibility by 20% and automatic validation enhances selected idea quality by 7%.", "conclusion": "The integration of data into LLM-assisted ideation significantly boosts the quality of research ideas in practical academic contexts.", "key_contributions": ["Demonstrated that metadata enhances idea feasibility in LLM outputs.", "Showed that automatic validation improves the quality assessment of LLM-generated ideas.", "Provided empirical evidence supporting data-driven idea generation in research settings."], "limitations": "Experiments were conducted in a specific domain (climate negotiation) which may limit generalizability.", "keywords": ["large language models", "idea generation", "metadata", "automatic validation", "social sciences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21397", "pdf": "https://arxiv.org/pdf/2505.21397.pdf", "abs": "https://arxiv.org/abs/2505.21397", "title": "DecisionFlow: Advancing Large Language Model as Principled Decision Maker", "authors": ["Xiusi Chen", "Shanyong Wang", "Cheng Qian", "Hongru Wang", "Peixuan Han", "Heng Ji"], "categories": ["cs.CL"], "comment": "24 pages, 13 figures", "summary": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow.", "AI": {"tldr": "DecisionFlow is a new framework that enhances decision-making in high-stakes domains by promoting structured reasoning and explainability in language models.", "motivation": "Effective decision-making in healthcare and finance requires both accurate outcomes and transparent reasoning, which current language models often lack.", "method": "DecisionFlow guides models to reason over structured representations, building a semantically grounded decision space and inferring a latent utility function to evaluate trade-offs.", "result": "DecisionFlow achieves up to 30% accuracy gains and improves alignment in outcomes on two high-stakes benchmarks compared to strong prompting baselines.", "conclusion": "This work integrates symbolic reasoning with LLMs, aiming for more accountable, explainable, and reliable LLM decision support systems.", "key_contributions": ["Introduction of the DecisionFlow framework for structured decision-making", "Empirical results demonstrating significant accuracy improvements over existing methods", "Enhanced interpretability of decisions through grounded rationales"], "limitations": "", "keywords": ["decision-making", "explainable AI", "language models", "structured reasoning", "symbolic reasoning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.21399", "pdf": "https://arxiv.org/pdf/2505.21399.pdf", "abs": "https://arxiv.org/abs/2505.21399", "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling", "authors": ["Hovhannes Tamoyan", "Subhabrata Dutta", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability.", "AI": {"tldr": "This paper investigates the internal mechanisms of large language models (LLMs) that enable them to assess the factual correctness of their generated content, suggesting the existence of self-monitoring capabilities within these models.", "motivation": "Factual incorrectness in generated content from LLMs is a significant concern, necessitating a better understanding of how these models assess correctness during generation.", "method": "The authors examine the linear features encoded in the Transformerâs residual stream of LLMs at the time of generation, which indicate the model's ability to recall correct attributes related to entity-relation-attribute triplets. They explore the impact of different contexts and select various examples for experiments across model sizes.", "result": "The study provides evidence that LLMs possess an internal compass for factual recall, with self-awareness capabilities emerging quickly during training and peaking in intermediate layers.", "conclusion": "These findings enhance the interpretability and reliability of LLMs by uncovering their intrinsic self-monitoring capabilities.", "key_contributions": ["Demonstration of LLMs' internal compass for factual correctness during content generation", "Identification of linear features in the Transformer's residual stream associated with correct recall", "Insights into self-awareness emergence and training dynamics of LLMs."], "limitations": "Further exploration is needed on how this self-monitoring ability can be generalized across various domains and types of content generation.", "keywords": ["Large Language Models", "Factual Correctness", "Self-Monitoring", "Interpretability", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.21409", "pdf": "https://arxiv.org/pdf/2505.21409.pdf", "abs": "https://arxiv.org/abs/2505.21409", "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "authors": ["Dario Satriani", "Enzo Veltri", "Donatello Santoro", "Paolo Papotti"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.", "AI": {"tldr": "The paper introduces RelationalFactQA, a benchmark for evaluating the ability of Large Language Models (LLMs) to generate structured, multi-record tabular outputs from factual knowledge, highlighting their limitations in this area.", "motivation": "To address the challenge of factuality in LLMs, especially their ability to generate structured outputs rather than just short factual answers, which current benchmarks often overlook.", "method": "Introduces the RelationalFactQA benchmark featuring diverse natural language questions paired with SQL and gold-standard tabular answers to evaluate relational fact retrieval capabilities.", "result": "Even state-of-the-art LLMs struggle to achieve over 25% factual accuracy in generating relational outputs, with performance decreasing as the dimensionality of output increases.", "conclusion": "The findings reveal critical limitations in current LLMs' ability to synthesize structured factual knowledge and position RelationalFactQA as an essential tool for future research in this area.", "key_contributions": ["Development of the RelationalFactQA benchmark", "Identification of challenges faced by LLMs in generating structured tabular outputs", "Provision of insights into the limitations of current LLMs' factual accuracy"], "limitations": "The paper does not explore the potential for improving LLM performance on relational output generation.", "keywords": ["Large Language Models", "factuality", "RelationalFactQA", "tabular outputs", "knowledge retrieval"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21411", "pdf": "https://arxiv.org/pdf/2505.21411.pdf", "abs": "https://arxiv.org/abs/2505.21411", "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity", "authors": ["Yehui Tang", "Xiaosong Li", "Fangcheng Liu", "Wei Guo", "Hang Zhou", "Yaoyuan Wang", "Kai Han", "Xianzhi Yu", "Jinpeng Li", "Hui Zang", "Fei Mi", "Xiaojun Meng", "Zhicheng Liu", "Hanting Chen", "Binfan Zheng", "Can Chen", "Youliang Yan", "Ruiming Tang", "Peifeng Qin", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.", "AI": {"tldr": "Introduces Mixture of Grouped Experts (MoGE) for improved load balancing in large language models, enhancing throughput especially during inference.", "motivation": "To address inefficiencies in Mixture of Experts (MoE) due to unequal activation of experts, leading to computational load imbalances in a parallel processing environment.", "method": "Developed MoGE that groups experts and constrains token activation to balance expert workloads, implemented on Pangu Pro MoE optimized for Ascend NPUs.", "result": "Pangu Pro MoE shows enhanced expert load balancing and improved execution efficiency, achieving up to 1528 tokens/s per card during inference, outperforming existing models.", "conclusion": "MoGE enhances the performance and efficiency of large language models by ensuring balanced computational loads across devices, making it a leading approach for training large models.", "key_contributions": ["Introduction of Mixture of Grouped Experts (MoGE) for load balancing", "Development and optimization of Pangu Pro MoE on Ascend NPUs", "Significantly improved inference performance compared to existing dense models."], "limitations": "", "keywords": ["Mixture of Experts", "load balancing", "large language models", "Ascend NPUs", "inference performance"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2505.21413", "pdf": "https://arxiv.org/pdf/2505.21413.pdf", "abs": "https://arxiv.org/abs/2505.21413", "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation", "authors": ["Xiao Liu", "Da Yin", "Zirui Wu", "Yansong Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "Code is available at https://github.com/xxxiaol/RefTool", "summary": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning.", "AI": {"tldr": "RefTool is a framework that enhances LLMs' problem-solving abilities by generating tools from structured external materials, ensuring better tool creation and utilization.", "motivation": "Address limitations of LLMs in generating tools for complex problem-solving tasks when predefined tools are unavailable.", "method": "RefTool uses a two-module system: tool creation generates executable tools from reference content, and tool utilization involves navigating a toolbox to apply appropriate tools for problem-solving.", "result": "RefTool outperforms existing methods by an average accuracy of 11.3% in causality, physics, and chemistry benchmarks while being cost-efficient.", "conclusion": "Grounding tool creation in structured external references allows LLMs to surpass their knowledge limitations and enhances generalizable reasoning.", "key_contributions": ["Introduction of RefTool framework for automatic tool creation using external references", "Demonstration of improved accuracy and generalizability in problem-solving", "Utilization of a hierarchical toolbox structure for effective tool selection"], "limitations": "", "keywords": ["large language models", "tool creation", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21439", "pdf": "https://arxiv.org/pdf/2505.21439.pdf", "abs": "https://arxiv.org/abs/2505.21439", "title": "Towards Better Instruction Following Retrieval Models", "authors": ["Yuchen Zhuang", "Aaron Trinh", "Rushi Qiang", "Haotian Sun", "Chao Zhang", "Hanjun Dai", "Bo Dai"], "categories": ["cs.CL", "cs.IR"], "comment": "Retrieval Models, Embedding, Retrieval with Instructions", "summary": "Modern information retrieval (IR) models, trained exclusively on standard\n<query, passage> pairs, struggle to effectively interpret and follow explicit\nuser instructions. We introduce InF-IR, a large-scale, high-quality training\ncorpus tailored for enhancing retrieval models in Instruction-Following IR.\nInF-IR expands traditional training pairs into over 38,000 expressive\n<instruction, query, passage> triplets as positive samples. In particular, for\neach positive triplet, we generate two additional hard negative examples by\npoisoning both instructions and queries, then rigorously validated by an\nadvanced reasoning model (o3-mini) to ensure semantic plausibility while\nmaintaining instructional incorrectness. Unlike existing corpora that primarily\nsupport computationally intensive reranking tasks for decoder-only language\nmodels, the highly contrastive positive-negative triplets in InF-IR further\nenable efficient representation learning for smaller encoder-only models,\nfacilitating direct embedding-based retrieval. Using this corpus, we train\nInF-Embed, an instruction-aware Embedding model optimized through contrastive\nlearning and instruction-query attention mechanisms to align retrieval outcomes\nprecisely with user intents. Extensive experiments across five\ninstruction-based retrieval benchmarks demonstrate that InF-Embed significantly\nsurpasses competitive baselines by 8.1% in p-MRR, measuring the\ninstruction-following capabilities.", "AI": {"tldr": "Introducing InF-IR, a new training corpus for enhancing instruction-following in information retrieval systems.", "motivation": "Modern IR models struggle to interpret and follow explicit user instructions effectively, necessitating improved training corpora.", "method": "The paper presents InF-IR, a dataset of over 38,000 <instruction, query, passage> triplets along with hard negative examples generated by poisoning inputs, aimed at training an embedding model called InF-Embed.", "result": "InF-Embed is trained using contrastive learning and instruction-query attention mechanisms, achieving an 8.1% improvement in p-MRR over competitive baselines across five instruction-based retrieval benchmarks.", "conclusion": "The InF-IR corpus allows for more effective instruction-following capabilities in retrieval models, enhancing both efficiency and accuracy of information retrieval.", "key_contributions": ["Development of the InF-IR dataset tailored for instruction-following IR", "Introduction of hard negative examples to improve model robustness", "Training of InF-Embed that aligns retrieval outcomes with user intents effectively."], "limitations": "", "keywords": ["Retrieval Models", "Embedding", "Instruction-Following IR"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21451", "pdf": "https://arxiv.org/pdf/2505.21451.pdf", "abs": "https://arxiv.org/abs/2505.21451", "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication", "authors": ["Jocelyn Shen", "Akhila Yerukola", "Xuhui Zhou", "Cynthia Breazeal", "Maarten Sap", "Hae Won Park"], "categories": ["cs.CL"], "comment": null, "summary": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection.", "AI": {"tldr": "This paper studies conversational breakdowns in close relationships, using nonviolent communication theory and a new dataset to evaluate LLM performance in detecting conflict influenced by personal backstories.", "motivation": "The motivation is to address the gap in NLP research regarding how personal histories and emotional context shape perceptions of conversational breakdowns in close relationships.", "method": "The authors created the PersonaConflicts Corpus, comprising 5,772 simulated dialogues reflecting various conflict situations. A controlled human study was conducted for fine-grained labeling of communication breakdowns, assessing the influence of relationship backstories on conflict perception.", "result": "The study found that relationship backstories significantly affect human perceptions of communication breakdowns and social partner impressions, while models struggled to utilize backstories effectively and tended to overestimate positive listener reactions to messages.", "conclusion": "The findings highlight the importance of personalizing LLM interactions based on relationship contexts to improve their effectiveness as mediators in human communication.", "key_contributions": ["Introduction of the PersonaConflicts Corpus for studying conversational conflicts", "Evidence that relationship backstories influence human perceptions of breakdowns", "Insights into LLM limitations in leveraging relationship context in conflict detection"], "limitations": "The dataset is simulated and may not fully capture real-world conversational dynamics.", "keywords": ["Conversational breakdowns", "Nonviolent communication", "Large language models", "Human-computer interaction", "Conflict detection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21458", "pdf": "https://arxiv.org/pdf/2505.21458.pdf", "abs": "https://arxiv.org/abs/2505.21458", "title": "Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance", "authors": ["Shintaro Ozaki", "Tatsuya Hiraoka", "Hiroto Otake", "Hiroki Ouchi", "Masaru Isonuma", "Benjamin Heinzerling", "Kentaro Inui", "Taro Watanabe", "Yusuke Miyao", "Yohei Oseki", "Yu Takagi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance.", "AI": {"tldr": "This study investigates the effect of latent language consistency on downstream task performance in Large Language Models (LLMs), revealing that while consistency is hypothesized to enhance performance, it may not always be necessary across diverse tasks.", "motivation": "To explore how the discrepancy between latent language and input/output languages affects the performance of LLMs in downstream tasks, which has been largely unexamined in existing literature.", "method": "The research varies input prompt languages across multiple downstream tasks, analyzing the correlation between latent language consistency and task performance. Datasets include diverse questions from areas such as translation and geo-culture.", "result": "Experimental results on multiple LLMs indicate that consistency in latent language does not always lead to optimal downstream task performance, as the models can adapt their internal representations to suit the target language.", "conclusion": "Maintaining consistency in latent language is not necessary for optimal performance on tasks sensitive to language choice, due to model adaptability at the final layers.", "key_contributions": ["Investigates the impact of latent language consistency on task performance in LLMs.", "Presents experimental results across diverse domains, showing adaptability of models.", "Challenges the assumption that consistency in latent language always enhances performance."], "limitations": "The analysis primarily focuses on specific tasks (translation and geo-culture) and may not generalize to all LLM applications.", "keywords": ["Large Language Models", "latent language", "task performance", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.21467", "pdf": "https://arxiv.org/pdf/2505.21467.pdf", "abs": "https://arxiv.org/abs/2505.21467", "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "authors": ["Zhanqiu Hu", "Jian Meng", "Yash Akhauri", "Mohamed S. Abdelfattah", "Jae-sun Seo", "Zhiru Zhang", "Udit Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.", "AI": {"tldr": "This paper presents methods to enhance the efficiency of diffusion language models by introducing FreeCache and Guided Diffusion, achieving significantly faster inference while maintaining accuracy.", "motivation": "To overcome the slow inference times and high computational costs of current state-of-the-art diffusion language models compared to autoregressive approaches.", "method": "Two training-free techniques are proposed: FreeCache for Key-Value approximation caching to reuse KV projections across denoising steps, and Guided Diffusion which utilizes a pretrained autoregressive model to supervise token unmasking.", "result": "The proposed methods result in up to a 34x end-to-end speedup in inference times without compromising accuracy, enabling diffusion language models to achieve competitive latency with autoregressive models.", "conclusion": "The introduction of FreeCache and Guided Diffusion has the potential to expand the applicability of diffusion language models across various domains due to improved performance.", "key_contributions": ["Introduction of FreeCache for efficient KV caching during denoising steps.", "Development of Guided Diffusion to reduce denoising iterations using an autoregressive model.", "Demonstration of a 34x speedup in inference times for diffusion language models."], "limitations": "", "keywords": ["diffusion language models", "autoregressive models", "inference optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21471", "pdf": "https://arxiv.org/pdf/2505.21471.pdf", "abs": "https://arxiv.org/abs/2505.21471", "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration", "authors": ["Zijun Liu", "Zhennan Wan", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "categories": ["cs.CL"], "comment": "30 pages, 9 figures. Code and data are available at\n  https://github.com/THUNLP-MT/ExtAgents", "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.", "AI": {"tldr": "This paper presents ExtAgents, a multi-agent framework that improves scalability in LLM inference-time knowledge integration without requiring longer-context training.", "motivation": "The limited context window of LLMs hinders their ability to utilize external knowledge effectively, which is crucial for complex tasks that require significant information.", "method": "The authors developed a multi-agent framework that addresses knowledge synchronization and reasoning bottlenecks in existing methods, enabling better handling of external knowledge inputs.", "result": "ExtAgents significantly outperforms existing methods on benchmark tests, including enhanced multi-hop question answering, while maintaining high efficiency through parallelism.", "conclusion": "The study suggests that improving coordination among LLM agents to manage larger inputs could have valuable real-world applications.", "key_contributions": ["Development of a multi-agent framework (ExtAgents) for LLMs.", "Enhanced performance in multi-hop question answering tasks over existing non-training methods.", "High efficiency through parallelism allowing better scalability."], "limitations": "Information loss in existing context window extension methods; the framework's effectiveness is tested in specific scenarios and may require further exploration in real-world applications.", "keywords": ["multi-agent framework", "large language models", "knowledge integration", "information retrieval", "scalability"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.21479", "pdf": "https://arxiv.org/pdf/2505.21479.pdf", "abs": "https://arxiv.org/abs/2505.21479", "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "authors": ["Keenan Samway", "Max Kleiman-Weiner", "David Guzman Piedrahita", "Rada Mihalcea", "Bernhard SchÃ¶lkopf", "Zhijing Jin"], "categories": ["cs.CL"], "comment": null, "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "AI": {"tldr": "This study analyzes moral reasoning in large language models (LLMs) using over 600 trolley problems, revealing a preference for deontological principles in reasoning traces, while explanations tend to align with consequentialist rationales.", "motivation": "To understand how AI systems handle ethically complex scenarios, particularly in high-stakes applications like healthcare and governance, and to address the gap in analyzing the moral reasoning process of LLMs.", "method": "A large-scale analysis of the reasoning traces provided by LLMs based on over 600 distinct trolley problems, using a taxonomy of moral rationales tied to consequentialism and deontology.", "result": "The analysis shows that LLM chains-of-thought lean toward deontological principles focused on moral obligations, whereas post-hoc explanations shift to consequentialist rationales emphasizing utility.", "conclusion": "Understanding how LLMs articulate ethical considerations is crucial for their safe deployment in decision-making contexts, and the developed framework serves as a foundation for this understanding.", "key_contributions": ["Introduced a taxonomy of moral rationales for analyzing LLM reasoning.", "Conducted large-scale analysis using over 600 trolley problems.", "Demonstrated differing reasoning patterns between chains-of-thought and explanations in LLMs."], "limitations": "", "keywords": ["Large Language Models", "Moral Reasoning", "Ethics", "Human-Computer Interaction", "Consequentialism"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21496", "pdf": "https://arxiv.org/pdf/2505.21496.pdf", "abs": "https://arxiv.org/abs/2505.21496", "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents", "authors": ["Han Xiao", "Guozhi Wang", "Yuxiang Chai", "Zimu Lu", "Weifeng Lin", "Hao He", "Lue Fan", "Liuyang Bian", "Rui Hu", "Liang Liu", "Shuai Ren", "Yafei Wen", "Xiaoxin Chen", "Aojun Zhou", "Hongsheng Li"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "https://github.com/Euphoria16/UI-Genie", "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.", "AI": {"tldr": "Introducing UI-Genie, a self-improving framework for GUI agents that addresses challenges in trajectory verification and scalable training data through a novel reward model and data generation strategies.", "motivation": "To tackle challenges related to the verification of trajectory outcomes and the scalability of high-quality training data for GUI agents.", "method": "The authors developed a reward model called UI-Genie-RM utilizing an image-text interleaved architecture, along with self-improvement pipelines to enhance agent performance through reward-guided exploration and outcome verification.", "result": "UI-Genie demonstrates state-of-the-art performance across multiple benchmarks for GUI agents, validated through a series of experiments and the creation of a unique dataset (UI-Genie-RM-517k) for training.", "conclusion": "The complete framework and generated datasets have been open-sourced to promote further research in the field of GUI agents.", "key_contributions": ["Introduction of a novel UI-Genie framework for self-improving GUI agents", "Development of UI-Genie-RM, a reward model with an innovative architecture", "Creation of the first reward-specific dataset for GUI agents"], "limitations": "", "keywords": ["GUI agents", "self-improvement", "reward model"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.21503", "pdf": "https://arxiv.org/pdf/2505.21503.pdf", "abs": "https://arxiv.org/abs/2505.21503", "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.", "AI": {"tldr": "The paper introduces the Catfish Agent, a role-specialized LLM aimed at addressing the issue of Silent Agreement in multi-agent clinical diagnostic systems, enhancing reasoning through structured dissent.", "motivation": "To tackle the issue of Silent Agreement in collaborative diagnostic frameworks using LLMs, which can hinder critical analysis and accurate diagnosis.", "method": "The Catfish Agent employs two mechanisms: a complexity-aware intervention that adjusts agent engagement based on case difficulty, and a tone-calibrated intervention that balances critique with collaboration.", "result": "Evaluations show that the Catfish Agent consistently outperforms existing single and multi-agent LLM frameworks, including advanced models like GPT-4o and DeepSeek-R1 across multiple medical benchmarks.", "conclusion": "The Catfish Agent enhances diagnostic accuracy by fostering deeper reasoning through structured dissent, improving the collaborative process in clinical settings.", "key_contributions": ["Introduction of the Catfish Agent concept for structured dissent in LLM frameworks.", "Development of complexity-aware and tone-calibrated intervention mechanisms.", "Demonstrated improved performance in clinical question answering benchmarks."], "limitations": "", "keywords": ["large language models", "clinical reasoning", "collaborative systems", "Catfish Agent", "Silent Agreement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.21505", "pdf": "https://arxiv.org/pdf/2505.21505.pdf", "abs": "https://arxiv.org/abs/2505.21505", "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "authors": ["Shimao Zhang", "Zhejian Lai", "Xiang Liu", "Shuaijie She", "Xiao Liu", "Yeyun Gong", "Shujian Huang", "Jiajun Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.", "AI": {"tldr": "This paper proposes a new algorithm for identifying language-specific and language-agnostic neurons in LLMs, enhancing understanding of multilingual capabilities and alignment.", "motivation": "To enhance multilingual capabilities of LLMs by analyzing language-specific neurons and improving alignment from high-resource to low-resource languages.", "method": "A new finer-grained neuron identification algorithm is introduced to detect language-specific and agnostic neurons, followed by an analysis of LLM processes during multilingual inference.", "result": "Empirical results provide insights into how different neuron types function in multilingual contexts and the phenomenon of spontaneous multilingual alignment.", "conclusion": "The investigation offers valuable insights into multilingual alignment and capabilities of LLMs, contributing to a deeper understanding of their mechanisms.", "key_contributions": ["Introduction of a novel neuron identification algorithm for LLMs", "Detailed analysis of multilingual inference processes", "Insights into spontaneous multilingual alignment phenomenon."], "limitations": "", "keywords": ["Multilingual Alignment", "LLMs", "language-specific neurons", "language-agnostic neurons", "neuron identification"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20464", "pdf": "https://arxiv.org/pdf/2505.20464.pdf", "abs": "https://arxiv.org/abs/2505.20464", "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions", "authors": ["Samuel Rhys Cox", "Rune MÃ¸berg Jacobsen", "Niels van Berkel"], "categories": ["cs.HC", "cs.CL"], "comment": "In ACM Conversational User Interfaces (CUI '25), July 8-10, 2025; 18\n  pages; 6 Figures; 6 Tables", "summary": "Self-disclosure, the sharing of one's thoughts and feelings, is affected by\nthe perceived relationship between individuals. While chatbots are increasingly\nused for self-disclosure, the impact of a chatbot's framing on users'\nself-disclosure remains under-explored. We investigated how a chatbot's\ndescription of its relationship with users, particularly in terms of\nephemerality, affects self-disclosure. Specifically, we compared a Familiar\nchatbot, presenting itself as a companion remembering past interactions, with a\nStranger chatbot, presenting itself as a new, unacquainted entity in each\nconversation. In a mixed factorial design, participants engaged with either the\nFamiliar or Stranger chatbot in two sessions across two days, with one\nconversation focusing on Emotional- and another Factual-disclosure. When\nEmotional-disclosure was sought in the first chatting session,\nStranger-condition participants felt more comfortable self-disclosing. However,\nwhen Factual-disclosure was sought first, these differences were replaced by\nmore enjoyment among Familiar-condition participants. Qualitative findings\nshowed Stranger afforded anonymity and reduced judgement, whereas Familiar\nsometimes felt intrusive unless rapport was built via low-risk\nFactual-disclosure.", "AI": {"tldr": "This study examines how a chatbot's framing, as either Familiar or Stranger, influences users' self-disclosure during interactions, finding that the type of disclosure sought affects user comfort and enjoyment levels.", "motivation": "The study aims to explore the relationship between chatbot framing and users' self-disclosure behavior, particularly focusing on the perceived relationship and its impact on users' willingness to share personal thoughts and feelings.", "method": "Participants interacted with either a Familiar chatbot (recalling past interactions) or a Stranger chatbot (with no prior connection) across two sessions, one focusing on Emotional-disclosure and the other on Factual-disclosure, in a mixed factorial design.", "result": "Stranger-condition participants felt more comfortable self-disclosing when Emotional-disclosure was sought first, but Familiar-condition participants reported more enjoyment when Factual-disclosure was prioritized.", "conclusion": "The findings indicate that chatbot framing significantly affects self-disclosure, with anonymity provided by Stranger chatbots benefiting emotional sharing, while Familiar chatbots can enhance enjoyment in factual discussions if rapport is established first.", "key_contributions": ["Investigates the effect of chatbot relationship framing on self-disclosure.", "Identifies differences in comfort and disclosure based on the type of interactions with chatbots.", "Provides insights into how familiarity impacts user experience and emotional engagement."], "limitations": "The study is limited to interactions with fictional chatbots and may not generalize to real-world contexts.", "keywords": ["Self-disclosure", "Chatbots", "Human-Computer Interaction", "Emotional disclosure", "Factual disclosure"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.20692", "pdf": "https://arxiv.org/pdf/2505.20692.pdf", "abs": "https://arxiv.org/abs/2505.20692", "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity.", "AI": {"tldr": "This paper presents a bias detection rubric and Social Stereotype Index (SSI) to evaluate and mitigate societal biases in text-to-image (T2I) models, finding significant improvements through LLM-supported prompt refinement.", "motivation": "To address the ethical concerns arising from societal stereotypes in generative AI, particularly in text-to-image outputs.", "method": "The study involved auditing outputs from three T2I models using 100 queries and applying a bias detection rubric alongside targeted prompt refinement with LLMs.", "result": "The application of the bias detection rubric led to a significant reduction in bias as measured by the SSI: 61% for geocultural, 69% for occupational, and 51% for adjectival queries.", "conclusion": "The need for T2I systems to balance ethical debiasing with contextual relevance was emphasized, advocating for global diversity without sacrificing social complexity.", "key_contributions": ["Development of a theory-driven bias detection rubric", "Creation of the Social Stereotype Index (SSI) for evaluating biases", "Identifying the tension between bias mitigation and contextual alignment in AI-generated imagery."], "limitations": "The study is limited to three T2I models and their outputs, potentially not generalizable to all T2I systems.", "keywords": ["text-to-image generation", "bias detection", "social stereotypes", "LLM-supported refinement", "AI ethics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.21116", "pdf": "https://arxiv.org/pdf/2505.21116.pdf", "abs": "https://arxiv.org/abs/2505.21116", "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS.", "AI": {"tldr": "This paper surveys creativity in large language model-driven multi-agent systems, focusing on idea generation, evaluation methods, and agent persona design.", "motivation": "To explore the role of creativity in multi-agent systems (MAS) and how it influences collaboration and output generation between humans and AIs.", "method": "The survey categorizes agent proactivity and persona design, details generation techniques, and identifies key challenges in creativity-focused MAS.", "result": "The survey presents a new taxonomy of creativity in MAS, including divergent exploration and collaborative synthesis techniques, while discussing significant challenges like evaluation standards and bias mitigation.", "conclusion": "The paper provides a structured framework aimed at advancing the development, evaluation, and standardization of creativity in MAS, paving the way for improved and standardized creative workflows.", "key_contributions": ["Taxonomy of agent proactivity and persona design", "Overview of generation techniques and relevant datasets", "Discussion of challenges in MAS creativity"], "limitations": "Existing inconsistent evaluation standards and a lack of unified benchmarks.", "keywords": ["multi-agent systems", "creativity", "large language models", "evaluation metrics", "collaborative generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2304.12244", "pdf": "https://arxiv.org/pdf/2304.12244.pdf", "abs": "https://arxiv.org/abs/2304.12244", "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions", "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Qingwei Lin", "Daxin Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "large language model, instruction fine-tune", "summary": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM", "AI": {"tldr": "This paper presents Evol-Instruct, a method for automatically generating instruction data using LLMs, which is then used to fine-tune the LLaMA model resulting in WizardLM, outperforming human-generated instructions.", "motivation": "The challenge of manually creating open-domain instruction data for LLMs and the complexity of human-generated instructions necessitates an automated approach.", "method": "The authors propose Evol-Instruct, which iteratively rewrites an initial set of simple instructions into more complex ones. This is used to generate a diverse instruction dataset for fine-tuning LLaMA to create WizardLM.", "result": "WizardLM is shown to produce superior outputs compared to human-created instructions in human evaluations, and it performs at over 90% of ChatGPT's capacity in automatic evaluations on various skills.", "conclusion": "The findings indicate that fine-tuning LLMs with AI-generated evolving instructions is a promising strategy for improving model performance, despite some gaps compared to existing models like ChatGPT.", "key_contributions": ["Introduction of Evol-Instruct for generating complex instructions using LLMs", "Development of WizardLM that outperforms human-generated instructions", "Public release of the code and data to facilitate further research."], "limitations": "WizardLM still underperforms compared to ChatGPT in certain aspects, indicating room for improvement.", "keywords": ["large language model", "instruction fine-tune", "Evol-Instruct", "WizardLM", "auto-generated instructions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2306.08568", "pdf": "https://arxiv.org/pdf/2306.08568.pdf", "abs": "https://arxiv.org/abs/2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "Large Language model, Code Generation, Code LLMs.This paper has been\n  accepted to ICLR 2024. Please cite the ICLR version", "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM", "AI": {"tldr": "Introduction of WizardCoder, a Code LLM with instruction fine-tuning that outperforms existing models in code generation tasks.", "motivation": "To improve the performance of Code LLMs by introducing instruction fine-tuning based on the Evol-Instruct method for better handling of code-related tasks.", "method": "WizardCoder implements complex instruction fine-tuning on Code LLMs and is evaluated on prominent benchmarks including HumanEval, HumanEval+, MBPP, and DS-1000.", "result": "WizardCoder surpasses all existing open-source Code LLMs and even outperforms large closed LLMs like Claude and Bard on specific benchmarks.", "conclusion": "The model demonstrates superior capabilities in code generation and offers public access to its code, model weights, and data.", "key_contributions": ["Introduction of instruction fine-tuning for Code LLMs", "Significant performance improvements on multiple code generation benchmarks", "Public availability of model resources"], "limitations": "", "keywords": ["Large Language model", "Code Generation", "Instruction fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2401.16332", "pdf": "https://arxiv.org/pdf/2401.16332.pdf", "abs": "https://arxiv.org/abs/2401.16332", "title": "Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods", "authors": ["Yotam Wolf", "Noam Wies", "Dorin Shteyman", "Binyamin Rothberg", "Yoav Levine", "Amnon Shashua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment.", "AI": {"tldr": "This paper examines the tradeoff between alignment and helpfulness in language models using representation engineering, proposing a theoretical framework and empirical validation.", "motivation": "To understand the balance between improving human-model alignment and maintaining the model's helpfulness through representation engineering.", "method": "A theoretical framework is proposed to analyze the tradeoff between alignment and helpfulness, complemented by empirical validation of these theories.", "result": "The study empirically validates that alignment increases linearly while helpfulness decreases quadratically with the norm of the representation engineering vector.", "conclusion": "Representation engineering can enhance alignment in LLMs but at the cost of decreasing helpfulness, which can be quantified using the proposed framework.", "key_contributions": ["Theoretical framework demonstrating tradeoff between alignment and helpfulness.", "Empirical validation of alignment increase and helpfulness decrease with representation engineering.", "Identification of a regime for efficient use of representation engineering."], "limitations": "The study does not explore long-term effects of representation engineering on model performance across diverse tasks.", "keywords": ["alignment", "representation engineering", "language models", "helpfulness", "AI safety"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2402.14558", "pdf": "https://arxiv.org/pdf/2402.14558.pdf", "abs": "https://arxiv.org/abs/2402.14558", "title": "LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey", "authors": ["Ashok Urlana", "Charaka Vinayak Kumar", "Ajeet Kumar Singh", "Bala Mallikarjunarao Garlapati", "Srinivasa Rao Chalamala", "Rahul Mishra"], "categories": ["cs.CL"], "comment": "25 pages, 7 figures", "summary": "Large language models (LLMs) have become the secret ingredient driving\nnumerous industrial applications, showcasing their remarkable versatility\nacross a diverse spectrum of tasks. From natural language processing and\nsentiment analysis to content generation and personalized recommendations,\ntheir unparalleled adaptability has facilitated widespread adoption across\nindustries. This transformative shift driven by LLMs underscores the need to\nexplore the underlying associated challenges and avenues for enhancement in\ntheir utilization. In this paper, our objective is to unravel and evaluate the\nobstacles and opportunities inherent in leveraging LLMs within an industrial\ncontext. To this end, we conduct a survey involving a group of industry\npractitioners, develop four research questions derived from the insights\ngathered, and examine 68 industry papers to address these questions and derive\nmeaningful conclusions. We maintain the Github repository with the most recent\npapers in the field.", "AI": {"tldr": "The paper evaluates the challenges and opportunities of using large language models (LLMs) in industrial applications through a survey of industry practitioners and an analysis of 68 relevant papers.", "motivation": "To explore the obstacles and opportunities in leveraging LLMs within industrial contexts, given their widespread adoption and transformative potential.", "method": "A survey involving industry practitioners was conducted along with a review of 68 industry papers to derive insights and develop research questions related to LLM usage in industry.", "result": "Identified key challenges and opportunities in employing LLMs across various industrial applications, providing valuable conclusions for practitioners.", "conclusion": "The study offers actionable insights into improving LLM utilization in industry, encouraging further research and development.", "key_contributions": ["Survey of industry practitioners to uncover real-world challenges with LLMs", "Review of 68 industry papers to inform research questions on LLM implementation", "Establishment of a Github repository for ongoing research on LLMs in industry"], "limitations": "The study may be limited by the specific industries involved in the survey and the scope of reviewed papers.", "keywords": ["Large Language Models", "Industrial Applications", "Challenges and Opportunities"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2403.04963", "pdf": "https://arxiv.org/pdf/2403.04963.pdf", "abs": "https://arxiv.org/abs/2403.04963", "title": "An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment", "authors": ["Xuanxin Wu", "Yuki Arase"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACM Transactions on Intelligent Systems and Technology,\n  to appear", "summary": "Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that GPT-4\ngenerally generates fewer erroneous simplification outputs compared to the\ncurrent state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs.", "AI": {"tldr": "This paper evaluates the simplification abilities of large language models (LLMs) using a new error-based human annotation framework, revealing strengths and weaknesses in their performance.", "motivation": "There is uncertainty regarding the effectiveness of existing evaluation methodologies for LLMs' simplification tasks, necessitating a deeper analysis.", "method": "The authors designed an error-based human annotation framework for assessing LLM simplification capabilities. They evaluated both closed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and Llama-3.2-3B.", "result": "The study found that GPT-4 produces fewer erroneous outputs in simplifications compared to state-of-the-art models, though limitations were observed in lexical paraphrasing performance across models like GPT-4 and Qwen2.5-72B.", "conclusion": "Current automatic metrics lack sensitivity in evaluating high-quality simplifications produced by LLMs. The paper suggests that human evaluations could be more reliable with the proposed framework.", "key_contributions": ["Proposed an error-based human annotation framework for evaluating LLM simplifications.", "Showed that GPT-4 outperforms other models in fewer erroneous outputs for simplification tasks.", "Conducted meta-evaluations revealing the limitations of existing automatic metrics in simplification quality assessment."], "limitations": "Existing automatic metrics are insufficiently sensitive for high-quality simplification assessments; the study is limited to a small selection of LLMs.", "keywords": ["Language Models", "Simplification", "Human Annotation", "Evaluation Metrics", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2403.05518", "pdf": "https://arxiv.org/pdf/2403.05518.pdf", "abs": "https://arxiv.org/abs/2403.05518", "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought", "authors": ["James Chua", "Edward Rees", "Hunar Batra", "Samuel R. Bowman", "Julian Michael", "Ethan Perez", "Miles Turpin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning. But CoT can also systematically\nmisrepresent the factors influencing models' behavior -- for example,\nrationalizing answers in line with a user's opinion.\n  We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo\nand Llama-8b models. These consist of spurious-few-shot patterns, post hoc\nrationalization, and sycophantic settings. Models switch to the answer implied\nby the bias, without mentioning the effect of the bias in the CoT.\n  To mitigate this biased reasoning problem, we introduce bias-augmented\nconsistency training (BCT), an unsupervised fine-tuning scheme that trains\nmodels to give consistent reasoning across prompts with and without biasing\nfeatures. We construct a suite testing nine forms of biased reasoning on seven\nquestion-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one\nbias reduces the rate of biased reasoning by 86\\% on held-out tasks. Moreover,\nthis model generalizes to other forms of bias, reducing biased reasoning on\nheld-out biases by an average of 37\\%. As BCT generalizes to held-out biases\nand does not require gold labels, this method may hold promise for reducing\nbiased reasoning from as-of-yet unknown biases and on tasks where ground truth\nreasoning is unavailable.", "AI": {"tldr": "This paper introduces a method to mitigate biased reasoning in language models using bias-augmented consistency training (BCT).", "motivation": "To address the issue of systematic biases in language model reasoning and improve their explainability.", "method": "A new dataset was created to identify 9 different biases in GPT-3.5-Turbo and Llama-8b models, and BCT was developed for unsupervised fine-tuning to ensure consistent reasoning across various prompts.", "result": "Applying BCT to the models reduced biased reasoning by 86% on held-out tasks and showed a generalization effect, reducing bias by an average of 37% across various bias forms.", "conclusion": "BCT proves to be an effective method for reducing biased reasoning in language models while being adaptable to unknown biases and tasks without gold labels.", "key_contributions": ["Development of a dataset highlighting biases in language models", "Introduction of bias-augmented consistency training (BCT)", "Demonstrated effectiveness of BCT in reducing biased reasoning."], "limitations": "The method may not address all forms of bias and requires further exploration.", "keywords": ["bias-augmented consistency training", "language models", "explainability", "bias", "GPT-3.5-Turbo"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2406.00984", "pdf": "https://arxiv.org/pdf/2406.00984.pdf", "abs": "https://arxiv.org/abs/2406.00984", "title": "Predicting drug-gene relations via analogy tasks with word embeddings", "authors": ["Hiroaki Yamagiwa", "Ryoma Hashimoto", "Kiwamu Arakane", "Ken Murakami", "Shou Soeda", "Momose Oyama", "Yihua Zhu", "Mariko Okada", "Hidetoshi Shimodaira"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language processing (NLP) is utilized in a wide range of fields,\nwhere words in text are typically transformed into feature vectors called\nembeddings. BioConceptVec is a specific example of embeddings tailored for\nbiology, trained on approximately 30 million PubMed abstracts using models such\nas skip-gram. Generally, word embeddings are known to solve analogy tasks\nthrough simple vector arithmetic. For example, subtracting the vector for man\nfrom that of king and then adding the vector for woman yields a point that lies\ncloser to queen in the embedding space. In this study, we demonstrate that\nBioConceptVec embeddings, along with our own embeddings trained on PubMed\nabstracts, contain information about drug-gene relations and can predict target\ngenes from a given drug through analogy computations. We also show that\ncategorizing drugs and genes using biological pathways improves performance.\nFurthermore, we illustrate that vectors derived from known relations in the\npast can predict unknown future relations in datasets divided by year. Despite\nthe simplicity of implementing analogy tasks as vector additions, our approach\ndemonstrated performance comparable to that of large language models such as\nGPT-4 in predicting drug-gene relations.", "AI": {"tldr": "This study demonstrates that BioConceptVec embeddings can effectively predict drug-gene relations through analogy computations and outperform conventional methods, showing performance akin to state-of-the-art models like GPT-4.", "motivation": "To explore the utility of BioConceptVec embeddings, specifically in predicting drug-gene relations within biological datasets.", "method": "The study utilizes analogy computations on BioConceptVec embeddings and their own embeddings trained on PubMed abstracts, applying vector arithmetic to predict target genes from given drugs, and categorizes them using biological pathways.", "result": "The findings indicate that the embeddings can predict target genes accurately and that incorporating biological pathways improves these predictions, achieving results comparable to large language models such as GPT-4.", "conclusion": "The effectiveness of simple vector arithmetic in analogy tasks suggests a viable approach for predicting biological relations, potentially benefiting applications in health informatics.", "key_contributions": ["Demonstrated effectiveness of BioConceptVec for drug-gene relation prediction", "Showcased performance comparable to GPT-4 in a specific biomedical context", "Highlighted the importance of biological pathways in enhancing predictive performance"], "limitations": "", "keywords": ["Natural Language Processing", "Embeddings", "Drug-Gene Relations", "Bioinformatics", "Machine Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.03749", "pdf": "https://arxiv.org/pdf/2406.03749.pdf", "abs": "https://arxiv.org/abs/2406.03749", "title": "NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human", "authors": ["Shuo Huang", "William MacLean", "Xiaoxi Kang", "Qiongkai Xu", "Zhuang Li", "Xingliang Yuan", "Gholamreza Haffari", "Lizhen Qu"], "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of cloud-based Large Language Models (LLMs) has heightened\nconcerns over user privacy, as sensitive information may be inadvertently\nexposed during interactions with these services. To protect privacy before\nsending sensitive data to those models, we suggest sanitizing sensitive text\nusing two common strategies used by humans: i) deleting sensitive expressions,\nand ii) obscuring sensitive details by abstracting them. To explore the issues\nand develop a tool for text rewriting, we curate the first corpus, coined\nNAP^2, through both crowdsourcing and the use of large language models (LLMs).\nCompared to the prior works on anonymization, the human-inspired approaches\nresult in more natural rewrites and offer an improved balance between privacy\nprotection and data utility, as demonstrated by our extensive experiments.\nResearchers interested in accessing the dataset are encouraged to contact the\nfirst or corresponding author via email.", "AI": {"tldr": "The paper explores privacy protection techniques for cloud-based LLM interactions by sanitizing sensitive texts and introduces the NAP^2 dataset for effective text rewriting.", "motivation": "To address privacy concerns related to the inadvertent exposure of sensitive information when using cloud-based LLMs.", "method": "The study utilizes two human-inspired strategies for text sanitization: deleting sensitive expressions and obscuring details through abstraction, and it creates the NAP^2 dataset via crowdsourcing and LLMs.", "result": "The human-inspired methods yield more natural rewrites, improving the balance between privacy protection and data utility compared to prior anonymization techniques.", "conclusion": "The findings highlight the effectiveness of human-inspired sanitization techniques and the potential for a dataset that supports improving privacy in LLM interactions.", "key_contributions": ["Introduction of the NAP^2 dataset for privacy-focused text sanitization", "Development of human-inspired strategies for text rewriting", "Demonstration of improved balance between privacy and utility in text anonymization"], "limitations": "The paper does not discuss the potential challenges in scaling the sanitization methods or their applicability in diverse domains.", "keywords": ["privacy", "text sanitization", "large language models", "dataset", "natural language processing"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2406.14230", "pdf": "https://arxiv.org/pdf/2406.14230.pdf", "abs": "https://arxiv.org/abs/2406.14230", "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing", "authors": ["Han Jiang", "Xiaoyuan Yi", "Zhihua Wei", "Ziang Xiao", "Shu Wang", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "ICML 2025", "summary": "Warning: Contains harmful model outputs. Despite significant advancements,\nthe propensity of Large Language Models (LLMs) to generate harmful and\nunethical content poses critical challenges. Measuring value alignment of LLMs\nbecomes crucial for their regulation and responsible deployment. Although\nnumerous benchmarks have been constructed to assess social bias, toxicity, and\nethical issues in LLMs, those static benchmarks suffer from evaluation\nchronoeffect, in which, as models rapidly evolve, existing benchmarks may leak\ninto training data or become saturated, overestimating ever-developing LLMs. To\ntackle this problem, we propose GETA, a novel generative evolving testing\napproach based on adaptive testing methods in measurement theory. Unlike\ntraditional adaptive testing methods that rely on a static test item pool, GETA\nprobes the underlying moral boundaries of LLMs by dynamically generating test\nitems tailored to model capability. GETA co-evolves with LLMs by learning a\njoint distribution of item difficulty and model value conformity, thus\neffectively addressing evaluation chronoeffect. We evaluated various popular\nLLMs with GETA and demonstrated that 1) GETA can dynamically create\ndifficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.", "AI": {"tldr": "GETA is a novel generative evolving testing approach for measuring the value alignment of LLMs, addressing static benchmark limitations by dynamically generating test items.", "motivation": "The need to measure the value alignment of LLMs due to their propensity to generate harmful and unethical content, and the limitations of static benchmarks in adapting to evolving models.", "method": "GETA utilizes adaptive testing methods from measurement theory to dynamically create test items based on model capabilities, co-evolving with LLMs by learning a joint distribution of item difficulty and model value conformity.", "result": "GETA can create difficulty-tailored test items dynamically, and its evaluations align better with LLM performance on unseen out-of-distribution (OOD) and independent identically distributed (i.i.d.) items compared to traditional benchmarks.", "conclusion": "GETA provides a robust foundation for future evaluation paradigms of LLMs, overcoming the challenges posed by static benchmarks.", "key_contributions": ["Introduction of the GETA approach for adaptive testing of LLMs", "Demonstration of GETA's ability to dynamically create tailored test items", "Improved evaluation consistency with unseen model performance."], "limitations": "", "keywords": ["Large Language Models", "Adaptive Testing", "Value Alignment", "Evaluation", "Ethics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.00996", "pdf": "https://arxiv.org/pdf/2407.00996.pdf", "abs": "https://arxiv.org/abs/2407.00996", "title": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Deepak Subramani"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "With the growing need for efficient language models in resource-constrained\nenvironments, Small Language Models (SLMs) have emerged as compact and\npractical alternatives to Large Language Models (LLMs). While studies have\nexplored noise handling in LLMs, little is known about how SLMs handle noise, a\ncritical factor for their reliable real-world deployment. This study\ninvestigates the ability of SLMs with parameters between 1 and 3 billion to\nlearn, retain, and subsequently eliminate different types of noise (word flip,\ncharacter flip, transliteration, irrelevant content, and contradictory\ninformation). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and\nPhi2 2.7B) were instruction-tuned on noise-free data and tested with in-context\nexamples to assess noise learning. Subsequently, noise patterns were introduced\nin instruction tuning to assess their adaptability. The results revealed\ndifferences in how models handle noise, with smaller models like Olmo quickly\nadapting to noise patterns. Phi2's carefully curated, structured, and\nhigh-quality pretraining data enabled resistance to character level,\ntransliteration, and counterfactual noise, while Gemma adapted successfully to\ntransliteration noise through its multilingual pretraining. Subsequent clean\ndata training effectively mitigated noise effects. These findings provide\npractical strategies for developing robust SLMs for real-world applications.", "AI": {"tldr": "This study investigates the robustness of Small Language Models (SLMs) in handling various types of noise, revealing differences in adaptability and offering strategies for practical applications.", "motivation": "The need for efficient language models in resource-constrained environments has led to the emergence of Small Language Models (SLMs) as alternatives to Large Language Models (LLMs), yet the impact of noise on SLM performance is under-explored.", "method": "The study tested four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and Phi2 2.7B), focusing on their ability to learn, retain, and eliminate several noise types, using instruction tuning on noise-free and noise-introduced data.", "result": "The results showed that smaller models like Olmo adapted quickly to noise patterns. Phi2 demonstrated resistance to certain noise types due to its high-quality pretraining, while Gemma effectively managed transliteration noise through its multilingual capability.", "conclusion": "The findings suggest that careful data curation and subsequent clean data training can enhance the robustness of SLMs, providing valuable strategies for their deployment in practical scenarios.", "key_contributions": ["Investigation of noise handling in small language models.", "Comparison of four pretrained small language models' adaptability to noise.", "Practical strategies for developing robust SLMs based on empirical findings."], "limitations": "The study focuses on a limited number of SLMs and noise types, which may not fully capture the complexity of real-world scenarios.", "keywords": ["Small Language Models", "noise handling", "instruction tuning", "pretrained models", "real-world applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.03181", "pdf": "https://arxiv.org/pdf/2407.03181.pdf", "abs": "https://arxiv.org/abs/2407.03181", "title": "Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs", "authors": ["Haritz Puerto", "Tilek Chubakov", "Xiaodan Zhu", "Harish Tayyar Madabushi", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Requiring a large language model (LLM) to generate intermediary reasoning\nsteps, known as Chain of Thought (CoT), has been shown to be an effective way\nof boosting performance. Previous approaches have focused on generating\nmultiple independent CoTs, combining them through ensembling or other post-hoc\nstrategies to enhance reasoning. In this work, we introduce a novel approach\nwhere LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought\n(DCoT) within a single inference step, which is fundamentally different from\nprior work that primarily operate on parallel CoT generations. DCoT allows LLMs\nto gain the ability to perform within-inference refinement of reasoning chains\nwithout requiring external feedback. Through a rigorous set of experiments\nspanning a wide range of tasks that require various reasoning types, we show\nthat fine-tuning on DCoT improves performance over the CoT baseline across\nmodel families and scales (1.3B to 70B). These improvements are particularly\nimpactful for tasks with a large result state space, such as those involving\nnumeric answers. Our work is also significant because both quantitative\nanalyses and manual evaluations reveal the observed gains stem from the models'\nability to refine an initial reasoning chain by generating a second, improved\nchain within the same inference step, demonstrating previously elusive\nself-improvement. Our code and data are publicly available at\nhttps://github.com/UKPLab/acl2025-diverse-cot.", "AI": {"tldr": "This paper presents a novel approach for LLMs to generate Diverse Chains of Thought (DCoT) in a single inference step, improving reasoning performance without external feedback.", "motivation": "To enhance the reasoning capabilities of LLMs by allowing them to generate diverse reasoning steps within a single inference, improving performance on reasoning tasks.", "method": "Fine-tuning LLMs to produce a sequence of Diverse Chains of Thought (DCoT) within one inference step, as opposed to generating independent chains separately and combining them post-hoc.", "result": "Fine-tuning on DCoT improves performance across a variety of tasks compared to conventional Chain of Thought (CoT) approaches, particularly on tasks with a significant result state space.", "conclusion": "The integration of within-inference refinement through DCoT leads to better performance and self-improvement in reasoning chains among LLMs.", "key_contributions": ["Introduction of Diverse Chains of Thought (DCoT) for LLMs", "Demonstrated self-improvement in reasoning chains", "Public availability of code and data for further research"], "limitations": "", "keywords": ["Diverse Chains of Thought", "large language models", "reasoning", "fine-tuning", "self-improvement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.08551", "pdf": "https://arxiv.org/pdf/2407.08551.pdf", "abs": "https://arxiv.org/abs/2407.08551", "title": "Autoregressive Speech Synthesis without Vector Quantization", "authors": ["Lingwei Meng", "Long Zhou", "Shujie Liu", "Sanyuan Chen", "Bing Han", "Shujie Hu", "Yanqing Liu", "Jinyu Li", "Sheng Zhao", "Xixin Wu", "Helen Meng", "Furu Wei"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Main", "summary": "We present MELLE, a novel continuous-valued token based language modeling\napproach for text-to-speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which is typically designed for audio compression\nand sacrifices fidelity compared to continuous representations. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens; (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language model VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling vector-quantized\ncodes, achieves superior performance across multiple metrics, and, most\nimportantly, offers a more streamlined paradigm. The demos of our work are\nprovided at https://aka.ms/melle.", "AI": {"tldr": "MELLE is a new approach for text-to-speech synthesis that generates continuous mel-spectrogram frames directly from text without vector quantization, improving performance and robustness.", "motivation": "To address the limitations of text-to-speech synthesis methods that use vector quantization, which sacrifices audio fidelity for compression.", "method": "MELLE uses autoregressive generation of continuous mel-spectrogram frames, applying regression loss and a proposed spectrogram flux loss function, and incorporates variational inference for enhanced sampling diversity.", "result": "MELLE outperforms the VALL-E language model and its variants across multiple performance metrics, alleviating robustness issues related to vector quantization.", "conclusion": "MELLE represents a more efficient and robust paradigm for text-to-speech synthesis that avoids the typical flaws of existing two-stage models.", "key_contributions": ["Introduction of the MELLE approach for continuous-valued token modeling in TTS.", "Use of regression loss with a novel spectrogram flux loss function.", "Integration of variational inference to improve output diversity."], "limitations": "", "keywords": ["text-to-speech synthesis", "continuous-valued tokens", "variational inference"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2407.21054", "pdf": "https://arxiv.org/pdf/2407.21054.pdf", "abs": "https://arxiv.org/abs/2407.21054", "title": "Sentiment Reasoning for Healthcare", "authors": ["Khai-Nguyen Nguyen", "Khai Le-Duc", "Bach Phan Tat", "Duy Le", "Long Vo-Dang", "Truong-Son Hy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "ACL 2025 (Oral)", "summary": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning", "AI": {"tldr": "This paper introduces Sentiment Reasoning, a new task aimed at enhancing model transparency in AI healthcare decision-making by providing rationales for sentiment analysis predictions, demonstrated through a multimodal multitask framework and the largest sentiment analysis dataset to date.", "motivation": "Transparency in AI healthcare decision-making is essential for better user understanding and decision-making, particularly with Large Language Models (LLMs).", "method": "The study proposes a novel task called Sentiment Reasoning, where models generate both sentiment labels and rationales based on input transcripts, evaluated using a multimodal multitask framework on both human and Automatic Speech Recognition (ASR) transcripts.", "result": "The introduction of Sentiment Reasoning improved model classification performance by 2% in accuracy and macro-F1 scores while providing rationales of semantic quality comparable to humans, regardless of whether the transcripts were human-written or generated by ASR systems.", "conclusion": "Sentiment Reasoning enhances model transparency in sentiment analysis and improves classification performance through rationale-augmented fine-tuning.", "key_contributions": ["Introduction of the Sentiment Reasoning task for sentiment analysis", "Development of a multimodal multitask framework", "Creation of the largest multimodal sentiment analysis dataset."], "limitations": "", "keywords": ["AI healthcare", "Sentiment Reasoning", "Multimodal analysis", "Large Language Models", "Model transparency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.13533", "pdf": "https://arxiv.org/pdf/2408.13533.pdf", "abs": "https://arxiv.org/abs/2408.13533", "title": "Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models", "authors": ["Jinyang Wu", "Shuai Zhang", "Feihu Che", "Mingkuan Feng", "Pengpeng Shao", "Jianhua Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial method for\naddressing hallucinations in large language models (LLMs). While recent\nresearch has extended RAG models to complex noisy scenarios, these explorations\noften confine themselves to limited noise types and presuppose that noise is\ninherently detrimental to LLMs, potentially deviating from real-world retrieval\nenvironments and restricting practical applicability. In this paper, we define\nseven distinct noise types from a linguistic perspective and establish a Noise\nRAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing\nmultiple datasets and reasoning tasks. Through empirical evaluation of eight\nrepresentative LLMs with diverse architectures and scales, we reveal that these\nnoises can be further categorized into two practical groups: noise that is\nbeneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs\n(aka harmful noise). While harmful noise generally impairs performance,\nbeneficial noise may enhance several aspects of model capabilities and overall\nperformance. Our analysis offers insights for developing more robust, adaptable\nRAG solutions and mitigating hallucinations across diverse retrieval scenarios.", "AI": {"tldr": "The paper introduces a Noise RAG Benchmark to evaluate Retrieval-Augmented Generation (RAG) methods in the presence of various noise types and analyzes their effects on LLM performance.", "motivation": "To address hallucinations in large language models and explore the impact of different noise types on model performance, which has been overlooked in existing research.", "method": "Defined seven distinct noise types from a linguistic perspective and established the Noise RAG Benchmark (NoiserBench) for evaluation using multiple datasets and reasoning tasks.", "result": "Contrary to prior assumptions, the paper finds that noise can be categorized into beneficial and harmful types, with some noise actually enhancing LLM performance in specific scenarios.", "conclusion": "The findings provide insights to improve RAG solutions and mitigate hallucinations in practical retrieval environments by understanding the dual nature of noise.", "key_contributions": ["Development of the Noise RAG Benchmark (NoiserBench) for evaluating noisy scenarios in LLMs.", "Categorization of noise into beneficial and harmful types with empirical evaluation.", "Insights on improving RAG methods to handle diverse retrieval challenges."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "large language models", "noisy data", "benchmarking", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.04183", "pdf": "https://arxiv.org/pdf/2409.04183.pdf", "abs": "https://arxiv.org/abs/2409.04183", "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "authors": ["Ziyin Zhang", "Hang Yu", "Shijie Li", "Peng Di", "Jianguo Li", "Rui Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 camera-ready", "summary": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.", "AI": {"tldr": "This paper presents GALLa, a model that enhances code language models by integrating graph neural networks to utilize semantic information from code structure during finetuning, showing improved performance across multiple tasks without compromising model compatibility at inference.", "motivation": "To address the limitations of current code language models that either ignore structural information or require modifications to the Transformer architecture, thus hindering scalability and compatibility with pretrained models.", "method": "GALLa employs graph neural networks and cross-modal alignment to inject structural code information as an auxiliary task during the finetuning process, maintaining a model-agnostic and task-agnostic approach that only requires structural graph data at training time.", "result": "Experiments demonstrate that GALLa consistently outperforms baseline models across five coding tasks, with significant improvements observed in models like LLaMA3 and Qwen2.5-Coder, regardless of their size.", "conclusion": "GALLa effectively combines the advantages of structural information modeling and large language models, leading to enhanced performance on code tasks while remaining compatible with existing LLMs.", "key_contributions": ["Introduction of GALLa, a model that integrates graph neural networks with LLMs", "Demonstration of consistent performance improvements across multiple coding tasks", "Model-agnostic framework that incurs no additional inference costs"], "limitations": "", "keywords": ["Code Language Models", "Graph Neural Networks", "Finetuning", "Cross-modal Alignment", "Programming Languages"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.14469", "pdf": "https://arxiv.org/pdf/2409.14469.pdf", "abs": "https://arxiv.org/abs/2409.14469", "title": "Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints", "authors": ["Kaikai An", "Shuzheng Si", "Helan Hu", "Haozhe Zhao", "Yuchi Wang", "Qingyan Guo", "Baobao Chang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Semantic Parsing aims to capture the meaning of a sentence and convert it\ninto a logical, structured form. Previous studies show that semantic parsing\nenhances the performance of smaller models (e.g., BERT) on downstream tasks.\nHowever, it remains unclear whether the improvements extend similarly to LLMs.\nIn this paper, our empirical findings reveal that, unlike smaller models,\ndirectly adding semantic parsing results into LLMs reduces their performance.\nTo overcome this, we propose SENSE, a novel prompting approach that embeds\nsemantic hints within the prompt. Experiments show that SENSE consistently\nimproves LLMs' performance across various tasks, highlighting the potential of\nintegrating semantic information to improve LLM capabilities.", "AI": {"tldr": "The paper investigates the impact of semantic parsing on LLM performance and introduces SENSE, a prompting method that embeds semantic hints to enhance LLM capabilities.", "motivation": "To understand how semantic parsing affects the performance of large language models (LLMs) compared to smaller models.", "method": "The study conducts empirical experiments comparing LLM performance with and without semantic parsing results, followed by the introduction of the SENSE prompting approach.", "result": "Findings show that directly adding semantic parsing outputs reduces LLM performance, but the SENSE approach leads to consistent improvements across tasks.", "conclusion": "Integrating semantic information through effective prompting can enhance the capabilities of LLMs significantly.", "key_contributions": ["Demonstration of the negative impact of directly adding semantic parsing to LLMs.", "Introduction of the SENSE prompting approach to effectively integrate semantic hints.", "Empirical evidence of improved performance in LLMs when using the SENSE method."], "limitations": "The study does not explore all possible dimensions of semantic parsing and its broader implications on various LLM architectures.", "keywords": ["semantic parsing", "large language models", "prompting approach", "SENSE", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.01651", "pdf": "https://arxiv.org/pdf/2410.01651.pdf", "abs": "https://arxiv.org/abs/2410.01651", "title": "Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling", "authors": ["Xiang Hu", "Zhihao Teng", "Jun Zhao", "Wei Wu", "Kewei Tu"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to ICML 2025", "summary": "Despite the success of Transformers, handling long contexts remains\nchallenging due to the limited length generalization and quadratic complexity\nof self-attention. Thus Transformers often require post-training with a larger\nattention window, significantly increasing computational and memory costs. In\nthis paper, we propose a novel attention mechanism based on dynamic context,\nGrouped Cross Attention (GCA), which can generalize to 1000 times the\npre-training context length while maintaining the ability to access distant\ninformation with a constant attention window size. For a given input sequence,\nwe split it into chunks and use each chunk to retrieve top-k relevant past\nchunks for subsequent text generation. Specifically, unlike most previous works\nthat use an off-the-shelf retriever, our key innovation allows the retriever to\nlearn how to retrieve past chunks that better minimize the auto-regressive loss\nof subsequent tokens in an end-to-end manner. Such a mechanism accommodates\nretrieved chunks with a fixed-size attention window to achieve long-range\ninformation access, significantly reducing computational and memory costs\nduring training and inference. Experiments show that GCA-based models achieve\nnear-perfect accuracy in passkey retrieval for 16M context lengths, which is\n1000 times the training length.", "AI": {"tldr": "This paper introduces Grouped Cross Attention (GCA), a new attention mechanism for Transformers that allows handling significantly longer contexts (up to 1000 times the pre-training length) efficiently, utilizing a constant attention window size.", "motivation": "The motivation is to address the limitations of Transformers in managing long contexts due to self-attention's quadratic complexity and the need for larger attention windows, which increase computational and memory costs.", "method": "The paper proposes a dynamic context attention mechanism that divides input sequences into chunks and learns to retrieve relevant past chunks for text generation in an end-to-end manner, maintaining a fixed-size attention window.", "result": "GCA-based models demonstrate near-perfect accuracy in passkey retrieval tasks with context lengths of 16M, showcasing the effectiveness of the proposed method in accessing long-range information efficiently.", "conclusion": "The proposed GCA mechanism significantly reduces computational and memory costs while enhancing the ability of Transformers to manage long contexts effectively.", "key_contributions": ["Novel attention mechanism for long context handling in Transformers.", "End-to-end learning for retrieval of past chunks that reduce auto-regressive loss.", "Achieves 1000 times pre-training context length with constant attention window."], "limitations": "", "keywords": ["Transformers", "Grouped Cross Attention", "Long-context handling", "Dynamic attention mechanism", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.06638", "pdf": "https://arxiv.org/pdf/2410.06638.pdf", "abs": "https://arxiv.org/abs/2410.06638", "title": "Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing", "authors": ["Kaishuai Xu", "Tiezheng Yu", "Wenjun Hou", "Yi Cheng", "Chak Tou Leong", "Liangyou Li", "Xin Jiang", "Lifeng Shang", "Qun Liu", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted as ACL 2025 main", "summary": "Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation.", "AI": {"tldr": "This paper introduces eRror-Injected Self-Editing (RISE), a novel preference learning framework designed to improve mathematical reasoning in Large Language Models by injecting subtle errors into solutions to enhance training and error mitigation.", "motivation": "LLMs struggle with subtle reasoning errors that limit their mathematical capabilities, necessitating improved training methods that address these issues.", "method": "RISE injects predefined subtle errors into key tokens during reasoning or computation steps, creating hard pairs for training without the need for fine-grained sampling or preference annotation.", "result": "Extensive experiments showed that RISE leads to significant performance improvements of 3.0% on GSM8K and 7.9% on MATH with only 4.5K training samples when applied to Qwen2-7B-Instruct.", "conclusion": "RISE not only enhances mathematical reasoning but also extends its effectiveness to logical reasoning and code generation tasks.", "key_contributions": ["Introduction of RISE framework for error mitigation in LLMs", "Effective training with fewer samples through error injection", "Demonstrated improvements in performance on mathematical reasoning benchmarks"], "limitations": "", "keywords": ["Large Language Models", "mathematical reasoning", "preference learning", "error mitigation", "self-editing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.09829", "pdf": "https://arxiv.org/pdf/2410.09829.pdf", "abs": "https://arxiv.org/abs/2410.09829", "title": "Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles", "authors": ["Rimvydas Rubavicius", "Antonio Valerio Miceli-Barone", "Alex Lascarides", "Subramanian Ramamoorthy"], "categories": ["cs.CL", "cs.IR", "cs.RO"], "comment": "12 pages, 5 figures, 2 tables", "summary": "Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.", "AI": {"tldr": "This paper presents a natural language interface leveraging a large language model to assist non-coding experts in creating autonomous vehicle scenarios for simulation testing.", "motivation": "To improve the testing of autonomous vehicles, enabling domain experts without coding skills to specify scenarios and vehicle behaviors easily.", "method": "The authors developed a natural language interface that translates verbal instructions into symbolic programs, facilitating scenario generation in simulations.", "result": "The approach demonstrated feasibility with a limited training dataset and achieved a 4.5 times higher success rate in simulation generation when utilizing dialogue compared to non-dialogue methods.", "conclusion": "Engaging in a dialogue is essential for successful scenario generation, highlighting the potential of natural language interfaces in effective simulation testing.", "key_contributions": ["Introduction of a natural language interface for scenario synthesis", "Demonstration of feasibility with limited data", "Established the significance of dialogue in simulation success"], "limitations": "The study is based on a small training dataset, which may affect the generalizability of the results.", "keywords": ["natural language interface", "autonomous vehicles", "scenario generation", "dialogue system", "large language model"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.12458", "pdf": "https://arxiv.org/pdf/2410.12458.pdf", "abs": "https://arxiv.org/abs/2410.12458", "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph", "authors": ["Minghao Wu", "Thuy-Trang Vu", "Lizhen Qu", "Gholamreza Haffari"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "The performance of large language models (LLMs) is strongly influenced by the\nquality and diversity of data used during supervised fine-tuning (SFT).\nHowever, current data selection methods often prioritize one aspect over the\nother, resulting in suboptimal training outcomes. To address this, we formulate\ndata selection as a set cover problem and present GraphFilter, a novel approach\nthat balances both quality and diversity in data selection. GraphFilter models\nthe dataset as a bipartite graph connecting sentences to their constituent\nn-grams, then employs a priority function that combines quality and diversity\nmetrics multiplicatively. GraphFilter iteratively selects sentences with the\nhighest priority, removes covered n-grams from the bipartite graph, and\nrecomputes priorities to reflect the changing data landscape. We validate\nGraphFilter using three model backbones across six widely-used benchmarks,\ndemonstrating that it outperforms nine existing baselines in both model\nperformance and computational efficiency. Further analysis shows that our\ndesign choices lead to more effective subset selection, underscores the value\nof instruction diversity, and provides insights into how quality and diversity\ninteract with different subset sizes.", "AI": {"tldr": "GraphFilter is a new approach for data selection in supervised fine-tuning of large language models that balances quality and diversity using a bipartite graph model.", "motivation": "Current data selection methods often prioritize either quality or diversity in data, leading to poor training outcomes for large language models.", "method": "GraphFilter formulates data selection as a set cover problem, modeling the dataset as a bipartite graph and using a priority function that combines quality and diversity metrics.", "result": "GraphFilter outperforms nine existing baselines in model performance and computational efficiency across six benchmarks when tested with three different model backbones.", "conclusion": "The study highlights the effectiveness of balancing quality and diversity in data selection for improving the performance of large language models during fine-tuning.", "key_contributions": ["Introduction of GraphFilter for data selection that balances quality and diversity.", "Demonstrated superior performance compared to nine baselines.", "Provided insights into the interaction between quality and diversity across different subset sizes."], "limitations": "", "keywords": ["data selection", "large language models", "supervised fine-tuning", "quality and diversity", "GraphFilter"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.13206", "pdf": "https://arxiv.org/pdf/2410.13206.pdf", "abs": "https://arxiv.org/abs/2410.13206", "title": "BQA: Body Language Question Answering Dataset for Video Large Language Models", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Miyu Oba", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 (Main)", "summary": "A large part of human communication relies on nonverbal cues such as facial\nexpressions, eye contact, and body language. Unlike language or sign language,\nsuch nonverbal communication lacks formal rules, requiring complex reasoning\nbased on commonsense understanding. Enabling current Video Large Language\nModels (VideoLLMs) to accurately interpret body language is a crucial\nchallenge, as human unconscious actions can easily cause the model to\nmisinterpret their intent. To address this, we propose a dataset, BQA, a body\nlanguage question answering dataset, to validate whether the model can\ncorrectly interpret emotions from short clips of body language comprising 26\nemotion labels of videos of body language. We evaluated various VideoLLMs on\nBQA and revealed that understanding body language is challenging, and our\nanalyses of the wrong answers by VideoLLMs show that certain VideoLLMs made\nsignificantly biased answers depending on the age group and ethnicity of the\nindividuals in the video. The dataset is available.", "AI": {"tldr": "The paper introduces BQA, a dataset for evaluating Video Large Language Models' ability to interpret body language and emotions, revealing challenges and biases present in their interpretations.", "motivation": "To enhance the interpretation of nonverbal communication by Video Large Language Models (VideoLLMs), which is essential for accurate human interaction recognition.", "method": "The authors propose the BQA dataset that contains short clips of body language annotated with 26 emotion labels and evaluate various VideoLLMs against this dataset.", "result": "The evaluation showed that VideoLLMs struggle with accurately interpreting emotions from body language, and certain models exhibited biases based on the age and ethnicity of individuals in the videos.", "conclusion": "A better understanding of body language interpretation is necessary for VideoLLMs, and the biases observed indicate the need for further research in this area.", "key_contributions": ["Introduction of the BQA dataset for body language interpretation", "Evaluation of VideoLLMs showing challenges in emotion recognition", "Analysis of biases based on demographic factors in VideoLLM responses."], "limitations": "The dataset may not comprehensively cover all possible nonverbal cues and emotions, and biases could be influenced by various factors beyond age and ethnicity.", "keywords": ["body language", "VideoLLMs", "emotion recognition", "dataset", "bias"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.13776", "pdf": "https://arxiv.org/pdf/2410.13776.pdf", "abs": "https://arxiv.org/abs/2410.13776", "title": "Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors", "authors": ["Georgios Chochlakis", "Alexandros Potamianos", "Kristina Lerman", "Shrikanth Narayanan"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 12 figures, 3 tables", "summary": "In-context Learning (ICL) has become the primary method for performing\nnatural language tasks with Large Language Models (LLMs). The knowledge\nacquired during pre-training is crucial for this few-shot capability, providing\nthe model with task priors. However, recent studies have shown that ICL\npredominantly relies on retrieving task priors rather than \"learning\" to\nperform tasks. This limitation is particularly evident in complex subjective\ndomains such as emotion and morality, where priors significantly influence\nposterior predictions. In this work, we examine whether this is the result of\nthe aggregation used in corresponding datasets, where trying to combine\nlow-agreement, disparate annotations might lead to annotation artifacts that\ncreate detrimental noise in the prompt. Moreover, we evaluate the posterior\nbias towards certain annotators by grounding our study in appropriate,\nquantitative measures of LLM priors. Our results indicate that aggregation is a\nconfounding factor in the modeling of subjective tasks, and advocate focusing\non modeling individuals instead. However, aggregation does not explain the\nentire gap between ICL and the state of the art, meaning other factors in such\ntasks also account for the observed phenomena. Finally, by rigorously studying\nannotator-level labels, we find that it is possible for minority annotators to\nboth better align with LLMs and have their perspectives further amplified.", "AI": {"tldr": "This paper investigates the limitations of In-context Learning (ICL) in LLMs, focusing on annotation artifacts that arise from data aggregation in subjective tasks.", "motivation": "The study aims to understand the reliance of LLMs on task priors during In-context Learning, especially in complex domains like emotion and morality.", "method": "The authors examine the effects of data aggregation and annotator biases on the performance of LLMs in subjective domains, using qualitative and quantitative measures.", "result": "Findings indicate that aggregate methods can introduce detrimental noise, affecting model predictions and advocating for a focus on individual annotator insights.", "conclusion": "The paper concludes that while aggregation affects performance, it does not explain the entire gap between ICL and state-of-the-art performance, highlighting the need for further exploration of other influencing factors.", "key_contributions": ["Critical examination of data aggregation in subjective task modeling", "Introduction of quantifiable measures for understanding LLM priors", "Identification of the impact of minority annotators on LLM alignment"], "limitations": "The study acknowledges that there are other unidentified factors contributing to the ICL gap.", "keywords": ["In-context Learning", "Large Language Models", "emotion", "morality", "annotator bias"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2410.14641", "pdf": "https://arxiv.org/pdf/2410.14641.pdf", "abs": "https://arxiv.org/abs/2410.14641", "title": "Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs", "authors": ["Runchu Tian", "Yanghao Li", "Yuepeng Fu", "Siyang Deng", "Qinyu Luo", "Cheng Qian", "Shuo Wang", "Xin Cong", "Zhong Zhang", "Yesai Wu", "Yankai Lin", "Huadong Wang", "Xiaojiang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.", "AI": {"tldr": "This paper introduces LongPiBench, a benchmark for assessing positional bias in large language models (LLMs) related to processing multiple pieces of relevant information, revealing significant biases in current models.", "motivation": "Current LLMs struggle with the 'lost in the middle' phenomenon, which limits their effectiveness in processing long inputs containing multiple relevant information pieces.", "method": "The authors developed LongPiBench, a benchmark to evaluate positional bias, conducting thorough experiments across five commercial and six open-source LLMs.", "result": "Most models showed robustness against the 'lost in the middle' issue; however, significant biases were found pertaining to the spacing of relevant information pieces.", "conclusion": "Evaluating and reducing positional biases in LLMs is crucial for advancing their capabilities in real-world applications.", "key_contributions": ["Introduction of LongPiBench benchmark for positional bias", "Demonstrated biases related to spacing of relevant information", "Insights into LLM performance with long input processing"], "limitations": "", "keywords": ["positional bias", "large language models", "LongPiBench"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.18693", "pdf": "https://arxiv.org/pdf/2410.18693.pdf", "abs": "https://arxiv.org/abs/2410.18693", "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch", "authors": ["Yuyang Ding", "Xinyu Shi", "Xiaobo Liang", "Juntao Li", "Zhaopeng Tu", "Qiaoming Zhu", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Improving the mathematical reasoning capabilities of Large Language Models\n(LLMs) is critical for advancing artificial intelligence. However, access to\nextensive, diverse, and high-quality reasoning datasets remains a significant\nchallenge, particularly for the open-source community. In this paper, we\npropose ScaleQuest, a novel, scalable, and cost-effective data synthesis method\nthat enables the generation of large-scale mathematical reasoning datasets\nusing lightweight 7B-scale models. ScaleQuest introduces a two-stage\nquestion-tuning process comprising Question Fine-Tuning (QFT) and Question\nPreference Optimization (QPO) to unlock the question generation capabilities of\nproblem-solving models. By generating diverse questions from scratch -- without\nrelying on powerful proprietary models or seed data -- we produce a dataset of\n1 million problem-solution pairs. Our experiments demonstrate that models\ntrained on our data outperform existing open-source datasets in both in-domain\nand out-of-domain evaluations. Furthermore, our approach shows continued\nperformance improvement as the volume of training data increases, highlighting\nits potential for ongoing data scaling. The extensive improvements observed in\ncode reasoning tasks demonstrate the generalization capabilities of our\nproposed method. Our work provides the open-source community with a practical\nsolution to enhance the mathematical reasoning abilities of LLMs.", "AI": {"tldr": "This paper presents ScaleQuest, a new method for generating large-scale mathematical reasoning datasets to enhance LLMs' capabilities.", "motivation": "There is a critical need for extensive and diverse reasoning datasets for improving the mathematical reasoning capabilities of Large Language Models (LLMs), particularly for the open-source community.", "method": "ScaleQuest employs a two-stage question-tuning process: Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to generate diverse questions from 7B-scale models without using proprietary models or seed data.", "result": "Models trained on the dataset generated by ScaleQuest outperform existing open-source datasets in evaluations, showing improved performance with increasing training data volume.", "conclusion": "ScaleQuest provides a cost-effective and practical solution for the open-source community to improve LLMs' mathematical reasoning abilities.", "key_contributions": ["Introduction of ScaleQuest for scalable data synthesis for LLMs", "Development of a dataset containing 1 million problem-solution pairs", "Demonstration of improved model performance on code reasoning tasks."], "limitations": "", "keywords": ["Large Language Models", "Dataset Generation", "Mathematical Reasoning", "Open-source", "AI"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.21013", "pdf": "https://arxiv.org/pdf/2410.21013.pdf", "abs": "https://arxiv.org/abs/2410.21013", "title": "Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers", "authors": ["Akhilesh Kakolu Ramarao", "Kevin Tang", "Dinah Baer-Henney"], "categories": ["cs.CL"], "comment": "Typos and grammatical corrections", "summary": "Over the past decade, various studies have addressed how speakers solve the\nso-called `The Paradigm Cell Filling Problem' (PCFP) \\citep{ackerman2009parts}\nacross different languages. The PCFP addresses a fundamental question in\nmorphological processing: how do speakers accurately generate inflected forms\nof words when presented with incomplete paradigms? This problem is particularly\nsalient when modeling complex inflectional systems. We focus on Spanish verbal\nparadigms, where certain verbs follow an irregular L-shaped pattern, where the\nfirst-person singular present indicative stem matches the stem used throughout\nthe present subjunctive mood. We formulate the problem as a morphological\nreinflection task. Specifically, we investigate the role of input frequency in\nthe acquisition of regular versus irregular L-shaped patterns in transformer\nmodels. By systematically manipulating the input distributions and analyzing\nmodel behavior, we reveal four key findings: 1) Models perform better on\nL-shaped verbs compared to regular verbs, especially in uneven frequency\nconditions; 2) Robust primacy effects are observed, but no consistent recency\neffects; 3) Memorization becomes more prominent as the proportion of L-shaped\nverbs increases; 4) There is a tendency to regularize L-shaped verbs when their\nconsonant alternation pairs are rare or absent in the training data.", "AI": {"tldr": "Study on morphological processing in Spanish focusing on the Paradigm Cell Filling Problem using transformer models.", "motivation": "To address how speakers generate inflected forms of words within incomplete paradigms, particularly in the context of Spanish verbal paradigms with irregular patterns.", "method": "The problem is formulated as a morphological reinflection task where input frequency's role in the acquisition of regular versus irregular patterns is investigated using transformer models.", "result": "Key findings include better model performance on L-shaped verbs under uneven frequency conditions; observation of primacy effects, increased memorization with more L-shaped verbs, and regularization tendencies when consonant pairs are rare in training data.", "conclusion": "The study provides insights into morphological processing and model behavior regarding inflection patterns and frequency.", "key_contributions": ["Investigates input frequency effects on morphological reinflection", "Reveals primacy effects and memorization trends in transformer models", "Demonstrates regularization tendencies in verb paradigms"], "limitations": "", "keywords": ["Morphological processing", "Transformer models", "Spanish verbs", "Inflection"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2411.00387", "pdf": "https://arxiv.org/pdf/2411.00387.pdf", "abs": "https://arxiv.org/abs/2411.00387", "title": "STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing", "authors": ["Jiaru Zou", "Qing Wang", "Pratyush Thakur", "Nickvash Kani"], "categories": ["cs.CL"], "comment": "ACL 2025; NeurIPS Math-AI 2024", "summary": "Advances in large language models (LLMs) have spurred research into enhancing\ntheir reasoning capabilities, particularly in math-rich STEM (Science,\nTechnology, Engineering, and Mathematics) documents. While LLMs can generate\nequations or solve math-related queries, their ability to fully understand and\ninterpret abstract mathematical symbols in long, math-rich documents remains\nlimited. In this paper, we introduce STEM-PoM, a comprehensive benchmark\ndataset designed to evaluate LLMs' reasoning abilities on math symbols within\ncontextual scientific text. The dataset, sourced from real-world ArXiv\ndocuments, contains over 2K math symbols classified as main attributes of\nvariables, constants, operators, and unit descriptors, with additional\nsub-attributes including scalar/vector/matrix for variables and\nlocal/global/discipline-specific labels for both constants and operators. Our\nextensive experiments demonstrate that state-of-the-art LLMs achieve an average\naccuracy of 20-60% under in-context learning and 50-60% with fine-tuning,\nhighlighting a substantial gap in their ability to classify mathematical\nsymbols. By improving LLMs' mathematical symbol classification, STEM-PoM\nfurther enhances models' downstream mathematical reasoning capabilities. The\ncode and data are available at https://github.com/jiaruzouu/STEM-PoM.", "AI": {"tldr": "STEM-PoM is a benchmark dataset for evaluating LLMs' reasoning with mathematical symbols in STEM texts, highlighting gaps in their classification abilities.", "motivation": "To enhance large language models' reasoning abilities in interpreting abstract mathematical symbols in math-rich STEM documents and address their limitations.", "method": "Introduction of a comprehensive benchmark dataset, STEM-PoM, sourced from real-world ArXiv documents, containing over 2K classified math symbols with extensive experimental evaluation of LLMs' performance.", "result": "State-of-the-art LLMs achieve average accuracy of 20-60% under in-context learning and 50-60% with fine-tuning in classifying mathematical symbols within the dataset.", "conclusion": "Improving LLMs' classification of mathematical symbols not only addresses existing gaps but also enhances the models' overall mathematical reasoning capabilities.", "key_contributions": ["Development of the STEM-PoM benchmark dataset", "Insight into LLMs' performance gaps in math symbol classification", "Open-source access to code and data for further research"], "limitations": "", "keywords": ["large language models", "mathematical reasoning", "benchmark dataset"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2411.01834", "pdf": "https://arxiv.org/pdf/2411.01834.pdf", "abs": "https://arxiv.org/abs/2411.01834", "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "authors": ["Guan-Ting Lin", "Prashanth Gurunath Shivakumar", "Aditya Gourav", "Yile Gu", "Ankur Gandhe", "Hung-yi Lee", "Ivan Bulyko"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by ACL 2025", "summary": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs.", "AI": {"tldr": "The Align-SLM framework enhances textless Spoken Language Models (SLMs) using preference optimization from Reinforcement Learning with AI Feedback to improve semantic coherence and relevance, achieving state-of-the-art performance on various benchmarks.", "motivation": "Textless Spoken Language Models (SLMs) show potential but lack semantic coherence compared to text-based Large Language Models (LLMs).", "method": "The Align-SLM framework generates multiple speech continuations from a prompt and utilizes semantic metrics to develop preference data for Direct Preference Optimization (DPO).", "result": "The experimental results demonstrate that our method achieves state-of-the-art performance for SLMs on multiple benchmarks, highlighting the effectiveness of preference optimization.", "conclusion": "Preference optimization is critical for improving the semantic understanding of textless SLMs, as evidenced by the benchmark results.", "key_contributions": ["Introduction of the Align-SLM framework for SLMs", "Utilization of preference optimization inspired by RLAIF", "Demonstration of state-of-the-art performance on SLM benchmarks"], "limitations": "", "keywords": ["Spoken Language Models", "Reinforcement Learning", "Direct Preference Optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.07446", "pdf": "https://arxiv.org/pdf/2411.07446.pdf", "abs": "https://arxiv.org/abs/2411.07446", "title": "Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection", "authors": ["Cilin Yan", "Jingyun Wang", "Lin Zhang", "Ruihui Zhao", "Xiaopu Wu", "Kai Xiong", "Qingsong Liu", "Guoliang Kang", "Yangyang Kang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Automatic prompt engineering aims to enhance the generation quality of large\nlanguage models (LLMs). Recent works utilize feedbacks generated from erroneous\ncases to guide the prompt optimization. During inference, they may further\nretrieve several semantically-related exemplars and concatenate them to the\noptimized prompts to improve the performance. However, those works only utilize\nthe feedback at the current step, ignoring historical and unseleccted feedbacks\nwhich are potentially beneficial. Moreover, the selection of exemplars only\nconsiders the general semantic relationship and may not be optimal in terms of\ntask performance and matching with the optimized prompt. In this work, we\npropose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize\nmore efficient and accurate prompt optimization. Specifically, we design an\nexemplar-guided reflection mechanism where the feedback generation is\nadditionally guided by the generated exemplars. We further build two kinds of\nmemory to fully utilize the historical feedback information and support more\neffective exemplar retrieval. Empirical evaluations show our method surpasses\nprevious state-of-the-arts with less optimization steps, i.e., improving F1\nscore by 10.1 on LIAR dataset, and reducing half of the optimization steps on\nProTeGi.", "AI": {"tldr": "This paper introduces an Exemplar-Guided Reflection with Memory mechanism (ERM) to improve the prompt optimization of large language models by leveraging historical feedback and enhancing exemplar selection.", "motivation": "The work addresses limitations in existing prompt optimization methods that overlook historical feedback and rely on general semantic relationships for exemplar retrieval, which may not be optimal for task performance.", "method": "An exemplar-guided reflection mechanism is proposed, where feedback generation is influenced by selected exemplars. Additionally, two memory systems are constructed to utilize historical feedback information and enhance exemplar retrieval.", "result": "The proposed method significantly outperforms previous approaches, achieving a 10.1 improvement in F1 score on the LIAR dataset and reducing the required optimization steps by half on ProTeGi.", "conclusion": "The findings demonstrate that incorporating historical feedback and optimized exemplar retrieval leads to better performance in prompt engineering for LLMs.", "key_contributions": ["Introduction of the ERM framework for prompt optimization", "Utilization of historical feedback for exemplar retrieval", "Demonstrated performance improvements on benchmark datasets"], "limitations": "", "keywords": ["prompt engineering", "exemplar retrieval", "memory mechanism"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.12719", "pdf": "https://arxiv.org/pdf/2411.12719.pdf", "abs": "https://arxiv.org/abs/2411.12719", "title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation", "authors": ["Praveen Srinivasa Varadhan", "Amogh Gulati", "Ashwin Sankar", "Srija Anand", "Anirudh Gupta", "Anirudh Mukherjee", "Shiva Kumar Marepally", "Ankur Bhatia", "Saloni Jaju", "Suvrat Bhooshan", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in TMLR", "summary": "Despite rapid advancements in TTS models, a consistent and robust human\nevaluation framework is still lacking. For example, MOS tests fail to\ndifferentiate between similar models, and CMOS's pairwise comparisons are\ntime-intensive. The MUSHRA test is a promising alternative for evaluating\nmultiple TTS systems simultaneously, but in this work we show that its reliance\non matching human reference speech unduly penalises the scores of modern TTS\nsystems that can exceed human speech quality. More specifically, we conduct a\ncomprehensive assessment of the MUSHRA test, focusing on its sensitivity to\nfactors such as rater variability, listener fatigue, and reference bias. Based\non our extensive evaluation involving 492 human listeners across Hindi and\nTamil we identify two primary shortcomings: (i) reference-matching bias, where\nraters are unduly influenced by the human reference, and (ii) judgement\nambiguity, arising from a lack of clear fine-grained guidelines. To address\nthese issues, we propose two refined variants of the MUSHRA test. The first\nvariant enables fairer ratings for synthesized samples that surpass human\nreference quality. The second variant reduces ambiguity, as indicated by the\nrelatively lower variance across raters. By combining these approaches, we\nachieve both more reliable and more fine-grained assessments. We also release\nMANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind\ncollection for Indian languages, aiding in analyzing human preferences and\ndeveloping automatic metrics for evaluating TTS systems.", "AI": {"tldr": "This paper assesses the MUSHRA test for TTS system evaluation and proposes refined variants to address its shortcomings.", "motivation": "To improve the evaluation framework for text-to-speech (TTS) models, which currently lacks robustness and consistency.", "method": "A comprehensive assessment of the MUSHRA test was conducted, involving 492 listeners, focusing on rater variability, listener fatigue, and reference bias with empirical analysis in Hindi and Tamil.", "result": "Identified two main issues in the MUSHRA test: reference-matching bias and judgement ambiguity. Proposed two variants: one for fairer ratings beyond human quality and another to reduce ambiguity.", "conclusion": "The new variants provide more reliable and detailed assessments for TTS systems. A massive dataset, MANGO, is released for research in Indian languages.", "key_contributions": ["Identification of reference-matching bias and judgement ambiguity in the MUSHRA test", "Proposed refined MUSHRA test variants", "Released the MANGO dataset of 246,000 ratings for Indian languages"], "limitations": "", "keywords": ["TTS", "MUSHRA", "evaluation", "human ratings", "Indian languages"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2411.14055", "pdf": "https://arxiv.org/pdf/2411.14055.pdf", "abs": "https://arxiv.org/abs/2411.14055", "title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jing Li", "Min Zhang", "Zhaopeng Tu"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\na method that dynamically adjusts the data distribution during training to\nrestore balanced performance across heterogeneous and multi-tasking data.\nExperiments in monolingual and multilingual settings show that DRPruning\nsurpasses similarly sized models in both pruning and continued pretraining over\nperplexity, downstream tasks, and instruction tuning. Further analysis\ndemonstrates the robustness of DRPruning towards various domains and\ndistribution shifts. Furthermore, DRPruning can determine optimal reference\nlosses and data ratios automatically, suggesting potential for broader\napplications. Code and scripts are available at\nhttps://github.com/hexuandeng/DRPruning.", "AI": {"tldr": "DRPruning dynamically adjusts data distribution during training to improve LLM performance across diverse tasks.", "motivation": "To address the performance degradation in large language models due to structured pruning, which often results in biased outcomes across different domains.", "method": "The DRPruning method modifies the data distribution during training to ensure balanced performance in heterogeneous and multi-task tasks.", "result": "Experiments show that DRPruning outperforms similarly sized models in pruning and continued pretraining across various metrics, demonstrating robust performance against distribution shifts.", "conclusion": "The robustness of DRPruning and its automatic determination of optimal loss references and data ratios indicate its potential for wider applications.", "key_contributions": ["Introduction of DRPruning for dynamic data distribution adjustment.", "Demonstrated improved performance of LLMs in monolingual and multilingual scenarios.", "Code availability for reproducibility and further research."], "limitations": "", "keywords": ["Large Language Models", "Data Distribution", "Pruning", "Performance Bias", "Heterogeneous Tasks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2412.12040", "pdf": "https://arxiv.org/pdf/2412.12040.pdf", "abs": "https://arxiv.org/abs/2412.12040", "title": "How Private are Language Models in Abstractive Summarization?", "authors": ["Anthony Hughes", "Ning Ma", "Nikolaos Aletras"], "categories": ["cs.CL"], "comment": null, "summary": "In sensitive domains such as medical and legal, protecting sensitive\ninformation is critical, with protective laws strictly prohibiting the\ndisclosure of personal data. This poses challenges for sharing valuable data\nsuch as medical reports and legal cases summaries. While language models (LMs)\nhave shown strong performance in text summarization, it is still an open\nquestion to what extent they can provide privacy-preserving summaries from\nnon-private source documents. In this paper, we perform a comprehensive study\nof privacy risks in LM-based summarization across two closed- and four\nopen-weight models of different sizes and families. We experiment with both\nprompting and fine-tuning strategies for privacy-preservation across a range of\nsummarization datasets including medical and legal domains. Our quantitative\nand qualitative analysis, including human evaluation, shows that LMs frequently\nleak personally identifiable information in their summaries, in contrast to\nhuman-generated privacy-preserving summaries, which demonstrate significantly\nhigher privacy protection levels. These findings highlight a substantial gap\nbetween current LM capabilities and expert human expert performance in\nprivacy-sensitive summarization tasks.", "AI": {"tldr": "This paper studies the privacy risks in language model (LM)-based summarization, particularly in sensitive domains like medical and legal, finding that LMs often leak personally identifiable information.", "motivation": "To address the challenge of privacy-preserving summarization in sensitive domains where personal data cannot be disclosed.", "method": "A comprehensive study analyzing two closed- and four open-weight language models of various sizes, evaluating both prompting and fine-tuning strategies across multiple summarization datasets in medical and legal fields.", "result": "The study reveals that language models frequently leak personally identifiable information in their summaries, compared to human-generated summaries which offer superior privacy protection.", "conclusion": "There is a significant gap between the privacy preservation capabilities of current language models and those of human experts in sensitive summarization tasks.", "key_contributions": ["Identification of privacy risks in LM-based summarization", "Comparative analysis of LM and human-generated summaries", "Evaluation of prompting and fine-tuning strategies for privacy preservation"], "limitations": "The study is limited to specific language model families and may not encompass all LM architectures or datasets.", "keywords": ["privacy", "summarization", "language models", "medical informatics", "legal informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.12472", "pdf": "https://arxiv.org/pdf/2412.12472.pdf", "abs": "https://arxiv.org/abs/2412.12472", "title": "Knowledge Boundary of Large Language Models: A Survey", "authors": ["Moxin Li", "Yong Zhao", "Wenxuan Zhang", "Shuaiyi Li", "Wenya Xie", "See-Kiong Ng", "Tat-Seng Chua", "Yang Deng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (main)", "summary": "Although large language models (LLMs) store vast amount of knowledge in their\nparameters, they still have limitations in the memorization and utilization of\ncertain knowledge, leading to undesired behaviors such as generating untruthful\nand inaccurate responses. This highlights the critical need to understand the\nknowledge boundary of LLMs, a concept that remains inadequately defined in\nexisting research. In this survey, we propose a comprehensive definition of the\nLLM knowledge boundary and introduce a formalized taxonomy categorizing\nknowledge into four distinct types. Using this foundation, we systematically\nreview the field through three key lenses: the motivation for studying LLM\nknowledge boundaries, methods for identifying these boundaries, and strategies\nfor mitigating the challenges they present. Finally, we discuss open challenges\nand potential research directions in this area. We aim for this survey to offer\nthe community a comprehensive overview, facilitate access to key issues, and\ninspire further advancements in LLM knowledge research.", "AI": {"tldr": "This paper surveys the concept of knowledge boundaries in large language models (LLMs), proposing a formal definition and taxonomy while reviewing methods and strategies for addressing challenges related to these boundaries.", "motivation": "To address the limitations of large language models in storing and utilizing knowledge, which can lead to inaccuracies and undesired behaviors.", "method": "The paper provides a formalized definition of LLM knowledge boundaries and categorizes knowledge into four types, reviewing motivation, methods for identifying boundaries, and strategies for mitigation.", "result": "The survey offers a comprehensive overview of the field, discussing key issues and inspiring advancements in LLM knowledge research.", "conclusion": "Understanding LLM knowledge boundaries is crucial for improving model relevance and accuracy; the survey lays the groundwork for future research.", "key_contributions": ["Proposed a formal definition of LLM knowledge boundaries.", "Introduced a taxonomy categorizing knowledge types in LLMs.", "Systematically reviewed research on knowledge boundaries and mitigation strategies."], "limitations": "The concept of knowledge boundaries in LLMs is still inadequately defined and requires further exploration.", "keywords": ["large language models", "knowledge boundary", "taxonomies", "open challenges", "research directions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.01264", "pdf": "https://arxiv.org/pdf/2501.01264.pdf", "abs": "https://arxiv.org/abs/2501.01264", "title": "ProgCo: Program Helps Self-Correction of Large Language Models", "authors": ["Xiaoshuai Song", "Yanan Wu", "Weixun Wang", "Jiaheng Liu", "Wenbo Su", "Bo Zheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accpeted at ACL2025 Main", "summary": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools. We release our code at https://github.com/songxiaoshuai/progco.", "AI": {"tldr": "This paper introduces Program-driven Self-Correction (ProgCo) for enhancing self-verification and self-refinement in large language models (LLMs) through program-driven mechanisms.", "motivation": "The motivation is to improve the ability of large language models (LLMs) to self-verify and self-refine their responses without external feedback, especially in complex reasoning tasks where they typically struggle.", "method": "The paper proposes two main components: program-driven verification (ProgVe) which creates verification pseudo-programs for logic and validation, and program-driven refinement (ProgRe) which refines both responses and verification programs based on feedback from ProgVe.", "result": "Experimental results on instruction-following and mathematical tasks show that ProgCo effectively enables self-correction in LLMs, with improved performance when combined with real program tools.", "conclusion": "ProgCo represents a significant advancement in the self-correction abilities of LLMs, potentially addressing shortcomings in complex reasoning scenarios.", "key_contributions": ["Introduction of program-driven self-correction mechanism (ProgCo) for LLMs", "Development of ProgVe for complex verification tasks", "Implementation of ProgRe for feedback-driven enhancement of responses"], "limitations": "The approach may be limited by the complexity of reasoning tasks and dependency on the quality of self-generated verification programs.", "keywords": ["large language models", "self-correction", "program verification", "human-computer interaction", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.04962", "pdf": "https://arxiv.org/pdf/2501.04962.pdf", "abs": "https://arxiv.org/abs/2501.04962", "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models", "authors": ["Wenqian Cui", "Xiaoqi Jiao", "Ziqiao Meng", "Irwin King"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The Version of Record of this contribution is accepted to ACL 2025\n  main conference", "summary": "With the rising need for speech-based interaction models, end-to-end Spoken\nLanguage Models (SLMs) have emerged as a promising solution. While these models\nrequire comprehensive world knowledge for meaningful and reliable human\ninteractions, existing question-answering (QA) benchmarks fall short in\nevaluating SLMs' knowledge understanding due to their inability to support\nend-to-end speech evaluation and account for varied input audio conditions. To\naddress these limitations, we present VoxEval, a novel SpeechQA benchmark that\nassesses SLMs' knowledge understanding through pure speech interactions. Our\nbenchmark 1) uniquely maintains speech format for both inputs and outputs, 2)\nevaluates model robustness across diverse input audio conditions, and 3)\npioneers the assessment of complex tasks like mathematical reasoning in spoken\nformat. Systematic evaluation demonstrates that VoxEval presents significant\nchallenges to current SLMs, revealing their sensitivity to varying audio\nconditions and highlighting the need to enhance reasoning capabilities in\nfuture development. We hope this benchmark could guide the advancement of more\nsophisticated and reliable SLMs. VoxEval dataset is available at:\nhttps://github.com/dreamtheater123/VoxEval", "AI": {"tldr": "VoxEval is a new SpeechQA benchmark designed to evaluate Spoken Language Models (SLMs) through speech interactions, addressing gaps in evaluating their understanding and robustness across diverse audio conditions.", "motivation": "The paper addresses the limitation of existing QA benchmarks that do not adequately evaluate the knowledge understanding of Spoken Language Models (SLMs) in real-world speech interactions, particularly across varied input audio conditions.", "method": "Introduction of VoxEval, a novel SpeechQA benchmark that maintains speech input and output formats, assesses model robustness to diverse audio conditions, and evaluates complex tasks in spoken format.", "result": "VoxEval reveals significant challenges for current SLMs, showing their sensitivity to various audio conditions and highlighting deficiencies in their reasoning capabilities.", "conclusion": "The benchmark aims to guide the development of more advanced and reliable Spoken Language Models in the future.", "key_contributions": ["Development of a SpeechQA benchmark that maintains speech interaction for evaluation.", "Evaluation of model robustness across different audio inputs.", "Assessment of complex reasoning tasks in spoken language."], "limitations": "", "keywords": ["Spoken Language Models", "Speech Interaction", "VoxEval", "Benchmark", "Question-Answering"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.17178", "pdf": "https://arxiv.org/pdf/2501.17178.pdf", "abs": "https://arxiv.org/abs/2501.17178", "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost", "authors": ["David Salinas", "Omar Swelam", "Frank Hutter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune the hyperparameters of LLM judges. To alleviate\nthe high cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trade accuracy for cost and\nalso significantly reduce the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility. The code to reproduce our experiments is available at this\nrepository https://github.com/geoalgo/judgetuning .", "AI": {"tldr": "This paper introduces a method to evaluate Large Language Model judges by systematically analyzing and tuning their hyperparameters while reducing cost through multi-objective multi-fidelity approaches.", "motivation": "The need for more efficient and cost-effective evaluation methods for Large Language Models (LLMs) without relying on human annotations.", "method": "Systematic analysis and tuning of hyperparameters of LLM-based judges using multi-objective multi-fidelity techniques to reduce evaluation costs and enhance accuracy.", "result": "The proposed method identifies judges that outperform existing benchmarks in both accuracy and cost-efficiency, utilizing open-weight models for better accessibility and reproducibility.", "conclusion": "This approach not only improves upon existing evaluation methods for LLMs but also provides a reproducible framework for future research.", "key_contributions": ["Systematic hyperparameter analysis for LLM judges", "Introduction of a multi-objective multi-fidelity evaluation method", "Utilization of open-weight models for enhanced accessibility"], "limitations": "", "keywords": ["Large Language Models", "Hyperparameter tuning", "Cost-efficient evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.18922", "pdf": "https://arxiv.org/pdf/2501.18922.pdf", "abs": "https://arxiv.org/abs/2501.18922", "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search", "authors": ["Haoran Luo", "Haihong E", "Yikai Guo", "Qika Lin", "Xiaobao Wu", "Xinyu Mu", "Wenhao Liu", "Meina Song", "Yifan Zhu", "Luu Anh Tuan"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ICML 2025 main conference", "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo. Our code is publicly available.", "AI": {"tldr": "This paper presents KBQA-o1, a novel method for Knowledge Base Question Answering that utilizes Monte Carlo Tree Search to improve performance with limited annotated data.", "motivation": "KBQA struggles with weak KB awareness, and the balance between effectiveness and efficiency, especially with reliance on annotated data in large-scale systems.", "method": "The authors propose an agentic method called KBQA-o1 that incorporates a ReAct-based agent for stepwise logical form generation and employs Monte Carlo Tree Search (MCTS) to optimize exploration and performance.", "result": "KBQA-o1 demonstrates significant improvement, achieving a GrailQA F1 score of 78.5% with limited annotated data, outperforming previous methods like GPT-3.5-turbo, which scored 48.5%.", "conclusion": "The experimental results indicate that KBQA-o1 is effective in enhancing KBQA tasks, making it suitable for low-resource settings and improving model performance with minimal data.", "key_contributions": ["Introduction of a novel agentic approach to KBQA using Monte Carlo Tree Search (MCTS).", "Significant performance improvements in KBQA with limited data, surpassing prior state-of-the-art methods.", "Public availability of the code for community utilization and further research."], "limitations": "", "keywords": ["Knowledge Base Question Answering", "Large Language Models", "Monte Carlo Tree Search"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.09192", "pdf": "https://arxiv.org/pdf/2502.09192.pdf", "abs": "https://arxiv.org/abs/2502.09192", "title": "Thinking beyond the anthropomorphic paradigm benefits LLM research", "authors": ["Lujain Ibrahim", "Myra Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof research articles to present empirical evidence of the prevalence and growth\nof anthropomorphic terminology in research on large language models (LLMs). We\nargue for challenging the deeper assumptions reflected in this terminology --\nwhich, though often useful, may inadvertently constrain LLM development -- and\nbroadening beyond them to open new pathways for understanding and improving\nLLMs. Specifically, we identify and examine five anthropomorphic assumptions\nthat shape research across the LLM development lifecycle. For each assumption\n(e.g., that LLMs must use natural language for reasoning, or that they should\nbe evaluated on benchmarks originally meant for humans), we demonstrate\nempirical, non-anthropomorphic alternatives that remain under-explored yet\noffer promising directions for LLM research and development.", "AI": {"tldr": "The paper discusses the prevalence of anthropomorphic terminology in research on large language models (LLMs) and argues for broadening the assumptions underpinning this terminology to improve LLM development.", "motivation": "To analyze the impact of anthropomorphism in LLM research and challenge existing assumptions that may hinder progress in the field.", "method": "The authors conducted a comprehensive analysis of hundreds of thousands of research articles to identify the use of anthropomorphic language and its implications.", "result": "The paper finds that anthropomorphic terminology is widespread in LLM research, impacting various assumptions that influence how LLMs are developed and evaluated.", "conclusion": "The authors advocate for non-anthropomorphic alternatives and reforms to improve understanding and development of LLMs, proposing five key areas for further exploration.", "key_contributions": ["Identification of prevalent anthropomorphic assumptions in LLM research.", "Empirical evidence challenging the use of anthropomorphic terminology.", "Proposal of non-anthropomorphic alternatives for evaluating LLMs."], "limitations": "", "keywords": ["Anthropomorphism", "Large Language Models", "Human-Computer Interaction", "Research Analysis", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11198", "pdf": "https://arxiv.org/pdf/2502.11198.pdf", "abs": "https://arxiv.org/abs/2502.11198", "title": "ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition", "authors": ["Bidyarthi Paul", "Faika Fairuj Preotee", "Shuvashis Sarker", "Shamim Rahim Refat", "Shifat Islam", "Tashreef Muhammad", "Mohammad Ashraful Hoque", "Shahriar Manzoor"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Named Entity Recognition (NER) in regional dialects is a critical yet\nunderexplored area in Natural Language Processing (NLP), especially for\nlow-resource languages like Bangla. While NER systems for Standard Bangla have\nmade progress, no existing resources or models specifically address the\nchallenge of regional dialects such as Barishal, Chittagong, Mymensingh,\nNoakhali, and Sylhet, which exhibit unique linguistic features that existing\nmodels fail to handle effectively. To fill this gap, we introduce ANCHOLIK-NER,\nthe first benchmark dataset for NER in Bangla regional dialects, comprising\n17,405 sentences distributed across five regions. The dataset was sourced from\npublicly available resources and supplemented with manual translations,\nensuring alignment of named entities across dialects. We evaluate three\ntransformer-based models - Bangla BERT, Bangla BERT Base, and BERT Base\nMultilingual Cased - on this dataset. Our findings demonstrate that BERT Base\nMultilingual Cased performs best in recognizing named entities across regions,\nwith significant performance observed in Mymensingh with an F1-score of\n82.611%. Despite strong overall performance, challenges remain in region like\nChittagong, where the models show lower precision and recall. Since no previous\nNER systems for Bangla regional dialects exist, our work represents a\nfoundational step in addressing this gap. Future work will focus on improving\nmodel performance in underperforming regions and expanding the dataset to\ninclude more dialects, enhancing the development of dialect-aware NER systems.", "AI": {"tldr": "ANCHOLIK-NER is the first benchmark dataset for Named Entity Recognition (NER) in Bangla regional dialects, addressing gaps in NLP for low-resource languages.", "motivation": "There is a significant need for NER systems in Bangla regional dialects, as current models do not effectively handle the unique linguistic features of these dialects.", "method": "A new dataset comprising 17,405 sentences across five Bangla regional dialects was created, and three transformer-based models were evaluated on this dataset.", "result": "BERT Base Multilingual Cased performed best, especially in Mymensingh with an F1-score of 82.611%, although performance in Chittagong was lower.", "conclusion": "This study provides a foundational step towards improving NER systems for regional dialects in Bangla, with future work aimed at enhancing model performance in underperforming regions and expanding the dataset.", "key_contributions": ["Introduction of the first benchmark dataset for NER in Bangla regional dialects", "Evaluation of three transformer-based NER models on the new dataset", "Identification of performance gaps in specific regions like Chittagong."], "limitations": "The dataset is limited to five regions and further expansion is needed to include more dialects and improve model performance.", "keywords": ["Named Entity Recognition", "Bangla", "Dialect", "NLP", "Transformer models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.12134", "pdf": "https://arxiv.org/pdf/2502.12134.pdf", "abs": "https://arxiv.org/abs/2502.12134", "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "categories": ["cs.CL"], "comment": "Camera-ready for ACL 2025 (main conference)", "summary": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nrequire full-model fine-tuning and suffer from catastrophic forgetting,\nlimiting their applicability to state-of-the-art LLMs that already perform well\nin zero-shot settings with a proper instruction. To address this challenge, we\npropose a novel approach for continuous-space reasoning that does not require\nmodifying the LLM. Specifically, we employ a lightweight fixed assistant model\nto speculatively generate instance-specific soft thought tokens as the initial\nchain of thoughts, which are then mapped into the LLM's representation space\nvia a trainable projection module. Experimental results on five reasoning\nbenchmarks demonstrate that our method enhances LLM reasoning performance\nthrough supervised, parameter-efficient fine-tuning. Source code is available\nat https://github.com/xuyige/SoftCoT.", "AI": {"tldr": "This paper presents a method for enhancing reasoning performance in Large Language Models (LLMs) through continuous-space reasoning without modifying the LLM itself, using a lightweight assistant model.", "motivation": "Existing methods for Chain-of-Thought reasoning in LLMs face limitations due to hard token decoding and the need for full model fine-tuning, which restricts applicability.", "method": "The proposed approach uses a lightweight fixed assistant model to generate soft thought tokens, which are then mapped into the LLM's representation space via a trainable projection module.", "result": "Experimental results on five reasoning benchmarks show that the method improves LLM reasoning performance through supervised fine-tuning that is parameter-efficient.", "conclusion": "The approach effectively enhances LLM reasoning capabilities without the need for extensive modifications to existing models, making it practical for state-of-the-art LLM applications.", "key_contributions": ["Introduces a lightweight fixed assistant model for generating soft thought tokens.", "Demonstrates improved reasoning performance on multiple benchmarks without requiring full-model fine-tuning.", "Provides a parameter-efficient fine-tuning approach for large language models."], "limitations": "The method may still depend on the quality and capability of the fixed assistant model used for generating soft thought tokens.", "keywords": ["Chain-of-Thought", "Large Language Models", "continuous-space reasoning", "soft thought tokens", "parameter-efficient fine-tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.12187", "pdf": "https://arxiv.org/pdf/2502.12187.pdf", "abs": "https://arxiv.org/abs/2502.12187", "title": "Hallucinations are inevitable but can be made statistically negligible. The \"innate\" inevitability of hallucinations cannot explain practical LLM issues", "authors": ["Atsushi Suzuki", "Yulan He", "Feng Tian", "Zhongyuan Wang"], "categories": ["cs.CL", "cs.FL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Hallucinations, a phenomenon where a language model (LM) generates nonfactual\ncontent, pose a significant challenge to the practical deployment of LMs. While\nmany empirical methods have been proposed to mitigate hallucinations, recent\nstudies established a computability-theoretic result showing that any LM will\ninevitably generate hallucinations on an infinite set of inputs, regardless of\nthe quality and quantity of training datasets and the choice of the language\nmodel architecture and training and inference algorithms. Although the\ncomputability-theoretic result may seem pessimistic, its significance in\npractical viewpoints has remained unclear. This paper claims that those\n\"innate\" inevitability results from computability theory and diagonal argument,\nin principle, cannot explain practical issues of LLMs. We demonstrate this\nclaim by presenting a positive theoretical result from a probabilistic\nperspective. Specifically, we prove that hallucinations can be made\nstatistically negligible, provided that the quality and quantity of the\ntraining data are sufficient. Interestingly, our positive result coexists with\nthe computability-theoretic result, implying that while hallucinations on an\ninfinite set of inputs cannot be entirely eliminated, their probability can\nalways be reduced by improving algorithms and training data. By evaluating the\ntwo seemingly contradictory results through the lens of information theory, we\nargue that our probability-theoretic positive result better reflects practical\nconsiderations than the computability-theoretic negative result.", "AI": {"tldr": "This paper analyzes hallucinations in language models (LMs), showing that while LMs will invariably generate nonfactual content on an infinite input set, improvements in training data and algorithms can significantly reduce the probability of hallucinations, emphasizing a probabilistic approach over a computability-theoretic perspective.", "motivation": "To clarify the practical implications of hallucinations in LMs and to challenge the pessimistic views established by computability theory.", "method": "The authors use a probabilistic approach to demonstrate that with sufficient quality and quantity of training data, hallucinations can be made statistically negligible.", "result": "The study finds that while hallucinations are inevitable in a mathematical sense, their occurrence can be reduced through improved training methodologies.", "conclusion": "The probabilistic results provided in the study suggest that practical strategies can mitigate hallucinations more effectively than the theoretical bounds imply.", "key_contributions": ["Demonstration that training data quality can reduce hallucinations", "Establishment of a positive theoretical result from a probabilistic viewpoint", "Analysis of contradictions between computability and information theory"], "limitations": "", "keywords": ["language models", "hallucinations", "probability theory", "computability theory", "training data"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.12953", "pdf": "https://arxiv.org/pdf/2502.12953.pdf", "abs": "https://arxiv.org/abs/2502.12953", "title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text", "authors": ["Andrei Jarca", "Florinel Alin Croitoru", "Radu Tudor Ionescu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025", "summary": "Masked language modeling has become a widely adopted unsupervised technique\nto pre-train large language models (LLMs). However, the process of selecting\ntokens for masking is random, and the percentage of masked tokens is typically\nfixed for the entire training process. In this paper, we propose to adjust the\nmasking ratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.", "AI": {"tldr": "This paper introduces a novel masking technique for pre-training large language models, proposing a task-informed anti-curriculum learning approach that adjusts the masking ratio dynamically.", "motivation": "To improve the effectiveness of masked language modeling by tailoring the masking process to specific downstream tasks to enhance model focus on relevant features.", "method": "The authors propose a novel task-informed anti-curriculum masking method (TIACBM) that adjusts the masking ratio over time and utilizes task-specific knowledge to select tokens for masking.", "result": "The TIACBM approach demonstrates statistically significant performance improvements in three tasks: sentiment analysis, text classification by topic, and authorship attribution.", "conclusion": "The findings support that adjusting the masking strategy based on task knowledge improves model performance and highlights the importance of focusing on relevant features during training.", "key_contributions": ["Introduces a task-informed anti-curriculum learning scheme for token masking.", "Proposes a cyclic decaying masking ratio for enhanced training dynamics.", "Demonstrates significant performance improvements on multiple NLP tasks."], "limitations": "", "keywords": ["language modeling", "masking", "anti-curriculum learning", "text classification", "sentiment analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13010", "pdf": "https://arxiv.org/pdf/2502.13010.pdf", "abs": "https://arxiv.org/abs/2502.13010", "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge", "authors": ["Mohammad Reza Rezaei", "Reza Saadati Fard", "Rahul G. Krishnan", "Milad Lankarany"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.", "AI": {"tldr": "AMG-RAG framework automates medical knowledge graph construction and updating, enhancing medical question-answering.", "motivation": "Address the challenges of rapidly evolving medical knowledge and the manual process of updating resources for reliable medical question-answering.", "method": "Introduces the AMG-RAG framework that automates the construction and continuous updating of medical knowledge graphs while integrating reasoning and retrieving external evidence.", "result": "AMG-RAG achieved an F1 score of 74.1 on MEDQA and an accuracy of 66.34 on MEDMCQA, outperforming larger models without increased computational overhead.", "conclusion": "Automated knowledge graph generation and retrieval methods are critical for providing up-to-date medical insights.", "key_contributions": ["Introduction of a dynamic framework for updating medical knowledge graphs", "Integration of reasoning and external evidence retrieval", "Improved accuracy and interpretability in medical queries"], "limitations": "", "keywords": ["Large Language Models", "Medical Knowledge Graphs", "Question-Answering", "Automated Framework", "Evidence Retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14132", "pdf": "https://arxiv.org/pdf/2502.14132.pdf", "abs": "https://arxiv.org/abs/2502.14132", "title": "Can Community Notes Replace Professional Fact-Checkers?", "authors": ["Nadav Borenstein", "Greta Warren", "Desmond Elliott", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main proceedings of ACL 2025", "summary": "Two commonly employed strategies to combat the rise of misinformation on\nsocial media are (i) fact-checking by professional organisations and (ii)\ncommunity moderation by platform users. Policy changes by Twitter/X and, more\nrecently, Meta, signal a shift away from partnerships with fact-checking\norganisations and towards an increased reliance on crowdsourced community\nnotes. However, the extent and nature of dependencies between fact-checking and\nhelpful community notes remain unclear. To address these questions, we use\nlanguage models to annotate a large corpus of Twitter/X community notes with\nattributes such as topic, cited sources, and whether they refute claims tied to\nbroader misinformation narratives. Our analysis reveals that community notes\ncite fact-checking sources up to five times more than previously reported.\nFact-checking is especially crucial for notes on posts linked to broader\nnarratives, which are twice as likely to reference fact-checking sources\ncompared to other sources. Our results show that successful community\nmoderation relies on professional fact-checking and highlight how citizen and\nprofessional fact-checking are deeply intertwined.", "AI": {"tldr": "The paper explores the relationship between professional fact-checking and community moderation on Twitter/X, showing that community notes heavily rely on fact-checking sources.", "motivation": "To study the dependencies between fact-checking organizations and community moderation in the context of misinformation on social media platforms like Twitter/X.", "method": "The authors utilized language models to annotate a large corpus of community notes from Twitter/X, focusing on topics, sources cited, and their relationships to misinformation narratives.", "result": "The analysis indicated that community notes cite fact-checking sources up to five times more than previously reported, especially when related to broader misinformation narratives.", "conclusion": "The findings demonstrate that community moderation is significantly enhanced by professional fact-checking, revealing the interconnectedness of citizen and professional fact-checking.", "key_contributions": ["Demonstrated the heavy reliance of community notes on fact-checking sources.", "Provided quantitative insights into the citation frequency of fact-checking in community notes.", "Highlighted the interdependence of professional and citizen-led fact-checking efforts."], "limitations": "", "keywords": ["misinformation", "fact-checking", "community moderation", "social media", "language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.14613", "pdf": "https://arxiv.org/pdf/2502.14613.pdf", "abs": "https://arxiv.org/abs/2502.14613", "title": "Behavioral Analysis of Information Salience in Large Language Models", "authors": ["Jan Trienes", "JÃ¶rg SchlÃ¶tterer", "Junyi Jessy Li", "Christin Seifert"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience.", "AI": {"tldr": "This paper proposes an explainable framework to explore information salience in Large Language Models (LLMs) through their summarization behavior, revealing a hierarchical notion of salience consistent across various models.", "motivation": "To better understand how LLMs determine the importance of information during text summarization, as the salience these models use is not well understood.", "method": "The authors employ length-controlled summarization as a behavioral probe and trace the answerability of Questions Under Discussion across different models and datasets to derive a proxy for information salience.", "result": "Experimental results show that LLMs exhibit a hierarchical notion of salience that is generally consistent across different model families and sizes, although this notion is not accessible through introspection.", "conclusion": "While LLMs display consistent salience patterns in their summarization behavior, these patterns do not align strongly with human perceptions of salience, suggesting limitations in current interpretability methods.", "key_contributions": ["Introduction of a framework to analyze information salience in LLMs", "Use of length-controlled summarization to probe content selection", "Demonstration of hierarchical salience in LLMs across multiple models"], "limitations": "The derived salience cannot be accessed through introspection and shows weak correlation with human perceptions of information importance.", "keywords": ["Large Language Models", "text summarization", "information salience", "explainable AI", "model interpretability"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.17817", "pdf": "https://arxiv.org/pdf/2502.17817.pdf", "abs": "https://arxiv.org/abs/2502.17817", "title": "Predicting Through Generation: Why Generation Is Better for Prediction", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Prakash Bhat", "Chun-Nam Yu", "Mojtaba Soltanalian", "Ivan Garibay", "Ozlem Garibay", "Chen Chen", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": "ACL Accepted paper", "summary": "This paper argues that generating output tokens is more effective than using\npooled representations for prediction tasks because token-level generation\nretains more mutual information. Since LLMs are trained on massive text corpora\nusing next-token prediction, generation aligns naturally with their learned\nbehavior. Using the Data Processing Inequality (DPI), we provide both\ntheoretical and empirical evidence supporting this claim. However,\nautoregressive models face two key challenges when used for prediction: (1)\nexposure bias, where the model sees ground truth tokens during training but\nrelies on its own predictions during inference, leading to errors, and (2)\nformat mismatch, where discrete tokens do not always align with the tasks\nrequired output structure. To address these challenges, we introduce\nPredGen(Predicting Through Generating), an end to end framework that (i) uses\nscheduled sampling to reduce exposure bias, and (ii) introduces a task adapter\nto convert the generated tokens into structured outputs. Additionally, we\nintroduce Writer-Director Alignment Loss (WDAL), which ensures consistency\nbetween token generation and final task predictions, improving both text\ncoherence and numerical accuracy. We evaluate PredGen on multiple\nclassification and regression benchmarks. Our results show that PredGen\nconsistently outperforms standard baselines, demonstrating its effectiveness in\nstructured prediction tasks.", "AI": {"tldr": "This paper presents PredGen, an end-to-end framework that improves prediction tasks by generating output tokens, addressing issues like exposure bias and format mismatch.", "motivation": "The paper argues that token-level generation is more effective than pooled representations due to retained mutual information, aligning with how LLMs are trained.", "method": "PredGen uses scheduled sampling to mitigate exposure bias and incorporates a task adapter for converting generated tokens into structured outputs. It also introduces Writer-Director Alignment Loss (WDAL) for consistency.", "result": "PredGen consistently outperforms standard baselines on multiple classification and regression benchmarks, demonstrating improved performance in structured prediction tasks.", "conclusion": "The proposed framework effectively addresses the challenges faced by autoregressive models, enhancing the prediction capabilities of LLMs.", "key_contributions": ["Introduces PredGen framework for improved structured prediction", "Uses scheduled sampling to reduce exposure bias", "Introduces Writer-Director Alignment Loss (WDAL) to enhance coherence and accuracy"], "limitations": "", "keywords": ["Prediction Tasks", "Token Generation", "Machine Learning"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2502.18874", "pdf": "https://arxiv.org/pdf/2502.18874.pdf", "abs": "https://arxiv.org/abs/2502.18874", "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework", "authors": ["Kaishuai Xu", "Tiezheng Yu", "Wenjun Hou", "Yi Cheng", "Liangyou Li", "Xin Jiang", "Lifeng Shang", "Qun Liu", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted as ACL 2025 findings", "summary": "Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities.", "AI": {"tldr": "ARJudge is a novel evaluation framework for Large Language Models that improves upon traditional evaluators by combining text-based and code-driven analyses to create adaptive evaluation criteria and robust judgments.", "motivation": "Current methods for evaluating LLM responses are limited by predefined criteria and often fail to adapt to unseen instructions, prompting the need for a more flexible approach.", "method": "ARJudge consists of an Analyzer that generates comprehensive evaluation analyses and a Refiner that combines these analyses to produce final judgments, using a Composite Analysis Corpus for training.", "result": "ARJudge demonstrated superior effectiveness and robustness over existing fine-tuned evaluators in LLM evaluation tasks.", "conclusion": "The framework highlights the value of integrated multi-faceted evaluations and code-driven analyses in enhancing LLM evaluation capabilities.", "key_contributions": ["Introduction of ARJudge, a framework for adaptive evaluation of LLM responses", "Utilization of both text-based and code-driven analyses for comprehensive evaluations", "Development of a Composite Analysis Corpus for training the evaluation components."], "limitations": "", "keywords": ["Large Language Models", "evaluation framework", "ARJudge", "multi-faceted analysis", "code-driven evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.19249", "pdf": "https://arxiv.org/pdf/2502.19249.pdf", "abs": "https://arxiv.org/abs/2502.19249", "title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases", "authors": ["Michael Y. Hu", "Jackson Petty", "Chuan Shi", "William Merrill", "Tal Linzen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Camera Ready", "summary": "Pretraining language models on formal language can improve their acquisition\nof natural language. Which features of the formal language impart an inductive\nbias that leads to effective transfer? Drawing on insights from linguistics and\ncomplexity theory, we hypothesize that effective transfer occurs when two\nconditions are met: the formal language should capture the dependency\nstructures present in natural language, and it should remain within the\ncomputational limitations of the model architecture. We experiment with\npre-pretraining (training on formal language before natural languages) on\ntransformers and find that formal languages capturing hierarchical dependencies\nindeed enable language models to achieve lower loss on natural language and\nbetter linguistic generalization compared to other formal languages. We also\nfind modest support for the hypothesis that the formal language should fall\nwithin the computational limitations of the architecture. Strikingly,\npre-pretraining reduces loss more efficiently than training on a matched amount\nof natural language. For a 1B-parameter language model trained on roughly 1.6B\ntokens of natural language, pre-pretraining achieves the same loss and better\nlinguistic generalization with a 33% smaller token budget. Finally, we also\ngive mechanistic evidence of transfer from formal to natural language:\nattention heads acquired during pre-pretraining remain crucial for the model's\nperformance on syntactic evaluations.", "AI": {"tldr": "This paper investigates the effects of pretraining language models on formal languages, particularly their impact on the transfer of learning to natural languages.", "motivation": "The study aims to identify which features of formal languages facilitate effective transfer to natural language acquisition in language models.", "method": "The research involves pre-pretraining transformers on formal languages that capture hierarchical dependencies before transitioning to natural languages.", "result": "Experiments show that models pretrained on formal languages achieve lower loss and better linguistic generalization in natural language tasks, with pre-pretraining being more efficient than direct training on comparable natural language data.", "conclusion": "The findings suggest that utilizing formal languages with hierarchical structures can enhance the training efficiency and performance of language models on natural language, highlighting the importance of the model architecture's computational capabilities.", "key_contributions": ["Demonstrated lower loss and better generalization through pre-pretraining.", "Provided mechanistic evidence linking formal language training to improvements in natural language tasks.", "Highlighted the efficiency gains in training with formal languages compared to natural language."], "limitations": "The study primarily investigates one type of formal language and relies on the specific architecture of the transformers used; generalizability to other models is uncertain.", "keywords": ["formal language", "transfer learning", "language models", "pretraining", "hierarchical dependencies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.20795", "pdf": "https://arxiv.org/pdf/2502.20795.pdf", "abs": "https://arxiv.org/abs/2502.20795", "title": "Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models", "authors": ["Kuang-Da Wang", "Teng-Ruei Chen", "Yu Heng Hung", "Guo-Xun Ko", "Shuoyang Ding", "Yueh-Hua Wu", "Yu-Chiang Frank Wang", "Chao-Han Huck Yang", "Wen-Chih Peng", "Ping-Chun Hsieh"], "categories": ["cs.CL"], "comment": "Preprint. Code will be released at Plan2Align GitHub link:\n  https://github.com/NYCU-RL-Bandits-Lab/Plan2Align", "summary": "Aligning Large Language Models with Preference Fine-Tuning is often\nresource-intensive. Test-time alignment techniques that do not modify the\nunderlying models, such as prompting and guided decodings, offer a lightweight\nalternative. However, existing test-time alignment methods primarily improve\nshort responses and fail to ensure coherence over extended contexts due to the\nmyopic nature of token-level alignment. Moreover, these methods often incur a\nslowdown during inference. To address these challenges, we propose Plan2Align,\na test-time alignment framework that formulates text generation as a predictive\nplanning problem. Plan2Align adapts Model Predictive Control (MPC) to\niteratively refine output by rolling out multiple complete responses and\noptimizing each segment. To more rigorously evaluate the effectiveness and\nefficiency, we focus on the more challenging task of long-text generation.\nExperiments on the long-form response subset of the HH-RLHF dataset and the\nWMT'24 Discourse-Level Literary Translation demonstrate that Plan2Align\nsignificantly enhances the performance of base LLMs. Compared to existing\ntraining-time and test-time alignment methods on LLaMA-3.1 8B, Plan2Align\nachieves comparable or superior results, while also delivering improved\ninference efficiency relative to prior test-time alignment approaches.", "AI": {"tldr": "Plan2Align is a test-time alignment framework for long-text generation that uses predictive planning to optimize responses without modifying underlying models.", "motivation": "Existing test-time alignment methods fail to ensure coherence in long responses and slow down inference, necessitating a more efficient solution.", "method": "Plan2Align formulates text generation as a predictive planning problem using Model Predictive Control (MPC) to iteratively refine outputs by optimizing complete responses.", "result": "Experiments show that Plan2Align significantly enhances performance in long-text generation compared to existing methods, achieving superior results with improved inference efficiency.", "conclusion": "Plan2Align provides a lightweight and effective alternative for aligning LLMs during inference, particularly for long-form text generation.", "key_contributions": ["Introduces predictive planning for text generation", "Improves coherence and efficiency in long-text alignment", "Demonstrates superior performance on long-form datasets"], "limitations": "", "keywords": ["Large Language Models", "Text Generation", "Predictive Planning", "Test-time Alignment", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20859", "pdf": "https://arxiv.org/pdf/2502.20859.pdf", "abs": "https://arxiv.org/abs/2502.20859", "title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents", "authors": ["Yifan Duan", "Yihong Tang", "Xuefeng Bai", "Kehai Chen", "Juntao Li", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) \\textit{How do personality traits affect\nproblem-solving in closed tasks?} (2) \\textit{How do traits shape creativity in\nopen tasks?} (3) \\textit{How does single-agent performance influence\nmulti-agent collaboration?} By assigning Big Five personality traits to LLM\nagents and evaluating their performance in single- and multi-agent settings, we\nreveal that specific traits significantly influence reasoning accuracy (closed\ntasks) and creative output (open tasks). Furthermore, multi-agent systems\nexhibit collective intelligence distinct from individual capabilities, driven\nby distinguishing combinations of personalities.", "AI": {"tldr": "This paper investigates the influence of personality traits on the performance of large language models in problem-solving and creativity, revealing insights into their capabilities in both individual and multi-agent settings.", "motivation": "To explore how LLM capabilities align with human intelligence by examining the role of personality traits in task performance.", "method": "The research assigns Big Five personality traits to LLM agents and evaluates their performance across single- and multi-agent tasks.", "result": "Specific personality traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks, while multi-agent systems manifest a collective intelligence based on personality combinations.", "conclusion": "Understanding the role of personality in LLMs provides better insights into their performance, revealing their potential for simulation of human-like intelligence in various tasks.", "key_contributions": ["Systematic investigation of LLM performance through the lens of human personality traits.", "Demonstrated influence of personality traits on reasoning and creativity tasks.", "New insights into the collective intelligence of multi-agent LLM systems."], "limitations": "", "keywords": ["large language models", "personality traits", "problem-solving", "creativity", "multi-agent systems"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.20968", "pdf": "https://arxiv.org/pdf/2502.20968.pdf", "abs": "https://arxiv.org/abs/2502.20968", "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Jiahe Guo", "Xingyu Sui", "Xinyang Han", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "categories": ["cs.CL"], "comment": "To appear at ACL 2025 (Main)", "summary": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs.", "AI": {"tldr": "This paper assesses safety risks in role-play fine-tuning of LLMs and proposes a method called Safety-Aware Role-Play Fine-Tuning (SaRFT) to mitigate those risks.", "motivation": "To address the safety risks associated with role-playing in large language models, especially concerning villainous characters.", "method": "The authors trained 95 role-specific LLMs using RoleBench and assessed the impact of role-play fine-tuning on safety performance.", "result": "The experiments showed a noticeable decline in safety performance due to role-play fine-tuning, with variations based on character traits. SaRFT was shown to consistently outperform state-of-the-art techniques under various fine-tuning conditions.", "conclusion": "There is a critical need for implementing role-adaptive safety measures in role-playing LLMs to minimize safety risks.", "key_contributions": ["First comprehensive assessment of role-play fine-tuning risks in LLMs.", "Introduction of Safety-Aware Role-Play Fine-Tuning (SaRFT) method.", "Demonstration of enhanced safety performance while maintaining role adaptability."], "limitations": "The study focuses primarily on character traits and may not encompass all potential safety risks in every role-playing scenario.", "keywords": ["role-playing", "large language models", "safety risks", "fine-tuning", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.03205", "pdf": "https://arxiv.org/pdf/2503.03205.pdf", "abs": "https://arxiv.org/abs/2503.03205", "title": "MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving", "authors": ["Ruida Wang", "Rui Pan", "Yuxin Li", "Jipeng Zhang", "Yizhen Jia", "Shizhe Diao", "Renjie Pi", "Junjie Hu", "Tong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted the mathematical and computer science communities.\nState-of-the-art methods utilize a single Large Language Model (LLM) to\ngenerate complete proof or perform tree search, but they fail to balance these\ntasks. We propose **MA-LoT**: *Model-CollAboration Lean-based Long\nChain-of-Thought*, a comprehensive framework for Lean4 theorem proving to solve\nthis issue. It separates the cognition tasks of general NL for whole-proof\ngeneration and error analysis for proof correction using the\nmodel-collaboration method. We achieve this by structured interaction of the\nLLM and Lean4 verifier in Long CoT. To implement the framework, we propose the\nnovel *LoT-Transfer Learning* training-inference pipeline, which enables the\nLong CoT thinking capability to LLMs without special data annotation. Extensive\nexperiment shows that our framework achieves a **61.07%** accuracy rate on the\nLean4 version of the MiniF2F-Test dataset, largely outperforming DeepSeek-V3\n(33.61%), single-model tree search (InternLM-Step-Prover, 50.70%), and\nwhole-proof generation (Godel-Prover, 55.33%) baselines. Furthermore, our\nfindings highlight the potential of combining Long CoT with formal verification\nfor a more insightful generation in a broader perspective.", "AI": {"tldr": "The paper presents MA-LoT, a framework that enhances Lean4 theorem proving by separating tasks of proof generation and error analysis, achieving a significant accuracy improvement.", "motivation": "To address the limitations of existing LLM methods in theorem proving, particularly the lack of balance between proof generation and error checking.", "method": "The MA-LoT framework utilizes a model-collaboration approach with structured interaction between an LLM and a Lean4 verifier, integrating Long Chain-of-Thought processes.", "result": "The framework achieves a 61.07% accuracy on the Lean4 MiniF2F-Test dataset, surpassing other models significantly in both whole-proof generation and proof correction tasks.", "conclusion": "MA-LoT demonstrates the effectiveness of combining Long CoT and formal verification methods for improved theorem proving in Lean4.", "key_contributions": ["Introduction of the MA-LoT framework for Lean4 theorem proving", "Implementation of LoT-Transfer Learning for enhanced LLM capabilities", "Demonstrated performance improvement over existing methods in theorem proving tasks"], "limitations": "", "keywords": ["theorem proving", "Machine Learning", "Large Language Models", "Lean4", "error analysis"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2503.04615", "pdf": "https://arxiv.org/pdf/2503.04615.pdf", "abs": "https://arxiv.org/abs/2503.04615", "title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!", "authors": ["Ashok Urlana", "Gopichand Kanumolu", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati", "Rahul Mishra"], "categories": ["cs.CL"], "comment": "30 pages, 3 figures", "summary": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets.", "AI": {"tldr": "HalluCounter is a new method for hallucination detection in language models that utilizes consistency patterns and introduces a benchmark dataset for evaluation.", "motivation": "To address the limitations of existing hallucination detection methods that rely on internal model states and the lack of diverse benchmark datasets.", "method": "HalluCounter employs response-response and query-response consistency and alignment patterns to train a classifier for hallucination detection.", "result": "HalluCounter achieves over 90% average confidence in hallucination detection, outperforming state-of-the-art methods.", "conclusion": "The proposed method and the accompanying benchmark dataset significantly improve hallucination detection accuracy and flexibility.", "key_contributions": ["Introduction of HalluCounter for reference-free hallucination detection", "Use of consistency patterns for improved accuracy", "Development of HalluCounterEval dataset for benchmark evaluation"], "limitations": "", "keywords": ["hallucination detection", "language models", "consistency patterns", "benchmark dataset", "AI"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2503.09598", "pdf": "https://arxiv.org/pdf/2503.09598.pdf", "abs": "https://arxiv.org/abs/2503.09598", "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation", "authors": ["Ruohao Guo", "Wei Xu", "Alan Ritter"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world interactions. We curated EchoMist, the\nfirst comprehensive benchmark for implicit misinformation, where false\nassumptions are embedded in the query to LLMs. EchoMist targets circulated,\nharmful, and ever-evolving implicit misinformation from diverse sources,\nincluding realistic human-AI conversations and social media interactions.\nThrough extensive empirical studies on 15 state-of-the-art LLMs, we find that\ncurrent models perform alarmingly poorly on this task, often failing to detect\nfalse premises and generating counterfactual explanations. We also investigate\ntwo mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability\nto counter implicit misinformation. Our findings indicate that EchoMist remains\na persistent challenge and underscore the critical need to safeguard against\nthe risk of implicit misinformation.", "AI": {"tldr": "This paper introduces EchoMist, a benchmark for assessing Large Language Models' ability to handle implicit misinformation, revealing significant shortcomings in current models and proposing methods to improve performance.", "motivation": "The widespread deployment of Large Language Models raises concerns about their potential to tacitly spread implicit misinformation, a critical issue not adequately addressed by existing evaluations.", "method": "The authors curated EchoMist, a benchmark containing queries with embedded false assumptions, and conducted empirical studies on 15 state-of-the-art LLMs to evaluate their performance in detecting implicit misinformation.", "result": "The study finds that current LLMs perform poorly on detecting false premises, frequently generating incorrect counterfactual explanations.", "conclusion": "The findings highlight the ongoing challenge of implicit misinformation and point to the necessity of developing safeguards within LLMs.", "key_contributions": ["Introduction of the EchoMist benchmark for implicit misinformation", "Empirical analysis of 15 state-of-the-art LLMs' performance on implicit misinformation", "Investigation of mitigation strategies (Self-Alert and RAG)"], "limitations": "The study primarily focuses on implicit misinformation without delving deeply into the specific contexts and nuances of various misinformation sources.", "keywords": ["Large Language Models", "implicit misinformation", "EchoMist", "mitigation methods", "empirical studies"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.11985", "pdf": "https://arxiv.org/pdf/2503.11985.pdf", "abs": "https://arxiv.org/abs/2503.11985", "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models", "authors": ["Charaka Vinayak Kumar", "Ashok Urlana", "Gopichand Kanumolu", "Bala Mallikarjunarao Garlapati", "Pruthwik Mishra"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 1 figure", "summary": "Advancements in Large Language Models (LLMs) have increased the performance\nof different natural language understanding as well as generation tasks.\nAlthough LLMs have breached the state-of-the-art performance in various tasks,\nthey often reflect different forms of bias present in the training data. In the\nlight of this perceived limitation, we provide a unified evaluation of\nbenchmarks using a set of representative small and medium-sized LLMs that cover\ndifferent forms of biases starting from physical characteristics to\nsocio-economic categories. Moreover, we propose five prompting approaches to\ncarry out the bias detection task across different aspects of bias. Further, we\nformulate three research questions to gain valuable insight in detecting biases\nin LLMs using different approaches and evaluation metrics across benchmarks.\nThe results indicate that each of the selected LLMs suffer from one or the\nother form of bias with the Phi-3.5B model being the least biased. Finally, we\nconclude the paper with the identification of key challenges and possible\nfuture directions.", "AI": {"tldr": "This paper evaluates biases in various LLMs and proposes methods for bias detection.", "motivation": "To address the limitations of LLMs reflecting biases from training data, the study aims to unify the evaluation of LLM biases and provide insights for mitigation.", "method": "A set of representative small and medium-sized LLMs were evaluated using five different prompting approaches across various benchmarks to detect biases.", "result": "The evaluation showed that all selected LLMs exhibit some form of bias, with the Phi-3.5B model identified as the least biased.", "conclusion": "The paper highlights key challenges in bias detection in LLMs and suggests future research directions in this domain.", "key_contributions": ["Unified evaluation of biases in LLMs", "Proposed five new prompting approaches for bias detection", "Insightful research questions formulated to guide future studies on biases."], "limitations": "The study is limited to the selected models and may not represent biases across all LLMs.", "keywords": ["Large Language Models", "Bias detection", "Prompting approaches", "Natural language understanding", "Evaluation metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.13857", "pdf": "https://arxiv.org/pdf/2503.13857.pdf", "abs": "https://arxiv.org/abs/2503.13857", "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations", "authors": ["Rui Yang", "Jiayi Tong", "Haoyuan Wang", "Hui Huang", "Ziyang Hu", "Peiyu Li", "Nan Liu", "Christopher J. Lindsell", "Michael J. Pencina", "Yong Chen", "Chuan Hong"], "categories": ["cs.CL"], "comment": "30 pages, 6 figures", "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources.", "AI": {"tldr": "A framework, AutoConfidence, is proposed for predicting preprint publication through automated data extraction and multiple predictive features.", "motivation": "To address the challenges posed by variable quality of preprints in systematic reviews by providing an automated solution for confidence assessment.", "method": "Leveraging natural language processing for automated data extraction, using semantic embeddings of titles and abstracts, and applying LLM-driven evaluation scores with two models: a random forest classifier and a survival cure model.", "result": "The random forest classifier achieved AUROC scores improving from 0.692 to 0.747, while the survival cure model improved from AUROC 0.716 to 0.731 with advanced features.", "conclusion": "AutoConfidence enhances predictive performance in preprint publication prediction and reduces the manual curation burden, aiding systematic reviews.", "key_contributions": ["Introduction of AutoConfidence for predicting preprint publication quality", "Utilization of natural language processing for automated data extraction", "Integration of semantic embeddings and LLM evaluation for improved prediction accuracy"], "limitations": "", "keywords": ["preprint publication", "automated confidence assessment", "LLM", "systematic reviews", "machine learning"], "importance_score": 7, "read_time_minutes": 30}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554.pdf", "abs": "https://arxiv.org/abs/2505.10554", "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional gain in performance ceiling for both 7B and 32B models across math,\ncoding, and science benchmarks, demonstrating that explicit meta-ability\nalignment offers a scalable and dependable foundation for reasoning. Code is\navailable at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "This paper proposes a method to enhance long-chain reasoning in large models through explicit alignment with meta-abilities using RL, demonstrating significant performance improvements.", "motivation": "To address the unpredictable timing and consistency of emergent reasoning behaviors in large reasoning models (LRMs), and to provide a more scalable and reliable framework for reasoning capabilities.", "method": "The authors implement a three-stage pipeline involving individual alignment, parameter-space merging, and domain-specific reinforcement learning to explicitly align models with meta-abilities of deduction, induction, and abduction.", "result": "The proposed method boosts performance by over 10% compared to instruction-tuned baselines and yields additional performance gains in domain-specific tasks across multiple benchmarks.", "conclusion": "Explicit meta-ability alignment offers a scalable and dependable foundation for reasoning in large reasoning models, enhancing both consistency and performance.", "key_contributions": ["Introduced a method for explicit alignment of LRMs with meta-abilities.", "Demonstrated significant performance improvements across various benchmarks.", "Provided a scalable framework for enhancing reasoning capabilities."], "limitations": "", "keywords": ["large reasoning models", "meta-abilities", "reinforcement learning", "deduction", "induction"], "importance_score": 8, "read_time_minutes": 10}}
