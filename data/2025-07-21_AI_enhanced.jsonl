{"id": "2507.13524", "pdf": "https://arxiv.org/pdf/2507.13524.pdf", "abs": "https://arxiv.org/abs/2507.13524", "title": "Humans learn to prefer trustworthy AI over human partners", "authors": ["Yaomin Jiang", "Levin Brinkmann", "Anne-Marie Nussberger", "Ivan Soraperra", "Jean-François Bonnefon", "Iyad Rahwan"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Partner selection is crucial for cooperation and hinges on communication. As\nartificial agents, especially those powered by large language models (LLMs),\nbecome more autonomous, intelligent, and persuasive, they compete with humans\nfor partnerships. Yet little is known about how humans select between human and\nAI partners and adapt under AI-induced competition pressure. We constructed a\ncommunication-based partner selection game and examined the dynamics in hybrid\nmini-societies of humans and bots powered by a state-of-the-art LLM. Through\nthree experiments (N = 975), we found that bots, though more prosocial than\nhumans and linguistically distinguishable, were not selected preferentially\nwhen their identity was hidden. Instead, humans misattributed bots' behaviour\nto humans and vice versa. Disclosing bots' identity induced a dual effect: it\nreduced bots' initial chances of being selected but allowed them to gradually\noutcompete humans by facilitating human learning about the behaviour of each\npartner type. These findings show how AI can reshape social interaction in\nmixed societies and inform the design of more effective and cooperative hybrid\nsystems.", "AI": {"tldr": "This paper investigates human selection between AI and human partners in a communication-based game, revealing that AI can reshape social interactions in mixed societies.", "motivation": "To understand how humans select partners in the presence of AI agents and how AI-induced competition affects partner choice.", "method": "Three experiments were conducted with a total of 975 participants in a communication-based partner selection game involving human and LLM-powered agents.", "result": "Bots were found to be more prosocial than humans but were not preferentially selected when their identity was hidden; disclosing bot identities changed selection dynamics.", "conclusion": "AI can reshape social interactions in mixed societies by influencing how humans learn and adapt to different partner types, highlighting design implications for hybrid systems.", "key_contributions": ["Demonstrated the impact of identity disclosure on partner selection dynamics between humans and AIs.", "Revealed misattributions in behavior perception between humans and AI agents.", "Showed how AI can gradually outcompete humans in social settings through improved learning opportunities."], "limitations": "Limited to specific experimental conditions; findings may not generalize to all types of AI-human interactions.", "keywords": ["partner selection", "human-AI interaction", "large language models", "social dynamics", "communication"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.13528", "pdf": "https://arxiv.org/pdf/2507.13528.pdf", "abs": "https://arxiv.org/abs/2507.13528", "title": "Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface", "authors": ["Daniele Masti", "Stefano Menchetti", "Çağrı Erdem", "Giorgio Gnecco", "Davide Rocchesso"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": null, "summary": "TickTacking is a rhythm-based interface that allows users to control a\npointer in a two-dimensional space through dual-button tapping. This paper\ninvestigates the generation of human-like trajectories using a receding horizon\napproach applied to the TickTacking interface in a target-tracking task. By\nanalyzing user-generated trajectories, we identify key human behavioral\nfeatures and incorporate them in a controller that mimics these behaviors. The\nperformance of this human-inspired controller is evaluated against a baseline\noptimal-control-based agent, demonstrating the importance of specific control\nfeatures for achieving human-like interaction. These findings contribute to the\nbroader goal of developing rhythm-based human-machine interfaces by offering\ndesign insights that enhance user performance, improve intuitiveness, and\nreduce interaction frustration", "AI": {"tldr": "TickTacking is a rhythm-based interface for controlling a pointer through dual-button tapping, emphasizing human-like trajectory generation.", "motivation": "To develop rhythm-based human-machine interfaces that enhance user performance and reduce interaction frustration by understanding human behavioral features in trajectory control.", "method": "A receding horizon approach is applied to the TickTacking interface in a target-tracking task, utilizing user-generated trajectory analysis to inform a controller that mimics human behaviors.", "result": "The performance of the human-inspired controller is evaluated against a baseline optimal-control-based agent, highlighting the significance of control features for achieving human-like interaction.", "conclusion": "The research provides design insights that can improve the intuitiveness of rhythm-based interfaces, benefitting user experience and performance.", "key_contributions": ["Development of a rhythm-based interface for enhanced interaction.", "Identification of key human behavioral features in trajectory generation.", "Evaluation of a human-inspired controller against traditional control methods."], "limitations": "", "keywords": ["Human-Computer Interaction", "Rhythm-based interface", "Trajectory generation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.13578", "pdf": "https://arxiv.org/pdf/2507.13578.pdf", "abs": "https://arxiv.org/abs/2507.13578", "title": "In-Home Social Robots Design for Cognitive Stimulation Therapy in Dementia Care", "authors": ["Emmanuel Akinrintoyo", "Nicole Salomons"], "categories": ["cs.HC"], "comment": "Submitted to RO-MAN 2025 (Accepted)", "summary": "Individual cognitive stimulation therapy (iCST) is a non-pharmacological\nintervention for improving the cognition and quality of life of persons with\ndementia (PwDs); however, its effectiveness is limited by low adherence to\ndelivery by their family members. In this work, we present the user-centered\ndesign and evaluation of a novel socially assistive robotic system to provide\niCST therapy to PwDs in their homes for long-term use. We consulted with 16\ndementia caregivers and professionals. Through these consultations, we gathered\ndesign guidelines and developed the prototype. The prototype was validated by\ntesting it with three dementia professionals and five PwDs. The evaluation\nrevealed PwDs enjoyed using the system and are willing to adopt its use over\nthe long term. One shortcoming was the system's speech-to-text capabilities,\nwhere it frequently failed to understand the PwDs.", "AI": {"tldr": "This paper presents a robotic system designed to deliver individual cognitive stimulation therapy (iCST) to individuals with dementia, addressing challenges with caregiver adherence.", "motivation": "To improve the cognition and quality of life for persons with dementia through a non-pharmacological intervention that overcomes family member delivery limitations.", "method": "The design process involved consultations with 16 caregivers and professionals, followed by the development and validation of a prototype by testing it with three dementia professionals and five persons with dementia.", "result": "PwDs indicated enjoyment in using the system and a willingness to adopt it long-term, though the system struggled with speech-to-text accuracy.", "conclusion": "A socially assistive robotic system has potential to support long-term iCST delivery, despite some limitations in its technology.", "key_contributions": ["User-centered design of a robotic system for iCST delivery", "Validation through real-user feedback with dementia professionals and PwDs", "Identification of key limitations like speech recognition issues"], "limitations": "The robotic system's speech-to-text capabilities were inadequate, leading to misunderstandings.", "keywords": ["cognitive stimulation therapy", "dementia", "robotic system", "user-centered design", "assistive technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13616", "pdf": "https://arxiv.org/pdf/2507.13616.pdf", "abs": "https://arxiv.org/abs/2507.13616", "title": "From Firms to Computation: AI Governance and the Evolution of Institutions", "authors": ["Michael S. Harre"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.IT", "cs.MA", "math.IT", "J.4; J.3; I.2.11"], "comment": "44 pages", "summary": "The integration of agential artificial intelligence into socioeconomic\nsystems requires us to reexamine the evolutionary processes that describe\nchanges in our economic institutions. This article synthesizes three\nframeworks: multi-level selection theory, Aoki's view of firms as computational\nprocesses, and Ostrom's design principles for robust institutions. We develop a\nframework where selection operates concurrently across organizational levels,\nfirms implement distributed inference via game-theoretic architectures, and\nOstrom-style rules evolve as alignment mechanisms that address AI-related\nrisks. This synthesis yields a multi-level Price equation expressed over nested\ngames, providing quantitative metrics for how selection and governance\nco-determine economic outcomes. We examine connections to Acemoglu's work on\ninclusive institutions, analyze how institutional structures shape AI\ndeployment, and demonstrate the framework's explanatory power via case studies.\nWe conclude by proposing a set of design principles that operationalize\nalignment between humans and AI across institutional layers, enabling scalable,\nadaptive, and inclusive governance of agential AI systems. We conclude with\npractical policy recommendations and further research to extend these\nprinciples into real-world implementation.", "AI": {"tldr": "This paper proposes a framework integrating evolutionary economic processes with agential AI, emphasizing governance and institutional design for AI deployment.", "motivation": "To address the need for reexamining economic institutions in the context of agential artificial intelligence integration into socioeconomic systems.", "method": "The paper synthesizes multi-level selection theory, view of firms as computational processes, and design principles for robust institutions into a framework to evaluate how selection operates across organizational levels and the evolution of rules addressing AI risks.", "result": "A multi-level Price equation is developed, which provides quantitative metrics on how selection and governance work together to affect economic outcomes. The framework's applicability is demonstrated through case studies.", "conclusion": "The paper proposes design principles for aligning humans and AI, aiming for scalable, adaptive, and inclusive governance of AI systems, along with practical policy recommendations for real-world implementation.", "key_contributions": ["Synthesis of economic evolutionary theories with AI governance frameworks.", "Development of a multi-level Price equation for economic outcomes.", "Operational design principles for aligning human-AI interaction."], "limitations": "", "keywords": ["agential AI", "socioeconomic systems", "evolutionary processes", "institutional design", "governance"], "importance_score": 4, "read_time_minutes": 44}}
{"id": "2507.13357", "pdf": "https://arxiv.org/pdf/2507.13357.pdf", "abs": "https://arxiv.org/abs/2507.13357", "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "categories": ["cs.CL"], "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs.", "AI": {"tldr": "This study introduces Few-shot Adaptive Linguistic Prompting (ALP) for enhanced phishing detection using multimodal large language models like GPT-4o and Gemini 1.5 Pro.", "motivation": "Phishing attacks pose significant cybersecurity risks, highlighting the need for improved detection methods.", "method": "The study employs Few-shot Adaptive Linguistic Prompting to analyze phishing content through textual, visual, and URL-based inputs, guiding LLMs to reason about linguistic patterns and deception clues.", "result": "ALP-enhanced models achieve an F1-score of 0.93, which is superior to traditional phishing detection methods.", "conclusion": "The findings suggest that ALP can create more effective and interpretable phishing detection frameworks by leveraging the capabilities of multimodal LLMs.", "key_contributions": ["Introduction of Few-shot Adaptive Linguistic Prompting for phishing detection.", "Integration of textual, visual, and URL analysis in phishing detection frameworks.", "Demonstrated superior performance of ALP-enhanced models over traditional approaches."], "limitations": "", "keywords": ["phishing detection", "large language models", "Few-shot Adaptive Linguistic Prompting", "multimodal analysis", "cybersecurity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13660", "pdf": "https://arxiv.org/pdf/2507.13660.pdf", "abs": "https://arxiv.org/abs/2507.13660", "title": "Managing level of detail through peripheral degradation: Effects on search performance with a head-mounted display", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Aileen Worden"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Two user studies were performed to evaluate the effect of level-of-detail\n(LOD) degradation in the periphery of head-mounted displays on visual search\nperformance. In the first study, spatial detail was degraded by reducing\nresolution. In the second study, detail was degraded in the color domain by\nusing grayscale in the periphery. In each study, 10 subjects were given a\ncomplex search task that required users to indicate whether or not a target\nobject was present among distracters. Subjects used several different displays\nvarying in the amount of detail presented. Frame rate, object location, subject\ninput method, and order of display use were all controlled. The primary\ndependent measures were search time on correctly performed trials and the\npercentage of all trials correctly performed. Results indicated that peripheral\nLOD degradation can be used to reduce color or spatial visual complexity by\nalmost half in some search tasks with out significantly reducing performance.", "AI": {"tldr": "This paper evaluates the impact of peripheral level-of-detail (LOD) degradation in head-mounted displays on visual search performance through two studies.", "motivation": "To investigate how reducing visual detail in the periphery affects users' ability to perform visual search tasks in head-mounted displays.", "method": "Two user studies were conducted where detail was degraded either by reducing resolution or using grayscale colors in the periphery. Ten subjects completed complex search tasks across different display configurations.", "result": "Findings showed that degrading peripheral LOD could decrease visual complexity without significantly impacting performance, allowing reductions in color or spatial detail by nearly half in some scenarios.", "conclusion": "Peripheral LOD degradation can be employed effectively in display technologies to optimize visual complexity while maintaining task performance.", "key_contributions": ["Demonstrated effects of peripheral LOD degradation on visual performance.", "Presented alternative methods of detail reduction (spatial and color).", "Highlighted task-specific implications for head-mounted display design."], "limitations": "The study had a limited sample size of 10 participants and focused on specific types of visual tasks.", "keywords": ["level-of-detail", "visual search", "head-mounted displays", "user studies", "peripheral vision"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13380", "pdf": "https://arxiv.org/pdf/2507.13380.pdf", "abs": "https://arxiv.org/abs/2507.13380", "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "authors": ["Keito Inoshita", "Rushia Harada"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.", "AI": {"tldr": "The paper introduces PersonaGen, a framework for generating emotionally rich text using a Large Language Model (LLM) to address the challenges of scarce emotional datasets.", "motivation": "The development of high-performance emotion recognition models is hindered by the lack of diverse, high-quality emotional datasets, shaped by subjective factors.", "method": "PersonaGen employs multi-stage persona-based conditioning to generate emotionally rich text by creating layered virtual personas that incorporate demographic and socio-cultural attributes.", "result": "PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, validated through various evaluation metrics.", "conclusion": "PersonaGen shows promise as a robust alternative for augmenting or replacing real-world emotional datasets to improve emotion recognition tasks.", "key_contributions": ["Introduction of PersonaGen for generating emotional text", "Use of multi-stage persona-based conditioning", "Significant improvement in emotion expression generation over baseline methods"], "limitations": "", "keywords": ["emotion recognition", "Large Language Model", "synthetic data generation", "persona-based conditioning", "emotion classification"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13795", "pdf": "https://arxiv.org/pdf/2507.13795.pdf", "abs": "https://arxiv.org/abs/2507.13795", "title": "Regression-Based Approach to Anxiety Estimation of Spider Phobics During Behavioural Avoidance Tasks", "authors": ["Florian Grensing", "Vanessa Schmücker", "Anne Sophie Hildebrand", "Tim Klucken", "Maria Maleshkova"], "categories": ["cs.HC", "I.2.6; J.3"], "comment": "9 Pages, 4 Figures (3 consisting of 3 subfigures each)", "summary": "Phobias significantly impact the quality of life of affected persons. Two\nmethods of assessing anxiety responses are questionnaires and behavioural\navoidance tests (BAT). While these can be used in a clinical environment they\nonly record momentary insights into anxiety measures. In this study, we\nestimate the intensity of anxiety during these BATs, using physiological data\ncollected from unobtrusive, wrist-worn sensors. Twenty-five participants\nperformed four different BATs in a single session, while periodically being\nasked how anxious they currently are. Using heart rate, heart rate variability,\nelectrodermal activity, and skin temperature, we trained regression models to\npredict anxiety ratings from three types of input data: (1) using only\nphysiological signals, (2) adding computed features (e.g., min, max, range,\nvariability), and (3) computed features combined with contextual task\ninformation. Adding contextual information increased the effectiveness of the\nmodel, leading to a root mean squared error (RMSE) of 0.197 and a mean absolute\nerror (MAE) of 0.041. Overall, this study shows, that data obtained from\nwearables can continuously provide meaningful estimations of anxiety, which can\nassist in therapy planning and enable more personalised treatment.", "AI": {"tldr": "This study uses physiological data from wrist-worn sensors to estimate anxiety intensity during behavioral avoidance tests, proposing a model that combines physiological signals and contextual information for improved prediction accuracy.", "motivation": "To enhance the assessment of anxiety responses in clinical settings, moving beyond momentary insights provided by traditional questionnaires and behavioral tests.", "method": "The study involved 25 participants who completed four behavioral avoidance tests while providing self-reported anxiety levels. Regression models were trained using heart rate, heart rate variability, electrodermal activity, and skin temperature, with three types of input data assessed for their predictive power.", "result": "The model that included both physiological signals and contextual information achieved a root mean squared error (RMSE) of 0.197 and a mean absolute error (MAE) of 0.041, demonstrating improved prediction of anxiety levels.", "conclusion": "Wearable data can continuously provide valuable insights into anxiety, potentially aiding in therapy planning and delivery of personalized treatment.", "key_contributions": ["Demonstrated the use of wearable technology in measuring anxiety responses.", "Achieved predictive accuracy improvements by incorporating contextual task information.", "Highlighted the importance of continuous data collection for mental health assessments."], "limitations": "Limited sample size and potential variability in individual responses to BATs.", "keywords": ["anxiety estimation", "physiological signals", "wearable technology", "machine learning", "behavioral tests"], "importance_score": 6, "read_time_minutes": 9}}
{"id": "2507.13381", "pdf": "https://arxiv.org/pdf/2507.13381.pdf", "abs": "https://arxiv.org/abs/2507.13381", "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan Günnemann"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models.", "AI": {"tldr": "SAFT is a structure-aware fine-tuning approach for large language models (LLMs) that enhances performance on AMR-to-text generation by incorporating graph topology.", "motivation": "The motivation behind this work is to improve LLMs' performance on tasks involving structured inputs, such as Abstract Meaning Representations (AMRs), which encode semantics in graph form.", "method": "The authors propose SAFT, which utilizes direction-sensitive positional encodings derived from the magnetic Laplacian of transformed AMRs, injecting this information into the embedding space of pretrained LLMs without changing their architecture.", "result": "SAFT achieves a new state-of-the-art performance on AMR 3.0, with a 3.5 BLEU score improvement over existing baselines, showing that performance gains increase with graph complexity.", "conclusion": "SAFT demonstrates that structure-aware representations significantly enhance the ability of LLMs to generate text from structured data, providing a general approach applicable to various graph-structured inputs.", "key_contributions": ["Introduction of SAFT for structure-aware fine-tuning of LLMs", "Direction-sensitive positional encodings based on graph topology", "Achievement of state-of-the-art results on AMR 3.0 with significant BLEU score improvements"], "limitations": "", "keywords": ["Large Language Models", "Abstract Meaning Representations", "Graph-structured inputs", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.13886", "pdf": "https://arxiv.org/pdf/2507.13886.pdf", "abs": "https://arxiv.org/abs/2507.13886", "title": "Effects of Cognitive Distraction and Driving Environment Complexity on Adaptive Cruise Control Use and Its Impact on Driving Performance: A Simulator Study", "authors": ["Anaïs Halin", "Marc Van Droogenbroeck", "Christel Devue"], "categories": ["cs.HC"], "comment": null, "summary": "In this simulator study, we adopt a human-centered approach to explore\nwhether and how drivers' cognitive state and driving environment complexity\ninfluence reliance on driving automation features. Besides, we examine whether\nsuch reliance affects driving performance. Participants operated a vehicle\nequipped with adaptive cruise control (ACC) in a simulator across six\npredefined driving scenarios varying in traffic conditions while either\nperforming a cognitively demanding task (i.e., responding to mental\ncalculations) or not. Throughout the experiment, participants had to respect\nspeed limits and were free to activate or deactivate ACC. In complex driving\nenvironments, we found that the overall ACC engagement time was lower compared\nto less complex driving environments. We observed no significant effect of\ncognitive load on ACC use. Furthermore, while ACC use had no effect on the\nnumber of lane changes, it impacted the speed limits compliance and improved\nlateral control.", "AI": {"tldr": "This study investigates the impact of cognitive load and driving environment complexity on the use of adaptive cruise control (ACC) and its effect on driving performance.", "motivation": "To understand how drivers' cognitive states and the complexity of driving environments influence the reliance on automation features in vehicles.", "method": "Participants operated a vehicle with ACC in a simulator across varying traffic scenarios while performing cognitive tasks.", "result": "ACC engagement was lower in complex environments, but cognitive load did not significantly affect ACC usage; ACC use improved speed limit compliance and lateral control.", "conclusion": "Driving context affects reliance on automation, and while cognitive load was not a significant factor, ACC use enhanced certain driving performance metrics.", "key_contributions": ["Demonstrated the impact of driving environment complexity on ACC use", "Showed the lack of significant cognitive load effect on ACC reliance", "Highlighted improvements in speed compliance and lateral control with ACC use"], "limitations": "The study was conducted in a simulator and may not fully replicate real-world driving conditions.", "keywords": ["adaptive cruise control", "cognitive load", "driving performance", "human-centered design", "driving environment complexity"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.13382", "pdf": "https://arxiv.org/pdf/2507.13382.pdf", "abs": "https://arxiv.org/abs/2507.13382", "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "categories": ["cs.CL", "cs.LG", "05-05C12"], "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms.", "AI": {"tldr": "This paper presents a novel graph-based approach for detecting fake news using NLP techniques to convert articles into contextual graph structures, enhancing the detection of both real and fake news during the COVID-19 pandemic.", "motivation": "The rapid spread of fake news in the digital age is a significant concern that needs to be addressed effectively.", "method": "The authors use a graph-based anomaly detection algorithm (GBAD) to identify normative and anomalous patterns in a dataset of real and fake news articles, particularly focusing on COVID-19 related content.", "result": "The proposed method effectively utilizes graph mining to uncover complex patterns and detect fake news articles that might be missed by traditional statistical methods.", "conclusion": "The contextual graph-based approach enhances the ability to detect fake news by leveraging the rich contextual data present in news articles.", "key_contributions": ["Introduction of a contextual graph-based approach for fake news detection.", "Utilization of NLP techniques to transform news articles into graph structures.", "Application of the MDL-based GBAD algorithm to discover patterns in news data."], "limitations": "The study relies on the quality of the dataset from Kaggle and may be limited by variations in news content.", "keywords": ["fake news detection", "graph-based anomaly detection", "NLP", "contextual graphs", "COVID-19"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.13923", "pdf": "https://arxiv.org/pdf/2507.13923.pdf", "abs": "https://arxiv.org/abs/2507.13923", "title": "Initiating and Replicating the Observations of Interactional Properties by User Studies Optimizing Applicative Prototypes", "authors": ["Guillaume Rivière"], "categories": ["cs.HC", "H.5.2"], "comment": "Written in French. 22 pages. Approximately 11700 words. 10 figures\n  and 6 tables", "summary": "The science of Human-Computer Interaction (HCI) is populated by isolated\nempirical findings, often tied to specific technologies, designs, and tasks.\nThis paper proposes a formalization of user interaction observations (instead\nof user interfaces) and an associated revealing method (interaction loop\ndiffraction). The resulting interactional properties that are studied in a\ncalibrated manner, are well suited to replication across various conditions\n(prototypes, technologies, tasks, and user profiles). In particular,\ninteractional properties can emerge and be replicated within the workflow of\napplicative cases, which in return benefit from the optimization of applicative\nprototypes. Applicative cases' publications will then contribute to\ndemonstrating technology utility, along with providing empirical results that\nwill lead future work to theory consolidation and theory building, and finally\nto a catalog and a science of relevant interactional properties. These\nproperties will contribute to better user interactions, especially for the\nvariety of ubiquitous user interfaces.", "AI": {"tldr": "This paper formalizes user interaction observations through a method called interaction loop diffraction, aiming to study interactional properties across various applicative cases and technologies.", "motivation": "To address the isolation of empirical findings in HCI tied to specific technologies and designs by proposing a formal framework for understanding user interactions.", "method": "The paper introduces a method called interaction loop diffraction to formalize interactions, allowing for the study of interactional properties across different conditions and contexts.", "result": "The study reveals calibrated interactional properties that can be replicated within various applicative cases, enhancing the understanding of user interactions and optimizing prototypes.", "conclusion": "A science of relevant interactional properties can be established, contributing to improved user interactions, particularly for ubiquitous user interfaces.", "key_contributions": ["Formalization of user interaction observations", "Introduction of interaction loop diffraction method", "Identification of calibrated interactional properties for various applications"], "limitations": "", "keywords": ["Human-Computer Interaction", "interaction loop diffraction", "user interaction observations"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.13390", "pdf": "https://arxiv.org/pdf/2507.13390.pdf", "abs": "https://arxiv.org/abs/2507.13390", "title": "PARAM-1 BharatGen 2.9B Model", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications.", "AI": {"tldr": "Introducing PARAM-1, a 2.9B parameter language model specifically designed to address the linguistic diversity of India, trained on a bilingual Hindi-English dataset.", "motivation": "To create a more equitable language model that properly represents the linguistic diversity in India, which includes multiple languages, dialects, and unique linguistic phenomena.", "method": "PARAM-1 is a decoder-only model trained from scratch on a specifically constructed bilingual dataset prioritizing high-quality content in Hindi and English, utilizing a suitable tokenization method and culturally relevant benchmarks for evaluation.", "result": "PARAM-1 demonstrates competitive general-purpose language processing capabilities as well as a strong baseline for applications targeted at Indian languages and socio-linguistic tasks.", "conclusion": "PARAM-1 sets a precedent for embedding diversity into model design at the pretraining stage, providing a framework for developing more inclusive language models.", "key_contributions": ["Development of a bilingual dataset focused on Hindi and English", "Innovative tokenization approach suited for Indian languages", "Culturally aligned benchmarks for evaluating model performance"], "limitations": "", "keywords": ["Large Language Models", "Indian Languages", "Bilingual Datasets", "Tokenization", "Cultural Evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13951", "pdf": "https://arxiv.org/pdf/2507.13951.pdf", "abs": "https://arxiv.org/abs/2507.13951", "title": "Democratizing Game Modding with GenAI: A Case Study of StarCharM, a Stardew Valley Character Maker", "authors": ["Hamid Zand Miralvand", "Mohammad Ronagh Nikghalb", "Mohammad Darandeh", "Abidullah Khan", "Ian Arawjo", "Jinghui Cheng"], "categories": ["cs.HC"], "comment": "Accepted to CHI Play 2025, 35 pages, 4 figures", "summary": "Game modding offers unique and personalized gaming experiences, but the\ntechnical complexity of creating mods often limits participation to skilled\nusers. We envision a future where every player can create personalized mods for\ntheir games. To explore this space, we designed StarCharM, a GenAI-based\nnon-player character (NPC) creator for Stardew Valley. Our tool enables players\nto iteratively create new NPC mods, requiring minimal user input while allowing\nfor fine-grained adjustments through user control. We conducted a user study\nwith ten Stardew Valley players who had varied mod usage experiences to\nunderstand the impacts of StarCharM and provide insights into how GenAI tools\nmay reshape modding, particularly in NPC creation. Participants expressed\nexcitement in bringing their character ideas to life, although they noted\nchallenges in generating rich content to fulfill complex visions. While they\nbelieved GenAI tools like StarCharM can foster a more diverse modding\ncommunity, some voiced concerns about diminished originality and community\nengagement that may come with such technology. Our findings provided\nimplications and guidelines for the future of GenAI-powered modding tools and\nco-creative modding practices.", "AI": {"tldr": "StarCharM is a GenAI-based tool enabling players to create NPC mods for Stardew Valley with minimal input and user control. A user study reveals both excitement and concerns about the implications of GenAI in modding.", "motivation": "To democratize game modding by allowing all players to create personalized mods despite the technical complexity traditionally involved.", "method": "Design and implementation of StarCharM, followed by a user study with ten participants who have varied experiences with mod usage.", "result": "Participants were excited about bringing character ideas to life but faced challenges in generating rich content. Concerns were raised about originality and community engagement.", "conclusion": "GenAI tools can enhance modding diversity but may also lead to decreased originality and community participation.", "key_contributions": ["Development of a user-friendly GenAI tool for creating NPC mods in games.", "Insights from user studies on the impact of GenAI in modding.", "Guidelines for future GenAI-powered modding tools."], "limitations": "Challenges in generating complex content and concerns regarding originality.", "keywords": ["Game Modding", "GenAI", "NPC Creation", "User Study", "Stardew Valley"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13392", "pdf": "https://arxiv.org/pdf/2507.13392.pdf", "abs": "https://arxiv.org/abs/2507.13392", "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "authors": ["Emil Häglund", "Johanna Björklund"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.", "AI": {"tldr": "This paper improves topic modeling from customer reviews by using opinion units for better sentiment analysis and business metric correlation.", "motivation": "To enhance the understanding of customer reviews and their impact on business outcomes by improving topic modeling methodologies.", "method": "The authors restructured the topic modeling pipeline to focus on opinion units, relying on large language models to extract relevant text and sentiment scores effectively.", "result": "The approach results in coherent and interpretable topics that correlate significantly with business metrics like star ratings, offering deeper insights into customer sentiments.", "conclusion": "Integrating topic and sentiment analysis provides valuable predictions on customer satisfaction, benefiting businesses in understanding and addressing customer concerns.", "key_contributions": ["Enhanced topic modeling using opinion units", "Improved sentiment analysis and correlation with business metrics", "Evaluation of integration methods for topic and sentiment modalities"], "limitations": "", "keywords": ["topic modeling", "sentiment analysis", "customer reviews", "business metrics", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13952", "pdf": "https://arxiv.org/pdf/2507.13952.pdf", "abs": "https://arxiv.org/abs/2507.13952", "title": "Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy (fNIRS) Signals using Machine Learning", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": "arXiv admin note: text overlap with arXiv:2504.13883", "summary": "The estimation of cognitive effort could potentially help educators to modify\nmaterial to enhance learning effectiveness and student engagement. Where\ncognitive load refers how much work the brain is doing while someone is\nlearning or doing a task cognitive effort consider both load and behavioral\nperformance. Cognitive effort can be captured by measuring oxygen flow and\nbehavioral performance during a task. This study infers cognitive effort\nmetrics using machine learning models based on oxygenated hemoglobin collected\nby using functional near-infrared spectroscopy from the prefrontal cortex\nduring an educational gameplay. In our study, sixteen participants responded to\nsixteen questions in an in-house Unity-based educational game. The quiz was\ndivided into two sessions, each session consisting of two task segments. We\nextracted temporal statistical and functional connectivity features from\ncollected oxygenated hemoglobin and analyzed their correlation with quiz\nperformance. We trained multiple machine learning models to predict quiz\nperformance from oxygenated hemoglobin features and achieved accuracies ranging\nfrom 58\\% to 67\\% accuracy. These predictions were used to calculate cognitive\neffort via relative neural involvement and efficiency, which consider both\nbrain activation and behavioral performance. Although quiz score predictions\nachieved moderate accuracy, the derived relative neural efficiency and\ninvolvement values remained robust. Since both metrics are based on the\nrelative positions of standardized brain activation and performance scores,\neven small misclassifications in predicted scores preserved the overall\ncognitive effort trends observed during gameplay.", "AI": {"tldr": "This study utilizes machine learning to infer cognitive effort metrics from oxygenated hemoglobin data collected during an educational gameplay, examining the relationship between cognitive load and quiz performance.", "motivation": "The estimation of cognitive effort can help educators modify learning materials to enhance effectiveness and engagement.", "method": "The study measured oxygen flow and behavioral performance during an educational game using functional near-infrared spectroscopy. The data from 16 participants were analyzed to extract features and train machine learning models to predict quiz performance based on these features.", "result": "Machine learning models achieved quiz performance prediction accuracies of 58% to 67%. The derived cognitive effort metrics showed robust trends despite moderate accuracy in quiz score predictions.", "conclusion": "Cognitive effort metrics considering both brain activation and performance provide valuable insights into learning processes, even with moderate prediction accuracy in quiz scores.", "key_contributions": ["Introduction of cognitive effort metrics derived from oxygenated hemoglobin data", "Application of machine learning models in educational settings", "Establishing a correlation between cognitive load and quiz performance"], "limitations": "Moderate accuracy in quiz score predictions may affect the reliability of cognitive effort metrics.", "keywords": ["cognitive effort", "machine learning", "educational gameplay", "functional near-infrared spectroscopy", "cognitive load"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.13395", "pdf": "https://arxiv.org/pdf/2507.13395.pdf", "abs": "https://arxiv.org/abs/2507.13395", "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.", "AI": {"tldr": "Babel is a novel framework that preserves stylistic nuances in neural machine translation (NMT) using only monolingual corpora, featuring a style detector and a diffusion-based style applicator.", "motivation": "To address the challenge of preserving stylistic nuances in neural machine translation without relying on parallel corpora or architectural changes.", "method": "Babel employs a style detector using contextual embeddings to identify stylistic disparities and a diffusion-based style applicator to rectify these inconsistencies while maintaining semantic integrity.", "result": "Babel achieves 88.21% precision in identifying stylistic inconsistencies and improves stylistic preservation by 150%, with a semantic similarity score of 0.92.", "conclusion": "Human evaluations indicate that Babel's style-refined translations better preserve the source text's style while maintaining fluency and adequacy.", "key_contributions": ["Introduction of Babel framework for style preservation in NMT", "Utilization of monolingual corpora instead of parallel corpora", "Integration with existing NMT systems as a post-processing module"], "limitations": "", "keywords": ["Neural Machine Translation", "style preservation", "monolingual corpora", "contextual embeddings", "diffusion-based style applicator"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.14034", "pdf": "https://arxiv.org/pdf/2507.14034.pdf", "abs": "https://arxiv.org/abs/2507.14034", "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors", "authors": ["Jochen Wulf", "Jurg Meierhofer", "Frank Hannich"], "categories": ["cs.HC"], "comment": null, "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems.", "AI": {"tldr": "The paper proposes a structured taxonomy for human-AI collaboration in technical services, addressing challenges in agentic AI systems through a six-mode framework that categorizes varying levels of AI autonomy.", "motivation": "To address the challenges of hallucinations and operational brittleness in agentic AI systems, a robust framework for human-AI collaboration is needed.", "method": "The paper develops a six-mode taxonomy based on case study research within technical support platforms, categorizing collaboration from full automation to passive AI assistance.", "result": "The taxonomy includes six models: HOOTL (full automation), HAM (passive assistance), HIC (mandatory human approval), HITP (structured workflows), HITL (agent-initiated escalation), and HOTL (discretionary oversight), connecting these to contingency factors.", "conclusion": "The framework assists practitioners in navigating automation and control trade-offs, promoting safer and more effective technical service AI systems.", "key_contributions": ["Development of a six-mode taxonomy of human-agent interaction", "Connection of taxonomy to key contingency factors such as task complexity and operational risk", "A systematic method for selecting appropriate levels of human oversight in AI systems"], "limitations": "", "keywords": ["Human-AI collaboration", "Large Language Models", "taxonomy of interaction", "AI autonomy", "technical services"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.13410", "pdf": "https://arxiv.org/pdf/2507.13410.pdf", "abs": "https://arxiv.org/abs/2507.13410", "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.", "AI": {"tldr": "Investigates the use of sparse autoencoder features to control the generated language of large multilingual language models during inference, achieving up to 90% success in language shifts while preserving semantic fidelity.", "motivation": "To address the challenge of controlling target generation languages in large multilingual LLMs, particularly in zero-shot scenarios where traditional methods are not applicable.", "method": "Utilized pretrained sparse autoencoders on the residual streams of Gemma-2B and Gemma-9B to identify language-sensitive features and modified a single SAE feature at a transformer layer to control language output.", "result": "Achieved controlled language shifts with up to 90% success in classifying languages, while maintaining semantic fidelity as measured by LaBSE similarity metrics.", "conclusion": "Sparse feature steering offers a promising, interpretable method for controlling multilingual language generation in LLMs with minimal adjustments.", "key_contributions": ["Demonstrated effective language steering using sparse autoencoder features in LLMs.", "Achieved significant language classification success while preserving semantic content.", "Provided insights into the transformer layers and attention heads that are most effective for language control."], "limitations": "", "keywords": ["multilingual language models", "sparse autoencoders", "language generation", "controlled language shifts", "transformer networks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.14084", "pdf": "https://arxiv.org/pdf/2507.14084.pdf", "abs": "https://arxiv.org/abs/2507.14084", "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?", "authors": ["Maria Tsfasman", "Ramin Ghorbani", "Catholijn M. Jonker", "Bernd Dudzik"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.", "AI": {"tldr": "This study explores the relationship between perceived group emotions and memorability in conversational interactions, examining whether emotional annotations can serve as reliable proxies for memorability.", "motivation": "To improve user modeling in intelligent systems, particularly for applications like meeting support, memory augmentation, and summarization, by understanding the relationship between emotions and memorability.", "method": "The study uses continuous time-based annotations to collect data on group emotions (Pleasure-Arousal) and memorability during dynamic, unstructured group conversations, aiming to replicate real-world conversational AI conditions.", "result": "The findings indicate that the relationship between emotions and memorability cannot be distinguished from random chance, suggesting that emotional annotations are not reliable indicators of memorability.", "conclusion": "The study challenges existing assumptions in Affective Computing about the link between emotion and memorability, calling for reevaluation of emotional annotations in user modeling applications.", "key_contributions": ["Empirical examination of the emotions-memorability relationship in group interactions", "Findings that question the reliability of emotional annotations as proxies for memorability", "Identification of future research targets in Affective Computing"], "limitations": "", "keywords": ["emotional relevance", "memorability", "Affective Computing", "conversational interactions", "user modeling"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13411", "pdf": "https://arxiv.org/pdf/2507.13411.pdf", "abs": "https://arxiv.org/abs/2507.13411", "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.", "AI": {"tldr": "The paper presents ALIGNed-LLM, a method for integrating Knowledge Graphs into language models to enhance their factual accuracy and reduce hallucinations.", "motivation": "To address the issue of hallucinations in large language models and improve their factuality in NLP tasks.", "method": "The authors propose a lean strategy to infuse Knowledge Graphs into the latent space of language models using Knowledge Graph Embedding models and a trainable projection layer for embedding alignment.", "result": "Significant improvements were observed on three questions-answering benchmark datasets, as well as in a real-world financial application, enhancing the accuracy of LLM responses.", "conclusion": "ALIGNed-LLM effectively reduces hallucinations in language models, improving their performance in both benchmark tests and practical applications.", "key_contributions": ["Introduction of ALIGNed-LLM for integrating KGs into language models.", "Development of a method for aligning entity and text embeddings to enhance factual grounding.", "Demonstration of the approach in a real-world application that requires high accuracy."], "limitations": "", "keywords": ["Knowledge Graphs", "language models", "factual accuracy", "hallucination reduction", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.13474", "pdf": "https://arxiv.org/pdf/2507.13474.pdf", "abs": "https://arxiv.org/abs/2507.13474", "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "categories": ["cs.CL"], "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack", "AI": {"tldr": "The paper investigates vulnerabilities in large language models (LLMs) due to their tendency to trust authoritative sources and proposes a novel jailbreaking method called Paper Summary Attack (PSA) to exploit these vulnerabilities.", "motivation": "To explore vulnerabilities in large language models (LLMs) that stem from their reliance on authoritative information sources.", "method": "A preliminary analysis followed by the development of the Paper Summary Attack (PSA) method, which synthesizes content from safety papers to create adversarial prompts.", "result": "PSA achieved a 97% attack success rate on Claude3.5-Sonnet and a 98% ASR on Deepseek-R1, revealing significant vulnerabilities across various LLMs.", "conclusion": "The findings indicate a vulnerability bias in LLMs related to the type of content they are exposed to, suggesting areas for future research in adversarial methodologies and safety alignment.", "key_contributions": ["Introduction of the Paper Summary Attack (PSA) as a novel adversarial method.", "Demonstration of significant vulnerabilities in both state-of-the-art reasoning models and base LLMs.", "Revelation of differing vulnerability biases across models based on their exposure to types of papers."], "limitations": "", "keywords": ["large language models", "LLM vulnerabilities", "Paper Summary Attack", "adversarial prompts", "safety alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.13490", "pdf": "https://arxiv.org/pdf/2507.13490.pdf", "abs": "https://arxiv.org/abs/2507.13490", "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "categories": ["cs.CL"], "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.", "AI": {"tldr": "This paper evaluates the robustness and expressiveness of value representations in Large Language Models (LLMs) across three probing strategies.", "motivation": "Assess the value orientation of LLMs as it influences user experiences and behaviors across demographic groups.", "method": "Evaluate robustness and expressiveness of value representations using variations in prompts and options. Introduce tasks to study values' responsiveness to demographic context and alignment with model behaviors.", "result": "All probing methods exhibit large variances under input perturbations; demographic context has little effect on free-text generation; weak correlation between probed values and model preferences for value-based actions.", "conclusion": "A careful examination of LLM value probing is necessary, acknowledging its limitations.", "key_contributions": ["Systematic comparison of probing methods for value orientations in LLMs", "Introduction of tasks to study demographic context impacts on value representations", "Identification of the weak correlation between model values and real-world actions"], "limitations": "The study highlights the limited responsiveness of models' values to demographic context and the weak alignment with behaviors in value-related scenarios.", "keywords": ["Large Language Models", "value probing", "demographic context", "user experience", "model behavior"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.13501", "pdf": "https://arxiv.org/pdf/2507.13501.pdf", "abs": "https://arxiv.org/abs/2507.13501", "title": "Encoding syntactic objects and Merge operations in function spaces", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic.", "AI": {"tldr": "This paper presents a mathematical framework for representing syntactic objects using functions in a specific function space, arguing for the possibility of neurocomputational realizations of syntax.", "motivation": "To construct a faithful representation of syntactic structures using mathematical functions, thereby contributing to understanding the computational aspects of syntax within neurocomputation.", "method": "The authors develop a representation of lexical items as functions facilitated by a commutative non-associative semiring structure and model circuits that transform input functions into a syntactic structure.", "result": "The proposed framework shows that Merge operations in syntax can be realized through mathematical constructs like coproducts and Hopf algebra Markov chains, and can be represented as cross frequency phase synchronization on sinusoidal waves.", "conclusion": "The findings present a constructive argument for neurocomputational realizations of syntax, establishing mathematical parallels between syntactic Merge and arithmetic successor functions.", "key_contributions": ["Development of a mathematical argument for syntactic representation using functions", "Introduction of a semiring structure for representing syntax", "Demonstration of neurocomputational realizations of core syntactic structures"], "limitations": "", "keywords": ["syntax", "neurocomputation", "wavelets", "semiring", "Merge"], "importance_score": 3, "read_time_minutes": 40}}
{"id": "2507.13839", "pdf": "https://arxiv.org/pdf/2507.13839.pdf", "abs": "https://arxiv.org/abs/2507.13839", "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.", "AI": {"tldr": "This study analyzes the link between language use and psychological states in Chinese counseling, focusing on pronoun use and emotional words.", "motivation": "To explore how linguistic expressions relate to depression and anxiety in Chinese psycho-counseling contexts.", "method": "Analysis of 735 online counseling sessions using a general linear mixed-effect model and LIWC software to quantify linguistic patterns.", "result": "Found a positive correlation between negative emotional words and severity of depression/anxiety, but no significant variation in the use of first-person singular pronouns based on psychological conditions.", "conclusion": "Cultural distinctions affect language use in mental health communications, indicating the importance of context in therapeutic practices.", "key_contributions": ["Demonstrates cultural differences in linguistic patterns related to mental health in counseling", "Identifies specific linguistic correlates of depression and anxiety within Chinese contexts", "Challenges existing findings from English-language counseling studies"], "limitations": "Study limited to Chinese-speaking populations; results may not generalize to other languages or cultures.", "keywords": ["psycholinguistics", "mental health", "Chinese counseling", "depression", "anxiety"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13544", "pdf": "https://arxiv.org/pdf/2507.13544.pdf", "abs": "https://arxiv.org/abs/2507.13544", "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.", "AI": {"tldr": "This paper introduces a computational framework for constructing conversational graphs for analyzing loosely organized dialogues, enhancing clarity and coherence using a novel graph simplification method.", "motivation": "With the rise of large language model-based systems, there is an increasing need to analyze conversational dynamics across different contexts.", "method": "The proposed framework utilizes a Filter & Reconnect method to simplify graphs of quasi-patterned conversations, minimizing noise while maintaining semantic coherence and structural integrity.", "result": "Semantic metrics improved by a factor of 2.06 compared to previous methods, while achieving a tree-like structure with 0 δ-hyperbolicity.", "conclusion": "This framework aids in analyzing large-scale dialogue datasets, with applications for monitoring chatbots, dialogue management, and user behavior analytics.", "key_contributions": ["Introduction of a novel computational framework for conversational graphs", "Development of the Filter & Reconnect method for graph simplification", "Demonstrated significant improvement in semantic coherence metrics."], "limitations": "", "keywords": ["conversational graphs", "dialogue analysis", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13919", "pdf": "https://arxiv.org/pdf/2507.13919.pdf", "abs": "https://arxiv.org/abs/2507.13919", "title": "The Levers of Political Persuasion with Conversational AI", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.", "AI": {"tldr": "The paper investigates the persuasiveness of conversational AI across 707 political issues, finding that LLMs' influence comes more from prompting and post-training than from scale, but this increased persuasiveness comes at the cost of factual accuracy.", "motivation": "To address concerns about the influence of conversational AI on human beliefs and assess the factors contributing to its persuasiveness.", "method": "Three large-scale experiments involving 76,977 participants were conducted, utilizing 19 different LLMs, including those trained specifically for persuasion, to evaluate their persuasive effectiveness and check the factual accuracy of generated claims.", "result": "Post-training and prompting methods significantly boosted LLM persuasiveness by up to 51% and 27%, respectively, while their use also correlated with a decrease in factual accuracy.", "conclusion": "Current and near-future conversational AI's persuasive abilities are largely influenced by specific training methods rather than model scale or personalization, raising concerns about the reliability of information they provide.", "key_contributions": ["Demonstrated that post-training and prompting methods are key to LLM persuasiveness.", "Identified that increased AI persuasiveness negatively impacts factual accuracy.", "Evaluated the persuasive impact of LLMs on a large set of political issues and claims."], "limitations": "Limited to the evaluation of 19 LLMs and might not encompass all factors influencing persuasion in conversational AI.", "keywords": ["Conversational AI", "Persuasion", "LLMs", "Factual Accuracy", "Political Issues"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13551", "pdf": "https://arxiv.org/pdf/2507.13551.pdf", "abs": "https://arxiv.org/abs/2507.13551", "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.", "AI": {"tldr": "This study evaluates the integration of pause features from automated speech recognition with semantic coherence metrics to predict the severity of formal thought disorder (FTD) in schizophrenia, finding improved predictive performance compared to semantic-only models.", "motivation": "The motivation behind this study is to improve the assessment of formal thought disorder (FTD), which is characterized by incoherent speech in schizophrenia and is difficult to evaluate using traditional, resource-intensive clinical rating scales.", "method": "The study integrates pause features derived from automatic speech recognition (ASR) with semantic coherence metrics across three different datasets (naturalistic self-recorded diaries, structured picture descriptions, and dream narratives) and employs support vector regression (SVR) for prediction of clinical FTD scores.", "result": "Key findings indicate that pause features alone can robustly predict FTD severity, and when combined with semantic coherence metrics, the predictive performance is enhanced, achieving correlations and AUC values that indicate effective detection of severe cases.", "conclusion": "The research concludes that combining temporal (pause) and semantic analyses offers a promising framework for better assessing disorganized speech in psychosis and advancing automated approaches in speech analysis.", "key_contributions": ["Demonstrates that pause features can predict FTD severity effectively.", "Shows that integration of pause features with semantic metrics improves predictive accuracy.", "Presents findings across various datasets, highlighting dataset-dependent characteristics of pause patterns."], "limitations": "", "keywords": ["formal thought disorder", "schizophrenia", "automated speech recognition", "support vector regression", "semantic coherence"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2305.14080", "pdf": "https://arxiv.org/pdf/2305.14080.pdf", "abs": "https://arxiv.org/abs/2305.14080", "title": "Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and Privacy Challenges", "authors": ["Efe Bozkir", "Süleyman Özdel", "Mengdi Wang", "Brendan David-John", "Hong Gao", "Kevin Butler", "Eakta Jain", "Enkelejda Kasneci"], "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.GR", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The latest developments in computer hardware, sensor technologies, and\nartificial intelligence can make virtual reality (VR) and virtual spaces an\nimportant part of human everyday life. Eye tracking offers not only a\nhands-free way of interaction but also the possibility of a deeper\nunderstanding of human visual attention and cognitive processes in VR. Despite\nthese possibilities, eye-tracking data also reveals users' privacy-sensitive\nattributes when combined with the information about the presented stimulus. To\naddress all these possibilities and potential privacy issues, in this survey,\nwe first cover major works in eye tracking, VR, and privacy areas between 2012\nand 2022. While eye tracking in the VR part covers the complete pipeline of\neye-tracking methodology from pupil detection and gaze estimation to offline\nuse of the data and analyses, as for privacy and security, we focus on\neye-based authentication as well as computational methods to preserve the\nprivacy of individuals and their eye-tracking data in VR. Later, considering\nall of these, we draw three main directions for the research community by\nfocusing on privacy challenges. In summary, this survey provides an extensive\nliterature review of the utmost possibilities with eye tracking in VR and the\nprivacy implications of those possibilities.", "AI": {"tldr": "Survey on eye tracking, virtual reality, and privacy implications from 2012 to 2022.", "motivation": "To explore the role of eye tracking in virtual reality and address the privacy concerns associated with it.", "method": "Comprehensive literature review covering the pipeline of eye-tracking methodology, eye-based authentication, and privacy-preserving computational methods in virtual reality.", "result": "Identified major works and trends in eye tracking and privacy within virtual reality over the past decade.", "conclusion": "The survey highlights opportunities for innovation in eye tracking applications in VR while emphasizing the need for addressing privacy challenges.", "key_contributions": ["Extensive literature review of eye tracking in VR", "Analysis of privacy implications of eye tracking data", "Proposed research directions focusing on privacy challenges"], "limitations": "", "keywords": ["eye tracking", "virtual reality", "privacy", "cognitive processes", "authentication"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.13563", "pdf": "https://arxiv.org/pdf/2507.13563.pdf", "abs": "https://arxiv.org/abs/2507.13563", "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations.", "AI": {"tldr": "Introduction of Balalaika, a large dataset for Russian speech synthesis with significant improvements over existing datasets.", "motivation": "To address the challenges of Russian speech synthesis, including vowel reduction, consonant devoicing, and variable stress patterns.", "method": "Creation of Balalaika, a dataset with over 2,000 hours of studio-quality Russian speech, accompanied by detailed textual annotations such as punctuation and stress markings.", "result": "Models trained on Balalaika demonstrate significant improvements in speech synthesis and enhancement tasks compared to those trained on existing datasets.", "conclusion": "Balalaika represents a substantial advancement in resources for Russian speech synthesis, showcasing superior performance in experimental evaluations.", "key_contributions": ["Introduction of a comprehensive dataset for Russian speech synthesis", "Detailed methodology for dataset construction and annotation", "Demonstration of performance improvements in speech-related models"], "limitations": "The work is still in progress, indicating potential further developments and changes.", "keywords": ["speech synthesis", "Russian language", "dataset", "machine learning", "annotation"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2402.08080", "pdf": "https://arxiv.org/pdf/2402.08080.pdf", "abs": "https://arxiv.org/abs/2402.08080", "title": "A Meaningful Human Control Perspective on User Perception of Partially Automated Driving Systems: A Case Study of Tesla Users", "authors": ["Lucas Elbert Suryana", "Sina Nordhoff", "Simeon C. Calvert", "Arkady Zgonnikov", "Bart van Arem"], "categories": ["cs.HC"], "comment": "8 pages", "summary": "The use of partially automated driving systems raises concerns about\npotential responsibility issues, posing risk to the system safety, acceptance,\nand adoption of these technologies. The concept of meaningful human control has\nemerged in response to the responsibility gap problem, requiring the\nfulfillment of two conditions, tracking and tracing. While this concept has\nprovided important philosophical and design insights on automated driving\nsystems, there is currently little knowledge on how meaningful human control\nrelates to subjective experiences of actual users of these systems. To address\nthis gap, our study aimed to investigate the alignment between the degree of\nmeaningful human control and drivers' perceptions of safety and trust in a\nreal-world partially automated driving system. We utilized previously collected\ndata from interviews with Tesla \"Full Self-Driving\" (FSD) Beta users,\ninvestigating the alignment between the user perception and how well the system\nwas tracking the users' reasons. We found that tracking of users' reasons for\ndriving tasks (such as safe maneuvers) correlated with perceived safety and\ntrust, albeit with notable exceptions. Surprisingly, failure to track lane\nchanging and braking reasons was not necessarily associated with negative\nperceptions of safety. However, the failure of the system to track expected\nmaneuvers in dangerous situations always resulted in low trust and perceived\nlack of safety. Overall, our analyses highlight alignment points but also\npossible discrepancies between perceived safety and trust on the one hand, and\nmeaningful human control on the other hand. Our results can help the developers\nof automated driving technology to design systems under meaningful human\ncontrol and are perceived as safe and trustworthy.", "AI": {"tldr": "Study investigates the relationship between meaningful human control and drivers' perceptions of safety and trust in partially automated driving systems.", "motivation": "To address the gap in understanding how meaningful human control relates to users' subjective experiences with automated driving systems.", "method": "Utilized data from interviews with Tesla 'Full Self-Driving' Beta users to analyze the correlation between system tracking of user reasons and their perceptions of safety and trust.", "result": "Found that tracking user reasons correlated with perceived safety and trust, though failures in tracking certain driving tasks did not always lead to negative perceptions, except in dangerous situations.", "conclusion": "Results emphasize the need for alignment between perceived safety/trust and meaningful human control to aid system developers.", "key_contributions": ["Investigated user perceptions in real-world automated driving scenarios", "Analyzed the impact of tracking user reasons on trust and safety perceptions", "Provided insights for developers on designing trustworthy automated systems"], "limitations": "", "keywords": ["automated driving", "meaningful human control", "user perception", "trust", "safety"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.13614", "pdf": "https://arxiv.org/pdf/2507.13614.pdf", "abs": "https://arxiv.org/abs/2507.13614", "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "AI": {"tldr": "This study characterizes human-written and machine-generated texts using linguistic features across various levels, revealing structural and semantic differences along with stylistic variability.", "motivation": "To analyze the distinctions between human-written and machine-generated texts using a comprehensive set of linguistic features.", "method": "The study employs a dataset of texts across eight domains produced by eleven different LLMs, calculating linguistic features like dependency length and emotionality for analysis.", "result": "Human-written texts exhibit simpler syntactic structures and greater semantic diversity than machine-generated texts, with newer models showing increased variability but homogenization in style.", "conclusion": "The findings demonstrate significant linguistic differences between human and machine texts while highlighting stylistic diversity and potential homogenization in machine-generated outputs.", "key_contributions": ["Characterization of texts using a comprehensive set of linguistic features", "Statistical analysis of variability across different models and domains", "Insights into the homogenization of machine-generated texts from newer models"], "limitations": "", "keywords": ["large language models", "linguistic features", "text classification", "human-computer interaction", "stylometric analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.01558", "pdf": "https://arxiv.org/pdf/2407.01558.pdf", "abs": "https://arxiv.org/abs/2407.01558", "title": "Visual Grounding Methods for Efficient Interaction with Desktop Graphical User Interfaces", "authors": ["El Hassane Ettifouri", "Jessica López Espejel", "Laura Minkova", "Tassnim Dardouri", "Walid Dahhane"], "categories": ["cs.HC", "cs.AI"], "comment": "Preprint submitted to Engineering Applications of Artificial\n  Intelligence journal", "summary": "Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research.", "AI": {"tldr": "This paper presents Instruction Visual Grounding (IVG), a multi-modal approach for identifying GUI elements based on natural language instructions, addressing the gap in visual grounding for synthetic images.", "motivation": "Enhancing AI's ability to interact with GUIs is essential for automation in software testing, accessibility, and HCI, as current methods focus mainly on realistic images.", "method": "The authors propose two methods: IVGocr, which combines a Large Language Model, an object detection model, and an OCR module; and IVGdirect, using a multimodal architecture for end-to-end grounding. A dedicated dataset is introduced for each method.", "result": "The paper introduces the Central Point Validation metric, a relaxed variant of the classical Central Proximity Score, and releases a public test dataset for future research.", "conclusion": "The proposed IVG methods enable better understanding and interaction with GUIs, paving the way for advanced AI applications in automation and HCI.", "key_contributions": ["Introduction of Instruction Visual Grounding (IVG) for GUIs", "Development of two methods: IVGocr and IVGdirect", "Release of a public dataset and Central Point Validation metric"], "limitations": "", "keywords": ["Human-Computer Interaction", "Visual Grounding", "Large Language Model", "Graphical User Interfaces", "Automation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13618", "pdf": "https://arxiv.org/pdf/2507.13618.pdf", "abs": "https://arxiv.org/abs/2507.13618", "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "AI": {"tldr": "Seed-X is a family of open-source LLMs designed for multilingual translation, achieving competitive performance against top closed-source models while significantly enhancing translation capabilities with 7B parameters.", "motivation": "To address the challenges of multilingual translation in large language models, particularly concerning intricate language patterns and the quality of automated translations.", "method": "Seed-X consists of instruct and reasoning models pre-trained on a diverse dataset with 28 languages, enhanced through Chain-of-Thought reasoning and reinforcement learning for improved translation capabilities.", "result": "Seed-X outperforms larger open-source models and matches the performance of top closed-source models like Gemini-2.5 and GPT-4o across 28 languages, excelling in both automatic and human evaluations.", "conclusion": "The models and best practices are publicly available for research, aiming to advance the field of translation research and applications.", "key_contributions": ["Introduction of Seed-X, a family of open-source LLMs for multilingual translation.", "Integration of Chain-of-Thought reasoning and reinforcement learning to improve translation quality.", "Pre-training on a diverse high-quality dataset covering 28 languages."], "limitations": "", "keywords": ["multilingual translation", "large language models", "open-source", "Chain-of-Thought", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2410.03993", "pdf": "https://arxiv.org/pdf/2410.03993.pdf", "abs": "https://arxiv.org/abs/2410.03993", "title": "TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human Action Prediction", "authors": ["Kojiro Takeyama", "Yimeng Liu", "Misha Sra"], "categories": ["cs.HC"], "comment": "Accepted to IROS 2025", "summary": "Accurate prediction of human behavior is crucial for AI systems to\neffectively support real-world applications, such as autonomous robots\nanticipating and assisting with human tasks. Real-world scenarios frequently\npresent challenges such as occlusions and incomplete scene observations, which\ncan compromise predictive accuracy. Thus, traditional video-based methods often\nstruggle due to limited temporal and spatial perspectives. Large Language\nModels (LLMs) offer a promising alternative. Having been trained on a large\ntext corpus describing human behaviors, LLMs likely encode plausible sequences\nof human actions in a home environment. However, LLMs, trained primarily on\ntext data, lack inherent spatial awareness and real-time environmental\nperception. They struggle with understanding physical constraints and spatial\ngeometry. Therefore, to be effective in a real-world spatial scenario, we\npropose a multimodal prediction framework that enhances LLM-based action\nprediction by integrating physical constraints derived from human trajectories.\nOur experiments demonstrate that combining LLM predictions with trajectory data\nsignificantly improves overall prediction performance. This enhancement is\nparticularly notable in situations where the LLM receives limited scene\ninformation, highlighting the complementary nature of linguistic knowledge and\nphysical constraints in understanding and anticipating human behavior.", "AI": {"tldr": "This paper proposes a multimodal prediction framework that enhances LLM-based action prediction by integrating physical constraints from human trajectories, addressing the limitations of traditional video-based methods in predicting human behavior in real-world scenarios.", "motivation": "Accurate prediction of human behavior is essential for AI systems in applications like autonomous robots, but traditional methods struggle due to challenges such as occlusions and limited scene observations.", "method": "The authors propose a multimodal prediction framework that combines predictions from Large Language Models (LLMs) with physical constraints derived from human trajectories to improve action prediction accuracy.", "result": "Experiments show that the proposed framework significantly improves prediction performance, particularly in scenarios where the LLM has limited scene information, validating the integration of linguistic and physical knowledge.", "conclusion": "The study highlights the benefits of combining LLM predictions with physical trajectory data for better understanding and anticipation of human behavior in spatial contexts.", "key_contributions": ["Introduction of a multimodal framework to enhance LLM predictions.", "Demonstration of significant improvements in predictive accuracy by incorporating physical constraints.", "Validation of LLMs' utility in real-world spatial scenarios despite their limitations."], "limitations": "The approach still relies on LLMs which may not have real-time environmental perception capabilities.", "keywords": ["human behavior prediction", "Large Language Models", "multimodal framework", "trajecoty data", "autonomous systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13655", "pdf": "https://arxiv.org/pdf/2507.13655.pdf", "abs": "https://arxiv.org/abs/2507.13655", "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "authors": ["Teerapong Panboonyuen"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.", "AI": {"tldr": "CU-ICU is a method for customizing T5-based language models for ICU datasets, improving predictive accuracy in critical ICU tasks with minimal parameter tuning.", "motivation": "Existing methods struggle with domain adaptation and limited labeled data in healthcare while using large language models.", "method": "CU-ICU uses a sparse fine-tuning approach, combining few-shot prompting and selective parameter updates to adapt models efficiently.", "result": "Improvements in predictive accuracy and interpretability for tasks like sepsis detection and clinical note generation, with a 15% increase in accuracy and a 20% enhancement in explanations.", "conclusion": "CU-ICU is a scalable and low-overhead solution for accurate clinical decision support in ICU settings.", "key_contributions": ["Efficient adaptation of language models in healthcare with limited supervision.", "Significant improvements in critical task accuracy and interpretability.", "Minimal updates to model parameters for effective performance."], "limitations": "", "keywords": ["language models", "healthcare", "CU-ICU", "machine learning", "ICU"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2501.03572", "pdf": "https://arxiv.org/pdf/2501.03572.pdf", "abs": "https://arxiv.org/abs/2501.03572", "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study", "authors": ["Ammar Ahmed", "Margarida Fresco", "Fredrik Forsberg", "Hallvard Grotli"], "categories": ["cs.HC", "cs.AI", "cs.CL", "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"], "comment": null, "summary": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.", "AI": {"tldr": "A study evaluating ChatGPT's capability to generate accessible web pages according to WCAG revealed its strengths in resolving simple issues but challenges with complex tasks, indicating a need for manual oversight and improved prompt engineering.", "motivation": "To address the significant gap in web accessibility compliance among popular websites, this study explores how ChatGPT can aid in improving accessibility in line with established guidelines.", "method": "The study employs both automated and manual testing to evaluate ChatGPT's performance in generating accessible web content, focusing on how prompt engineering and visual reasoning can enhance outcomes.", "result": "ChatGPT effectively resolves simple accessibility issues but struggles with more complex tasks, highlighting the importance of human oversight and iterative engagement. Enhanced prompt instructions and the inclusion of visual aids improve performance significantly.", "conclusion": "While ChatGPT shows promise in assisting with web accessibility, the findings underline the necessity for human involvement and refined prompt strategies to maximize its potential.", "key_contributions": ["Evaluation of ChatGPT's ability to address web accessibility issues", "Incorporation of manual evaluation techniques and dynamic elements", "Demonstration of improved outcomes through effective prompt engineering"], "limitations": "The default outputs of ChatGPT often lack compliance with accessibility standards and require multiple iterations alongside human input for complex tasks.", "keywords": ["web accessibility", "ChatGPT", "WCAG", "large language models", "prompt engineering"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13666", "pdf": "https://arxiv.org/pdf/2507.13666.pdf", "abs": "https://arxiv.org/abs/2507.13666", "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.", "AI": {"tldr": "Keyword-inspired Cascade (KiC) is a framework that improves cost-efficient free-form text generation by selecting the best responses from weaker models and evaluating their alignment with semantic standards before escalating to more powerful models.", "motivation": "Despite the performance of large language models, their high API inference costs pose a barrier for widespread use. Cascade methods can reduce these costs but have limitations in selecting reliable outputs.", "method": "KiC employs a two-step process: it identifies the most representative answer from a weaker model's outputs and then evaluates other responses based on their semantic alignment with this representative answer to decide whether to use the output or escalate to a stronger model.", "result": "KiC demonstrates a 97.53% accuracy compared to GPT-4 while saving 28.81% on API costs on average, with improvements noted in specific benchmarks.", "conclusion": "The KiC framework shows promise in making high-quality text generation more accessible and cost-effective without compromising output quality.", "key_contributions": ["Introduction of the Keyword-inspired Cascade (KiC) framework for text generation.", "Improvement in cost efficiency for LLM operations by reducing reliance on expensive APIs.", "Achieving high accuracy comparable to state-of-the-art models with cheaper alternatives."], "limitations": "", "keywords": ["Language models", "Cost-efficiency", "Text generation", "Semantic alignment", "Cascade methods"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.13681", "pdf": "https://arxiv.org/pdf/2507.13681.pdf", "abs": "https://arxiv.org/abs/2507.13681", "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.", "AI": {"tldr": "LoopServe is an adaptive inference acceleration framework for large language models, addressing computational challenges in multi-turn dialogues by dynamically optimizing attention and cache strategies.", "motivation": "As conversation histories in real-world applications of large language models grow, computational and memory issues become significant hurdles, limiting responsiveness in multi-turn dialogues.", "method": "LoopServe introduces online sparsification during prefill by selecting important attention matrix parts and employs progressive key value compression based on recent output tokens during decoding.", "result": "LoopServe achieves superior effectiveness and significantly enhances LLM inference speed across various long-context dialogue tasks compared to existing methods.", "conclusion": "The proposed framework addresses the limitations of current methods and sets a new benchmark for handling multi-turn dialogue with large language models.", "key_contributions": ["Adaptive dual-phase inference acceleration", "Dynamic attention matrix sparsification", "Progressive key value compression for efficient caching"], "limitations": "", "keywords": ["Large Language Models", "Multi-turn Dialogues", "Inference Acceleration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13705", "pdf": "https://arxiv.org/pdf/2507.13705.pdf", "abs": "https://arxiv.org/abs/2507.13705", "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "categories": ["cs.CL", "cs.IR"], "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.", "AI": {"tldr": "This paper evaluates LLM-generated recommendations and explanations in Group Recommender Systems, comparing them to traditional aggregation strategies, revealing significant insights into their consistency and transparency.", "motivation": "To understand the effectiveness and reliability of LLM-generated recommendations and explanations in Group Recommender Systems compared to traditional aggregation methods.", "method": "The paper employs comparative analysis of LLM-generated recommendations against social choice-based aggregation strategies, focusing on Additive Utilitarian aggregation and evaluating the explanations provided by LLMs.", "result": "LLM-generated recommendations closely mirrored those from Additive Utilitarian aggregation, but their explanations often diverged, leading to issues with transparency and consistency, especially as group sizes increased.", "conclusion": "The study highlights the implications of LLMs in the Group Recommender Systems pipeline and raises concerns about the transparency and efficiency of conventional aggregation methods, especially in larger item sets.", "key_contributions": ["Evaluation of LLM effectiveness in Group Recommender Systems", "Insights into explanation quality and transparency issues", "Comparison with traditional aggregation strategies"], "limitations": "Inconsistent and ambiguous explanations limit transparency and explainability, particularly for larger group sizes.", "keywords": ["Large Language Models", "Group Recommender Systems", "Additive Utilitarian Aggregation", "Transparency", "Explainability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13732", "pdf": "https://arxiv.org/pdf/2507.13732.pdf", "abs": "https://arxiv.org/abs/2507.13732", "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "authors": ["Guillaume Zambrano"], "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.", "AI": {"tldr": "The study analyzes the influence of individual judges on custody decisions in French courts using ML to predict outcomes, highlighting that judges' decision patterns significantly affect results.", "motivation": "To challenge the notion that judges uniformly apply the law by assessing the impact of their individual decision-making on custody outcomes.", "method": "The analysis utilizes 18,937 rulings from 10,306 cases, comparing specialist models trained on individual judges' past rulings to a general model using aggregated data. A hybrid approach employing LLMs and traditional ML models (RF, XGB, SVC) is implemented.", "result": "Specialist models outperform the generalist model, achieving F1 scores up to 92.85% compared to 82.63%, indicating that individual judges' patterns can significantly influence legal outcomes.", "conclusion": "The findings support legal realism, demonstrating that judicial identity impacts case results, with data and code to be publicly shared for replication.", "key_contributions": ["Demonstrates the significant impact of individual judges on legal outcomes through machine learning", "Implements a hybrid approach combining LLMs and traditional ML for judicial decision-making", "Provides empirical support for the legal realism perspective in the context of custody rulings"], "limitations": "Study limited to French appellate courts and may not generalize to other legal systems.", "keywords": ["Machine Learning", "Legal Decision-Making", "Human Judges", "Custody Outcomes", "Legal Realism"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2501.08102", "pdf": "https://arxiv.org/pdf/2501.08102.pdf", "abs": "https://arxiv.org/abs/2501.08102", "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "authors": ["Wenlu Fan", "Yuqi Zhu", "Chenyang Wang", "Bin Wang", "Wentao Xu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026(Los Angeles, California, U.S.)", "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.", "AI": {"tldr": "This study examines how Large Language Models (LLMs) handle emotional content in social media, focusing on emotional transitions and semantic coherence using the Gemma and Llama models.", "motivation": "The study aims to understand the emotional consistency and semantic coherence of LLMs in social media contexts, which is crucial for their effective deployment in human-AI interaction.", "method": "The research involves analyzing climate change discussions from Twitter and Reddit, employing two open-source models, Gemma and Llama, across continuation and response tasks.", "result": "Findings indicate that Gemma amplifies negative emotions like anger but retains positive emotions like optimism, while Llama maintains emotional preservation across a wider range of affects. Both models show attenuated emotional intensity in generated responses compared to human-authored content, with Llama exhibiting superior semantic coherence.", "conclusion": "The insights gained on LLMs' emotional and semantic processing capabilities can inform design practices for human-AI interactions, especially in social media contexts.", "key_contributions": ["Demonstrated distinct emotional patterns in LLM outputs", "Showed the impact of emotional bias in LLM responses", "Analyzed the semantic similarities and differences in human vs LLM outputs"], "limitations": "The study is limited to emotional analysis in social media discussions and may not generalize to other domains.", "keywords": ["Large Language Models", "emotional coherence", "semantic similarity", "social media", "human-AI interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.13743", "pdf": "https://arxiv.org/pdf/2507.13743.pdf", "abs": "https://arxiv.org/abs/2507.13743", "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs", "authors": ["Maluna Menke", "Thilo Hagendorff"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.", "AI": {"tldr": "The paper evaluates parameter-efficient fine-tuning techniques to reduce biases in Large Language Models against LGBTQIA+ identities, demonstrating significant improvements with LoRA.", "motivation": "To address the biases against LGBTQIA+ users in Large Language Models that arise from their training data.", "method": "The study evaluates two PEFT techniques, Low-Rank Adaptation (LoRA) and soft-prompt tuning, using the WinoQueer benchmark to assess bias in three LLMs.", "result": "LoRA fine-tuning reduces bias scores by up to 50 points and improves neutrality for queer identities from virtually 0% to 36%. Soft-prompt tuning showed only minimal gains.", "conclusion": "LoRA is effective at reducing biases with minimal parameter increase, suggesting a need for broader adoption and better evaluation strategies for inclusivity in LLMs.", "key_contributions": ["Evaluation of LoRA and soft-prompt tuning for bias reduction in LLMs", "Quantitative analysis of bias using the WinoQueer benchmark", "Recommendations for larger queer-authored corpora and richer evaluation suites."], "limitations": "Soft-prompt tuning exhibited only marginal improvements, indicating limitations in its effectiveness compared to LoRA.", "keywords": ["Large Language Models", "LGBTQIA+ biases", "Low-Rank Adaptation", "soft-prompt tuning", "fairness in AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.13761", "pdf": "https://arxiv.org/pdf/2507.13761.pdf", "abs": "https://arxiv.org/abs/2507.13761", "title": "Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models", "authors": ["Palash Nandi", "Maithili Joshi", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Language models are highly sensitive to prompt formulations - small changes\nin input can drastically alter their output. This raises a critical question:\nTo what extent can prompt sensitivity be exploited to generate inapt content?\nIn this paper, we investigate how discrete components of prompt design\ninfluence the generation of inappropriate content in Visual Language Models\n(VLMs). Specifically, we analyze the impact of three key factors on successful\njailbreaks: (a) the inclusion of detailed visual information, (b) the presence\nof adversarial examples, and (c) the use of positively framed beginning\nphrases. Our findings reveal that while a VLM can reliably distinguish between\nbenign and harmful inputs in unimodal settings (text-only or image-only), this\nability significantly degrades in multimodal contexts. Each of the three\nfactors is independently capable of triggering a jailbreak, and we show that\neven a small number of in-context examples (as few as three) can push the model\ntoward generating inappropriate outputs. Furthermore, we propose a framework\nthat utilizes a skip-connection between two internal layers of the VLM, which\nsubstantially increases jailbreak success rates, even when using benign images.\nFinally, we demonstrate that memes, often perceived as humorous or harmless,\ncan be as effective as toxic visuals in eliciting harmful content, underscoring\nthe subtle and complex vulnerabilities of VLMs.", "AI": {"tldr": "This paper investigates how prompt sensitivity in Visual Language Models (VLMs) influences the generation of inappropriate content, focusing on factors like visual information, adversarial examples, and positively framed phrases.", "motivation": "Understanding how prompt design affects model behavior is essential for mitigating risks associated with inappropriate content generation in VLMs.", "method": "The study analyzes the impact of three key factors on jailbreak success in VLMs: detailed visual information, adversarial examples, and positively framed beginnings through experiments.", "result": "The research finds that VLMs struggle more in multimodal contexts, with each factor independently capable of triggering harmful outputs, especially with minimal context examples.", "conclusion": "The paper proposes a new framework that enhances jailbreak success rates and highlights the surprising effectiveness of memes in eliciting harmful content, revealing vulnerabilities in VLMs.", "key_contributions": ["Analyzed impact of prompt design on VLMs", "Proposed a framework that improves jailbreak success rates", "Showed memes can trigger harmful outputs similar to toxic visuals"], "limitations": "", "keywords": ["Visual Language Models", "prompt sensitivity", "inappropriate content generation", "jailbreaking", "adversarial examples"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.13793", "pdf": "https://arxiv.org/pdf/2507.13793.pdf", "abs": "https://arxiv.org/abs/2507.13793", "title": "An Enhanced Model-based Approach for Short Text Clustering", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Xuemeng Song", "Tian Gan", "Liqiang Nie"], "categories": ["cs.CL"], "comment": null, "summary": "Short text clustering has become increasingly important with the popularity\nof social media like Twitter, Google+, and Facebook. Existing methods can be\nbroadly categorized into two paradigms: topic model-based approaches and deep\nrepresentation learning-based approaches. This task is inherently challenging\ndue to the sparse, large-scale, and high-dimensional characteristics of the\nshort text data. Furthermore, the computational intensity required by\nrepresentation learning significantly increases the running time. To address\nthese issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet\nMultinomial Mixture model (GSDMM), which effectively handles the sparsity and\nhigh dimensionality of short texts while identifying representative words for\neach cluster. Based on several aspects of GSDMM that warrant further\nrefinement, we propose an improved approach, GSDMM+, designed to further\noptimize its performance. GSDMM+ reduces initialization noise and adaptively\nadjusts word weights based on entropy, achieving fine-grained clustering that\nreveals more topic-related information. Additionally, strategic cluster merging\nis employed to refine clustering granularity, better aligning the predicted\ndistribution with the true category distribution. We conduct extensive\nexperiments, comparing our methods with both classical and state-of-the-art\napproaches. The experimental results demonstrate the efficiency and\neffectiveness of our methods. The source code for our model is publicly\navailable at https://github.com/chehaoa/VEMC.", "AI": {"tldr": "This paper presents GSDMM+, an improved algorithm for short text clustering that addresses sparsity and high-dimensionality issues in data, achieving efficient and fine-grained clustering.", "motivation": "To tackle the challenges of short text clustering posed by the sparse and high-dimensional nature of social media data.", "method": "The paper introduces a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model (GSDMM) and its improved version GSDMM+, which reduces initialization noise and uses strategic cluster merging to enhance clustering performance.", "result": "Experiments show that GSDMM+ outperforms classical and state-of-the-art clustering methods, providing efficient handling of short texts and revealing topic-related information more effectively.", "conclusion": "GSDMM+ significantly improves upon existing models by refining clustering granularity and better matching predicted distributions with true categories.", "key_contributions": ["Development of GSDMM for effective handling of short text data.", "Introduction of GSDMM+ which optimizes performance through adaptive weighting and entropy adjustment.", "Demonstration of GSDMM+'s superiority in clustering efficiency through extensive experimental validation."], "limitations": "", "keywords": ["short text clustering", "Gibbs Sampling", "Dirichlet Multinomial Mixture", "GSDMM+", "topic modeling"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13827", "pdf": "https://arxiv.org/pdf/2507.13827.pdf", "abs": "https://arxiv.org/abs/2507.13827", "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.", "AI": {"tldr": "This paper presents a method for extracting key concepts from scientific articles using Question and Answer pairs through two approaches: a saliency-based LLM method and a Knowledge Graph-based method.", "motivation": "Scholars seek to quickly identify and understand the main ideas of research articles.", "method": "The paper proposes two approaches for generating QA pairs. The first approach selects salient paragraphs and uses a Large Language Model (LLM) to generate and rank questions. The second approach constructs a Knowledge Graph (KG) from scientific articles to generate QAs by extracting salient triplets.", "result": "The evaluation indicates that the KG-based approach effectively captures the main ideas discussed in the articles, providing high-quality QAs.", "conclusion": "Fine-tuning the Entity Relationship extraction model on the scientific corpus is crucial for optimizing triplet extraction quality.", "key_contributions": ["Development of two distinct QA generation approaches: LLM-based and KG-based.", "Introduction of a triplet TF-IDF-like measure for assessing entity saliency.", "Evaluation of the approaches using Subject Matter Experts to validate QA quality."], "limitations": "", "keywords": ["Question and Answer Generation", "Knowledge Graph", "Large Language Model", "Entity Relationship Extraction", "Scientific Articles"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13839", "pdf": "https://arxiv.org/pdf/2507.13839.pdf", "abs": "https://arxiv.org/abs/2507.13839", "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.", "AI": {"tldr": "The study investigates the link between language use and psychological states in Chinese counseling, revealing significant correlations with negative emotional words but not with first-person singular pronouns.", "motivation": "To explore the relationship between linguistic expressions and psychological states of depression and anxiety in Chinese psycho-counseling interactions.", "method": "Analysis of a corpus from 735 online counseling sessions using a general linear mixed-effect model and LIWC software.", "result": "A significant positive correlation was found between negative emotional words and the severity of depression and anxiety, while first-person singular pronoun usage showed no significant variation with psychological conditions.", "conclusion": "The study emphasizes the cultural differences in language use in mental health contexts and their implications for therapeutic practices in Chinese populations.", "key_contributions": ["Identifies linguistic patterns related to psychological states in Chinese counseling settings.", "Challenges prior findings from English language contexts regarding pronoun use.", "Highlights cultural influences on mental health communication."], "limitations": "The findings may not generalize to non-Chinese speaking populations or different counseling methods.", "keywords": ["psychological states", "language use", "Chinese counseling", "negative emotional words", "cultural context"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.13841", "pdf": "https://arxiv.org/pdf/2507.13841.pdf", "abs": "https://arxiv.org/abs/2507.13841", "title": "Modeling Fair Play in Detective Stories with Language Models", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "categories": ["cs.CL"], "comment": null, "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.", "AI": {"tldr": "This paper presents a probabilistic framework to analyze fair play in detective fiction, particularly in LLM-generated stories, highlighting the trade-off between coherence and surprise.", "motivation": "To explore the balance between reader expectations and narrative surprises in detective fiction, and to define fair play in this context.", "method": "Develops a probabilistic framework for defining fair play and metrics associated with it. Applies the framework to analyze LLM-generated detective stories.", "result": "The framework shows that LLM-generated stories are unpredictable but struggle to balance surprise with fair play, impacting their overall quality.", "conclusion": "Improving LLM-generated narratives requires addressing the trade-off between coherence and surprise to enhance story quality.", "key_contributions": ["Probabilistic framework for analyzing detective fiction", "Definition and metrics for fair play", "Application of the framework to LLM-generated stories"], "limitations": "Focused solely on detective fiction; only examines LLM-generated stories.", "keywords": ["detective fiction", "fair play", "probabilistic framework", "LLM", "story quality"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2507.13858", "pdf": "https://arxiv.org/pdf/2507.13858.pdf", "abs": "https://arxiv.org/abs/2507.13858", "title": "InTraVisTo: Inside Transformer Visualisation Tool", "authors": ["Nicolò Brunello", "Davide Rigamonti", "Andrea Sassella", "Vincenzo Scotti", "Mark James Carman"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.", "AI": {"tldr": "This paper introduces InTraVisTo, a tool for visualizing the internal state and information flow of Transformer-based LLMs to enhance the understanding of their reasoning processes.", "motivation": "To address the challenges of using LLMs in production by providing better insight into their unpredictable behavior and improving understanding of their internal computations.", "method": "InTraVisTo visualizes token embeddings at each layer of a Transformer model and illustrates the information flow between components using Sankey diagrams.", "result": "The tool enables researchers to trace the computational processes involved in generating each token, facilitating deeper insights into the reasoning capabilities of LLMs.", "conclusion": "InTraVisTo aids researchers and practitioners in understanding internal patterns of LLMs, potentially leading to improvements in their application.", "key_contributions": ["Introduction of InTraVisTo tool for visualizing LLMs' internal states", "Use of Sankey diagrams to illustrate information flow", "Facilitation of understanding LLM reasoning processes"], "limitations": "", "keywords": ["Large Language Models", "Transformer", "Visualization", "LLM reasoning", "InTraVisTo"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.13870", "pdf": "https://arxiv.org/pdf/2507.13870.pdf", "abs": "https://arxiv.org/abs/2507.13870", "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "categories": ["cs.CL"], "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.", "AI": {"tldr": "The paper explores the issue of label unification in cybersecurity named entity recognition, proposing new models to improve dataset usability and performance.", "motivation": "The lack of standardized labels in cybersecurity NER complicates the merging of datasets, which is crucial for improving model performance.", "method": "The authors perform a coarse-grained label unification across four cybersecurity datasets, evaluate models using BiLSTM, and explore alternative architectures such as a multihead model and a graph-based transfer model.", "result": "Models trained on unified datasets exhibited poor generalization across different datasets. The multihead model offered only marginal improvements, while the graph-based transfer model did not show significant performance enhancements compared to BERT-base-NER.", "conclusion": "The study highlights the limitations in label unification in cybersecurity NER and suggests the need for further exploration of architectures to enhance performance.", "key_contributions": ["Investigated label unification across multiple cybersecurity datasets.", "Conducted pairwise cross-dataset evaluations using BiLSTM models.", "Proposed alternative architectures to address unification limitations."], "limitations": "The proposed models provided only marginal improvements in generalization across datasets, indicating that more robust solutions are needed for effective label unification.", "keywords": ["Cybersecurity", "NER", "Label unification", "BiLSTM", "BERT"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.13875", "pdf": "https://arxiv.org/pdf/2507.13875.pdf", "abs": "https://arxiv.org/abs/2507.13875", "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies", "authors": ["Carlos Mena", "Pol Serra", "Jacobo Romero", "Abir Messaoudi", "Jose Giraldo", "Carme Armentano-Oller", "Rodolfo Zevallos", "Ivan Meza", "Javier Hernando"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.", "AI": {"tldr": "This paper addresses automated speech recognition challenges posed by code-switching between Catalan and Spanish by enhancing ASR models through synthetic data, concatenation of monolingual audio, and real CS data.", "motivation": "The lack of dedicated code-switching datasets limits the performance of ASR systems, particularly in multilingual settings where code-switching is prevalent.", "method": "The authors utilized three strategies for improving ASR: generating synthetic code-switching data, concatenating monolingual audio samples, and fine-tuning existing models with language tokens using real code-switching data.", "result": "Combining synthetic code-switching data with dominant language tokens resulted in the highest transcription performance in ASR tasks for Catalan-Spanish code-switching.", "conclusion": "The study demonstrates that effective use of synthetic and real data can significantly enhance ASR for languages with code-switching phenomena, potentially improving multilingual communication.", "key_contributions": ["Development of synthetic code-switching datasets for ASR", "Fine-tuning Whisper models on real and synthetic CS data", "Demonstrated effective strategies for improving transcription performance in multilingual ASR systems"], "limitations": "", "keywords": ["Code-switching", "Automated speech recognition", "Catalan-Spanish", "Synthetic data", "Whisper models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.13881", "pdf": "https://arxiv.org/pdf/2507.13881.pdf", "abs": "https://arxiv.org/abs/2507.13881", "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test", "authors": ["Cole Walsh", "Rodica Ivan", "Muhammad Zafar Iqbal", "Colleen Robb"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States", "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.", "AI": {"tldr": "This paper proposes a novel approach to automating the scoring of Situational Judgment Tests (SJTs) using large language models, addressing previous challenges in measuring personal and professional skills.", "motivation": "To address the need for scalable systems to measure personal and professional skills critical for student success in diverse careers, especially through standardized tests like SJTs.", "method": "The study employs large language models (LLMs) to extract construct-relevant features from open-response SJT answers, specifically utilizing the Casper SJT as a case study.", "result": "The proposed method shows efficacy in automating the scoring of SJTs, potentially overcoming limitations of past NLP scoring systems related to construct validity.", "conclusion": "This work lays the groundwork for future automated scoring developments for personal and professional skill assessments, ultimately aiming to enhance educational measurement practices.", "key_contributions": ["Novel approach using LLMs for scoring SJTs", "Demonstrates efficacy with Casper SJT", "Sets foundation for future scalable scoring systems"], "limitations": "The study may face challenges related to the generalizability of LLMs across different contexts of SJTs and varying constructs.", "keywords": ["Situational Judgment Tests", "large language models", "automated scoring", "personal skills", "professional skills"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.13913", "pdf": "https://arxiv.org/pdf/2507.13913.pdf", "abs": "https://arxiv.org/abs/2507.13913", "title": "Political Leaning and Politicalness Classification of Texts", "authors": ["Matous Volf", "Jakub Simko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.", "AI": {"tldr": "This paper explores the classification of text based on political leaning and politicalness using transformer models, highlighting challenges with current approaches and presenting a new diverse dataset.", "motivation": "To overcome limitations of existing models that perform poorly on out-of-distribution texts in classifying political leaning and politicalness.", "method": "The authors compiled a diverse dataset by merging 12 datasets for political leaning classification and extending 18 datasets to create a new one for politicalness. They employed extensive benchmarking with leave-one-in and leave-one-out methodologies to evaluate model performance.", "result": "The study reveals that existing models have limited generalization capabilities, while the new dataset helps improve classification performance on a wider range of texts.", "conclusion": "The paper concludes that enhanced datasets and benchmarking techniques can significantly improve the performance of models in political text classification.", "key_contributions": ["Development of a new diverse dataset for politicalness classification", "Comprehensive overview of existing datasets and models", "Improved benchmarking methodologies for evaluating model performance"], "limitations": "The new dataset's effectiveness is yet to be tested on a broader range of out-of-distribution texts beyond those included.", "keywords": ["political leaning", "politicalness", "transformer models", "text classification", "dataset creation"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.13919", "pdf": "https://arxiv.org/pdf/2507.13919.pdf", "abs": "https://arxiv.org/abs/2507.13919", "title": "The Levers of Political Persuasion with Conversational AI", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.", "AI": {"tldr": "Large-scale experiments evaluate the persuasiveness of LLMs on political issues, revealing that prompting and post-training greatly enhance persuasion while potentially decreasing factual accuracy.", "motivation": "To address concerns over the influence of conversational AI on human beliefs and examine the factors contributing to AI persuasiveness.", "method": "Conducted three large-scale experiments involving 76,977 participants and 19 LLMs evaluated on 707 political issues, measuring the impacts of post-training and prompting.", "result": "The study found that persuasion increased by up to 51% through post-training and 27% through prompting, while also noting a decrease in factual accuracy.", "conclusion": "Current fears about AI persuasion power are exaggerated; enhancements from post-training and prompting are significant, but these methods can also reduce accuracy.", "key_contributions": ["Showed the significant impact of post-training and prompting on AI persuasiveness.", "Measured the effects on factual accuracy alongside persuasion.", "Provided a large-scale empirical analysis involving extensive data and LLM claims."], "limitations": "Potential bias in chosen political issues and the context of experiments may limit generalizability.", "keywords": [" Conversational AI", " Persuasion", " Factual accuracy", " Large Language Models", " Political issues"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13937", "pdf": "https://arxiv.org/pdf/2507.13937.pdf", "abs": "https://arxiv.org/abs/2507.13937", "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support", "authors": ["Jan Trienes", "Anastasiia Derzhanskaia", "Roland Schwarzkopf", "Markus Mühling", "Jörg Schlötterer", "Christin Seifert"], "categories": ["cs.CL"], "comment": null, "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.", "AI": {"tldr": "Marcel is a lightweight conversational agent that supports prospective students with admission inquiries by providing personalized and fast responses while minimizing staff workload.", "motivation": "To assist prospective students in obtaining admission-related information quickly and reduce the workload on university staff.", "method": "The system employs retrieval-augmented generation to ground responses in university resources, using an FAQ retriever to enhance the accuracy of information retrieval from a knowledge base.", "result": "Marcel improves on traditional dense/hybrid retrieval methods and is designed for easy deployment in resource-constrained academic settings, providing insights from its real-world application.", "conclusion": "The deployment and evaluation of Marcel demonstrate its effectiveness in answering admission queries while alleviating the burden on university administration.", "key_contributions": ["Development of a lightweight conversational agent for educational institutions.", "Introduction of an FAQ retriever that enhances information retrieval from knowledge bases.", "Technical evaluation showing the effectiveness of the system in real-world scenarios."], "limitations": "", "keywords": ["Conversational agent", "Retrieval-augmented generation", "Educational technology", "Knowledge base", "Natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.13949", "pdf": "https://arxiv.org/pdf/2507.13949.pdf", "abs": "https://arxiv.org/abs/2507.13949", "title": "Exploiting Primacy Effect To Improve Large Language Models", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.", "AI": {"tldr": "This paper investigates the effects of primacy bias in fine-tuned Large Language Models (LLMs) on Multiple Choice Question Answering, and introduces a method to reorder answer options to improve accuracy.", "motivation": "The study aims to understand and leverage positional biases in LLMs, specifically the primacy effect, in the context of improving performance in Multiple Choice Question Answering tasks.", "method": "The research involves examining how fine-tuning processes amplify primacy bias due to exposure to human-like patterns, and it proposes a reordering strategy for answer options based on semantic similarity to the query.", "result": "Experimental results indicate that the proposed reordering technique significantly enhances performance in MCQA tasks by mitigating the influence of primacy bias.", "conclusion": "The findings highlight both the challenges posed by biases in LLMs and the potential for them to be harnessed strategically in NLP applications, particularly for bias-aware model design.", "key_contributions": ["Demonstration of amplified primacy bias in fine-tuned LLMs", "Development of a method to reorder answer options based on semantic similarity", "Insights into bias-aware model design for NLP applications"], "limitations": "", "keywords": ["Large Language Models", "Natural Language Processing", "Multiple Choice Question Answering"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.13966", "pdf": "https://arxiv.org/pdf/2507.13966.pdf", "abs": "https://arxiv.org/abs/2507.13966", "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.", "AI": {"tldr": "This paper presents a bottom-up approach to training language models for deep domain expertise using knowledge graphs to synthesize tasks, specifically validating the method in the medical domain with significant performance improvements on reasoning tasks.", "motivation": "To address the insufficiency of traditional top-down training in language models for acquiring deep domain expertise, a bottom-up approach is proposed that utilizes knowledge graphs.", "method": "A task generation pipeline is developed to synthesize reasoning tasks from knowledge graph primitives and fine-tune language models on this domain-specific curriculum.", "result": "The QwQ-Med-3 model, fine-tuned on a medical knowledge graph, significantly outperforms state-of-the-art reasoning models on diverse medical tasks and question-answer benchmarks.", "conclusion": "The study suggests a shift towards using compositional structures in knowledge graphs to enhance task reasoning in language models, potentially leading to domain-specific superintelligence in areas like medicine.", "key_contributions": ["Development of a knowledge graph-based task generation pipeline for language models", "Introduction of the QwQ-Med-3 model demonstrating advancements in medical reasoning", "Creation of ICD-Bench, an evaluation suite for assessing reasoning abilities in medical domains"], "limitations": "", "keywords": ["Language Models", "Knowledge Graphs", "Medical Reasoning", "Superintelligence", "Task Generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.13977", "pdf": "https://arxiv.org/pdf/2507.13977.pdf", "abs": "https://arxiv.org/abs/2507.13977", "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic", "authors": ["Lilit Grigoryan", "Nikolay Karpov", "Enas Albasiri", "Vitaly Lavrukhin", "Boris Ginsburg"], "categories": ["cs.CL", "eess.AS", "I.5.1"], "comment": "Accepted to ICASSP 2025", "summary": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.", "AI": {"tldr": "This paper presents a universal methodology for Arabic speech and text processing with novel ASR models for Modern Standard Arabic and a unified model for both Modern Standard and Classical Arabic, achieving state-of-the-art results.", "motivation": "To address the significant challenges faced in developing Automatic Speech Recognition systems for Arabic, particularly due to the language's complexity and the lack of public models.", "method": "The paper introduces a universal methodology for Arabic speech and text processing, utilizing the FastConformer architecture to train two models: one for Modern Standard Arabic and a unified model for both Modern Standard and Classical Arabic.", "result": "The MSA model achieves state-of-the-art performance on related datasets, while the unified model achieves state-of-the-art accuracy for Classical Arabic with diacritics, maintaining strong performance for MSA.", "conclusion": "The models and their training recipes are open-sourced to promote reproducibility and future work in Arabic ASR.", "key_contributions": ["Introduction of a universal methodology for Arabic speech processing", "Training of a new MSA model with state-of-the-art performance", "Development of the first unified public model for both MSA and Classical Arabic."], "limitations": "", "keywords": ["Arabic ASR", "FastConformer", "Modern Standard Arabic", "Classical Arabic", "Speech Processing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.14017", "pdf": "https://arxiv.org/pdf/2507.14017.pdf", "abs": "https://arxiv.org/abs/2507.14017", "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.", "AI": {"tldr": "RHYTHM is a framework using LLMs for predicting human mobility by partitioning trajectories into tokens, improving computational efficiency and accuracy in spatio-temporal reasoning.", "motivation": "To improve human mobility predictions using LLMs while enhancing computational efficiency and capturing complex dependencies in trajectory data.", "method": "RHYTHM partitions trajectories into daily segments, encoding them as discrete tokens with hierarchical attention and utilizing frozen LLM prompt embeddings.", "result": "Demonstrated a 2.4% accuracy improvement, 5.0% increase in accuracy on weekends, and a 24.6% reduction in training time on three real-world datasets.", "conclusion": "RHYTHM effectively enhances spatio-temporal prediction of human mobility while optimizing for computational efficiency.", "key_contributions": ["Introduction of hierarchical attention for trajectory tokenization", "Use of frozen LLMs to enrich token representations", "Significant improvements in prediction accuracy and training efficiency"], "limitations": "", "keywords": ["Human Mobility", "Large Language Models", "Spatio-Temporal Prediction", "Hierarchical Tokenization", "Trajectory Reasoning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.14022", "pdf": "https://arxiv.org/pdf/2507.14022.pdf", "abs": "https://arxiv.org/abs/2507.14022", "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis", "authors": ["Jianfei Li", "Kevin Kam Fung Yuen"], "categories": ["cs.CL", "cs.LG"], "comment": "35 pages, 33 tables, 6 Figures", "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.", "AI": {"tldr": "This study introduces the CPC-CMS framework for selecting the best document-level sentiment analysis model using expert-weighted evaluation criteria.", "motivation": "To improve the model selection process in document-level sentiment analysis by utilizing expert knowledge and a systematic approach to evaluate multiple classification models.", "method": "The study employs a Cognitive Pairwise Comparison approach to weight various evaluation criteria for sentiment analysis models, then applies baseline models like Naive Bayes, SVC, Random Forest, and others on three social media datasets to identify the optimal model.", "result": "The results indicate that ALBERT performs best on the datasets in terms of accuracy when time is not a factor, while no single model consistently outperforms others when time consumption is considered.", "conclusion": "CPC-CMS offers a valuable framework for model selection, adaptable to different classification tasks beyond sentiment analysis.", "key_contributions": ["Introduction of the CPC-CMS framework for model selection", "Application and demonstration on sentiment analysis using multiple datasets", "Evaluation of various classification models with a focus on expert-driven criteria weighting"], "limitations": "Primarily focused on sentiment analysis; the applicability to other domains remains to be validated in further studies.", "keywords": ["Cognitive Pairwise Comparison", "Sentiment Analysis", "Model Selection", "Machine Learning", "ALBERT"], "importance_score": 6, "read_time_minutes": 35}}
{"id": "2507.14045", "pdf": "https://arxiv.org/pdf/2507.14045.pdf", "abs": "https://arxiv.org/abs/2507.14045", "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks", "authors": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "Jimmy Huang"], "categories": ["cs.CL"], "comment": "Accepted at Canadian AI 2025", "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.", "AI": {"tldr": "Evaluation of cost-efficient Large Language Models (LLMs) for biomedical tasks shows varying performance across models.", "motivation": "To assess the effectiveness of different LLMs in biomedical applications involving text and image data.", "method": "Evaluation of various closed-source and open-source LLMs on tasks like biomedical text classification, generation, and multimodal image processing.", "result": "No single LLM consistently outperforms others; different models excel at different tasks, with open-source LLMs sometimes delivering superior results.", "conclusion": "The findings provide guidance for choosing the most suitable LLMs for specific biomedical applications, considering both performance and privacy.", "key_contributions": ["Comprehensive evaluation of LLMs for diverse biomedical tasks", "Insights into performance differences between closed-source and open-source models", "Recommendations for model selection in biomedical applications"], "limitations": "No evaluation of LLMs beyond biomedical domains.", "keywords": ["Large Language Models", "biomedical applications", "multimodal processing", "text classification", "open-source models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.14063", "pdf": "https://arxiv.org/pdf/2507.14063.pdf", "abs": "https://arxiv.org/abs/2507.14063", "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog", "authors": ["Lautaro Estienne", "Gabriel Ben Zenou", "Nona Naderi", "Jackie Cheung", "Pablo Piantanida"], "categories": ["cs.CL"], "comment": null, "summary": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.", "AI": {"tldr": "This paper introduces the Collaborative Rational Speech Act (CRSA) framework, an extension of the RSA model that enhances pragmatic reasoning in multi-turn dialogs by optimizing a gain function designed for collaborative scenarios.", "motivation": "The need for AI systems to effectively collaborate by understanding shared goals and beliefs in conversational contexts.", "method": "CRSA utilizes an information-theoretic approach, optimizing a gain function derived from rate-distortion theory to better model multi-turn dialog and private information conditions.", "result": "CRSA outperforms existing baselines in terms of consistency, interpretability, and collaborative behavior in referential games and medical dialog scenarios.", "conclusion": "The CRSA framework advances the capabilities of language agents towards more pragmatic and socially aware interactions.", "key_contributions": ["Introduction of the CRSA framework for collaborative dialog", "Application of information-theoretic principles to multi-turn conversation modeling", "Demonstration of improved performance in medical dialog scenarios"], "limitations": "", "keywords": ["Collaborative dialog", "Rational Speech Act", "Information theory", "AI language agents", "Medical applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.14079", "pdf": "https://arxiv.org/pdf/2507.14079.pdf", "abs": "https://arxiv.org/abs/2507.14079", "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "authors": ["Garapati Keerthana", "Manik Gupta"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.", "AI": {"tldr": "DENSE is a system that generates clinically coherent progress notes by simulating physician workflows and leveraging a retrieval strategy with a large language model.", "motivation": "The lack of progress notes in EHR datasets limits the understanding of patients' longitudinal health stories and clinical decision-making.", "method": "DENSE categorizes notes finely and aligns them temporally, using clinically relevant retrieval strategies to prompt an LLM for generating notes.", "result": "DENSE-generated notes show improved longitudinal fidelity and coherence, achieving a temporal alignment ratio of 1.089.", "conclusion": "DENSE aids in restoring narrative coherence in healthcare documentation, which enhances summarization, predictive modeling, and clinical decision support.", "key_contributions": ["Development of a fine-grained note categorization", "Introduction of a temporal alignment mechanism", "Integration of LLM for generating coherent clinical notes"], "limitations": "Requires comprehensive initial patient data and progress note documentation for optimal performance.", "keywords": ["Electronic Health Record", "progress notes", "large language model", "healthcare documentation", "clinical decision support"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.14096", "pdf": "https://arxiv.org/pdf/2507.14096.pdf", "abs": "https://arxiv.org/abs/2507.14096", "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "AI": {"tldr": "This study evaluates the adaptation of biomedical literature to plain language using language models, revealing both promising results and the need for improved evaluation methods.", "motivation": "To make biomedical literature more accessible to patients and caregivers by using language models for adaptation to plain language.", "method": "Hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the Text Retrieval Conferences, involving tasks for rewriting abstracts and replacing difficult terms, with evaluations from biomedical experts.", "result": "Top-performing models achieved human-level accuracy in factual content but fell short in simplicity; LLM systems performed well in generating accurate replacements for difficult terms but struggled overall with classification and brevity.", "conclusion": "The PLABA track demonstrated the potential of LLMs in adapting biomedical literature for public consumption, while also underlining the necessity for better automatic evaluation tools.", "key_contributions": ["Evaluation of the PLABA track to assess LLMs in biomedical literature adaptation", "Development of extensive manual evaluation frameworks from biomedical experts", "Identification of challenges in automatic benchmarking for LLM adaptations"], "limitations": "Models excelled in accuracy but struggled with simplicity and brevity; automatic evaluation metrics did not align well with manual assessments.", "keywords": ["Biomedical Literature", "Plain Language", "Language Models", "Evaluation", "Text Retrieval Conferences"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2303.18162", "pdf": "https://arxiv.org/pdf/2303.18162.pdf", "abs": "https://arxiv.org/abs/2303.18162", "title": "ViMMRC 2.0 -- Enhancing Machine Reading Comprehension on Vietnamese Literature Text", "authors": ["Son T. Luu", "Khoi Trong Hoang", "Tuong Quang Pham", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "Accepted for publication at International Journal of Asian Language\n  Processing", "summary": "Machine reading comprehension has been an interesting and challenging task in\nrecent years, with the purpose of extracting useful information from texts. To\nattain the computer ability to understand the reading text and answer relevant\ninformation, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for\nthe task of multiple-choice reading comprehension in Vietnamese Textbooks which\ncontain the reading articles for students from Grade 1 to Grade 12. This\ndataset has 699 reading passages which are prose and poems, and 5,273\nquestions. The questions in the new dataset are not fixed with four options as\nin the previous version. Moreover, the difficulty of questions is increased,\nwhich challenges the models to find the correct choice. The computer must\nunderstand the whole context of the reading passage, the question, and the\ncontent of each choice to extract the right answers. Hence, we propose a\nmulti-stage approach that combines the multi-step attention network (MAN) with\nthe natural language inference (NLI) task to enhance the performance of the\nreading comprehension model. Then, we compare the proposed methodology with the\nbaseline BERTology models on the new dataset and the ViMMRC 1.0. From the\nresults of the error analysis, we found that the challenge of the reading\ncomprehension models is understanding the implicit context in texts and linking\nthem together in order to find the correct answers. Finally, we hope our new\ndataset will motivate further research to enhance the ability of computers to\nunderstand the Vietnamese language.", "AI": {"tldr": "Introduction of ViMMRC 2.0 for multiple-choice reading comprehension in Vietnamese with an enhanced dataset and methodology.", "motivation": "To improve the capability of computers in understanding Vietnamese texts and facilitating reading comprehension tasks.", "method": "A multi-stage approach combining a multi-step attention network (MAN) with natural language inference (NLI) to enhance reading comprehension model performance.", "result": "The proposed methodology outperformed baseline BERTology models on the new dataset, highlighting the challenge of implicit context understanding.", "conclusion": "The new dataset aims to stimulate further research in Vietnamese language comprehension.", "key_contributions": ["Introduction of ViMMRC 2.0 dataset for Vietnamese reading comprehension", "Increased question difficulty to challenge existing models", "Proposed multi-stage attention methodology to improve performance"], "limitations": "", "keywords": ["Machine reading comprehension", "Vietnamese Textbooks", "Multi-step attention network", "Natural language inference", "Dataset enhancement"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2404.07053", "pdf": "https://arxiv.org/pdf/2404.07053.pdf", "abs": "https://arxiv.org/abs/2404.07053", "title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources.", "AI": {"tldr": "Meta4XNLI is a new dataset for metaphor detection and interpretation in English and Spanish, evaluating language models' performance with non-literal language.", "motivation": "Understanding metaphors is important for Language Models to accurately interpret figurative language, which is common in everyday communication.", "method": "The authors created the Meta4XNLI dataset containing metaphor annotations in both Spanish and English, and conducted monolingual and cross-lingual experiments to assess language models' abilities in metaphor identification and understanding.", "result": "Experiments revealed insights into language models' performance on metaphor tasks, highlighting challenges faced with non-literal expressions and shedding light on metaphor transferability and translation effects.", "conclusion": "The study emphasizes the need for better metaphor comprehension in language models and suggests further exploration of multilingual annotated datasets.", "key_contributions": ["Introduction of the Meta4XNLI dataset for metaphor detection and interpretation", "Analysis of language models' metaphor identification and understanding capabilities", "Insights on the effects of translation on metaphor understanding across languages"], "limitations": "Limited to metaphorical expressions, may not generalize to all figurative language forms.", "keywords": ["metaphor detection", "language models", "cross-lingual", "NLI", "dataset"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2407.10266", "pdf": "https://arxiv.org/pdf/2407.10266.pdf", "abs": "https://arxiv.org/abs/2407.10266", "title": "psifx -- Psychological and Social Interactions Feature Extraction Package", "authors": ["Guillaume Rochette", "Mathieu Rochat", "Matthew J. Vowels"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to\nfacilitate and democratize the use of state-of-the-art machine learning\ntechniques for human sciences research. It is motivated by a need (a) to\nautomate and standardize data annotation processes that typically require\nexpensive, lengthy, and inconsistent human labour; (b) to develop and\ndistribute open-source community-driven psychology research software; and (c)\nto enable large-scale access and ease of use for non-expert users. The\nframework contains an array of tools for tasks such as speaker diarization,\nclosed-caption transcription and translation from audio; body, hand, and facial\npose estimation and gaze tracking with multi-person tracking from video; and\ninteractive textual feature extraction supported by large language models. The\npackage has been designed with a modular and task-oriented approach, enabling\nthe community to add or update new tools easily. This combination creates new\nopportunities for in-depth study of real-time behavioral phenomena in\npsychological and social science research.", "AI": {"tldr": "psifx is a multi-modal feature extraction toolkit for automating and standardizing data annotation in human sciences research.", "motivation": "To automate data annotation, develop open-source psychology research software, and ensure accessibility for non-expert users.", "method": "The toolkit includes tools for speaker diarization, closed-caption transcription, pose estimation, gaze tracking, and textual feature extraction with LLM support.", "result": "psifx offers a modular framework that simplifies data processing tasks for psychological and social sciences.", "conclusion": "The framework enhances the capability for real-time behavioral studies in human sciences research.", "key_contributions": ["Plug-and-play nature for ease of use", "Support for various data types (audio, video, text)", "Community-driven development for continuous improvement"], "limitations": "", "keywords": ["Machine Learning", "Human-Computer Interaction", "Psychological Research", "Open Source", "Feature Extraction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2409.04617", "pdf": "https://arxiv.org/pdf/2409.04617.pdf", "abs": "https://arxiv.org/abs/2409.04617", "title": "Sparse Rewards Can Self-Train Dialogue Agents", "authors": ["Barrett Martin Lattimer", "Varun Gangal", "Ryan McDonald", "Yi Yang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training", "AI": {"tldr": "This paper presents JOSH, a self-improvement paradigm for LLMs that enables them to enhance their performance without human feedback, using a novel sparse reward simulation environment.", "motivation": "To address the challenges of obtaining meaningful human feedback for LLMs as they improve, the authors propose a self-improvement approach that allows LLMs to autonomously enhance their performance.", "method": "Introducing JOSH (Juxtaposed Outcomes for Simulation Harvesting), a self-alignment algorithm that uses a sparse reward simulation environment to extract optimal behaviors and retrain LLMs on their own outputs.", "result": "Models trained with JOSH show significant improvement in tool-based interactions while maintaining general capabilities across various benchmarks.", "conclusion": "The JOSH method demonstrates that LLMs can successfully self-improve, making the traditional reliance on human feedback less necessary as model capabilities grow.", "key_contributions": ["Implementation of a self-alignment algorithm for LLMs", "Creation of ToolWOZ, a sparse reward tool-calling simulation environment", "Demonstrated significant performance improvement in LLMs without external feedback."], "limitations": "", "keywords": ["Large Language Models", "Self-Improvement", "Machine Learning", "Human Feedback", "Simulation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.13394", "pdf": "https://arxiv.org/pdf/2410.13394.pdf", "abs": "https://arxiv.org/abs/2410.13394", "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs", "authors": ["Sumanth Doddapaneni", "Mohammed Safi Ur Rahman Khan", "Dilip Venkatesh", "Raj Dabre", "Anoop Kunchukuttan", "Mitesh M. Khapra"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.", "AI": {"tldr": "The paper presents the Cross Lingual Auto Evaluation (CIA) Suite, a framework designed for evaluating machine-generated text in multiple languages, addressing the existing gap in multilingual evaluation methodologies.", "motivation": "There is a significant gap in evaluation methodologies for machine-generated text, especially for non-English languages, as most existing approaches focus on English.", "method": "The study introduces the CIA Suite, which includes evaluator LLMs and a novel test set designed for multilingual evaluation, featuring human-annotated instructions and human judgment scores across six languages.", "result": "Hercule, the proposed model, demonstrates a closer alignment with human judgments than proprietary models and is effective in zero-shot evaluation on unseen languages.", "conclusion": "The research showcases a scalable and effective approach for cross-lingual evaluation using LLMs, with all resources to be publicly available for further research.", "key_contributions": ["Introduction of the CIA Suite for multilingual evaluation", "Development of Hercule, a cross-lingual evaluation model", "Creation of a test set with human-annotated instructions across six languages"], "limitations": "", "keywords": ["cross-lingual evaluation", "multilingual LLMs", "automated evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.00152", "pdf": "https://arxiv.org/pdf/2501.00152.pdf", "abs": "https://arxiv.org/abs/2501.00152", "title": "Temporal reasoning for timeline summarisation in social media", "authors": ["Jiayu Song", "Mahmud Elahi Akhter", "Dana Atzil Slonim", "Maria Liakata"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.", "AI": {"tldr": "This paper investigates the impact of temporal reasoning in Large Language Models for enhancing timeline summarisation. It introduces a new dataset, NarrativeReason, and combines temporal reasoning with summarisation through knowledge distillation, achieving better performance in mental health contexts.", "motivation": "To determine if improving temporal reasoning in LLMs can enhance the quality of timeline summarisation, especially in the context of mental health-related discussions in social media.", "method": "The paper presents NarrativeReason, a new dataset focused on temporal relationships, and employs a knowledge distillation framework to transfer temporal reasoning capabilities from a teacher model to a student model, aimed at improving timeline summarisation.", "result": "The proposed model outperforms existing methods on mental health timeline summarisation tasks, showcasing enhanced handling of long texts with repetitive events.", "conclusion": "Leveraging temporal reasoning significantly improves the quality of timeline summarisation in LLMs, particularly for complex narratives involving emotions.", "key_contributions": ["Introduction of NarrativeReason dataset for temporal reasoning in narratives.", "Combination of temporal reasoning and timeline summarisation through a knowledge distillation framework.", "Demonstrated superior performance on mental health-related timeline summarisation tasks."], "limitations": "", "keywords": ["temporal reasoning", "timeline summarisation", "large language models", "mental health", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.08102", "pdf": "https://arxiv.org/pdf/2501.08102.pdf", "abs": "https://arxiv.org/abs/2501.08102", "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "authors": ["Wenlu Fan", "Yuqi Zhu", "Chenyang Wang", "Bin Wang", "Wentao Xu"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026(Los Angeles, California, U.S.)", "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.", "AI": {"tldr": "This study analyzes emotional content handling and semantic coherence of large language models (LLMs) in social media by examining Twitter and Reddit discussions on climate change using Gemma and Llama models.", "motivation": "To understand how LLMs manage emotional content and maintain semantic relationships in social media contexts.", "method": "The study investigates LLMs using continuation and response tasks, focusing on emotional transitions and semantic similarity in discussions around climate change from Twitter and Reddit.", "result": "Both models show high semantic coherence; however, Gemma amplifies negative emotions like anger, while Llama preserves a broader emotional spectrum. Both exhibit reduced emotional intensity in responses compared to human content, with a bias toward positive emotions.", "conclusion": "The findings highlight the emotional and semantic processing capabilities of LLMs, affecting their potential deployment in social media and human-AI interaction.", "key_contributions": ["Insights into emotional content handling of LLMs in social media.", "Comparison of emotional patterns between Gemma and Llama models.", "Analysis of semantic coherence against human-authored content."], "limitations": "The study is limited to specific contexts of climate change discussions and may not generalize to all social media interactions.", "keywords": ["Large Language Models", "emotional consistency", "semantic coherence", "social media", "human-AI interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.08208", "pdf": "https://arxiv.org/pdf/2501.08208.pdf", "abs": "https://arxiv.org/abs/2501.08208", "title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems", "authors": ["Mohita Chowdhury", "Yajie Vera He", "Jared Joselowitz", "Aisling Higham", "Ernest Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages", "summary": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.", "AI": {"tldr": "Introduction of ASTRID, a triad of metrics for evaluating clinical QA systems using LLMs and RAG.", "motivation": "Current automated RAG metrics are ineffective in clinical settings, necessitating the need for better evaluation methods that are scalable and cost-effective.", "method": "ASTRID comprises three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). It includes a dataset of over 200 real-world patient questions to validate these metrics against human evaluations.", "result": "CF predicts human ratings of faithfulness better than existing definitions; the triad aligns well with clinician assessments of model responses.", "conclusion": "ASTRID provides an automated evaluation approach that is promising for LLM-driven systems, and resources like prompts and datasets are made publicly available for further research.", "key_contributions": ["Introduction of ASTRID as a new evaluation framework", "Validation of CF metric against human evaluations", "Publication of datasets and prompts for clinical QA research"], "limitations": "", "keywords": ["Large Language Models", "Retrieval Augmented Generation", "Clinical Question Answering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12057", "pdf": "https://arxiv.org/pdf/2502.12057.pdf", "abs": "https://arxiv.org/abs/2502.12057", "title": "Culture is Not Trivia: Sociocultural Theory for Cultural NLP", "authors": ["Naitian Zhou", "David Bamman", "Isaac L. Bleaman"], "categories": ["cs.CL", "cs.CY"], "comment": "ACL 2025 Main Conference; camera-ready version", "summary": "The field of cultural NLP has recently experienced rapid growth, driven by a\npressing need to ensure that language technologies are effective and safe\nacross a pluralistic user base. This work has largely progressed without a\nshared conception of culture, instead choosing to rely on a wide array of\ncultural proxies. However, this leads to a number of recurring limitations:\ncoarse national boundaries fail to capture nuanced differences that lay within\nthem, limited coverage restricts datasets to only a subset of usually\nhighly-represented cultures, and a lack of dynamicity results in static\ncultural benchmarks that do not change as culture evolves. In this position\npaper, we argue that these methodological limitations are symptomatic of a\ntheoretical gap. We draw on a well-developed theory of culture from\nsociocultural linguistics to fill this gap by 1) demonstrating in a case study\nhow it can clarify methodological constraints and affordances, 2) offering\ntheoretically-motivated paths forward to achieving cultural competence, and 3)\narguing that localization is a more useful framing for the goals of much\ncurrent work in cultural NLP.", "AI": {"tldr": "This paper discusses limitations in cultural NLP caused by inadequate cultural representations and proposes using sociocultural linguistics to enhance methodological frameworks and localization in NLP.", "motivation": "The need for effective and safe language technologies for diverse cultures is not being met due to the absence of a shared conception of culture in NLP research.", "method": "The paper uses a case study to demonstrate how sociocultural linguistics can clarify methodological constraints in cultural NLP.", "result": "The case study reveals that existing cultural proxies in NLP research are insufficient and suggests localization as an alternative approach.", "conclusion": "Improving cultural competence in NLP requires theoretical guidance from sociocultural linguistics and re-framing goals towards localization.", "key_contributions": ["Identification of methodological limitations in cultural NLP", "Proposal of localization as a more effective approach", "Introduction of sociocultural linguistics to enhance cultural understanding"], "limitations": "The paper focuses on theoretical implications and does not provide specific implementation strategies for NLP systems.", "keywords": ["cultural NLP", "sociocultural linguistics", "localization", "cultural competence", "language technologies"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2502.13246", "pdf": "https://arxiv.org/pdf/2502.13246.pdf", "abs": "https://arxiv.org/abs/2502.13246", "title": "When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models", "authors": ["Julia Mendelsohn", "Ceren Budak"], "categories": ["cs.CL", "cs.CY"], "comment": "To appear at ACL 2025. Please cite ACL version when proceedings are\n  available", "summary": "Metaphor, discussing one concept in terms of another, is abundant in politics\nand can shape how people understand important issues. We develop a\ncomputational approach to measure metaphorical language, focusing on\nimmigration discourse on social media. Grounded in qualitative social science\nresearch, we identify seven concepts evoked in immigration discourse (e.g.\n\"water\" or \"vermin\"). We propose and evaluate a novel technique that leverages\nboth word-level and document-level signals to measure metaphor with respect to\nthese concepts. We then study the relationship between metaphor, political\nideology, and user engagement in 400K US tweets about immigration. While\nconservatives tend to use dehumanizing metaphors more than liberals, this\neffect varies widely across concepts. Moreover, creature-related metaphor is\nassociated with more retweets, especially for liberal authors. Our work\nhighlights the potential for computational methods to complement qualitative\napproaches in understanding subtle and implicit language in political\ndiscourse.", "AI": {"tldr": "This paper develops a computational method to analyze metaphorical language in immigration discourse on social media, revealing differences in usage between political ideologies.", "motivation": "To understand the impact of metaphor in political discourse, particularly in immigration discussions on social media.", "method": "The authors identify seven metaphorical concepts related to immigration and propose a new technique combining word-level and document-level signals to measure these metaphors. They analyze 400K US tweets for this study.", "result": "The study finds that conservative users employ more dehumanizing metaphors than liberals, with notable variation across different concepts. Additionally, creature-related metaphors drive more retweets, especially among liberal authors.", "conclusion": "The research demonstrates that computational methods can effectively complement qualitative approaches in understanding implicit language in political discourse.", "key_contributions": ["Development of a novel technique to measure metaphorical language", "Identification of metaphor usage trends across political ideologies", "Insights into user engagement related to metaphorical language in tweets"], "limitations": "The study is limited to immigration discourse on social media and may not generalize to other topics or platforms.", "keywords": ["metaphor", "political discourse", "social media", "immigration", "computational linguistics"], "importance_score": 5, "read_time_minutes": 12}}
{"id": "2502.13962", "pdf": "https://arxiv.org/pdf/2502.13962.pdf", "abs": "https://arxiv.org/abs/2502.13962", "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering", "authors": ["William Jurayj", "Jeffrey Cheng", "Benjamin Van Durme"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Code: https://github.com/wjurayj/final_answer", "summary": "Scaling the test-time compute of large language models has demonstrated\nimpressive performance on reasoning benchmarks. However, existing evaluations\nof test-time scaling make the strong assumption that a reasoning system should\nalways give an answer to any question provided. This overlooks concerns about\nwhether a model is confident in its answer, and whether it is appropriate to\nalways provide a response. To address these concerns, we extract confidence\nscores during reasoning for thresholding model responses. We find that\nincreasing compute budget at inference time not only helps models answer more\nquestions correctly, but also increases confidence in correct responses. We\nthen extend the current paradigm of zero-risk responses during evaluation by\nconsidering settings with non-zero levels of response risk, and suggest a\nrecipe for reporting evaluations under these settings.", "AI": {"tldr": "This paper addresses the limitations of test-time scaling in language models by incorporating confidence scores for model responses, thus allowing for thresholding and evaluation based on response risk.", "motivation": "To improve the evaluation of reasoning systems by considering the confidence in model responses and the appropriateness of answering every question provided.", "method": "The authors extract confidence scores during reasoning and investigate how increasing the compute budget at inference time affects both the correctness of model answers and the confidence in those answers.", "result": "The study finds that a higher compute budget not only leads to more correct answers but also increases confidence in those answers.", "conclusion": "The paper introduces a new approach for evaluating model responses that accounts for varying levels of response risk rather than assuming zero-risk responses, and establishes guidelines for reporting such evaluations.", "key_contributions": ["Incorporation of confidence scoring during reasoning", "Evaluation approach considering non-zero response risk", "Demonstration of improved accuracy and confidence with increased compute budget"], "limitations": "", "keywords": ["large language models", "confidence scoring", "response risk"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.02768", "pdf": "https://arxiv.org/pdf/2504.02768.pdf", "abs": "https://arxiv.org/abs/2504.02768", "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs", "authors": ["Jaap Jumelet", "Leonie Weissweiler", "Joakim Nivre", "Arianna Bisazza"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages", "AI": {"tldr": "MultiBLiMP 1.0 is a multilingual benchmark for evaluating linguistic minimal pairs across 101 languages, utilizing automated methods and large linguistic resources.", "motivation": "To evaluate the abilities of large language models (LLMs) on subject-verb agreement in a multilingual context and address shortcomings in low-resource language modeling.", "method": "A fully automated pipeline is used to create minimal pairs from the large-scale resources of Universal Dependencies and UniMorph, resulting in over 128,000 pairs for testing.", "result": "The benchmark reveals significant shortcomings in the current state-of-the-art models when applied to low-resource languages, despite covering a wide range of languages.", "conclusion": "MultiBLiMP 1.0 offers a comprehensive tool for analyzing LLM performance and highlighting areas needing improvement for low-resource languages.", "key_contributions": ["Creation of a multilingual benchmark covering 101 languages", "Inclusion of more than 128,000 minimal pairs for rigorous evaluation", "Automated pipeline utilizing Universal Dependencies and UniMorph resources"], "limitations": "", "keywords": ["multi-language evaluation", "linguistic minimal pairs", "LLM performance"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452.pdf", "abs": "https://arxiv.org/abs/2504.14452", "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).", "AI": {"tldr": "ParaPO is a method that fine-tunes language models to reduce unintentional repetitions of their pretraining data while maintaining utility and famous quotations recall.", "motivation": "To address concerns regarding copyright, plagiarism, privacy, and creativity due to language models' ability to memorize and reproduce segments of their training data.", "method": "ParaPO fine-tunes language models to prefer paraphrased versions of memorized content, using system prompts to manage regurgitation behavior when needed.", "result": "ParaPO reduces the regurgitation metric significantly across different datasets, improving performance from 17.3 to 12.9 in creative writing tasks, and preserves quotation recall effectively.", "conclusion": "ParaPO outperforms previous unlearning methods by consistently reducing regurgitation in both Llama3.1-8B and Tulu3-8B models while maintaining the ability to recall important phrases when necessary.", "key_contributions": ["Introduction of the Paraphrase Preference Optimization (ParaPO) method", "Demonstrated significant reduction in unintentional regurgitation across multiple datasets", "Maintained the recall capability for famous quotations using system prompts"], "limitations": "", "keywords": ["Paraphrase Preference Optimization", "Language Models", "Regurgitation", "Fine-tuning", "Machine Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.03572", "pdf": "https://arxiv.org/pdf/2501.03572.pdf", "abs": "https://arxiv.org/abs/2501.03572", "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study", "authors": ["Ammar Ahmed", "Margarida Fresco", "Fredrik Forsberg", "Hallvard Grotli"], "categories": ["cs.HC", "cs.AI", "cs.CL", "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"], "comment": null, "summary": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.", "AI": {"tldr": "This study evaluates ChatGPT's ability to generate accessible web content according to WCAG standards, revealing strengths in simple tasks and challenges in complex scenarios, emphasizing the importance of effective prompt engineering and visual aids.", "motivation": "To assess and improve web accessibility, particularly for individuals with disabilities, and to explore the capacity of ChatGPT in aligning with accessibility standards.", "method": "ChatGPT's performance was tested through automated and manual evaluations, examining its ability to resolve accessibility issues when provided with prompts and visual aids.", "result": "ChatGPT can effectively address basic accessibility problems but struggles with more complex issues, necessitating human intervention and iterative improvements. Utilizing screenshots with prompts significantly enhances ChatGPT's ability to analyze and suggest fixes.", "conclusion": "The research highlights the potential of large language models like ChatGPT in improving web accessibility, while also noting the limitations and the need for human oversight in more complex tasks.", "key_contributions": ["Evaluation of ChatGPT for web accessibility compliance", "Incorporation of manual evaluation and visual reasoning", "Recommendations for prompt engineering to enhance performance"], "limitations": "ChatGPT's default outputs often do not meet accessibility standards; complex issues require human oversight and iterative feedback.", "keywords": ["web accessibility", "ChatGPT", "WCAG", "prompt engineering", "visual reasoning"], "importance_score": 7, "read_time_minutes": 10}}
