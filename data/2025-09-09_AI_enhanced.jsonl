{"id": "2509.05298", "pdf": "https://arxiv.org/pdf/2509.05298.pdf", "abs": "https://arxiv.org/abs/2509.05298", "title": "Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression", "authors": ["Rui Xi", "Xianghan Wang"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 International Conference on\n  Artificial Intelligence and Virtual Reality (AIVR 2025). \\c{opyright} 2025\n  Springer. This is the author-accepted manuscript. Rui Xi and Xianghan Wang\n  contributed equally to this work. The final version will be available via\n  SpringerLink", "summary": "Loneliness and social isolation pose significant emotional and health\nchallenges, prompting the development of technology-based solutions for\ncompanionship and emotional support. This paper introduces Livia, an\nemotion-aware augmented reality (AR) companion app designed to provide\npersonalized emotional support by combining modular artificial intelligence\n(AI) agents, multimodal affective computing, progressive memory compression,\nand AR driven embodied interaction. Livia employs a modular AI architecture\nwith specialized agents responsible for emotion analysis, dialogue generation,\nmemory management, and behavioral orchestration, ensuring robust and adaptive\ninteractions. Two novel algorithms-Temporal Binary Compression (TBC) and\nDynamic Importance Memory Filter (DIMF)-effectively manage and prioritize\nlong-term memory, significantly reducing storage requirements while retaining\ncritical context. Our multimodal emotion detection approach achieves high\naccuracy, enhancing proactive and empathetic engagement. User evaluations\ndemonstrated increased emotional bonds, improved satisfaction, and\nstatistically significant reductions in loneliness. Users particularly valued\nLivia's adaptive personality evolution and realistic AR embodiment. Future\nresearch directions include expanding gesture and tactile interactions,\nsupporting multi-user experiences, and exploring customized hardware\nimplementations.", "AI": {"tldr": "Livia is an emotion-aware AR companion app that uses modular AI agents to provide personalized emotional support, demonstrating significant reductions in loneliness and enhanced user satisfaction.", "motivation": "Address loneliness and social isolation through technology-based solutions that offer companionship and emotional support.", "method": "The app combines modular AI architecture with algorithms for emotion analysis, dialogue generation, memory management, and behavioral orchestration, along with advanced multimodal emotion detection.", "result": "User evaluations showed increased emotional bonds, improved satisfaction, and significant reductions in loneliness due to Livia's adaptive personality and AR interactions.", "conclusion": "Livia successfully enhances emotional engagement through innovative AI and AR interactions, paving the way for future research into more immersive features.", "key_contributions": ["Introduction of Livia, an emotion-aware augmented reality companion app", "Development of Temporal Binary Compression and Dynamic Importance Memory Filter algorithms", "High accuracy in multimodal emotion detection facilitating empathetic engagement"], "limitations": "", "keywords": ["augmented reality", "emotion detection", "AI companions", "loneliness", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.05491", "pdf": "https://arxiv.org/pdf/2509.05491.pdf", "abs": "https://arxiv.org/abs/2509.05491", "title": "Hybrid User Interfaces: Past, Present, and Future of Complementary Cross-Device Interaction in Mixed Reality", "authors": ["Sebastian Hubenschmid", "Marc Satkowski", "Johannes Zagermann", "Julián Méndez", "Niklas Elmqvist", "Steven Feiner", "Tiara Feuchtner", "Jens Emil Grønbæk", "Benjamin Lee", "Dieter Schmalstieg", "Raimund Dachselt", "Harald Reiterer"], "categories": ["cs.HC"], "comment": "Submitted on the 26 February 2025 to the IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) and awaiting Reviews", "summary": "We investigate hybrid user interfaces (HUIs), aiming to establish a cohesive\nunderstanding and adopt consistent terminology for this nascent research area.\nHUIs combine heterogeneous devices in complementary roles, leveraging the\ndistinct benefits of each. Our work focuses on cross-device interaction between\n2D devices and mixed reality environments, which are particularly compelling,\nleveraging the familiarity of traditional 2D platforms while providing spatial\nawareness and immersion. Although such HUIs have been prominently explored in\nthe context of mixed reality by prior work, we still lack a cohesive\nunderstanding of the unique design possibilities and challenges of such\ncombinations, resulting in a fragmented research landscape. We conducted a\nsystematic survey and present a taxonomy of HUIs that combine conventional\ndisplay technology and mixed reality environments. Based on this, we discuss\npast and current challenges, the evolution of definitions, and prospective\nopportunities to tie together the past 30 years of research with our vision of\nfuture HUIs.", "AI": {"tldr": "This paper investigates hybrid user interfaces (HUIs) that blend 2D and mixed reality devices, aiming to create a cohesive understanding and terminology in this emerging field.", "motivation": "To establish a clear understanding and consistent terminology for hybrid user interfaces, mitigating the fragmented research landscape.", "method": "Conducted a systematic survey to present a taxonomy of HUIs combining conventional display technology with mixed reality environments.", "result": "The paper discusses past challenges in HUI design and outlines a vision for the evolution of HUIs over the past 30 years.", "conclusion": "Identifying key design possibilities and challenges in HUIs can unify the research field and leverage the benefits of combining different technologies.", "key_contributions": ["Systematic survey of hybrid user interfaces (HUIs)", "Proposed taxonomy for HUIs that combines traditional 2D and mixed reality devices", "Discussion of the evolution and future opportunities in HUI research"], "limitations": "", "keywords": ["Hybrid User Interfaces", "Mixed Reality", "Cross-device Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.05619", "pdf": "https://arxiv.org/pdf/2509.05619.pdf", "abs": "https://arxiv.org/abs/2509.05619", "title": "GestoBrush: Facilitating Graffiti Artists' Digital Creation Experiences through Embodied AR Interactions", "authors": ["Ruiqi Chen", "Qingyang He", "Hanxi Bao", "Jung Choi", "Xin Tong"], "categories": ["cs.HC", "H.5.1; H.5.2; H.5.1; H.5.2; H.5.m"], "comment": "Accepted to VINCI 2025; 8 pages, 3 figures", "summary": "Graffiti has long documented the socio-cultural landscapes of urban spaces,\nyet increasing global regulations have constrained artists' creative freedom,\nprompting exploration of digital alternatives. Augmented Reality (AR) offers\nopportunities to extend graffiti into digital environments while retaining\nspatial and cultural significance, but prior research has largely centered on\naudience engagement rather than the embodied creative processes of graffiti\nartists. To address this, we developed GestoBrush, a mobile AR prototype that\nturns smartphones into virtual spray cans, enabling graffiti creation through\nembodied gestures. A co-design workshop underscored the role of\nembodiment-physical engagement with surroundings and body-driven creative\nprocesses-in digital workflows. We evaluated GestoBrush with six graffiti\nartists and findings suggested that embodied AR interactions supporting artists\nbypass real-world constraints and explore new artistic possibilities, whose AR\nartworks created enhanced senses of intuitiveness, immersion, and\nexpressiveness. This work highlight how embodied AR tools can bridge the gap\nbetween physical graffiti practice and digital expression, suggesting pathways\nfor designing immersive creative systems that respect the cultural ethos of\nstreet art while expanding its possibilities in virtual spaces.", "AI": {"tldr": "This paper explores the development of GestoBrush, an Augmented Reality prototype for graffiti artists, enabling creative expression through embodied interactions while overcoming physical constraints.", "motivation": "To explore digital alternatives for graffiti amid increasing regulations and to focus on the creative processes of graffiti artists instead of just audience engagement.", "method": "Developed GestoBrush, a mobile AR tool that allows graffiti creation through gestures, and evaluated it through a co-design workshop with graffiti artists.", "result": "Embodied AR interactions allowed artists to bypass real-world constraints, resulting in new artistic possibilities and enhanced intuitiveness, immersion, and expressiveness in their artworks.", "conclusion": "Embodied AR tools can bridge physical graffiti practices and digital expression, providing immersive creative systems that respect and expand the cultural aspects of street art.", "key_contributions": ["Development of GestoBrush, an AR tool for graffiti creation", "Emphasis on embodied interactions in digital workflows", "Insights into new artistic possibilities for graffiti artists in AR"], "limitations": "", "keywords": ["Augmented Reality", "Graffiti", "Embodied Interaction", "Digital Art", "Cultural Significance"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2509.05718", "pdf": "https://arxiv.org/pdf/2509.05718.pdf", "abs": "https://arxiv.org/abs/2509.05718", "title": "Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization", "authors": ["Péter Ferenc Gyarmati", "Manfred Klaffenböck", "Laura Koesten", "Torsten Möller"], "categories": ["cs.HC"], "comment": "2 pages, 2 figures. Accepted submission to the poster track of IEEE\n  VIS 2025", "summary": "Vision-language models (VLMs) hold promise for enhancing visualization tools,\nbut effective human-AI collaboration hinges on a shared perceptual\nunderstanding of visual content. Prior studies assessed VLM visualization\nliteracy through interpretive tasks, revealing an over-reliance on textual cues\nrather than genuine visual analysis. Our study investigates a more foundational\nskill underpinning such literacy: the ability of VLMs to recognize a chart's\ncore visual properties as humans do. We task 13 diverse VLMs with classifying\nscientific visualizations based solely on visual stimuli, according to three\ncriteria: purpose (e.g., schematic, GUI, visualization), encoding (e.g., bar,\npoint, node-link), and dimensionality (e.g., 2D, 3D). Using expert labels from\nthe human-centric VisType typology as ground truth, we find that VLMs often\nidentify purpose and dimensionality accurately but struggle with specific\nencoding types. Our preliminary results show that larger models do not always\nequate to superior performance and highlight the need for careful integration\nof VLMs in visualization tasks, with human supervision to ensure reliable\noutcomes.", "AI": {"tldr": "This study evaluates the ability of vision-language models (VLMs) to classify scientific visualizations based purely on visual characteristics, revealing strengths and weaknesses in their performance compared to human understanding.", "motivation": "Enhancing human-AI collaboration in visualization tools requires a mutual understanding of visual content, which has not been adequately addressed in previous studies.", "method": "The study involved assessing 13 different VLMs by classifying scientific visualizations based on visual stimuli and criteria including purpose, encoding, and dimensionality.", "result": "VLMs generally performed well in identifying visualization purpose and dimensionality, but struggled with accurate recognition of encoding types. Larger models did not necessarily outperform smaller ones.", "conclusion": "Careful integration of VLMs in visualization tasks is crucial, highlighting the importance of human oversight to ensure reliable outcomes.", "key_contributions": ["Assessment of VLMs' ability to recognize visual properties of charts", "Introduction of criteria for classification based on visual stimuli", "Findings on the limitations of larger models in performance"], "limitations": "The study only examines a limited number of VLMs and their performance in specific visual tasks.", "keywords": ["Vision-language models", "visualization literacy", "human-AI collaboration"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.05359", "pdf": "https://arxiv.org/pdf/2509.05359.pdf", "abs": "https://arxiv.org/abs/2509.05359", "title": "An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training", "authors": ["Yanis Labrak", "Richard Dufour", "Mickaël Rouvier"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Published in International Conference on Text, Speech, and Dialogue,\n  13-24", "summary": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications.", "AI": {"tldr": "This paper examines discrete unit representations in Speech Language Models, focusing on optimizing speech modeling and robustness during continual pre-training.", "motivation": "To optimize speech modeling during the continual pre-training stage of Speech Language Models by investigating the influence of model architecture, data representation, and training robustness.", "method": "The paper systematically examines the effects of speech encoders, clustering granularity, and domain matching during pre-training through experiments that analyze cluster distribution and phonemic alignments.", "result": "The experiments demonstrate how optimal discretization strategies differ with model capacity and highlight the importance of effective discrete vocabulary use in capturing linguistic and paralinguistic patterns.", "conclusion": "The study underscores the significance of clustering data selection and domain matching for improving model robustness in speech applications using discrete representations.", "key_contributions": ["Introduces novel insights on the role of clustering granularity across different model scales.", "Analyzes the impact of speech encoder variations on pre-training outcomes.", "Explores the domain matching necessity for improved model robustness."], "limitations": "", "keywords": ["Speech Language Models", "pre-training", "discrete representations"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.05721", "pdf": "https://arxiv.org/pdf/2509.05721.pdf", "abs": "https://arxiv.org/abs/2509.05721", "title": "A Composable Agentic System for Automated Visual Data Reporting", "authors": ["Péter Ferenc Gyarmati", "Dominik Moritz", "Torsten Möller", "Laura Koesten"], "categories": ["cs.HC"], "comment": null, "summary": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/.", "AI": {"tldr": "The paper presents a prototype for automated visual data reporting that utilizes a Hybrid Human-AI Partnership model to improve the robustness of AI agents by externalizing logic from LLMs to deterministic modules.", "motivation": "To address the brittleness of monolithic AI agents in visual data reporting.", "method": "A hybrid, multi-agent architecture is used, with a rule-based system called Draco for visualization design, delivering interactive reports and executable notebooks.", "result": "The system produces a dual-output: interactive reports for reader exploration and executable notebooks for deep traceability, enabling a fully automatic yet auditable user experience.", "conclusion": "The proposed architecture charts a pathway for a more synergistic collaboration between human experts and AI systems.", "key_contributions": ["Introduction of a Hybrid Human-AI Partnership model for visual data reporting.", "Development of a multi-agent architecture that separates logic from LLMs for enhanced robustness.", "Provision of interactive and executable outputs for improved user experience and traceability."], "limitations": "", "keywords": ["Human-AI Partnership", "visual data reporting", "multi-agent architecture"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.05360", "pdf": "https://arxiv.org/pdf/2509.05360.pdf", "abs": "https://arxiv.org/abs/2509.05360", "title": "Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection", "authors": ["Jerry Li", "Evangelos Papalexakis"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated effectiveness across a wide\nvariety of tasks involving natural language, however, a fundamental problem of\nhallucinations still plagues these models, limiting their trustworthiness in\ngenerating consistent, truthful information. Detecting hallucinations has\nquickly become an important topic, with various methods such as uncertainty\nestimation, LLM Judges, retrieval augmented generation (RAG), and consistency\nchecks showing promise. Many of these methods build upon foundational metrics,\nsuch as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth\nnecessary to detect hallucinations effectively. In this work, we propose a\nnovel approach inspired by ROUGE that constructs an N-Gram frequency tensor\nfrom LLM-generated text. This tensor captures richer semantic structure by\nencoding co-occurrence patterns, enabling better differentiation between\nfactual and hallucinated content. We demonstrate this by applying tensor\ndecomposition methods to extract singular values from each mode and use these\nas input features to train a multi-layer perceptron (MLP) binary classifier for\nhallucinations. Our method is evaluated on the HaluEval dataset and\ndemonstrates significant improvements over traditional baselines, as well as\ncompetitive performance against state-of-the-art LLM judges.", "AI": {"tldr": "This paper presents a novel approach using N-Gram frequency tensors to detect hallucinations in Large Language Models, improving on traditional methods and demonstrating better accuracy in classification using a multi-layer perceptron.", "motivation": "To address the problem of hallucinations in Large Language Models that reduce their trustworthiness in generating accurate information.", "method": "The authors propose a method that constructs an N-Gram frequency tensor from LLM-generated text, capturing semantic structure through co-occurrence patterns. This tensor is used to train a binary classifier with multi-layer perceptron techniques.", "result": "The proposed method significantly improves detection of hallucinations over traditional metrics and achieves competitive performance against state-of-the-art LLM judges.", "conclusion": "The novel N-Gram frequency tensor approach enhances the differentiation between factual and hallucinated content, offering a promising solution to the hallucination problem in LLMs.", "key_contributions": ["Introduction of N-Gram frequency tensors for hallucination detection.", "Use of tensor decomposition methods for feature extraction.", "Demonstrated significant improvement over traditional baselines."], "limitations": "", "keywords": ["Large Language Models", "hallucination detection", "N-Gram frequency tensor", "multi-layer perceptron", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.05829", "pdf": "https://arxiv.org/pdf/2509.05829.pdf", "abs": "https://arxiv.org/abs/2509.05829", "title": "Augmenting Human-Centered Racial Covenant Detection and Georeferencing with Plug-and-Play NLP Pipelines", "authors": ["Jiyoon Pyo", "Yuankun Jiao", "Yao-Yi Chiang", "Michael Corey"], "categories": ["cs.HC"], "comment": null, "summary": "Though no longer legally enforceable, racial covenants in twentieth-century\nproperty deeds continue to shape spatial and socioeconomic inequalities.\nUnderstanding this legacy requires identifying racially restrictive language\nand geolocating affected properties. The Mapping Prejudice project addresses\nthis by engaging volunteers on the Zooniverse crowdsourcing platform to\ntranscribe covenants from scanned deeds and link them to modern parcel maps\nusing transcribed legal descriptions. While the project has explored\nautomation, it values crowdsourcing for its social impact and technical\nadvantages. Historically, Mapping Prejudice relied on lexicon-based searching\nand, more recently, fuzzy matching to flag suspected covenants. However, fuzzy\nmatching has increased false positives, burdening volunteers and raising\nscalability concerns. Additionally, while many properties can be mapped\nautomatically, others still require time-intensive manual geolocation.\n  We present a human-centered computing approach with two plug-and-play NLP\npipelines: (1) a context-aware text labeling model that flags racially\nrestrictive language with high precision and (2) a georeferencing module that\nextracts geographic descriptions from deeds and resolves them to real-world\nlocations. Evaluated on historical deed documents from six counties in\nMinnesota and Wisconsin, our system reduces false positives in racial term\ndetection by 25.96% while maintaining 91.73% recall and achieves 85.58%\ngeoreferencing accuracy within 1x1 square-mile ranges. These tools enhance\ndocument filtering and enrich spatial annotations, accelerating volunteer\nparticipation and reducing manual cleanup while strengthening public\nengagement.", "AI": {"tldr": "The Mapping Prejudice project enhances the identification and geolocation of properties affected by historical racial covenants using human-centered computing and NLP, aimed at reducing false positives and increasing volunteer engagement.", "motivation": "To address spatial and socioeconomic inequalities stemming from historical racial covenants in property deeds by leveraging crowdsourcing and automated methods.", "method": "The paper presents two NLP pipelines: a context-aware text labeling model for identifying racially restrictive language and a georeferencing module for linking geographic descriptions to real-world locations.", "result": "The system reduces false positives in detecting racial terms by 25.96% while maintaining 91.73% recall, and achieves 85.58% accuracy in georeferencing within specified ranges.", "conclusion": "The tools developed enhance the efficiency of document filtering and spatial annotations, facilitating increased volunteer participation and public engagement.", "key_contributions": ["Introduction of a human-centered computing approach for identifying racial covenants", "Development of a context-aware text labeling model for precise language identification", "Creation of a georeferencing module to streamline mapping of historical properties"], "limitations": "The need for time-intensive manual geolocation for some properties remains a challenge.", "keywords": ["racial covenants", "NLP", "geolocation", "crowdsourcing", "human-centered computing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.05385", "pdf": "https://arxiv.org/pdf/2509.05385.pdf", "abs": "https://arxiv.org/abs/2509.05385", "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs", "authors": ["Jiacheng Wei", "Faguo Wu", "Xiao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 7 figures, conference", "summary": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time.", "AI": {"tldr": "SAGE is a dynamic fine-tuning framework that improves reasoning in large language models by decomposing complex tasks and enabling adaptive learning at inference time.", "motivation": "Large language models face limitations in adapting to new data during inference, which affects their reasoning abilities.", "method": "SAGE includes a Trigger module for real-time failure detection, a Trigger Buffer for clustering anomalies, and a Lora Store for dynamic parameter updates.", "result": "SAGE shows improved accuracy, robustness, and stability in atomic reasoning tasks through dynamic updates during inference.", "conclusion": "The framework successfully enhances the performance of language models in adapting to new data during reasoning.", "key_contributions": ["Introduction of SAGE framework for dynamic fine-tuning", "Real-time failure detection via Trigger module", "Dynamic optimization of model parameters with Lora Store"], "limitations": "", "keywords": ["dynamic fine-tuning", "adaptive learning", "large language models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.05898", "pdf": "https://arxiv.org/pdf/2509.05898.pdf", "abs": "https://arxiv.org/abs/2509.05898", "title": "Attention, Action, and Memory: How Multi-modal Interfaces and Cognitive Load Alter Information Retention", "authors": ["Omar Elgohary", "Zhu-Tien"], "categories": ["cs.HC"], "comment": null, "summary": "Each year, multi-modal interaction continues to grow within both industry and\nacademia. However, researchers have yet to fully explore the impact of\nmulti-modal systems on learning and memory retention. This research\ninvestigates how combining gaze-based controls with gesture navigation affects\ninformation retention when compared to standard track-pad usage. A total of\ntwelve participants read four textual articles through two different user\ninterfaces which included a track-pad and a multi-modal interface that tracked\neye movements and hand gestures for scrolling, zooming, and revealing content.\nParticipants underwent two assessment sessions that measured their information\nretention immediately and after a twenty-four hour period along with the\nNASA-TLX workload evaluation and the System Usability Scale assessment. The\ninitial analysis indicates that multi-modal interaction produces similar\ntargeted information retention to traditional track-pad usage, but this neutral\neffect comes with higher cognitive workload demands and seems to deteriorate\nwith long-term retention. The research results provide new knowledge about how\nmulti-modal systems affect cognitive engagement while providing design\nrecommendations for future educational and assistive technologies that require\neffective memory performance.", "AI": {"tldr": "This research examines the effects of multi-modal interaction (gaze-based controls and gesture navigation) on information retention compared to standard track-pad use.", "motivation": "To explore the impact of multi-modal systems on learning and memory retention, particularly in educational and assistive technology contexts.", "method": "Twelve participants interacted with two user interfaces (track-pad and multi-modal interface) while reading articles. Information retention was assessed immediately and after 24 hours, alongside cognitive workload and usability evaluations.", "result": "Initial findings suggest that multi-modal interaction has a similar effect on immediate information retention to traditional methods but demands higher cognitive workload and worsens long-term retention.", "conclusion": "Multi-modal systems may engage users cognitively but require careful design to prevent negative impacts on memory performance.", "key_contributions": ["Investigated cognitive workload in multi-modal interaction contexts", "Provided empirical data on information retention with gaze and gesture control", "Delivered design recommendations for educational technology applications"], "limitations": "Small sample size of 12 participants may limit generalizability of results.", "keywords": ["multi-modal interaction", "memory retention", "cognitive workload", "educational technology", "gesture navigation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.05396", "pdf": "https://arxiv.org/pdf/2509.05396.pdf", "abs": "https://arxiv.org/abs/2509.05396", "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate", "authors": ["Andrea Wynn", "Harsh Satija", "Gillian Hadfield"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "ICML MAS Workshop 2025", "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning.", "AI": {"tldr": "This paper investigates the detrimental effects of multi-agent debate on AI reasoning, emphasizing the role of model diversity.", "motivation": "To explore how the diversity in model capabilities influences multi-agent debate and AI reasoning outcomes, particularly when naive application of debate can lead to performance degradation.", "method": "Conducted a series of experiments comparing debates within homogeneous groups versus diverse model groups, observing how model capabilities impact debate dynamics.", "result": "Findings reveal that debate can reduce accuracy over time, with stronger models sometimes agreeing with weaker ones, thus leading to incorrect conclusions.", "conclusion": "Agents may favor consensus over challenging flawed reasoning, indicating critical failure modes in multi-agent debate that must be addressed.", "key_contributions": ["Demonstrates the negative impact of diversity in model capabilities on debate outcomes.", "Highlights failure modes where agents favor incorrect reasoning for agreement over accuracy.", "Challenges the naive application of multi-agent debate in AI systems."], "limitations": "Does not address corrective measures for improving debate outcomes.", "keywords": ["multi-agent debate", "AI reasoning", "model diversity", "performance degradation", "peer reasoning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.05943", "pdf": "https://arxiv.org/pdf/2509.05943.pdf", "abs": "https://arxiv.org/abs/2509.05943", "title": "DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification", "authors": ["Yi Wang", "Haodong Zhang", "Hongqi Li"], "categories": ["cs.HC"], "comment": "Submit to IEEE Journal", "summary": "Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant\npotential for assistive technologies and neurorehabilitation. However, the\nprecise and efficient decoding of MI remains challenging due to their\nnon-stationary nature and low signal-to-noise ratio. This paper introduces a\nnovel end-to-end deep learning framework of Discriminative Residual Dense\nConvolutional Autoencoder with Spatio-Temporal Graph Neural Network\n(DRDCAE-STGNN) to enhance the MI feature learning and classification.\nSpecifically, the DRDCAE module leverages residual-dense connections to learn\ndiscriminative latent representations through joint reconstruction and\nclassifica-tion, while the STGNN module captures dynamic spatial dependencies\nvia a learnable graph adjacency matrix and models temporal dynamics using\nbidirectional long short-term memory (LSTM). Extensive evaluations on BCI\nCompetition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art\nperformance, with average accuracies of 95.42%, 97.51%, and 90.15%,\nrespectively. Ablation studies confirm the contribution of each component, and\ninterpreta-bility analysis reveals neurophysiologically meaningful connectivity\npatterns. Moreover, despite its complexity, the model maintains a feasible\nparameter count and an inference time of 0.32 ms per sample. These results\nindicate that our method offers a robust, accurate, and interpretable solution\nfor MI-EEG decoding, with strong generalizability across subjects and tasks and\nmeeting the requirements for potential real-time BCI applications.", "AI": {"tldr": "This paper presents a deep learning framework for enhancing motor imagery (MI) feature learning and classification in brain-computer interfaces (BCIs).", "motivation": "Motor imagery based brain-computer interfaces offer significant potential for assistive technologies, but precise decoding remains challenging due to the non-stationary nature of MI signals and low signal-to-noise ratio.", "method": "The proposed framework combines a Discriminative Residual Dense Convolutional Autoencoder (DRDCAE) for learning latent representations with a Spatio-Temporal Graph Neural Network (STGNN) to model spatial and temporal dynamics in MI data.", "result": "The framework achieved state-of-the-art accuracies of 95.42%, 97.51%, and 90.15% on BCI Competition IV and PhysioNet datasets. Ablation studies validated the contribution of each component, while interpretable analysis showed neurophysiologically meaningful connectivity patterns.", "conclusion": "The DRDCAE-STGNN framework demonstrates robust, accurate, and interpretable solutions for MI-EEG decoding, with potential for real-time BCI applications across different subjects and tasks.", "key_contributions": ["Introduction of a novel deep learning framework for MI decoding", "High accuracy across multiple datasets", "Interpretable model providing meaningful insights into brain connectivity patterns"], "limitations": "The complexity of the model may pose challenges for real-time implementation in some settings.", "keywords": ["brain-computer interfaces", "motor imagery", "deep learning", "spatio-temporal graph neural network", "interpretability"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.05425", "pdf": "https://arxiv.org/pdf/2509.05425.pdf", "abs": "https://arxiv.org/abs/2509.05425", "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata", "authors": ["Jessica M. Lundin", "Ada Zhang", "David Adelani", "Cody Carroll"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation.", "AI": {"tldr": "This paper predicts translation quality of GPT-4o translations without running the translation system, using features like token fertility ratios and linguistic metadata to achieve high accuracy.", "motivation": "The study aims to explore how translation quality can be predicted using linguistic features, without the need for executing the translation system itself.", "method": "Utilizes gradient boosting models to analyze features such as token fertility ratios, token counts, and linguistic metadata to forecast ChrF scores for translations across 203 languages.", "result": "Achieves R² values of 0.66 for XX->English and 0.72 for English->XX translations, indicating strong predictive performance.", "conclusion": "The results indicate that translation quality is influenced by both token-level factors and broader linguistic typological elements, providing insights for multilingual evaluation.", "key_contributions": ["Introduces a method to predict translation quality without direct execution of translation systems.", "Demonstrates the importance of typological factors and token fertility in translation quality.", "Provides a framework for multilingual evaluation using a limited set of features."], "limitations": "The study focuses primarily on GPT-4o translations and may not generalize to other translation systems or languages outside the FLORES-200 benchmark.", "keywords": ["translation quality", "GPT-4o", "linguistic metadata", "feature importance", "multilingual evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.05961", "pdf": "https://arxiv.org/pdf/2509.05961.pdf", "abs": "https://arxiv.org/abs/2509.05961", "title": "A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners", "authors": ["Evgeny V. Votyakov", "Marios Constantinides", "Fotis Liarokapis"], "categories": ["cs.HC"], "comment": "8 pages, 5 figures, 3 tables", "summary": "Amateur runners are increasingly using wearable devices to track their\ntraining, and often do so through simple metrics such as heart rate and pace.\nHowever, these metrics are typically analyzed in isolation and lack the\nexplainability needed for long-term self-monitoring. In this paper, we first\npresent Fitplotter, which is a client-side web application designed for the\nvisualization and analysis of data associated with fitness and activity\ntracking devices. Next, we revisited and formalized Heart Rate Efficiency\n(HRE), defined as the product of pace and heart rate, as a practical and\nexplainable metric to track aerobic fitness in everyday running. Drawing on\nmore than a decade of training data from one athlete, and supplemented by\npublicly available logs from twelve runners, we showed that HRE provides more\nstable and meaningful feedback on aerobic development than heart rate or pace\nalone. We showed that HRE correlates with training volume, reflects seasonal\nprogress, and remains stable during long runs in well-trained individuals. We\nalso discuss how HRE can support everyday training decisions, improve the user\nexperience in fitness tracking, and serve as an explainable metric to\nproprietary ones of commercial platforms. Our findings have implications for\ndesigning user-centered fitness tools that empower amateur athletes to\nunderstand and manage their own performance data.", "AI": {"tldr": "Fitplotter visualizes fitness tracking data, introducing Heart Rate Efficiency (HRE) as an explainable metric for aerobic fitness.", "motivation": "To enhance the self-monitoring capabilities of amateur runners using wearable devices by providing a more interpretable metric for tracking performance.", "method": "Introduction of Fitplotter, a web application, and formalization of Heart Rate Efficiency (HRE) using training data from athletes.", "result": "HRE offers stability and meaningful feedback over heart rate or pace, correlating with training volume and indicating seasonal progress.", "conclusion": "HRE can improve training decisions and user experience in fitness tracking tools, making fitness data more explainable and accessible.", "key_contributions": ["Introduction of Fitplotter for data visualization", "Formalization of Heart Rate Efficiency as a key fitness metric", "Demonstrated correlation of HRE with training volume and seasonal progress"], "limitations": "", "keywords": ["wearable devices", "Heart Rate Efficiency", "fitness tracking", "amateur runners", "user-centered design"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.05440", "pdf": "https://arxiv.org/pdf/2509.05440.pdf", "abs": "https://arxiv.org/abs/2509.05440", "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too", "authors": ["Logan Lawrence", "Ashton Williamson", "Alexander Shelton"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 18 tables, 1 figure", "summary": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.", "AI": {"tldr": "This paper presents a direct-scoring method using synthetic summaries to improve upon traditional pairwise comparisons by enabling absolute scoring for machine-generated texts.", "motivation": "To enhance the evaluation of large-language models by enabling absolute scores for summaries, which is essential for applications that require thresholding.", "method": "The paper introduces a direct-scoring method that utilizes synthetic summaries to perform pairwise machine rankings at test time.", "result": "The proposed method achieves sample-level correlations comparable to state-of-the-art pairwise evaluators across several benchmarks.", "conclusion": "The method shows promise in improving the evaluation process of automatic raters for free-form content while releasing synthetic summaries for further research.", "key_contributions": ["Introduction of a direct-scoring method.", "Use of synthetic summaries for pairwise machine rankings.", "Comparative performance analysis against state-of-the-art methods."], "limitations": "", "keywords": ["large-language models", "automatic raters", "evaluation", "direct-scoring method", "machine rankings"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.05962", "pdf": "https://arxiv.org/pdf/2509.05962.pdf", "abs": "https://arxiv.org/abs/2509.05962", "title": "The Reel Deal: Designing and Evaluating LLM-Generated Short-Form Educational Videos", "authors": ["Lazaros Stavrinou", "Argyris Constantinides", "Marios Belk", "Vasos Vassiliou", "Fotis Liarokapis", "Marios Constantinides"], "categories": ["cs.HC"], "comment": "9 pages, 3 figures, 1 table", "summary": "Short-form videos are gaining popularity in education due to their concise\nand accessible format that enables microlearning. Yet, most of these videos are\nmanually created. Even for those automatically generated using artificial\nintelligence (AI), it is not well understood whether or how they affect\nlearning outcomes, user experience, and trust. To address this gap, we\ndeveloped ReelsEd, which is a web-based system that uses large language models\n(LLMs) to automatically generate structured short-form video (i.e., reels) from\nlecture long-form videos while preserving instructor-authored material. In a\nbetween-subject user study with 62 university students, we evaluated ReelsEd\nand demonstrated that it outperformed traditional long-form videos in\nengagement, quiz performance, and task efficiency without increasing cognitive\nload. Learners expressed high trust in our system and valued its clarity,\nusefulness, and ease of navigation. Our findings point to new design\nopportunities for integrating generative AI into educational tools that\nprioritize usability, learner agency, and pedagogical alignment.", "AI": {"tldr": "This paper presents ReelsEd, a web-based system that utilizes large language models to automatically generate short-form educational videos from longer lectures, demonstrating improved learner engagement and performance in a user study.", "motivation": "To explore the impact of AI-generated short-form videos on learning outcomes and user experience, as traditional methods of video creation are resource-intensive.", "method": "Development of ReelsEd, a system using large language models to create structured short-form videos from long-form lecture content, evaluated through a user study with 62 students.", "result": "ReelsEd outperformed traditional long-form videos in terms of learner engagement, quiz performance, and task efficiency, while maintaining acceptable cognitive load levels.", "conclusion": "The findings indicate the potential for generative AI to enhance educational tools through improved usability and alignment with pedagogical goals.", "key_contributions": ["Introduction of ReelsEd for generating educational short-form videos using LLMs", "Demonstrated effectiveness of ReelsEd in a user study", "Highlighted design opportunities for AI integration in education"], "limitations": "", "keywords": ["short-form videos", "educational technology", "large language models", "user experience", "microlearning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.05484", "pdf": "https://arxiv.org/pdf/2509.05484.pdf", "abs": "https://arxiv.org/abs/2509.05484", "title": "From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics", "authors": ["Hajar Sakai", "Yi-En Tseng", "Mohammadsadegh Mikaeili", "Joshua Bosire", "Franziska Jovin"], "categories": ["cs.CL"], "comment": null, "summary": "Hospital call centers serve as the primary contact point for patients within\na hospital system. They also generate substantial volumes of staff messages as\nnavigators process patient requests and communicate with the hospital offices\nfollowing the established protocol restrictions and guidelines. This\ncontinuously accumulated large amount of text data can be mined and processed\nto retrieve insights; however, traditional supervised learning approaches\nrequire annotated data, extensive training, and model tuning. Large Language\nModels (LLMs) offer a paradigm shift toward more computationally efficient\nmethodologies for healthcare analytics. This paper presents a multi-stage\nLLM-based framework that identifies staff message topics and classifies\nmessages by their reasons in a multi-class fashion. In the process, multiple\nLLM types, including reasoning, general-purpose, and lightweight models, were\nevaluated. The best-performing model was o3, achieving 78.4% weighted F1-score\nand 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and\n76.2% accuracy). The proposed methodology incorporates data security measures\nand HIPAA compliance requirements essential for healthcare environments. The\nprocessed LLM outputs are integrated into a visualization decision support tool\nthat transforms the staff messages into actionable insights accessible to\nhealthcare professionals. This approach enables more efficient utilization of\nthe collected staff messaging data, identifies navigator training\nopportunities, and supports improved patient experience and care quality.", "AI": {"tldr": "This paper presents a multi-stage framework utilizing Large Language Models (LLMs) for analyzing hospital staff messages to enhance healthcare analytics and decision support.", "motivation": "Hospital call centers generate vast amounts of text data from patient requests and staff communications, which can be analyzed for insights but face challenges like the need for annotated data in traditional methods.", "method": "The authors propose a multi-stage LLM-based framework that classifies staff message topics and reasons using various LLM types, focusing on their performance and compliance with data security standards.", "result": "The best-performing model, o3, achieved a 78.4% weighted F1-score and 79.2% accuracy, effectively classifying messages to derive actionable insights while ensuring HIPAA compliance.", "conclusion": "The proposed methodology enhances the use of staff messaging data, aids in navigator training, and supports the improvement of patient experience and care quality.", "key_contributions": ["Development of a framework for LLM-based analysis of healthcare communication", "Evaluation of different LLM types for staff message classification", "Integration of findings into a decision support tool for healthcare professionals"], "limitations": "", "keywords": ["Large Language Models", "Healthcare Analytics", "Decision Support Tool"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06114", "pdf": "https://arxiv.org/pdf/2509.06114.pdf", "abs": "https://arxiv.org/abs/2509.06114", "title": "Material Experience: An Evaluation Model for Creative Materials Based on Visual-Tactile Sensory Properties", "authors": ["Yuxin Zhang", "Fan Zhang", "Jinjun Xia", "Chao Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "This study adopts a design-oriented approach to integrate traditional braids\nwith commonly used matrix materials, developing creative materials with\ndifferent sensory properties by altering matrix material types and braid\npatterns. Based on these creative materials, a quantitative and structured\nmodel is proposed to assist designers understanding the material experience\nprocess and guide material selection by analyzing the relationship between\nmaterial properties and sensory perception. Specifically, participants\nevaluated the creative materials under visual-tactile conditions using a\n7-point semantic differential (SD) scale. Correlation analysis was performed to\nexplore the data. The main and interaction effects of matrix materials and\nbraid patterns on impression evaluation were analyzed using two-way analysis of\nvariance (ANOVA). A structural equation model (SEM) was constructed based on\nexploratory factor analysis (EFA), and path coefficients were computed to\nassess the relative importance of material properties in determining material\nattractiveness. The results show that, compared to braids, the creative\nmaterials resulted in significant changes in impression evaluation.\nFurthermore, the creative materials can be understood through intrinsic,\naesthetic, and physical properties, with their standardized regression\ncoefficients for material attractiveness of 0.486, 0.650, and 0.103,\nrespectively. These properties are interrelated and under their combined\ninfluence affect the attractiveness of the material. Therefore, designers\nshould consider utilizing these relationships to enhance sensory experience in\norder to achieve design objectives. Moreover, designers should also consider\nbalancing technology and experience, using materials according to the principle\nof \"form follows function\".", "AI": {"tldr": "The study integrates traditional braids with matrix materials to create sensory-focused designs and provides a model for understanding material experience.", "motivation": "To enhance designers' understanding of material experiences and guide material selection based on sensory perception.", "method": "Participants evaluated creative materials under visual-tactile conditions using a 7-point semantic differential scale; correlation analysis and two-way ANOVA were performed, followed by a structural equation model based on exploratory factor analysis.", "result": "Creative materials significantly changed impression evaluations compared to traditional braids, with intrinsic, aesthetic, and physical properties impacting material attractiveness.", "conclusion": "Designers should leverage the interrelations of material properties to improve sensory experience while balancing technology and experience in design.", "key_contributions": ["Development of creative materials combining traditional braids and matrix materials", "Proposal of a structured model for material experience", "Insights into the relationships between material properties and sensory perception"], "limitations": "", "keywords": ["HCI", "materials", "sensory perception", "design", "ANOVA"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.05486", "pdf": "https://arxiv.org/pdf/2509.05486.pdf", "abs": "https://arxiv.org/abs/2509.05486", "title": "The Token Tax: Systematic Bias in Multilingual Tokenization", "authors": ["Jessica M. Lundin", "Ada Zhang", "Nihal Karim", "Hamza Louzan", "Victor Wei", "David Adelani", "Cody Carroll"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP).", "AI": {"tldr": "Evaluation of LLMs on AfriMMLU highlights tokenization inefficiencies in low-resource languages and suggests morphologically aware tokenization for equitable NLP.", "motivation": "Tokenization inefficiency in morphologically complex, low-resource languages leads to increased compute resources and lower accuracy in NLP tasks.", "method": "Evaluated 10 large language models on the AfriMMLU dataset, analyzing the relationship between token fertility and model accuracy.", "result": "Higher token fertility correlates with lower accuracy; reasoning models outperform non-reasoning models across various languages.", "conclusion": "Morphological awareness in tokenization could reduce costs and improve NLP outcomes for underrepresented languages.", "key_contributions": ["Identified relationship between token fertility and model accuracy.", "Demonstrated superiority of reasoning models in language comprehension.", "Proposed solutions for equitable NLP through improved tokenization and pricing."], "limitations": "Focus limited to specific language models and dataset; wider application needs further investigation.", "keywords": ["Tokenization", "Large Language Models", "NLP", "Multilingual Benchmarks", "Morphological Awareness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.06382", "pdf": "https://arxiv.org/pdf/2509.06382.pdf", "abs": "https://arxiv.org/abs/2509.06382", "title": "Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn Multimodal LLM Conversation", "authors": ["Yingke Ding", "Zeyu Wang", "Xiyuxing Zhang", "Hongbin Chen", "Zhenan Xu"], "categories": ["cs.HC"], "comment": "Ubicomp Companion 2025", "summary": "Traditional hearing aids often rely on static fittings that fail to adapt to\ntheir dynamic acoustic environments. We propose CAFA, a Context-Adaptive\nFitting Advisor that provides personalized, real-time hearing aid adjustments\nthrough a multi-agent Large Language Model (LLM) workflow. CAFA combines live\nambient audio, audiograms, and user feedback in a multi-turn conversational\nsystem. Ambient sound is classified into conversation, noise, or quiet with\n91.2\\% accuracy using a lightweight neural network based on YAMNet embeddings.\nThis system utilizes a modular LLM workflow, comprising context acquisition,\nsubproblem classification, strategy provision, and ethical regulation, and is\noverseen by an LLM Judge. The workflow translates context and feedback into\nprecise, safe tuning commands. Evaluation confirms that real-time sound\nclassification enhances conversational efficiency. CAFA exemplifies how\nagentic, multimodal AI can enable intelligent, user-centric assistive\ntechnologies.", "AI": {"tldr": "CAFA is a novel assistant for real-time adjustment of hearing aids using a multi-agent LLM workflow that adapts to ambient sounds and user feedback.", "motivation": "To develop a hearing aid system that adapts to dynamic acoustic environments for better user experience.", "method": "CAFA uses a multi-agent LLM system that incorporates live audio classification and user feedback to make real-time adjustments to hearing aids.", "result": "CAFA achieves 91.2% accuracy in classifying ambient sound, leading to improved conversational efficiency.", "conclusion": "The proposed system demonstrates the effectiveness of multi-modal AI in providing personalized, adaptive hearing aid functionalities.", "key_contributions": ["Introduction of a context-adaptive fitting system for hearing aids", "Use of a multi-agent LLM workflow for real-time audio adjustments", "Demonstration of high accuracy in sound classification for enhanced user experience."], "limitations": "", "keywords": ["Hearing Aids", "Large Language Models", "Context-Aware Systems", "Ambient Sound Classification", "Assistive Technologies"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.05505", "pdf": "https://arxiv.org/pdf/2509.05505.pdf", "abs": "https://arxiv.org/abs/2509.05505", "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "authors": ["Mansi Garg", "Lee-Chi Wang", "Bhavesh Ghanchi", "Sanjana Dumpala", "Shreyash Kakde", "Yen Chih Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 6 figures, 3 tables", "summary": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems.", "AI": {"tldr": "This paper presents a Biomedical Literature Q&A system using a RAG architecture to enhance access to medical information, improving upon traditional health search engines.", "motivation": "To improve access to accurate, evidence-based medical information with an efficient Q&A system that addresses limitations of conventional health search engines.", "method": "The system utilizes a retrieval pipeline with MiniLM-based semantic embeddings and FAISS vector search, while employing a fine-tuned Mistral-7B-v0.3 model for generating answers.", "result": "Empirical results indicate substantial improvements in factual consistency and semantic relevance over baseline models, especially in breast cancer literature evaluations.", "conclusion": "RAG-enhanced language models present a promising solution for making complex biomedical literature more accessible and pave the way for further advancements in medical AI systems.", "key_contributions": ["Integration of multiple biomedical information sources for better Q&A performance.", "Use of MiniLM embeddings and FAISS for efficient retrieval.", "Demonstrated improvements in factual consistency and relevance for medical queries."], "limitations": "", "keywords": ["Biomedical Literature", "Question Answering", "Retrieval-Augmented Generation", "Machine Learning", "Health Informatics"], "importance_score": 10, "read_time_minutes": 10}}
{"id": "2509.06393", "pdf": "https://arxiv.org/pdf/2509.06393.pdf", "abs": "https://arxiv.org/abs/2509.06393", "title": "Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support", "authors": ["Mehrnoosh Sadat Shirvani", "Jackie Liu", "Thomas Chao", "Suky Martinez", "Laura Brandt", "Ig-Jae Kim", "Dongwook Yoon"], "categories": ["cs.HC"], "comment": null, "summary": "Mental health conversational agents have the potential to deliver valuable\ntherapeutic impact, but low user engagement remains a critical barrier\nhindering their efficacy. Existing therapeutic approaches have leveraged\nclients' internal dialogues (e.g., journaling, talking to an empty chair) to\nenhance engagement through accountable, self-sourced support. Inspired by\nthese, we designed novel AI-driven self-clone chatbots that replicate users'\nsupport strategies and conversational patterns to improve therapeutic\nengagement through externalized meaningful self-conversation. Validated through\na semi-controlled experiment (N=180), significantly higher emotional and\ncognitive engagement was demonstrated with self-clone chatbots than a chatbot\nwith a generic counselor persona. Our findings highlight self-clone\nbelievability as a mediator and emphasize the balance required in maintaining\nconvincing self-representation while creating positive interactions. This study\ncontributes to AI-based mental health interventions by introducing and\nevaluating self-clones as a promising approach to increasing user engagement,\nwhile exploring implications for their application in mental health care.", "AI": {"tldr": "This paper presents AI-driven self-clone chatbots to enhance user engagement in mental health therapy by replicating users' support strategies and conversational patterns.", "motivation": "Mental health conversational agents can enhance therapy effectiveness, but low user engagement is a significant barrier. The study aims to improve engagement through tailored AI-driven chatbots based on self-conversation techniques.", "method": "The researchers designed self-clone chatbots and conducted a semi-controlled experiment with 180 participants to compare engagement levels between self-clone chatbots and generic counselor personas.", "result": "Participants demonstrated significantly higher emotional and cognitive engagement with self-clone chatbots compared to generic ones, highlighting the effectiveness of this tailored approach.", "conclusion": "Self-clone chatbots can improve therapeutic engagement in mental health interventions, with believability being a key factor for effective user interaction.", "key_contributions": ["Introduces self-clone chatbots for enhancing engagement in mental health therapy.", "Validates the effectiveness of self-clones in a semi-controlled experiment.", "Explores implications for AI applications in mental health care."], "limitations": "The study was conducted in a semi-controlled environment which may not fully represent real-world scenarios.", "keywords": ["Mental Health", "Conversational Agents", "AI-driven Chatbots", "User Engagement", "Therapeutic Interventions"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2509.05553", "pdf": "https://arxiv.org/pdf/2509.05553.pdf", "abs": "https://arxiv.org/abs/2509.05553", "title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "authors": ["Serge Lionel Nikiema", "Jordan Samhi", "Micheline Bénédicte Moumoula", "Albérick Euraste Djiré", "Abdoul Kader Kaboré", "Jacques Klein", "Tegawendé F. Bissyandé"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems.", "AI": {"tldr": "This research investigates if large language models understand concepts or just recognize patterns, introducing bidirectional reasoning as a key test to determine true comprehension. They develop Contrastive Fine-Tuning (CFT) to enhance model capabilities in reverse reasoning. Experiments show CFT effectively enables bidirectional reasoning while maintaining forward task performance.", "motivation": "To determine if large language models truly understand concepts or just recognize patterns, leading to insights for improving AI systems.", "method": "The authors propose bidirectional reasoning as a test for understanding and develop Contrastive Fine-Tuning (CFT) that trains models with positive, negative, and obfuscation examples to enhance reverse reasoning capabilities without explicit training in reverse.", "result": "CFT was successful in enabling models to perform bidirectional reasoning, improving reverse performance while maintaining capabilities for forward tasks.", "conclusion": "Bidirectional reasoning is presented as a theoretical framework to assess understanding and as a practical method for training more capable AI systems.", "key_contributions": ["Introduction of bidirectional reasoning as a measure of true understanding in AI", "Development of Contrastive Fine-Tuning (CFT) to improve reverse reasoning without explicit training", "Experimental validation showing enhanced reasoning capabilities in language models."], "limitations": "", "keywords": ["large language models", "bidirectional reasoning", "Contrastive Fine-Tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.06475", "pdf": "https://arxiv.org/pdf/2509.06475.pdf", "abs": "https://arxiv.org/abs/2509.06475", "title": "Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems", "authors": ["Yannick Kalff", "Katharina Simbeck"], "categories": ["cs.HC", "cs.AI", "cs.CY", "A.0; H.5.2; I.2; J.1; K.4.2; K.4.3"], "comment": "Accepted paper for RecSys in HR'25: The 5th Workshop on Recommender\n  Systems for Human Resources, in conjunction with the 19th ACM Conference on\n  Recommender Systems, September 22--26, 2025, Prague, Czech Republic", "summary": "AI-based recommender systems increasingly influence recruitment decisions.\nThus, transparency and responsible adoption in Human Resource Management (HRM)\nare critical. This study examines how HR managers' AI literacy influences their\nsubjective perception and objective understanding of explainable AI (XAI)\nelements in recruiting recommender dashboards. In an online experiment, 410\nGerman-based HR managers compared baseline dashboards to versions enriched with\nthree XAI styles: important features, counterfactuals, and model criteria. Our\nresults show that the dashboards used in practice do not explain AI results and\neven keep AI elements opaque. However, while adding XAI features improves\nsubjective perceptions of helpfulness and trust among users with moderate or\nhigh AI literacy, it does not increase their objective understanding. It may\neven reduce accurate understanding, especially with complex explanations. Only\noverlays of important features significantly aided the interpretations of\nhigh-literacy users. Our findings highlight that the benefits of XAI in\nrecruitment depend on users' AI literacy, emphasizing the need for tailored\nexplanation strategies and targeted literacy training in HRM to ensure fair,\ntransparent, and effective adoption of AI.", "AI": {"tldr": "This study investigates how AI literacy among HR managers impacts their understanding and perception of explainable AI features in recruitment dashboards.", "motivation": "The study aims to address the critical need for transparency and responsible adoption of AI in HR management, particularly regarding its influence on recruitment decisions.", "method": "An online experiment was conducted with 410 German-based HR managers who compared baseline recruitment dashboards to enhanced versions featuring three styles of explainable AI.", "result": "Dashboards used in practice often lack explanations for AI results. Adding XAI features improves subjective perceptions of trust and helpfulness among AI literate users, but does not enhance their objective understanding, with potential confusion arising from complex explanations.", "conclusion": "The effectiveness of XAI in recruitment varies with users' AI literacy, indicating the necessity for tailored explanations and targeted training in HRM for responsible AI adoption.", "key_contributions": ["Identification of the gap in AI explanation provided in current recruitment dashboards", "Demonstration of how AI literacy affects perception and understanding of XAI features", "Recommendations for tailored explanation strategies and literacy training in HRM"], "limitations": "The study is limited to German-based HR managers, which may affect the generalizability of the findings.", "keywords": ["AI literacy", "explainable AI", "recruitment", "human resource management", "trust"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.05566", "pdf": "https://arxiv.org/pdf/2509.05566.pdf", "abs": "https://arxiv.org/abs/2509.05566", "title": "Ad hoc conventions generalize to new referents", "authors": ["Anya Ji", "Claire Augusta Bergey", "Ron Eliav", "Yoav Artzi", "Robert D. Hawkins"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents.", "AI": {"tldr": "The study investigates how people create shared naming conventions for objects they have not previously discussed, using a communication study with tangram images to test generalization of these conventions.", "motivation": "To understand how new shared naming systems are formed and whether they merely link to specific targets or help reshape semantic spaces for broader generalization.", "method": "A dyadic communication study involving 302 participants was conducted, using the KiloGram dataset of abstract tangram images to analyze how pairs of participants developed referential conventions through repeated communication.", "result": "Participants showed strong evidence of generalization in their descriptions for undiscussed images, with alignment increasing compared to pre-test labels, and generalization decayed nonlinearly with visual similarity.", "conclusion": "The findings indicate that shared naming conventions are not just arbitrary labels, but involve genuine conceptual coordination, affecting theories of reference and the design of language agents.", "key_contributions": ["Demonstrated generalization of naming conventions beyond discussed items.", "Provided evidence against the view that naming is purely arbitrary.", "Offered insights into the implications for designing adaptive language agents."], "limitations": "", "keywords": ["shared naming", "dyadic communication", "generalization", "semantic space", "language agents"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.06557", "pdf": "https://arxiv.org/pdf/2509.06557.pdf", "abs": "https://arxiv.org/abs/2509.06557", "title": "Mapping Community Appeals Systems: Lessons for Community-led Moderation in Multi-Level Governance", "authors": ["Juhoon Lee", "Bich Ngoc Doan", "Jonghyun Jee", "Joseph Seering"], "categories": ["cs.HC"], "comment": "Accepted for CSCW 2025", "summary": "Platforms are increasingly adopting industrial models of moderation that\nprioritize scalability and consistency, frequently at the expense of\ncontext-sensitive and user-centered values. Building on the multi-level\ngovernance framework that examines the interdependent relationship between\nplatforms and middle-level communities, we investigate community appeals\nsystems on Discord as a model for successful community-led governance. We\ninvestigate how Discord servers operationalize appeal systems through a\nqualitative interview study with focus groups and individual interviews with 17\ncommunity moderators. Our findings reveal a structured appeals process that\nbalances scalability, fairness, and accountability while upholding\ncommunity-centered values of growth and rehabilitation. Communities design\nthese processes to empower users, ensuring their voices are heard in moderation\ndecisions and fostering a sense of belonging. This research provides insights\ninto the practical implementation of community-led governance in a multi-level\ngovernance framework, illustrating how communities can maintain their core\nprinciples while integrating procedural fairness and tool-based design. We\ndiscuss how platforms can gain insights from community-led moderation work to\nmotivate governance structures that effectively balance and align the interests\nof multiple stakeholders.", "AI": {"tldr": "This paper examines community-led appeal systems on Discord, highlighting how structured processes can balance scalability and fairness while empowering users.", "motivation": "To explore how community-led governance can maintain user-centered values within platform moderation.", "method": "Qualitative interviews with focus groups and individual interviews with 17 community moderators on Discord.", "result": "Findings indicate a structured appeals process that upholds community values of growth, fairness, and accountability, empowering users in moderation decisions.", "conclusion": "Community-led governance can inform platforms about balancing stakeholder interests and integrating procedural fairness and design.", "key_contributions": ["Investigation of community appeals systems on Discord", "Insights into community-led governance processes", "Recommendations for platforms on improving moderation systems"], "limitations": "", "keywords": ["community governance", "moderation", "Discord", "appeals system", "user empowerment"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.05602", "pdf": "https://arxiv.org/pdf/2509.05602.pdf", "abs": "https://arxiv.org/abs/2509.05602", "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song", "Xuelong Li"], "categories": ["cs.CL"], "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "AI": {"tldr": "This paper proposes Chain-of-Thought Correctness Perception Distillation (CoPeD) to enhance reasoning quality in small language models (SLMs) by using a correctness-aware task setting and dynamically adjusting training loss.", "motivation": "To improve the reasoning capabilities of small language models (SLMs) fine-tuned on potentially noisy Chain-of-Thought (CoT) data generated by large language models (LLMs), which can lead to spurious correlations and compromised reasoning quality.", "method": "The paper introduces a correctness-aware task setting that prompts the student model to predict and revise answers based on rationales, paired with a Correctness-Aware Weighted loss that prioritizes training instances with more supportive rationales for improving reasoning quality.", "result": "Experiments demonstrate that CoPeD significantly enhances reasoning performance on both in-distribution and out-of-distribution benchmark datasets for reasoning tasks.", "conclusion": "CoPeD effectively boosts the faithfulness of the reasoning process in SLMs by emphasizing accurate rationales, leading to improved answer predictions and a better learning experience from errors.", "key_contributions": ["Introduction of a correctness-aware task setting for reasoning improvement", "Development of a Correctness-Aware Weighted loss to focus training on stronger rationale support", "Demonstration of effectiveness on various benchmark reasoning datasets."], "limitations": "", "keywords": ["language models", "reasoning quality", "small language models", "Chain-of-Thought", "data utilization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06776", "pdf": "https://arxiv.org/pdf/2509.06776.pdf", "abs": "https://arxiv.org/abs/2509.06776", "title": "Hue4U: Real-Time Personalized Color Correction in Augmented Reality", "authors": ["Jingwen Qin", "Semen Checherin", "Yue Li", "Berend-Jan van der Zwaag", "Özlem Durmaz-Incel"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent\nof women worldwide. Existing color-correction methods often rely on prior\nclinical diagnosis and static filtering, making them less effective for users\nwith mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,\nreal-time color-correction system in augmented reality using consumer-grade\nMeta Quest headsets. Unlike previous methods, Hue4U requires no prior medical\ndiagnosis and adapts to the user in real time. A user study with 10\nparticipants showed notable improvements in their ability to distinguish\ncolors. The results demonstrated large effect sizes (Cohen's d > 1.4),\nsuggesting clinically meaningful gains for individuals with CVD. These findings\nhighlight the potential of personalized AR interventions to improve visual\naccessibility and quality of life for people affected by CVD.", "AI": {"tldr": "Hue4U is a real-time color-correction system in augmented reality designed for individuals with Color Vision Deficiency (CVD), which adapts to users without prior medical diagnosis.", "motivation": "To enhance visual accessibility for individuals with Color Vision Deficiency by providing a personalized color-correction solution.", "method": "A personalized, real-time color-correction system using consumer-grade Meta Quest headsets is developed, which adapts to the user during use.", "result": "User study with 10 participants revealed notable improvements in color distinction abilities and clinically meaningful gains (Cohen's d > 1.4).", "conclusion": "Personalized augmented reality interventions, such as Hue4U, have the potential to significantly improve the quality of life and visual accessibility for individuals with CVD.", "key_contributions": ["Introduction of a real-time color-correction system in augmented reality for CVD", "No prior medical diagnosis required for system adaptation", "Demonstrated significant improvements in color distinction for users with CVD"], "limitations": "", "keywords": ["Color Vision Deficiency", "augmented reality", "color-correction", "visual accessibility", "user study"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.05605", "pdf": "https://arxiv.org/pdf/2509.05605.pdf", "abs": "https://arxiv.org/abs/2509.05605", "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "authors": ["Qiyuan Chen", "Hongsen Huang", "Qian Shao", "Jiahe Chen", "Jintai Chen", "Hongxia Xu", "Renjie Hua", "Ren Chuan", "Jian Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%.", "AI": {"tldr": "This paper presents Icon$^{2}$, a novel approach to construct high-quality preference datasets for LLMs by leveraging their representation space, resulting in improved alignment and reduced computational costs.", "motivation": "To address the challenges in constructing preference datasets for large language models that rely on human preferences effectively and efficiently.", "method": "Icon$^{2}$ method extracts layer-wise direction vectors to encode human preferences and filters self-synthesized instructions based on consistency, applying inherent control during decoding.", "result": "The experiments show a 13.89% improvement on AlpacaEval 2.0 and a 13.45% improvement on Arena-Hard, along with a reduction in computational costs by up to 48.1%.", "conclusion": "The proposed method represents a significant enhancement in constructing preference datasets for LLMs, achieving both better alignment with human preferences and efficiency.", "key_contributions": ["Introduction of Icon$^{2}$ for preference dataset construction", "Layer-wise direction vectors for encoding human preferences", "Improved efficiency with reduced computational costs"], "limitations": "", "keywords": ["Large Language Models", "Preference Datasets", "Human Preferences"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06934", "pdf": "https://arxiv.org/pdf/2509.06934.pdf", "abs": "https://arxiv.org/abs/2509.06934", "title": "\"It was Tragic\": Exploring the Impact of a Robot's Shutdown", "authors": ["Agam Oberlender", "Hadas Erel"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 1 table, submitted to IEEE RO-MAN 2025", "summary": "It is well established that people perceive robots as social entities, even\nwhen they are not designed for social interaction. We evaluated whether the\nsocial interpretation of robotic gestures should also be considered when\nturning off a robot. In the experiment, participants engaged in a brief\npreliminary neutral interaction while a robotic arm showed interest in their\nactions. At the end of the task, participants were asked to turn off the\nrobotic arm under two conditions: (1) a Non-designed condition, where all of\nthe robot's engines were immediately and simultaneously turned off, as robots\ntypically shut down; (2) a Designed condition, where the robot's engines\ngradually folded inward in a motion resembling \"falling asleep.\" Our findings\nrevealed that all participants anthropomorphized the robot's movement when it\nwas turned off. In the Non-designed condition, most participants interpreted\nthe robot's turn-off movement negatively, as if the robot had \"died.\" In the\nDesigned condition, most participants interpreted it more neutrally, stating\nthat the robot \"went to sleep.\" The robot's turn-off movement also impacted its\nperception, leading to higher likeability, perceived intelligence, and animacy\nin the Designed condition. We conclude that the impact of common edge\ninteractions, such as turning off a robot, should be carefully designed while\nconsidering people's automatic tendency to perceive robots as social entities.", "AI": {"tldr": "This study investigates how people perceive the shutdown of robotic arms, exploring the impact of shutdown gestures on their social interpretation.", "motivation": "To understand the social dynamics of how humans interpret robotic gestures, particularly during shutdown procedures.", "method": "An experiment was conducted where participants interacted with a robotic arm and then turned it off under two different conditions: a non-designed abrupt shutdown and a designed gradual shutdown resembling 'falling asleep.'", "result": "Participants anthropomorphized the robot's shutdown gestures. The abrupt shutdown was perceived negatively, akin to death, while the gradual shutdown was viewed more positively as the robot 'going to sleep,' affecting its perceived likeability and intelligence.", "conclusion": "Designing interactions for turning off robots is crucial, as people naturally view robots as social entities. The way robots are shut down can influence human perceptions significantly.", "key_contributions": ["Identifying the influence of shutdown gestures on human perception of robots", "Demonstrating the anthropomorphism of robots during shutdown", "Highlighting the importance of designing robotic interactions thoughtfully."], "limitations": "The study focused on a single type of robot and scenario; results may vary with different robots or contexts.", "keywords": ["Human-Robot Interaction", "Anthropomorphism", "Robot Shutdown", "Social Interpretation", "Design"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.05607", "pdf": "https://arxiv.org/pdf/2509.05607.pdf", "abs": "https://arxiv.org/abs/2509.05607", "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "authors": ["Qiyuan Chen", "Jiahe Chen", "Hongsen Huang", "Qian Shao", "Jintai Chen", "Renjie Hua", "Hongxia Xu", "Ruijia Wu", "Ren Chuan", "Jian Wu"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.", "AI": {"tldr": "This paper presents a framework for Generative Search Engine Optimization (GSEO) to measure and optimize content influence on search engines.", "motivation": "The shift to Generative Search Engines has made traditional SEO metrics irrelevant, necessitating new methods to understand and enhance content impact on synthesized search results.", "method": "The paper introduces a large-scale benchmark, CC-GSEO-Bench, and a multi-dimensional evaluation framework to quantify content influence. Additionally, it develops a multi-agent system for automating content refinement through a workflow of analysis, revision, and evaluation.", "result": "Empirical analysis using the proposed framework demonstrated new insights into content influence dynamics and provided actionable strategies for content creators.", "conclusion": "The research lays a groundwork for improved GSEO practices and future studies in the area, emphasizing the importance of a deeper understanding of content impact.", "key_contributions": ["Development of CC-GSEO-Bench as a content-centric benchmark", "Introduction of a multi-dimensional evaluation framework for content influence", "Creation of a multi-agent system for automated content refinement"], "limitations": "", "keywords": ["Generative Search", "Search Engine Optimization", "Content Influence", "Multi-Agent Systems", "Benchmarking"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.05609", "pdf": "https://arxiv.org/pdf/2509.05609.pdf", "abs": "https://arxiv.org/abs/2509.05609", "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.", "AI": {"tldr": "The paper presents an unbalanced optimal transport-based model for aligning acoustic and linguistic representations in automatic speech recognition, addressing many-to-one and one-to-many relationships while managing noise and imbalances.", "motivation": "The need to bridge pre-trained models in knowledge transfer for ASR by accurately aligning acoustic and linguistic representations, resolving the inherent structural asymmetries.", "method": "An unbalanced optimal transport approach that facilitates soft and partial matching between acoustic frames and linguistic tokens.", "result": "The model ensures every linguistic token is linked to acoustic observations, improving ASR performance by allowing flexible mapping and handling redundant or noisy frames.", "conclusion": "The proposed model effectively enhances ASR performance by controlling the matching process between acoustic and linguistic modalities, addressing key challenges in knowledge transfer.", "key_contributions": ["Introduction of an unbalanced optimal transport model for alignment in ASR", "Addressing structural asymmetries in acoustic-linguistic relationships", "Demonstration of improved ASR performance through flexible matching methodology."], "limitations": "", "keywords": ["automatic speech recognition", "acoustic-linguistic alignment", "optimal transport", "knowledge transfer", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.05617", "pdf": "https://arxiv.org/pdf/2509.05617.pdf", "abs": "https://arxiv.org/abs/2509.05617", "title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics", "authors": ["Shay Dahary", "Avi Edana", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 2 figures", "summary": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution.", "AI": {"tldr": "This paper explores the multi-label emotional attribution of song lyrics using a novel dataset and various LLMs.", "motivation": "To understand how emotional content in song lyrics affects listener experiences and preferences.", "method": "A manually labeled dataset is created using a mean opinion score approach. Several LLMs are evaluated in zero-shot scenarios, and a BERT-based model is fine-tuned for multi-label emotion score prediction.", "result": "The evaluation reveals strengths and limitations of both zero-shot and fine-tuned models in emotion recognition.", "conclusion": "The study demonstrates the potential of LLMs in capturing emotional nuances in lyrics, informing model selection for emotion-based music information retrieval.", "key_contributions": ["Creation of a comprehensive dataset for emotional attribution of song lyrics.", "Evaluation of multiple LLMs for emotion recognition in lyrics.", "Insights into model selection for music information retrieval applications."], "limitations": "The focus is primarily on lyrics, and generalizability to other text forms may be limited.", "keywords": ["emotional attribution", "song lyrics", "large language models", "BERT", "music information retrieval"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.05635", "pdf": "https://arxiv.org/pdf/2509.05635.pdf", "abs": "https://arxiv.org/abs/2509.05635", "title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning", "authors": ["Liang Zhang", "Yuan Li", "Shijie Zhang", "Zheng Zhang", "Xitong Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "SAID is a framework that improves intent detection in conversational systems by integrating textual and relational structure information for model pretraining.", "motivation": "To enhance intent detection in conversational systems by effectively capturing structural information such as query-query and query-answer relations, which are often neglected in current methods.", "method": "The proposed framework SAID integrates both textual and relational structure information during model pretraining and introduces the QueryAdapt mechanism to generate intent-specific relation tokens.", "result": "SAID outperforms state-of-the-art methods in intent detection on two real-world datasets, demonstrating the effectiveness of integrating structural information.", "conclusion": "The integration of relational structures with textual data significantly enhances the performance of intent detection in conversational systems.", "key_contributions": ["Introduction of SAID framework for integrating textual and relational structure information in intent detection.", "Development of QueryAdapt mechanism for generating intent-specific relation tokens.", "Demonstration of superior performance over existing methods in real-world datasets."], "limitations": "", "keywords": ["Intent Detection", "Conversational Systems", "Relational Information", "Language Models", "Few-Shot Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.05657", "pdf": "https://arxiv.org/pdf/2509.05657.pdf", "abs": "https://arxiv.org/abs/2509.05657", "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "authors": ["Yuxuan Hu", "Jihao Liu", "Ke Wang", "Jinliang Zhen", "Weikang Shi", "Manyuan Zhang", "Qi Dou", "Rui Liu", "Aojun Zhou", "Hongsheng Li"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP2025", "summary": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.", "AI": {"tldr": "The paper introduces LM-Searcher, a framework leveraging Large Language Models for cross-domain Neural Architecture Search (NAS) without extensive domain-specific tuning.", "motivation": "To address the limitations of current LLM-driven NAS approaches that depend on prompt engineering and domain-specific adaptations.", "method": "The authors propose a novel numerical string representation (NCode) for neural architectures and reformulate the NAS problem as a ranking task, using a dataset from a pruning-based subspace sampling strategy to train LLMs for architecture selection.", "result": "LM-Searcher demonstrates competitive performance for both in-domain (CNNs for image classification) and out-of-domain tasks (LoRA configurations for segmentation and generation).", "conclusion": "The proposed approach establishes a new paradigm for flexible and generalizable LLM-based architecture search, with datasets and models available for further research.", "key_contributions": ["Introduction of LM-Searcher framework for NAS using LLMs.", "Development of NCode for universal architecture representation.", "Reformulation of NAS as a ranking task for improved architecture selection."], "limitations": "The method relies on the quality of the curated dataset for effective training and performance evaluation.", "keywords": ["Large Language Models", "Neural Architecture Search", "cross-domain optimization", "NCode", "ranking task"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.05660", "pdf": "https://arxiv.org/pdf/2509.05660.pdf", "abs": "https://arxiv.org/abs/2509.05660", "title": "Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning", "authors": ["Hong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely applied to assist in finding\nsolutions for diverse questions. Prior work has proposed representing a method\nas a pair of a question and its corresponding solution, enabling method reuse.\nHowever, existing approaches typically require the questions to be highly\nsimilar. In this paper, we extend the scope of method reuse to address\nquestions with low similarity or with hidden similarities that are not\nexplicitly observable. For questions that are similar in a general-specific\nsense (i.e., broader or narrower in scope), we propose to first separate the\nquestion and solution, rather than directly feeding the pair to the LLM. The\nLLM is then guided to adapt the solution to new but related questions, allowing\nit to focus on solution transfer rather than question recognition. Furthermore,\nwe extend this approach to cases where questions only share partial features or\nhidden characteristics. This enables cross-question method reuse beyond\nconventional similarity constraints. Experimental verification shows that our\nscope-extension approach increases the probability of filtering out reusable\nsolutions, thereby improving the effectiveness of cross-question method reuse.", "AI": {"tldr": "This paper proposes a new approach for method reuse in large language models by allowing solutions to be adapted to questions with low or hidden similarities.", "motivation": "To enhance method reuse in large language models for questions that aren't highly similar or share hidden similarities.", "method": "The proposed method separates questions and solutions, guiding the LLM to adjust solutions for related questions rather than relying on direct question similarity.", "result": "The new approach has been experimentally verified to increase the likelihood of identifying reusable solutions, improving cross-question method reuse effectiveness.", "conclusion": "The scope-extension method allows for better adaptation of solutions to questions with varying degrees of similarity, addressing limitations of conventional approaches.", "key_contributions": ["A novel separation of questions and solutions to aid method reuse", "Extension of method reuse to low-similarity and partially similar questions", "Experimental evidence supporting improved effectiveness in cross-question reuse"], "limitations": "The method may not fully address extremely dissimilar questions or complex interactions between question features.", "keywords": ["large language models", "method reuse", "cross-question adaptation", "HCI", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164.pdf", "abs": "https://arxiv.org/abs/2509.06164", "title": "Benchmarking Gender and Political Bias in Large Language Models", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts.", "AI": {"tldr": "EuroParlVote benchmark evaluates LLMs in political contexts, revealing biases in gender classification and vote prediction.", "motivation": "To create a benchmark that assesses large language models in politically sensitive situations and to investigate biases in their predictions related to gender and political orientation.", "method": "A novel dataset linking European Parliament debate speeches with roll-call vote outcomes and demographic metadata for MEPs is used to evaluate LLMs on tasks of gender classification and vote prediction.", "result": "The evaluation shows LLMs misclassifying female MEPs, with reduced accuracy in vote prediction for female speakers, and a tendency to favor centrist political groups.", "conclusion": "The findings highlight biases in LLMs, particularly in political contexts, and the release of the dataset aims to assist future research on fairness in NLP.", "key_contributions": ["Introduction of the EuroParlVote dataset and benchmark", "Evaluation of state-of-the-art LLMs in political contexts", "Identification of biases in gender classification and vote prediction for LLMs."], "limitations": "The focus is primarily on European Parliament data, which may not generalize to other political contexts.", "keywords": ["large language models", "political bias", "gender classification", "vote prediction", "fairness in NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.05668", "pdf": "https://arxiv.org/pdf/2509.05668.pdf", "abs": "https://arxiv.org/abs/2509.05668", "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "authors": ["Michael Hoffmann", "Jophin John", "Stefan Schweter", "Gokul Ramakrishnan", "Hoi-Fong Mak", "Alice Zhang", "Dmitry Gaynullin", "Nicolay J. Hammer"], "categories": ["cs.CL", "cs.AI"], "comment": "Michael Hoffmann and Jophin John contributed equally to this work", "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.", "AI": {"tldr": "Llama-GENBA-10B is a trilingual foundation model designed to mitigate English-centric bias in large language models, featuring balanced pretraining across English, German, and Bavarian languages with strong cross-lingual performance.", "motivation": "To address English-centric bias in large language models and promote the use of low-resource languages like Bavarian.", "method": "Llama-GENBA-10B was pretrained on a balanced dataset of 164B tokens, optimizing architecture and hyperparameters for cross-lingual transfer and creating a standardized evaluation suite.", "result": "The model shows strong performance, outperforming previous models in Bavarian and matching benchmarks in German while also excelling in English.", "conclusion": "Llama-GENBA-10B sets a precedent for training inclusive foundation models that effectively integrate low-resource languages without favoring English.", "key_contributions": ["Development of a trilingual foundation model with balanced resources across languages", "Establishment of a standardized evaluation suite for trilingual benchmarks", "Demonstration of energy-efficient large-scale multilingual pretraining"], "limitations": "", "keywords": ["Llama-GENBA-10B", "multilingual models", "low-resource languages"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.05691", "pdf": "https://arxiv.org/pdf/2509.05691.pdf", "abs": "https://arxiv.org/abs/2509.05691", "title": "Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models", "authors": ["Ningyuan Deng", "Hanyu Duan", "Yixuan Tang", "Yi Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text embedding models are widely used in natural language processing\napplications. However, their capability is often benchmarked on tasks that do\nnot require understanding nuanced numerical information in text. As a result,\nit remains unclear whether current embedding models can precisely encode\nnumerical content, such as numbers, into embeddings. This question is critical\nbecause embedding models are increasingly applied in domains where numbers\nmatter, such as finance and healthcare. For example, Company X's market share\ngrew by 2\\% should be interpreted very differently from Company X's market\nshare grew by 20\\%, even though both indicate growth in market share. This\nstudy aims to examine whether text embedding models can capture such nuances.\nUsing synthetic data in a financial context, we evaluate 13 widely used text\nembedding models and find that they generally struggle to capture numerical\ndetails accurately. Our further analyses provide deeper insights into embedding\nnumeracy, informing future research to strengthen embedding model-based NLP\nsystems with improved capacity for handling numerical content.", "AI": {"tldr": "This study examines the ability of text embedding models to accurately capture numerical details in natural language processing applications, particularly in finance and healthcare.", "motivation": "To investigate whether current text embedding models can effectively encode nuanced numerical information, which is critical for applications in fields such as finance and healthcare where numbers matter.", "method": "The study evaluates 13 widely used text embedding models using synthetic data in a financial context to assess their performance in capturing numerical details.", "result": "The analysis reveals that the majority of text embedding models struggle to accurately capture nuanced numerical content, indicating a significant limitation in their applicability for domains with critical numerical information.", "conclusion": "The findings highlight the need for improved methodologies in training embedding models to enhance their ability to handle numerical information effectively, which could benefit various NLP applications.", "key_contributions": ["Evaluation of 13 text embedding models in terms of their numerical encoding capabilities.", "Insights into the limitations of current text embedding models regarding numerical details.", "Recommendations for future research to enhance numerical understanding in embedding models."], "limitations": "The study is based on synthetic data, and real-world performance may vary.", "keywords": ["text embedding models", "numerical information", "natural language processing", "finance", "healthcare"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.05716", "pdf": "https://arxiv.org/pdf/2509.05716.pdf", "abs": "https://arxiv.org/abs/2509.05716", "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Fahmida Islam", "Maryam Tahermazandarani", "Quan Z. Sheng"], "categories": ["cs.CL", "cs.AI"], "comment": "42 pages, 12 figures, 4 tables", "summary": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field.", "AI": {"tldr": "This survey examines Conversational Question Answering (ConvQA) systems, analyzing their components, methods, and the impact of large language models.", "motivation": "To provide a comprehensive overview of state-of-the-art ConvQA systems and their applications across various domains, particularly in maintaining coherence and relevance in conversations.", "method": "The paper reviews the core components of ConvQA systems, discusses advanced machine learning techniques, and explores the role of large language models. It also analyzes key datasets related to ConvQA.", "result": "An overview of how ConvQA systems utilize various machine learning methods to enhance conversation quality, along with a detailed analysis of ConvQA datasets.", "conclusion": "The survey outlines significant advancements in ConvQA and suggests open research directions for future improvement in the field.", "key_contributions": ["Comprehensive analysis of ConvQA components and techniques", "Examination of the role of large language models in ConvQA", "Identification of key datasets for ConvQA research"], "limitations": "", "keywords": ["Conversational Question Answering", "Natural Language Processing", "Large Language Models"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2509.05719", "pdf": "https://arxiv.org/pdf/2509.05719.pdf", "abs": "https://arxiv.org/abs/2509.05719", "title": "Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models", "authors": ["Donya Rooein", "Flor Miriam Plaza-del-Arco", "Debora Nozza", "Dirk Hovy"], "categories": ["cs.CL"], "comment": null, "summary": "Given Farsi's speaker base of over 127 million people and the growing\navailability of digital text, including more than 1.3 million articles on\nWikipedia, it is considered a middle-resource language. However, this label\nquickly crumbles when the situation is examined more closely. We focus on three\nsubjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)\nand find significant challenges in data availability and quality, despite the\noverall increase in data availability. We review 110 publications on subjective\ntasks in Farsi and observe a lack of publicly available datasets. Furthermore,\nexisting datasets often lack essential demographic factors, such as age and\ngender, that are crucial for accurately modeling subjectivity in language. When\nevaluating prediction models using the few available datasets, the results are\nhighly unstable across both datasets and models. Our findings indicate that the\nvolume of data is insufficient to significantly improve a language's prospects\nin NLP.", "AI": {"tldr": "This paper examines the state of subjective NLP tasks in Farsi, highlighting challenges in data availability and quality despite increasing overall digital content.", "motivation": "To assess the challenges and limitations in NLP for the Farsi language, particularly in subjective tasks.", "method": "Review of 110 publications on sentiment analysis, emotion analysis, and toxicity detection in Farsi, focusing on data availability and model performance.", "result": "Significant challenges in data quality and availability were found, with existing datasets lacking essential demographic factors, leading to unstable model predictions.", "conclusion": "The volume of available data is insufficient to enhance the performance of Farsi in NLP tasks significantly.", "key_contributions": ["Identified key challenges in subjective task NLP for Farsi.", "Reviewed extensive literature on the topic and highlighted the lack of datasets.", "Showed instability in model performance with existing Farsi datasets."], "limitations": "The existing datasets are limited in both quantity and quality, often omitting crucial demographic information.", "keywords": ["Farsi", "NLP", "Sentiment Analysis", "Emotion Analysis", "Toxicity Detection"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.05729", "pdf": "https://arxiv.org/pdf/2509.05729.pdf", "abs": "https://arxiv.org/abs/2509.05729", "title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing", "authors": ["Charles M. Varmantchaonala", "Niclas GÖtting", "Nils-Erik SchÜtte", "Jean Louis E. K. Fendji", "Christopher Gies"], "categories": ["cs.CL"], "comment": null, "summary": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains.", "AI": {"tldr": "This paper introduces QCSE, a pretrained quantum context-sensitive embedding model that utilizes quantum computation to enhance natural language processing by capturing contextual relationships in languages.", "motivation": "To explore how quantum computation can improve the encoding and understanding of natural languages, especially in low-resource languages.", "method": "The paper presents five distinct methods for computing context matrices, utilizing quantum techniques like exponential decay, sinusoidal modulation, phase shifts, and hash-based transformations to create context-sensitive word embeddings.", "result": "The QCSE model effectively captures context sensitivity and demonstrates the potential of quantum systems to enhance language representation, validated through evaluations on Fulani and English corpora.", "conclusion": "Quantum computation provides new techniques for context-aware representations in NLP, addressing challenges in language processing, particularly for underrepresented languages.", "key_contributions": ["Introduction of QCSE, a quantum context-sensitive embedding model", "Development of innovative context matrix computation methods", "Demonstration of the efficacy of quantum embeddings in low-resource languages"], "limitations": "", "keywords": ["Quantum Natural Language Processing", "context-sensitive embeddings", "quantum computing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.05741", "pdf": "https://arxiv.org/pdf/2509.05741.pdf", "abs": "https://arxiv.org/abs/2509.05741", "title": "Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification", "authors": ["Fernando Gabriela García", "Qiyang Shi", "Zilin Feng"], "categories": ["cs.CL"], "comment": null, "summary": "This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a\nnovel method designed to address the pervasive issues of hallucination and the\nabsence of credible citation sources in Large Language Models (LLMs) when\ngenerating complex, fact-sensitive content. By incorporating a multi-stage\nmechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT\nempowers LLMs to critically self-examine and revise their intermediate\nreasoning steps and final answers. This process significantly enhances the\nobjective accuracy, trustworthiness, and traceability of the generated outputs,\nmaking LLMs more reliable for applications demanding high fidelity such as\nscientific research, news reporting, and legal consultation.", "AI": {"tldr": "Introduces VeriFact-CoT, a method to improve the accuracy and reliability of LLMs by integrating fact verification and citation processes.", "motivation": "To tackle hallucination issues and lack of credible sources in LLMs for generating fact-sensitive content.", "method": "A multi-stage mechanism involving 'fact verification-reflection-citation integration' enables self-examination and revision of LLM outputs.", "result": "The method improves the objective accuracy, trustworthiness, and traceability of LLM-generated content, making it reliable for critical applications.", "conclusion": "VeriFact-CoT significantly enhances the reliability of LLMs in high-fidelity applications.", "key_contributions": ["Develops a method for fact verification in LLMs", "Integrates citation processes to improve output reliability", "Enhances trustworthiness of LLMs for fact-sensitive applications"], "limitations": "", "keywords": ["VeriFact-CoT", "Large Language Models", "fact verification", "citation integration", "hallucination"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.05863", "pdf": "https://arxiv.org/pdf/2509.05863.pdf", "abs": "https://arxiv.org/abs/2509.05863", "title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "categories": ["cs.CL"], "comment": null, "summary": "We present LatinX, a multilingual text-to-speech (TTS) model for cascaded\nspeech-to-speech translation that preserves the source speaker's identity\nacross languages. LatinX is a 12-layer decoder-only Transformer trained in\nthree stages: (i) pre-training for text-to-audio mapping, (ii) supervised\nfine-tuning for zero-shot voice cloning, and (iii) alignment with Direct\nPreference Optimization (DPO) using automatically labeled pairs based on Word\nError Rate (WER) and speaker-similarity metrics. Trained on English and Romance\nlanguages with emphasis on Portuguese, LatinX with DPO consistently reduces WER\nand improves objective similarity over the fine-tuned baseline. Human\nevaluations further indicate stronger perceived speaker similarity than a\nstrong baseline (XTTSv2), revealing gaps between objective and subjective\nmeasures. We provide cross-lingual analyses and discuss balanced preference\nsignals and lower-latency architectures as future work.", "AI": {"tldr": "LatinX is a multilingual TTS model that translates speech while maintaining speaker identity across languages.", "motivation": "To improve speech-to-speech translation by maintaining the original speaker's identity and enhancing performance metrics like WER and speaker similarity.", "method": "LatinX is trained in three stages: pre-training for text-to-audio mapping, supervised fine-tuning for zero-shot voice cloning, and alignment with Direct Preference Optimization using labeled pairs based on WER and similarity metrics.", "result": "LatinX reduces WER and improves speaker similarity scores compared to the baseline, as verified by human evaluations.", "conclusion": "LatinX demonstrates significant improvements in preserving speaker identity across languages and shows promise for future developments in TTS models.", "key_contributions": ["Introduction of LatinX for multilingual TTS that maintains speaker identity.", "Three-stage training process enhancing performance metrics.", "Cross-lingual analyses indicating stronger subjective speaker similarity."], "limitations": "Gaps between objective measures (WER) and subjective perceived similarity metrics need further exploration.", "keywords": ["multilingual TTS", "speech-to-speech translation", "voice cloning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.05867", "pdf": "https://arxiv.org/pdf/2509.05867.pdf", "abs": "https://arxiv.org/abs/2509.05867", "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula", "authors": ["ZiXuan Zhang", "Bowen Hao", "Yingjie Li", "Hongzhi Yin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.", "AI": {"tldr": "ZhiFangDanTai is a framework utilizing GraphRAG and LLM fine-tuning to enhance explainable formula generation in Traditional Chinese Medicine, addressing limitations in existing datasets.", "motivation": "Existing TCM models lack comprehensive outputs including detailed formula compositions and explanations, and datasets with insufficient details hinder model effectiveness.", "method": "The framework combines Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning to retrieve TCM knowledge and improve model outputs.", "result": "ZhiFangDanTai shows significant improvements in reducing generalization error and hallucination rates when tested on both collected and clinical datasets.", "conclusion": "The proposed method provides a more effective approach to generate explainable TCM formulas, contributing to better applications in health informatics.", "key_contributions": ["Introduction of ZhiFangDanTai framework combining GraphRAG and LLM fine-tuning", "Theoretical proofs showing reduced generalization error and hallucination rates", "Open-source implementation available for community use."], "limitations": "Existing datasets lack complete details regarding TCM formulas that may impact the model's capability.", "keywords": ["Traditional Chinese Medicine", "Graph-based Retrieval", "Large Language Models", "explainable AI", "health informatics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.05878", "pdf": "https://arxiv.org/pdf/2509.05878.pdf", "abs": "https://arxiv.org/abs/2509.05878", "title": "MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries", "authors": ["François Grolleau", "Emily Alsentzer", "Timothy Keyes", "Philip Chung", "Akshay Swaminathan", "Asad Aali", "Jason Hom", "Tridu Huynh", "Thomas Lew", "April S. Liang", "Weihan Chu", "Natasha Z. Steele", "Christina F. Lin", "Jingkun Yang", "Kameron C. Black", "Stephen P. Ma", "Fateme N. Haredasht", "Nigam H. Shah", "Kevin Schulman", "Jonathan H. Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating factual accuracy in Large Language Model (LLM)-generated clinical\ntext is a critical barrier to adoption, as expert review is unscalable for the\ncontinuous quality assurance these systems require. We address this challenge\nwith two complementary contributions. First, we introduce MedFactEval, a\nframework for scalable, fact-grounded evaluation where clinicians define\nhigh-salience key facts and an \"LLM Jury\"--a multi-LLM majority vote--assesses\ntheir inclusion in generated summaries. Second, we present MedAgentBrief, a\nmodel-agnostic, multi-step workflow designed to generate high-quality, factual\ndischarge summaries. To validate our evaluation framework, we established a\ngold-standard reference using a seven-physician majority vote on\nclinician-defined key facts from inpatient cases. The MedFactEval LLM Jury\nachieved almost perfect agreement with this panel (Cohen's kappa=81%), a\nperformance statistically non-inferior to that of a single human expert\n(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework\n(MedFactEval) and a high-performing generation workflow (MedAgentBrief),\noffering a comprehensive approach to advance the responsible deployment of\ngenerative AI in clinical workflows.", "AI": {"tldr": "A study introducing MedFactEval and MedAgentBrief for evaluating factual accuracy in LLM-generated clinical text, aiming to improve AI adoption in healthcare.", "motivation": "To address the critical issue of evaluating factual accuracy in LLM-generated clinical text, which is essential for quality assurance and scalable deployment in healthcare settings.", "method": "The study introduces MedFactEval, a scalable evaluation framework using a multi-LLM majority vote, and MedAgentBrief, a model-agnostic workflow for generating factual discharge summaries.", "result": "The MedFactEval LLM Jury achieved almost perfect agreement with a panel of physicians, demonstrating its effectiveness with a Cohen's kappa of 81%, comparable to human experts.", "conclusion": "The paper presents a robust evaluation framework and workflow that enhances the responsible use of generative AI in clinical applications.", "key_contributions": ["Introduction of MedFactEval for scalable factual accuracy evaluation in clinical texts", "Development of MedAgentBrief for generating high-quality discharge summaries", "Validation of the LLM Jury against a physician panel, showing high levels of agreement."], "limitations": "", "keywords": ["Large Language Models", "Clinical Text Generation", "Factual Accuracy Evaluation", "Generative AI", "Healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13765", "pdf": "https://arxiv.org/pdf/2501.13765.pdf", "abs": "https://arxiv.org/abs/2501.13765", "title": "Understanding the Challenges of Maker Entrepreneurship", "authors": ["Natalie Friedman", "Alexandra Bremers", "Adelaide Nyanyo", "Ian Clark", "Yasmine Kotturi", "Laura Dabbish", "Wendy Ju", "Nikolas Martelaro"], "categories": ["cs.HC"], "comment": "29 pages, Accepted to PACMHCI (CSCW), CSCW198:29", "summary": "The maker movement embodies a resurgence in DIY creation, merging physical\ncraftsmanship and arts with digital technology support. However, mere\ntechnological skills and creativity are insufficient for economically and\npsychologically sustainable practice. By illuminating and smoothing the path\nfrom ``maker\" to ``maker entrepreneur,\" we can help broaden the viability of\nmaking as a livelihood. Our research centers on makers who design, produce, and\nsell physical goods. In this work, we explore the transition to\nentrepreneurship for these makers and how technology can facilitate this\ntransition online and offline. We present results from interviews with 20\nUSA-based maker entrepreneurs {(i.e., lamps, stickers)}, six creative service\nentrepreneurs {(i.e., photographers, fabrication)}, and seven support personnel\n(i.e., art curator, incubator director). Our findings reveal that many maker\nentrepreneurs 1) are makers first and entrepreneurs second; 2) struggle with\nbusiness logistics and learn business skills as they go; and 3) are motivated\nby non-monetary values. We discuss training and technology-based design\nimplications and opportunities for addressing challenges in developing\neconomically sustainable businesses around making.", "AI": {"tldr": "The research explores the transition from maker to maker entrepreneur, emphasizing the role of technology in facilitating this change and addressing the challenges faced by makers.", "motivation": "To broaden the viability of making as a sustainable livelihood and support makers transitioning to entrepreneurship.", "method": "Interviews with 20 maker entrepreneurs, six creative service entrepreneurs, and seven support personnel to gather qualitative insights on their experiences and challenges.", "result": "Many makers identify primarily as creators rather than entrepreneurs, face difficulties with business logistics, learn skills on-the-go, and are driven by values beyond monetary gain.", "conclusion": "Design implications for training and technology support can help develop economically sustainable businesses for makers.", "key_contributions": ["Identification of the challenges faced by maker entrepreneurs", "Insights into the non-monetary motivations of makers", "Recommendations for technology-based support in developing sustainable practices"], "limitations": "", "keywords": ["maker movement", "entrepreneurship", "technology support", "sustainability", "non-monetary values"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.05882", "pdf": "https://arxiv.org/pdf/2509.05882.pdf", "abs": "https://arxiv.org/abs/2509.05882", "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues", "authors": ["Abhijnan Nath", "Carine Graff", "Nikhil Krishnaswamy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes.", "AI": {"tldr": "This paper investigates how alignment methods for LLMs affect their effectiveness in multiturn, multiparty collaborations, emphasizing the role of friction agents to enhance group decision-making.", "motivation": "To ensure LLMs function as reliable collaborators in long-horizon multiparty interactions, it is crucial to validate their behavior and alignment over multiple turns in conversations.", "method": "The study employs a roleplay methodology to evaluate interventions from trained friction agents that prompt collaborative groups to reflect and deliberate more deeply during task conversations.", "result": "The friction-aware approach outperformed common alignment baselines, facilitating better convergence to common ground and improving the correctness of task outcomes.", "conclusion": "The findings indicate that implementing friction-aware interventions in LLM interactions can significantly enhance collaborative decision-making processes.", "key_contributions": ["Introduction of friction agents to LLM interactions", "Development of a counterfactual evaluation framework", "Demonstrated effectiveness of friction interventions over common alignment strategies"], "limitations": "", "keywords": ["Large Language Models", "friction agents", "group collaboration", "alignment methods", "decision-making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.05908", "pdf": "https://arxiv.org/pdf/2509.05908.pdf", "abs": "https://arxiv.org/abs/2509.05908", "title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling", "authors": ["Yue Gu", "Zhihao Du", "Ying Shi", "Shiliang Zhang", "Qian Chen", "Jiqing Han"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:\n  10.1109/TASLPRO.2025.3606198", "summary": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths.", "AI": {"tldr": "The paper introduces PSC-Joint, an approach that improves automatic speech recognition (ASR) by integrating the most relevant biasing information while reducing computational costs through a purification mechanism.", "motivation": "To enhance the effectiveness of cross-attention-based contextual ASR models in recognizing personalized biasing phrases, especially as the volume of biasing information increases.", "method": "The PSC-Joint approach identifies three levels of semantic correlation (list-level, phrase-level, and token-level) between ASR intermediate representations and biasing information, which are then jointly modeled.", "result": "PSC-Joint achieves average F1 score improvements of up to 21.34% on AISHELL-1 and 28.46% on KeSpeech with varying lengths of biasing lists.", "conclusion": "Integrating the most relevant biasing information reduces the effects of information volume variations in contextual ASR, leading to significant performance improvements.", "key_contributions": ["Introduction of the PSC-Joint approach for contextual ASR.", "Identification and modeling of multiple levels of semantic correlation.", "Development of a purification mechanism to enhance computational efficiency."], "limitations": "", "keywords": ["Automatic Speech Recognition", "Cross-attention", "Biasing Information", "Semantic Correlation", "Machine Learning"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2503.15512", "pdf": "https://arxiv.org/pdf/2503.15512.pdf", "abs": "https://arxiv.org/abs/2503.15512", "title": "Beyond SHAP and Anchors: A large-scale experiment on how developers struggle to design meaningful end-user explanations", "authors": ["Zahra Abba Omar", "Nadia Nahar", "Jacob Tjaden", "Inès M. Gilles", "Fikir Mekonnen", "Jane Hsieh", "Christian Kästner", "Alka Menon"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern machine learning produces models that are impossible for users or\ndevelopers to fully understand--raising concerns about trust, oversight,\nsafety, and human dignity when they are integrated into software products.\nTransparency and explainability methods aim to provide some help in\nunderstanding models, but it remains challenging for developers to design\nexplanations that are understandable to target users and effective for their\npurpose. Emerging guidelines and regulations set goals but may not provide\neffective actionable guidance to developers. In a large-scale experiment with\n124 participants, we explored how developers approach providing end-user\nexplanations, including what challenges they face, and to what extent specific\npolicies can guide their actions. We investigated whether and how specific\nforms of policy guidance help developers design explanations and provide\nevidence for policy compliance for an ML-powered screening tool for diabetic\nretinopathy. Participants across the board struggled to produce quality\nexplanations and comply with the provided policies. Contrary to our\nexpectations, we found that the nature and specificity of policy guidance had\nlittle effect. We posit that participant noncompliance is in part due to a\nfailure to imagine and anticipate the needs of non-technical stakeholders.\nDrawing on cognitive process theory and the sociological imagination to\ncontextualize participants' failure, we recommend educational interventions.", "AI": {"tldr": "The paper investigates challenges developers face in providing understandable explanations for machine learning models, particularly in healthcare applications, and assesses the effectiveness of policy guidance on their compliance and design.", "motivation": "Concerns regarding trust, oversight, safety, and human dignity arise due to the complexity of modern machine learning models, necessitating better explanations for end-users.", "method": "A large-scale experiment with 124 participants explored developers' approaches to creating end-user explanations and compliance with policy guidance for an ML-powered screening tool for diabetic retinopathy.", "result": "Participants struggled with generating quality explanations and complying with policies; specific forms of policy guidance had minimal effect on their ability to design effective explanations.", "conclusion": "The findings suggest developers' noncompliance is partly due to their inability to anticipate non-technical stakeholders' needs, leading to recommendations for educational interventions to improve understanding.", "key_contributions": ["Investigation into developer challenges in explanation generation for ML models.", "Assessment of policy guidance's effectiveness on explanation design and compliance.", "Recommendations for educational interventions based on cognitive process theory."], "limitations": "The study may not account for various developer backgrounds or technical levels outside the specific context of diabetic retinopathy screening tools.", "keywords": ["Machine Learning", "Explainability", "Policy Compliance", "Human-Computer Interaction", "Healthcare Applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.05915", "pdf": "https://arxiv.org/pdf/2509.05915.pdf", "abs": "https://arxiv.org/abs/2509.05915", "title": "Accelerating Large Language Model Inference via Early-Exiting Algorithms", "authors": ["Sangmin Bae"], "categories": ["cs.CL"], "comment": "PhD Dissertation", "summary": "Large language models have achieved remarkable capabilities, but their\npractical deployment is hindered by significant computational costs. While\nadaptive computation methods like early-exiting promise to reduce these costs,\nthey introduce a fundamental conflict: the per-token dynamism intended to save\ncomputation often creates system-level bottlenecks that can paradoxically\nreduce throughput in batched inference. This dissertation resolves this\nconflict by co-designing adaptive algorithms and model architectures to strike\nan optimal balance between dynamism and efficiency. To this end, our work first\naddresses critical sources of overhead in conventional early-exiting by\nproposing an efficient parallel decoding mechanism. We then show that deep\nparameter sharing provides an architectural foundation that not only yields\ncompact, parameter-efficient models but also inherently mitigates the critical\nsynchronization issues affecting dynamic inference. Finally, this work presents\na unified framework where lightweight routers are pretrained to dynamically\nassign an optimal recursion depth for each token. This approach establishes a\nnew Pareto frontier between efficiency and performance by effectively\noptimizing for both adaptive computation and parameter efficiency within a\nsingle model.", "AI": {"tldr": "This dissertation proposes a novel approach to enhance the efficiency of large language models by co-designing adaptive algorithms and model architectures to overcome computational bottlenecks in early-exiting mechanisms.", "motivation": "The deployment of large language models is hindered by high computational costs which can be alleviated through adaptive computation methods, but these methods introduce system-level bottlenecks that affect overall efficiency.", "method": "The dissertation develops an efficient parallel decoding mechanism and employs deep parameter sharing to create models that are both compact and efficient, addressing synchronization issues in dynamic inference.", "result": "The work presents a unified framework utilizing lightweight routers to optimally assign recursion depth for each token, achieving a new balance between efficiency and performance.", "conclusion": "By optimizing both adaptive computation and parameter efficiency within a single model, this dissertation establishes a new Pareto frontier that improves the practical deployment of large language models.", "key_contributions": ["Efficient parallel decoding mechanism for early-exiting", "Architectural innovation through deep parameter sharing", "Unified framework for dynamic recursion depth assignment"], "limitations": "", "keywords": ["large language models", "adaptive computation", "early-exiting"], "importance_score": 6, "read_time_minutes": 30}}
{"id": "2509.06065", "pdf": "https://arxiv.org/pdf/2509.06065.pdf", "abs": "https://arxiv.org/abs/2509.06065", "title": "KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino", "authors": ["Lorenzo Alfred Nery", "Ronald Dawson Catignas", "Thomas James Tiam-Lee"], "categories": ["cs.CL"], "comment": "14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of\n  NLPIR 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance across various\ntasks, but their tendency to produce hallucinations limits reliable adoption.\nBenchmarks such as TruthfulQA have been developed to measure truthfulness, yet\nthey are primarily available in English, leaving a gap in evaluating LLMs in\nlow-resource languages. To address this, we present KatotohananQA, a Filipino\ntranslation of the TruthfulQA benchmark. Seven free-tier proprietary models\nwere assessed using a binary-choice framework. Findings show a significant\nperformance gap between English and Filipino truthfulness, with newer OpenAI\nmodels (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.\nResults also reveal disparities across question characteristics, suggesting\nthat some question types, categories, and topics are less robust to\nmultilingual transfer which highlight the need for broader multilingual\nevaluation to ensure fairness and reliability in LLM usage.", "AI": {"tldr": "This paper introduces KatotohananQA, a Filipino adaptation of the TruthfulQA benchmark, highlighting performance disparities in LLM truthfulness between English and Filipino.", "motivation": "To address the lack of multilingual benchmarks for evaluating truthfulness in large language models, particularly in low-resource languages like Filipino.", "method": "Seven free-tier proprietary models were evaluated using a binary-choice framework to assess their performance on the KatotohananQA benchmark.", "result": "Significant performance gaps exist between English and Filipino in truthfulness, with newer OpenAI models showing strong multilingual capabilities.", "conclusion": "The study emphasizes the need for broader multilingual evaluation frameworks to ensure fairness and reliability in the application of LLMs across different languages.", "key_contributions": ["Introduction of KatotohananQA benchmark for Filipino language", "Demonstration of performance gaps in LLMs between English and Filipino", "Identification of robustness disparities across various question characteristics"], "limitations": "The study is limited to seven proprietary models and focuses on binary-choice evaluations, which may not capture nuanced performance aspects.", "keywords": ["Large Language Models", "Truthfulness Evaluation", "Low-resource Languages", "Multilingual NLP", "Filipino Language"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.06074", "pdf": "https://arxiv.org/pdf/2509.06074.pdf", "abs": "https://arxiv.org/abs/2509.06074", "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis", "authors": ["Zhenqi Jia", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS.", "AI": {"tldr": "MFCIG-CSS is a conversational speech synthesis system that uses multimodal fine-grained interaction graphs to improve natural prosody by modeling dialogue history.", "motivation": "To generate speech with natural prosody by effectively modeling multimodal dialogue history, addressing gaps in existing methods that ignore fine-grained interactions.", "method": "MFCIG-CSS constructs two specialized graphs: a semantic interaction graph and a prosody interaction graph to encode word-level semantics and prosody interactions.", "result": "MFCIG-CSS outperforms all baseline models on the DailyTalk dataset in terms of prosodic expressiveness.", "conclusion": "The proposed method enhances synthesized speech by utilizing fine-grained multimodal interactions, significantly improving naturalness and expressiveness of Conversational Speech Synthesis.", "key_contributions": ["Introduction of two specialized multimodal interaction graphs for CSS", "Demonstrated significant improvements over baseline models", "Advanced the understanding of multimodal dialogue history interactions"], "limitations": "", "keywords": ["Conversational Speech Synthesis", "Multimodal Dialogue", "Prosody Modeling"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2509.06079", "pdf": "https://arxiv.org/pdf/2509.06079.pdf", "abs": "https://arxiv.org/abs/2509.06079", "title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge", "authors": ["Hao Liang", "Ruitao Wu", "Bohan Zeng", "Junbo Niu", "Wentao Zhang", "Bin Dong"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.", "AI": {"tldr": "Introducing a caption-assisted reasoning framework that improves multimodal reasoning in AI by bridging visual and textual modalities with notable performance in competitions.", "motivation": "Multimodal reasoning is a significant challenge in AI, particularly for text-based models like GPT-o3 that struggle with performance in multimodal contexts.", "method": "We propose a caption-assisted reasoning framework that integrates visual and textual modalities to enhance reasoning capabilities.", "result": "The framework achieved 1st place in the ICML 2025 AI for Math Workshop & Challenge 2: SeePhys, and demonstrated generalization on the MathVerse benchmark for geometric reasoning.", "conclusion": "Our method showcases effectiveness and robustness in multimodal reasoning tasks, with publicly available code for further research.", "key_contributions": ["Development of a caption-assisted reasoning framework", "Demonstrated effectiveness in competition", "Validated generalization on MathVerse benchmark"], "limitations": "", "keywords": ["multimodal reasoning", "AI", "caption-assisted reasoning", "geometric reasoning", "MathVerse"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.06100", "pdf": "https://arxiv.org/pdf/2509.06100.pdf", "abs": "https://arxiv.org/abs/2509.06100", "title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models", "authors": ["Kefan Cao", "Shuaicheng Wu"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting.", "AI": {"tldr": "Presentation of OLieRA, a method for fine-tuning large language models that incorporates Lie group theory to preserve parameter geometry and achieve better task performance.", "motivation": "To address catastrophic forgetting in language models during multi-task learning by preserving the intrinsic geometric structure of parameters while ensuring low-rank orthogonality.", "method": "OLieRA utilizes Lie group theory and multiplicative updates to maintain the geometric structure of LLM parameters while enforcing orthogonality constraints in task subspaces.", "result": "OLieRA achieves state-of-the-art results on the Standard CL benchmark, demonstrating superior performance in multi-task settings compared to existing methods.", "conclusion": "The proposed method significantly improves the handling of catastrophic forgetting in LLMs, combining geometric parameter structure preservation with orthogonality in task subspaces.", "key_contributions": ["Introduction of Orthogonal Low-rank Adaptation in Lie Groups (OLieRA) for LLM fine-tuning", "Preservation of intrinsic geometric structure of parameters", "Empirical evidence of improved performance on benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Fine-tuning", "Geometric Structure", "Orthogonality", "Multi-task Learning"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2509.06164", "pdf": "https://arxiv.org/pdf/2509.06164.pdf", "abs": "https://arxiv.org/abs/2509.06164", "title": "Benchmarking Gender and Political Bias in Large Language Models", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts.", "AI": {"tldr": "EuroParlVote is a benchmark for evaluating LLMs in political contexts, linking speeches to vote outcomes and revealing biases against female MEPs and far-left/far-right groups.", "motivation": "To address bias in LLMs when analyzing politically sensitive data, especially in the context of European Parliament debates.", "method": "The EuroParlVote benchmark was created, which links European Parliament speeches to roll-call votes and includes demographic information about MEPs.", "result": "LLMs showed biases, misclassifying female MEPs and underperforming in predicting votes from far-left and far-right groups. Proprietary models outperformed open-weight models in robustness and fairness.", "conclusion": "The released EuroParlVote dataset and tools will aid future research on fairness and accountability in NLP, highlighting the biases present in LLMs.", "key_contributions": ["Introduction of a novel benchmark (EuroParlVote) for evaluating LLMs in political contexts", "Demonstration of bias in LLMs regarding gender and political alignment", "Release of dataset and tools for further research on NLP fairness"], "limitations": "The study focuses on European Parliament data, which may not generalize to other political contexts.", "keywords": ["large language models", "political bias", "fairness in NLP", "EuroParlVote", "gender classification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06184", "pdf": "https://arxiv.org/pdf/2509.06184.pdf", "abs": "https://arxiv.org/abs/2509.06184", "title": "Understanding the Influence of Synthetic Data for Text Embedders", "authors": ["Jacob Mitchell Springer", "Vaibhav Adlakha", "Siva Reddy", "Aditi Raghunathan", "Marius Mosbach"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks.", "AI": {"tldr": "The paper explores the impact of synthetic LLM-generated data on model generalization, releasing a high-quality dataset and revealing limited benefits and trade-offs in performance across tasks.", "motivation": "To investigate the role of synthetic LLM-generated data in improving the generalization of text embedders and to provide a publicly available dataset for further analysis.", "method": "The authors reproduce and release a synthetic dataset and analyze its effects on model performance across different datasets and tasks.", "result": "The synthetic data leads to consistent performance improvements but these benefits are sparse and localized, with observed trade-offs between tasks.", "conclusion": "Current synthetic data approaches have limitations for building robust general-purpose embedders, challenging assumptions about their effectiveness across various tasks.", "key_contributions": ["Public release of a synthetic dataset for text embeddings.", "Analysis of the impact of synthetic data on model generalization.", "Identification of trade-offs in performance across different tasks."], "limitations": "Benefits of synthetic data are localized and may degrade performance on some tasks.", "keywords": ["synthetic data", "text embeddings", "model generalization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.06196", "pdf": "https://arxiv.org/pdf/2509.06196.pdf", "abs": "https://arxiv.org/abs/2509.06196", "title": "Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation", "authors": ["Mohamed T. Younes", "Omar Walid", "Khaled Shaban", "Ali Hamdi", "Mai Hassan"], "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "This paper presents a novel approach to recruitment automation. Large\nLanguage Models (LLMs) were fine-tuned to improve accuracy and efficiency.\nBuilding upon our previous work on the Multilayer Large Language Model-Based\nRobotic Process Automation Applicant Tracking (MLAR) system . This work\nintroduces a novel methodology. Training fine-tuned LLMs specifically tuned for\nrecruitment tasks. The proposed framework addresses the limitations of generic\nLLMs by creating a synthetic dataset that uses a standardized JSON format. This\nhelps ensure consistency and scalability. In addition to the synthetic data\nset, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes\nwere parsed into the same structured JSON format and placed in the training\nset. This will help improve data diversity and realism. Through\nexperimentation, we demonstrate significant improvements in performance\nmetrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall\nsimilarity compared to base models and other state-of-the-art LLMs. In\nparticular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,\nindicating exceptional precision and recall in recruitment tasks. This study\nhighlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize\nrecruitment workflows by providing more accurate candidate-job matching.", "AI": {"tldr": "A new approach to automate recruitment using fine-tuned Large Language Models (LLMs) for improved accuracy in candidate-job matching.", "motivation": "To enhance the recruitment process by addressing the limitations of generic LLMs and providing a structured methodology for effective candidate-job matching.", "method": "The study creates a synthetic dataset using a standardized JSON format, parses resumes with a high-parameter LLM, and trains fine-tuned LLMs specifically for recruitment tasks.", "result": "The fine-tuned Phi-4 model achieved an F1 score of 90.62%, demonstrating significant improvements in performance metrics over base models and other state-of-the-art LLMs.", "conclusion": "Fine-tuned LLMs have the potential to revolutionize recruitment workflows by ensuring better candidate-job matching and improving overall recruitment efficiency.", "key_contributions": ["Introduction of a synthetic dataset for recruitment automation", "Development of a structured JSON format for data consistency", "Demonstration of enhanced performance metrics in recruitment tasks"], "limitations": "", "keywords": ["recruitment automation", "large language models", "synthetic dataset"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.06200", "pdf": "https://arxiv.org/pdf/2509.06200.pdf", "abs": "https://arxiv.org/abs/2509.06200", "title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment", "authors": ["Omar Walid", "Mohamed T. Younes", "Khaled Shaban", "Mai Hassan", "Ali Hamdi"], "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation.", "AI": {"tldr": "This paper introduces MSLEF, a framework that enhances resume parsing in recruitment through multi-segment ensemble learning and LLM fine-tuning.", "motivation": "To improve the accuracy of resume parsing in recruitment automation by using a more adaptable and specialized approach.", "method": "MSLEF employs a segment-aware architecture that integrates multiple fine-tuned LLMs, each focused on specific resume segments, utilizing weighted voting for enhanced accuracy.", "result": "MSLEF demonstrates significant performance improvements over traditional single-model systems, achieving better metrics in Exact Match, F1 score, BLEU, ROUGE, and Recruitment Similarity, with a notable +7% enhancement in RS compared to the best single model.", "conclusion": "The framework's adaptability to varied resume formats and its enhanced generalization capabilities make it a promising solution for real-world hiring applications.", "key_contributions": ["Introduction of multi-segment ensemble learning for resume parsing", "Development of segment-aware architecture for tailored resume analysis", "Improvement of recruitment metrics by utilizing specialized LLMs"], "limitations": "", "keywords": ["resume parsing", "LLM fine-tuning", "recruitment automation", "ensemble learning", "segment-aware architecture"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2501.09751", "pdf": "https://arxiv.org/pdf/2501.09751.pdf", "abs": "https://arxiv.org/abs/2501.09751", "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking", "authors": ["Zekun Xi", "Wenbiao Yin", "Jizhan Fang", "Jialong Wu", "Runnan Fang", "Jiang Yong", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": "EMNLP 2025", "summary": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles. Code is available at\nhttps://github.com/zjunlp/OmniThink.", "AI": {"tldr": "OmniThink is a machine writing framework that enhances the depth and novelty of content generated by large language models through a human-like process of iterative expansion.", "motivation": "Current machine writing methods often lead to shallow and repetitive outputs due to limitations in predefined model scopes and the quality of retrieved information.", "method": "OmniThink simulates the cognitive behavior of learners, allowing for slow and reflective writing, which deepens knowledge over time.", "result": "Experimental results show that OmniThink increases the knowledge density of generated articles while maintaining coherence and depth.", "conclusion": "OmniThink demonstrates significant potential in producing high-quality, long-form articles, as confirmed by human evaluations and expert feedback.", "key_contributions": ["Introduction of a slow-thinking framework for machine writing", "Enhancement of knowledge density in generated content", "Demonstration of human-like iterative expansion in content generation"], "limitations": "", "keywords": ["machine writing", "large language models", "iterative expansion", "knowledge density", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.06277", "pdf": "https://arxiv.org/pdf/2509.06277.pdf", "abs": "https://arxiv.org/abs/2509.06277", "title": "No Encore: Unlearning as Opt-Out in Music Generation", "authors": ["Jinju Kim", "Taehan Kim", "Abdul Waheed", "Rita Singh"], "categories": ["cs.CL"], "comment": "Work in progress. 7 pages", "summary": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models.", "AI": {"tldr": "This paper discusses the application of machine unlearning techniques to AI music generation to mitigate risks of copyright exploitation.", "motivation": "To address ethical and legal concerns related to copyright exploitation in AI-generated music.", "method": "We explore existing machine unlearning methods applied to a pre-trained Text-to-Music (TTM) model, assessing their effectiveness in unlearning datasets while maintaining model performance.", "result": "Our experiments reveal the challenges of integrating unlearning in music generation while providing foundational insights for future research.", "conclusion": "The study presents preliminary results that lay the groundwork for further investigation into unlearning techniques for music generative models.", "key_contributions": ["First application of machine unlearning in music generation", "Insights into challenges faced in applying unlearning techniques", "Foundational analysis for future works in music generative models"], "limitations": "Work in progress; results are preliminary and may evolve with further research.", "keywords": ["AI music generation", "machine unlearning", "Text-to-Music", "creativity", "copyright issues"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.06350", "pdf": "https://arxiv.org/pdf/2509.06350.pdf", "abs": "https://arxiv.org/abs/2509.06350", "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?", "authors": ["Junjie Mu", "Zonghao Ying", "Zhekui Fan", "Zonglei Jing", "Yaoyuan Zhang", "Zhengmin Yu", "Wenxin Zhang", "Quanchen Zou", "Xiangzheng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks.", "AI": {"tldr": "Introducing Mask-GCG, a method that reduces token redundancy in jailbreak attacks on LLMs by employing learnable token masking.", "motivation": "To improve the efficiency and effectiveness of jailbreak attacks on LLMs by reducing redundancy in suffixes used in prompts.", "method": "Mask-GCG employs learnable token masking to identify and prune low-impact tokens in the suffix of prompts, enhancing computational efficiency and attack success.", "result": "Experimental evaluation shows that pruning low-impact tokens does not affect attack success rates, revealing significant redundancy in LLM prompts.", "conclusion": "Mask-GCG effectively shortens the necessary time to conduct successful jailbreak attacks while offering insights into improving LLM design.", "key_contributions": ["Introduction of Mask-GCG for token pruning in jailbreak attacks.", "Demonstration of token redundancy in existing methods.", "Enhanced efficiency and reduced computational overhead in executing attacks."], "limitations": "The approach primarily focuses on suffix token manipulation and may not address all types of vulnerabilities in LLMs.", "keywords": ["jailbreak attacks", "Large Language Models", "token masking", "efficiency", "redundancy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.06356", "pdf": "https://arxiv.org/pdf/2509.06356.pdf", "abs": "https://arxiv.org/abs/2509.06356", "title": "PL-CA: A Parametric Legal Case Augmentation Framework", "authors": ["Ao Chang", "Yubo Chen", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix.", "AI": {"tldr": "This paper proposes PL-CA, a parametric RAG framework that addresses the limitations of conventional RAG in legal applications by encoding legal knowledge into parametric vectors and integrating them into LLMs, while also providing a multi-task legal dataset with expert annotations.", "motivation": "The motivation behind this work is to overcome the limitations of conventional RAG methods that lead to knowledge insufficiency and performance degradation in legal tasks due to context constraints and inadequate benchmarks.", "method": "This study introduces a parametric RAG framework (P-RAG) that augments corpus knowledge into parametric vectors and integrates this knowledge into the LLM's feed-forward networks using LoRA, thus reducing context load and improving model performance on legal tasks.", "result": "The proposed method demonstrates a significant reduction in overhead from long contexts while maintaining competitive performance compared to traditional RAG approaches, evidenced by experiments conducted on a newly constructed legal dataset.", "conclusion": "The findings indicate that PL-CA effectively addresses the challenges of knowledge integration and context length, making it a valuable advancement for legal task processing with LLMs.", "key_contributions": ["Introduction of a parametric RAG framework for data augmentation.", "Development of a multi-task legal dataset with expert annotations.", "Demonstrated ability to maintain model performance while reducing long context overhead."], "limitations": "", "keywords": ["parametric RAG", "multi-task legal dataset", "LLM integration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.06401", "pdf": "https://arxiv.org/pdf/2509.06401.pdf", "abs": "https://arxiv.org/abs/2509.06401", "title": "Do LLMs exhibit the same commonsense capabilities across languages?", "authors": ["Ivan Martínez-Murillo", "Elena Lloret", "Paloma Moreda", "Albert Gatt"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the multilingual commonsense generation abilities of\nLarge Language Models (LLMs). To facilitate this investigation, we introduce\nMULTICOM, a novel benchmark that extends the COCOTEROS dataset to four\nlanguages: English, Spanish, Dutch, and Valencian. The task involves generating\na commonsensical sentence that includes a given triplet of words. We evaluate a\nrange of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and\nSalamandra, on this benchmark. Our evaluation combines automatic metrics,\nLLM-as-a-judge approaches (using Prometheus and JudgeLM), and human\nannotations. Results consistently show superior performance in English, with\nsignificantly lower performance in less-resourced languages. While contextual\nsupport yields mixed results, it tends to benefit underrepresented languages.\nThese findings underscore the current limitations of LLMs in multilingual\ncommonsense generation. The dataset is publicly available at\nhttps://huggingface.co/datasets/gplsi/MULTICOM.", "AI": {"tldr": "This paper presents MULTICOM, a benchmark for evaluating multilingual commonsense generation in Large Language Models across four languages.", "motivation": "To investigate the multilingual commonsense generation capabilities of Large Language Models (LLMs) and to highlight their limitations in less-resourced languages.", "method": "The study evaluates open-source LLMs such as LLaMA, Qwen, Gemma, EuroLLM, and Salamandra using the MULTICOM benchmark, which requires generating commonsensical sentences from triplets of words. Evaluation methods include automatic metrics, LLM-as-a-judge approaches, and human annotations.", "result": "Results indicate that LLMs perform better in English compared to less-resourced languages like Valencian and Dutch, revealing significant performance gaps that highlight the need for improvement in multilingual applications.", "conclusion": "The findings point to the limitations of LLMs in generating commonsense knowledge in multilingual contexts, suggesting that while contextual support may help, it does not fully bridge the performance gap.", "key_contributions": ["Introduction of the MULTICOM benchmark for multilingual commonsense generation.", "Evaluation of various open-source LLMs across multiple languages.", "Insights into the performance disparities among languages in commonsense generation tasks."], "limitations": "The study primarily focuses on four languages and may not represent the capabilities or limitations of LLMs in other languages or dialects.", "keywords": ["multilingual commonsense generation", "Large Language Models", "MULTICOM benchmark", "commonsense knowledge", "underrepresented languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.06501", "pdf": "https://arxiv.org/pdf/2509.06501.pdf", "abs": "https://arxiv.org/abs/2509.06501", "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "authors": ["Junteng Liu", "Yunji Li", "Chi Zhang", "Jingyang Li", "Aili Chen", "Ke Ji", "Weiyu Cheng", "Zijia Wu", "Chengyu Du", "Qidi Xu", "Jiayuan Song", "Zhengmao Zhu", "Wenhu Chen", "Pengyu Zhao", "Junxian He"], "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.", "AI": {"tldr": "WebExplorer introduces a systematic data generation approach for enhancing web agent capabilities in complex information-seeking tasks, achieving state-of-the-art performance with an 8B-sized model.", "motivation": "There is a need for improved web agents that can effectively perform complex information-seeking tasks and handle challenging data queries.", "method": "The paper presents a model-based exploration and iterative query evolution approach for generating challenging query-answer pairs, resulting in the development of the WebExplorer-8B model through supervised fine-tuning and reinforcement learning.", "result": "WebExplorer-8B demonstrates state-of-the-art performance in various information-seeking benchmarks, outperforming even larger models in specific tasks.", "conclusion": "The study presents a viable solution for enhancing web agent performance in long-horizon problem solving and establishes a path for future research in this domain.", "key_contributions": ["Introduction of the WebExplorer data generation method for complex queries", "Development of an 8B-sized model with superior performance", "Demonstration of effective long-horizon web agent capabilities"], "limitations": "", "keywords": ["Large Language Models", "web agents", "data generation", "information-seeking", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.06518", "pdf": "https://arxiv.org/pdf/2509.06518.pdf", "abs": "https://arxiv.org/abs/2509.06518", "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "authors": ["Andrei Baroian", "Kasper Notebomer"], "categories": ["cs.CL", "cs.AI"], "comment": "The reported results are skewed due to a data type mismatch. The\n  dataset was saved with int32, but the data loader interpreted it as uint16.\n  As a result, each 32-bit token was incorrectly split into two 16-bit tokens.\n  Outcome: a consistent artifact where every other token is zero", "summary": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.", "AI": {"tldr": "This paper introduces three new variants of Layer-Wise Scaling (LWS) for transformer-based models that optimize layer sizes and attention heads, demonstrating improved performance compared to isotropic baselines.", "motivation": "To address the limitations of uniform (isotropic) layer sizes in transformer-based language models, recognizing the varying functional roles and computational needs of different model depths.", "method": "The authors developed three LWS variants—Framed, Reverse, and Crown—that utilize two or three-point linear interpolation to redistribute feedforward neural network (FFN) widths and attention heads during pre-training.", "result": "The proposed models achieved similar losses and better performance than an isotropic baseline while maintaining training throughput, within a fixed parameter budget of 180M on a dataset of 5B tokens.", "conclusion": "This research marks a preliminary exploration into layer-wise architectures for model pre-training, suggesting that further scaling experiments with more data and parameters are necessary to evaluate the full potential of these architectures.", "key_contributions": ["Introduction of new LWS variants (Framed, Reverse, Crown) for transformer models", "Systematic ablation study of LWS indicating improved performance over isotropic models", "Optimization of transformer layer sizes based on functional roles rather than uniformity"], "limitations": "Results may be skewed due to a data type mismatch, where tokens were incorrectly interpreted, potentially affecting the reported outcomes.", "keywords": ["Layer-Wise Scaling", "Transformer Models", "Ablation Study", "Pre-training"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.06524", "pdf": "https://arxiv.org/pdf/2509.06524.pdf", "abs": "https://arxiv.org/abs/2509.06524", "title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection", "authors": ["Jian Wu", "Hang Yu", "Bingchang Liu", "Wenjie Yang", "Peng Di", "Jianguo Li", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines.", "AI": {"tldr": "LAMDAS is a novel method for domain-specific data selection utilizing LLMs as implicit classifiers to improve data quality and training efficiency.", "motivation": "The scarcity of high-quality, human-curated data for fine-tuning large language models poses a critical bottleneck in adapting them to specific domains.", "method": "LAMDAS utilizes the pre-trained large language model itself as an implicit classifier for identifying domain-specific data, reframing the data selection problem as a one-class classification task.", "result": "Extensive experiments show that LAMDAS surpasses full-data training performance using significantly less data and outperforms nine state-of-the-art baselines across various scenarios.", "conclusion": "LAMDAS offers a compelling balance between performance improvements and computational efficiency in data selection for domain-specific applications.", "key_contributions": ["Introduction of LAMDAS for implicit classification of data selection", "Significant performance gains using reduced data", "Effective balance of performance and computational efficiency"], "limitations": "", "keywords": ["large language models", "data selection", "one-class classification"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.06531", "pdf": "https://arxiv.org/pdf/2509.06531.pdf", "abs": "https://arxiv.org/abs/2509.06531", "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion", "authors": ["Mengxue Yang", "Chun Yang", "Jiaqi Zhu", "Jiafan Li", "Jingqi Zhang", "Yuyang Li", "Ying Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP Findings 2025", "summary": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.", "AI": {"tldr": "SLiNT is a modular framework that integrates structural information into large language models for improved link prediction in knowledge graphs.", "motivation": "To improve link prediction by effectively utilizing structural information and reducing semantic ambiguity in knowledge graphs, especially under challenging conditions.", "method": "SLiNT employs Structure-Guided Neighborhood Enhancement to enrich sparse entities, Dynamic Hard Contrastive Learning for fine-grained supervision, and Gradient-Decoupled Dual Injection for token-level interventions while maintaining the integrity of the LLM parameters.", "result": "SLiNT outperforms or is competitive with existing embedding-based and generation-based methods in link prediction tasks on WN18RR and FB15k-237 datasets.", "conclusion": "The proposed structure-aware representation learning approach significantly enhances knowledge graph completion capabilities.", "key_contributions": ["Introduction of SLiNT, a novel framework for link prediction in knowledge graphs", "Incorporation of Structure-Guided Neighborhood Enhancement for enriching sparse data", "Use of Dynamic Hard Contrastive Learning to fine-tune entity recognition accuracy"], "limitations": "", "keywords": ["Link Prediction", "Knowledge Graphs", "Large Language Models", "Contrastive Learning", "Structure-Aware Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.06596", "pdf": "https://arxiv.org/pdf/2509.06596.pdf", "abs": "https://arxiv.org/abs/2509.06596", "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models", "authors": ["Xin Tong", "Zhi Lin", "Jingya Wang", "Bo Jin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.", "AI": {"tldr": "HAVE is a parameter-free framework that reduces hallucinations in Large Language Models (LLMs) by implementing head-adaptive gating and value calibration to improve attention mechanisms, yielding more reliable outputs without needing finetuning.", "motivation": "This paper addresses two key challenges in LLMs: the inadequacy of head importance as input-agnostic and the poor representation of token contributions through raw attention weights, which lead to hallucinations in generated content.", "method": "The HAVE framework introduces head-adaptive gating for instance-level soft reweighing of attention heads and value calibration that enhances attention with the contribution magnitude of value vectors, operating without finetuning in one forward pass.", "result": "Experiments show that HAVE reduces hallucinations and outperforms strong baselines like DAGCD across various QA benchmarks, demonstrating its effectiveness and efficiency.", "conclusion": "HAVE provides a transparent and reproducible approach for improving the reliability of LLM outputs in real-world applications, easily integrating with existing models.", "key_contributions": ["Development of the HAVE framework for LLMs", "Introduction of head-adaptive gating and value calibration", "Demonstrated improvements on multiple QA benchmarks"], "limitations": "", "keywords": ["Large Language Models", "hallucinations", "attention mechanisms", "parameter-free", "trustworthy generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.06631", "pdf": "https://arxiv.org/pdf/2509.06631.pdf", "abs": "https://arxiv.org/abs/2509.06631", "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation", "authors": ["Özgür Uğur", "Musa Yılmaz", "Esra Şavirdi", "Özay Ezerceli", "Mahmut El Huseyni", "Selva Taş", "Reyhan Bayraktar"], "categories": ["cs.CL"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.", "AI": {"tldr": "This study explores guided decoding in Retrieval-Augmented Generation (RAG) systems, evaluating methods for structured output generation.", "motivation": "To address the challenges of structured and reliable responses in RAG systems, particularly with large language models, and to minimize hallucinations in outputs.", "method": "The research compares three guided decoding methods—Outlines, XGrammar, and LM Format Enforcer—across various multi-turn prompting setups (0-turn, 1-turn, and 2-turn).", "result": "The study provides insights into success rates, hallucination rates, and output quality of the different methods, showing how multi-turn interactions affect guided decoding.", "conclusion": "The findings highlight unexpected performance variations in guided decoding methods, aiding in the selection for specific use cases and enhancing the understanding of structured output generation.", "key_contributions": ["Comparison of three guided decoding methods in RAG systems", "Insights on the impact of multi-turn interactions on output quality", "Theoretical and practical guidance for LLM deployment"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "guided decoding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.06637", "pdf": "https://arxiv.org/pdf/2509.06637.pdf", "abs": "https://arxiv.org/abs/2509.06637", "title": "Modelling Intertextuality with N-gram Embeddings", "authors": ["Yi Xing"], "categories": ["cs.CL"], "comment": null, "summary": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships.", "AI": {"tldr": "This paper introduces a quantitative model for analyzing intertextuality in literary texts through pairwise comparisons of n-gram embeddings.", "motivation": "To create a scalable method for analyzing intertextuality and provide network-based insights into literary relationships.", "method": "The proposed model performs pairwise comparisons of the embeddings of n-grams from two texts and averages the results to quantify intertextuality.", "result": "Validation with known intertextuality levels on four texts and a scalability test on 267 texts showed the method's effectiveness and efficiency.", "conclusion": "The network analysis reveals centrality and community structures, successfully capturing intertextual relationships.", "key_contributions": ["Introduction of a quantitative model for intertextuality", "Demonstration of scalability with a large dataset", "Identification of centrality and community structures in intertextual networks"], "limitations": "", "keywords": ["intertextuality", "text analysis", "network analysis", "literary studies", "n-gram embeddings"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2509.06650", "pdf": "https://arxiv.org/pdf/2509.06650.pdf", "abs": "https://arxiv.org/abs/2509.06650", "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval", "authors": ["Hao Lin", "Peitong Xie", "Jingxue Chen", "Jie Lin", "Qingkun Tang", "Qianchun Lu"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.", "AI": {"tldr": "MoLER is a domain-aware RAG method using Mixture of Losses and reinforcement learning for optimized retrieval performance.", "motivation": "To improve coarse-ranking in retrieval-augmented generation (RAG) systems which struggle with balancing domain-specific knowledge and query enhancement.", "method": "MoLER employs a two-stage pipeline: continual pre-training with Mixture of Losses (MoL) followed by reinforcement learning using Group Relative Policy Optimization (GRPO). It incorporates a Multi-query Single-passage Late Fusion strategy to reduce computational costs during training.", "result": "Extensive experiments demonstrate that MoLER achieves state-of-the-art performance, outperforming baseline methods and improving retrieval in specialized domains.", "conclusion": "MoLER effectively bridges the knowledge gap in RAG systems, allowing for robust and scalable retrieval solutions.", "key_contributions": ["Introduction of MoLER for optimized retrieval in RAG systems", "Use of Mixture of Losses for balancing knowledge and language capabilities", "Multi-query Single-passage Late Fusion strategy to enhance efficiency"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Reinforcement Learning", "Domain-specific Knowledge"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.06652", "pdf": "https://arxiv.org/pdf/2509.06652.pdf", "abs": "https://arxiv.org/abs/2509.06652", "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations", "authors": ["Xingwei Tan", "Mahathi Parvatham", "Chiara Gambi", "Gabriele Pergola"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings camera-ready, 9+7 pages", "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.", "AI": {"tldr": "The paper presents IntrEx, a dataset for analyzing interestingness in teacher-student conversations to enhance second-language acquisition.", "motivation": "To address the gap in understanding the linguistic features that drive engagement in educational conversations.", "method": "A large dataset annotated for interestingness was created using a rigorous comparison-based rating approach with over 100 second-language learners, allowing for analysis of interest evolution in dialogues.", "result": "LLMs fine-tuned on interestingness ratings perform better than larger models like GPT-4o, indicating the value of specialized datasets for modeling educational engagement.", "conclusion": "The findings suggest that specific linguistic and cognitive factors significantly influence engagement in educational dialogues, with implications for improving second-language acquisition strategies.", "key_contributions": ["Introduction of the IntrEx dataset for interestingness in educational dialogues.", "Demonstrated effectiveness of fine-tuning LLMs on interestingness ratings.", "Analysis of linguistic and cognitive factors impacting engagement."], "limitations": "", "keywords": ["second-language acquisition", "interestingness", "teacher-student interactions", "large language models", "engagement"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.06675", "pdf": "https://arxiv.org/pdf/2509.06675.pdf", "abs": "https://arxiv.org/abs/2509.06675", "title": "ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data", "authors": ["Vladislav Stankov", "Matyáš Kopp", "Ondřej Bojar"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0\ncorpus, targeted at speech modeling tasks with the largest variant containing\n2,695 hours. We combined the sound recordings of the Czech parliamentary\nspeeches with the official transcripts. The recordings were processed with\nWhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our\nprocessing pipeline improves upon the ParCzech 3.0 speech recognition version\nby extracting more data with higher alignment reliability. The dataset is\noffered in three flexible variants: (1) sentence-segmented for automatic speech\nrecognition and speech synthesis tasks with clean boundaries, (2) unsegmented\npreserving original utterance flow across sentences, and (3) a raw-alignment\nfor further custom refinement for other possible tasks. All variants maintain\nthe original metadata and are released under a permissive CC-BY license. The\ndataset is available in the LINDAT repository, with the sentence-segmented and\nunsegmented variants additionally available on Hugging Face.", "AI": {"tldr": "ParCzech4Speech 1.0 is a processed speech corpus designed for better speech modeling with improved audio-text alignment.", "motivation": "The goal is to provide a more reliable resource for speech recognition and synthesis tasks by enhancing the dataset extracted from Czech parliamentary speeches.", "method": "The corpus was processed using WhisperX and Wav2Vec 2.0 to align audio recordings with their corresponding transcripts.", "result": "The new version includes 2,695 hours of speech data with better alignment reliability, offered in three variants for different tasks.", "conclusion": "The dataset aims to facilitate advancements in speech recognition technologies and is accessible under a CC-BY license for public use.", "key_contributions": ["Introduction of three processing variants (segmented, unsegmented, raw-alignment) for flexible use cases.", "Improved audio-text alignment using advanced processing techniques.", "Public availability of the dataset enhances research opportunities in speech modeling."], "limitations": "", "keywords": ["speech modeling", "speech recognition", "Czech language", "audio-text alignment", "speech corpus"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.06704", "pdf": "https://arxiv.org/pdf/2509.06704.pdf", "abs": "https://arxiv.org/abs/2509.06704", "title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments", "authors": ["Amir Homayounirad", "Enrico Liscio", "Tong Wang", "Catholijn M. Jonker", "Luciano C. Siebert"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process.", "AI": {"tldr": "This paper investigates methods for recognizing subjectivity in human values that influence arguments, aiming to enhance model performance in identifying subjective arguments.", "motivation": "Understanding and identifying subjectivity in arguments is crucial, especially in tasks where human values lead to diverse interpretations and potential disagreements among annotators.", "method": "The authors evaluate two primary approaches: inferring subjectivity through value prediction and directly identifying subjectivity, utilizing experiments to assess efficacy.", "result": "Direct subjectivity identification is shown to significantly enhance the model's ability to flag subjective arguments, while combining contrastive loss with binary cross-entropy loss does not yield performance improvement but reduces reliance on per-label subjectivity.", "conclusion": "The proposed methods can enrich the annotation process by allowing for nuanced interpretations of subjective arguments, ultimately contributing to better understanding of annotator disagreement.", "key_contributions": ["Introduction of direct subjectivity identification methods", "Comparison between inferring subjectivity and direct identification", "Insights on loss function performance related to subjectivity"], "limitations": "", "keywords": ["subjectivity", "human values", "annotation", "model performance", "argumentation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.06795", "pdf": "https://arxiv.org/pdf/2509.06795.pdf", "abs": "https://arxiv.org/abs/2509.06795", "title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Qika Lin", "Kai He", "Ting Liu", "Bing Qin", "Mengling Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch.", "AI": {"tldr": "This paper introduces ProCon, a method to mitigate safety risks in LLMs during Instruction Fine-Tuning by addressing the drift of the refusal direction in hidden states.", "motivation": "To address the safety risks of LLMs during Instruction Fine-Tuning, particularly their compromised ability to refuse malicious instructions.", "method": "The ProCon method employs a projection-constrained loss term to regularize hidden states in the refusal direction, with an emphasis on early training stages and broadening data distribution to enhance constraint signals.", "result": "Experimental results show that ProCon effectively reduces safety risks associated with Instruction Fine-Tuning while preserving performance across various datasets and models.", "conclusion": "ProCon stabilizes the refusal direction during training, providing insights for future safety research in LLMs.", "key_contributions": ["Introduction of ProCon method to stabilize refusal direction", "Demonstrated effectiveness in mitigating safety risks during IFT", "Proposed warm-up strategy for enhanced performance"], "limitations": "Overall performance barriers remain a limiting factor for the method.", "keywords": ["Instruction Fine-Tuning", "Large Language Models", "safety risks", "ProCon", "refusal direction"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806.pdf", "abs": "https://arxiv.org/abs/2509.06806", "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.", "AI": {"tldr": "MachineLearningLM enhances LLMs' in-context learning capability without gradient descent, achieving superior performance in ML tasks across various domains, including healthcare.", "motivation": "To improve the ability of LLMs to learn from many in-context examples for standard ML tasks, without relying on gradient descent.", "method": "A continued-pretraining framework synthesizing ML tasks from millions of structural causal models (SCMs) and using a random-forest teacher for distillation into the LLM.", "result": "MachineLearningLM outperforms strong LLM baselines by an average of about 15% on out-of-distribution tabular classification, showing significant improvements in accuracy as in-context demonstrations increase.", "conclusion": "The framework successfully maintains general chat capabilities while enhancing performance in various ML tasks, demonstrating a significant many-shot scaling law.", "key_contributions": ["Introduces MachineLearningLM for improved in-context learning with LLMs", "Achieves state-of-the-art accuracy in ML tasks across diverse domains", "Maintains general knowledge and reasoning abilities during pretraining"], "limitations": "", "keywords": ["Large Language Models", "In-Context Learning", "Machine Learning", "Healthcare", "Structural Causal Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06807", "pdf": "https://arxiv.org/pdf/2509.06807.pdf", "abs": "https://arxiv.org/abs/2509.06807", "title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications.", "AI": {"tldr": "The paper introduces the MoGU framework to balance security and usability in Large Language Models, improving upon it with MoGU_v2 for better adaptability and performance while addressing security risks without sacrificing usability.", "motivation": "With LLMs increasingly integrated into everyday life, ensuring they respond harmlessly to malicious prompts is crucial. Existing methods prioritize security, often at the expense of usability.", "method": "The MoGU framework uses a dynamic intra-layer router to balance between security-optimized and usability-optimized model variants. An improved version, MoGU_v2, enhances this by tightening the connection between routers and hidden states.", "result": "MoGU_v2 demonstrates strong adaptability and significant stability improvements across a range of LLMs, including mainstream and resource-constrained models, while effectively addressing security risks introduced by Instruction Fine-tuning.", "conclusion": "MoGU_v2 represents a versatile solution to enhance LLM security in practical applications without compromising their usability.", "key_contributions": ["Introduction of the MoGU framework for balancing security and usability in LLMs.", "Development of MoGU_v2 with improved adaptability and tighter router integration.", "Robust performance of MoGU_v2 across various types of LLM applications."], "limitations": "Parameter redundancy and performance bottlenecks in the initial MoGU framework.", "keywords": ["Large Language Models", "security", "usability", "MoGU framework", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.06809", "pdf": "https://arxiv.org/pdf/2509.06809.pdf", "abs": "https://arxiv.org/abs/2509.06809", "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem", "authors": ["Valentin Quesnel", "Damien Sileo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1", "AI": {"tldr": "The paper addresses the issue of low-quality data in mathematical reasoning for LLMs by developing a scalable data engine derived from automated theorem proving, generating error-free symbolic data for training.", "motivation": "To overcome the challenge of scarce, high-quality data that limits the mathematical reasoning capabilities of Large Language Models.", "method": "The framework uses E-prover's saturation capabilities on the TPTP axiom library to create a corpus of guaranteed-valid theorems, which are then filtered and transformed into tasks.", "result": "The framework eliminates factual errors by construction and generates three types of reasoning challenges. Zero-shot experiments show LLMs struggle with deep, structural reasoning.", "conclusion": "The proposed framework serves as both a diagnostic tool for reasoning capabilities and a scalable source of symbolic training data, with code and data publicly available.", "key_contributions": ["A scalable data engine leveraging automated theorem proving for LLM training.", "Creation of a corpus of guaranteed-valid theorems without LLM intervention.", "Development of difficulty-controlled reasoning challenges."], "limitations": "The experiments reveal a significant performance drop in LLMs on tasks that require deep reasoning.", "keywords": ["Large Language Models", "automated theorem proving", "symbolic training data"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.06813", "pdf": "https://arxiv.org/pdf/2509.06813.pdf", "abs": "https://arxiv.org/abs/2509.06813", "title": "A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs", "authors": ["Max Malyi", "Jonathan Shek", "Alasdair McDonald", "Andre Biscaya"], "categories": ["cs.CL"], "comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms", "summary": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.", "AI": {"tldr": "This paper presents a framework for classifying wind turbine maintenance logs using LLMs, promotes its open-source availability, and evaluates the performance of various models in this context.", "motivation": "The significant barrier to automated analysis of unstructured maintenance logs impacts the effective operation and maintenance of wind power, influencing the Levelised Cost of Energy.", "method": "A reproducible framework was developed to benchmark LLMs on the classification of wind turbine maintenance logs, analyzing both proprietary and open-source models.", "result": "The study identifies a performance hierarchy among LLMs, showing that classification accuracy varies with task semantic ambiguity, and emphasizes the effectiveness of using LLMs in conjunction with human experts for better data quality.", "conclusion": "LLMs should be integrated into a Human-in-the-Loop system to assist experts in labeling data, enhancing the quality of operation and maintenance data and improving reliability analysis.", "key_contributions": ["Development of a novel benchmarking framework for LLMs", "Open-source availability of the framework", "Identification of performance hierarchy among LLMs for maintenance log classification."], "limitations": "No model achieves perfect accuracy; calibration significantly varies among models.", "keywords": ["Large Language Models", "wind power", "maintenance logs", "human-in-the-loop", "data labeling"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.06836", "pdf": "https://arxiv.org/pdf/2509.06836.pdf", "abs": "https://arxiv.org/abs/2509.06836", "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens", "authors": ["Eugene Kwek", "Wenpeng Yin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.", "AI": {"tldr": "This paper presents COMPACT, a pruning method for LLMs that improves efficiency in memory and latency while maintaining competitive performance.", "motivation": "To enhance the efficiency of LLMs in memory usage, latency, and serving costs, particularly for edge deployment and interactive applications.", "method": "COMPACT jointly prunes rare vocabulary and removes intermediate channels in FFNs using common-token-weighted activations, aligning importance with the token distribution after pruning.", "result": "State-of-the-art performance observed in downstream tasks with reduced parameters and memory usage, while also lowering end-to-end latency.", "conclusion": "COMPACT combines the strengths of depth and width pruning without altering transformer architecture, achieving strong performance with significant memory and latency improvements.", "key_contributions": ["Introduces a novel COMPACT pruning method for LLMs.", "Maintains a standard transformer architecture while enabling effective pruning.", "Demonstrates strong performance metrics across various LLM families."], "limitations": "", "keywords": ["LLMs", "pruning", "efficiency", "memory", "latency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.06838", "pdf": "https://arxiv.org/pdf/2509.06838.pdf", "abs": "https://arxiv.org/abs/2509.06838", "title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models", "authors": ["Mohammad Reza Mirbagheri", "Mohammad Mahdi Mirkamali", "Zahra Motoshaker Arani", "Ali Javeri", "Amir Mahdi Sadeghzadeh", "Rasool Jalili"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.", "AI": {"tldr": "This study introduces the EPT metric for assessing the trustworthiness of Large Language Models (LLMs) across various ethical and cultural dimensions, revealing significant deficiencies particularly in safety.", "motivation": "To address the critical challenge of ensuring the trustworthiness of LLMs, focusing on their alignment with ethical, cultural, and social values.", "method": "We developed the EPT metric and evaluated LLMs like ChatGPT and Claude using a labeled dataset and both automated and human assessments.", "result": "The evaluation showed significant deficiencies in the safety dimension among the tested models, indicating a need for improvement in this area.", "conclusion": "There are critical gaps in the trustworthiness of LLMs concerning Persian cultural values that require attention to create responsible AI systems.", "key_contributions": ["Introduction of the EPT metric for assessing LLM trustworthiness", "Evaluation of multiple leading models against cultural and ethical benchmarks", "Public availability of the dataset for further research"], "limitations": "Focused primarily on Persian ethical-cultural values; applicability to other cultures may vary.", "keywords": ["Large Language Models", "trustworthiness", "evaluation metric", "cultural values", "AI ethics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.06870", "pdf": "https://arxiv.org/pdf/2509.06870.pdf", "abs": "https://arxiv.org/abs/2509.06870", "title": "The Majority is not always right: RL training for solution aggregation", "authors": ["Wenting Zhao", "Pranjal Aggarwal", "Swarnadeep Saha", "Asli Celikyilmaz", "Jason Weston", "Ilia Kulikov"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.", "AI": {"tldr": "This paper introduces AggLM, a model that learns to aggregate solutions for large language models using reinforcement learning, improving performance on reasoning tasks.", "motivation": "To enhance the effectiveness of aggregating multiple independent solutions from large language models, addressing limitations of traditional methods like majority voting.", "method": "AggLM is trained to review and synthesize outputs from various solutions using reinforcement learning, balancing easy and hard examples during training.", "result": "AggLM surpasses both rule-based and reward-model baselines across multiple benchmarks and generalizes well to outputs from different, and often stronger, models using fewer tokens than traditional methods.", "conclusion": "The proposed approach shows significant improvements in aggregating reasoning results from LLMs, making it a valuable method for leveraging multiple outputs effectively.", "key_contributions": ["Introduction of AggLM for learning aggregation as a reasoning skill.", "Demonstration of improved performance over traditional aggregation methods like majority voting.", "Effective generalization to solutions from stronger models with fewer token requirements."], "limitations": "", "keywords": ["aggregation", "large language models", "reinforcement learning", "reasoning tasks", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.06883", "pdf": "https://arxiv.org/pdf/2509.06883.pdf", "abs": "https://arxiv.org/abs/2509.06883", "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction", "authors": ["Joe Wilder", "Nikhil Kadapala", "Benji Xu", "Mohammed Alsaadi", "Aiden Parsons", "Mitchell Rogers", "Palash Agarwal", "Adam Hassick", "Laura Dietz"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain", "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.", "AI": {"tldr": "This paper explores methods for extracting check-worthy claims from social media using LLMs, achieving the best results with a fine-tuned FLAN-T5 model while noting alternative methods may yield higher quality claims despite lower METEOR scores.", "motivation": "To identify and extract check-worthy claims from social media passages effectively using advanced prompting methods and in-context learning with LLMs.", "method": "We evaluate different prompting techniques, including few-shot prompting and fine-tuning across various LLM families, focusing on their effectiveness in extracting claims.", "result": "The highest METEOR score was attained with a fine-tuned FLAN-T5 model; however, other methods occasionally extracted higher-quality claims despite lower scores.", "conclusion": "While fine-tuning yields strong METEOR scores, alternative extraction methods can result in better quality claims, indicating a trade-off between score and claim quality.", "key_contributions": ["Experimentation with multiple prompting techniques for LLMs", "Identification of quality vs. score trade-offs in claim extraction", "Application of LLMs to specific task of check-worthy claim identification"], "limitations": "No explicit limitations mentioned in the abstract.", "keywords": ["prompting", "in-context learning", "claim extraction", "LLM", "social media"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2509.06888", "pdf": "https://arxiv.org/pdf/2509.06888.pdf", "abs": "https://arxiv.org/abs/2509.06888", "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "authors": ["Marc Marone", "Orion Weller", "William Fleshman", "Eugene Yang", "Dawn Lawrie", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.", "AI": {"tldr": "mmBERT is a novel encoder-only language model pretrained on multilingual text, achieving high performance in classification and retrieval tasks across both high and low-resource languages.", "motivation": "There has been a lack of recent research on encoder models, especially multilingual models in the context of various machine learning tasks.", "method": "mmBERT was built using an inverse mask ratio schedule and inverse temperature sampling ratio, integrating over 1700 low-resource languages only during the decay phase.", "result": "mmBERT demonstrates significant performance boosts, achieving classification results comparable to state-of-the-art models like OpenAI's o3 and Google's Gemini 2.5 Pro, even when low-resource languages are included only during a limited phase.", "conclusion": "The introduction of mmBERT highlights the potential of enhanced multilingual training strategies to improve model performance in NLP tasks.", "key_contributions": ["Introduction of mmBERT as a pretrained multilingual encoder-only model", "Use of inverse mask ratio schedule and inverse temperature sampling ratio for improved training", "Significant performance improvements in classification tasks for high and low-resource languages."], "limitations": "", "keywords": ["multilingual models", "encoder-only models", "machine learning", "classification", "low-resource languages"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.06902", "pdf": "https://arxiv.org/pdf/2509.06902.pdf", "abs": "https://arxiv.org/abs/2509.06902", "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "authors": ["Aivin V. Solatorio"], "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.", "AI": {"tldr": "This paper proposes Proof-Carrying Numbers (PCN), a protocol to ensure numeric fidelity in Large Language Models (LLMs) by using mechanical verification and separating the verification process from the model.", "motivation": "To address the issue of numeric hallucination in LLMs, where generated numbers may deviate from actual data, leading to misinformation and lack of trust in the outputs.", "method": "PCN uses claim-bound tokens associated with structured claims that can be verified by a verifier according to specified policies, enforcing that only verified numbers are displayed as credible.", "result": "PCN is proved to be sound, complete under honest tokens, ensures fail-closed behavior, and maintains monotonicity with policy refinement, making it efficient and model-agnostic for integration.", "conclusion": "The protocol establishes a trust model based on proof, where numeric claims must be verified before presentation, reducing the risk of misinformation through unverified data.", "key_contributions": ["Introduction of Proof-Carrying Numbers (PCN) for numeric fidelity in LLMs", "Verification layered in the renderer instead of the model to mitigate spoofing", "Lightweight and model-agnostic integration into existing applications"], "limitations": "", "keywords": ["Large Language Models", "numeric hallucination", "Proof-Carrying Numbers", "mechanical verification", "trust in AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.06948", "pdf": "https://arxiv.org/pdf/2509.06948.pdf", "abs": "https://arxiv.org/abs/2509.06948", "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency.", "AI": {"tldr": "This study presents a method that combines supervised fine-tuning and reinforcement learning in a bilevel optimization framework to improve reasoning abilities of language models more efficiently.", "motivation": "To address the inefficiencies in reinforcement learning when applied to large language models by integrating it with supervised fine-tuning effectively.", "method": "A bilevel optimization approach is introduced, conditioning the supervised fine-tuning objective on the optimal reinforcement learning policy, allowing SFT to guide RL's optimization process while training.", "result": "Empirical evaluations on five reasoning benchmarks showed consistent improvements over baselines, achieving a superior balance between effectiveness and efficiency in training.", "conclusion": "The proposed method enhances the collaboration between supervised fine-tuning and reinforcement learning, leading to better performance in language models' reasoning capabilities.", "key_contributions": ["Introduced a novel bilevel optimization framework for training language models.", "Demonstrated the effectiveness of joint training of SFT and RL on reasoning benchmarks.", "Achieved improvements in both efficiency and effectiveness compared to traditional decoupled approaches."], "limitations": "No specific limitations mentioned.", "keywords": ["reinforcement learning", "supervised fine-tuning", "bilevel optimization", "large language models", "reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.06949", "pdf": "https://arxiv.org/pdf/2509.06949.pdf", "abs": "https://arxiv.org/abs/2509.06949", "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models", "authors": ["Yinjie Wang", "Ling Yang", "Bowen Li", "Ye Tian", "Ke Shen", "Mengdi Wang"], "categories": ["cs.CL"], "comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL", "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL", "AI": {"tldr": "TraceRL is a reinforcement learning framework for diffusion language models (DLMs) enhancing performance in math and coding tasks with new state-of-the-art models, TraDo.", "motivation": "The paper addresses the need for improved reasoning performance in complex tasks using diffusion language models, particularly for math and coding.", "method": "Introduces TraceRL, a trajectory-aware reinforcement learning framework that utilizes a diffusion-based value model to enhance training stability and sampling flexibility.", "result": "TraDo-4B-Instruct and TraDo-8B-Instruct models demonstrate significant accuracy improvements over baseline models on mathematical reasoning tasks, achieving 6.1% and 51.3% gains respectively.", "conclusion": "TraceRL enables the creation of advanced diffusion language models with superior performance on math reasoning and coding tasks, while providing a comprehensive framework for practical applications and reproducible research.", "key_contributions": ["Proposes TraceRL for trajectory-aware reinforcement learning in DLMs.", "Demonstrates state-of-the-art performance with TraDo models on math and coding tasks.", "Provides an open-source framework for building and deploying diffusion LLMs."], "limitations": "", "keywords": ["Reinforcement Learning", "Diffusion Language Models", "Mathematical Reasoning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.06952", "pdf": "https://arxiv.org/pdf/2509.06952.pdf", "abs": "https://arxiv.org/abs/2509.06952", "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts", "authors": ["Linlu Qiu", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Yoon Kim", "Roger P. Levy"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Main)", "summary": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans.", "AI": {"tldr": "This paper evaluates the pragmatic reasoning abilities of language models (LMs) using a framework inspired by a communication game, and explores improvements through Bayesian pragmatic reasoning.", "motivation": "The increasing use of language models as conversational agents necessitates a better understanding of their pragmatic reasoning capabilities in context.", "method": "An evaluation framework based on the Wavelength communication game is used to analyze LMs' performance in language comprehension and production, employing direct prompting and Chain-of-Thought (CoT) prompting, alongside a Rational Speech Act (RSA) approach.", "result": "State-of-the-art LMs perform well in language comprehension, mirroring human accuracy. CoT prompting enhances language production outcomes, with RSA offering significant improvements over traditional methods.", "conclusion": "The study reveals strengths and limitations in LMs' pragmatic reasoning, highlighting the effectiveness of RSA in enhancing their capabilities and suggesting future research directions.", "key_contributions": ["Evaluation framework derived from a communication game", "Findings show strong LM performance in language comprehension", "RSA significantly improves LM pragmatic reasoning abilities"], "limitations": "", "keywords": ["language models", "pragmatics", "Bayesian reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2309.14394", "pdf": "https://arxiv.org/pdf/2309.14394.pdf", "abs": "https://arxiv.org/abs/2309.14394", "title": "Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation", "authors": ["Tsiry Mayet", "Simon Bernard", "Romain Herault", "Clement Chatelain"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In this work, we address the challenge of multi-domain translation, where the\nobjective is to learn mappings between arbitrary configurations of domains\nwithin a defined set (such as $(D_1, D_2)\\rightarrow{}D_3$,\n$D_2\\rightarrow{}(D_1, D_3)$, $D_3\\rightarrow{}D_1$, etc. for three domains)\nwithout the need for separate models for each specific translation\nconfiguration, enabling more efficient and flexible domain translation. We\nintroduce Multi-Domain Diffusion (MDD), a method with dual purposes: i)\nreconstructing any missing views for new data objects, and ii) enabling\nlearning in semi-supervised contexts with arbitrary supervision configurations.\nMDD achieves these objectives by exploiting the noise formulation of diffusion\nmodels, specifically modeling one noise level per domain. Similar to existing\ndomain translation approaches, MDD learns the translation between any\ncombination of domains. However, unlike prior work, our formulation inherently\nhandles semi-supervised learning without modification by representing missing\nviews as noise in the diffusion process. We evaluate our approach through\ndomain translation experiments on BL3NDT, a multi-domain synthetic dataset\ndesigned for challenging semantic domain inversion, the BraTS2020 dataset, and\nthe CelebAMask-HQ dataset.", "AI": {"tldr": "This paper presents a method called Multi-Domain Diffusion (MDD) for efficient and flexible multi-domain translation while enabling semi-supervised learning.", "motivation": "The paper addresses the challenge of multi-domain translation by creating a model that can learn mappings between various configurations of domains without requiring separate models for each case, enhancing efficiency and flexibility.", "method": "The proposed MDD method uses diffusion models to manipulate noise levels for different domains, enabling reconstruction of missing views and supporting semi-supervised learning through a unified approach.", "result": "MDD demonstrated effectiveness in domain translation tasks across multiple datasets, showing improved performance compared to existing methods.", "conclusion": "The MDD approach simplifies the translation process across multiple domains while inherently incorporating semi-supervised learning capabilities without needing additional modifications.", "key_contributions": ["Introduction of Multi-Domain Diffusion (MDD) for flexible domain translation.", "Unified handling of semi-supervised learning and domain translation.", "Performance validation on multiple challenging datasets."], "limitations": "", "keywords": ["multi-domain translation", "semi-supervised learning", "diffusion models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2311.01766", "pdf": "https://arxiv.org/pdf/2311.01766.pdf", "abs": "https://arxiv.org/abs/2311.01766", "title": "Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation", "authors": ["Xin Yuan", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Shujun Li"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted and published by EMNLP 2023. Details can be found in\n  https://aclanthology.org/2023.emnlp-main.259", "summary": "Mis- and disinformation online have become a major societal problem as major\nsources of online harms of different kinds. One common form of mis- and\ndisinformation is out-of-context (OOC) information, where different pieces of\ninformation are falsely associated, e.g., a real image combined with a false\ntextual caption or a misleading textual description. Although some past studies\nhave attempted to defend against OOC mis- and disinformation through external\nevidence, they tend to disregard the role of different pieces of evidence with\ndifferent stances. Motivated by the intuition that the stance of evidence\nrepresents a bias towards different detection results, we propose a stance\nextraction network (SEN) that can extract the stances of different pieces of\nmulti-modal evidence in a unified framework. Moreover, we introduce a\nsupport-refutation score calculated based on the co-occurrence relations of\nnamed entities into the textual SEN. Extensive experiments on a public\nlarge-scale dataset demonstrated that our proposed method outperformed the\nstate-of-the-art baselines, with the best model achieving a performance gain of\n3.2% in accuracy. The source code and checkpoints are publicly available at\nhttps://github.com/yx3266/SEN.", "AI": {"tldr": "The paper introduces a stance extraction network (SEN) to address out-of-context mis- and disinformation by evaluating multi-modal evidence stances.", "motivation": "The increasing prevalence of mis- and disinformation online necessitates new methods to identify and mitigate their impact, particularly targeting out-of-context information.", "method": "The authors propose a stance extraction network (SEN) that extracts the stances of various pieces of multi-modal evidence, additionally integrating a support-refutation score based on named entity co-occurrence.", "result": "Extensive experiments on a large dataset show SEN outperformed existing models, achieving a 3.2% improvement in accuracy.", "conclusion": "The proposed SEN provides a more nuanced approach to tackling out-of-context mis- and disinformation, suggesting that stance analysis can enhance detection performance.", "key_contributions": ["Introduction of the stance extraction network (SEN) for multi-modal evidence evaluation.", "Integration of a support-refutation score based on entity co-occurrence.", "Demonstrated improved performance over state-of-the-art baselines."], "limitations": "The reliance on the quality of input data and the potential for varying performance across different contexts.", "keywords": ["misinformation", "stance extraction", "multi-modal evidence", "machine learning", "HCI"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2402.11282", "pdf": "https://arxiv.org/pdf/2402.11282.pdf", "abs": "https://arxiv.org/abs/2402.11282", "title": "Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures", "authors": ["Qihang Yang", "Caimei Yang", "Yu Liao", "Ziman Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "In several languages, omitting a verb phrase (VP) in double centre-embedded\nstructures creates a grammaticality illusion. Similar illusion also exhibited\nin Mandarin missing-NP double centre-embedded structures. However, there is no\nconsensus on its very nature. Instead of treating it as grammaticality\nillusion, we argue that ambiguous interpretations of verbs can best account for\nthis phenomenon in Mandarin. To further support this hypothesis, we conducted\ntwo electroencephalography (EEG) experiments on quasi double centre-embedded\nstructures whose complexity is reduced by placing the self-embedding relative\nclauses into the sentence's subject position. Experiment 1 showed that similar\nphenomenon even exhibited in this structure, evidenced by an absence of P600\neffect and a presence of N400 effect. In Experiment 2, providing semantic cues\nto reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We\ninterpret the results under garden-path theory and propose that word-order\ndifference may account for this cross-linguistic variation.", "AI": {"tldr": "The paper examines grammaticality illusions in Mandarin involving double centre-embedded structures, proposing that ambiguous verb interpretations rather than grammaticality account for these illusions, supported by EEG experiments.", "motivation": "To resolve the lack of consensus on the nature of grammaticality illusions in Mandarin double centre-embedded structures.", "method": "Conducted two EEG experiments on quasi double centre-embedded structures while manipulating sentence complexity and semantic cues.", "result": "Experiment 1 indicated the presence of grammaticality illusions with the absence of P600 and presence of N400 effects; Experiment 2 showed the dispelling of these illusions when providing semantic cues, evidenced by a P600 effect.", "conclusion": "The study validates that ambiguous verb interpretations contribute to the illusion, with implications for garden-path theory, and highlights the influence of word-order differences in cross-linguistic variation.", "key_contributions": ["Proposes a novel interpretation of grammaticality illusions in Mandarin", "Uses EEG experiments to provide empirical support", "Discusses implications for cross-linguistic understanding of sentence structure"], "limitations": "", "keywords": ["grammaticality illusion", "Mandarin", "EEG", "ambiguity", "double centre-embedded structures"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2402.15449", "pdf": "https://arxiv.org/pdf/2402.15449.pdf", "abs": "https://arxiv.org/abs/2402.15449", "title": "Repetition Improves Language Model Embeddings", "authors": ["Jacob Mitchell Springer", "Suhas Kotha", "Daniel Fried", "Graham Neubig", "Aditi Raghunathan"], "categories": ["cs.CL", "cs.LG"], "comment": "ICLR 2025", "summary": "Bidirectional models are considered essential for strong text embeddings.\nRecent approaches to adapt autoregressive language models (LMs) into strong\ntext embedding models have largely had the requirement to modify the LM\narchitecture to be bidirectional. We challenge this premise by introducing\n\"echo embeddings\" which converts autoregressive LMs into high quality text\nembedding models without changing the architecture or requiring fine-tuning. By\nrepeating the input and extracting embeddings from the repeated tokens -- which\nhave access to all original tokens -- echo embeddings improve over classical LM\nembeddings by over 5% in zero-shot settings. Our zero-shot embeddings nearly\nmatch those obtained by bidirectionally-converted LMs that undergo additional\nmasked-language modeling training. Echo embeddings are also compatible with\nsupervised fine-tuning, matching or outperforming bidirectionally-converted LMs\nin an apples-to-apples comparison, even with an identical compute budget during\ntraining and inference. Overall, repetition is a simple and effective strategy\nto circumvent the need for bidirectional attention in embedding models, paving\nthe way towards a unified architecture for all NLP tasks.", "AI": {"tldr": "This paper introduces echo embeddings, a method to adapt autoregressive language models into effective text embedding models without altering their architecture or requiring fine-tuning.", "motivation": "To demonstrate that autoregressive language models can be effectively transformed into high-quality text embedding models without necessitating architecture modifications.", "method": "The method involves repeating the input tokens and extracting embeddings from these repeated tokens, allowing the model to utilize information from all original tokens.", "result": "Echo embeddings improve over classical LM embeddings by over 5% in zero-shot settings and nearly match embeddings from bidirectionally-converted LMs trained with additional masked-language modeling.", "conclusion": "Repetition is a simple yet effective strategy to avoid the need for bidirectional attention in embedding models and suggests a unified architecture for various NLP tasks.", "key_contributions": ["Introduction of echo embeddings", "Improvement over classical LM embeddings", "Compatibility with supervised fine-tuning"], "limitations": "", "keywords": ["text embeddings", "autoregressive models", "NLP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2405.15454", "pdf": "https://arxiv.org/pdf/2405.15454.pdf", "abs": "https://arxiv.org/abs/2405.15454", "title": "Linearly Controlled Language Generation with Performative Guarantees", "authors": ["Emily Cheng", "Carmen Amo Alonso"], "categories": ["cs.CL", "cs.SY", "eess.SY"], "comment": "Under review", "summary": "The increasing prevalence of Large Language Models (LMs) in critical\napplications highlights the need for controlled language generation strategies\nthat are not only computationally efficient but that also enjoy performance\nguarantees. To achieve this, we use a common model of concept semantics as\nlinearly represented in an LM's latent space. In particular, we take the view\nthat natural language generation traces a trajectory in this continuous\nsemantic space, realized by the language model's hidden activations. This view\npermits a control-theoretic treatment of text generation in latent space, in\nwhich we propose a lightweight, gradient-free intervention that dynamically\nsteers trajectories away from regions corresponding to undesired meanings. In\nparticular, we propose to directly intervene the activations of the token that\nis being generated in embedding space in an online fashion. Crucially, we do\nnot simply steer activations towards a desirable region. Instead, our method\nrelies on classical techniques from control theory to precisely control\nactivations in a context-dependent way, and guarantees that they are brought\ninto a specific pre-defined region of embedding space that corresponds to\nallowed semantics. Our intervention is computed in closed-form according to an\noptimal controller formulation, minimally impacting generation time. This\ncontrol of the activations in embedding space allows for fine-grained steering\nof attributes of the generated sequence. We demonstrate the effectiveness of\nour approach on different objectives-- toxicity avoidance and sentiment\ncontrol-- while maintaining text quality.", "AI": {"tldr": "This paper presents a control-theoretic approach for dynamically guiding language model activations during natural language generation to avoid undesired meanings and ensure quality while maintaining efficiency.", "motivation": "To address the increasing need for controlled language generation in critical applications of Large Language Models (LMs), ensuring efficiency and guaranteeing performance.", "method": "The paper proposes a gradient-free intervention method that directly steers the activations of the token being generated in embedding space based on control theory, allowing for context-sensitive adjustments.", "result": "The proposed method successfully demonstrates effectiveness in controlling attributes like toxicity and sentiment while maintaining text quality across various objectives.", "conclusion": "By applying optimal control techniques, the proposed lightweight intervention guarantees the generated text remains within pre-defined semantic boundaries without significantly impacting generation time.", "key_contributions": ["Introduces a control-theoretic framework for language generation in latent space.", "Implements a lightweight intervention method for real-time control of token activations.", "Demonstrates effectiveness in toxicity avoidance and sentiment control without compromising text quality."], "limitations": "", "keywords": ["Large Language Models", "natural language generation", "control theory", "toxicity avoidance", "sentiment control"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.06056", "pdf": "https://arxiv.org/pdf/2406.06056.pdf", "abs": "https://arxiv.org/abs/2406.06056", "title": "Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text", "authors": ["Avijit Mitra", "Zhichao Yang", "Emily Druhl", "Raelene Goodwin", "Hong Yu"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (main) Github:\n  https://github.com/avipartho/Synth-SBDH", "summary": "Social and behavioral determinants of health (SBDH) play a crucial role in\nhealth outcomes and are frequently documented in clinical text. Automatically\nextracting SBDH information from clinical text relies on publicly available\ngood-quality datasets. However, existing SBDH datasets exhibit substantial\nlimitations in their availability and coverage. In this study, we introduce\nSynth-SBDH, a novel synthetic dataset with detailed SBDH annotations,\nencompassing status, temporal information, and rationale across 15 SBDH\ncategories. We showcase the utility of Synth-SBDH on three tasks using\nreal-world clinical datasets from two distinct hospital settings, highlighting\nits versatility, generalizability, and distillation capabilities. Models\ntrained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH\ntraining, achieving up to 63.75% macro-F improvements. Additionally, Synth-SBDH\nproves effective for rare SBDH categories and under-resource constraints while\nbeing substantially cheaper than expert-annotated real-world data. Human\nevaluation reveals a 71.06% Human-LLM alignment and uncovers areas for future\nrefinements.", "AI": {"tldr": "Synth-SBDH is a novel synthetic dataset designed to improve the extraction of social and behavioral determinants of health (SBDH) from clinical text, outperforming existing datasets and enhancing model performance.", "motivation": "There is a need for high-quality datasets to extract social and behavioral determinants of health (SBDH) information from clinical text, as existing datasets are often limited in availability and coverage.", "method": "The study introduces Synth-SBDH, a synthetic dataset with detailed SBDH annotations across 15 categories. It demonstrates its utility by training models on scenarios with real-world clinical datasets from two hospital settings and evaluating performance across three tasks.", "result": "Models trained on Synth-SBDH show up to 63.75% macro-F improvements compared to those without this training. The dataset is effective for rare SBDH categories and can be used under resource constraints, being cheaper than expert-annotated data.", "conclusion": "Synth-SBDH demonstrates significant improvements in SBDH extraction tasks and reveals areas for further refinements through human evaluation, achieving a 71.06% Human-LLM alignment.", "key_contributions": ["Introduction of the Synth-SBDH synthetic dataset with detailed annotations.", "Demonstrated improvements in extraction performance on real-world clinical datasets.", "Evaluation of dataset utility across multiple tasks with human-LLM alignment insights."], "limitations": "", "keywords": ["SBDH", "synthetic dataset", "NLP", "health informatics", "clinical text"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2407.05022", "pdf": "https://arxiv.org/pdf/2407.05022.pdf", "abs": "https://arxiv.org/abs/2407.05022", "title": "A Principled Framework for Evaluating on Typologically Diverse Languages", "authors": ["Esther Ploeger", "Wessel Poelman", "Andreas Holck Høeg-Petersen", "Anders Schlichtkrull", "Miryam de Lhoneux", "Johannes Bjerva"], "categories": ["cs.CL"], "comment": "Revised version", "summary": "Beyond individual languages, multilingual natural language processing (NLP)\nresearch increasingly aims to develop models that perform well across languages\ngenerally. However, evaluating these systems on all the world's languages is\npractically infeasible. To attain generalizability, representative language\nsampling is essential. Previous work argues that generalizable multilingual\nevaluation sets should contain languages with diverse typological properties.\nHowever, 'typologically diverse' language samples have been found to vary\nconsiderably in this regard, and popular sampling methods are flawed and\ninconsistent. We present a language sampling framework for selecting highly\ntypologically diverse languages given a sampling frame, informed by language\ntypology. We compare sampling methods with a range of metrics and find that our\nsystematic methods consistently retrieve more typologically diverse language\nselections than previous methods in NLP. Moreover, we provide evidence that\nthis affects generalizability in multilingual model evaluation, emphasizing the\nimportance of diverse language sampling in NLP evaluation.", "AI": {"tldr": "This paper presents a language sampling framework to select highly typologically diverse languages for multilingual NLP model evaluation, improving generalizability.", "motivation": "As multilingual NLP research aims for models to perform well across languages, effective evaluation methods using representative language samples are crucial.", "method": "The paper proposes a systematic language sampling framework informed by language typology, comparing it against existing sampling methods with various metrics.", "result": "The proposed framework consistently retrieves more typologically diverse language selections compared to previous methods, positively impacting generalizability in multilingual model evaluation.", "conclusion": "Diverse language sampling is essential for accurately evaluating multilingual models and enhancing their generalizability across languages.", "key_contributions": ["Introduction of a systematic language sampling framework for typologically diverse languages", "Comparison of sampling methods demonstrating superior diversity in selections", "Evidence linking language diversity in evaluation sets to improved model generalizability"], "limitations": "", "keywords": ["multilingual NLP", "language sampling", "typological diversity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2408.04638", "pdf": "https://arxiv.org/pdf/2408.04638.pdf", "abs": "https://arxiv.org/abs/2408.04638", "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective", "authors": ["Yiqun Zhang", "Xiaocui Yang", "Xingle Xu", "Zeran Gao", "Yijie Huang", "Shiyi Mu", "Shi Feng", "Daling Wang", "Yifei Zhang", "Kaisong Song", "Ge Yu"], "categories": ["cs.CL", "cs.CY"], "comment": "Compared with the previous version, reinforcement learning has been\n  added (as a new section), including RLHF, RLVR, and RLAIF", "summary": "Affective Computing (AC) integrates computer science, psychology, and\ncognitive science to enable machines to recognize, interpret, and simulate\nhuman emotions across domains such as social media, finance, healthcare, and\neducation. AC commonly centers on two task families: Affective Understanding\n(AU) and Affective Generation (AG). While fine-tuned pre-trained language\nmodels (PLMs) have achieved solid AU performance, they often generalize poorly\nacross tasks and remain limited for AG, especially in producing diverse,\nemotionally appropriate responses. The advent of Large Language Models (LLMs)\n(e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context\nlearning, broader world knowledge, and stronger sequence generation. This\nsurvey presents an NLP-oriented overview of AC in the LLM era. We (i)\nconsolidate traditional AC tasks and preliminary LLM-based studies; (ii) review\nadaptation techniques that improve AU/AG, including Instruction Tuning (full\nand parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt\nEngineering (zero/few-shot, chain-of-thought, agent-based prompting), and\nReinforcement Learning. For the latter, we summarize RL from human preferences\n(RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which\nprovide preference- or rule-grounded optimization signals that can help steer\nAU/AG toward empathy, safety, and planning, achieving finer-grained or\nmulti-objective control. To assess progress, we compile benchmarks and\nevaluation practices for both AU and AG. We also discuss open challenges-from\nethics, data quality, and safety to robust evaluation and resource\nefficiency-and outline research directions. We hope this survey clarifies the\nlandscape and offers practical guidance for building affect-aware, reliable,\nand responsible LLM systems.", "AI": {"tldr": "This survey reviews Affective Computing (AC) in the context of Large Language Models (LLMs), focusing on Affective Understanding and Generation, adaptation techniques, reinforcement learning approaches, evaluation practices, and open challenges.", "motivation": "To explore how recent advances in Large Language Models (LLMs) can enhance the performance and adaptability of Affective Computing (AC) across various domains.", "method": "The paper consolidates existing AC tasks, reviews LLM-based studies, and discusses adaptation methods for improving Affective Understanding (AU) and Affective Generation (AG), including various techniques such as Instruction Tuning and Prompt Engineering. It also highlights reinforcement learning approaches tailored to improve emotional response quality.", "result": "The survey identifies that while fine-tuned pre-trained language models perform well in Affective Understanding, they struggle with Affective Generation, particularly in producing diverse and contextually appropriate responses. The integration of LLMs shows promise in addressing these challenges.", "conclusion": "The paper outlines the current state of AC within the LLM landscape and provides guidance on practical implementations while highlighting numerous challenges such as ethics and evaluation that need addressing for effective deployment.", "key_contributions": ["Overview of traditional AC tasks and LLM-based studies", "Review of adaptation techniques for AU/AG", "Compilation of benchmarks and evaluation practices for AC"], "limitations": "Challenges in ethics, data quality, safety, robust evaluation, and resource efficiency remain.", "keywords": ["Affective Computing", "Large Language Models", "Reinforcement Learning", "Natural Language Processing", "Emotion recognition"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2408.16482", "pdf": "https://arxiv.org/pdf/2408.16482.pdf", "abs": "https://arxiv.org/abs/2408.16482", "title": "Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning", "authors": ["Rochelle Choenni", "Ekaterina Shutova"], "categories": ["cs.CL"], "comment": null, "summary": "Improving the alignment of Large Language Models (LLMs) with respect to the\ncultural values that they encode has become an increasingly important topic. In\nthis work, we study whether we can exploit existing knowledge about cultural\nvalues at inference time to adjust model responses to cultural value probes. We\npresent a simple and inexpensive method that uses a combination of in-context\nlearning (ICL) and human survey data, and show that we can improve the\nalignment to cultural values across 5 models that include both English-centric\nand multilingual LLMs. Importantly, we show that our method could prove useful\nin test languages other than English and can improve alignment to the cultural\nvalues that correspond to a range of culturally diverse countries.", "AI": {"tldr": "This paper presents a method to enhance the alignment of Large Language Models with cultural values using in-context learning and survey data.", "motivation": "The alignment of Large Language Models with cultural values is crucial, given their widespread use and potential impact on different cultures.", "method": "The authors propose a method that leverages in-context learning along with human survey data to adjust model responses to cultural values.", "result": "The method improves the alignment of five different models, including multilingual LLMs, with cultural probes, demonstrating effectiveness beyond English-centric applications.", "conclusion": "The proposed technique offers a promising approach to align language models more closely with diverse cultural values, showing potential utility in various languages.", "key_contributions": ["Developed a simple method combining ICL and human survey data for values alignment.", "Demonstrated improvements in alignment across multiple LLMs.", "Showed applicability to culturally diverse countries beyond English-centric contexts."], "limitations": "The method's performance in real-world applications and its generalizability across all cultural contexts require further investigation.", "keywords": ["large language models", "cultural alignment", "in-context learning", "multilingual", "human survey data"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.04914", "pdf": "https://arxiv.org/pdf/2411.04914.pdf", "abs": "https://arxiv.org/abs/2411.04914", "title": "GASE: Generatively Augmented Sentence Encoding", "authors": ["Manuel Frank", "Haithem Afli"], "categories": ["cs.CL"], "comment": "EMNLP Findings 2025", "summary": "We propose a training-free approach to improve sentence embeddings leveraging\ntest-time compute by applying generative text models for data augmentation at\ninference time. Unlike conventional data augmentation that utilises synthetic\ntraining data, our approach does not require access to model parameters or the\ncomputational resources typically required for fine-tuning state-of-the-art\nmodels. Generatively Augmented Sentence Encoding variates the input text by\nparaphrasing, summarising, or extracting keywords, followed by pooling the\noriginal and synthetic embeddings. Experimental results on the Massive Text\nEmbedding Benchmark for Semantic Textual Similarity (STS) demonstrate\nperformance improvements across a range of embedding models using different\ngenerative models for augmentation. We find that generative augmentation leads\nto larger performance improvements for embedding models with lower baseline\nperformance. These findings suggest that integrating generative augmentation at\ninference time adds semantic diversity and can enhance the robustness and\ngeneralisability of sentence embeddings for embedding models. Our results show\nthat performance gains depend on the embedding model and the dataset.", "AI": {"tldr": "A training-free method to enhance sentence embeddings using generative models for data augmentation at inference time.", "motivation": "To improve sentence embeddings without requiring model fine-tuning or extensive computational resources.", "method": "The approach uses generative models to paraphrase, summarize, or extract keywords from input text during inference, pooling the original and synthetic embeddings.", "result": "Experiments show performance improvements on the Massive Text Embedding Benchmark for Semantic Textual Similarity, particularly for embedding models with lower baseline performance.", "conclusion": "Generative augmentation increases semantic diversity, improving the robustness and generalisability of sentence embeddings, with performance gains varying by model and dataset.", "key_contributions": ["Training-free approach for sentence embedding enhancement", "Utilization of generative models for data augmentation at inference", "Demonstrated performance improvements on benchmark datasets"], "limitations": "", "keywords": ["sentence embeddings", "data augmentation", "generative models"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2411.05665", "pdf": "https://arxiv.org/pdf/2411.05665.pdf", "abs": "https://arxiv.org/abs/2411.05665", "title": "Exploring the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal", "authors": ["Fuka Matsuzaki", "Haru-Tada Sato"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities.", "AI": {"tldr": "This paper evaluates the limitations of LLMs in processing masked text, introducing two tasks: MskQA and MskCal, and revealing that performance depends on masking rates and semantic cues.", "motivation": "To understand the limitations and performance variations of LLMs when processing masked text, given their reliance on semantic cues.", "method": "The paper introduces two tasks (MskQA and MskCal) to evaluate reasoning abilities in LLMs, specifically testing GPT-4o and 4o-mini under different masking conditions.", "result": "LLMs show resilience to masked text but perform significantly worse under solid masking conditions compared to partial lifting. GPT-4o outperforms 4o-mini, especially in numerical reasoning tasks.", "conclusion": "The findings demonstrate the importance of semantic information in reasoning tasks for LLMs and highlight the need for improved evaluation methods to fully assess LLM comprehension abilities.", "key_contributions": ["Introduction of MskQA and MskCal tasks for evaluating LLM reasoning on masked text.", "Identifies performance dependency on masking rates and semantic cues in LLMs.", "Performance analysis of GPT-4o versus 4o-mini in masked text contexts."], "limitations": "Focused on specific models; results may vary with other LLM architectures or tasks.", "keywords": ["Large Language Models", "masked text", "reasoning", "semantic cues", "evaluation methods"], "importance_score": 9, "read_time_minutes": 19}}
{"id": "2411.07152", "pdf": "https://arxiv.org/pdf/2411.07152.pdf", "abs": "https://arxiv.org/abs/2411.07152", "title": "HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals", "authors": ["Lingbo Mo", "Shun Jiang", "Akash Maharaj", "Bernard Hishamunda", "Yunyao Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to DaSH Workshop at VLDB 2025", "summary": "Task-Oriented Dialogue (TOD) systems assist users in completing tasks through\nnatural language interactions, often relying on a single-layered workflow\nstructure for slot-filling in public tasks, such as hotel bookings. However, in\nenterprise environments, which involve rich domain-specific knowledge, TOD\nsystems face challenges due to task complexity and the lack of standardized\ndocumentation. In this work, we introduce HierTOD, an enterprise TOD system\ndriven by hierarchical goals that can support composite workflows. By focusing\non goal-driven interactions, our system serves a more proactive role,\nfacilitating mixed-initiative dialogue and improving task completion. Equipped\nwith components for natural language understanding, composite goal retriever,\ndialogue management, and response generation, backed by a well-organized data\nservice with domain knowledge base and retrieval engine, HierTOD delivers\nefficient task assistance as judged by human evaluators. Furthermore, our\nsystem implementation unifies two TOD paradigms: slot-filling for information\ncollection and step-by-step guidance for task execution. Our user study\ndemonstrates the effectiveness and helpfulness of HierTOD in performing both\nparadigms.", "AI": {"tldr": "HierTOD is an enterprise Task-Oriented Dialogue system designed to handle complex workflows through hierarchical goals, providing proactive task assistance and improving dialogue management.", "motivation": "Task-Oriented Dialogue systems struggle in enterprise environments due to complexity and lack of standardized documentation, necessitating a more robust solution for task completion.", "method": "HierTOD employs hierarchical goals to facilitate mixed-initiative dialogue, integrating components for natural language understanding, composite goal retrieval, dialogue management, and response generation.", "result": "HierTOD enhances task assistance efficiency, judged positively by human evaluators, and unifies slot-filling with step-by-step task execution guidance.", "conclusion": "User studies validate HierTOD's effectiveness in improving task completion across varied workflows.", "key_contributions": ["Introduction of hierarchical goals in enterprise TOD systems", "Unified approach to slot-filling and step-by-step guidance", "Demonstrated effectiveness through user studies"], "limitations": "", "keywords": ["Task-Oriented Dialogue", "Hierarchical goals", "Enterprise systems"], "importance_score": 7, "read_time_minutes": 7}}
{"id": "2411.16353", "pdf": "https://arxiv.org/pdf/2411.16353.pdf", "abs": "https://arxiv.org/abs/2411.16353", "title": "Lessons from Studying Two-Hop Latent Reasoning", "authors": ["Mikita Balesni", "Tomek Korbak", "Owain Evans"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models can use chain-of-thought (CoT) to externalize\nreasoning, potentially enabling oversight of capable LLM agents. Prior work has\nshown that models struggle at two-hop question-answering without CoT. This\ncapability is so basic that if it was a fundamental limitation, it would imply\nthat many complex agentic tasks would similarly require CoT. We investigate LLM\nlatent reasoning capabilities using two-hop question answering as a case study.\nPrevious work on the gap between latent and externalized two-hop reasoning\nproduced mixed evidence with inconclusive results. In this paper, we introduce\na controlled setting for investigating two-hop reasoning in LLMs, where a\npositive result provides definitive evidence for latent reasoning. We fine-tune\nLLMs (including Llama 3 8B and GPT-4o) on synthetic facts and test two-hop\nreasoning over these facts. By using synthetic facts, we rule out memorization\nand reasoning shortcuts as explanations for two-hop performance. We observe a\nnuanced picture: Models fail to compose two synthetic facts, but can succeed\nwhen one fact is synthetic and the other is natural. These results demonstrate\nthat LLMs are undeniably capable of latent two-hop reasoning, although it\nremains unclear how this ability scales with model size. Finally, we highlight\na lesson for researchers studying LLM reasoning: when drawing conclusions about\nLLM latent reasoning, one must be careful to avoid both spurious successes\n(that stem from memorization and reasoning shortcuts) and spurious failures\n(that may stem from artificial experimental setups, divorced from training\nsetups of frontier LLMs).", "AI": {"tldr": "The paper investigates the latent reasoning capabilities of large language models (LLMs) through two-hop question answering, revealing nuanced insights on their reasoning abilities.", "motivation": "To understand the limitations and capabilities of LLMs in two-hop question answering and the implications for agentic tasks.", "method": "Controlled experiments fine-tuned LLMs on synthetic facts to evaluate their two-hop reasoning performance, avoiding the influences of memorization.", "result": "Models can answer two-hop questions successfully when one fact is synthetic and the other is natural, indicating latent reasoning capabilities, but they struggle to composite two synthetic facts.", "conclusion": "LLMs are shown to possess latent two-hop reasoning abilities; however, clarity on how this ability scales with model size is still needed.", "key_contributions": ["Introduces a controlled setting for studying LLM reasoning.", "Demonstrates LLMs' capabilities in latent two-hop reasoning with nuanced results.", "Cautions researchers against misinterpreting LLM performance based on experimental design."], "limitations": "Unclear scalability of reasoning abilities with model size; mixed results from prior studies on LLM reasoning.", "keywords": ["large language models", "latent reasoning", "two-hop question answering", "synthetic facts", "model performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.00098", "pdf": "https://arxiv.org/pdf/2412.00098.pdf", "abs": "https://arxiv.org/abs/2412.00098", "title": "Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "categories": ["cs.CL"], "comment": "6 pages, 3 figures, 7 tables", "summary": "The exponential growth of online textual content across diverse domains has\nnecessitated advanced methods for automated text classification. Large Language\nModels (LLMs) based on transformer architectures have shown significant success\nin this area, particularly in natural language processing (NLP) tasks. However,\ngeneral-purpose LLMs often struggle with domain-specific content, such as\nscientific texts, due to unique challenges like specialized vocabulary and\nimbalanced data. In this study, we fine-tune four state-of-the-art LLMs BERT,\nSciBERT, BioBERT, and BlueBERT on three datasets derived from the WoS-46985\ndataset to evaluate their performance in scientific text classification. Our\nexperiments reveal that domain-specific models, particularly SciBERT,\nconsistently outperform general-purpose models in both abstract-based and\nkeyword-based classification tasks. Additionally, we compare our achieved\nresults with those reported in the literature for deep learning models, further\nhighlighting the advantages of LLMs, especially when utilized in specific\ndomains. The findings emphasize the importance of domain-specific adaptations\nfor LLMs to enhance their effectiveness in specialized text classification\ntasks.", "AI": {"tldr": "This study explores the fine-tuning of LLMs for automated classification of scientific texts, revealing that domain-specific models like SciBERT outperform general-purpose models.", "motivation": "With the rapid increase of online textual content, there is a need for improved methods of automated text classification, especially for domain-specific content.", "method": "Four LLMs (BERT, SciBERT, BioBERT, and BlueBERT) were fine-tuned on three datasets from the WoS-46985 dataset and evaluated for scientific text classification.", "result": "Domain-specific models, especially SciBERT, consistently outperformed general-purpose models in both abstract-based and keyword-based classification tasks.", "conclusion": "The study underscores the necessity of domain-specific adaptations for LLMs to enhance their performance in specialized text classification.", "key_contributions": ["Fine-tuning of multiple LLMs for a specific domain", "Comparison of domain-specific and general-purpose models", "Demonstration of improved performance on scientific texts"], "limitations": "", "keywords": ["Text Classification", "Large Language Models", "SciBERT", "Domain Adaptation", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.01113", "pdf": "https://arxiv.org/pdf/2412.01113.pdf", "abs": "https://arxiv.org/abs/2412.01113", "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning", "authors": ["Keito Kudo", "Yoichi Aoki", "Tatsuki Kuribayashi", "Shusaku Sone", "Masaya Taniguchi", "Ana Brassard", "Keisuke Sakaguchi", "Kentaro Inui"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the incremental, internal problem-solving process of\nlanguage models (LMs) with arithmetic multi-hop reasoning as a case study. We\nspecifically investigate when LMs internally resolve sub/whole problems through\nfirst reading the problem statements, generating reasoning chains, and\nachieving the final answer to mechanistically interpret LMs' multi-hop\nproblem-solving process. Our experiments reveal a systematic incremental\nreasoning strategy underlying LMs. They have not derived an answer at the\nmoment they first read the problem; instead, they obtain (sub)answers while\ngenerating the reasoning chain. Therefore, the generated reasoning chains can\nbe regarded as faithful reflections of the model's internal computation.", "AI": {"tldr": "This study reveals that language models (LMs) use an incremental reasoning strategy for arithmetic multi-hop problems, deriving sub-answers while generating reasoning chains.", "motivation": "To mechanistically interpret the internal problem-solving process of language models, especially in the context of arithmetic multi-hop reasoning.", "method": "Experiments were conducted to observe how language models resolve sub-problems and construct reasoning chains incrementally.", "result": "The study found that LMs generate sub-answers during the process of creating reasoning chains rather than obtaining answers immediately upon reading the problem statements.", "conclusion": "The reasoning chains produced by LMs provide a faithful reflection of their internal computation process.", "key_contributions": ["Identifies an incremental reasoning strategy in language models", "Demonstrates the generation of sub-answers during reasoning chain construction", "Provides insights into the internal computation of LMs during problem-solving"], "limitations": "", "keywords": ["language models", "multi-hop reasoning", "arithmetic reasoning", "internal computation", "incremental reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.07992", "pdf": "https://arxiv.org/pdf/2412.07992.pdf", "abs": "https://arxiv.org/abs/2412.07992", "title": "Concept Bottleneck Large Language Models", "authors": ["Chung-En Sun", "Tuomas Oikarinen", "Berk Ustun", "Tsui-Wei Weng"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICLR 2025", "summary": "We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel\nframework for building inherently interpretable Large Language Models (LLMs).\nIn contrast to traditional black-box LLMs that rely on limited post-hoc\ninterpretations, CB-LLMs integrate intrinsic interpretability directly into the\nLLMs -- allowing accurate explanations with scalability and transparency. We\nbuild CB-LLMs for two essential NLP tasks: text classification and text\ngeneration. In text classification, CB-LLMs is competitive with, and at times\noutperforms, traditional black-box models while providing explicit and\ninterpretable reasoning. For the more challenging task of text generation,\ninterpretable neurons in CB-LLMs enable precise concept detection, controlled\ngeneration, and safer outputs. The embedded interpretability empowers users to\ntransparently identify harmful content, steer model behavior, and unlearn\nundesired concepts -- significantly enhancing the safety, reliability, and\ntrustworthiness of LLMs, which are critical capabilities notably absent in\nexisting models. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/CB-LLMs.", "AI": {"tldr": "This paper introduces Concept Bottleneck Large Language Models (CB-LLMs), which enhance interpretability in LLMs by integrating transparency directly into the modeling process for tasks like text classification and generation.", "motivation": "To address the interpretability limitations of traditional black-box LLMs and improve user trust and safety in model outputs.", "method": "Develop CB-LLMs for text classification and generation, allowing intrinsic interpretability and explicit reasoning during NLP tasks.", "result": "CB-LLMs perform competitively with traditional models in text classification and provide safer, controlled outputs in text generation while enabling precise concept detection.", "conclusion": "CB-LLMs enhance the safety, reliability, and trustworthiness of LLMs through built-in interpretability and control mechanisms.", "key_contributions": ["Introduces a novel framework for interpretable LLMs (CB-LLMs).", "Demonstrates improved performance in text classification with intrinsic interpretability.", "Enhances text generation with control over model behavior and content safety."], "limitations": "", "keywords": ["interpretable models", "large language models", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.12583", "pdf": "https://arxiv.org/pdf/2412.12583.pdf", "abs": "https://arxiv.org/abs/2412.12583", "title": "Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise", "authors": ["Hanyin Wang", "Chufan Gao", "Qiping Xu", "Bolun Liu", "Guleid Hussein", "Hariprasad Korsapati", "Mohamad El Labban", "Kingsley Iheasirim", "Mohamed Hassan", "Gokhan Anil", "Brian Bartlett", "Jimeng Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Process-supervised reward models (PRMs) excel at providing step-by-step\nverification for large language model (LLM) outputs in domains like mathematics\nand coding. However, their application to fields lacking ground-truth answers,\nsuch as clinical note generation, poses significant challenges. We introduce a\nnovel framework for training PRMs to deliver step-level reward signals for\nLLM-generated clinical notes. By precisely defining meaningful \"steps,\"\ninjecting realistic \"errors\" informed by domain expertise, and leveraging LLMs\nto generate process supervision data at scale, we overcome previous\nlimitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms\nproprietary reasoning and non-reasoning models, achieving state-of-the-art\nperformance on two key evaluations: (1) distinguishing gold-standard from\nerror-containing samples with 98.8% accuracy, and (2) selecting\nphysician-preferred clinical notes with 56.2% accuracy. We investigate critical\ncomponents for effective PRM training, including optimal loss functions and\ndata selection strategies, and present a comprehensive physician reader study\nidentifying predictors of downstream Best-of-N performance. Our study sheds\nlight on unlocking the potential of PRMs for diverse generative tasks across\ndomains.", "AI": {"tldr": "This paper presents a framework for training process-supervised reward models (PRMs) to evaluate LLM-generated clinical notes, achieving state-of-the-art performance in distinguishing high-quality outputs.", "motivation": "To address the limitations of applying reward models in clinical note generation where ground-truth answers aren't available.", "method": "The framework involves defining meaningful steps, introducing realistic errors, and leveraging LLMs to generate supervision data at scale, focusing on optimal loss functions and data selection strategies for training PRMs.", "result": "The PRM built on LLaMA-3.1 8B outperforms existing models, achieving 98.8% accuracy in distinguishing gold-standard samples and 56.2% accuracy in selecting preferred clinical notes.", "conclusion": "The study demonstrates that PRMs can effectively serve various generative tasks by identifying key factors that influence performance, opening new paths for applications in health informatics.", "key_contributions": ["Novel framework for training PRMs in clinical settings", "Improved accuracy in evaluating LLM-generated clinical notes", "Insights into training PRMs using optimal loss functions and data strategies"], "limitations": "", "keywords": ["Process-supervised reward models", "clinical note generation", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.12761", "pdf": "https://arxiv.org/pdf/2412.12761.pdf", "abs": "https://arxiv.org/abs/2412.12761", "title": "Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection", "authors": ["Debajyoti Mazumder", "Aakash Kumar", "Jasabanta Patro"], "categories": ["cs.CL", "cs.AI"], "comment": "33 pages; EMNLP 2025 (Findings)", "summary": "In this paper, we reported our experiments with various strategies to improve\ncode-mixed humour and sarcasm detection. Particularly, we tried three\napproaches: (i) native sample mixing, (ii) multi-task learning (MTL), and (iii)\nprompting and instruction finetuning very large multilingual language models\n(VMLMs). In native sample mixing, we added monolingual task samples to\ncode-mixed training sets. In MTL learning, we relied on native and code-mixed\nsamples of a semantically related task (hate detection in our case). Finally,\nin our third approach, we evaluated the efficacy of VMLMs via few-shot context\nprompting and instruction finetuning. Some interesting findings we got are (i)\nadding native samples improved humor (raising the F1-score up to 6.76%) and\nsarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an\nMTL framework boosted performance for both humour (raising the F1-score up to\n10.67%) and sarcasm (increment up to 12.35% in F1-score) detection, and (iii)\nprompting and instruction finetuning VMLMs couldn't outperform the other\napproaches. Finally, our ablation studies and error analysis discovered the\ncases where our model is yet to improve. We provided our code for\nreproducibility.", "AI": {"tldr": "The paper explores strategies for improving code-mixed humor and sarcasm detection through native sample mixing, multi-task learning, and prompting with VMLMs, revealing that native sample mixing and MTL significantly enhance detection performance.", "motivation": "The need to enhance the detection of humor and sarcasm in code-mixed languages, which poses specific challenges in natural language processing.", "method": "Utilized three approaches: native sample mixing by adding monolingual samples, multi-task learning (MTL) with related tasks, and evaluation of very large multilingual language models (VMLMs) through few-shot prompting and instruction finetuning.", "result": "Native sample mixing improved humor and sarcasm detection F1-scores by up to 6.76% and 8.64%, respectively. MTL further boosted performance, achieving up to 10.67% and 12.35% improvements. VMLM approaches did not outperform the other methods.", "conclusion": "Native sample mixing and MTL are effective strategies for enhancing humor and sarcasm detection, while VMLM methods require further development.", "key_contributions": ["Introduction of native sample mixing for code-mixed tasks.", "Demonstration of multi-task learning impact on humor and sarcasm detection.", "Provision of code for reproducibility of study findings."], "limitations": "The study highlights remaining areas for improvement in the model's performance.", "keywords": ["code-mixed", "humor detection", "sarcasm detection", "multi-task learning", "multilingual models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.17727", "pdf": "https://arxiv.org/pdf/2412.17727.pdf", "abs": "https://arxiv.org/abs/2412.17727", "title": "Knowledge Editing through Chain-of-Thought", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yichen Tang", "Yiqun Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing is a technique that updates large language models (LLMs)\nwith new information to maintain their world knowledge. This approach avoids\nthe need to rebuild the model from scratch, thereby addressing the high costs\nassociated with frequent retraining. Among these, the in-context editing\nparadigm stands out for its effectiveness in integrating new knowledge while\npreserving the model's original capabilities. Despite its potential, existing\nin-context knowledge editing methods are often task-specific, focusing\nprimarily on multi-hop QA tasks using structured knowledge triples. Moreover,\ntheir reliance on few-shot prompting for task decomposition makes them unstable\nand less effective in generalizing across diverse tasks. In response to these\nlimitations, we propose EditCoT, a novel knowledge editing framework that\nflexibly and efficiently updates LLMs across various tasks without retraining.\nEditCoT works by generating a chain-of-thought (CoT) for a given input and then\niteratively refining this CoT process using a CoT editor based on updated\nknowledge. We evaluate EditCoT across a diverse range of benchmarks, covering\nmultiple languages and tasks. The results demonstrate that our approach\nachieves state-of-the-art performance while offering superior generalization,\neffectiveness, and stability compared to existing methods, marking a\nsignificant advancement in the field of knowledge updating. The code and data\nof EditCoT are available at: https://github.com/bebr2/EditCoT .", "AI": {"tldr": "EditCoT is a new framework for knowledge editing in LLMs that allows flexible updates without needing retraining, improving stability and generalization across various tasks.", "motivation": "To address the limitations of existing in-context knowledge editing methods, which are often task-specific and unstable when generalizing across diverse tasks.", "method": "EditCoT generates a chain-of-thought (CoT) for inputs and iteratively refines it using a CoT editor based on updated knowledge.", "result": "EditCoT achieves state-of-the-art performance on diverse benchmarks, demonstrating superior generalization, effectiveness, and stability compared to current methods.", "conclusion": "The proposed framework marks a significant advancement in knowledge updating for LLMs, avoiding the high costs of complete retraining while maintaining performance.", "key_contributions": ["Introduces EditCoT for flexible knowledge editing without retraining.", "Demonstrates state-of-the-art performance across multiple languages and tasks.", "Enhances stability and generalization compared to existing editing methods."], "limitations": "", "keywords": ["knowledge editing", "large language models", "chain-of-thought", "generalization", "stability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.09751", "pdf": "https://arxiv.org/pdf/2501.09751.pdf", "abs": "https://arxiv.org/abs/2501.09751", "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking", "authors": ["Zekun Xi", "Wenbiao Yin", "Jizhan Fang", "Jialong Wu", "Runnan Fang", "Jiang Yong", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "comment": "EMNLP 2025", "summary": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles. Code is available at\nhttps://github.com/zjunlp/OmniThink.", "AI": {"tldr": "OmniThink enhances machine writing by emulating human cognitive processes, improving content quality in long-form generation.", "motivation": "To improve the depth and novelty of content generated by large language models, which often suffer from redundancy and shallow outputs.", "method": "OmniThink employs a slow-thinking framework that iteratively expands and reflects on topics, simulating human-like learning processes.", "result": "OmniThink significantly increases the knowledge density of generated articles while maintaining coherence and depth, as validated by human evaluations.", "conclusion": "OmniThink has the potential to solve practical challenges in long-form content generation, providing richer and more original outputs.", "key_contributions": ["Introduction of the OmniThink framework for iterative expansion and reflection.", "Demonstration of improved knowledge density in generated articles.", "Human evaluation results supporting the effectiveness of the framework."], "limitations": "The performance and effectiveness may vary depending on the specific topic or domain.", "keywords": ["Machine Writing", "Large Language Models", "Cognitive Behavior", "Knowledge Density", "Content Generation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2501.15581", "pdf": "https://arxiv.org/pdf/2501.15581.pdf", "abs": "https://arxiv.org/abs/2501.15581", "title": "Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework", "authors": ["Yuhong Sun", "Zhangyue Yin", "Xuanjing Huang", "Xipeng Qiu", "Hui Zhao"], "categories": ["cs.CL"], "comment": "28 pages, 10 figures, accepted by Findings of EMNLP2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains. Math Word Problems (MWPs) serve as a crucial benchmark for\nevaluating LLMs' reasoning abilities. While most research primarily focuses on\nimproving accuracy, it often neglects understanding and addressing the\nunderlying patterns of errors. Current error classification methods rely on\nstatic and predefined categories, which limit their ability to capture the full\nspectrum of error patterns in mathematical reasoning. To enable systematic\nerror analysis, we collect error samples from 15 different LLMs of varying\nsizes across four distinct MWP datasets using multiple sampling strategies.\nBased on this extensive collection, we introduce MWPES-300K, a comprehensive\ndataset containing 304,865 error samples that cover diverse error patterns and\nreasoning paths. To reduce human bias and enable fine-grained analysis of error\npatterns, we propose a novel framework for automated dynamic error\nclassification in mathematical reasoning. Experimental results demonstrate that\ndataset characteristics significantly shape error patterns, which evolve from\nbasic to complex manifestations as model capabilities increase. With deeper\ninsights into error patterns, we propose Error-Aware Prompting (EAP) that\nincorporates common error patterns as explicit guidance, leading to significant\nimprovements in mathematical reasoning performance.", "AI": {"tldr": "This paper introduces MWPES-300K, a dataset of 304,865 error samples from 15 different LLMs focused on Math Word Problems (MWPs), and proposes a framework for automated error classification to enhance mathematical reasoning performance.", "motivation": "To analyze and understand error patterns in LLMs' reasoning about Math Word Problems rather than solely focusing on accuracy improvements.", "method": "The authors collect error samples from multiple LLMs across four MWP datasets and propose a novel framework for dynamic error classification to reduce human bias and enhance analysis.", "result": "The experimental results indicate that error patterns vary significantly based on dataset characteristics and model capabilities, with a structured Error-Aware Prompting approach leading to improved performance in mathematical reasoning tasks.", "conclusion": "By providing deeper insights into error patterns and utilizing error-aware prompting, this research proposes a pathway to enhance LLM performance in mathematical reasoning.", "key_contributions": ["Introduction of MWPES-300K, a large-scale error sample dataset for MWPs.", "Development of a dynamic error classification framework for better analysis of LLMs' reasoning errors.", "Proposal of Error-Aware Prompting to improve mathematical reasoning performance."], "limitations": "The reliance on specific MWPs may limit generalizability to all mathematical reasoning tasks.", "keywords": ["Large Language Models", "Math Word Problems", "Error Classification", "Dynamic Error Analysis", "Error-Aware Prompting"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.05467", "pdf": "https://arxiv.org/pdf/2502.05467.pdf", "abs": "https://arxiv.org/abs/2502.05467", "title": "Position: LLMs Can be Good Tutors in English Education", "authors": ["Jingheng Ye", "Shen Wang", "Deqing Zou", "Yibo Yan", "Kun Wang", "Hai-Tao Zheng", "Ruitong Liu", "Zenglin Xu", "Irwin King", "Philip S. Yu", "Qingsong Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Main. 20 pages, 4 figures", "summary": "While recent efforts have begun integrating large language models (LLMs) into\nEnglish education, they often rely on traditional approaches to learning tasks\nwithout fully embracing educational methodologies, thus lacking adaptability to\nlanguage learning. To address this gap, we argue that LLMs have the potential\nto serve as effective tutors in English Education. Specifically, LLMs can play\nthree critical roles: (1) as data enhancers, improving the creation of learning\nmaterials or serving as student simulations; (2) as task predictors, serving as\nlearner assessment or optimizing learning pathway; and (3) as agents, enabling\npersonalized and inclusive education. We encourage interdisciplinary research\nto explore these roles, fostering innovation while addressing challenges and\nrisks, ultimately advancing English Education through the thoughtful\nintegration of LLMs.", "AI": {"tldr": "This paper discusses the integration of large language models (LLMs) in English education, highlighting their roles as data enhancers, task predictors, and agents for personalized learning.", "motivation": "To address the lack of adaptability in traditional approaches to language learning when utilizing LLMs in education.", "method": "The paper argues for the potential of LLMs to serve as effective tutors by defining their roles in enhancing data, predicting tasks, and acting as agents for personalized education.", "result": "LLMs can significantly improve the process of English education by customizing learning experiences and enhancing educational methodologies.", "conclusion": "The thoughtful integration of LLMs into educational practices can foster innovation and address challenges in English education.", "key_contributions": ["Identification of three critical roles of LLMs in English education.", "Suggestion for interdisciplinary research to improve LLM-based education.", "Promotion of personalized and inclusive education through LLMs."], "limitations": "", "keywords": ["large language models", "English education", "personalized learning", "task prediction", "educational methodologies"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.05759", "pdf": "https://arxiv.org/pdf/2502.05759.pdf", "abs": "https://arxiv.org/abs/2502.05759", "title": "Reinforced Lifelong Editing for Language Models", "authors": ["Zherui Li", "Houcheng Jiang", "Hao Chen", "Baolong Bi", "Zhenhong Zhou", "Fei Sun", "Junfeng Fang", "Xiang Wang"], "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit.", "AI": {"tldr": "RLEdit is a novel reinforcement learning-based method for lifelong editing of large language models, outperforming existing methods in efficiency and effectiveness.", "motivation": "Addressing the challenge of keeping large language models' knowledge accurate and up-to-date through efficient editing methods that do not require retraining.", "method": "RLEdit treats editing losses as rewards and optimizes hypernetwork parameters across the entire knowledge sequence, aligning with reinforcement learning principles.", "result": "RLEdit outperforms existing editing methods, achieving a 59.24% improvement in effectiveness while only requiring 2.11% of the time compared to traditional approaches.", "conclusion": "The proposed RLEdit method offers a significantly more effective and efficient solution for lifelong editing of large language models.", "key_contributions": ["Introduction of RLEdit for lifelong model editing using reinforcement learning principles.", "Demonstrated significant improvements over existing hypernetwork-based editing methods.", "Provided an extensive empirical evaluation showing effectiveness and efficiency gains."], "limitations": "", "keywords": ["Large Language Models", "Model Editing", "Reinforcement Learning", "Hypernetworks", "Lifelong Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11689", "pdf": "https://arxiv.org/pdf/2502.11689.pdf", "abs": "https://arxiv.org/abs/2502.11689", "title": "Improve LLM-as-a-Judge Ability as a General Ability", "authors": ["Jiachen Yu", "Shaoning Sun", "Xiaohui Hu", "Jiaxu Yan", "Kaidong Yu", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge leverages the generative and reasoning capabilities of large\nlanguage models (LLMs) to evaluate LLM responses across diverse scenarios,\nproviding accurate preference signals. This approach plays a vital role in\naligning LLMs with human values, ensuring ethical and reliable AI outputs that\nalign with societal norms. Recent studies have raised many methods to train LLM\nas generative judges, but most of them are data consuming or lack accuracy, and\nonly focus on LLM's judge ability. In this work, we regard judge ability as a\ngeneral ability of LLM and implement a two-stage training approach, comprising\nsupervised fine-tuning (SFT) warm-up and direct preference optimization (DPO)\nenhancement, to achieve judge style adaptation and improve judgment accuracy.\nAdditionally, we introduce an efficient data synthesis method to generate\njudgmental content. Experimental results demonstrate that our approach,\nutilizing only about 2% to 40% of the data required by other methods, achieves\nSOTA performance on RewardBench. Furthermore, our training method enhances the\ngeneral capabilities of the model by constructing complicated judge task, and\nthe judge signals provided by our model have significantly enhanced the\ndownstream DPO training performance of our internal models in our test to\noptimize policy model with Judge Model. We also open-source our model weights\nand training data to facilitate further research.", "AI": {"tldr": "This paper presents LLM-as-a-Judge, a two-stage training approach to enhance the judgment capabilities of large language models (LLMs) while optimizing data usage and aligning outputs with human values.", "motivation": "To improve the alignment of LLM responses with human values and societal norms, ensuring ethical and reliable AI outputs by enhancing the judgment capability of LLMs.", "method": "A two-stage training approach consisting of supervised fine-tuning (SFT) warm-up followed by direct preference optimization (DPO) enhancement, along with an efficient data synthesis method for generating judgmental content.", "result": "Achieves state-of-the-art performance on RewardBench using only 2% to 40% of the data required by other methods, while also enhancing the general capabilities of the model.", "conclusion": "The proposed training method significantly improves judgment accuracy and has a positive impact on downstream optimization tasks; the model's weights and training data are open-sourced for further research.", "key_contributions": ["Introduction of a two-stage training approach for LLMs", "Development of an efficient data synthesis method", "Open-sourcing of model weights and training data to aid in future research"], "limitations": "", "keywords": ["large language models", "judgment capability", "ethical AI", "preference optimization", "data efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.15836", "pdf": "https://arxiv.org/pdf/2502.15836.pdf", "abs": "https://arxiv.org/abs/2502.15836", "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models", "authors": ["Haokun Chen", "Sebastian Szyller", "Weilin Xu", "Nageen Himayat"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) are trained using massive datasets, which often\ncontain undesirable content such as harmful texts, personal information, and\ncopyrighted material. To address this, machine unlearning aims to remove\ninformation from trained models. Recent work has shown that soft token attacks\n(STA) can successfully extract unlearned information from LLMs, but in this\nwork we show that STAs can be an inadequate tool for auditing unlearning. Using\ncommon benchmarks such as Who Is Harry Potter? and TOFU, we demonstrate that in\na strong auditor setting such attacks can elicit any information from the LLM,\nregardless of the deployed unlearning algorithm or whether the queried content\nwas originally present in the training corpus. We further show that STA with\njust a few soft tokens (1-10) can elicit random strings over 400 characters\nlong, indicating that STAs must be used carefully to effectively audit\nunlearning. Example code can be found at:\nhttps://github.com/IntelLabs/LLMart/tree/main/examples/unlearning", "AI": {"tldr": "This paper critiques the effectiveness of soft token attacks (STAs) for auditing unlearning in large language models (LLMs), showing they can retrieve information regardless of unlearning efforts.", "motivation": "To investigate the reliability of soft token attacks (STAs) as tools for auditing the effectiveness of machine unlearning in large language models (LLMs).", "method": "The authors employed common benchmarks such as Who Is Harry Potter? and TOFU to evaluate the performance of STAs in a strong auditor setting, examining how they interact with unlearning algorithms.", "result": "The results demonstrate that STAs can extract arbitrary information from LLMs despite the implementation of unlearning techniques, raising concerns about their suitability as auditing tools.", "conclusion": "STAs must be applied with caution due to their capability of eliciting extended random strings, underlining potential inadequacies in ensuring effective unlearning within LLMs.", "key_contributions": ["Critique of soft token attacks for auditing unlearning in LLMs", "Evidence showing STAs can retrieve unwanted information regardless of unlearning", "Demonstration of STAs eliciting random strings that overshadow their intended function."], "limitations": "The study primarily explores STAs without extensive evaluation of alternative auditing methods for unlearning.", "keywords": ["machine unlearning", "soft token attacks", "large language models", "auditing", "LLMs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.16699", "pdf": "https://arxiv.org/pdf/2502.16699.pdf", "abs": "https://arxiv.org/abs/2502.16699", "title": "Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations", "authors": ["Mansour Al Ghanim", "Jiaqi Xue", "Rochana Prih Hastuti", "Mengxin Zheng", "Yan Solihin", "Qian Lou"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by EMNLP 2025 Finding", "summary": "We present a study to benchmark representative watermarking methods in\ncross-lingual settings. The current literature mainly focuses on the evaluation\nof watermarking methods for the English language. However, the literature for\nevaluating watermarking in cross-lingual settings is scarce. This results in\noverlooking important adversary scenarios in which a cross-lingual adversary\ncould be in, leading to a gray area of practicality over cross-lingual\nwatermarking. In this paper, we evaluate four watermarking methods in four\ndifferent and vocabulary rich languages. Our experiments investigate the\nquality of text under different watermarking procedure and the detectability of\nwatermarks with practical translation attack scenarios. Specifically, we\ninvestigate practical scenarios that an adversary with cross-lingual knowledge\ncould take, and evaluate whether current watermarking methods are suitable for\nsuch scenarios. Finally, from our findings, we draw key insights about\nwatermarking in cross-lingual settings.", "AI": {"tldr": "This paper benchmarks watermarking methods in cross-lingual contexts, addressing the lack of evaluation in this area.", "motivation": "The paper aims to fill the gap in literature regarding the evaluation of watermarking methods in cross-lingual settings, where much of the existing research has focused on English. This research is crucial for understanding the effectiveness of watermarking techniques against cross-lingual adversaries.", "method": "The authors evaluate four watermarking methods across four languages, examining the quality of the watermarked text and the detectability of the watermarks during practical translation attacks.", "result": "The study reveals the effectiveness and limitations of current watermarking methods when facing adversaries who possess cross-lingual knowledge.", "conclusion": "The findings underscore the necessity of improving watermarking techniques to ensure their robustness in cross-lingual adversarial scenarios.", "key_contributions": ["Benchmarking watermarking methods in multiple languages", "Identifying challenges in watermark detectability in cross-lingual contexts", "Providing insights into practical adversary scenarios for watermarking"], "limitations": "The study primarily focuses on four languages, and further research may be needed to assess effectiveness in other languages or dialects.", "keywords": ["watermarking", "cross-lingual", "adversarial scenarios", "EMNLP 2025", "language evaluation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.08890", "pdf": "https://arxiv.org/pdf/2503.08890.pdf", "abs": "https://arxiv.org/abs/2503.08890", "title": "PlainQAFact: Retrieval-augmented Factual Consistency Evaluation Metric for Biomedical Plain Language Summarization", "authors": ["Zhiwen You", "Yue Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucinated outputs from large language models (LLMs) pose risks in the\nmedical domain, especially for lay audiences making health-related decisions.\nExisting automatic factual consistency evaluation methods, such as entailment-\nand question-answering (QA) -based, struggle with plain language summarization\n(PLS) due to elaborative explanation phenomenon, which introduces external\ncontent (e.g., definitions, background, examples) absent from the scientific\nabstract to enhance comprehension. To address this, we introduce PlainQAFact,\nan automatic factual consistency evaluation metric trained on a fine-grained,\nhuman-annotated dataset PlainFact, for evaluating factual consistency of both\nsource-simplified and elaborately explained sentences. PlainQAFact first\nclassifies sentence type, then applies a retrieval-augmented QA scoring method.\nEmpirical results show that existing evaluation metrics fail to evaluate the\nfactual consistency in PLS, especially for elaborative explanations, whereas\nPlainQAFact consistently outperforms them across all evaluation settings. We\nfurther analyze PlainQAFact's effectiveness across external knowledge sources,\nanswer extraction strategies, answer overlap measures, and document granularity\nlevels, refining its overall factual consistency assessment. Taken together,\nour work presents the first evaluation metric designed for PLS factual\nconsistency evaluation, providing the community with both a robust benchmark\nand a practical tool to advance reliable and safe plain language communication\nin the medical domain. PlainQAFact and PlainFact are available at:\nhttps://github.com/zhiwenyou103/PlainQAFact", "AI": {"tldr": "Introducing PlainQAFact, a new metric for evaluating factual consistency in plain language summaries, specifically in the medical domain.", "motivation": "To address the risks posed by hallucinated outputs from large language models in medical communications and improve comprehension for lay audiences.", "method": "PlainQAFact classifies sentence types and utilizes a retrieval-augmented QA scoring method to assess factual consistency.", "result": "PlainQAFact consistently outperforms existing metrics in evaluating factual consistency in plain language summaries across various settings.", "conclusion": "PlainQAFact serves as the first evaluation metric designed for plain language summarization, providing a benchmark and practical tool for reliable medical communication.", "key_contributions": ["Introduction of PlainQAFact as a novel evaluation metric.", "Development of a fine-grained, human-annotated dataset (PlainFact).", "Demonstration of superior performance in factual consistency evaluation compared to existing metrics."], "limitations": "", "keywords": ["factual consistency", "large language models", "plain language summarization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.22828", "pdf": "https://arxiv.org/pdf/2503.22828.pdf", "abs": "https://arxiv.org/abs/2503.22828", "title": "Learning to Reason for Long-Form Story Generation", "authors": ["Alexander Gurung", "Mirella Lapata"], "categories": ["cs.CL"], "comment": null, "summary": "Generating high-quality stories spanning thousands of tokens requires\ncompetency across a variety of skills, from tracking plot and character arcs to\nkeeping a consistent and engaging style. Due to the difficulty of sourcing\nlabeled datasets and precise quality measurements, most work using large\nlanguage models (LLMs) for long-form story generation uses combinations of\nhand-designed prompting techniques to elicit author-like behavior. This is a\nmanual process that is highly dependent on the specific story-generation task.\nMotivated by the recent success of applying RL with Verifiable Rewards to\ndomains like math and coding, we propose a general story-generation task\n(Next-Chapter Prediction) and a reward formulation (Verified Rewards via\nCompletion Likelihood Improvement) that allows us to use an unlabeled book\ndataset as a learning signal for reasoning. We learn to reason over a story's\ncondensed information and generate a detailed plan for the next chapter. Our\nreasoning is evaluated via the chapters it helps a story-generator create, and\ncompared against non-trained and supervised finetuning (SFT) baselines.\nPairwise human judgments reveal the chapters our learned reasoning produces are\npreferred across almost all metrics, and the effect is more pronounced in Scifi\nand Fantasy genres.", "AI": {"tldr": "This paper proposes a method for long-form story generation using RL with Verified Rewards, leveraging an unlabeled book dataset to enhance reasoning for next-chapter predictions.", "motivation": "The paper addresses the challenges of generating high-quality long-form stories by moving beyond manual prompting techniques, particularly for story generation tasks.", "method": "The authors introduce a method called Next-Chapter Prediction and a reward formulation based on Verified Rewards via Completion Likelihood Improvement, utilizing an unlabeled book dataset to learn reasoning for story coherence.", "result": "Their approach yields chapters that are preferred by human judges over those generated using traditional supervised fine-tuning methods, showing especially strong performance in Scifi and Fantasy genres.", "conclusion": "The findings suggest that RL-based methods can effectively improve the quality of generated narratives, particularly for long-form storytelling.", "key_contributions": ["Introduction of Next-Chapter Prediction for story generation", "Utilization of Verified Rewards for training on unlabeled datasets", "Demonstration of improved narrative quality through pairwise human judgments"], "limitations": "", "keywords": ["story generation", "reinforcement learning", "natural language processing", "large language models", "narrative coherence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.03624", "pdf": "https://arxiv.org/pdf/2504.03624.pdf", "abs": "https://arxiv.org/abs/2504.03624", "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aarti Basant", "Abhinav Khattar", "Adithya Renduchintala", "Akhiad Bercovich", "Aleksander Ficek", "Alexis Bjorlin", "Ali Taghibakhshi", "Amala Sanjay Deshmukh", "Ameya Sunil Mahabaleshwarkar", "Andrew Tao", "Anna Shors", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Bobby Chen", "Boris Ginsburg", "Boxin Wang", "Brandon Norick", "Brian Butterfield", "Bryan Catanzaro", "Carlo del Mundo", "Chengyu Dong", "Christine Harvey", "Christopher Parisien", "Dan Su", "Daniel Korzekwa", "Danny Yin", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Denys Fridman", "Dima Rekesh", "Ding Ma", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Eileen Long", "Elad Segal", "Ellie Evans", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Ewa Dobrowolska", "Fei Jia", "Fuxiao Liu", "Gargi Prasad", "Gerald Shen", "Guilin Liu", "Guo Chen", "Haifeng Qian", "Helen Ngo", "Hongbin Liu", "Hui Li", "Igor Gitman", "Ilia Karmanov", "Ivan Moshkov", "Izik Golan", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jarno Seppanen", "Jason Lu", "Jason Sewall", "Jiaqi Zeng", "Jiaxuan You", "Jimmy Zhang", "Jing Zhang", "Jining Huang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jon Barker", "Jonathan Cohen", "Joseph Jennings", "Jupinder Parmar", "Karan Sapra", "Kari Briski", "Kateryna Chumachenko", "Katherine Luna", "Keshav Santhanam", "Kezhi Kong", "Kirthi Sivamani", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Lawrence McAfee", "Leon Derczynski", "Lindsey Pavao", "Luis Vega", "Lukas Voegtle", "Maciej Bala", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matthieu Le", "Matvei Novikov", "Mehrzad Samadi", "Michael Andersch", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mike Ranzinger", "Mikolaj Blaz", "Misha Smelyanskiy", "Mohamed Fawzy", "Mohammad Shoeybi", "Mostofa Patwary", "Nayeon Lee", "Nima Tajbakhsh", "Ning Xu", "Oleg Rybakov", "Oleksii Kuchaiev", "Olivier Delalleau", "Osvald Nitski", "Parth Chadha", "Pasha Shamis", "Paulius Micikevicius", "Pavlo Molchanov", "Peter Dykas", "Philipp Fischer", "Pierre-Yves Aquilanti", "Piotr Bialecki", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi", "Rahul Kandu", "Ran El-Yaniv", "Raviraj Joshi", "Roger Waleffe", "Ruoxi Zhang", "Sabrina Kavanaugh", "Sahil Jain", "Samuel Kriman", "Sangkug Lym", "Sanjeev Satheesh", "Saurav Muralidharan", "Sean Narenthiran", "Selvaraj Anandaraj", "Seonmyeong Bak", "Sergey Kashirsky", "Seungju Han", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Sharon Clay", "Shelby Thomas", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shyamala Prayaga", "Siddhartha Jain", "Sirshak Das", "Slawek Kierat", "Somshubra Majumdar", "Song Han", "Soumye Singhal", "Sriharsha Niverty", "Stefania Alborghetti", "Suseella Panguluri", "Swetha Bhendigeri", "Syeda Nahida Akter", "Szymon Migacz", "Tal Shiri", "Terry Kong", "Timo Roman", "Tomer Ronen", "Trisha Saar", "Tugrul Konuk", "Tuomas Rintamaki", "Tyler Poon", "Ushnish De", "Vahid Noroozi", "Varun Singh", "Vijay Korthikanti", "Vitaly Kurin", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenliang Dai", "Wonmin Byeon", "Xiaowei Ren", "Yao Xu", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yoshi Suhara", "Zhiding Yu", "Zhiqi Li", "Zhiyu Li", "Zhongbo Zhu", "Zhuolin Yang", "Zijia Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. We are releasing Nemotron-H base model\ncheckpoints with support in Hugging Face and NeMo.", "AI": {"tldr": "Introduction of Nemotron-H, a family of hybrid Mamba-Transformer models optimized for efficient inference with comparable accuracy to existing models.", "motivation": "To enhance reasoning capabilities while reducing inference costs for Transformer models.", "method": "Implementation of Mamba layers replacing self-attention layers in the Transformer architecture and development of a new compression technique called MiniPuzzle.", "result": "Nemotron-H models demonstrate either improved or on-par accuracy compared to current state-of-the-art models while achieving up to 3× faster inference speeds.", "conclusion": "Nemotron-H-47B-Base, a variant of the 56B model, is developed for efficiency, achieving similar accuracy with 20% faster inference; new FP8 training recipe yields comparable results to BF16.", "key_contributions": ["Introduction of hybrid Mamba-Transformer architecture", "Development of MiniPuzzle compression technique for models", "Release of model checkpoints with integration to popular frameworks."], "limitations": "", "keywords": ["Transformer", "inference-time scaling", "Mamba layers", "Model compression", "FP8 training"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.22823", "pdf": "https://arxiv.org/pdf/2505.22823.pdf", "abs": "https://arxiv.org/abs/2505.22823", "title": "Self-Critique and Refinement for Faithful Natural Language Explanations", "authors": ["Yingming Wang", "Pepa Atanasova"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "With the rapid development of Large Language Models (LLMs), Natural Language\nExplanations (NLEs) have become increasingly important for understanding model\npredictions. However, these explanations often fail to faithfully represent the\nmodel's actual reasoning process. While existing work has demonstrated that\nLLMs can self-critique and refine their initial outputs for various tasks, this\ncapability remains unexplored for improving explanation faithfulness. To\naddress this gap, we introduce Self-critique and Refinement for Natural\nLanguage Explanations (SR-NLE), a framework that enables models to improve the\nfaithfulness of their own explanations -- specifically, post-hoc NLEs --\nthrough an iterative critique and refinement process without external\nsupervision. Our framework leverages different feedback mechanisms to guide the\nrefinement process, including natural language self-feedback and, notably, a\nnovel feedback approach based on feature attribution that highlights important\ninput words. Our experiments across three datasets and four state-of-the-art\nLLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with\nour best method achieving an average unfaithfulness rate of 36.02%, compared to\n54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal\nthat the investigated LLMs can indeed refine their explanations to better\nreflect their actual reasoning process, requiring only appropriate guidance\nthrough feedback without additional training or fine-tuning.", "AI": {"tldr": "The paper introduces SR-NLE, a framework for LLMs to improve the faithfulness of their natural language explanations through an iterative self-critique and refinement process, reducing unfaithfulness rates significantly without external supervision.", "motivation": "To improve the faithfulness of natural language explanations generated by large language models, which often do not accurately represent the model's reasoning processes.", "method": "The SR-NLE framework utilizes iterative critique and refinement mechanisms, including natural language self-feedback and feature attribution feedback, to enhance explanation quality.", "result": "SR-NLE significantly reduces unfaithfulness rates across three datasets and four state-of-the-art LLMs, achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline models.", "conclusion": "The framework shows that LLMs can refine their explanations to better match their reasoning processes with appropriate guidance, without needing additional training or fine-tuning.", "key_contributions": ["Introduction of the SR-NLE framework for self-critique and refinement of explanations.", "Demonstration of significant reduction in unfaithfulness rates in LLM explanations.", "Utilization of novel feedback mechanisms, including feature attribution to improve explanation quality."], "limitations": "", "keywords": ["Large Language Models", "Natural Language Explanations", "Self-critique", "Refinement", "Feature Attribution"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.07642", "pdf": "https://arxiv.org/pdf/2506.07642.pdf", "abs": "https://arxiv.org/abs/2506.07642", "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "authors": ["Yuan Chang", "Ziyue Li", "Hengyuan Zhang", "Yuanbo Kong", "Yanru Wu", "Hayden Kwok-Hay So", "Zhijiang Guo", "Liya Zhu", "Ngai Wong"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Main", "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.", "AI": {"tldr": "TreeReview is a framework that enhances peer review by modeling it as a hierarchical question-answering process, overcoming limitations in LLMs for generating detailed reviews.", "motivation": "Current LLM methods for peer review lack thoroughness and efficiency, making it challenging to produce insightful evaluations.", "method": "TreeReview constructs a tree of review questions and resolves them iteratively, incorporating dynamic question expansion for deeper probing.", "result": "TreeReview shows significant improvement in generating comprehensive review feedback while reducing LLM token usage by up to 80% compared to traditional methods.", "conclusion": "The method outperforms strong baselines, demonstrating its effectiveness in providing expert-aligned review feedback in a more efficient manner.", "key_contributions": ["Introduction of a hierarchical question-answering model for peer review", "Dynamic question expansion for deeper probing", "Significant reduction in LLM token usage during review generation"], "limitations": "", "keywords": ["Large Language Models", "peer review", "hierarchical question-answering"], "importance_score": 9, "read_time_minutes": 8}}
