{"id": "2205.02225", "pdf": "https://arxiv.org/pdf/2205.02225.pdf", "abs": "https://arxiv.org/abs/2205.02225", "title": "HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction", "authors": ["Shuliang Liu", "Xuming Hu", "Chenwei Zhang", "Shu`ang Li", "Lijie Wen", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "In NAACL 2022 as a long paper. Code and data available at\n  https://github.com/THU-BPM/HiURE", "summary": "Unsupervised relation extraction aims to extract the relationship between\nentities from natural language sentences without prior information on\nrelational scope or distribution. Existing works either utilize self-supervised\nschemes to refine relational feature signals by iteratively leveraging adaptive\nclustering and classification that provoke gradual drift problems, or adopt\ninstance-wise contrastive learning which unreasonably pushes apart those\nsentence pairs that are semantically similar. To overcome these defects, we\npropose a novel contrastive learning framework named HiURE, which has the\ncapability to derive hierarchical signals from relational feature space using\ncross hierarchy attention and effectively optimize relation representation of\nsentences under exemplar-wise contrastive learning. Experimental results on two\npublic datasets demonstrate the advanced effectiveness and robustness of HiURE\non unsupervised relation extraction when compared with state-of-the-art models.", "AI": {"tldr": "This paper introduces HiURE, a novel contrastive learning framework aimed at improving unsupervised relation extraction by leveraging hierarchical signals and exemplar-wise contrastive learning.", "motivation": "To address the limitations of existing unsupervised relation extraction methods that suffer from gradual drift problems and unreasonable distance constraints between similar entities.", "method": "HiURE employs a contrastive learning approach with cross hierarchy attention to derive hierarchical signals from relational feature space for better optimization of relation representations in sentences.", "result": "HiURE demonstrates superior effectiveness and robustness on two public datasets compared to state-of-the-art models in the field of unsupervised relation extraction.", "conclusion": "The proposed HiURE framework enhances unsupervised relation extraction by addressing known deficiencies in previous methodologies and yielding improved performance metrics.", "key_contributions": ["Introduction of a novel hierarchical approach for relation extraction", "Implementation of exemplar-wise contrastive learning", "Demonstration of superior performance on benchmark datasets"], "limitations": "", "keywords": ["unsupervised relation extraction", "contrastive learning", "hierarchical signals"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2402.13534", "pdf": "https://arxiv.org/pdf/2402.13534.pdf", "abs": "https://arxiv.org/abs/2402.13534", "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling", "authors": ["Xuemei Tang", "Jun Wang", "Qi Su", "Chu-ren Huang", "Jinghang Gu"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 9 tables, 3 figures, Accepted by ACL 2025 (short paper)", "summary": "Sequence labeling models often benefit from incorporating external knowledge.\nHowever, this practice introduces data heterogeneity and complicates the model\nwith additional modules, leading to increased expenses for training a\nhigh-performing model. To address this challenge, we propose a two-stage\ncurriculum learning (TCL) framework specifically designed for sequence labeling\ntasks. The TCL framework enhances training by gradually introducing data\ninstances from easy to hard, aiming to improve both performance and training\nspeed. Furthermore, we explore different metrics for assessing the difficulty\nlevels of sequence labeling tasks. Through extensive experimentation on six\nChinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we\ndemonstrate the effectiveness of our model in enhancing the performance of\nsequence labeling models. Additionally, our analysis indicates that TCL\naccelerates training and alleviates the slow training problem associated with\ncomplex models.", "AI": {"tldr": "This paper presents a two-stage curriculum learning framework for sequence labeling tasks that improves performance and speeds up training by gradually introducing data from easy to hard.", "motivation": "To enhance sequence labeling performance while managing data heterogeneity and training costs associated with incorporating external knowledge.", "method": "A two-stage curriculum learning (TCL) framework is proposed for sequence labeling tasks, progressively introducing data instances based on difficulty levels.", "result": "Experiments show that the TCL framework improves performance on six Chinese word segmentation and part-of-speech tagging datasets, while also speeding up the training process.", "conclusion": "The TCL framework effectively enhances the performance of sequence labeling models and accelerates training, addressing challenges posed by complex model architectures.", "key_contributions": ["Development of a two-stage curriculum learning framework for sequence labeling tasks", "Demonstration of improved performance and faster training on multiple datasets", "Introduction of metrics for evaluating task difficulty levels."], "limitations": "", "keywords": ["sequence labeling", "curriculum learning", "machine learning", "natural language processing", "training speed"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2406.03847", "pdf": "https://arxiv.org/pdf/2406.03847.pdf", "abs": "https://arxiv.org/abs/2406.03847", "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems", "authors": ["Huaiyuan Ying", "Zijian Wu", "Yihan Geng", "Zheng Yuan", "Dahua Lin", "Kai Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.", "AI": {"tldr": "The paper presents a synthetic data generation pipeline to enhance LLMs' capabilities in translating mathematical problems into formal languages, addressing the shortage of training data in math theorem proving.", "motivation": "The motivation is to improve the performance of large language models (LLMs) in solving mathematical problems using formal languages like Lean, where existing training data is limited.", "method": "The authors propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements and vice versa.", "result": "The pipeline generates a dataset of approximately 57,000 formal-informal question pairs, enhancing LLM performance in translation and understanding of complex mathematical problems and proofs.", "conclusion": "The synthetic data approach effectively provides valuable training data that helps LLMs perform better in mathematical problem solving and theorem proving.", "key_contributions": ["Introduction of a synthetic data generation pipeline for LLMs in mathematical contexts", "Creation of a substantial dataset with 57K formal-informal pairs", "Open-sourcing of code and dataset for community use"], "limitations": "", "keywords": ["large language models", "synthetic data", "mathematics", "Lean 4", "theorem proving"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2407.09861", "pdf": "https://arxiv.org/pdf/2407.09861.pdf", "abs": "https://arxiv.org/abs/2407.09861", "title": "A Systematic Survey of Natural Language Processing for the Greek Language", "authors": ["Juli Bakagianni", "Kanella Pouli", "Maria Gavriilidou", "John Pavlopoulos"], "categories": ["cs.CL", "cs.AI"], "comment": "This version matches the paper published in Patterns (Cell Press).\n  The title has been updated to reflect the published version", "summary": "Comprehensive monolingual Natural Language Processing (NLP) surveys are\nessential for assessing language-specific challenges, resource availability,\nand research gaps. However, existing surveys often lack standardized\nmethodologies, leading to selection bias and fragmented coverage of NLP tasks\nand resources. This study introduces a generalizable framework for systematic\nmonolingual NLP surveys. Our approach integrates a structured search protocol\nto minimize bias, an NLP task taxonomy for classification, and language\nresource taxonomies to identify potential benchmarks and highlight\nopportunities for improving resource availability. We apply this framework to\nGreek NLP (2012-2023), providing an in-depth analysis of its current state,\ntask-specific progress, and resource gaps. The survey results are publicly\navailable (https://doi.org/10.5281/zenodo.15314882) and are regularly updated\nto provide an evergreen resource. This systematic survey of Greek NLP serves as\na case study, demonstrating the effectiveness of our framework and its\npotential for broader application to other not so well-resourced languages as\nregards NLP.", "AI": {"tldr": "The paper introduces a framework for conducting systematic monolingual NLP surveys, applied to Greek NLP, addressing selection bias and resource gaps.", "motivation": "To provide a standardized methodology for monolingual NLP surveys that assesses language-specific challenges and identifies resource availability.", "method": "Development of a structured search protocol and taxonomies for NLP tasks and resources to minimize bias and improve coverage.", "result": "The framework is successfully applied to Greek NLP, revealing task-specific progress and resource gaps, with results publicly available and regularly updated.", "conclusion": "The study demonstrates the framework's effectiveness for Greek NLP and suggests its applicability to other lesser-resourced languages.", "key_contributions": ["Generalizable framework for monolingual NLP surveys", "Structured search protocol to minimize bias", "Creation of NLP task taxonomy and resource identification methodologies"], "limitations": "", "keywords": ["Natural Language Processing", "monolingual surveys", "Greek NLP"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2407.11770", "pdf": "https://arxiv.org/pdf/2407.11770.pdf", "abs": "https://arxiv.org/abs/2407.11770", "title": "Robust Utility-Preserving Text Anonymization Based on Large Language Models", "authors": ["Tianyu Yang", "Xiaodan Zhu", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted by ACL'2025 Main Conference", "summary": "Anonymizing text that contains sensitive information is crucial for a wide\nrange of applications. Existing techniques face the emerging challenges of the\nre-identification ability of large language models (LLMs), which have shown\nadvanced capability in memorizing detailed information and reasoning over\ndispersed pieces of patterns to draw conclusions. When defending against\nLLM-based re-identification, anonymization could jeopardize the utility of the\nresulting anonymized data in downstream tasks. In general, the interaction\nbetween anonymization and data utility requires a deeper understanding within\nthe context of LLMs. In this paper, we propose a framework composed of three\nkey LLM-based components: a privacy evaluator, a utility evaluator, and an\noptimization component, which work collaboratively to perform anonymization.\nExtensive experiments demonstrate that the proposed model outperforms existing\nbaselines, showing robustness in reducing the risk of re-identification while\npreserving greater data utility in downstream tasks. We provide detailed\nstudies on these core modules. To consider large-scale and real-time\napplications, we investigate the distillation of the anonymization capabilities\ninto lightweight models. All of our code and datasets will be made publicly\navailable at https://github.com/UKPLab/acl2025-rupta.", "AI": {"tldr": "This paper presents a framework for anonymizing text that balances privacy and data utility in the context of LLMs.", "motivation": "Existing anonymization techniques struggle against LLMs' re-identification capability while aiming to maintain data utility for downstream tasks.", "method": "The framework consists of three components: a privacy evaluator, a utility evaluator, and an optimization component, working together to enhance anonymization techniques.", "result": "Experimental results show that the proposed framework outperforms existing methods in reducing re-identification risks and improving data utility.", "conclusion": "The study provides insights into the integration of anonymization processes with LLMs, emphasizing the dual focus on privacy and downstream task efficacy.", "key_contributions": ["Proposed a novel framework for anonymization using LLMs", "Demonstrated improved performance over existing anonymization methods", "Investigated lightweight models for real-time anonymization applications."], "limitations": "", "keywords": ["anonymization", "large language models", "data utility", "privacy evaluation", "real-time applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.15621", "pdf": "https://arxiv.org/pdf/2407.15621.pdf", "abs": "https://arxiv.org/abs/2407.15621", "title": "RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering", "authors": ["Soroosh Tayebi Arasteh", "Mahshad Lotfinia", "Keno Bressem", "Robert Siepmann", "Lisa Adams", "Dyke Ferber", "Christiane Kuhl", "Jakob Nikolas Kather", "Sven Nebelung", "Daniel Truhn"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in Radiology: Artificial Intelligence", "summary": "Large language models (LLMs) often generate outdated or inaccurate\ninformation based on static training datasets. Retrieval-augmented generation\n(RAG) mitigates this by integrating outside data sources. While previous RAG\nsystems used pre-assembled, fixed databases with limited flexibility, we have\ndeveloped Radiology RAG (RadioRAG), an end-to-end framework that retrieves data\nfrom authoritative radiologic online sources in real-time. We evaluate the\ndiagnostic accuracy of various LLMs when answering radiology-specific questions\nwith and without access to additional online information via RAG. Using 80\nquestions from the RSNA Case Collection across radiologic subspecialties and 24\nadditional expert-curated questions with reference standard answers, LLMs\n(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were\nprompted with and without RadioRAG in a zero-shot inference scenario RadioRAG\nretrieved context-specific information from Radiopaedia in real-time. Accuracy\nwas investigated. Statistical analyses were performed using bootstrapping. The\nresults were further compared with human performance. RadioRAG improved\ndiagnostic accuracy across most LLMs, with relative accuracy increases ranging\nup to 54% for different LLMs. It matched or exceeded non-RAG models and the\nhuman radiologist in question answering across radiologic subspecialties,\nparticularly in breast imaging and emergency radiology. However, the degree of\nimprovement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1\nsaw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,\nhighlighting variability in RadioRAG's effectiveness. LLMs benefit when\nprovided access to domain-specific data beyond their training data. RadioRAG\nshows potential to improve LLM accuracy and factuality in radiology question\nanswering by integrating real-time domain-specific data.", "AI": {"tldr": "This paper presents RadioRAG, an end-to-end framework that enhances the diagnostic accuracy of large language models (LLMs) in radiology by using real-time data retrieval from authoritative online sources.", "motivation": "To overcome the limitation of LLMs generating outdated or inaccurate information, this research aims to harness real-time data through a retrieval-augmented generation (RAG) framework specifically designed for radiology.", "method": "The study involved prompting various LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3) with and without the RadioRAG framework using 80 radiology-specific questions to investigate accuracy improvements.", "result": "RadioRAG improved the diagnostic accuracy of most tested LLMs, with relative accuracy increases up to 54%, and matched or exceeded the performance of human radiologists in certain radiologic subspecialties, notably in breast imaging and emergency radiology.", "conclusion": "Integrating real-time domain-specific data through RadioRAG significantly enhances the factual accuracy of LLMs in radiology, demonstrating the effectiveness of RAG in medical applications.", "key_contributions": ["Development of the RadioRAG framework for real-time data retrieval in radiology", "Improved diagnostic accuracy of LLMs in radiology-specific queries", "Demonstration of variability in LLM performance based on data augmentation"], "limitations": "Variance in RadioRAG's effectiveness across different LLM models.", "keywords": ["retrieval-augmented generation", "large language models", "radiology", "diagnostic accuracy", "real-time data retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.06809", "pdf": "https://arxiv.org/pdf/2410.06809.pdf", "abs": "https://arxiv.org/abs/2410.06809", "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level", "authors": ["Xinyi Zeng", "Yuying Shang", "Jiawei Chen", "Jingyuan Zhang", "Yu Tian"], "categories": ["cs.CL", "cs.CR"], "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.", "AI": {"tldr": "This paper proposes a novel decoder-oriented defense mechanism for large language models (LLMs) that improves their ability to handle harmful outputs while maintaining usability and response speed.", "motivation": "As large language models advance, they face increased risks of producing harmful outputs from prompts. Current solutions have limitations that affect effectiveness and usability.", "method": "The proposed method involves a robust decoder-oriented defense architecture that corrects harmful queries instead of rejecting them, along with the introduction of speculative decoding to improve performance.", "result": "Experiments show that the new approach enhances model security and usability, improving the capacity to recognize harmful outputs without compromising reasoning speed.", "conclusion": "The architecture provides a more effective and user-friendly way for LLMs to handle potentially harmful instructions, thereby maintaining their helpfulness.", "key_contributions": ["Novel decoder-oriented defense mechanism", "Speculative decoding for enhanced usability", "Improved model security without sacrificing reasoning speed"], "limitations": "", "keywords": ["large language models", "harmful outputs", "defense mechanism", "speculative decoding", "usability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.07009", "pdf": "https://arxiv.org/pdf/2410.07009.pdf", "abs": "https://arxiv.org/abs/2410.07009", "title": "Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs", "authors": ["Valentin Knappich", "Simon Razniewski", "Anna Hätty", "Annemarie Friedrich"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.", "AI": {"tldr": "This paper presents PAP2PAT, a benchmark for patent drafting using LLMs, emphasizing the challenges of long technical documents and the effectiveness of outline-guided generation from prepublication research papers.", "motivation": "The need for efficient patent drafting and the underutilization of LLMs in generating complex technical texts, particularly for patents which consist largely of descriptions.", "method": "Development of PAP2PAT, a benchmark with 1.8k patent-paper pairs; implementation of chunk-based outline-guided generation by leveraging the research paper to generate patent drafts.", "result": "LLMs can utilize information from research papers for patent drafting but struggle with detail; fine-tuning improves language style but increases hallucination errors.", "conclusion": "While LLMs show promise in supporting patent drafting, challenges remain in detail and accuracy, necessitating further research and improvement.", "key_contributions": ["Introduction of PAP2PAT benchmark for patent drafting", "Proposed chunk-based outline-guided generation approach", "Findings on LLM performance in patent-style language generation"], "limitations": "The study is limited by the availability of suitable prepublication research papers to serve as invention reports and the inherent challenges of patent drafting complexity.", "keywords": ["Large Language Models", "patent drafting", "PAP2PAT", "NLP", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.17161", "pdf": "https://arxiv.org/pdf/2410.17161.pdf", "abs": "https://arxiv.org/abs/2410.17161", "title": "Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence", "authors": ["İlker Işık", "Ramazan Gokberk Cinbis", "Ebru Aydin Gol"], "categories": ["cs.CL", "cs.LG", "cs.LO"], "comment": "ICML 2025 Poster Paper, Camera Ready Version", "summary": "Language models lack the notion of interchangeable tokens: symbols that are\nsemantically equivalent yet distinct, such as bound variables in formal logic.\nThis limitation prevents generalization to larger vocabularies and hinders the\nmodel's ability to recognize alpha-equivalence, where renaming bound variables\npreserves meaning. We formalize this machine learning problem and introduce\nalpha-covariance, a metric for evaluating robustness to such transformations.\nTo tackle this task, we propose a dual-part token embedding strategy: a shared\ncomponent ensures semantic consistency, while a randomized component maintains\ntoken distinguishability. Compared to a baseline that relies on alpha-renaming\nfor data augmentation, our approach demonstrates improved generalization to\nunseen tokens in linear temporal logic solving, propositional logic assignment\nprediction, and copying with an extendable vocabulary, while introducing a\nfavorable inductive bias for alpha-equivalence. Our findings establish a\nfoundation for designing language models that can learn interchangeable token\nrepresentations, a crucial step toward more flexible and systematic reasoning\nin formal domains. Our code and project page are available at\nhttps://necrashter.github.io/interchangeable-token-embeddings", "AI": {"tldr": "The paper addresses the limitation of language models in handling interchangeable tokens, proposing a dual-part token embedding strategy to enhance generalization and alpha-equivalence recognition.", "motivation": "This study explores the incapacity of language models to recognize interchangeable tokens, which hampers their performance in formal logic tasks and generalization to larger vocabularies.", "method": "The authors introduce alpha-covariance as a metric for robustness to transformations and propose a dual-part token embedding strategy that combines semantic consistency with token distinguishability.", "result": "The proposed method outperforms a baseline relying on alpha-renaming, showing improved generalization in tasks like linear temporal logic solving and propositional logic assignment prediction.", "conclusion": "The findings lay groundwork for developing language models capable of effective interchangeable token representation, enhancing reasoning in formal domains.", "key_contributions": ["Introduction of alpha-covariance metric for evaluating robustness in token transformations", "Development of a dual-part token embedding strategy for semantic consistency and distinguishability", "Demonstrated improved generalization capabilities in several formal logic tasks"], "limitations": "", "keywords": ["language models", "interchangeable tokens", "alpha-equivalence", "token embeddings", "formal logic"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2412.03092", "pdf": "https://arxiv.org/pdf/2412.03092.pdf", "abs": "https://arxiv.org/abs/2412.03092", "title": "REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization", "authors": ["Peiyan Zhang", "Haibo Jin", "Leyang Hu", "Xinnuo Li", "Liying Kang", "Man Luo", "Yangqiu Song", "Haohan Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.8"], "comment": "20 pages, 2 figures, accepted by ICML 2025", "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced the ability of LLM-based systems to perform complex tasks through\nnatural language processing and tool interaction. However, optimizing these\nLLM-based systems for specific tasks remains challenging, often requiring\nmanual interventions like prompt engineering and hyperparameter tuning.\nExisting automatic optimization methods, such as textual feedback-based\ntechniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to\nusing immediate derivatives in traditional numerical gradient descent. However,\nrelying solely on such feedback can be limited when the adjustments made in\nresponse to this feedback are either too small or fluctuate irregularly,\npotentially slowing down or even stalling the optimization process. To overcome\nthese challenges, more adaptive methods are needed, especially in situations\nwhere the system's response is evolving slowly or unpredictably. In this paper,\nwe introduce REVOLVE, an optimization method that tracks how \"R\"esponses\n\"EVOLVE\" across iterations in LLM systems. By focusing on the evolution of\nresponses over time, REVOLVE enables more stable and effective optimization by\nmaking thoughtful, progressive adjustments at each step. Experimental results\ndemonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%\nimprovement in prompt optimization, a 20.72% gain in solution refinement, and a\n29.17% increase in code optimization. Additionally, REVOLVE converges in fewer\niterations, resulting in significant computational savings. Beyond its\npractical contributions, REVOLVE highlights a promising direction, where the\nrich knowledge from established optimization principles can be leveraged to\nenhance LLM systems, which paves the way for further advancements in this\nhybrid domain.", "AI": {"tldr": "REVOLVE is an optimization method for LLM systems focusing on the evolution of responses over time, leading to more effective and stable results in task-specific optimizations.", "motivation": "To address the challenges in optimizing LLM-based systems for specific tasks without relying on manual interventions, due to limitations of existing methods focused on immediate feedback.", "method": "REVOLVE tracks the evolution of responses across iterations in LLM systems to enable thoughtful and progressive adjustments for optimization.", "result": "REVOLVE outperforms competitive baselines with a 7.8% improvement in prompt optimization, a 20.72% gain in solution refinement, and a 29.17% increase in code optimization, converging in fewer iterations.", "conclusion": "REVOLVE suggests leveraging established optimization principles in LLM systems, paving the way for further advancements in this hybrid domain.", "key_contributions": ["Introduces the REVOLVE optimization approach", "Achieves significant performance improvements over existing methods", "Demonstrates computational savings with fewer iterations needed for convergence."], "limitations": "", "keywords": ["large language models", "optimization", "human-computer interaction", "machine learning"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2412.13612", "pdf": "https://arxiv.org/pdf/2412.13612.pdf", "abs": "https://arxiv.org/abs/2412.13612", "title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition", "authors": ["Xuemei Tang", "Xufeng Duan", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures, 5 tables", "summary": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews.", "AI": {"tldr": "This paper evaluates LLMs' capacity to automate literature reviews by introducing a framework and metrics to assess their performance in reference generation, summaries, and review composition.", "motivation": "To assess how effectively LLMs can automate literature reviews, which are traditionally complex and resource-intensive tasks.", "method": "A framework is introduced for automatic evaluation of LLMs focusing on reference generation, literature summaries, and review composition, using multidimensional metrics to measure performance.", "result": "The study reveals that advanced LLMs still generate inaccurate references and their performance varies by discipline, indicating significant room for improvement.", "conclusion": "LLMs need further research and refinement to enhance their reliability in automating literature reviews.", "key_contributions": ["Introduction of a systematic framework for evaluating LLM performance in literature reviews", "Development of multidimensional metrics for assessing hallucination rates and factual consistency", "Insights into the varying effectiveness of LLMs across different academic disciplines."], "limitations": "Results depend on the specific models tested; the framework may require further refinement for broader applicability.", "keywords": ["large language models", "literature review", "evaluation metrics", "automation", "hallucination rates"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2412.18043", "pdf": "https://arxiv.org/pdf/2412.18043.pdf", "abs": "https://arxiv.org/abs/2412.18043", "title": "Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review", "authors": ["Yidong Gan", "Maciej Rybinski", "Ben Hachey", "Jonathan K. Kummerfeld"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the ACL 2025 Main Conference", "summary": "Clinical coding is crucial for healthcare billing and data analysis. Manual\nclinical coding is labour-intensive and error-prone, which has motivated\nresearch towards full automation of the process. However, our analysis, based\non US English electronic health records and automated coding research using\nthese records, shows that widely used evaluation methods are not aligned with\nreal clinical contexts. For example, evaluations that focus on the top 50 most\ncommon codes are an oversimplification, as there are thousands of codes used in\npractice. This position paper aims to align AI coding research more closely\nwith practical challenges of clinical coding. Based on our analysis, we offer\neight specific recommendations, suggesting ways to improve current evaluation\nmethods. Additionally, we propose new AI-based methods beyond automated coding,\nsuggesting alternative approaches to assist clinical coders in their workflows.", "AI": {"tldr": "This position paper addresses the misalignment between AI clinical coding research and real-world clinical practice, proposing eight recommendations for better evaluation methods and alternative AI-based approaches to assist clinical coders.", "motivation": "To enhance the practicality of AI coding research by addressing the shortcomings of current evaluation methods and aligning them with real clinical contexts.", "method": "The paper analyzes existing evaluation methods in automated clinical coding and offers recommendations based on challenges observed in the use of electronic health records.", "result": "The authors identify that many evaluations oversimplify the task by focusing on a limited number of coding results, which does not reflect real-world coding complexity.", "conclusion": "The paper concludes with eight recommendations for improving evaluation methods and suggests alternative AI methods to support clinical coders more effectively.", "key_contributions": ["Eight specific recommendations to improve current evaluation methods in clinical coding.", "Proposed new AI-based methods to assist clinical coders.", "Analysis of the misalignment between existing evaluation methods and real clinical practice."], "limitations": "The scope of the recommendations is based on the evaluation of US English electronic health records, and may not be generalizable to other contexts or languages.", "keywords": ["clinical coding", "AI", "health informatics", "evaluation methods", "automation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.03491", "pdf": "https://arxiv.org/pdf/2501.03491.pdf", "abs": "https://arxiv.org/abs/2501.03491", "title": "Can LLMs Ask Good Questions?", "authors": ["Yueheng Zhang", "Xiaoyuan Liu", "Yiyou Sun", "Atheer Alharbi", "Hend Alzahrani", "Tianneng Shi", "Basel Alomair", "Dawn Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We evaluate questions generated by large language models (LLMs) from context,\ncomparing them to human-authored questions across six dimensions: question\ntype, question length, context coverage, answerability, uncommonness, and\nrequired answer length. Our study spans two open-source and two proprietary\nstate-of-the-art models. Results reveal that LLM-generated questions tend to\ndemand longer descriptive answers and exhibit more evenly distributed context\nfocus, in contrast to the positional bias often seen in QA tasks. These\nfindings provide insights into the distinctive characteristics of LLM-generated\nquestions and inform future work on question quality and downstream\napplications.", "AI": {"tldr": "This study evaluates the quality of questions generated by large language models (LLMs) compared to human-authored questions across several dimensions.", "motivation": "To understand the distinctive characteristics of questions generated by LLMs and inform future work on question quality.", "method": "The study compares questions generated by two open-source and two proprietary LLMs across six dimensions: type, length, context coverage, answerability, uncommonness, and required answer length.", "result": "LLM-generated questions tend to require longer descriptive answers and have a more evenly distributed context focus, differing from the positional bias found in traditional QA tasks.", "conclusion": "The findings highlight key differences in question generation between LLMs and humans, which can guide future improvements in question quality.", "key_contributions": ["Comparison of LLM-generated vs human-authored questions across six dimensions", "Insights into the characteristics of LLM-generated questions", "Implications for future work on question quality and applications"], "limitations": "", "keywords": ["large language models", "question generation", "human-computer interaction", "evaluate questions", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.09265", "pdf": "https://arxiv.org/pdf/2501.09265.pdf", "abs": "https://arxiv.org/abs/2501.09265", "title": "Perspective Transition of Large Language Models for Solving Subjective Tasks", "authors": ["Xiaolong Wang", "Yuanchi Zhang", "Ziyue Wang", "Yuzhuang Xu", "Fuwen Luo", "Yile Wang", "Peng Li", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.", "AI": {"tldr": "This paper introduces Reasoning through Perspective Transition (RPT), a novel method that enhances large language models' performance on subjective tasks by enabling them to dynamically select the best perspective for problem-solving.", "motivation": "The paper addresses the limitations of large language models on subjective tasks and proposes a method that allows them to leverage different perspectives for improved contextual understanding and response accuracy.", "method": "The authors propose RPT, which uses in-context learning to enable LLMs to switch between direct, role, and third-person perspectives depending on the subjective problem at hand.", "result": "RPT significantly outperforms traditional fixed perspective methods in twelve subjective tasks using various LLMs, demonstrating its effectiveness in enhancing response quality.", "conclusion": "The study concludes that perspective adaptation can lead to more nuanced and effective responses from LLMs on subjective tasks, highlighting the potential for better interpretative performance in varying contexts.", "key_contributions": ["Introduction of the RPT method for dynamic perspective selection", "Demonstration of improved performance on subjective tasks over fixed perspective methods", "Extensive experiments with multiple LLMs showing the applicability of the method."], "limitations": "", "keywords": ["Large Language Models", "Perspective Transition", "Natural Language Processing", "Subjective Tasks", "In-Context Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.14693", "pdf": "https://arxiv.org/pdf/2502.14693.pdf", "abs": "https://arxiv.org/abs/2502.14693", "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search", "authors": ["Zujie Liang", "Feng Wei", "Wujiang Xu", "Lin Chen", "Yuxi Qian", "Xinhui Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.\nFurthermore, we integrate a Large Language Model (LLM)-based value model to\nfacilitate direct evaluation of each node's solution prior to conducting\ncomprehensive computational rollouts. A hybrid rewarding mechanism is\nimplemented to seamlessly transition the Q-value from LLM-estimated scores to\nactual performance scores. This allows higher-quality nodes to be traversed\nearlier. Applied to the various ML tasks, our approach demonstrates a 6%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.\nResource available at https://github.com/jokieleung/I-MCTS", "AI": {"tldr": "This paper presents Introspective Monte Carlo Tree Search (I-MCTS), an enhanced method for automating machine learning tasks using large language models, addressing issues in code generation diversity and quality.", "motivation": "The study aims to improve the effectiveness of LLM-based agents in automating machine learning tasks, particularly addressing their limitations in diversity and suboptimal code generation.", "method": "The authors introduce I-MCTS, which refines tree nodes through introspection and analysis of solutions from parent and sibling nodes, alongside integrating an LLM-based value model for better node evaluation.", "result": "The proposed approach results in a 6% absolute performance improvement over existing AutoML agents, indicating its effectiveness in enhancing AutoML systems.", "conclusion": "I-MCTS significantly refines the agentic decision-making process in AutoML by enhancing node selection and evaluation methods, while improving overall performance.", "key_contributions": ["Introduction of Introspective Monte Carlo Tree Search (I-MCTS) for better decision-making in ML tasks", "Integration of an LLM-based value model for direct node evaluation", "Implementation of a hybrid rewarding mechanism for optimizing node traversal"], "limitations": "", "keywords": ["Large Language Models", "Monte Carlo Tree Search", "AutoML"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.16645", "pdf": "https://arxiv.org/pdf/2502.16645.pdf", "abs": "https://arxiv.org/abs/2502.16645", "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale", "authors": ["Chenlong Wang", "Zhaoyang Chu", "Zhengxiang Cheng", "Xuyi Yang", "Kaiyue Qiu", "Yao Wan", "Zhou Zhao", "Xuanhua Shi", "Dongping Chen"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.", "AI": {"tldr": "The paper introduces CODESYNC, a data engine for real-time code knowledge updates in Python libraries, and CODESYNCBENCH, a benchmark to evaluate LLMs' adaptability to dynamic code evolution, revealing current LLM limitations in handling code updates.", "motivation": "To address the challenges faced by Large Language Models (LLMs) in adapting to evolving third-party library APIs, which often results in non-executable or suboptimal code.", "method": "The paper presents CODESYNC for identifying outdated code patterns and performing real-time updates from third-party Python libraries, along with CODESYNCBENCH to benchmark LLMs on their ability to handle dynamic code evolution.", "result": "Experiments on 14 state-of-the-art LLMs demonstrate that they struggle to keep up with dynamic code evolution, even when employing advanced knowledge updating techniques.", "conclusion": "CODESYNC and CODESYNCBENCH provide a framework and benchmark for future research on improving LLMs' capabilities in real-time code knowledge updating.", "key_contributions": ["Introduction of CODESYNC for real-time code updates from Python libraries", "Development of CODESYNCBENCH, a benchmark for evaluating LLMs against evolving code patterns", "Public availability of experimental code and dataset for further research"], "limitations": "Focus is primarily on Python libraries, which might limit applicability to other programming languages or ecosystems.", "keywords": ["Large Language Models", "real-time code updates", "benchmarking LLMs", "Python libraries", "code evolution"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.19941", "pdf": "https://arxiv.org/pdf/2502.19941.pdf", "abs": "https://arxiv.org/abs/2502.19941", "title": "Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation", "authors": ["Xiang Geng", "Zhejian Lai", "Jiajun Chen", "Hao Yang", "Shujian Huang"], "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "Quality Estimation (QE) models evaluate the quality of machine translations\nwithout reference translations, serving as the reward models for the\ntranslation task. Due to the data scarcity, synthetic data generation has\nemerged as a promising solution. However, synthetic QE data often suffers from\ndistribution shift, which can manifest as discrepancies between pseudo and real\ntranslations, or in pseudo labels that do not align with human preferences. To\ntackle this issue, we introduce DCSQE, a novel framework for alleviating\ndistribution shift in synthetic QE data. To reduce the difference between\npseudo and real translations, we employ the constrained beam search algorithm\nand enhance translation diversity through the use of distinct generation\nmodels. DCSQE uses references, i.e., translation supervision signals, to guide\nboth the generation and annotation processes, enhancing the quality of\ntoken-level labels. DCSQE further identifies the shortest phrase covering\nconsecutive error tokens, mimicking human annotation behavior, to assign the\nfinal phrase-level labels. Specially, we underscore that the translation model\ncan not annotate translations of itself accurately. Extensive experiments\ndemonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both\nsupervised and unsupervised settings. Further analysis offers insights into\nsynthetic data generation that could benefit reward models for other tasks. The\ncode is available at https://github.com/NJUNLP/njuqe.", "AI": {"tldr": "DCSQE is a novel framework designed to address distribution shift in synthetic Quality Estimation (QE) data for machine translation, enhancing model performance via constrained beam search and diverse generation models.", "motivation": "To improve the quality of Quality Estimation models by addressing the distribution shift in synthetic data generated for machine translations without reference translations.", "method": "The DCSQE framework employs constrained beam search and distinct generation models to enhance translation diversity, alongside a method for annotating translations with improved accuracy mimicking human behavior.", "result": "DCSQE demonstrated superior performance compared to state-of-the-art baselines such as CometKiwi in both supervised and unsupervised settings.", "conclusion": "The proposed DCSQE framework not only mitigates the effects of distribution shift but also provides insights beneficial for synthetic data generation in reward models across various tasks.", "key_contributions": ["Introduction of DCSQE framework for synthetic QE data", "Use of constrained beam search for improved translation quality", "Identification of phrase-level errors to mimic human annotation"], "limitations": "", "keywords": ["Quality Estimation", "Machine Translation", "Synthetic Data Generation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2503.08327", "pdf": "https://arxiv.org/pdf/2503.08327.pdf", "abs": "https://arxiv.org/abs/2503.08327", "title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation", "authors": ["José Pombal", "Nuno M. Guerreiro", "Ricardo Rei", "André F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (MINT), i.e., the use of the same\nor related metrics for both model tuning and evaluation. MINT can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of MINT in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMINT strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for-questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MINTADJUST, a method for more reliable\nevaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks\ntranslations and systems more accurately than state-of-the-art metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.", "AI": {"tldr": "This paper addresses the problem of metric interference in machine translation, proposing MINTADJUST for more reliable evaluation.", "motivation": "The rise of strong automatic metrics has increased the risk of gaming these metrics during model development, which can lead to misleading performance evaluations due to metric interference.", "method": "The paper analyzes cases of metric interference (MINT) in machine translation, particularly in training data filtering and decoding with quality signals, and proposes MINTADJUST as a solution for reliable evaluation.", "result": "MINTADJUST is shown to rank translations and systems more accurately than state-of-the-art metrics on the WMT24 MT shared task test set, particularly benefiting high-quality systems.", "conclusion": "MINTADJUST offers a solution to the problem of metric interference, enabling more trustworthy performance evaluations in machine translation.", "key_contributions": ["Introduction of the term metric interference (MINT) in the context of machine translation.", "Proposition of MINTADJUST for effective evaluation despite MINT.", "Demonstrated improvement over existing evaluation metrics in multiple language pairs."], "limitations": "", "keywords": ["metric interference", "MINTADJUST", "machine translation", "evaluation metrics", "language pairs"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.11895", "pdf": "https://arxiv.org/pdf/2503.11895.pdf", "abs": "https://arxiv.org/abs/2503.11895", "title": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing", "authors": ["Bhiman Kumar Baghel", "Scott M. Jordan", "Zheyuan Ryan Shi", "Xiang Lorraine Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method.", "AI": {"tldr": "This paper introduces two methods for more efficient model editing of Large Language Models (LLMs) that aim to reduce the issues of UnderEdit and OverEdit during parameter updates.", "motivation": "To improve the efficiency of updating LLMs without the high computational costs associated with retraining or fine-tuning, addressing the limitations of existing editing methods.", "method": "The paper proposes two methods: iterative model editing, which uses successive edits to combat UnderEdit, and neighbor-assisted model editing, which utilizes neighboring knowledge to mitigate OverEdit.", "result": "The proposed methods significantly enhance editing performance, reducing UnderEdit by up to 38 percentage points and OverEdit by up to 6 across various LLMs and benchmarks.", "conclusion": "These techniques not only provide a more effective approach to model editing but are also broadly applicable to any locate-and-edit strategy.", "key_contributions": ["Iterative model editing to improve UnderEdit.", "Neighbor-assisted model editing to minimize OverEdit.", "Demonstrated performance improvements across multiple LLMs, algorithms, and benchmarks."], "limitations": "", "keywords": ["Large Language Models", "model editing", "UnderEdit", "OverEdit", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
