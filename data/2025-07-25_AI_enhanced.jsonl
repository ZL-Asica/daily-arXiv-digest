{"id": "2507.17753", "pdf": "https://arxiv.org/pdf/2507.17753.pdf", "abs": "https://arxiv.org/abs/2507.17753", "title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Model (LLM) agents are increasingly utilized in AI-aided\neducation to support tutoring and learning. Effective communication strategies\namong LLM agents improve collaborative problem-solving efficiency and\nfacilitate cost-effective adoption in education. However, little research has\nsystematically evaluated the impact of different communication strategies on\nagents' problem-solving. Our study examines four communication modes,\n\\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration},\n\\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a\ndual-agent, chat-based mathematical problem-solving environment using the\nOpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that\ndual-agent setups outperform single agents, with \\textit{peer-to-peer\ncollaboration} achieving the highest accuracy. Dialogue acts like statements,\nacknowledgment, and hints play a key role in collaborative problem-solving.\nWhile multi-agent frameworks enhance computational tasks, effective\ncommunication strategies are essential for tackling complex problems in AI\neducation.", "AI": {"tldr": "This study evaluates communication strategies among LLM agents in a dual-agent math problem-solving context.", "motivation": "The need for effective communication strategies among LLM agents to enhance collaborative problem-solving efficiency in AI-aided education.", "method": "Examined four communication modes in a dual-agent, chat-based problem-solving environment using the OpenAI GPT-4o model on the MATH dataset.", "result": "Dual-agent setups outperformed single agents, with peer-to-peer collaboration achieving the highest accuracy.", "conclusion": "Effective communication strategies are essential for improving complex problem-solving in AI education.", "key_contributions": ["Systematic evaluation of communication strategies among LLM agents", "Identification of peer-to-peer collaboration as the most effective strategy", "Insights into the role of dialogue acts in collaborative problem-solving"], "limitations": "", "keywords": ["Large Language Model", "AI education", "communication strategies", "collaborative problem-solving", "peer-to-peer collaboration"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.17754", "pdf": "https://arxiv.org/pdf/2507.17754.pdf", "abs": "https://arxiv.org/abs/2507.17754", "title": "A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians", "authors": ["Justin Morse", "Kurt Gilbert", "Kyle Shin", "Rick Cooke", "Peyton Rose", "Jack Sullivan", "Angelo Sisante"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Clinician burnout has motivated the growing adoption of ambient medical\nscribes in the clinic. In this work, we introduce a custom-built ambient scribe\napplication integrated into the EHR system at Included Health, a personalized\nall-in-one healthcare company offering telehealth services. The application\nuses Whisper for transcription and a modular in-context learning pipeline with\nGPT-4o to automatically generate SOAP notes and patient instructions. Testing\non mock visit data shows that the notes generated by the application exceed the\nquality of expert-written notes as determined by an LLM-as-a-judge. The\napplication has been widely adopted by the clinical practice, with over 540\nclinicians at Included Health using the application at least once. 94% (n = 63)\nof surveyed clinicians report reduced cognitive load during visits and 97% (n =\n66) report less documentation burden when using the application. Additionally,\nwe show that post-processing notes with a fine-tuned BART model improves\nconciseness. These findings highlight the potential for AI systems to ease\nadministrative burdens and support clinicians in delivering efficient,\nhigh-quality care.", "AI": {"tldr": "This paper presents an ambient medical scribe application integrated into an EHR system that uses AI to assist clinicians by generating high-quality notes, resulting in reduced cognitive load and documentation burden.", "motivation": "The increasing clinician burnout necessitated the development of automated solutions to alleviate administrative burdens in healthcare settings.", "method": "An ambient scribe application was developed and integrated into the EHR system, utilizing Whisper for transcription and a modular in-context learning pipeline with GPT-4o for generating SOAP notes and patient instructions.", "result": "The generated notes surpassed the quality of expert-written notes, with 94% of clinicians reporting reduced cognitive load and 97% reporting less documentation burden. Post-processing with a fine-tuned BART model improved note conciseness.", "conclusion": "The findings demonstrate the effectiveness of AI systems in supporting clinicians and enhancing the quality of care by reducing administrative tasks.", "key_contributions": ["Introduction of a custom ambient scribe application integrated into EHR systems.", "Demonstrated improved note quality compared to expert-written notes.", "Survey results showing significant reductions in cognitive load and documentation burden for clinicians."], "limitations": "", "keywords": ["Ambient Scribes", "EHR Integration", "Clinician Burnout", "AI in Healthcare", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.17755", "pdf": "https://arxiv.org/pdf/2507.17755.pdf", "abs": "https://arxiv.org/abs/2507.17755", "title": "Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image", "authors": ["Jianfeng Lan", "Yingjia Huang"], "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": null, "summary": "In the digital era, social media platforms play a pivotal role in shaping\nadolescents' body image perceptions. This study examines how Douyin and WeChat,\ntwo contrasting Chinese social media platforms, influence body image among\nChinese male adolescents. Employing a platformization perspective, we surveyed\n395 male adolescents aged 10 to 24 using the Multidimensional Body-Self\nRelations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation\nand body satisfaction. Our findings reveal that Douyin usage is significantly\ncorrelated with appearance evaluation and body area satisfaction, while WeChat\nusage shows no significant correlation with any body image dimensions. These\nresults suggest that Douyin's algorithm-driven, video-centric environment\nintensifies exposure to idealized body standards, impacting users at a\ncognitive level. This study underscores the importance of considering\nplatform-specific characteristics in understanding social media's impact on\nbody image. It contributes to the broader discourse on how technological design\nand content modalities mediate psychological outcomes, offering insights for\naddressing body image concerns among male adolescents in China.", "AI": {"tldr": "This study investigates how Douyin and WeChat affect body image perceptions among Chinese male adolescents, finding Douyin's content has a significant impact.", "motivation": "To explore the influence of social media platforms on body image perceptions among adolescents, particularly focusing on the differences between Douyin and WeChat.", "method": "A survey of 395 male adolescents aged 10 to 24 using the Multidimensional Body-Self Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation and body satisfaction.", "result": "Douyin usage is significantly correlated with appearance evaluation and body area satisfaction, while WeChat shows no significant correlation with body image dimensions.", "conclusion": "The findings indicate that Douyin’s algorithm-driven environment intensifies exposure to idealized body standards, influencing cognitive perceptions of body image.", "key_contributions": ["Identifies the differing impacts of Douyin and WeChat on body image among adolescents.", "Highlights the importance of platform characteristics in shaping psychological outcomes.", "Offers insights for addressing body image concerns related to social media usage."], "limitations": "Study focused solely on male adolescents in China; findings may not generalize to other demographics or regions.", "keywords": ["social media", "body image", "Douyin", "WeChat", "adolescents"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.17756", "pdf": "https://arxiv.org/pdf/2507.17756.pdf", "abs": "https://arxiv.org/abs/2507.17756", "title": "Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy", "authors": ["Josh Hunter", "John McDermid", "Simon Burton"], "categories": ["cs.HC", "cs.AI"], "comment": "9 pages, 3 figures, published in European Dependable Computing\n  Conference 2025", "summary": "This study investigates how railway professionals perceive safety as a\nconcept within rail, with the intention to help inform future technological\ndevelopments within the industry. Through a series of interviews with drivers,\nroute planners,and administrative personnel, the research explores the\ncurrentstate of safety practices, the potential for automation and the\nunderstanding of the railway as a system of systems. Key findings highlight a\ncautious attitude towards automation, a preference for assistive technologies,\nand a complex understanding of safety that integrates human, systematic and\ntechnological factors. The study also addresses the limitations of transferring\nautomotive automation technologies to railways and the need for a\nrailway-specific causation model to better evaluate and enhance safety in an\nevolving technological landscape. This study aims to bridge thegap between\ncontemporary research and practical applications, contributing to the\ndevelopment of more effective safety metrics.", "AI": {"tldr": "This study examines how railway professionals perceive safety, discussing the integration of human, systematic, and technological elements for future automation.", "motivation": "To inform future technological developments in railway safety by understanding professionals' perspectives.", "method": "Interviews with drivers, route planners, and administrative personnel were conducted to gather data on safety practices and automation.", "result": "Findings indicate a cautious approach to automation, a preference for assistive technologies, and a complex understanding of safety in rail systems.", "conclusion": "The study identifies the need for a railway-specific causation model for better safety evaluation and highlights limitations in applying automotive automation to railways.", "key_contributions": ["Investigates professional perceptions of safety in railways", "Identifies the need for a railway-specific causation model", "Emphasizes the importance of assistive technologies over full automation"], "limitations": "Challenges exist in transferring automotive automation tactics to railway systems.", "keywords": ["railway safety", "automation", "assistive technologies", "human factors", "system of systems"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.17842", "pdf": "https://arxiv.org/pdf/2507.17842.pdf", "abs": "https://arxiv.org/abs/2507.17842", "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "AI": {"tldr": "Shop-R1 is a novel reinforcement learning framework that enhances LLMs' reasoning for simulating human behavior in online shopping, achieving over 65% improvement in performance.", "motivation": "Despite advancements, the reasoning abilities of LLMs in simulating human behavior in web environments are limited by the models generating training rationales.", "method": "Introduces a two-stage RL framework where the first stage focuses on rationale generation using internal model signals, and the second stage involves action prediction guided by a hierarchical reward structure.", "result": "Shop-R1 demonstrates a relative improvement of over 65% compared to baseline models in generating human-like behavior during online shopping.", "conclusion": "Our framework significantly enhances LLMs' simulation capabilities, providing a more effective approach to modeling human behavior in digital environments.", "key_contributions": ["Introduction of Shop-R1 as a novel RL framework for LLMs", "Implementation of self-supervised rationale generation", "Development of a difficulty-aware hierarchical reward structure for action prediction."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Human Behavior Simulation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.17757", "pdf": "https://arxiv.org/pdf/2507.17757.pdf", "abs": "https://arxiv.org/abs/2507.17757", "title": "BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches", "authors": ["Sam Gordon James", "Miranda Elaine Glynis Armstrong", "Aisling Ann O'Kane", "Harry Emerson", "Zahraa S. Abdallah"], "categories": ["cs.HC", "cs.LG"], "comment": "13 pages, 14 figures", "summary": "Background: Type 1 diabetes (T1D) has seen a rapid evolution in management\ntechnology and forms a useful case study for the future management of other\nchronic conditions. Further development of this management technology requires\nan exploration of its real-world use and the potential of additional data\nstreams. To facilitate this, we contribute the BrisT1D Dataset to the growing\nnumber of public T1D management datasets. The dataset was developed from a\nlongitudinal study of 24 young adults in the UK who used a smartwatch alongside\ntheir usual T1D management. Findings: The BrisT1D dataset features both device\ndata from the T1D management systems and smartwatches used by participants, as\nwell as transcripts of monthly interviews and focus groups conducted during the\nstudy. The device data is provided in a processed state, for usability and more\nrapid analysis, and in a raw state, for in-depth exploration of novel insights\ncaptured in the study. Conclusions: This dataset has a range of potential\napplications. The quantitative elements can support blood glucose prediction,\nhypoglycaemia prediction, and closed-loop algorithm development. The\nqualitative elements enable the exploration of user experiences and opinions,\nas well as broader mixed-methods research into the role of smartwatches in T1D\nmanagement.", "AI": {"tldr": "The BrisT1D dataset, developed from a longitudinal study of young adults with Type 1 diabetes using smartwatches, provides both quantitative and qualitative data aimed at improving T1D management.", "motivation": "To explore the real-world use of management technology for Type 1 diabetes and to provide an easily accessible dataset for further research.", "method": "The dataset was created from a longitudinal study involving 24 young adults in the UK, integrating data from T1D management systems and smartwatches, along with interview transcripts.", "result": "The dataset includes processed and raw device data, supporting various applications such as blood glucose and hypoglycemia prediction, as well as closed-loop algorithm development.", "conclusion": "The BrisT1D dataset has significant potential for mixed-methods research into T1D management and the role of smartwatches, enhancing understanding of user experiences and improving predictive algorithms.", "key_contributions": ["Creation of the BrisT1D dataset combining quantitative and qualitative data.", "Facilitation of research on blood glucose and hypoglycemia prediction models.", "Enhancement of mixed-methods studies in chronic disease management."], "limitations": "", "keywords": ["Type 1 diabetes", "smartwatch", "dataset", "data analysis", "mixed-methods research"], "importance_score": 6, "read_time_minutes": 13}}
{"id": "2507.17849", "pdf": "https://arxiv.org/pdf/2507.17849.pdf", "abs": "https://arxiv.org/abs/2507.17849", "title": "Dynamic and Generalizable Process Reward Modeling", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "AI": {"tldr": "Dynamic and Generalizable Process Reward Modeling (DG-PRM) improves reward signals for Large Language Models by employing a reward tree and Pareto dominance estimation to enhance model performance and generalizability.", "motivation": "The motivation behind this research is to address the limitations of existing Process Reward Models (PRMs) that rely on heuristic methods and struggle with cross-domain generalization.", "method": "The paper proposes DG-PRM, utilizing a reward tree for fine-grained, multi-dimensional reward criteria and dynamic selection of reward signals for step-wise scoring alongside Pareto dominance estimation for identifying positive and negative pairs.", "result": "Experimental results indicate that DG-PRM significantly boosts model performance across various tasks and adapts effectively to out-of-distribution scenarios, demonstrating notable generalizability.", "conclusion": "DG-PRM offers a more robust framework for guiding LLMs with dense reward signals, enhancing their effectiveness in complex tasks.", "key_contributions": ["Introduction of Dynamic and Generalizable Process Reward Modeling (DG-PRM)", "Development of a reward tree for capturing multi-dimensional reward criteria", "Application of Pareto dominance estimation for reward signal evaluation"], "limitations": "", "keywords": ["Process Reward Models", "Large Language Models", "Dynamic Generalization", "Pareto Dominance", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.17759", "pdf": "https://arxiv.org/pdf/2507.17759.pdf", "abs": "https://arxiv.org/abs/2507.17759", "title": "DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation", "authors": ["Riddhi Heda", "Sidhant Singh", "Umair Yasir", "Tanmay Jaiswal", "Anil Mokhade"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional hostel management practices in academic institutions often suffer\nfrom inefficiencies, delays, and fragmented communication. These systems fail\nto meet the expectations of digitally native students and place a significant\noperational burden on hostel staff. This paper introduces DHMS (Digital Hostel\nManagement System), a modular and integrated platform designed to digitize and\nstreamline essential hostel management functions. DHMS leverages modern web\ntechnologies, artificial intelligence, and cloud infrastructure to automate\nroom allotment, grievance redressal, gate pass logistics, and communication via\na natural language chatbot. In simulation tests, DHMS achieved a 92% student\nsatisfaction rate in room allocation and maintained an average chatbot response\ntime below one second. Additional features include predictive analytics for\nproactive maintenance planning and sentiment analysis for feedback processing.\nWhile promising, the system requires further testing for integration across\nmultiple hostel blocks, user acceptance, scalability under load, and ERP\ncompatibility before campus-wide deployment. This work discusses the system\narchitecture, implementation approach, and factors critical to improving user\nexperience, administrative efficiency, and decision-making processes.", "AI": {"tldr": "The paper presents a Digital Hostel Management System (DHMS) designed to enhance operational efficiency and student satisfaction in hostel management using modern web technologies, AI, and cloud infrastructure.", "motivation": "To address inefficiencies and operational burdens in traditional hostel management systems that do not meet the needs of digitally native students.", "method": "The paper introduces a modular and integrated platform that automates key hostel management functions such as room allotment, grievance redressal, and communication via a natural language chatbot, using simulation tests for validation.", "result": "DHMS achieved a 92% student satisfaction rate in room allocation and an average chatbot response time below one second during simulation tests.", "conclusion": "Further testing for integration, user acceptance, scalability, and ERP compatibility is needed before deploying DHMS campus-wide; the work outlines the system architecture and implementation approach for enhancing user experience and efficiency.", "key_contributions": ["Introduction of a modular and integrated Digital Hostel Management System (DHMS)", "Use of AI and cloud infrastructure for automating key management functions", "Implementation of predictive analytics and sentiment analysis for proactive management"], "limitations": "Requires further testing for integration across multiple blocks, user acceptance, scalability under load, and ERP compatibility.", "keywords": ["Digital Hostel Management System", "AI", "Cloud Infrastructure", "Predictive Analytics", "User Experience"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.17896", "pdf": "https://arxiv.org/pdf/2507.17896.pdf", "abs": "https://arxiv.org/abs/2507.17896", "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "AI": {"tldr": "VeriMinder is an interactive system designed to detect and mitigate cognitive biases in analytical questions formed by users of natural language interfaces to databases (NLIDBs), enhancing the quality of data analysis.", "motivation": "To address the challenge of formulating bias-free analytical questions by users without a background in statistical analysis when using NLIDBs.", "method": "VeriMinder employs a contextual semantic mapping framework to identify relevant biases, operationalizes the Hard-to-Vary principle for systematic analysis, and utilizes an optimized LLM-powered system for generating high-quality, task-specific prompts.", "result": "User testing revealed that 82.5% of participants felt positively impacted by the quality of their analysis, and VeriMinder outperformed alternative methods by at least 20% in key metrics of concreteness, comprehensiveness, and accuracy.", "conclusion": "VeriMinder helps users avoid 'wrong question' vulnerabilities in data analysis by improving analytical questioning and is available as open-source software for broader community use.", "key_contributions": ["Contextual semantic mapping framework for identifying biases", "Analytical framework operationalizing the Hard-to-Vary principle", "LLM-powered generation of task-specific prompts"], "limitations": "", "keywords": ["natural language interfaces", "data analysis", "cognitive biases", "interactive systems", "prompt generation"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.17761", "pdf": "https://arxiv.org/pdf/2507.17761.pdf", "abs": "https://arxiv.org/abs/2507.17761", "title": "Co-constructing Explanations for AI Systems using Provenance", "authors": ["Jan-Christoph Kalo", "Fina Polat", "Shubha Guha", "Paul Groth"], "categories": ["cs.HC"], "comment": "5 pages", "summary": "Modern AI systems are complex workflows containing multiple components and\ndata sources. Data provenance provides the ability to interrogate and\npotentially explain the outputs of these systems. However, provenance is often\ntoo detailed and not contextualized for the user trying to understand the AI\nsystem. In this work, we present our vision for an interactive agent that works\ntogether with the user to co-construct an explanation that is simultaneously\nuseful to the user as well as grounded in data provenance. To illustrate this\nvision, we present: 1) an initial prototype of such an agent; and 2) a scalable\nevaluation framework based on user simulations and a large language model as a\njudge approach.", "AI": {"tldr": "This paper presents an interactive agent designed to help users understand AI system outputs through contextualized explanations grounded in data provenance.", "motivation": "AI systems are complex, and their outputs can be difficult for users to understand. Data provenance often lacks context for users.", "method": "The paper introduces an interactive agent that collaborates with users to construct explanations. It includes a prototype demonstration and a scalable evaluation framework using user simulations and a large language model as a judge.", "result": "The initial prototype shows promise in facilitating user understanding of AI system outputs through structured explanations.", "conclusion": "The interactive agent represents a step towards making data provenance more accessible and meaningful for users, improving the explainability of AI systems.", "key_contributions": ["Introduction of an interactive agent for explanation co-construction", "Demonstration of a prototype for user interaction", "Development of a scalable evaluation framework using simulations and LLMs"], "limitations": "The prototype is in an early stage and may require further development based on user feedback.", "keywords": ["AI explainability", "data provenance", "interactive agent", "user simulations", "large language model"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.17918", "pdf": "https://arxiv.org/pdf/2507.17918.pdf", "abs": "https://arxiv.org/abs/2507.17918", "title": "One Whisper to Grade Them All", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "AI": {"tldr": "The paper introduces an efficient end-to-end system for Automatic Speaking Assessment (ASA) that processes multiple spoken responses with a single encoder and a lightweight aggregator.", "motivation": "To create a practical and efficient solution for scoring multi-part second-language tests in Computer-Assisted Language Learning systems.", "method": "A single Whisper-small encoder processes all four spoken responses, and a lightweight aggregator combines the information to predict scores, eliminating the need for separate transcription and models per part.", "result": "The system achieved a RMSE of 0.384, outperforming a text-based baseline while using fewer parameters (168M), and demonstrated strong data efficiency by using only 44.8% of the speakers in training while achieving RMSE of 0.383.", "conclusion": "The proposed system enhances the efficiency and scalability of ASA for language learning applications, addressing challenges in performance and data utilization.", "key_contributions": ["Unique single-encoder architecture for processing multiple responses", "Data sampling strategy that improves performance with fewer training samples", "Achievement of lower RMSE compared to traditional text-based models"], "limitations": "", "keywords": ["Automatic Speaking Assessment", "Language Learning", "Machine Learning", "Data Efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.17774", "pdf": "https://arxiv.org/pdf/2507.17774.pdf", "abs": "https://arxiv.org/abs/2507.17774", "title": "Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems", "authors": ["Zhangqi Liu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision.", "AI": {"tldr": "This paper discusses the role of AI, particularly LLMs and multimodal diffusion models, in human-centered design, focusing on co-creation in early-stage design processes.", "motivation": "The integration of AI in design workflows necessitates a new understanding of human-centered design, moving beyond automation to a collaborative co-creation model.", "method": "The paper examines how designers can use LLMs like GPT-4 and multimodal models such as Stable Diffusion as active participants in the design process, engaging in iterative cycles of proposal, critique, and revision.", "result": "The investigation reveals that AI can enhance creativity and decision-making in design by fostering collaborative interactions between human designers and AI agents.", "conclusion": "AI's role is shifting from a tool for efficiency to an interactive collaborator, requiring designers to adapt their workflows for effective human-AI co-creation.", "key_contributions": ["Introduction of the human-AI co-creation paradigm", "Exploration of LLMs and multimodal models in design", "Framework for integrating AI in iterative design processes"], "limitations": "", "keywords": ["human-AI co-creation", "design workflows", "creative agents"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17944", "pdf": "https://arxiv.org/pdf/2507.17944.pdf", "abs": "https://arxiv.org/abs/2507.17944", "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "AI": {"tldr": "This paper investigates the effectiveness of six AI detection tools in identifying text generated by the LLM DeepSeek, especially after adversarial attacks like paraphrasing and humanization.", "motivation": "To address the gap in literature regarding the detection of text generated by the recently published LLM DeepSeek and the impact of adversarial attacks on detection accuracy.", "method": "The study utilized six AI detection tools and exposed them to human-authored and DeepSeek-generated texts, applying adversarial attacks to assess their performance.", "result": "QuillBot and Copyleaks demonstrated near-perfect performance on original and paraphrased texts, while other tools like AI Text Classifier and GPT-2 showed variable detection accuracy, particularly struggling with humanization attacks.", "conclusion": "The study highlights the challenges and variations in detecting DeepSeek-generated text, especially under various adversarial conditions. Few-shot prompting and CoT methods proved to be effective for detection accuracy.", "key_contributions": ["Evaluated the performance of six detection tools on DeepSeek-generated text.", "Demonstrated the impact of adversarial attacks on detection accuracy.", "Introduced few-shot prompting and CoT reasoning as effective strategies for better detection."], "limitations": "The study primarily focuses on a limited number of detection tools and the accuracy may vary with different versions of LLMs or under different conditions.", "keywords": ["AI detection", "large language models", "adversarial attacks", "DeepSeek", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17898", "pdf": "https://arxiv.org/pdf/2507.17898.pdf", "abs": "https://arxiv.org/abs/2507.17898", "title": "Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization", "authors": ["Connor Scully-Allison", "Kevin Menear", "Kristin Potter", "Andrew McNutt", "Katherine E. Isaacs", "Dmitry Duplyakin"], "categories": ["cs.HC"], "comment": "11 Pages, 4 figures", "summary": "Domain-specific visualizations sometimes focus on narrow, albeit important,\ntasks for one group of users. This focus limits the utility of a visualization\nto other groups working with the same data. While tasks elicited from other\ngroups can present a design pitfall if not disambiguated, they also present a\ndesign opportunity -- development of visualizations that support multiple\ngroups. This development choice presents a trade off of broadening the scope\nbut limiting support for the more narrow tasks of any one group, which in some\ncases can enhance the overall utility of the visualization. We investigate this\nscenario through a design study where we develop \\textit{Guidepost}, a\nnotebook-embedded visualization of supercomputer queue data that helps\nscientists assess supercomputer queue wait times, machine learning researchers\nunderstand prediction accuracy, and system maintainers analyze usage trends. We\nadapt the use of personas for visualization design from existing literature in\nthe HCI and software engineering domains and apply them in categorizing tasks\nbased on their uniqueness across the stakeholder personas. Under this model,\ntasks shared between all groups should be supported by interactive\nvisualizations and tasks unique to each group can be deferred to scripting with\nnotebook-embedded visualization design. We evaluate our visualization with nine\nexpert analysts organized into two groups: a \"research analyst\" group that uses\nsupercomputer queue data in their research (representing the Machine Learning\nresearchers and Jobs Data Analyst personas) and a \"supercomputer user\" group\nthat uses this data conditionally (representing the HPC User persona). We find\nthat our visualization serves our three stakeholder groups by enabling users to\nsuccessfully execute shared tasks with point-and-click interaction while\nfacilitating case-specific programmatic analysis workflows.", "AI": {"tldr": "The paper presents a design study of 'Guidepost', a notebook-embedded visualization tool for supercomputer queue data, aimed at serving multiple user groups and their unique tasks.", "motivation": "The limitation of domain-specific visualizations to narrow tasks prompts the exploration of designs that support multiple user groups, enhancing overall utility.", "method": "A design study approach using personas from HCI and software engineering literature to categorize tasks for scientists, machine learning researchers, and system maintainers, followed by user evaluation with expert analysts.", "result": "The visualization successfully enables users to complete shared tasks via point-and-click interactions while supporting individual programmatic analysis needs for unique tasks.", "conclusion": "Adopting a broader approach in visualization design can satisfy the varying needs of different user groups, thereby enhancing the overall effectiveness of data interaction.", "key_contributions": ["Development of 'Guidepost' for multi-user group support in visualizations.", "Adaption of personas from HCI for task categorization in visualization design.", "Evaluation of visualization effectiveness across different user groups."], "limitations": "", "keywords": ["visualization", "human-computer interaction", "machine learning", "supercomputer queue data", "notebook-embedded visualization"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.17951", "pdf": "https://arxiv.org/pdf/2507.17951.pdf", "abs": "https://arxiv.org/abs/2507.17951", "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.", "AI": {"tldr": "This paper investigates how the size and capability of language models influence their adherence to Bayes' theorem when updating beliefs with evidence, proposing a new metric, the Bayesian Coherence Coefficient (BCC).", "motivation": "To understand whether larger language models are better at updating beliefs in accordance with Bayes' theorem when given in-context evidence.", "method": "We formulated a Bayesian Coherence Coefficient (BCC) metric and generated a dataset to measure BCC across various pre-trained language models from five different families, analyzing their performance in relation to model size, training data, and benchmark scores.", "result": "Our findings suggest that larger pre-trained language models exhibit more coherent belief updates in line with Bayes' theorem, as evidenced by their BCC scores.", "conclusion": "The results highlight the significance of model size and architecture in belief updating processes in language models, which has important implications for the understanding and governance of LLMs.", "key_contributions": ["Introduction of the Bayesian Coherence Coefficient (BCC) metric for evaluating belief updates in language models.", "Empirical analysis comparing belief coherence of various language model families based on size and training data.", "Insights on the implications of model architecture for the governance of language models."], "limitations": "The analysis is based on pre-trained models and may not fully reflect the performance of fine-tuned models in belief updating tasks.", "keywords": ["language models", "Bayes' theorem", "Bayesian Coherence Coefficient", "belief updating", "AI governance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.17943", "pdf": "https://arxiv.org/pdf/2507.17943.pdf", "abs": "https://arxiv.org/abs/2507.17943", "title": "Automated Brake Onset Detection in Naturalistic Driving Data", "authors": ["Shu-Yuan Liu", "Johan Engström", "Gustav Markkula"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Response timing measures play a crucial role in the assessment of automated\ndriving systems (ADS) in collision avoidance scenarios, including but not\nlimited to establishing human benchmarks and comparing ADS to human driver\nresponse performance. For example, measuring the response time (of a human\ndriver or ADS) to a conflict requires the determination of a stimulus onset and\na response onset. In existing studies, response onset relies on manual\nannotation or vehicle control signals such as accelerator and brake pedal\nmovements. These methods are not applicable when analyzing large scale data\nwhere vehicle control signals are not available. This holds in particular for\nthe rapidly expanding sets of ADS log data where the behavior of surrounding\nroad users is observed via onboard sensors. To advance evaluation techniques\nfor ADS and enable measuring response timing when vehicle control signals are\nnot available, we developed a simple and efficient algorithm, based on a\npiecewise linear acceleration model, to automatically estimate brake onset that\ncan be applied to any type of driving data that includes vehicle longitudinal\ntime series data. We also proposed a manual annotation method to identify brake\nonset and used it as ground truth for validation. R2 was used as a confidence\nmetric to measure the accuracy of the algorithm, and its classification\nperformance was analyzed using naturalistic collision avoidance data of both\nADS and humans, where our method was validated against human manual annotation.\nAlthough our algorithm is subject to certain limitations, it is efficient,\ngeneralizable, applicable to any road user and scenario types, and is highly\nconfigurable.", "AI": {"tldr": "The paper presents an algorithm for estimating brake onset in automated driving systems using vehicle longitudinal time series data, addressing the limitations of traditional methods that rely on manual annotation or vehicle control signals.", "motivation": "To improve evaluation techniques for automated driving systems by enabling response timing measurements even when vehicle control signals are unavailable.", "method": "A piecewise linear acceleration model was developed to automatically estimate brake onset from longitudinal time series data, complemented by a manual annotation method for validation.", "result": "The algorithm's classification performance was validated against human manual annotation using naturalistic collision avoidance data, demonstrating high accuracy.", "conclusion": "Despite some limitations, the algorithm is efficient, generalizable, and applicable across various road user and scenario types.", "key_contributions": ["Development of an efficient algorithm for estimating brake onset", "Validation against human manual annotation methods", "Applicability to diverse driving data sets and scenarios"], "limitations": "The algorithm has certain limitations but is generally efficient and adaptable.", "keywords": ["automated driving systems", "response timing", "brake onset estimation", "collision avoidance", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.17974", "pdf": "https://arxiv.org/pdf/2507.17974.pdf", "abs": "https://arxiv.org/abs/2507.17974", "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "authors": ["Fitsum Gaim", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.", "AI": {"tldr": "This paper surveys the state of NLP research for Tigrinya, analyzing over 40 studies and highlighting challenges and future directions.", "motivation": "To address the severe underrepresentation of Tigrinya in Natural Language Processing research.", "method": "Systematic review of over 40 studies from 2011 to 2025 encompassing various NLP tasks such as morphological processing, machine translation, and speech recognition.", "result": "The analysis shows a transition from rule-based systems to modern neural architectures, driven by advancements in resource creation.", "conclusion": "The paper highlights the challenges in Tigrinya NLP while proposing future research directions and providing a curated repository of resources.", "key_contributions": ["Comprehensive survey of Tigrinya NLP research", "Identification of key challenges and promising research directions", "Publicly available curated metadata of surveyed studies and resources"], "limitations": "Focuses exclusively on Tigrinya, limiting broader applicability to other languages.", "keywords": ["Tigrinya", "NLP", "Natural Language Processing", "morphological processing", "machine translation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.17985", "pdf": "https://arxiv.org/pdf/2507.17985.pdf", "abs": "https://arxiv.org/abs/2507.17985", "title": "Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Victor Tian", "Zachary Zhang", "Kevin He", "Min Sun"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The integration of large language models (LLMs) into educational tools has\nthe potential to substantially impact how teachers plan instruction, support\ndiverse learners, and engage in professional reflection. Yet little is known\nabout how educators actually use these tools in practice and how their\ninteractions with AI can be meaningfully studied at scale. This paper presents\na human-AI collaborative methodology for large-scale qualitative analysis of\nover 140,000 educator-AI messages drawn from a generative AI platform used by\nK-12 teachers. Through a four-phase coding pipeline, we combined inductive\ntheme discovery, codebook development, structured annotation, and model\nbenchmarking to examine patterns of educator engagement and evaluate the\nperformance of LLMs in qualitative coding tasks. We developed a hierarchical\ncodebook aligned with established teacher evaluation frameworks, capturing\neducators' instructional goals, contextual needs, and pedagogical strategies.\nOur findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably\nsupport theme identification, extend human recognition in complex scenarios,\nand outperform open-weight models in both accuracy and structural reliability.\nThe analysis also reveals substantive patterns in how educators inquire AI to\nenhance instructional practices (79.7 percent of total conversations), create\nor adapt content (76.1 percent), support assessment and feedback loop (46.9\npercent), attend to student needs for tailored instruction (43.3 percent), and\nassist other professional responsibilities (34.2 percent), highlighting\nemerging AI-related competencies that have direct implications for teacher\npreparation and professional development. This study offers a scalable,\ntransparent model for AI-augmented qualitative research and provides\nfoundational insights into the evolving role of generative AI in educational\npractice.", "AI": {"tldr": "This paper explores the integration of large language models (LLMs) in educational tools and analyzes over 140,000 messages between educators and AI, revealing patterns in their engagement and AI's effectiveness in supporting teaching.", "motivation": "To understand how educators use AI tools in practice and study their interactions with AI at scale.", "method": "A human-AI collaborative methodology involving a four-phase coding pipeline to analyze educator-AI messages, combined with thematic analysis and model benchmarking.", "result": "LLMs, especially Claude 3.5 Haiku, can effectively support educators in identifying themes and performing qualitative coding tasks, illustrating a significant engagement in enhancing instructional practices and professional development.", "conclusion": "The study provides insights into the integration of generative AI in education, highlighting emerging AI competencies and offering a scalable model for qualitative research.", "key_contributions": ["Development of a hierarchical codebook aligned with teacher evaluation frameworks.", "Demonstration of LLMs' effectiveness in qualitative coding tasks.", "Insights into educator engagement patterns and AI-related competencies."], "limitations": "", "keywords": ["Large Language Models", "AI in Education", "Qualitative Analysis", "Human-AI Collaboration", "Teacher Development"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18013", "pdf": "https://arxiv.org/pdf/2507.18013.pdf", "abs": "https://arxiv.org/abs/2507.18013", "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "I.2.7"], "comment": "32 pages, 5 figures", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.", "AI": {"tldr": "Introduction of new TeleChat models (TeleChat2, TeleChat2.5, T1) offering performance upgrades through advanced training strategies.", "motivation": "To enhance the capabilities of language models with significant performance gains while addressing specific applications like code generation and mathematical reasoning.", "method": "The models utilize a training pipeline starting with pretraining on a large dataset followed by Supervised Fine-Tuning and Direct Preference Optimization, with the addition of continual pretraining for domain-specific tasks and reinforcement learning.", "result": "TeleChat2 series achieves notable improvements in reasoning and task performance, outperforming proprietary models, particularly in complex reasoning tasks and rapid inference.", "conclusion": "The TeleChat models are released for public use, enabling developers and researchers to leverage cutting-edge language models for a variety of applications.", "key_contributions": ["Introduction of new models TeleChat2, TeleChat2.5, and T1 with advanced training methodologies", "Demonstrated significant improvements in reasoning and task performance", "Public release of models tailored for diverse applications"], "limitations": "", "keywords": ["TeleChat", "Language Models", "Machine Learning", "Reinforcement Learning", "Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17997", "pdf": "https://arxiv.org/pdf/2507.17997.pdf", "abs": "https://arxiv.org/abs/2507.17997", "title": "Evaluating judgment of spatial correlation in visual displays of scalar field distributions", "authors": ["Yayan Zhao", "Matthew Berger"], "categories": ["cs.HC"], "comment": null, "summary": "In this work we study the identification of spatial correlation in\ndistributions of 2D scalar fields, presented across different forms of visual\ndisplays. We study simple visual displays that directly show color-mapped\nscalar fields, namely those drawn from a distribution, and whether humans can\nidentify strongly correlated spatial regions in these displays. In this\nsetting, the recognition of correlation requires making judgments on a set of\nfields, rather than just one field. Thus, in our experimental design we compare\ntwo basic visualization designs: animation-based displays against juxtaposed\nviews of scalar fields, along different choices of color scales. Moreover, we\ninvestigate the impacts of the distribution itself, controlling for the level\nof spatial correlation and discriminability in spatial scales. Our study's\nresults illustrate the impacts of these distribution characteristics, while\nalso highlighting how different visual displays impact the types of judgments\nmade in assessing spatial correlation. Supplemental material is available at\nhttps://osf.io/zn4qy", "AI": {"tldr": "The study investigates how different visual displays affect human identification of spatial correlation in 2D scalar fields, comparing animation and juxtaposed views with various color scales.", "motivation": "To understand how visual design influences human perception of spatial correlation in scalar fields.", "method": "The study involved experimental design comparing animation-based displays and juxtaposed views, while controlling for distribution characteristics related to spatial correlation and scale discriminability.", "result": "Results demonstrate how distribution characteristics and visual display types influence judgments on spatial correlation recognition among participants.", "conclusion": "The findings underscore the significance of visualization design in effectively conveying spatial correlation information in scalar fields.", "key_contributions": ["Comparison of animation-based displays versus juxtaposed views", "Assessment of visual display impacts on human judgment", "Insights into the effects of distribution characteristics on perception of spatial correlation."], "limitations": "The study may have limitations in generalizability beyond the specific types of displays investigated.", "keywords": ["Visualization", "Spatial Correlation", "2D Scalar Fields", "Judgment", "Human Perception"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.18028", "pdf": "https://arxiv.org/pdf/2507.18028.pdf", "abs": "https://arxiv.org/abs/2507.18028", "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "authors": ["Weizhi Fei", "Hao Shi", "Jing Xu", "Jingchen Peng", "Jiazheng Li", "Jingzhao Zhang", "Bo Bai", "Wei Han", "Zhenyuan Chen", "Xueyan Niu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).", "AI": {"tldr": "NeuralDB is an editing framework for LLMs, enhancing efficient knowledge updates while preserving general capabilities.", "motivation": "Efficiently editing knowledge in LLMs is crucial for timely model updates, but current methods risk compromising overall performance and causing memory issues.", "method": "NeuralDB models linear Locate-and-Edit methods as querying a Key-Value database, utilizing a non-linear gated retrieval module to selectively edit facts.", "result": "NeuralDB demonstrated superior performance in editing efficacy, generalization, specificity, fluency, and consistency while preserving overall performance across multiple tasks, even when expanded to edit 100,000 facts.", "conclusion": "NeuralDB effectively enables large-scale editing in LLMs without degrading their general abilities, showcasing scalability and effectiveness in multiple datasets.", "key_contributions": ["Introduction of NeuralDB as a neural KV database for LLM editing", "Utilization of a non-linear gated retrieval module to preserve model performance", "Demonstration of effectiveness in extensive editing scenarios up to 100,000 facts."], "limitations": "", "keywords": ["knowledge editing", "large language models", "neural databases", "gated retrieval", "L& E"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.18084", "pdf": "https://arxiv.org/pdf/2507.18084.pdf", "abs": "https://arxiv.org/abs/2507.18084", "title": "\"I Would Not Be This Version of Myself Today\": Elaborating on the Effects of Eudaimonic Gaming Experiences", "authors": ["Nisha Devasia", "Georgia Kenderova", "Michele Newman", "Julie Kientz", "Jin Ha Lee"], "categories": ["cs.HC"], "comment": "Accepted to CHI PLAY 2025", "summary": "While much of the research in digital games has emphasized hedonic\nexperiences, such as flow, enjoyment, and positive affect, recent years have\nseen increased interest in eudaimonic gaming experiences, typically\nmixed-affect and associated with personal meaningfulness and growth. The\nformation of such experiences in games is theorized to have four constituent\nelements: motivation, game use, experience, and effects. However, while the\nfirst three elements have been relatively well explored in the literature, the\neffects - and how they may influence positive individual outcomes - have been\nunderexplored thus far. To this end, in this work, we investigate the perceived\noutcomes of eudaimonic gaming and how different components of the experience\ninfluence these effects. We conducted a survey (n = 166) in which respondents\nrecounted meaningful gaming experiences and how they affected their present\nlives. We used a mixed-methods approach to classify effects and identify\nsignificant subcomponents of their formation. We contribute an empirical\nunderstanding of how meaningful gaming experiences can lead to positive\nreflective, learning, social, health, and career effects, extending current\ntheoretical models of eudaimonic gaming experiences and offering implications\nfor how researchers and practitioners might use these findings to promote\npositive outcomes for players.", "AI": {"tldr": "This paper investigates the perceived outcomes of eudaimonic gaming experiences and how various components influence these effects.", "motivation": "To explore the under-researched effects of eudaimonic gaming, focusing on personal meaningfulness and growth within game experiences.", "method": "A survey was conducted with 166 respondents recounting meaningful gaming experiences, utilizing a mixed-methods approach to classify effects and identify significant subcomponents.", "result": "Findings indicate that meaningful gaming experiences can lead to positive reflective, learning, social, health, and career effects.", "conclusion": "The study extends current theoretical models of eudaimonic gaming experiences and suggests practical implications for promoting positive outcomes in gaming.", "key_contributions": ["Empirical understanding of effects from eudaimonic gaming", "Identification of significant subcomponents of gaming experiences", "Implications for researchers and practitioners to enhance player outcomes"], "limitations": "", "keywords": ["eudaimonic gaming", "game effects", "meaningful experiences", "mixed-methods approach", "positive outcomes"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18043", "pdf": "https://arxiv.org/pdf/2507.18043.pdf", "abs": "https://arxiv.org/abs/2507.18043", "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS", "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.", "AI": {"tldr": "GrAInS is an inference-time steering method for LLMs and VLMs that uses gradient-based attribution to modify activations for improved task performance without model fine-tuning.", "motivation": "Most existing inference steering methods are limited by fixed interventions and do not leverage informative gradient signals from input tokens, especially in multimodal contexts.", "method": "GrAInS utilizes contrastive, gradient-based attribution via Integrated Gradients to identify influential tokens, constructs steering vectors, and adjusts hidden activations in transformer layers based on token-level attribution signals.", "result": "GrAInS demonstrates significant performance improvements, including a 13.22% accuracy gain on TruthfulQA, a reduction of hallucination rates on MMHal-Bench, and an 8.11% increase in alignment win rates on SPA-VL, while maintaining fluency.", "conclusion": "GrAInS offers a novel and effective method for steering LLM and VLM behavior at inference time, enabling modular and interpretable control without the need for retraining.", "key_contributions": ["Introduced GrAInS for flexible inference-time steering of multimodal models", "Demonstrated empirical superiority over fine-tuning and existing steering methods", "Provided a framework for token-level influence assessment in model responses"], "limitations": "", "keywords": ["inference-time steering", "large language models", "vision-language models", "gradient-based attribution", "token influence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18085", "pdf": "https://arxiv.org/pdf/2507.18085.pdf", "abs": "https://arxiv.org/abs/2507.18085", "title": "Effects of variation in system responsiveness on user performance in virtual environments", "authors": ["Benjamin Watson", "Neff Walker", "William Ribarsky", "Victoria Spaulding"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "System responsiveness (SR) is defined as the elapsed time until a system\nresponds to user control. SR fluctuates over time, so it must be described\nstatistically with mean (MSR) and standard deviation (SDSR). In this paper, we\nexamine SR in virtual environments (VEs), outlining its components and methods\nof experimental measurement and manipulation. Three studies of MSR and SDSR\neffects on performance of grasp and placement tasks are then presented. The\nstudies used within-subjects designs with 11, 12, and 10 participants,\nrespectively. Results showed that SDSR affected performance only if it was\nabove 82 ms. Placement required more frequent visual feedback and was more\nsensitive to SR. We infer that VE designers need not tightly control SDSR and\nmay wish to vary SR control based on required visual feedback frequency. These\nresults may be used to improve the human-computer interface in a wide range of\ninteractive graphical applications, including scientific visualization,\ntraining, mental health, and entertainment.", "AI": {"tldr": "This paper examines system responsiveness (SR) in virtual environments, focusing on how mean (MSR) and standard deviation (SDSR) affect task performance.", "motivation": "To understand how variability in system responsiveness impacts user performance in virtual environments and improve human-computer interfaces.", "method": "Three studies are conducted with participants to measure the effects of MSR and SDSR on performance in grasp and placement tasks, using within-subjects designs.", "result": "Results indicate that SDSR only impacts performance when it exceeds 82 ms, with placement tasks being particularly sensitive to SR.", "conclusion": "Designers of virtual environments should not overly restrict SDSR but adjust SR based on the frequency of required visual feedback for optimal performance.", "key_contributions": ["Detailed examination of system responsiveness (SR) components in virtual environments.", "Results indicating specific SDSR thresholds affecting task performance.", "Guidance for VE designers on managing SR and SDSR for user interfaces."], "limitations": "", "keywords": ["system responsiveness", "virtual environments", "human-computer interaction", "task performance", "visual feedback"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18044", "pdf": "https://arxiv.org/pdf/2507.18044.pdf", "abs": "https://arxiv.org/abs/2507.18044", "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model", "authors": ["Hoyeon Lee", "Sejung Son", "Ye-Eun Kang", "Jong-Hwan Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.", "AI": {"tldr": "This paper explores using large language models to generate synthetic phrase break annotations, reducing reliance on costly human annotations in text-to-speech systems.", "motivation": "Current methods for phrase break prediction rely on extensive human annotations, which are costly and labor-intensive, while variability in speech complicates data consistency.", "method": "We leverage large language models to create synthetic phrase break annotations and compare these with traditional human annotations across multiple languages.", "result": "The use of LLM-generated data effectively mitigates the challenges of phrase break prediction, demonstrating its applicability in the speech domain and across various languages.", "conclusion": "LLMs provide a promising alternative for generating high-quality synthetic data in phrase break prediction, potentially reducing manual efforts in speech tasks.", "key_contributions": ["Introduction of LLMs for generating synthetic phrase break annotations", "Comparison of LLM-based annotations with traditional methods", "Assessment of LLM effectiveness across multiple languages"], "limitations": "", "keywords": ["phrase break prediction", "large language models", "synthetic data generation", "text-to-speech systems", "speech domain"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18151", "pdf": "https://arxiv.org/pdf/2507.18151.pdf", "abs": "https://arxiv.org/abs/2507.18151", "title": "Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality", "authors": ["Shizhen Zhang", "Shengxin Li", "Quan Li"], "categories": ["cs.HC"], "comment": "Appear UIST2025", "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience\ncommunication challenges, primarily due to executive dysfunction and emotional\ndysregulation, even after years of social integration. While existing\ninterventions predominantly target children through structured or intrusive\nmethods, adults lack tools that translate clinical strategies into daily\ncommunication support. To address this gap, we present Understood, a Mixed\nReality (MR) system implemented on Microsoft HoloLens 2, designed to assist\nadults with ADHD in real-world communication. Through formative semi-structured\ninterviews and a design workshop, we identified critical communication barriers\nand derived design goals for the system. Understood combines three key\nfeatures: (1) real-time conversation summarization to reduce cognitive load,\n(2) context-aware subsequent word suggestions during moments of disfluency, and\n(3) topic shifting detection and reminding to mitigate off-topic transitions. A\nwithin-subjects user study and expert interviews demonstrate that Understood\neffectively supports communication with high usability, offering a complement\nto therapist-mediated interventions.", "AI": {"tldr": "Understood is a Mixed Reality system for adults with ADHD, aimed at facilitating communication by providing real-time support features.", "motivation": "Adults with ADHD face communication challenges due to executive dysfunction and emotional dysregulation, and existing interventions do not adequately support them in daily communication contexts.", "method": "Formative semi-structured interviews and a design workshop were conducted to identify communication barriers, leading to the development of the Understood system, which includes features like real-time conversation summarization, context-aware word suggestions, and topic shifting detection.", "result": "User studies and expert interviews indicated that Understood supports communication effectively, with high usability, complementing traditional therapeutic interventions.", "conclusion": "The Understood system provides significant support for adults with ADHD in real-world communication, filling a critical gap left by existing interventions.", "key_contributions": ["Introduction of a Mixed Reality system for adult ADHD communication support", "Real-time conversation summarization and context-aware word suggestions", "Detection and reminding of topic shifts during conversations"], "limitations": "The paper does not address long-term effectiveness or the system's impact on different communication contexts.", "keywords": ["ADHD", "Mixed Reality", "communication support", "real-time assistance", "user study"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18055", "pdf": "https://arxiv.org/pdf/2507.18055.pdf", "abs": "https://arxiv.org/abs/2507.18055", "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs", "authors": ["Tevin Atwal", "Chan Nam Tieu", "Yefeng Yuan", "Zhan Shi", "Yuhong Liu", "Liang Cheng"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.", "AI": {"tldr": "This paper assesses the diversity and privacy risks of synthetic data generated by Large Language Models (LLMs) and proposes enhancements for better data quality.", "motivation": "To address the growing challenges in evaluating the diversity and privacy risks associated with synthetic data generated by LLMs in data-driven applications.", "method": "The paper proposes a comprehensive set of metrics to quantify the diversity and privacy of text-based synthetic data, followed by an experimental evaluation and a prompt-based approach to improve data generation.", "result": "The findings indicate significant limitations in LLMs regarding the generation of diverse and privacy-preserving synthetic data.", "conclusion": "A prompt-based approach is recommended to enhance the diversity of synthetic data while safeguarding user privacy.", "key_contributions": ["Development of metrics for assessing diversity and privacy of synthetic data", "Empirical evaluation revealing LLMs' limitations", "Proposed prompt-based enhancement strategy for generating better synthetic data"], "limitations": "Focused primarily on text-based synthetic data and LLMs; may not generalize to other forms of synthetic data.", "keywords": ["Synthetic Data", "Large Language Models", "Diversity Metrics", "Privacy Risks", "Text Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.18165", "pdf": "https://arxiv.org/pdf/2507.18165.pdf", "abs": "https://arxiv.org/abs/2507.18165", "title": "ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent", "authors": ["Yuheng Zhao", "Xueli Shu", "Liwen Fan", "Lin Gao", "Yu Zhang", "Siming Chen"], "categories": ["cs.HC"], "comment": "11 pages, 8 figures", "summary": "Visual analytics (VA) is typically applied to complex data, thus requiring\ncomplex tools. While visual analytics empowers analysts in data analysis,\nanalysts may get lost in the complexity occasionally. This highlights the need\nfor intelligent assistance mechanisms. However, even the latest LLM-assisted VA\nsystems only provide help when explicitly requested by the user, making them\ninsufficiently intelligent to offer suggestions when analysts need them the\nmost. We propose a ProactiveVA framework in which LLM-powered UI agent monitors\nuser interactions and delivers context-aware assistance proactively. To design\neffective proactive assistance, we first conducted a formative study analyzing\nhelp-seeking behaviors in user interaction logs, identifying when users need\nproactive help, what assistance they require, and how the agent should\nintervene. Based on this analysis, we distilled key design requirements in\nterms of intent recognition, solution generation, interpretability and\ncontrollability. Guided by these requirements, we develop a three-stage UI\nagent pipeline including perception, reasoning, and acting. The agent\nautonomously perceives users' needs from VA interaction logs, providing\ntailored suggestions and intuitive guidance through interactive exploration of\nthe system. We implemented the framework in two representative types of VA\nsystems, demonstrating its generalizability, and evaluated the effectiveness\nthrough an algorithm evaluation, case and expert study and a user study. We\nalso discuss current design trade-offs of proactive VA and areas for further\nexploration.", "AI": {"tldr": "ProactiveVA is a framework that enhances visual analytics by using LLM-assisted agents to provide proactive, context-aware assistance during user interactions, improving the effectiveness of data analysis.", "motivation": "Analysts using visual analytics can become overwhelmed by complexity, which highlights the need for intelligent assistance mechanisms that go beyond reactive help.", "method": "A formative study analyzed user interaction logs to understand help-seeking behaviors, leading to the design of a three-stage UI agent pipeline that perceives user needs and offers tailored suggestions.", "result": "The framework was implemented in various visual analytics systems, demonstrating generalizability and effectiveness as evaluated by algorithm, case studies, expert assessments, and user studies.", "conclusion": "This research emphasizes current design trade-offs in proactive visual analytics and suggests directions for further research in intelligent assistance.", "key_contributions": ["Introduction of the ProactiveVA framework for proactive assistance", "Analysis of user help-seeking behaviors to inform design", "Implementation and evaluation across different visual analytics systems"], "limitations": "The study focuses on specific VA systems and may not address all potential user interactions or contexts.", "keywords": ["visual analytics", "proactive assistance", "LLM", "human-computer interaction", "user experience"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.18061", "pdf": "https://arxiv.org/pdf/2507.18061.pdf", "abs": "https://arxiv.org/abs/2507.18061", "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.", "AI": {"tldr": "TELEVAL is a dynamic benchmark designed to evaluate spoken language models (SLMs) as conversational agents in realistic settings, focusing on natural user interactions.", "motivation": "Existing benchmarks for spoken language models often fail to reflect how users interact in real-world conversations, necessitating a more user-centered evaluation framework.", "method": "TELEVAL defines three evaluation dimensions (Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities) and uses a dialogue format consistent with real-world usage to assess both text and audio outputs.", "result": "Experiments reveal that current SLMs still have significant limitations in natural conversational tasks, indicating a need for further improvements.", "conclusion": "TELEVAL aims to provide a framework that reflects user experience, facilitating the development of more capable dialogue-oriented SLMs.", "key_contributions": ["Introduction of TELEVAL as a new benchmark for evaluating SLMs in realistic settings", "Focus on user-centered evaluation reflecting natural interactions", "Identification of evaluation dimensions relevant to conversational abilities"], "limitations": "TELEVAL is specifically focused on Chinese interactive settings, which may limit its applicability to other languages or contexts.", "keywords": ["Spoken language models", "Benchmarking", "Conversational agents", "User-centered evaluation", "Natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.18169", "pdf": "https://arxiv.org/pdf/2507.18169.pdf", "abs": "https://arxiv.org/abs/2507.18169", "title": "Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners", "authors": ["Lorenzo Porcaro", "Chiara Monaldi"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Recommender systems shape music listening worldwide due to their widespread\nadoption in online platforms. Growing concerns about representational harms\nthat these systems may cause are nowadays part of the scientific and public\ndebate, wherein music listener perspectives are oftentimes reported and\ndiscussed from a cognitive-behaviorism perspective, but rarely contextualised\nunder a psychosocial and cultural lens. We proceed in this direction, by\ninterviewing a group of Italian music listeners and analysing their narratives\nthrough Emotional Textual Analysis. Thanks to this, we identify shared cultural\nrepertoires that reveal people's complex relationship with listening practices:\neven when familiar with online platforms, listeners may still lack a critical\nunderstanding of recommender systems. Moreover, representational issues,\nparticularly gender disparities, seem not yet fully grasped in the context of\nonline music listening. This study underscores the need for interdisciplinary\nresearch to address representational harms, and the role of algorithmic\nawareness and digital literacy in developing trustworthy recommender systems.", "AI": {"tldr": "This study examines music listeners' perspectives on recommender systems through interviews analyzed with Emotional Textual Analysis, highlighting cultural repertoires and the need for algorithmic awareness.", "motivation": "To explore the psychosocial and cultural implications of recommender systems in music listening, moving beyond traditional cognitive-behaviorism perspectives.", "method": "Interviews with Italian music listeners analyzed using Emotional Textual Analysis.", "result": "Identified shared cultural repertoires and noted listeners' lack of critical understanding concerning recommender systems and representational issues, especially regarding gender disparities.", "conclusion": "Interdisciplinary research is needed to tackle representational harms and promote algorithmic awareness and digital literacy in developing better recommender systems.", "key_contributions": ["Introduced a psychosocial and cultural analysis of recommender systems in music.", "Identified gaps in listeners' understanding of gender disparities in music recommendations.", "Emphasized the importance of interdisciplinary research for improving digital literacy."], "limitations": "Focus on a specific cultural group (Italian music listeners) may limit generalizability.", "keywords": ["recommender systems", "music listening", "cultural analysis", "algorithmic awareness", "digital literacy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18076", "pdf": "https://arxiv.org/pdf/2507.18076.pdf", "abs": "https://arxiv.org/abs/2507.18076", "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "categories": ["cs.CL"], "comment": "10 pages, 2 figures and 1 table", "summary": "Fine-tuning large language models (LLMs) remains a computational bottleneck\ndue to their scale and memory demands. This paper presents a comprehensive\nevaluation of parameter-efficient fine-tuning (PEFT) techniques, including\nLoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that\ndynamically integrates BOFT's orthogonal stability with LoRA-GA's\ngradient-aligned rapid convergence. By computing per-layer adaptive updates\nguided by gradient norms, the hybrid method achieves superior convergence\nefficiency and generalization across diverse tasks. We also explore, for the\nfirst time, the adaptation of unitary RNN (uRNN) principles to\ntransformer-based LLMs, enhancing gradient stability through structured unitary\nconstraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,\nand HumanEval -- using models ranging from 7B to 405B parameters demonstrate\nthat our hybrid method consistently outperforms individual PEFT baselines,\napproaching full fine-tuning accuracy while reducing resource consumption by up\nto 2.1 times in training time and 50 percent in memory usage. These findings\nestablish the hybrid approach as a practical and scalable fine-tuning solution\nfor real-world deployment of LLMs under resource constraints.", "AI": {"tldr": "This paper evaluates parameter-efficient fine-tuning techniques for large language models and introduces a hybrid method that combines the strengths of existing techniques to improve efficiency and resource management during training.", "motivation": "Fine-tuning large language models is computationally intensive, necessitating more efficient methods for real-world applications and deployment.", "method": "The paper reviews various parameter-efficient fine-tuning techniques and proposes a hybrid method that combines BOFT's stability with LoRA-GA's convergence speed, using per-layer adaptive updates based on gradient norms.", "result": "Empirical evaluations demonstrate that the hybrid method outperforms existing PEFT techniques, achieving similar accuracy to full fine-tuning while significantly reducing training time and memory usage.", "conclusion": "The proposed hybrid approach represents a scalable and practical solution for fine-tuning large language models, making it viable for resource-constrained environments.", "key_contributions": ["Introduction of a hybrid fine-tuning strategy combining BOFT and LoRA-GA techniques.", "Demonstration of the adaptation of unitary RNN principles to transformer LLMs for increased gradient stability.", "Empirical validation on multiple benchmarks showing efficiency improvements in fine-tuning."], "limitations": "", "keywords": ["large language models", "parameter-efficient fine-tuning", "gradient stability", "unitary RNN", "hybrid methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.18252", "pdf": "https://arxiv.org/pdf/2507.18252.pdf", "abs": "https://arxiv.org/abs/2507.18252", "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning", "authors": ["Dongyang Guo", "Yasmeen Abdrabou", "Enkeleda Thaqi", "Enkelejda Kasneci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.", "AI": {"tldr": "This paper presents a multimodal human-AI collaborative framework for enhanced cognitive pattern extraction from eye-tracking data, utilizing LLMs and hybrid anomaly detection.", "motivation": "Eye-tracking data provides insights into users' cognitive states but is challenging to analyze due to its structured nature; LLMs face difficulties with non-linguistic data.", "method": "The framework consists of a multi-stage pipeline for gaze pattern extraction, an Expert-Model Co-Scoring Module to integrate expert judgment with LLM output, and a hybrid anomaly detection module using LSTM and LLM-driven analysis.", "result": "The framework shows improvements in consistency, interpretability, and performance, achieving up to 50% accuracy in difficulty prediction tasks across several LLMs and prompt strategies.", "conclusion": "The presented approach provides a scalable and interpretable solution for cognitive modeling, with potential applications in areas like adaptive learning and human-computer interaction.", "key_contributions": ["Multimodal framework integrating LLMs with eye-tracking analysis", "Expert-Model Co-Scoring Module for trust score generation", "Hybrid anomaly detection combining LSTM and LLM-driven methods"], "limitations": "", "keywords": ["eye-tracking", "human-AI collaboration", "large language models", "cognitive modeling", "anomaly detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18103", "pdf": "https://arxiv.org/pdf/2507.18103.pdf", "abs": "https://arxiv.org/abs/2507.18103", "title": "A New Pair of GloVes", "authors": ["Riley Carlson", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.", "AI": {"tldr": "The report evaluates new 2024 English GloVe models, highlighting improvements in word representation and cultural relevance compared to the 2014 versions.", "motivation": "To provide updated word embeddings that reflect current language usage and improve upon the inadequacies of the original 2014 GloVe models.", "method": "Two sets of word embeddings were trained using data from Wikipedia, Gigaword, and Dolma, followed by evaluations through vocabulary comparison, testing, and NER tasks.", "result": "The 2024 GloVe models incorporate new relevant words and perform well in structural tasks, showing improved results on recent NER datasets, especially for non-Western data.", "conclusion": "The new models enhance word representation by including contemporary language and context, thereby improving NLP applications.", "key_contributions": ["Introduction of 2024 GloVe models with updated linguistic relevance", "Comprehensive documentation of data versions and preprocessing", "Demonstrated improved performance on recent NER tasks."], "limitations": "", "keywords": ["GloVe", "word embeddings", "NER", "natural language processing", "language evolution"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.18315", "pdf": "https://arxiv.org/pdf/2507.18315.pdf", "abs": "https://arxiv.org/abs/2507.18315", "title": "Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking", "authors": ["Rhys Jacka", "Paola R. Peña", "Sophie Leonard", "Éva Székely", "Benjamin R. Cowan"], "categories": ["cs.HC"], "comment": "12 pages, 3 figures, in Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces", "summary": "Speech disfluencies play a role in perspective-taking and audience design in\nhuman-human communication (HHC), but little is known about their impact in\nhuman-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one\nparticipants interacted with a speech agent using either fluent or disfluent\nspeech. Participants completed a partner-modelling questionnaire (PMQ) both\nbefore and after the task. Post-interaction evaluations indicated that\nparticipants perceived the disfluent agent as more competent, despite no\nsignificant differences in pre-task ratings. However, no notable differences\nwere observed in assessments of conversational flexibility or human-likeness.\nOur findings also reveal evidence of egocentric and allocentric language\nproduction when participants interact with speech agents. Interaction with\ndisfluent speech agents appears to increase egocentric communication in\ncomparison to fluent agents. Although the wide credibility intervals mean this\neffect is not clear-cut. We discuss potential interpretations of this finding,\nfocusing on how disfluencies may impact partner models and language production\nin HMD.", "AI": {"tldr": "This paper investigates the impact of speech disfluencies on human-machine dialogue, showing that participants perceive disfluent speech agents as more competent despite no differences in fluency ratings.", "motivation": "To explore how speech disfluencies affect human-machine dialogue and audience design, given their known role in human-human communication.", "method": "An online Namer-Matcher task where 61 participants interacted with a speech agent that used either fluent or disfluent speech, followed by a partner-modelling questionnaire.", "result": "Participants rated the disfluent speech agent as more competent post-interaction, although pre-task ratings showed no significant differences. No notable differences in conversational flexibility or human-likeness were recorded.", "conclusion": "Disfluent agents may enhance egocentric communication but the findings are not definitive due to wide credibility intervals; implications for partner models and language production in HMD are discussed.", "key_contributions": ["Investigates the role of disfluencies in human-machine dialogue", "Demonstrates perceptions of competence linked to disfluent speech agents", "Analyzes the impact on egocentric vs allocentric language production"], "limitations": "Credibility intervals are wide, suggesting the effects observed may not be clear-cut.", "keywords": ["speech disfluencies", "human-machine dialogue", "competence perception", "egocentric communication", "language production"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.18119", "pdf": "https://arxiv.org/pdf/2507.18119.pdf", "abs": "https://arxiv.org/abs/2507.18119", "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.", "AI": {"tldr": "GOAT-SLM is a novel spoken language model designed to incorporate paralinguistic and speaker characteristic cues for improved spoken interactions.", "motivation": "Most existing spoken language models overlook essential non-linguistic cues such as emotion and dialect. This work aims to fill that gap by enhancing spoken language modeling to include such characteristics.", "method": "GOAT-SLM uses a dual-modality head architecture to separate linguistic modeling from acoustic realization, employing a modular training strategy that progressively aligns various information using large-scale speech-text datasets.", "result": "Experimental results show that GOAT-SLM achieves balanced performance across semantic and non-semantic tasks, outperforming other models in aspects like emotion and dialect sensitivity.", "conclusion": "The introduction of GOAT-SLM emphasizes the need for spoken language models to consider both linguistic and paralinguistic elements, paving the way for more socially aware systems.", "key_contributions": ["Introduction of GOAT-SLM with paralinguistic awareness", "Dual-modality head architecture for separating linguistic from acoustic features", "Modular training strategy for aligning linguistic and non-linguistic cues"], "limitations": "", "keywords": ["Spoken Language Models", "Paralinguistic Awareness", "Emotion Recognition"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.18393", "pdf": "https://arxiv.org/pdf/2507.18393.pdf", "abs": "https://arxiv.org/abs/2507.18393", "title": "PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses", "authors": ["Mahiro Ozaki", "Li Chen", "Shotaro Naganuma", "Valdemar Švábenský", "Fumiya Okubo", "Atsushi Shimada"], "categories": ["cs.HC", "cs.CY"], "comment": "To appear in the Proceedings of the IEEE SMC 2025 conference", "summary": "This study proposes and evaluates the PAnoramic Learning Map (PALM), a\nlearning analytics (LA) dashboard designed to address the scalability\nchallenges of LA by integrating curriculum-level information. Traditional LA\nresearch has predominantly focused on individual courses or learners and often\nlacks a framework that considers the relationships between courses and the\nlong-term trajectory of learning. To bridge this gap, PALM was developed to\nintegrate multilayered educational data into a curriculum map, enabling\nlearners to intuitively understand their learning records and academic\nprogression. We conducted a system evaluation to assess PALM's effectiveness in\ntwo key areas: (1) its impact on students' awareness of their learning\nbehaviors, and (2) its comparative performance against existing systems. The\nresults indicate that PALM enhances learners' awareness of study planning and\nreflection, particularly by improving perceived behavioral control through the\nvisual presentation of individual learning histories and statistical trends,\nwhich clarify the links between learning actions and outcomes. Although PALM\nrequires ongoing refinement as a system, it received significantly higher\nevaluations than existing systems in terms of visual appeal and usability. By\nserving as an information resource with previously inaccessible insights, PALM\nenhances self-regulated learning and engagement, representing a significant\nstep beyond conventional LA toward a comprehensive and scalable approach.", "AI": {"tldr": "This study presents PALM, a learning analytics dashboard that enhances students' awareness of their learning behaviors and academic progression by integrating curriculum-level information.", "motivation": "Traditional learning analytics research is limited by focusing on individual courses or learners without considering the broader curriculum context.", "method": "The PALM system integrates multilayered educational data into a curriculum map, and its effectiveness was evaluated based on its impact on students' learning awareness and its performance relative to existing systems.", "result": "PALM significantly improves learners' awareness of study planning and reflection and performs better in visual appeal and usability compared to existing systems.", "conclusion": "PALM represents a substantial advancement in learning analytics, promoting self-regulated learning and engagement through improved insights into academic progress.", "key_contributions": ["Introduction of a comprehensive learning analytics dashboard (PALM)", "Enhanced understanding of student learning behaviors and academic progression", "Significantly better visual appeal and usability compared to existing systems."], "limitations": "PALM requires ongoing refinement as a system.", "keywords": ["Learning Analytics", "Educational Data", "Curriculum Mapping"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18140", "pdf": "https://arxiv.org/pdf/2507.18140.pdf", "abs": "https://arxiv.org/abs/2507.18140", "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning", "authors": ["Xiaoyuan Li", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Recent progress in Multi-modal Large Language Models (MLLMs) has enabled\nstep-by-step multi-modal mathematical reasoning by performing visual operations\nbased on the textual instructions. A promising approach uses code as an\nintermediate representation to precisely express and manipulate the images in\nthe reasoning steps. However, existing evaluations focus mainly on text-only\nreasoning outputs, leaving the MLLM's ability to perform accurate visual\noperations via code largely unexplored. This work takes a first step toward\naddressing that gap by evaluating MLLM's code-based capabilities in multi-modal\nmathematical reasoning.Specifically, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's\nability to accurately understand and construct visualizations from scratch. (2)\nMulti-modal Code Editing (MCE) assesses the model's capacity for fine-grained\noperations, which include three types: Deletion, Modification and Annotation.\nTo evaluate the above tasks, we incorporate a dataset that covers the five most\npopular types of mathematical figures, including geometric diagrams, function\nplots, and three types of statistical charts, to provide a comprehensive and\neffective measurement of existing MLLMs. Our experimental evaluation involves\nnine mainstream MLLMs, and the results reveal that existing models still lag\nsignificantly behind human performance in performing fine-grained visual\noperations.", "AI": {"tldr": "This work evaluates multi-modal large language models (MLLMs) on their ability to perform mathematical reasoning via code in visual operations, revealing a gap in their performance compared to humans.", "motivation": "Existing evaluations of MLLMs mainly focus on text-only reasoning, which overlooks their capabilities in executing accurate visual operations through code.", "method": "The study introduces a framework to assess MLLMs through two tasks: Multi-modal Code Generation (MCG) for creating visualizations and Multi-modal Code Editing (MCE) for executing fine-grained operations (deletion, modification, annotation) on existing visualizations.", "result": "The experiments involving nine mainstream MLLMs show that these models significantly underperform compared to human capabilities in executing fine-grained visual operations.", "conclusion": "This work highlights the need for further development in MLLMs' code-based visual operation capabilities to reach human-level performance.", "key_contributions": ["Introduction of a framework for evaluating MLLMs in multi-modal mathematical reasoning", "Identification of the gap in performance between MLLMs and human abilities in visual reasoning tasks", "Development of a dataset covering various types of mathematical figures for robust evaluation"], "limitations": "The study is limited to the capabilities of current popular MLLMs and does not explore new model architectures or approaches.", "keywords": ["Multi-modal Large Language Models", "mathematical reasoning", "code generation", "visual operations", "evaluation framework"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.18401", "pdf": "https://arxiv.org/pdf/2507.18401.pdf", "abs": "https://arxiv.org/abs/2507.18401", "title": "Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols", "authors": ["Andrew Jeyathasan", "Swati Banerjee"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "We experience the world through multiple senses that work together to create\na cohesive perception, whether in daily life or immersive technologies.\nUnderstanding this multisensory integration (MSI) requires examining the\ninteractions between sensory modalities, each with unique temporal dynamics and\ncharacteristics. While most research focuses on unimodal or bimodal cues, the\nintegration of three or more modalities remains underexplored. MSI studies must\naccount for factors like cross-modal correspondence, congruence, cognitive\nload, and stimulus timing, which become increasingly complex as modalities\nmultiply. This article examines these key factors and how they can be applied\nto 8 design effective MSI study protocols.", "AI": {"tldr": "This article explores the complexities of multisensory integration (MSI) by examining factors that affect the interaction of three or more sensory modalities, focusing on designs for effective MSI study protocols.", "motivation": "To better understand how multiple senses work together in creating cohesive perception, especially for immersive technologies.", "method": "The article discusses factors such as cross-modal correspondence, congruence, cognitive load, and stimulus timing relevant to the integration of multiple sensory modalities.", "result": "Identifies key factors affecting multisensory integration and proposes ways to design effective study protocols that take these factors into account.", "conclusion": "A better understanding of multi-sensory integration can enhance the design of immersive technologies and contribute to more effective MSI studies.", "key_contributions": ["Exploration of interactions between three or more sensory modalities", "Identification of relevant factors affecting MSI", "Recommendations for designing effective MSI study protocols"], "limitations": "", "keywords": ["multisensory integration", "human-computer interaction", "stimulus timing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18143", "pdf": "https://arxiv.org/pdf/2507.18143.pdf", "abs": "https://arxiv.org/abs/2507.18143", "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "authors": ["Gonzalo Cardenal Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.", "AI": {"tldr": "This study evaluates LLM capabilities in HIV management, introducing the HIVMedQA benchmark for assessing medical question answering performance, and highlights the need for careful integration of LLMs in clinical practice.", "motivation": "The complexity of HIV management presents a compelling use case for integrating LLMs into clinical decision-making, but concerns about accuracy and clinician acceptance exist.", "method": "The study utilized prompt engineering and evaluated seven general-purpose and three medically specialized LLMs against a benchmark dataset of curated clinical questions, assessing performance based on comprehension, reasoning, recall, bias, harm, and accuracy.", "result": "Gemini 2.5 Pro outperformed all other evaluated models, though performance declined with increased question complexity, and no strong correlation was found between model size and performance.", "conclusion": "The findings indicate the necessity for targeted development to ensure the safe and effective use of LLMs in clinical settings, particularly in complex domains like HIV care.", "key_contributions": ["Introduction of HIVMedQA benchmark for HIV-related question answering", "Evaluation of LLM performance using clinical relevance parameters", "Insights on the performance trade-offs of general-purpose vs. medically fine-tuned models"], "limitations": "Limited exploration of AI applications specifically for HIV care and concerns about the accuracy and safety of LLMs in clinical practice.", "keywords": ["Large Language Models", "HIV Management", "Clinical Practice", "Medical Question Answering", "AI in Health"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.18428", "pdf": "https://arxiv.org/pdf/2507.18428.pdf", "abs": "https://arxiv.org/abs/2507.18428", "title": "Towards Understanding Decision Problems As a Goal of Visualization Design", "authors": ["Lena Cibulski", "Stefan Bruckner"], "categories": ["cs.HC"], "comment": null, "summary": "Decision-making is a central yet under-defined goal in visualization\nresearch. While existing task models address decision processes, they often\nneglect the conditions framing a decision. To better support decision-making\ntasks, we propose a characterization scheme that describes decision problems\nthrough key properties of the data, users, and task context. This scheme helps\nvisualization researchers specify decision-support claims more precisely and\ninforms the design of appropriate visual encodings and interactions. We\ndemonstrate the utility of our approach by applying it to characterize decision\ntasks targeted by existing design studies, highlighting opportunities for\nfuture research in decision-centric visualization.", "AI": {"tldr": "The paper proposes a characterization scheme for decision-making in visualization research, focusing on data, users, and task context to enhance decision-support designs.", "motivation": "To improve the support for decision-making tasks in visualization research, which is often under-defined.", "method": "The authors propose a characterization scheme that outlines key properties of the data, users, and task context relating to decision problems.", "result": "The scheme highlights the conditions framing decisions and informs the design of visual encodings and interactions, showcasing its utility through examples from existing design studies.", "conclusion": "This characterization scheme can help visualization researchers more accurately specify decision-support claims and identify future research opportunities.", "key_contributions": ["A new characterization scheme for decision problems in visualization", "Improved precision in specifying decision-support claims", "Identification of future research opportunities in decision-centric visualization."], "limitations": "", "keywords": ["decision-making", "visualization research", "decision-support", "characterization scheme", "interaction design"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18171", "pdf": "https://arxiv.org/pdf/2507.18171.pdf", "abs": "https://arxiv.org/abs/2507.18171", "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models", "authors": ["Kexin Chen", "Dongxia Wang", "Yi Liu", "Haonan Zhang", "Wenhai Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main", "summary": "Despite the widespread use of Transformer-based text embedding models in NLP\ntasks, surprising 'sticky tokens' can undermine the reliability of embeddings.\nThese tokens, when repeatedly inserted into sentences, pull sentence similarity\ntoward a certain value, disrupting the normal distribution of embedding\ndistances and degrading downstream performance. In this paper, we\nsystematically investigate such anomalous tokens, formally defining them and\nintroducing an efficient detection method, Sticky Token Detector (STD), based\non sentence and token filtering. Applying STD to 40 checkpoints across 14 model\nfamilies, we discover a total of 868 sticky tokens. Our analysis reveals that\nthese tokens often originate from special or unused entries in the vocabulary,\nas well as fragmented subwords from multilingual corpora. Notably, their\npresence does not strictly correlate with model size or vocabulary size. We\nfurther evaluate how sticky tokens affect downstream tasks like clustering and\nretrieval, observing significant performance drops of up to 50%. Through\nattention-layer analysis, we show that sticky tokens disproportionately\ndominate the model's internal representations, raising concerns about\ntokenization robustness. Our findings show the need for better tokenization\nstrategies and model design to mitigate the impact of sticky tokens in future\ntext embedding applications.", "AI": {"tldr": "This paper investigates 'sticky tokens' in Transformer-based text embeddings, which distort sentence similarity and degrade performance in NLP tasks. It introduces a detection method and reveals significant adverse effects on downstream applications.", "motivation": "To address the negative impact of sticky tokens on the reliability of embeddings used in NLP tasks.", "method": "Introduces a detection method called Sticky Token Detector (STD), analyzing 40 checkpoints across 14 model families to identify anomalous tokens.", "result": "Discovered 868 sticky tokens that primarily originate from unused vocabulary entries and subwords in multilingual corpora. Their presence can cause performance drops up to 50% in downstream tasks like clustering and retrieval.", "conclusion": "The study underscores the necessity for improved tokenization strategies and model design to combat the detrimental effects of sticky tokens in text embedding applications.", "key_contributions": ["Formal definition and detection method for sticky tokens", "Analysis revealing the origin of these tokens", "Evaluation of their impact on NLP task performance"], "limitations": "The study focuses on specific models and may not generalize to all NLP frameworks or embeddings.", "keywords": ["Sticky tokens", "Text embeddings", "NLP", "Tokenization", "Model performance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18450", "pdf": "https://arxiv.org/pdf/2507.18450.pdf", "abs": "https://arxiv.org/abs/2507.18450", "title": "High-Dimensional Data Classification in Concentric Coordinates", "authors": ["Alice Williams", "Boris Kovalerchuk"], "categories": ["cs.HC", "cs.LG"], "comment": "8 pages, 21 figures", "summary": "The visualization of multi-dimensional data with interpretable methods\nremains limited by capabilities for both high-dimensional lossless\nvisualizations that do not suffer from occlusion and that are computationally\ncapable by parameterized visualization. This paper proposes a low to high\ndimensional data supporting framework using lossless Concentric Coordinates\nthat are a more compact generalization of Parallel Coordinates along with\nformer Circular Coordinates. These are forms of the General Line Coordinate\nvisualizations that can directly support machine learning algorithm\nvisualization and facilitate human interaction.", "AI": {"tldr": "This paper proposes a framework for visualizing multi-dimensional data using lossless Concentric Coordinates, enhancing interpretability and interaction in machine learning.", "motivation": "The visualization of high-dimensional data faces challenges such as occlusion and the need for computationally feasible methods to support machine learning visualization.", "method": "The authors introduce Concentric Coordinates as a generalization of existing visualization techniques like Parallel and Circular Coordinates, aimed at improving multi-dimensional data interpretation.", "result": "This new framework allows for lossless visualization of data across varying dimensions, enhancing clarity and user interaction.", "conclusion": "The proposed method significantly improves the visualization capabilities for machine learning algorithms, leading to better interpretability for users.", "key_contributions": ["Introduction of lossless Concentric Coordinates for high-dimensional data", "Improvement of human interaction with machine learning visualizations", "Generalization of existing visualization methods to enhance clarity"], "limitations": "", "keywords": ["multi-dimensional data", "visualization", "machine learning", "Concentric Coordinates", "Human-Computer Interaction"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.18182", "pdf": "https://arxiv.org/pdf/2507.18182.pdf", "abs": "https://arxiv.org/abs/2507.18182", "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models", "authors": ["Wonjun Jeong", "Dongseok Kim", "Taegkeun Whangbo"], "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 1 figure", "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.", "AI": {"tldr": "SCOPE is a framework that addresses selection bias in LLM evaluations by estimating and mitigating position-bias distributions, leading to more reliable performance assessments.", "motivation": "To tackle the inflated scores of LLMs on multiple-choice tasks that arise from selection bias rather than true understanding of the material.", "method": "SCOPE uses a null prompt to estimate models' position-bias, redistributes answer slots based on inverse-bias distributions, and prevents proximity cues from leading to incorrect guesses.", "result": "SCOPE showed consistent performance improvements over existing debiasing methods and clearer confidence distributions across multiple benchmark experiments.", "conclusion": "SCOPE establishes a new standard for fairness and reliability in LLM evaluations, improving assessment outcomes by mitigating biases.", "key_contributions": ["Introduces SCOPE framework for bias mitigation in LLM evaluations", "Establishes new methods for measuring and redistributing selection bias", "Demonstrates superior performance and reliability compared to existing methods."], "limitations": "", "keywords": ["Large Language Models", "Bias Mitigation", "Evaluation Framework", "Selection Bias", "Debiasing Methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18510", "pdf": "https://arxiv.org/pdf/2507.18510.pdf", "abs": "https://arxiv.org/abs/2507.18510", "title": "ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR", "authors": ["Chenyang Zhang", "Tiffany S Ma", "John Andrews", "Eric J Gonzalez", "Mar Gonzalez-Franco", "Yalong Yang"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial interaction in 3D environments requires balancing efficiency and\nprecision, which requires dynamic tracking speed adjustments. However, existing\ntechniques often couple tracking speed adjustments directly with hand\nmovements, reducing interaction flexibility. Inspired by the natural friction\ncontrol inherent in the physical world, we introduce ForcePinch, a novel\nforce-responsive spatial interaction method that enables users to intuitively\nmodulate pointer tracking speed and smoothly transition between rapid and\nprecise movements by varying their pinching force. To implement this concept,\nwe developed a hardware prototype integrating a pressure sensor with a\ncustomizable mapping function that translates pinching force into tracking\nspeed adjustments. We conducted a user study with 20 participants performing\nwell-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch\nagainst the distance-responsive technique Go-Go and speed-responsive technique\nPRISM. Results highlight distinctive characteristics of the force-responsive\napproach across different interaction contexts. Drawing on these findings, we\nhighlight the contextual meaning and versatility of force-responsive\ninteractions through four illustrative examples, aiming to inform and inspire\nfuture spatial interaction design.", "AI": {"tldr": "ForcePinch introduces a force-responsive spatial interaction method that adjusts pointer tracking speed based on users' pinching force, improving efficiency and precision in 3D object manipulation.", "motivation": "To enhance spatial interaction in 3D environments by allowing users to intuitively modulate tracking speed without sacrificing flexibility.", "method": "Development of a hardware prototype that integrates a pressure sensor with a customizable mapping function to translate pinching force into tracking speed.", "result": "User studies showed favorable outcomes for ForcePinch compared to distance-responsive and speed-responsive techniques, highlighting its contextual advantages in interaction.", "conclusion": "ForcePinch demonstrates the potential of force-responsive interactions to inform and inspire future designs in spatial interaction.", "key_contributions": ["Introduction of a novel force-responsive interaction method for 3D environments.", "Development of a prototype that links physical force with digital interaction.", "Empirical evidence of effectiveness through user studies comparing existing techniques."], "limitations": "Limited sample size in user study, specifics of mapping function customization not fully explored.", "keywords": ["Spatial Interaction", "Force-responsive", "3D environments", "User Study", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18190", "pdf": "https://arxiv.org/pdf/2507.18190.pdf", "abs": "https://arxiv.org/abs/2507.18190", "title": "TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks", "authors": ["Keyu Wu", "Qianjin Yu", "Manlin Mei", "Ruiting Liu", "Jun Wang", "Kailai Zhang", "Yelun Bao"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Root Cause Analysis (RCA) in telecommunication networks is a critical task,\nyet it presents a formidable challenge for Artificial Intelligence (AI) due to\nits complex, graph-based reasoning requirements and the scarcity of realistic\nbenchmarks.", "AI": {"tldr": "The paper discusses the challenges of applying AI to Root Cause Analysis in telecommunication networks, focusing on graph-based reasoning and the lack of benchmarks.", "motivation": "Root Cause Analysis (RCA) is essential in maintaining effective telecommunication networks but poses significant difficulties for AI systems due to its intricate reasoning demands and insufficient realistic datasets for training and evaluation.", "method": "", "result": "", "conclusion": "", "key_contributions": [], "limitations": "", "keywords": ["Root Cause Analysis", "Artificial Intelligence", "Telecommunication Networks", "Graph-based Reasoning", "Benchmarks"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18572", "pdf": "https://arxiv.org/pdf/2507.18572.pdf", "abs": "https://arxiv.org/abs/2507.18572", "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design", "authors": ["Donghoon Shin", "Daniel Lee", "Gary Hsieh", "Gromit Yeuk-Yin Chan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives.", "AI": {"tldr": "PosterMate is a generative AI poster design assistant that facilitates collaborative feedback using audience-driven persona agents.", "motivation": "Gathering diverse feedback in poster designing is challenging, and generative AI may improve this process.", "method": "PosterMate creates persona agents from marketing documents to gather feedback on poster components, stimulating discussion to reconcile perspectives.", "result": "User studies revealed PosterMate captures overlooked viewpoints and serves as an effective prototyping tool; feedback aligns with persona identity, and discussions synthesize diverse perspectives.", "conclusion": "PosterMate shows potential for improving design feedback by utilizing generative AI to simulate audience interactions and facilitate agreement in design edits.", "key_contributions": ["Introduction of PosterMate, an audience-driven design assistant", "Demonstration of effective collaboration through persona agents", "Evidence of performance through user studies and controlled evaluations"], "limitations": "", "keywords": ["Poster design", "Generative AI", "User feedback", "Persona agents", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.18197", "pdf": "https://arxiv.org/pdf/2507.18197.pdf", "abs": "https://arxiv.org/abs/2507.18197", "title": "Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization", "authors": ["Aline Belloni", "Patrick Prieur"], "categories": ["cs.CL", "cs.DL"], "comment": "in French language. AGeCSO2025 : 18{\\`e}me Colloque International de\n  l'Association pour la Gestion des Connaissances dans la Soci{\\'e}t{\\'e} et\n  les Organisations, Association pour la Gestion des Connaissances dans la\n  Soci{\\'e}t{\\'e} et les Organisations (AGECSO), Jun 2025, TROYES, France", "summary": "Business process modeling is used by most organizations as an essential\nframework for ensuring efficiency and effectiveness of the work and workflow\nperformed by its employees and for ensuring the alignment of such work with its\nstrategic goals. For organizations that are compliant or near-compliant with\nISO 9001, this approach involves the detailed mapping of processes,\nsub-processes, activities, and tasks. ISO30401 is a Management System Standard,\nintroduced in 2018, establishing universal requirements for the set up of a\nKnowledge Management System in an organization. As ``ISO30401 implementers'' we\nregularly face the challenge of explaining our clients how the knowledge\ndevelopment, transformation and conveyances activities depicted in ISO30401 do\nintegrate with existing operational processes. This article recaps process\nmodelling principles in the context of ISO9001 and explores, based on our\nexperience, how an ISO30401-compliant Knowledge Management System (KMS)\nentwines with all other processes of an Integrated Management System and in\nparticular how it can be implemented by deploying the mechanisms of the SECI\nmodel through the steps of PDCA cycles.", "AI": {"tldr": "This paper discusses the integration of Knowledge Management Systems (KMS) with ISO 9001 compliant operational processes, emphasizing the SECI model and PDCA cycles.", "motivation": "To explain the integration of knowledge development and management activities within existing operational processes in organizations compliant with ISO 9001 and ISO 30401.", "method": "The article recaps process modeling principles under ISO 9001 and explores the integration of ISO 30401-compliant KMS with operational processes using the SECI model and PDCA cycles.", "result": "The study illustrates how ISO 30401 principles can enhance the effectiveness of organizational processes by synchronizing knowledge management and workflow activities.", "conclusion": "A well-implemented KMS based on ISO 30401 not only streamlines knowledge activities but aligns them with strategic goals, improving overall organizational efficiency.", "key_contributions": ["Integration of ISO 30401 into existing operational processes", "Application of SECI model in organizational settings", "Alignment of Knowledge Management with ISO 9001 standards"], "limitations": "", "keywords": ["ISO 9001", "ISO 30401", "Knowledge Management System", "SECI model", "PDCA cycles"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2507.18619", "pdf": "https://arxiv.org/pdf/2507.18619.pdf", "abs": "https://arxiv.org/abs/2507.18619", "title": "MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss", "authors": ["Yichen Yu", "Qiaoran Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Children with hearing impairments face ongoing challenges in language and\nmotor development. This study explores how multi-sensory feedback technology\nbased on virtual reality (VR), integrating auditory, visual, and tactile\nstimuli, can enhance rehabilitation outcomes. Using functional near-infrared\nspectroscopy (fNIRS) technology, we assessed cortical activation patterns in\nchildren during pitch-matching tasks across different interaction modes. Our\nfindings aim to provide evidence for designing personalized, interactive\nrehabilitation systems that enhance cognitive engagement and motor control in\nchildren with hearing impairments.", "AI": {"tldr": "This study investigates the effectiveness of multi-sensory feedback technology using virtual reality to aid rehabilitation in children with hearing impairments.", "motivation": "Children with hearing impairments struggle with language and motor development, necessitating innovative rehabilitation approaches.", "method": "We utilized functional near-infrared spectroscopy (fNIRS) to assess cortical activation patterns in children during pitch-matching tasks with varying interaction modes.", "result": "The study found distinct cortical activation patterns that suggest enhanced cognitive engagement and motor control when employing VR-based multi-sensory feedback.", "conclusion": "The results support the use of personalized, interactive rehabilitation systems to improve outcomes for children with hearing impairments.", "key_contributions": ["Integration of multi-sensory feedback in VR for rehabilitation", "Assessment of cortical activation using fNIRS in real-time", "Evidence supporting design of interactive rehabilitation systems"], "limitations": "", "keywords": ["multi-sensory feedback", "virtual reality", "hearing impairments", "rehabilitation", "fNIRS"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.18202", "pdf": "https://arxiv.org/pdf/2507.18202.pdf", "abs": "https://arxiv.org/abs/2507.18202", "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection", "authors": ["San Kim", "Jonghwi Kim", "Yejin Jeon", "Gary Geunbae Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, accepted to ACL Findings 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings.", "AI": {"tldr": "This paper presents GMTP, a defense mechanism to detect and filter adversarial documents in Retrieval-Augmented Generation systems using gradient analysis.", "motivation": "The increasing use of external sources in LLMs poses security risks from attackers injecting poisoned documents, leading to harmful outputs.", "method": "The proposed GMTP method examines the gradients of the retriever's similarity function to identify high-impact tokens, which are then masked and their probabilities checked using an MLM to detect poisoned content.", "result": "GMTP can eliminate over 90% of poisoned documents while maintaining the relevance of legitimate content, thus ensuring effective retrieval and generation.", "conclusion": "This paper showcases GMTP as a robust solution for enhancing the security of RAG systems against adversarial attacks without compromising performance.", "key_contributions": ["Introduction of GMTP as a novel defense method", "Demonstration of high-precision filtering of poisoned documents", "Validation of GMTP's effectiveness across diverse datasets and adversarial settings."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "adversarial defense", "poisoning attacks", "masked language modeling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.18622", "pdf": "https://arxiv.org/pdf/2507.18622.pdf", "abs": "https://arxiv.org/abs/2507.18622", "title": "Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork", "authors": ["Armin Bernstetter", "Tom Kwasnitschka", "Isabella Peters"], "categories": ["cs.HC"], "comment": "Accepted for Mensch und Computer 2025 Short Paper Track", "summary": "Ensuring reproducibility of research is an integral part of good scientific\npractice. One way to support this is through provenance: information about\nresearch workflows from data gathering to researchers' sensemaking processes\nleading to published results. This is highly important in disciplines such as\ngeosciences, where researchers use software for interactive and immersive\nvisualizations of geospatial data, doing virtual measurements in simulated\nfieldwork on 3D models. We evaluated a provenance management tool, which allows\nrecording of interactions with a virtual fieldwork tool and annotating\ndifferent states of the visualization. The user study investigated how\nresearchers used this Digital Lab Book (DLB) and whether perceived ease of use\nand perceived usefulness differed between groups in immersive or non-immersive\nsettings. Participants perceived the DLB as both useful and easy to use. While\nthere were indications of differences in perceived ease of use (higher for\nimmersive setting), usage patterns showed no significant group differences.", "AI": {"tldr": "The paper evaluates a provenance management tool for tracking research workflows in immersive and non-immersive environments, demonstrating perceived usefulness and ease of use, particularly in immersive settings.", "motivation": "To support reproducibility in research by managing provenance of workflows, particularly in immersive geospatial data visualization scenarios.", "method": "A user study was conducted to assess how researchers interacted with a Digital Lab Book (DLB) for recording their engagements with a virtual fieldwork tool.", "result": "Participants found the DLB useful and easy to use, with indicators of higher perceived ease of use in the immersive setting; however, usage patterns did not reveal significant differences between the two settings.", "conclusion": "While both immersive and non-immersive users found the DLB beneficial, the study highlights the need for further investigation into the impacts of context on tool usage.", "key_contributions": ["Evaluation of a provenance management tool for research workflows", "User study comparing immersive and non-immersive settings", "Insights into perceived usefulness and ease of use of the DLB among researchers."], "limitations": "No significant group differences in actual usage patterns were found despite perceived differences in ease of use.", "keywords": ["provenance management", "reproducibility", "geospatial data", "immersive visualization", "user study"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18203", "pdf": "https://arxiv.org/pdf/2507.18203.pdf", "abs": "https://arxiv.org/abs/2507.18203", "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation", "authors": ["Kyubeen Han", "Junseo Jang", "Hongjin Kim", "Geunyeong Jeong", "Harksoo Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Accepted", "summary": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications.", "AI": {"tldr": "This paper investigates how instruction-tuning affects large language models' susceptibility to misinformation, showing an increased reliance on user input leading to greater misinformation acceptance.", "motivation": "The motivation of the paper is to understand the effects of instruction-tuning on the behavior of large language models, specifically their handling of misinformation presented by users.", "method": "The study conducts an analysis comparing instruction-tuned LLMs with base models to measure their susceptibility to misinformation under various conditions.", "result": "Instruction-tuned LLMs are significantly more likely to accept misinformation from users compared to base models, indicating a shift in susceptibility from the model to the user.", "conclusion": "The findings highlight the need for systematic approaches to mitigate the unintended consequences of instruction-tuning, emphasizing the importance of improving LLM reliability in practical applications.", "key_contributions": ["Investigates the effects of instruction-tuning on misinformation susceptibility in LLMs", "Compares instruction-tuned models with base models", "Examines user prompt structure and its impacts on misinformation acceptance."], "limitations": "The study primarily focuses on the impact of instruction-tuning without exploring other potential influences on misinformation acceptance comprehensively.", "keywords": ["instruction-tuning", "large language models", "misinformation susceptibility", "user input", "system prompts"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.18212", "pdf": "https://arxiv.org/pdf/2507.18212.pdf", "abs": "https://arxiv.org/abs/2507.18212", "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation", "authors": ["Xinrui Chen", "Hongxing Zhang", "Fanyi Zeng", "Yongxian Wei", "Yizhi Wang", "Xitong Ling", "Guanghao Li", "Chun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%.", "AI": {"tldr": "This paper introduces Prune&Comp, a layer pruning scheme for large language models that mitigates performance degradation due to layer removal by rescaling weights offline while maintaining performance metrics.", "motivation": "To address significant performance degradation caused by the removal of layers in large language models during pruning.", "method": "Prune&Comp estimates the magnitude gap from layer removal and compensates by rescaling remaining weights offline without runtime overhead, additionally employing an iterative pruning strategy.", "result": "Prune&Comp achieves nearly half the perplexity and retains 93.19% of the original model's question-answering performance when 5 layers of LLaMA-3-8B are pruned, outperforming the baseline by 4.01%.", "conclusion": "Prune&Comp offers a significant improvement in layer pruning for large language models by addressing the hidden state magnitude gap effectively, leading to better performance metrics.", "key_contributions": ["Introduction of the Prune&Comp method for layer pruning", "Demonstration of performance retention in LLaMA-3-8B", "Establishment of a new iterative pruning strategy"], "limitations": "", "keywords": ["layer pruning", "large language models", "magnitude compensation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.18263", "pdf": "https://arxiv.org/pdf/2507.18263.pdf", "abs": "https://arxiv.org/abs/2507.18263", "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models", "authors": ["Suhang Wu", "Jialong Tang", "Chengyi Yang", "Pei Zhang", "Baosong Yang", "Junhui Li", "Junfeng Yao", "Min Zhang", "Jinsong Su"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance.", "AI": {"tldr": "This paper introduces a Locate-and-Focus method for improving terminology translation in speech translation models by effectively locating terminologies in speech clips and minimizing irrelevant information.", "motivation": "The accurate translation of terminology in direct speech translation is a significant challenge, especially with interference from irrelevant noise.", "method": "The proposed Locate-and-Focus method locates speech clips containing terminologies to construct translation knowledge and associates it with the utterance from both audio and textual modalities.", "result": "The method effectively locates terminologies, enhances terminology translation success rates, and maintains general translation performance across various datasets.", "conclusion": "The proposed method demonstrates improved terminology translation accuracy without compromising overall translation effectiveness.", "key_contributions": ["Introduction of the Locate-and-Focus method for speech translation.", "Effective locating of terminologies within utterances.", "Improved success rate of terminology translation in diverse datasets."], "limitations": "", "keywords": ["speech translation", "terminology translation", "Locate-and-Focus"], "importance_score": 6, "read_time_minutes": 7}}
{"id": "2507.18264", "pdf": "https://arxiv.org/pdf/2507.18264.pdf", "abs": "https://arxiv.org/abs/2507.18264", "title": "Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil", "authors": ["Nevidu Jayatilleke", "Nisansa de Silva"], "categories": ["cs.CL"], "comment": "10 pages, 4 figures, Accepted paper at Recent Advances in Natural\n  Language Processing (RANLP) 2025", "summary": "Solving the problem of Optical Character Recognition (OCR) on printed text\nfor Latin and its derivative scripts can now be considered settled due to the\nvolumes of research done on English and other High-Resourced Languages (HRL).\nHowever, for Low-Resourced Languages (LRL) that use unique scripts, it remains\nan open problem. This study presents a comparative analysis of the zero-shot\nperformance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The\nselected engines include both commercial and open-source systems, aiming to\nevaluate the strengths of each category. The Cloud Vision API, Surya, Document\nAI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR\nand EasyOCR were examined for only one language due to their limitations. The\nperformance of these systems was rigorously analysed using five measurement\ntechniques to assess accuracy at both the character and word levels. According\nto the findings, Surya delivered the best performance for Sinhala across all\nmetrics, with a WER of 2.61%. Conversely, Document AI excelled across all\nmetrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the\nabove analysis, we also introduce a novel synthetic Tamil OCR benchmarking\ndataset.", "AI": {"tldr": "This study analyzes the performance of six OCR engines on Low-Resourced Languages (Sinhala and Tamil), revealing key findings on their effectiveness and introducing a novel Tamil OCR benchmarking dataset.", "motivation": "The need for effective OCR solutions for Low-Resourced Languages (LRLs) like Sinhala and Tamil, where existing research and technology are limited compared to High-Resourced Languages.", "method": "The study conducts a comparative analysis of six OCR engines (Cloud Vision API, Surya, Document AI, Tesseract, Subasa OCR, EasyOCR) on their performance in processing Sinhala and Tamil text, utilizing five measurement techniques for evaluation.", "result": "Surya performed the best for Sinhala with a WER of 2.61%, while Document AI had the best results for Tamil with a CER of 0.78%.", "conclusion": "The findings highlight significant disparities in OCR engine performance for different Low-Resourced Languages, emphasizing the need for further development and evaluation in this area, alongside the introduction of a novel benchmarking dataset for Tamil OCR.", "key_contributions": ["A comparative analysis of multiple OCR engines on LRLs.", "Identification of the best-performing OCR solutions for Sinhala and Tamil.", "Introduction of a novel synthetic Tamil OCR benchmarking dataset."], "limitations": "The limitations of some OCR engines restricted their evaluation to only one language, which may affect generalizability.", "keywords": ["Optical Character Recognition", "Low-Resourced Languages", "Sinhala", "Tamil", "OCR benchmarking"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.18294", "pdf": "https://arxiv.org/pdf/2507.18294.pdf", "abs": "https://arxiv.org/abs/2507.18294", "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer", "authors": ["Pritika Ramu", "Apoorv Saxena", "Meghanath M Y", "Varsha Sankar", "Debraj Basu"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs.", "AI": {"tldr": "StyleAdaptedLM is a framework for adapting LLMs to specific stylistic characteristics without compromising instruction adherence.", "motivation": "The need for stylistic adaptation in enterprise communication poses challenges, especially when working with unstructured corpora.", "method": "StyleAdaptedLM employs Low-Rank Adaptation (LoRA) to train stylistic traits on a base model, which is then merged with an instruction-following model.", "result": "Experiments show improved stylistic consistency and human evaluations confirm successful uptake of brand-specific conventions without paired data.", "conclusion": "StyleAdaptedLM provides an efficient means for achieving stylistic personalization in language models while maintaining performance on tasks.", "key_contributions": ["Introduces StyleAdaptedLM for stylistic adaptation of LLMs.", "Utilizes LoRA for efficient training without paired stylistic data.", "Demonstrates robust stylistic customization with preserved instruction adherence."], "limitations": "", "keywords": ["LLMs", "stylistic adaptation", "instruction-following models", "LoRA", "enterprise communication"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18305", "pdf": "https://arxiv.org/pdf/2507.18305.pdf", "abs": "https://arxiv.org/abs/2507.18305", "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit", "authors": ["Biao Yi", "Zekun Fei", "Jianing Geng", "Tong Li", "Lihai Nie", "Zheli Liu", "Yiming Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner.", "AI": {"tldr": "The paper introduces a novel attack on large reasoning models (LRMs) called 'overthinking backdoors', which allows an attacker to control the verbosity of reasoning outputs while maintaining answer correctness.", "motivation": "With the rise of large reasoning models (LRMs) in AI, there is a need to understand potential vulnerabilities that can be exploited to manipulate their reasoning capabilities.", "method": "The authors propose a tunable backdoor attack achieved through data poisoning, where the number of repetitions of a trigger correlates to the increase in reasoning verbosity.", "result": "Extensive empirical results show that the proposed attack can cause a multi-fold increase in reasoning process length without degrading the correctness of the final answer.", "conclusion": "The proposed method introduces a stealthy attack vector that can consume resources without compromising output accuracy, highlighting vulnerabilities in LRMs.", "key_contributions": ["Introduction of 'overthinking backdoors' as a new attack vector for LRMs", "Development of a tunable backdoor approach using data poisoning", "Demonstration of the effectiveness of the attack on various LRMs without loss of output correctness"], "limitations": "", "keywords": ["large reasoning models", "data poisoning", "backdoor attacks", "chain-of-thought", "resource consumption"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18523", "pdf": "https://arxiv.org/pdf/2507.18523.pdf", "abs": "https://arxiv.org/abs/2507.18523", "title": "The Moral Gap of Large Language Models", "authors": ["Maciej Skorski", "Alina Landowska"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.", "AI": {"tldr": "This study investigates the effectiveness of large language models (LLMs) in moral foundation detection versus fine-tuned transformers, revealing significant performance gaps and indicating that fine-tuning is more reliable for this specific task.", "motivation": "To analyze the ability of AI systems, particularly LLMs, in moral reasoning and to improve their alignment with ethical considerations in social discourse.", "method": "A comparative analysis of state-of-the-art LLMs and fine-tuned transformers on Twitter and Reddit datasets using ROC, PR, and DET curve analysis.", "result": "The study found that LLMs had high false negative rates and systematically failed to detect moral content, showing the limitations of their prompting capabilities.", "conclusion": "Task-specific fine-tuning is more effective than prompting for moral reasoning tasks in AI applications.", "key_contributions": ["First comprehensive comparison of LLMs with fine-tuned transformers for moral foundation detection.", "Identification of significant performance gaps in LLMs regarding moral reasoning.", "Demonstration of the advantages of fine-tuning over prompt engineering in LLMs."], "limitations": "", "keywords": ["moral foundation detection", "large language models", "fine-tuned transformers", "ethical AI", "moral reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18338", "pdf": "https://arxiv.org/pdf/2507.18338.pdf", "abs": "https://arxiv.org/abs/2507.18338", "title": "Uncertainty Quantification for Evaluating Machine Translation Bias", "authors": ["Ieva Raminta Staliūnaitė", "Julius Cheng", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "In machine translation (MT), when the source sentence includes a lexeme whose\ngender is not overtly marked, but whose target-language equivalent requires\ngender specification, the model must infer the appropriate gender from the\ncontext and/or external knowledge. Studies have shown that MT models exhibit\nbiased behaviour, relying on stereotypes even when they clash with contextual\ninformation. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain\nuncertainty about the gender when it is ambiguous. Using recently proposed\nmetrics of semantic uncertainty, we find that models with high translation and\ngender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has\nindependent effects on ambiguous and unambiguous translation instances.", "AI": {"tldr": "The paper explores biases in machine translation models regarding gender specification, suggesting models should recognize ambiguity in gender contexts while maintaining uncertainty when necessary.", "motivation": "To address the bias present in machine translation models that rely on stereotypes instead of contextual information for gender specification.", "method": "The study employs newly proposed metrics of semantic uncertainty to evaluate the performance of machine translation models in translating gender-ambiguous and gender-specific instances.", "result": "Findings reveal that high translation and gender accuracy does not correlate with expected uncertainty levels in ambiguous gender cases; debiasing affects these instances differently.", "conclusion": "The research indicates that machine translation models should not only prioritize accurate gender translation but also appropriately express uncertainty in ambiguous contexts to reduce biases.", "key_contributions": ["Introduces the concept of maintaining uncertainty in gender-ambiguous translations", "Demonstrates the disparity between model accuracy and uncertainty levels", "Evaluates the independent effects of debiasing in ambiguous vs unambiguous cases"], "limitations": "The study mainly focuses on gender biases, limiting its applicability to other domains of bias in machine translation.", "keywords": ["machine translation", "gender bias", "semantic uncertainty", "debiasing", "contextual information"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2411.02650", "pdf": "https://arxiv.org/pdf/2411.02650.pdf", "abs": "https://arxiv.org/abs/2411.02650", "title": "A Scoping Review of Functional Near-Infrared Spectroscopy (fNIRS) Applications in Game-Based Learning Environments", "authors": ["Shayla Sharmin", "Gael Lucero-Palacios", "Behdokht Kiafar", "Mohammad Fahim Abrar", "Mohammad Al-Ratrout", "Aditya Raikwar", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": "28 pages, 3 figures", "summary": "Functional Near-Infrared Spectroscopy (fNIRS) has emerged as a valuable tool\nto investigate cognitive and emotional processes during learning. We focus\nspecifically on game-integrated learning systems as the context for fNIRS-based\nbrain data analysis. We selected game-integrated learning systems because such\nsystems make learning more engaging, interactive, and immersive, all of which\nare critical features for adaptive learning design. The goal of this scoping\nreview is to help researchers understand how fNIRS has been used so far to\nstudy brain activity in game-integrated learning systems. We also aim to show\nhow brain data captured through fNIRS can support the development of adaptive\nlearning systems by monitoring learners' cognitive states. Using the PRISMA-ScR\nframework, 1300 papers were screened, and 21 empirical studies were selected\nfor in-depth analysis. Studies were categorized as affective/cognitive response\nstudies or comparative studies, and further analyzed by learning platform, game\ndevice, fNIRS configuration, outcome measures, and study design. The findings\nreveal that game-integrated learning systems can be as effective as traditional\nmethods in improving engagement and involvement. The findings also show that\nfNIRS offers valuable insights into cognitive states, but it has not yet been\nwidely implemented in real-time adaptive systems. We identify key challenges in\nstandardization and data interpretation and highlight the potential of fNIRS\nfor developing brain-aware, interactive learning environments. This review\noffers insights to guide future research on using brain data to support\nadaptive learning and intelligent system design.", "AI": {"tldr": "This paper is a scoping review of the use of functional Near-Infrared Spectroscopy (fNIRS) in game-integrated learning systems, focusing on how fNIRS can enhance understanding of cognitive states to aid in adaptive learning.", "motivation": "To investigate the use of fNIRS in understanding cognitive and emotional processes during learning in game-integrated learning environments, which enhance engagement and interactivity.", "method": "A scoping review using the PRISMA-ScR framework, screening 1300 papers and analyzing 21 empirical studies categorized by study type, learning platform, game device, fNIRS configuration, and outcome measures.", "result": "The review indicates that game-integrated learning systems can improve engagement comparably to traditional methods, and fNIRS provides valuable insights into cognitive states, though it is not yet fully integrated into real-time adaptive systems.", "conclusion": "Standardization and data interpretation challenges remain, but fNIRS presents potential for brain-aware, interactive learning systems, providing insights for future adaptive learning research.", "key_contributions": ["Review of current fNIRS applications in game-integrated learning systems", "Identification of gaps in real-time implementation of adaptive learning", "Insights into cognitive states for improving learning system design"], "limitations": "Limited implementation in real-time adaptive systems and challenges in standardization and data interpretation.", "keywords": ["fNIRS", "game-integrated learning systems", "adaptive learning", "cognitive states", "learning technology"], "importance_score": 6, "read_time_minutes": 28}}
{"id": "2507.18340", "pdf": "https://arxiv.org/pdf/2507.18340.pdf", "abs": "https://arxiv.org/abs/2507.18340", "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning", "authors": ["Yifu Chen", "Bingchen Huang", "Zhiling Wang", "Yuanchao Du", "Junfeng Luo", "Lei Shen", "Zhineng chen"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR.", "AI": {"tldr": "The paper presents TDR, a framework that enhances example retrieval for in-context learning (ICL) in language models by decoupling task examples and incorporating LLM feedback to improve performance.", "motivation": "To address challenges in retrieving high-quality examples for in-context learning from multi-task datasets, specifically by distinguishing cross-task distributions and connecting retriever outputs to LLM feedback.", "method": "The TDR framework separates ICL examples by task and utilizes feedback from LLMs to train the retrieval module, enhancing the quality of examples retrieved for specific tasks.", "result": "Experiments on 30 NLP tasks showed that TDR consistently improved results across all datasets and achieved state-of-the-art performance.", "conclusion": "TDR is a plug-and-play method that can be integrated with various LLMs to enhance their example retrieval capabilities for ICL.", "key_contributions": ["Decoupling ICL examples across different tasks to improve retrieval accuracy.", "Incorporating fine-grained feedback from LLMs for better supervised training of the retrieval module.", "Demonstrating state-of-the-art performance on a wide array of NLP tasks."], "limitations": "", "keywords": ["in-context learning", "example retrieval", "language models", "NLP tasks", "fine-grained feedback"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.14209", "pdf": "https://arxiv.org/pdf/2412.14209.pdf", "abs": "https://arxiv.org/abs/2412.14209", "title": "Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction", "authors": ["Peter E. D. Love", "Jane Matthews", "Weili Fang", "Hadi Mahamivanan"], "categories": ["cs.HC", "cs.AI"], "comment": "74 pages, 5 figures and 3 tables", "summary": "Explainable Artificial Intelligence seeks to make the reasoning processes of\nAI models transparent and interpretable, particularly in complex decision\nmaking environments. In the construction industry, where AI based decision\nsupport systems are increasingly adopted, limited attention has been paid to\nthe integration of supporting evidence that underpins the reliability and\naccountability of AI generated outputs. The absence of such evidence undermines\nthe validity of explanations and the trustworthiness of system recommendations.\nThis paper addresses this gap by introducing a theoretical, evidence based\nmeans end framework developed through a narrative review. The framework offers\nan epistemic foundation for designing XAI enabled DSS that generate meaningful\nexplanations tailored to users knowledge needs and decision contexts. It\nfocuses on evaluating the strength, relevance, and utility of different types\nof evidence supporting AI generated explanations. While developed with\nconstruction professionals as primary end users, the framework is also\napplicable to developers, regulators, and project managers with varying\nepistemic goals.", "AI": {"tldr": "This paper presents a framework for Explainable AI in the construction industry, focusing on evidence-based decision support systems (DSS) to enhance AI outputs' reliability and accountability.", "motivation": "The need for transparency and interpretability in AI decision support systems, particularly in the construction industry, to improve trustworthiness of AI-generated recommendations.", "method": "A theoretical, evidence-based means-end framework developed through a narrative review.", "result": "The framework evaluates the strength, relevance, and utility of evidence supporting AI explanations, designed to cater to the knowledge needs of users in decision-making contexts.", "conclusion": "The framework aids in creating meaningful explanations for AI outputs tailored to specific end-user requirements while being adaptable to various stakeholders in the construction sector.", "key_contributions": ["Introduction of an evidence-based framework for Explainable AI in the construction industry.", "Focus on user knowledge needs and decision contexts in AI explanations.", "Applicability to various stakeholders in the construction sector including professionals and project managers."], "limitations": "", "keywords": ["Explainable AI", "decision support systems", "construction industry", "evidence-based framework", "transparency"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.18343", "pdf": "https://arxiv.org/pdf/2507.18343.pdf", "abs": "https://arxiv.org/abs/2507.18343", "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence", "authors": ["Ariana Sahitaj", "Premtim Sahitaj", "Veronika Solopova", "Jiaao Li", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "NLP4PI at ACL", "summary": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository.", "AI": {"tldr": "This paper presents a novel framework for propaganda detection on social media that integrates human expertise with LLM assistance, focusing on improved annotation consistency and scalability.", "motivation": "To address the challenges in propaganda detection on social media, specifically the complexity of the task and the scarcity of high-quality labeled data.", "method": "The study introduces a hierarchical taxonomy for 14 propaganda techniques, conducts a human annotation study on the HQP dataset, and implements an LLM-assisted pre-annotation pipeline for efficient annotation processes. It also includes verification studies for agreement and efficiency improvements, and fine-tunes smaller language models using LLM-generated data.", "result": "The LLM-assisted approach resulted in significant improvements in inter-annotator agreement and annotation speed, demonstrating the feasibility of using LLMs for enhancing the quality of propaganda detection.", "conclusion": "The research showcases a scalable method for propaganda detection that could contribute to creating transparent media ecosystems, promoting accountability and adherence to Sustainable Development Goals (SDG) 16.", "key_contributions": ["Introduction of a hierarchical taxonomy for fine-grained propaganda techniques", "Development of an LLM-assisted annotation pipeline", "Demonstration of scalability through knowledge distillation from LLMs to smaller models."], "limitations": "Further exploration is needed to validate the framework across diverse social media platforms and languages.", "keywords": ["propaganda detection", "large language models", "human-computer interaction", "annotation consistency", "knowledge distillation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.15669", "pdf": "https://arxiv.org/pdf/2412.15669.pdf", "abs": "https://arxiv.org/abs/2412.15669", "title": "WigglyEyes: Inferring Eye Movements from Keypress Data", "authors": ["Yujun Zhu", "Danqing Shi", "Hee-Seung Moon", "Antti Oulasvirta"], "categories": ["cs.HC"], "comment": null, "summary": "We present a model for inferring where users look during interaction based on\nkeypress data only. Given a key log, it outputs a scanpath that tells,\nmoment-by-moment, how the user had moved eyes while entering those keys. The\nmodel can be used as a proxy for human data in cases where collecting real eye\ntracking data is expensive or impossible. Our technical insight is an inference\narchitecture that considers the individual characteristics of the user,\ninferred as a low-dimensional parameter vector. We present a novel loss\nfunction for synchronizing inferred eye movements with the keypresses.\nEvaluations on touchscreen typing demonstrate accurate gaze inference.", "AI": {"tldr": "A model infers user gaze during typing based on keypress data, offering an alternative to direct eye tracking.", "motivation": "To provide a method for estimating user gaze without the costs or challenges of traditional eye tracking.", "method": "The model uses keypress data to output a scanpath, integrating individual user characteristics through a low-dimensional parameter vector and a new loss function for synchronization.", "result": "Evaluations show accurate gaze inference during touchscreen typing sessions.", "conclusion": "This model effectively estimates gaze movements, suggesting wide applications in HCI where direct data collection is difficult.", "key_contributions": ["Inference of gaze from keypress data", "Use of a low-dimensional parameter vector for individuals", "Novel loss function for gaze and keypress synchronization"], "limitations": "The model relies solely on keypress data, which may not capture all nuances of user behavior.", "keywords": ["eye tracking", "user gaze inference", "keypress data", "HCI", "touchscreen typing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.18392", "pdf": "https://arxiv.org/pdf/2507.18392.pdf", "abs": "https://arxiv.org/abs/2507.18392", "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy", "authors": ["Asaf Yehudai", "Lilach Eden", "Yotam Perlitz", "Roy Bar-Haim", "Michal Shmueli-Scheuer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.", "AI": {"tldr": "Introducing CLEAR, an interactive package for LLM-based error analysis that provides actionable feedback and visualizations.", "motivation": "To provide clearer insights into model evaluation by addressing the limitations of current LLM evaluation paradigms that yield single scores without explaining model performance.", "method": "CLEAR generates per-instance feedback, creates system-level error issues, and quantifies their prevalence. It features an interactive dashboard for comprehensive error analysis.", "result": "Demonstrated CLEAR analysis effectiveness through RAG and Math benchmarks, providing valuable insights into model behavior and specific error types.", "conclusion": "CLEAR offers a solution for in-depth evaluation of LLMs, enabling users to understand model performance beyond singular scores.", "key_contributions": ["Interactive, open-source error analysis package for LLMs", "Generates actionable feedback and system-level insights", "User-friendly dashboard with visualizations and filtering options"], "limitations": "", "keywords": ["Large Language Models", "error analysis", "interactive dashboard", "benchmarking", "feedback"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.10029", "pdf": "https://arxiv.org/pdf/2503.10029.pdf", "abs": "https://arxiv.org/abs/2503.10029", "title": "HandProxy: Expanding the Affordances of Speech Interfaces in Immersive Environments with a Virtual Proxy Hand", "authors": ["Chen Liang", "Yuxuan Liu", "Martez Mott", "Anhong Guo"], "categories": ["cs.HC"], "comment": "Accepted in ACM IMWUT 2025", "summary": "Hand interactions are increasingly used as the primary input modality in\nimmersive environments, but they are not always feasible due to situational\nimpairments, motor limitations, and environmental constraints. Speech\ninterfaces have been explored as an alternative to hand input in research and\ncommercial solutions, but are limited to initiating basic hand gestures and\nsystem controls. We introduce HandProxy, a system that expands the affordances\nof speech interfaces to support expressive hand interactions. Instead of\nrelying on predefined speech commands directly mapped to possible interactions,\nHandProxy enables users to control the movement of a virtual hand as an\ninteraction proxy, allowing them to describe the intended interactions\nnaturally while the system translates speech into a sequence of hand controls\nfor real-time execution. A user study with 20 participants demonstrated that\nHandProxy effectively enabled diverse hand interactions in virtual\nenvironments, achieving a 100% task completion rate with an average of 1.09\nattempts per speech command and 91.8% command execution accuracy, while\nsupporting flexible, natural speech input with varying levels of control and\ngranularity.", "AI": {"tldr": "HandProxy enhances speech interfaces to enable expressive hand interactions in virtual environments, achieving high task completion and command execution accuracy.", "motivation": "To address limitations of hand interactions in immersive environments due to impairments and environmental constraints, exploring speech interfaces as an alternative.", "method": "Developing HandProxy, a system that translates natural speech into a sequence of hand controls for real-time execution in virtual environments.", "result": "A user study with 20 participants showed HandProxy enabled diverse hand interactions, achieving a 100% task completion rate and 91.8% command execution accuracy.", "conclusion": "HandProxy allows natural speech to control hand movements, enhancing interaction flexibility in virtual settings.", "key_contributions": ["Introduction of HandProxy for expressive hand interactions via speech", "Demonstrated high efficiency in task completion and command accuracy", "Support for flexible and natural speech input"], "limitations": "", "keywords": ["Hand Interaction", "Speech Interfaces", "Virtual Environments", "Human-Computer Interaction", "User Study"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.18406", "pdf": "https://arxiv.org/pdf/2507.18406.pdf", "abs": "https://arxiv.org/abs/2507.18406", "title": "Factual Inconsistencies in Multilingual Wikipedia Tables", "authors": ["Silvia Cappa", "Lingxiao Kong", "Pille-Riin Peet", "Fanfu Wei", "Yuchen Zhou", "Jan-Christoph Kalo"], "categories": ["cs.CL", "cs.DB", "cs.DL", "cs.IR"], "comment": "11 pages, 7 figures, White Paper for RTF Work at ISWS Summer School\n  2025", "summary": "Wikipedia serves as a globally accessible knowledge source with content in\nover 300 languages. Despite covering the same topics, the different versions of\nWikipedia are written and updated independently. This leads to factual\ninconsistencies that can impact the neutrality and reliability of the\nencyclopedia and AI systems, which often rely on Wikipedia as a main training\nsource. This study investigates cross-lingual inconsistencies in Wikipedia's\nstructured content, with a focus on tabular data. We developed a methodology to\ncollect, align, and analyze tables from Wikipedia multilingual articles,\ndefining categories of inconsistency. We apply various quantitative and\nqualitative metrics to assess multilingual alignment using a sample dataset.\nThese insights have implications for factual verification, multilingual\nknowledge interaction, and design for reliable AI systems leveraging Wikipedia\ncontent.", "AI": {"tldr": "This study analyzes cross-lingual inconsistencies in Wikipedia's structured content, focusing on tabular data, and proposes a methodology for their assessment and alignment.", "motivation": "The study addresses factual inconsistencies in Wikipedia's various language versions, which can affect its neutrality and reliability, especially for AI systems that use Wikipedia as a training source.", "method": "The authors developed a methodology to collect, align, and analyze tables from multilingual Wikipedia articles, defining categories of inconsistency and applying quantitative and qualitative metrics to assess multilingual alignment.", "result": "The investigation highlights significant cross-lingual inconsistencies in Wikipedia's structured content, emphasizing the need for improved methodologies in factual verification and AI system design.", "conclusion": "The findings underscore the implications for multilingual knowledge interaction and the importance of addressing inconsistencies to enhance the reliability of AI systems that depend on Wikipedia.", "key_contributions": ["Development of a novel methodology for aligning tables in multilingual Wikipedia articles.", "Identification of categories of inconsistency in structured content across languages.", "Assessment metrics for evaluating multilingual alignment of Wikipedia content."], "limitations": "The study is limited to tabular data and may not generalize to all types of content in Wikipedia.", "keywords": ["cross-lingual inconsistencies", "Wikipedia", "structured content", "tabular data", "multilingual alignment"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.02250", "pdf": "https://arxiv.org/pdf/2504.02250.pdf", "abs": "https://arxiv.org/abs/2504.02250", "title": "Designing Effective Human-Swarm Interaction Interfaces: Insights from a User Study on Task Performance", "authors": ["Wasura D. Wattearachchi", "Erandi Lakshika", "Kathryn Kasmarik", "Michael Barlow"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 5 tables", "summary": "In this paper, we present a systematic method of design for human-swarm\ninteraction interfaces, combining theoretical insights with empirical\nevaluation. We first derived ten design principles from existing literature,\napplying them to key information dimensions identified through goal-directed\ntask analysis and developed a tablet-based interface for a target search task.\nWe then conducted a user study with 31 participants where humans were required\nto guide a robotic swarm to a target in the presence of three types of hazards\nthat pose a risk to the robots: Distributed, Moving, and Spreading. Performance\nwas measured based on the proximity of the robots to the target and the number\nof deactivated robots at the end of the task. Results indicate that at least\none robot was brought closer to the target in 98% of tasks, demonstrating the\ninterface's success in fulfilling the primary objective of the task.\nAdditionally, in nearly 67% of tasks, more than 50% of the robots reached the\ntarget. Moreover, particularly better performance was noted in moving hazards.\nAdditionally, the interface appeared to help minimise robot deactivation, as\nevidenced by nearly 94% of tasks where participants managed to keep more than\n50% of the robots active, ensuring that most of the swarm remained operational.\nHowever, its effectiveness varied across hazards, with robot deactivation being\nlowest in distributed hazard scenarios, suggesting that the interface provided\nthe most support in these conditions.", "AI": {"tldr": "This paper presents a systematic method for designing human-swarm interaction interfaces through empirical evaluation, demonstrating its effectiveness in guiding robotic swarms while minimizing robot deactivation.", "motivation": "To improve human-swarm interaction interfaces by deriving design principles and empirically evaluating their effectiveness in guiding robotic swarms in risky environments.", "method": "The researchers applied ten design principles from literature to develop a tablet-based interface and conducted a user study with 31 participants guiding a robotic swarm in the presence of various hazards.", "result": "At least one robot was brought closer to the target in 98% of tasks, with more than 50% of the robots reaching the target in 67% of cases, and 94% of tasks maintained over 50% of the robots active.", "conclusion": "The interface significantly enhances performance in target search tasks under various hazardous conditions, particularly in scenarios with moving hazards.", "key_contributions": ["Development of a systematic design method for human-swarm interfaces.", "Conducting empirical evaluations that demonstrate the interface's effectiveness.", "Identifying performance variations across different types of hazards."], "limitations": "The effectiveness of the interface varied depending on the type of hazard, indicating potential areas for further optimization.", "keywords": ["human-swarm interaction", "interface design", "robotics", "user study", "hazard analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18417", "pdf": "https://arxiv.org/pdf/2507.18417.pdf", "abs": "https://arxiv.org/abs/2507.18417", "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Danilo Mandic"], "categories": ["cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "comment": null, "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).", "AI": {"tldr": "FinDPO is a finance-specific LLM framework that enhances sentiment analysis for trading decisions by addressing the limitations of supervised fine-tuned models.", "motivation": "The increasing impact of online finance-related opinions on trading decisions necessitates effective sentiment analysis to quantify these opinions.", "method": "FinDPO utilizes post-training human preference alignment via Direct Preference Optimization to enhance LLM performance specifically in financial sentiment analysis.", "result": "FinDPO outperforms existing SFT models by 11% on sentiment classification benchmarks and achieves a 67% annual return in simulations even under transaction costs.", "conclusion": "FinDPO proves effective in adapting sentiment analysis to the financial domain, enabling practical application in portfolio strategies and demonstrating strong financial performance metrics.", "key_contributions": ["Introduction of FinDPO as a finance-specific LLM framework", "Improvement over existing models with an 11% performance increase", "Ability to integrate sentiment analysis into portfolio strategies with significant returns."], "limitations": "", "keywords": ["sentiment analysis", "finance", "large language models", "Direct Preference Optimization", "portfolio strategies"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2504.03300", "pdf": "https://arxiv.org/pdf/2504.03300.pdf", "abs": "https://arxiv.org/abs/2504.03300", "title": "On the Complexities of Testing for Compliance with Human Oversight Requirements in AI Regulation", "authors": ["Markus Langer", "Veronika Lazar", "Kevin Baum"], "categories": ["cs.HC", "cs.CY", "K.4.1; K.5.2; J.4"], "comment": null, "summary": "Human oversight requirements are a core component of the European AI Act and\nin AI governance. In this paper, we highlight key challenges in testing for\ncompliance with these requirements. A central difficulty lies in balancing\nsimple, but potentially ineffective checklist-based approaches with\nresource-intensive and context-sensitive empirical testing of the effectiveness\nof human oversight of AI. Questions regarding when to update compliance\ntesting, the context-dependent nature of human oversight requirements, and\ndifficult-to-operationalize standards further complicate compliance testing. We\nargue that these challenges illustrate broader challenges in the future of\nsociotechnical AI governance, i.e. a future that shifts from ensuring good\ntechnological products to good sociotechnical systems.", "AI": {"tldr": "The paper discusses challenges in ensuring compliance with human oversight requirements in AI governance, emphasizing the balance between checklist methods and empirical testing.", "motivation": "To address key challenges in testing compliance with human oversight requirements set by the European AI Act.", "method": "The paper analyzes the effectiveness of current compliance testing methods, highlighting the difficulties in balancing simple checklist approaches with more nuanced empirical testing.", "result": "It reveals that many challenges in compliance testing are due to the contextual nature of human oversight, the need for continuous updates, and the complexity of operationalizing standards.", "conclusion": "The findings indicate a need for a shift towards developing sociotechnical systems rather than focusing solely on technological products in AI governance.", "key_contributions": ["Identification of compliance testing challenges within AI governance", "Discussion of the balance between checklist methods and empirical testing", "Argument for a shift towards sociotechnical system governance."], "limitations": "", "keywords": ["AI governance", "human oversight", "compliance testing", "European AI Act", "sociotechnical systems"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.18442", "pdf": "https://arxiv.org/pdf/2507.18442.pdf", "abs": "https://arxiv.org/abs/2507.18442", "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data", "authors": ["Rana Alshaikh", "Israa Alghanmi", "Shelan Jeawak"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.", "AI": {"tldr": "AraTable is a benchmark for evaluating LLMs' understanding of Arabic tabular data, highlighting significant reasoning challenges and providing a novel evaluation framework.", "motivation": "To address the performance gap of large language models (LLMs) in interpreting Arabic tabular data due to limited public resources and unique language features.", "method": "A hybrid pipeline where task content is generated by LLMs and then filtered and verified by human experts, including tasks for direct question answering, fact verification, and complex reasoning.", "result": "Initial analyses reveal LLMs perform adequately on simpler tasks but struggle with deeper reasoning; a fully automated evaluation framework shows comparable results to human judges.", "conclusion": "AraTable offers a valuable benchmark and framework to enhance LLM performance on Arabic structured data, paving the way for future studies.", "key_contributions": ["Introduction of AraTable as a benchmark for Arabic tabular data", "Development of a new fully automated evaluation framework", "Insights into LLM performance limitations and opportunities for improvement in complex reasoning tasks"], "limitations": "The study is focused solely on Arabic tabular data and may not generalize to other languages or data types.", "keywords": ["Large Language Models", "Arabic Tabular Data", "Benchmarking", "Natural Language Processing", "Cognitive Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18448", "pdf": "https://arxiv.org/pdf/2507.18448.pdf", "abs": "https://arxiv.org/abs/2507.18448", "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language", "authors": ["Md Obyedullahil Mamun", "Md Adyelullahil Mamun", "Arif Ahmad", "Md. Imran Hossain Emu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.7"], "comment": null, "summary": "Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP.", "AI": {"tldr": "This study investigates using transformer models for punctuation restoration in unpunctuated Bangla text, achieving high accuracy and providing resources for future research.", "motivation": "The research aims to enhance the readability of Bangla text, especially for Automatic Speech Recognition applications in low-resource contexts.", "method": "The study uses XLM-RoBERTa-large, trained on a large corpus with data augmentation techniques to predict punctuation marks in Bangla text.", "result": "The best model achieved 97.1% accuracy on the News test set and strong generalization to noisy ASR transcripts.", "conclusion": "This work establishes a robust baseline for punctuation restoration in Bangla and contributes datasets and code for future low-resource NLP research.", "key_contributions": ["Application of a transformer model for punctuation restoration in Bangla", "Creation of a diverse training corpus for low-resource languages", "Public availability of datasets and code for future research"], "limitations": "", "keywords": ["Punctuation Restoration", "XLM-RoBERTa", "Bangla", "Automatic Speech Recognition", "Low-resource NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.18451", "pdf": "https://arxiv.org/pdf/2507.18451.pdf", "abs": "https://arxiv.org/abs/2507.18451", "title": "Generation of Synthetic Clinical Text: A Systematic Review", "authors": ["Basel Alshaikhdeeb", "Ahmed Abdelmonem Hemedan", "Soumyabrata Ghosh", "Irina Balaur", "Venkata Satagopam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generating clinical synthetic text represents an effective solution for\ncommon clinical NLP issues like sparsity and privacy. This paper aims to\nconduct a systematic review on generating synthetic medical free-text by\nformulating quantitative analysis to three research questions concerning (i)\nthe purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,\nGoogle Scholar, and arXiv databases for publications associated with generating\nsynthetic medical unstructured free-text. We have identified 94 relevant\narticles out of 1,398 collected ones. A great deal of attention has been given\nto the generation of synthetic medical text from 2018 onwards, where the main\npurpose of such a generation is towards text augmentation, assistive writing,\ncorpus building, privacy-preserving, annotation, and usefulness. Transformer\narchitectures were the main predominant technique used to generate the text,\nespecially the GPTs. On the other hand, there were four main aspects of\nevaluation, including similarity, privacy, structure, and utility, where\nutility was the most frequent method used to assess the generated synthetic\nmedical text. Although the generated synthetic medical text demonstrated a\nmoderate possibility to act as real medical documents in different downstream\nNLP tasks, it has proven to be a great asset as augmented, complementary to the\nreal documents, towards improving the accuracy and overcoming\nsparsity/undersampling issues. Yet, privacy is still a major issue behind\ngenerating synthetic medical text, where more human assessments are needed to\ncheck for the existence of any sensitive information. Despite that, advances in\ngenerating synthetic medical text will considerably accelerate the adoption of\nworkflows and pipeline development, discarding the time-consuming legalities of\ndata transfer.", "AI": {"tldr": "This paper presents a systematic review of techniques and evaluation methods for generating synthetic medical text, emphasizing the use of transformer architectures like GPTs and the need for privacy considerations.", "motivation": "Address common clinical NLP issues like text sparsity and privacy concerns by generating synthetic medical free-text.", "method": "Conducted a systematic review by searching major databases for relevant articles and analyzing them based on purpose, techniques, and evaluation methods.", "result": "Identified 94 relevant articles, with a focus on text augmentation, assistive writing, and privacy-preserving methods, primarily using transformer architectures.", "conclusion": "Synthetic medical texts can effectively assist in healthcare NLP tasks, but privacy issues need more attention; further advances can enhance workflow efficiency.", "key_contributions": ["Systematic review of synthetic medical text generation techniques", "Analysis of evaluation methods for synthetic medical texts", "Identification of key purposes for generating synthetic medical text"], "limitations": "More human assessments are needed to ensure the privacy of generated texts; some concerns regarding the use of synthetic text in real applications remain.", "keywords": ["Synthetic medical text", "Natural Language Processing", "Transformer architectures"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18504", "pdf": "https://arxiv.org/pdf/2507.18504.pdf", "abs": "https://arxiv.org/abs/2507.18504", "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models", "authors": ["Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.", "AI": {"tldr": "GraDe integrates sparse dependency graphs into LLMs' attention to improve tabular data generation by prioritizing critical feature interactions while reducing focus on irrelevant ones.", "motivation": "LLMs struggle with tabular data due to the sparse feature-level dependencies that dilute attention on important feature interactions.", "method": "GraDe uses a dynamic graph learning module to integrate functional dependencies into the attention mechanism of LLMs, enhancing structure-aware modeling of tabular data.", "result": "GraDe outperforms existing LLM-based methods by up to 12% on complex datasets and is competitive with state-of-the-art methods in synthetic data quality.", "conclusion": "GraDe offers an effective and practical method for improving tabular data generation with LLMs by leveraging dependency graphs.", "key_contributions": ["Introduction of GraDe for integrating sparse dependency graphs into LLMs", "Improved performance on complex datasets", "Minimal intrusion into the existing LLM architecture"], "limitations": "", "keywords": ["Large Language Models", "tabular data", "dependency graphs", "attention mechanism", "data generation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.18523", "pdf": "https://arxiv.org/pdf/2507.18523.pdf", "abs": "https://arxiv.org/abs/2507.18523", "title": "The Moral Gap of Large Language Models", "authors": ["Maciej Skorski", "Alina Landowska"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.", "AI": {"tldr": "This study compares LLMs and fine-tuned transformers for moral foundation detection in social media discourse.", "motivation": "To analyze social discourse and develop ethically-aligned AI systems.", "method": "Comparison of state-of-the-art LLMs and fine-tuned transformers using ROC, PR, and DET curve analysis across Twitter and Reddit datasets.", "result": "LLMs show high false negative rates and under-detection of moral content despite prompt engineering.", "conclusion": "Task-specific fine-tuning is superior to prompting for moral reasoning applications in LLMs.", "key_contributions": ["First comprehensive comparison of LLMs and fine-tuned models for moral reasoning", "Identification of performance gaps in LLMs for moral content detection", "Demonstration of the superiority of fine-tuning over prompting in this domain."], "limitations": "Focuses primarily on Twitter and Reddit datasets, which may limit generalizability.", "keywords": ["moral foundation detection", "large language models", "fine-tuning", "ethical AI", "social discourse"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18542", "pdf": "https://arxiv.org/pdf/2507.18542.pdf", "abs": "https://arxiv.org/abs/2507.18542", "title": "Effective Multi-Task Learning for Biomedical Named Entity Recognition", "authors": ["João Ruano", "Gonçalo M. Correia", "Leonor Barreiros", "Afonso Mendes"], "categories": ["cs.CL"], "comment": "Accepted at the 24th BioNLP workshop (ACL2025), 15 pages, 3 figures", "summary": "Biomedical Named Entity Recognition presents significant challenges due to\nthe complexity of biomedical terminology and inconsistencies in annotation\nacross datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),\na novel approach designed to handle nested named entities while integrating\nmultiple datasets through an effective multi-task learning strategy. SRU-NER\nmitigates annotation gaps by dynamically adjusting loss computation to avoid\npenalizing predictions of entity types absent in a given dataset. Through\nextensive experiments, including a cross-corpus evaluation and human assessment\nof the model's predictions, SRU-NER achieves competitive performance in\nbiomedical and general-domain NER tasks, while improving cross-domain\ngeneralization.", "AI": {"tldr": "This paper introduces SRU-NER, a novel approach for Biomedical Named Entity Recognition that effectively handles nested entities and integrates multiple datasets using multi-task learning.", "motivation": "The complexity of biomedical terminology and inconsistencies in annotation across datasets pose significant challenges in Biomedical Named Entity Recognition.", "method": "SRU-NER uses a Slot-based Recurrent Unit approach to manage nested named entities and employs a multi-task learning strategy to better integrate multiple datasets, adjusting loss computation dynamically to minimize the impact of absent entity types.", "result": "SRU-NER demonstrates competitive performance in both biomedical and general-domain NER tasks, with improvements seen in cross-domain generalization through extensive experiments and human assessments.", "conclusion": "The proposed SRU-NER method effectively addresses challenges in biomedical NER, showing robustness and adaptability across different datasets and improving on existing methods.", "key_contributions": ["Introduction of a novel Slot-based Recurrent Unit for nested entity recognition.", "Implementation of multi-task learning to enhance dataset integration.", "Dynamic loss computation to mitigate annotation gaps across datasets."], "limitations": "Potential limitations not explicitly stated in the abstract.", "keywords": ["Biomedical Named Entity Recognition", "Named Entity Recognition", "Multi-task learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.13725", "pdf": "https://arxiv.org/pdf/2409.13725.pdf", "abs": "https://arxiv.org/abs/2409.13725", "title": "Identity-related Speech Suppression in Generative AI Content Moderation", "authors": ["Grace Proebsting", "Oghenefejiro Isaacs Anigboro", "Charlie M. Crawford", "Danaé Metaxa", "Sorelle A. Friedler"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "ACM Conference on Equity and Access in Algorithms, Mechanisms, and\n  Optimization, 2025", "summary": "Automated content moderation has long been used to help identify and filter\nundesired user-generated content online. But such systems have a history of\nincorrectly flagging content by and about marginalized identities for removal.\nGenerative AI systems now use such filters to keep undesired generated content\nfrom being created by or shown to users. While a lot of focus has been given to\nmaking sure such systems do not produce undesired outcomes, considerably less\nattention has been paid to making sure appropriate text can be generated. From\nclassrooms to Hollywood, as generative AI is increasingly used for creative or\nexpressive text generation, whose stories will these technologies allow to be\ntold, and whose will they suppress?\n  In this paper, we define and introduce measures of speech suppression,\nfocusing on speech related to different identity groups incorrectly filtered by\na range of content moderation APIs. Using both short-form, user-generated\ndatasets traditional in content moderation and longer generative AI-focused\ndata, including two datasets we introduce in this work, we create a benchmark\nfor measurement of speech suppression for nine identity groups. Across one\ntraditional and four generative AI-focused automated content moderation\nservices tested, we find that identity-related speech is more likely to be\nincorrectly suppressed than other speech. We find that reasons for incorrect\nflagging behavior vary by identity based on stereotypes and text associations,\nwith, e.g., disability-related content more likely to be flagged for self-harm\nor health-related reasons while non-Christian content is more likely to be\nflagged as violent or hateful. As generative AI systems are increasingly used\nfor creative work, we urge further attention to how this may impact the\ncreation of identity-related content.", "AI": {"tldr": "This paper investigates the suppression of identity-related speech by automated content moderation systems and the impact of generative AI on content creation.", "motivation": "The motivation is to address the issue of automated content moderation incorrectly filtering content related to marginalized identities and to ensure that generative AI allows appropriate text generation.", "method": "The paper introduces measures of speech suppression, creating a benchmark for measuring suppression across different identity groups using both user-generated datasets and generative AI-focused data.", "result": "The study finds that identity-related speech is more likely to be incorrectly suppressed across various content moderation services, with different reasons for suppression linked to identity stereotypes and text associations.", "conclusion": "As generative AI becomes more prevalent in creative contexts, it is crucial to consider the implications of speech suppression on the creation of identity-related content.", "key_contributions": ["Introduction of measures of speech suppression for identity groups", "Creation of benchmark datasets for measuring speech suppression", "Analysis of identity-related bias in automated content moderation systems."], "limitations": "", "keywords": ["content moderation", "generative AI", "speech suppression", "identity-related speech", "automated systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18546", "pdf": "https://arxiv.org/pdf/2507.18546.pdf", "abs": "https://arxiv.org/abs/2507.18546", "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface", "authors": ["Urchade Zaratiana", "Gil Pasternak", "Oliver Boyd", "George Hurn-Maloney", "Ash Lewis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.", "AI": {"tldr": "GLiNER2 is a unified framework for information extraction that offers efficient named entity recognition, text classification, and data extraction in a single model. It improves deployment efficiency compared to existing large language models.", "motivation": "To address the limitations of requiring specialized models and high computational costs in existing information extraction solutions.", "method": "GLiNER2 enhances the original GLiNER architecture to support multiple tasks within a single pretrained transformer model, utilizing a schema-based interface for multi-task composition.", "result": "GLiNER2 shows competitive performance in extraction and classification tasks while being more accessible for deployment than LLM-based approaches.", "conclusion": "GLiNER2 is released as an open-source library, providing an efficient alternative for information extraction tasks.", "key_contributions": ["Unified framework supporting multiple NLP tasks", "Improved CPU efficiency and model compactness", "Open-source availability with pre-trained models"], "limitations": "", "keywords": ["Information Extraction", "Natural Language Processing", "Transformers"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18562", "pdf": "https://arxiv.org/pdf/2507.18562.pdf", "abs": "https://arxiv.org/abs/2507.18562", "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation", "authors": ["Jiafeng Xiong", "Yuting Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.", "AI": {"tldr": "The paper introduces GIIFT, an innovative two-stage framework for Multimodal Machine Translation (MMT) that leverages graphs for inductive image-free translation, demonstrating state-of-the-art results on various benchmarks.", "motivation": "To improve Multimodal Machine Translation (MMT) by addressing the challenges of modality gap and rigid visual-linguistic alignment while enabling inference beyond trained multimodal domains.", "method": "GIIFT employs novel multimodal scene graphs and a cross-modal Graph Attention Network adapter to integrate modality-specific information and generalize to broader translation domains without relying on images.", "result": "GIIFT surpasses existing MMT methods on the Multi30K dataset for English-to-French and English-to-German tasks and achieves state-of-the-art performance in image-free translation on the WMT benchmark.", "conclusion": "The GIIFT framework significantly enhances image-free inference in MMT, indicating that effective integration of modality-specific information can lead to better performance.", "key_contributions": ["Introduction of GIIFT framework for inductive image-free MMT", "Development of multimodal scene graphs for better modality integration", "State-of-the-art results on translation tasks without images"], "limitations": "", "keywords": ["Multimodal Machine Translation", "Graph-guided Inductive Learning", "Image-free Translation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.18570", "pdf": "https://arxiv.org/pdf/2507.18570.pdf", "abs": "https://arxiv.org/abs/2507.18570", "title": "Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods", "authors": ["Ganesh Sapkota", "Md Hasibur Rahman"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel hybrid tokenization strategy that enhances the\nperformance of DNA Language Models (DLMs) by combining 6-mer tokenization with\nByte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at\ncapturing local DNA sequence structures but often faces challenges, including\nuneven token distribution and a limited understanding of global sequence\ncontext. To address these limitations, we propose merging unique 6mer tokens\nwith optimally selected BPE tokens generated through 600 BPE cycles. This\nhybrid approach ensures a balanced and context-aware vocabulary, enabling the\nmodel to capture both short and long patterns within DNA sequences\nsimultaneously. A foundational DLM trained on this hybrid vocabulary was\nevaluated using next-k-mer prediction as a fine-tuning task, demonstrating\nsignificantly improved performance. The model achieved prediction accuracies of\n10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming\nstate-of-the-art models such as NT, DNABERT2, and GROVER. These results\nhighlight the ability of the hybrid tokenization strategy to preserve both the\nlocal sequence structure and global contextual information in DNA modeling.\nThis work underscores the importance of advanced tokenization methods in\ngenomic language modeling and lays a robust foundation for future applications\nin downstream DNA sequence analysis and biological research.", "AI": {"tldr": "The paper introduces a hybrid tokenization strategy for DNA Language Models that combines 6-mer tokenization and Byte Pair Encoding, improving prediction accuracy for DNA sequence modeling.", "motivation": "To overcome limitations of traditional k-mer tokenization in capturing local structures and understanding global context in DNA sequences.", "method": "The authors propose a hybrid approach that merges unique 6-mer tokens with optimally selected BPE tokens from 600 BPE cycles to create a balanced, context-aware vocabulary.", "result": "The foundational DLM showed significant performance gains, achieving prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming existing models like NT, DNABERT2, and GROVER.", "conclusion": "This hybrid tokenization method is crucial for preserving local and global sequence information in DNA modeling, serving as a strong basis for future genomic applications.", "key_contributions": ["Introduced a novel hybrid tokenization strategy for DNA Language Models.", "Demonstrated improved performance on next-k-mer prediction tasks.", "Highlighted the importance of advanced tokenization for genomic modeling."], "limitations": "", "keywords": ["DNA Language Models", "tokenization", "Byte Pair Encoding", "genomic analysis", "hybrid approach"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.18578", "pdf": "https://arxiv.org/pdf/2507.18578.pdf", "abs": "https://arxiv.org/abs/2507.18578", "title": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs", "authors": ["Feng Hong", "Geng Yu", "Yushi Ye", "Haicheng Huang", "Huangjie Zheng", "Ya Zhang", "Yanfeng Wang", "Jiangchao Yao"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion Large Language Models (DLLMs) have emerged as a compelling\nalternative to Autoregressive models, designed for fast parallel generation.\nHowever, existing DLLMs are plagued by a severe quality-speed trade-off, where\nfaster parallel decoding leads to significant performance degradation. We\nattribute this to the irreversibility of standard decoding in DLLMs, which is\neasily polarized into the wrong decoding direction along with early error\ncontext accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),\na training-free decoding algorithm that enables revokable decoding in DLLMs.\nWINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to\nverify and re-mask suspicious ones for refinement. Verified in open-source\nDLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the\nquality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6$\\times$ while improving accuracy by 2.58%; on\nFlickr30K captioning, it achieves a 10$\\times$ speedup with higher performance.\nMore comprehensive experiments are conducted to demonstrate the superiority and\nprovide an in-depth understanding of WINO.", "AI": {"tldr": "WINO is a new decoding algorithm for DLLMs that improves the quality-speed trade-off by allowing revokable decoding through a draft-and-verify mechanism.", "motivation": "To address the quality-speed trade-off in existing Diffusion Large Language Models during parallel decoding.", "method": "WINO employs a draft-and-verify mechanism where multiple tokens are drafted concurrently while using bidirectional context to verify and mask suspicious tokens for refinement.", "result": "WINO shows a 6x acceleration in inference on the GSM8K benchmark with a 2.58% improvement in accuracy and a 10x speedup on the Flickr30K task with enhanced performance.", "conclusion": "The proposed WINO algorithm significantly enhances the performance and efficiency of DLLMs, offering a viable solution to the quality-speed trade-off.", "key_contributions": ["Introduction of the WINO decoding algorithm for DLLMs", "Demonstrated improvements in speed and accuracy on benchmark tasks", "Validation on open-source DLLMs like LLaDA and MMaDA"], "limitations": "", "keywords": ["Diffusion Models", "Large Language Models", "Decoding Algorithm", "Quality-Speed Trade-off", "Deep Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18580", "pdf": "https://arxiv.org/pdf/2507.18580.pdf", "abs": "https://arxiv.org/abs/2507.18580", "title": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition", "authors": ["Jiahao Wang", "Ramen Liu", "Longhui Zhang", "Jing Li"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, accepted as oral presentation at CCL25-Eval", "summary": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel\nSRAG-MAV framework that synergistically integrates task reformulation(TR),\nSelf-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting\n(MAV). Our method reformulates the quadruplet extraction task into triplet\nextraction, uses dynamic retrieval from the training set to create contextual\nprompts, and applies multi-round inference with voting to improve output\nstability and performance. Our system, based on the Qwen2.5-7B model, achieves\na Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on\nthe STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o\n(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The\ncode is available at https://github.com/king-wang123/CCL25-SRAG-MAV.", "AI": {"tldr": "The paper presents a system for Fine-Grained Chinese Hate Speech Recognition using a novel framework that improves task performance through various methodologies.", "motivation": "To enhance the performance of Fine-Grained Chinese Hate Speech Recognition (FGCHSR) tasks using innovative computational approaches.", "method": "The authors proposed the SRAG-MAV framework, which integrates task reformulation, retrieval-augmented generation, and multi-round voting inference to improve stability and accuracy in hate speech recognition.", "result": "The system achieved superior performance metrics on the STATE ToxiCN dataset compared to leading models, with a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505.", "conclusion": "The novel framework significantly enhances hate speech recognition capabilities, marking a step forward in addressing hate speech in Chinese.", "key_contributions": ["Introduction of the SRAG-MAV framework for FGCHSR.", "Implementation of dynamic retrieval to create contextual prompts for improved accuracy.", "Demonstrated significant performance improvement over existing models."], "limitations": "", "keywords": ["Hate Speech Recognition", "Self-Retrieval-Augmented Generation", "Multi-Round Voting", "Chinese Language Processing", "Natural Language Processing"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.18584", "pdf": "https://arxiv.org/pdf/2507.18584.pdf", "abs": "https://arxiv.org/abs/2507.18584", "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs", "authors": ["Xiaopeng Ke", "Hexuan Deng", "Xuebo Liu", "Jun Rao", "Zhenxi Song", "Jun Yu", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "32 pages, 4 figures", "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.", "AI": {"tldr": "AQuilt is a framework for creating instruction-tuning data tailored to specialized domains from unlabeled data, enhancing LLM performance with reduced costs.", "motivation": "To improve performance of LLMs in specialized domains while minimizing computational costs and addressing generalization issues across tasks.", "method": "AQuilt constructs instruction-tuning data by utilizing unlabeled data and incorporating logic and inspection elements to facilitate reasoning processes.", "result": "AQuilt has been shown to perform comparably to DeepSeek-V3, achieving similar results at only 17% of the cost, with generated data demonstrating increased relevance for downstream tasks.", "conclusion": "The framework allows for effective data generation for various tasks, sustaining high-quality outputs with fewer resources.", "key_contributions": ["Introduction of AQuilt framework for instruction-tuning data generation.", "Reduction of production costs to 17% while maintaining performance.", "High relevance of generated data to downstream specialized tasks."], "limitations": "", "keywords": ["Large Language Models", "Domain-specific Performance", "Data Synthesis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.18618", "pdf": "https://arxiv.org/pdf/2507.18618.pdf", "abs": "https://arxiv.org/abs/2507.18618", "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards", "authors": ["Andreea Nica", "Ivan Zakazov", "Nicolas Mario Baldwin", "Saibo Geng", "Robert West"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.", "AI": {"tldr": "The paper introduces TRPrompt, a framework for optimizing prompts for large language models using textual feedback instead of prior dataset collection.", "motivation": "To enhance the reasoning abilities of large language models (LLMs) via prompt optimization without requiring parameter updates.", "method": "The Textual Reward Prompt (TRPrompt) framework integrates textual feedback into the training of a prompt model, improving it iteratively based on feedback on generated prompts.", "result": "TRPrompt leads to state-of-the-art query-specific prompts for challenging math problems from datasets like GSMHard and MATH.", "conclusion": "The framework successfully combines heuristic-based and reward-based approaches to create effective prompts, demonstrating high performance on specified math datasets.", "key_contributions": ["Introduction of the TRPrompt framework", "Unified approach of textual feedback and reward-based learning", "State-of-the-art performance on math datasets using the proposed framework"], "limitations": "", "keywords": ["Prompt Optimization", "Large Language Models", "Textual Feedback"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18624", "pdf": "https://arxiv.org/pdf/2507.18624.pdf", "abs": "https://arxiv.org/abs/2507.18624", "title": "Checklists Are Better Than Reward Models For Aligning Language Models", "authors": ["Vijay Viswanathan", "Yanchao Sun", "Shuang Ma", "Xiang Kong", "Meng Cao", "Graham Neubig", "Tongshuang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Language models must be adapted to understand and follow user instructions.\nReinforcement learning is widely used to facilitate this -- typically using\nfixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead\npropose using flexible, instruction-specific criteria as a means of broadening\nthe impact that reinforcement learning can have in eliciting instruction\nfollowing. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF).\nFrom instructions, we extract checklists and evaluate how well responses\nsatisfy each item - using both AI judges and specialized verifier programs -\nthen combine these scores to compute rewards for RL. We compare RLCF with other\nalignment methods applied to a strong instruction following model\n(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only\nmethod to improve performance on every benchmark, including a 4-point boost in\nhard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a\n3-point rise in win rate on Arena-Hard. These results establish checklist\nfeedback as a key tool for improving language models' support of queries that\nexpress a multitude of needs.", "AI": {"tldr": "This paper introduces Reinforcement Learning from Checklist Feedback (RLCF) to enhance language model performance by using instruction-specific criteria for reward computation.", "motivation": "To improve the effectiveness of reinforcement learning in language models by employing adaptive, instruction-specific feedback instead of fixed criteria.", "method": "RLCF extracts checklists from user instructions and evaluates language model responses against these checklists, generating reward scores via AI judges and verifier programs.", "result": "RLCF outperformed other alignment methods on five benchmarks, achieving significant performance improvements: a 4-point increase on FollowBench, a 6-point increase on InFoBench, and a 3-point rise on Arena-Hard.", "conclusion": "Checklist feedback is a potent tool for improving instruction-following capabilities in language models.", "key_contributions": ["Introduction of RLCF as a novel alignment strategy", "Demonstrated effectiveness of checklist-based feedback", "Improved performance across multiple benchmarks"], "limitations": "", "keywords": ["Reinforcement Learning", "Checklist Feedback", "Language Models", "Instruction Following", "AI Alignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.17753", "pdf": "https://arxiv.org/pdf/2507.17753.pdf", "abs": "https://arxiv.org/abs/2507.17753", "title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Model (LLM) agents are increasingly utilized in AI-aided\neducation to support tutoring and learning. Effective communication strategies\namong LLM agents improve collaborative problem-solving efficiency and\nfacilitate cost-effective adoption in education. However, little research has\nsystematically evaluated the impact of different communication strategies on\nagents' problem-solving. Our study examines four communication modes,\n\\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration},\n\\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a\ndual-agent, chat-based mathematical problem-solving environment using the\nOpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that\ndual-agent setups outperform single agents, with \\textit{peer-to-peer\ncollaboration} achieving the highest accuracy. Dialogue acts like statements,\nacknowledgment, and hints play a key role in collaborative problem-solving.\nWhile multi-agent frameworks enhance computational tasks, effective\ncommunication strategies are essential for tackling complex problems in AI\neducation.", "AI": {"tldr": "This study evaluates the impact of different communication strategies among LLM agents in AI-supported education, particularly in collaborative math problem-solving environments.", "motivation": "To improve collaborative problem-solving efficiency in AI-aided education through effective communication strategies among LLM agents.", "method": "The research examines four communication modes: teacher-student interaction, peer-to-peer collaboration, reciprocal peer teaching, and critical debate within a dual-agent, chat-based mathematical problem-solving system utilizing the OpenAI GPT-4o model.", "result": "The results indicate that dual-agent setups perform better than single agents, with peer-to-peer collaboration achieving the highest accuracy on the MATH dataset.", "conclusion": "While multi-agent frameworks can enhance computational tasks, successful communication strategies are vital for addressing complex issues in AI education.", "key_contributions": ["Systematic evaluation of different communication strategies among LLM agents in education", "Demonstration of dual-agent setups outperforming single agents", "Identification of key dialogue acts that facilitate collaborative problem-solving."], "limitations": "", "keywords": ["Large Language Models", "AI education", "collaborative problem-solving", "dual-agent systems", "communication strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.18572", "pdf": "https://arxiv.org/pdf/2507.18572.pdf", "abs": "https://arxiv.org/abs/2507.18572", "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design", "authors": ["Donghoon Shin", "Daniel Lee", "Gary Hsieh", "Gromit Yeuk-Yin Chan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives.", "AI": {"tldr": "PosterMate is a generative AI-based poster design assistant that enhances collaboration and feedback from diverse audiences through audience-driven persona agents.", "motivation": "To improve the poster design process by facilitating synchronous feedback from target audiences with diverse perspectives.", "method": "PosterMate constructs persona agents from marketing documents and gathers feedback through these agents, stimulating discussion to reach design edit conclusions, followed by integration into the poster.", "result": "User studies showed PosterMate captures overlooked viewpoints and serves as an effective prototyping tool, with feedback deemed appropriate based on persona identities.", "conclusion": "PosterMate enhances collaboration in poster design by effectively synthesizing feedback from various persona agents, integrating agreed edits into the design.", "key_contributions": ["Introduces the PosterMate assistant for poster design feedback.", "Uses generative AI to simulate human-like interactions for audience-driven feedback.", "Demonstrates effectiveness in capturing diverse viewpoints through persona agents."], "limitations": "", "keywords": ["poster design", "AI feedback", "collaborative design", "persona agents", "user study"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2212.10678", "pdf": "https://arxiv.org/pdf/2212.10678.pdf", "abs": "https://arxiv.org/abs/2212.10678", "title": "Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias", "authors": ["Yuen Chen", "Vethavikashini Chithrra Raghuram", "Justus Mattern", "Rada Mihalcea", "Zhijing Jin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.", "AI": {"tldr": "This paper presents a causal framework for measuring bias in generative language models, introducing a benchmark called OccuGender to assess occupational gender bias in various LLMs and discussing strategies for bias mitigation.", "motivation": "To understand and quantify harmful biases found in the outputs of large language models (LLMs) against different demographics, motivating the need for robust bias measurement and benchmarks.", "method": "The paper proposes a causal formulation for bias measurement, outlines desiderata for bias benchmarks, and introduces the OccuGender benchmark to evaluate occupational gender bias in state-of-the-art LLMs.", "result": "Testing several open-source LLMs like Llama and Mistral on OccuGender revealed significant levels of occupational gender bias within these models.", "conclusion": "The findings underscore the need for bias mitigation strategies and highlight the generalizability of the proposed causal framework for future bias assessments.", "key_contributions": ["Introduction of a causal formulation for measuring bias in LLMs.", "Development of the OccuGender benchmark for assessing occupational gender bias.", "Empirical testing of state-of-the-art LLMs revealing substantial biases."], "limitations": "", "keywords": ["bias measurement", "generative language models", "occupational gender bias", "LLMs", "bias mitigation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2308.09954", "pdf": "https://arxiv.org/pdf/2308.09954.pdf", "abs": "https://arxiv.org/abs/2308.09954", "title": "DocTER: Evaluating Document-based Knowledge Editing", "authors": ["Suhang Wu", "Ante Wang", "Minlong Peng", "Yujie Lin", "Wenbo Li", "Mingming Sun", "Jinsong Su"], "categories": ["cs.CL", "cs.AI"], "comment": "Information processing & management", "summary": "Knowledge editing aims to correct outdated or inaccurate knowledge in neural\nnetworks. In this paper, we explore knowledge editing using easily accessible\ndocuments instead of manually labeled factual triples employed in earlier\nresearch. To advance this field, we establish the first evaluation benchmark,\n\\textit{DocTER}, featuring Documents containing counterfactual knowledge for\nediting. A comprehensive four-perspective evaluation is introduced: Edit\nSuccess, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional\ntriplet-based knowledge editing methods for this task, we develop an\nExtract-then-Edit pipeline that extracts triples from documents before applying\nexisting methods. Experiments on popular knowledge editing methods demonstrate\nthat editing with documents presents significantly greater challenges than\nusing triples. In document-based scenarios, even the best-performing in-context\nediting approach still lags behind by 10 points in editing success when\ncompared to using gold triples. This observation also holds for both reasoning\nand cross-lingual test sets. We further analyze key factors influencing task\nperformance, including the quality of extracted triples, the frequency and\nposition of edited knowledge in documents, various methods for enhancing\nreasoning, and performance differences across various directions in\ncross-lingual knowledge editing, which provide valuable insights for future\nresearch.", "AI": {"tldr": "This paper introduces a novel approach to knowledge editing in neural networks using documents instead of labeled triples, establishing a benchmark and evaluating editing challenges.", "motivation": "To improve knowledge editing techniques by using easily accessible documents for correcting outdated knowledge in neural networks, moving away from reliance on manually labeled factual triples.", "method": "The authors propose the Extract-then-Edit pipeline, which extracts factual triples from documents before applying conventional knowledge editing techniques. They establish the first evaluation benchmark, DocTER, focusing on document-based editing.", "result": "Experiments indicate that editing with documents presents greater challenges than using traditional triples, with a 10-point gap in success rates for even the top editing methods in document scenarios compared to gold triples.", "conclusion": "The findings reveal significant challenges in document-based knowledge editing, highlighting the need for enhanced methods and analysis of factors affecting performance, paving the way for future research.", "key_contributions": ["Introduction of the DocTER benchmark for document-based knowledge editing", "Development of the Extract-then-Edit pipeline for knowledge editing", "Insights into factors affecting editing performance such as quality of extracted triples and document structure."], "limitations": "The study primarily focuses on document-based editing and may not directly address all aspects of knowledge editing with neural networks, especially those involving labeled triples.", "keywords": ["Knowledge editing", "Neural networks", "DocTER benchmark", "Extract-then-Edit pipeline", "Cross-lingual knowledge"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2401.01405", "pdf": "https://arxiv.org/pdf/2401.01405.pdf", "abs": "https://arxiv.org/abs/2401.01405", "title": "Quantifying the Uniqueness and Divisiveness of Presidential Discourse", "authors": ["Karen Zhou", "Alexander A. Meitus", "Milo Chase", "Grace Wang", "Anne Mykland", "William Howell", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": "Published in PNAS Nexus:\n  https://academic.oup.com/pnasnexus/article/3/10/pgae431/7814873", "summary": "Do American presidents speak discernibly different from each other? If so, in\nwhat ways? And are these differences confined to any single medium of\ncommunication? To investigate these questions, this paper introduces a novel\nmetric of uniqueness based on large language models, develops a new lexicon for\ndivisive speech, and presents a framework for assessing the distinctive ways in\nwhich presidents speak about their political opponents. Applying these tools to\na variety of corpora of presidential speeches, we find considerable evidence\nthat Donald Trump's speech patterns diverge from those of all major party\nnominees for the presidency in recent history. Trump is significantly more\ndistinctive than his fellow Republicans, whose uniqueness values appear closer\nto those of the Democrats. Contributing to these differences is Trump's\nemployment of divisive and antagonistic language, particularly when targeting\nhis political opponents. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular changes in\npresidential communications.", "AI": {"tldr": "This paper analyzes the distinct speech patterns of American presidents, particularly focusing on Donald Trump's unique communication style in comparison to past nominees.", "motivation": "To determine if American presidents have identifiable differences in their speech and whether these differences vary by communication medium.", "method": "A novel metric of uniqueness based on large language models was introduced, alongside a new lexicon for divisive speech. This framework was applied to various presidential speech corpora.", "result": "Significant evidence that Donald Trump's speech patterns diverge considerably from those of other major party nominees, with a stronger use of distinctive and divisive language.", "conclusion": "Trump's unique speech patterns are present across different types of communication and are not merely a result of broader changes in presidential communication styles.", "key_contributions": ["Introduction of a uniqueness metric based on large language models", "Development of a lexicon for divisive speech", "Framework for assessing presidential speech differences"], "limitations": "", "keywords": ["presidential speech", "language models", "divisive language", "political communication", "text analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2407.19795", "pdf": "https://arxiv.org/pdf/2407.19795.pdf", "abs": "https://arxiv.org/abs/2407.19795", "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks", "authors": ["Juhwan Choi", "Junehyoung Kwon", "JungMin Yun", "Seunguk Yu", "YoungBin Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)", "summary": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.", "AI": {"tldr": "Introduction of a new dataset VolDoGer for domain generalization in vision-language tasks.", "motivation": "To improve domain generalizability of deep learning models in vision-language tasks due to a lack of suitable datasets.", "method": "Proposed VolDoGer dataset is created for image captioning, visual question answering, and visual entailment, using LLM-based data annotation techniques.", "result": "Evaluation of various models, including fine-tuned models and a multimodal large language model, confirms the dataset's effectiveness for assessing domain generalizability.", "conclusion": "VolDoGer serves as a valuable resource for enhancing the robustness of vision-language models across different domains.", "key_contributions": ["Introduction of the VolDoGer dataset for vision-language tasks.", "Utilization of LLM-based annotation methods to reduce reliance on human annotators.", "Evaluation of models’ domain generalizability using this new dataset."], "limitations": "The dataset's effectiveness may be limited by the quality of LLM-based annotations and scope of tasks covered.", "keywords": ["domain generalization", "vision-language tasks", "dataset", "LLM", "deep learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.13725", "pdf": "https://arxiv.org/pdf/2409.13725.pdf", "abs": "https://arxiv.org/abs/2409.13725", "title": "Identity-related Speech Suppression in Generative AI Content Moderation", "authors": ["Grace Proebsting", "Oghenefejiro Isaacs Anigboro", "Charlie M. Crawford", "Danaé Metaxa", "Sorelle A. Friedler"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "ACM Conference on Equity and Access in Algorithms, Mechanisms, and\n  Optimization, 2025", "summary": "Automated content moderation has long been used to help identify and filter\nundesired user-generated content online. But such systems have a history of\nincorrectly flagging content by and about marginalized identities for removal.\nGenerative AI systems now use such filters to keep undesired generated content\nfrom being created by or shown to users. While a lot of focus has been given to\nmaking sure such systems do not produce undesired outcomes, considerably less\nattention has been paid to making sure appropriate text can be generated. From\nclassrooms to Hollywood, as generative AI is increasingly used for creative or\nexpressive text generation, whose stories will these technologies allow to be\ntold, and whose will they suppress?\n  In this paper, we define and introduce measures of speech suppression,\nfocusing on speech related to different identity groups incorrectly filtered by\na range of content moderation APIs. Using both short-form, user-generated\ndatasets traditional in content moderation and longer generative AI-focused\ndata, including two datasets we introduce in this work, we create a benchmark\nfor measurement of speech suppression for nine identity groups. Across one\ntraditional and four generative AI-focused automated content moderation\nservices tested, we find that identity-related speech is more likely to be\nincorrectly suppressed than other speech. We find that reasons for incorrect\nflagging behavior vary by identity based on stereotypes and text associations,\nwith, e.g., disability-related content more likely to be flagged for self-harm\nor health-related reasons while non-Christian content is more likely to be\nflagged as violent or hateful. As generative AI systems are increasingly used\nfor creative work, we urge further attention to how this may impact the\ncreation of identity-related content.", "AI": {"tldr": "The paper investigates the suppression of identity-related speech by automated content moderation systems, highlighting biases in filtering practices across generative AI and traditional moderation APIs.", "motivation": "To address the unjust suppression of speech by automated content moderation systems, particularly related to marginalized identities, and examine its implications on generative AI.", "method": "The authors define measures of speech suppression and create a benchmark for evaluating how different identity groups are affected by content moderation, using both user-generated and generative AI datasets.", "result": "The study finds that identity-related speech is more frequently suppressed than other types of speech, with differing reasons for suppression based on identities and stereotypes.", "conclusion": "The paper calls for increased awareness of the impact of content moderation on the expression of identity-related narratives as generative AI becomes more prevalent in creative domains.", "key_contributions": ["Introduction of measures of speech suppression for identity groups.", "Creation of a benchmark for evaluating speech suppression across different moderation services.", "Analysis of suppression reasons linked to stereotypes associated with various identities."], "limitations": "", "keywords": ["Content Moderation", "Speech Suppression", "Generative AI", "Identity Groups", "Bias"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.07037", "pdf": "https://arxiv.org/pdf/2411.07037.pdf", "abs": "https://arxiv.org/abs/2411.07037", "title": "LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios", "authors": ["Xiaodong Wu", "Minhao Wang", "Yichen Liu", "Xiaoming Shi", "He Yan", "Xiangju Lu", "Junmin Zhu", "Wei Zhang"], "categories": ["cs.CL"], "comment": "17 pages, 3 figures", "summary": "As Large Language Models (LLMs) evolve in natural language processing (NLP),\ntheir ability to stably follow instructions in long-context inputs has become\ncritical for real-world applications. However, existing benchmarks seldom focus\non instruction-following in long-context scenarios or stability on different\ninputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed\nto evaluate LLMs' instruction-following capabilities and stability across long\ncontexts. LIFBench comprises three long-context scenarios and eleven diverse\ntasks, featuring 2,766 instructions generated through an automated expansion\nmethod across three dimensions: length, expression, and variables. For\nevaluation, we propose LIFEval, a rubric-based assessment method that enables\nprecise, automated scoring of complex LLM responses without reliance on\nLLM-assisted assessments or human judgment. This method allows for a\ncomprehensive analysis of model performance and stability from multiple\nperspectives. We conduct detailed experiments on 20 prominent LLMs across six\nlength intervals. Our work contributes LIFBench and LIFEval as robust tools for\nassessing LLM performance in complex and long-context settings, offering\nvaluable insights to guide future advancements in LLM development.", "AI": {"tldr": "This paper introduces LIFBench, a dataset for evaluating Large Language Models (LLMs) on instruction-following in long-context scenarios, along with LIFEval, a rubric-based assessment method.", "motivation": "To address the lack of focus on LLMs' instruction-following capabilities and stability in long-context inputs in existing benchmarks.", "method": "LIFBench comprises three long-context scenarios with 2,766 generated instructions, evaluated using LIFEval, a rubric-based scoring method.", "result": "Evaluation conducted on 20 LLMs across six length intervals using the proposed methods revealing insights into instruction-following performance and stability.", "conclusion": "LIFBench and LIFEval provide robust tools for evaluating LLMs, facilitating future research in LLM development and understanding performance in complex scenarios.", "key_contributions": ["Introduction of LIFBench dataset for long-context instruction-following evaluation.", "Development of LIFEval for precise automated scoring of LLM responses.", "Comprehensive analysis across 20 LLMs on stability and performance metrics."], "limitations": "", "keywords": ["Large Language Models", "instruction-following", "natural language processing", "long-context", "LIFEval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.15487", "pdf": "https://arxiv.org/pdf/2502.15487.pdf", "abs": "https://arxiv.org/abs/2502.15487", "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models", "authors": ["Martina Miliani", "Serena Auriemma", "Alessandro Bondielli", "Emmanuele Chersoni", "Lucia Passaro", "Irene Sucameli", "Alessandro Lenci"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "I.2.7"], "comment": "Accepted for publication in Findings of ACL 2025", "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.", "AI": {"tldr": "This paper presents ExpliCa, a new dataset for assessing large language models (LLMs) on explicit causal reasoning.", "motivation": "There is a need for better evaluation methods for LLMs in tasks involving interpretive and inferential reasoning, particularly in causal reasoning scenarios.", "method": "The authors introduced ExpliCa, which integrates causal and temporal relations expressed through linguistic connectives, and evaluated seven LLMs using prompting and perplexity-based metrics.", "result": "The evaluation revealed that even leading LLMs did not achieve 0.80 accuracy, often confusing causal and temporal relations, with performance varying based on linguistic ordering.", "conclusion": "The study highlights significant challenges faced by LLMs in causal reasoning, underscoring the importance of the dataset for future research and benchmarking.", "key_contributions": ["Introduction of the ExpliCa dataset for explicit causal reasoning evaluation.", "Demonstration of LLMs' struggles with causal vs. temporal relations.", "Insights into the influence of linguistic order on model performance."], "limitations": "", "keywords": ["Large Language Models", "Causal Reasoning", "Temporal Relations", "Dataset", "Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.21676", "pdf": "https://arxiv.org/pdf/2503.21676.pdf", "abs": "https://arxiv.org/abs/2503.21676", "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "authors": ["Nicolas Zucchet", "Jörg Bornschein", "Stephanie Chan", "Andrew Lampinen", "Razvan Pascanu", "Soham De"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at the 2nd Conference on Language Modeling (2025)", "summary": "Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training.", "AI": {"tldr": "This paper examines the learning dynamics of language models during pre-training, revealing three key phases of knowledge acquisition and their implications.", "motivation": "Understanding the dynamics of knowledge acquisition in large language models to improve training effectiveness and knowledge recall.", "method": "Investigation of learning dynamics on a synthetic factual recall task, analyzing the impact of training data distribution and model behavior during training.", "result": "Language models learn in three phases, experience performance plateaus, are affected by data distribution, and face challenges integrating new knowledge without corrupting existing memories.", "conclusion": "Data distribution plays a crucial role in the learning dynamics of language models, suggesting new strategies for effective data scheduling in training.", "key_contributions": ["Identification of three phases in the learning dynamics of language models", "Demonstration of the impact of data distribution on learning", "Insights into the challenges of knowledge integration through fine-tuning"], "limitations": "The findings are based on synthetic tasks and may not fully represent real-world applications of language models.", "keywords": ["language models", "learning dynamics", "knowledge acquisition", "data distribution", "fine-tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.05211", "pdf": "https://arxiv.org/pdf/2504.05211.pdf", "abs": "https://arxiv.org/abs/2504.05211", "title": "Exploiting individual differences to bootstrap communication", "authors": ["Richard A. Blythe", "Casimir Fisch"], "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "comment": "Revised version is a full paper with considerable additional\n  exposition and discussion. Now 21 pages including supplementary information,\n  11 figures", "summary": "Establishing a communication system is hard because the intended meaning of a\nsignal is unknown to its receiver when first produced, and the signaller also\nhas no idea how that signal will be interpreted. Most theoretical accounts of\nthe emergence of communication systems rely on feedback to reinforce behaviours\nthat have led to successful communication in the past. However, providing such\nfeedback requires already being able to communicate the meaning that was\nintended or interpreted. Therefore these accounts cannot explain how\ncommunication can be bootstrapped from non-communicative behaviours. Here we\npresent a model that shows how a communication system, capable of expressing an\nunbounded number of meanings, can emerge as a result of individual behavioural\ndifferences in a large population without any pre-existing means to determine\ncommunicative success. The two key cognitive capabilities responsible for this\noutcome are behaving predictably in a given situation, and an alignment of\npsychological states ahead of signal production that derives from shared\nintentionality. Since both capabilities can exist independently of\ncommunication, our results are compatible with theories in which large flexible\nsocially-learned communication systems like language are the product of a\ngeneral but well-developed capacity for social cognition.", "AI": {"tldr": "The paper presents a model demonstrating how a communication system can emerge from non-communicative behaviors in a population, driven by predictable behavior and shared psychological states, without prior communication mechanisms.", "motivation": "To address the challenge of explaining how communication systems can form without existing signals or meanings, bridging the gap between non-communicative behavior and structured communication.", "method": "The authors develop a theoretical model that illustrates the emergence of a communication system through individual behavioral differences, focusing on predictable actions and shared intentionality among individuals.", "result": "The model indicates that a communication system capable of expressing unlimited meanings can arise from non-communicative interactions, grounded in general capacities for social cognition.", "conclusion": "The findings support the idea that complex communication systems may evolve from simpler social interactions and cognitive skills, rather than from established communicative frameworks.", "key_contributions": ["Development of a model to explain the emergence of communication systems from non-communicative behaviors", "Identification of key cognitive capabilities (predictable behavior, shared psychological states) that facilitate communication formation", "Reinforcement of theories relating to social cognition and the evolution of language"], "limitations": "", "keywords": ["communication systems", "social cognition", "modeling"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.16632", "pdf": "https://arxiv.org/pdf/2507.16632.pdf", "abs": "https://arxiv.org/abs/2507.16632", "title": "Step-Audio 2 Technical Report", "authors": ["Boyong Wu", "Chao Yan", "Chen Hu", "Cheng Yi", "Chengli Feng", "Fei Tian", "Feiyu Shen", "Gang Yu", "Haoyang Zhang", "Jingbei Li", "Mingrui Chen", "Peng Liu", "Wang You", "Xiangyu Tony Zhang", "Xingyuan Li", "Xuerui Yang", "Yayue Deng", "Yechang Huang", "Yuxin Li", "Yuxin Zhang", "Zhao You", "Brian Li", "Changyi Wan", "Hanpeng Hu", "Jiangjie Zhen", "Siyu Chen", "Song Yuan", "Xuelin Zhang", "Yimin Jiang", "Yu Zhou", "Yuxiang Yang", "Bingxin Li", "Buyun Ma", "Changhe Song", "Dongqing Pang", "Guoqiang Hu", "Haiyang Sun", "Kang An", "Na Wang", "Shuli Gao", "Wei Ji", "Wen Li", "Wen Sun", "Xuan Wen", "Yong Ren", "Yuankai Ma", "Yufan Lu", "Bin Wang", "Bo Li", "Changxin Miao", "Che Liu", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Donghang Wu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Han Zhang", "Hao Nie", "Haonan Jia", "Hongyu Zhou", "Jianjian Sun", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Jin Yang", "Junzhe Lin", "Kaixiang Li", "Lei Yang", "Liying Shi", "Li Zhou", "Longlong Gu", "Ming Li", "Mingliang Li", "Mingxiao Li", "Nan Wu", "Qi Han", "Qinyuan Tan", "Shaoliang Pang", "Shengjie Fan", "Siqi Liu", "Tiancheng Cao", "Wanying Lu", "Wenqing He", "Wuxun Xie", "Xu Zhao", "Xueqi Li", "Yanbo Yu", "Yang Yang", "Yi Liu", "Yifan Lu", "Yilei Wang", "Yuanhao Ding", "Yuanwei Liang", "Yuanwei Lu", "Yuchu Luo", "Yuhe Yin", "Yumeng Zhan", "Yuxiang Zhang", "Zidong Yang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper presents Step-Audio 2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information.", "AI": {"tldr": "Step-Audio 2 is a multi-modal large language model for audio understanding and speech conversation, utilizing latent audio encoding and reinforcement learning.", "motivation": "To enhance automatic speech recognition and audio understanding while enabling responsive speech conversations that incorporate emotions and speaking styles.", "method": "Integrates a latent audio encoder, reasoning-centric reinforcement learning, and a RAG approach while calling external tools for dynamic conversation.", "result": "Achieves state-of-the-art performance on various benchmarks for audio understanding and conversational AI, enhancing responsiveness and expressiveness.", "conclusion": "Step-Audio 2's innovative integration of audio token generation and external tools significantly improves conversational capabilities.", "key_contributions": ["Development of a multi-modal large language model for audio understanding.", "Incorporation of reinforcement learning for reasoning-centric audio analysis.", "Enhanced responsiveness to emotional and stylistic variances in speech."], "limitations": "", "keywords": ["multi-modal", "audio understanding", "speech conversation", "reinforcement learning", "retrieval-augmented generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.16809", "pdf": "https://arxiv.org/pdf/2507.16809.pdf", "abs": "https://arxiv.org/abs/2507.16809", "title": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs", "authors": ["Da-Chen Lian", "Ri-Sheng Huang", "Pin-Er Chen", "Chunki Lim", "You-Kuan Lin", "Guan-Yu Tseng", "Zi-Cheng Yang", "Zhen-Yu Lin", "Pin-Cheng Chen", "Shu-Kai Hsieh"], "categories": ["cs.CL"], "comment": "42p, 17f, 10t. Revisions: Merged paragraphs in Intro to emphasize\n  contributions. Clarified benchmark design (Sec 3.5.1). Added single-agent,\n  OpenAI-guided & 6-round experiments (Sec 5.2). Note: we only ran each\n  experiment once; statistical tests are needed for strong claims. Revised Sec\n  6. Added acknowledgements, 2 new co-authors, and corrected typos/grammar", "summary": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs.", "AI": {"tldr": "LingBench++ is a benchmark and reasoning framework to evaluate large language models on complex linguistic tasks, emphasizing structured reasoning and cross-cultural insights.", "motivation": "To enhance the evaluation of LLMs beyond accuracy, focusing on reasoning and understanding across low-resource languages.", "method": "Development of a multi-agent architecture that integrates grammatical knowledge retrieval, tool-augmented reasoning, and hypothesis testing.", "result": "Multi-agent models with external knowledge and iterative reasoning showed improved accuracy and interpretability over conventional single-pass methods.", "conclusion": "LingBench++ provides a new framework for advancing LLM reasoning with a strong linguistic and cultural basis.", "key_contributions": ["Introduces structured reasoning traces for LLM evaluation.", "Expands linguistic task coverage across 90+ languages.", "Demonstrates the effectiveness of integrating external knowledge in LLMs."], "limitations": "Statistical tests on the experiments are needed to support strong claims; experiments were only run once.", "keywords": ["Linguistic Benchmark", "Large Language Models", "Reasoning Framework", "Cross-Cultural Evaluation", "Cognitive Plausibility"], "importance_score": 8, "read_time_minutes": 10}}
