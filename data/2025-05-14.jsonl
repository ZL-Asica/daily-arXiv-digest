{"id": "2505.08031", "pdf": "https://arxiv.org/pdf/2505.08031.pdf", "abs": "https://arxiv.org/abs/2505.08031", "title": "Measuring and predicting variation in the difficulty of questions about data visualizations", "authors": ["Arnav Verma", "Judith E. Fan"], "categories": ["cs.HC"], "comment": null, "summary": "Understanding what is communicated by data visualizations is a critical\ncomponent of scientific literacy in the modern era. However, it remains unclear\nwhy some tasks involving data visualizations are more difficult than others.\nHere we administered a composite test composed of five widely used tests of\ndata visualization literacy to a large sample of U.S. adults (N=503\nparticipants).We found that items in the composite test spanned the full range\nof possible difficulty levels, and that our estimates of item-level difficulty\nwere highly reliable. However, the type of data visualization shown and the\ntype of task involved only explained a modest amount of variation in\nperformance across items, relative to the reliability of the estimates we\nobtained. These results highlight the need for finer-grained ways of\ncharacterizing these items that predict the reliable variation in difficulty\nmeasured in this study, and that generalize to other tests of data\nvisualization understanding."}
{"id": "2505.08048", "pdf": "https://arxiv.org/pdf/2505.08048.pdf", "abs": "https://arxiv.org/abs/2505.08048", "title": "Partisan Fact-Checkers' Warnings Can Effectively Correct Individuals' Misbeliefs About Political Misinformation", "authors": ["Sian Lee", "Haeseung Seo", "Aiping Xiong", "Dongwon Lee"], "categories": ["cs.HC"], "comment": null, "summary": "Political misinformation, particularly harmful when it aligns with\nindividuals' preexisting beliefs and political ideologies, has become\nwidespread on social media platforms. In response, platforms like Facebook and\nX introduced warning messages leveraging fact-checking results from third-party\nfact-checkers to alert users against false content. However, concerns persist\nabout the effectiveness of these fact-checks, especially when fact-checkers are\nperceived as politically biased. To address these concerns, this study presents\nfindings from an online human-subject experiment (N=216) investigating how the\npolitical stances of fact-checkers influence their effectiveness in correcting\nmisbeliefs about political misinformation. Our findings demonstrate that\npartisan fact-checkers can decrease the perceived accuracy of political\nmisinformation and correct misbeliefs without triggering backfire effects. This\ncorrection is even more pronounced when the misinformation aligns with\nindividuals' political ideologies. Notably, while previous research suggests\nthat fact-checking warnings are less effective for conservatives than liberals,\nour results suggest that explicitly labeled partisan fact-checkers, positioned\nas political counterparts to conservatives, are particularly effective in\nreducing conservatives' misbeliefs toward pro-liberal misinformation."}
{"id": "2505.08063", "pdf": "https://arxiv.org/pdf/2505.08063.pdf", "abs": "https://arxiv.org/abs/2505.08063", "title": "Who's the Leader? Analyzing Novice Workflows in LLM-Assisted Debugging of Machine Learning Code", "authors": ["Jessica Y. Bo", "Majeed Kazemitabaar", "Emma Zhuang", "Ashton Anderson"], "categories": ["cs.HC"], "comment": "Tools for Thought Workshop at CHI 2025", "summary": "While LLMs are often touted as tools for democratizing specialized knowledge\nto beginners, their actual effectiveness for improving task performance and\nlearning is still an open question. It is known that novices engage with LLMs\ndifferently from experts, with prior studies reporting meta-cognitive pitfalls\nthat affect novices' ability to verify outputs and prompt effectively. We focus\non a task domain, machine learning (ML), which embodies both high complexity\nand low verifiability to understand the impact of LLM assistance on novices.\nProvided a buggy ML script and open access to ChatGPT, we conduct a formative\nstudy with eight novice ML engineers to understand their reliance on,\ninteractions with, and perceptions of the LLM. We find that user actions can be\nroughly categorized into leading the LLM and led-by the LLM, and further\ninvestigate how they affect reliance outcomes like over- and under-reliance.\nThese results have implications on novices' cognitive engagement in\nLLM-assisted tasks and potential negative effects on downstream learning.\nLastly, we pose potential augmentations to the novice-LLM interaction paradigm\nto promote cognitive engagement."}
{"id": "2505.07831", "pdf": "https://arxiv.org/pdf/2505.07831.pdf", "abs": "https://arxiv.org/abs/2505.07831", "title": "Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces", "authors": ["Michael Pichat", "William Pogrund", "Paloma Pichat", "Judicael Poumay", "Armanouche Gasparian", "Samuel Demarchi", "Martin Corbet", "Alois Georgeon", "Michael Veillet-Guillem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The polysemantic nature of synthetic neurons in artificial intelligence\nlanguage models is currently understood as the result of a necessary\nsuperposition of distributed features within the latent space. We propose an\nalternative approach, geometrically defining a neuron in layer n as a\ncategorical vector space with a non-orthogonal basis, composed of categorical\nsub-dimensions extracted from preceding neurons in layer n-1. This categorical\nvector space is structured by the activation space of each neuron and enables,\nvia an intra-neuronal attention process, the identification and utilization of\na critical categorical zone for the efficiency of the language model - more\nhomogeneous and located at the intersection of these different categorical\nsub-dimensions."}
{"id": "2505.08064", "pdf": "https://arxiv.org/pdf/2505.08064.pdf", "abs": "https://arxiv.org/abs/2505.08064", "title": "Justified Evidence Collection for Argument-based AI Fairness Assurance", "authors": ["Alpay Sabuncuoglu", "Christopher Burr", "Carsten Maple"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "The paper is accepted for ACM Conference on Fairness, Accountability,\n  and Transparency (ACM FAccT '25)", "summary": "It is well recognised that ensuring fair AI systems is a complex\nsociotechnical challenge, which requires careful deliberation and continuous\noversight across all stages of a system's lifecycle, from defining requirements\nto model deployment and deprovisioning. Dynamic argument-based assurance cases,\nwhich present structured arguments supported by evidence, have emerged as a\nsystematic approach to evaluating and mitigating safety risks and hazards in\nAI-enabled system development and have also been extended to deal with broader\nnormative goals such as fairness and explainability. This paper introduces a\nsystems-engineering-driven framework, supported by software tooling, to\noperationalise a dynamic approach to argument-based assurance in two stages. In\nthe first stage, during the requirements planning phase, a multi-disciplinary\nand multi-stakeholder team define goals and claims to be established (and\nevidenced) by conducting a comprehensive fairness governance process. In the\nsecond stage, a continuous monitoring interface gathers evidence from existing\nartefacts (e.g. metrics from automated tests), such as model, data, and use\ncase documentation, to support these arguments dynamically. The framework's\neffectiveness is demonstrated through an illustrative case study in finance,\nwith a focus on supporting fairness-related arguments."}
{"id": "2505.07850", "pdf": "https://arxiv.org/pdf/2505.07850.pdf", "abs": "https://arxiv.org/abs/2505.07850", "title": "A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas", "authors": ["Pranav Narayanan Venkit", "Jiayi Li", "Yingfan Zhou", "Sarah Rajtmajer", "Shomir Wilson"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As LLMs (large language models) are increasingly used to generate synthetic\npersonas particularly in data-limited domains such as health, privacy, and HCI,\nit becomes necessary to understand how these narratives represent identity,\nespecially that of minority communities. In this paper, we audit synthetic\npersonas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the\nlens of representational harm, focusing specifically on racial identity. Using\na mixed methods approach combining close reading, lexical analysis, and a\nparameterized creativity framework, we compare 1512 LLM generated personas to\nhuman-authored responses. Our findings reveal that LLMs disproportionately\nforeground racial markers, overproduce culturally coded language, and construct\npersonas that are syntactically elaborate yet narratively reductive. These\npatterns result in a range of sociotechnical harms, including stereotyping,\nexoticism, erasure, and benevolent bias, that are often obfuscated by\nsuperficially positive narrations. We formalize this phenomenon as algorithmic\nothering, where minoritized identities are rendered hypervisible but less\nauthentic. Based on these findings, we offer design recommendations for\nnarrative-aware evaluation metrics and community-centered validation protocols\nfor synthetic identity generation."}
{"id": "2505.08072", "pdf": "https://arxiv.org/pdf/2505.08072.pdf", "abs": "https://arxiv.org/abs/2505.08072", "title": "Perspectives on Capturing Emotional Expressiveness in Sign Language", "authors": ["Phoebe Chua", "Cathy Mengying Fang", "Yasith Samaradivakara", "Pattie Maes", "Suranga Nanayakkara"], "categories": ["cs.HC"], "comment": null, "summary": "Significant advances have been made in our ability to understand and generate\nemotionally expressive content such as text and speech, yet comparable progress\nin sign language technologies remain limited. While computational approaches to\nsign language translation have focused on capturing lexical content, the\nemotional dimensions of sign language communication remain largely unexplored.\nThrough semi-structured interviews with eight sign language users across\nSingapore, Sri Lanka and the United States, including both Deaf and Hard of\nhearing (DHH) and hearing signers, we investigate how emotions are expressed\nand perceived in sign languages. Our findings highlight the role of both manual\nand non-manual elements in emotional expression, revealing universal patterns\nas well as individual and cultural variations in how signers communicate\nemotions. We identify key challenges in capturing emotional nuance for sign\nlanguage translation, and propose design considerations for developing more\nemotionally-aware sign language technologies. This work contributes to both\ntheoretical understanding of emotional expression in sign language and\npractical development of interfaces to better serve diverse signing\ncommunities."}
{"id": "2505.07852", "pdf": "https://arxiv.org/pdf/2505.07852.pdf", "abs": "https://arxiv.org/abs/2505.07852", "title": "Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment", "authors": ["Ali Senol", "Garima Agrawal", "Huan Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Detecting fake interactions in digital communication platforms remains a\nchallenging and insufficiently addressed problem. These interactions may appear\nas harmless spam or escalate into sophisticated scam attempts, making it\ndifficult to flag malicious intent early. Traditional detection methods often\nrely on static anomaly detection techniques that fail to adapt to dynamic\nconversational shifts. One key limitation is the misinterpretation of benign\ntopic transitions referred to as concept drift as fraudulent behavior, leading\nto either false alarms or missed threats. We propose a two stage detection\nframework that first identifies suspicious conversations using a tailored\nensemble classification model. To improve the reliability of detection, we\nincorporate a concept drift analysis step using a One Class Drift Detector\n(OCDD) to isolate conversational shifts within flagged dialogues. When drift is\ndetected, a large language model (LLM) assesses whether the shift indicates\nfraudulent manipulation or a legitimate topic change. In cases where no drift\nis found, the behavior is inferred to be spam like. We validate our framework\nusing a dataset of social engineering chat scenarios and demonstrate its\npractical advantages in improving both accuracy and interpretability for real\ntime fraud detection. To contextualize the trade offs, we compare our modular\napproach against a Dual LLM baseline that performs detection and judgment using\ndifferent language models."}
{"id": "2505.08119", "pdf": "https://arxiv.org/pdf/2505.08119.pdf", "abs": "https://arxiv.org/abs/2505.08119", "title": "Will Your Next Pair Programming Partner Be Human? An Empirical Evaluation of Generative AI as a Collaborative Teammate in a Semester-Long Classroom Setting", "authors": ["Wenhan Lyu", "Yimeng Wang", "Yifan Sun", "Yixuan Zhang"], "categories": ["cs.HC"], "comment": "Accepted by Learning @ Scale 2025", "summary": "Generative AI (GenAI), especially Large Language Models (LLMs), is rapidly\nreshaping both programming workflows and computer science education. Many\nprogrammers now incorporate GenAI tools into their workflows, including for\ncollaborative coding tasks such as pair programming. While prior research has\ndemonstrated the benefits of traditional pair programming and begun to explore\nGenAI-assisted coding, the role of LLM-based tools as collaborators in pair\nprogramming remains underexamined. In this work, we conducted a mixed-methods\nstudy with 39 undergraduate students to examine how GenAI influences\ncollaboration, learning, and performance in pair programming. Specifically,\nstudents completed six in-class assignments under three conditions: Traditional\nPair Programming (PP), Pair Programming with GenAI (PAI), and Solo Programming\nwith GenAI (SAI). They used both LLM-based inline completion tools (e.g.,\nGitHub Copilot) and LLM-based conversational tools (e.g., ChatGPT). Our results\nshow that students in PAI achieved the highest assignment scores, whereas those\nin SAI attained the lowest. Additionally, students' attitudes toward LLMs'\nprogramming capabilities improved significantly after collaborating with\nLLM-based tools, and preferences were largely shaped by the perceived\nusefulness for completing assignments and learning programming skills, as well\nas the quality of collaboration. Our qualitative findings further reveal that\nwhile students appreciated LLM-based tools as valuable pair programming\npartners, they also identified limitations and had different expectations\ncompared to human teammates. Our study provides one of the first empirical\nevaluations of GenAI as a pair programming collaborator through a comparison of\nthree conditions (PP, PAI, and SAI). We also discuss the design implications\nand pedagogical considerations for future GenAI-assisted pair programming\napproaches."}
{"id": "2505.07853", "pdf": "https://arxiv.org/pdf/2505.07853.pdf", "abs": "https://arxiv.org/abs/2505.07853", "title": "CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis", "authors": ["Hao Zhen", "Jidong J. Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 7 figures", "summary": "Road crashes claim over 1.3 million lives annually worldwide and incur global\neconomic losses exceeding \\$1.8 trillion. Such profound societal and financial\nimpacts underscore the urgent need for road safety research that uncovers crash\nmechanisms and delivers actionable insights. Conventional statistical models\nand tree ensemble approaches typically rely on structured crash data,\noverlooking contextual nuances and struggling to capture complex relationships\nand underlying semantics. Moreover, these approaches tend to incur significant\ninformation loss, particularly in narrative elements related to multi-vehicle\ninteractions, crash progression, and rare event characteristics. This study\npresents CrashSage, a novel Large Language Model (LLM)-centered framework\ndesigned to advance crash analysis and modeling through four key innovations.\nFirst, we introduce a tabular-to-text transformation strategy paired with\nrelational data integration schema, enabling the conversion of raw,\nheterogeneous crash data into enriched, structured textual narratives that\nretain essential structural and relational context. Second, we apply\ncontext-aware data augmentation using a base LLM model to improve narrative\ncoherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B\nmodel for crash severity inference, demonstrating superior performance over\nbaseline approaches, including zero-shot, zero-shot with chain-of-thought\nprompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini,\nLLaMA3-70B). Finally, we employ a gradient-based explainability technique to\nelucidate model decisions at both the individual crash level and across broader\nrisk factor dimensions. This interpretability mechanism enhances transparency\nand enables targeted road safety interventions by providing deeper insights\ninto the most influential factors."}
{"id": "2505.08143", "pdf": "https://arxiv.org/pdf/2505.08143.pdf", "abs": "https://arxiv.org/abs/2505.08143", "title": "Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information", "authors": ["Jiawei Zhou", "Kritika Venkatachalam", "Minje Choi", "Koustuv Saha", "Munmun De Choudhury"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the wide adoption of large language models (LLMs) in information\nassistance, it is essential to examine their alignment with human communication\nstyles and values. We situate this study within the context of fact-checking\nhealth information, given the critical challenge of rectifying conceptions and\nbuilding trust. Recent studies have explored the potential of LLM for health\ncommunication, but style differences between LLMs and human experts and\nassociated reader perceptions remain under-explored. In this light, our study\nevaluates the communication styles of LLMs, focusing on how their explanations\ndiffer from those of humans in three core components of health communication:\ninformation, sender, and receiver. We compiled a dataset of 1498 health\nmisinformation explanations from authoritative fact-checking organizations and\ngenerated LLM responses to inaccurate health information. Drawing from health\ncommunication theory, we evaluate communication styles across three key\ndimensions of information linguistic features, sender persuasive strategies,\nand receiver value alignments. We further assessed human perceptions through a\nblinded evaluation with 99 participants. Our findings reveal that LLM-generated\narticles showed significantly lower scores in persuasive strategies, certainty\nexpressions, and alignment with social values and moral foundations. However,\nhuman evaluation demonstrated a strong preference for LLM content, with over\n60% responses favoring LLM articles for clarity, completeness, and\npersuasiveness. Our results suggest that LLMs' structured approach to\npresenting information may be more effective at engaging readers despite\nscoring lower on traditional measures of quality in fact-checking and health\ncommunication."}
{"id": "2505.07856", "pdf": "https://arxiv.org/pdf/2505.07856.pdf", "abs": "https://arxiv.org/abs/2505.07856", "title": "Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights", "authors": ["Paweł Walkowiak", "Marek Klonowski", "Marcin Oleksy", "Arkadiusz Janz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various techniques are used in the generation of adversarial examples,\nincluding methods such as TextBugger which introduce minor, hardly visible\nperturbations to words leading to changes in model behaviour. Another class of\ntechniques involves substituting words with their synonyms in a way that\npreserves the text's meaning but alters its predicted class, with TextFooler\nbeing a prominent example of such attacks. Most adversarial example generation\nmethods are developed and evaluated primarily on non-inflectional languages,\ntypically English. In this work, we evaluate and explain how adversarial\nattacks perform in inflectional languages. To explain the impact of inflection\non model behaviour and its robustness under attack, we designed a novel\nprotocol inspired by mechanistic interpretability, based on Edge Attribution\nPatching (EAP) method. The proposed evaluation protocol relies on parallel\ntask-specific corpora that include both inflected and syncretic variants of\ntexts in two languages -- Polish and English. To analyse the models and explain\nthe relationship between inflection and adversarial robustness, we create a new\nbenchmark based on task-oriented dataset MultiEmo, enabling the identification\nof mechanistic inflection-related elements of circuits within the model and\nanalyse their behaviour under attack."}
{"id": "2505.08312", "pdf": "https://arxiv.org/pdf/2505.08312.pdf", "abs": "https://arxiv.org/abs/2505.08312", "title": "Investigating Resolution Strategies for Workspace-Occlusion in Augmented Virtuality", "authors": ["Nico Feld", "Pauline Bimberg", "Michael Feldmann", "Matthias Wölwer", "Eike Langbehn", "Benjamin Weyers", "Daniel Zielasko"], "categories": ["cs.HC"], "comment": null, "summary": "Augmented Virtuality integrates physical content into virtual environments,\nbut the occlusion of physical by virtual content is a challenge. This unwanted\nocclusion may disrupt user interactions with physical devices and compromise\nsafety and usability. This paper investigates two resolution strategies to\naddress this issue: Redirected Walking, which subtly adjusts the user's\nmovement to maintain physical-virtual alignment, and Automatic Teleport\nRotation, which realigns the virtual environment during travel. A user study\nset in a virtual forest demonstrates that both methods effectively reduce\nocclusion. While in our testbed, Automatic Teleport Rotation achieves higher\nocclusion resolution, it is suspected to increase cybersickness compared to the\nless intrusive Redirected Walking approach."}
{"id": "2505.07857", "pdf": "https://arxiv.org/pdf/2505.07857.pdf", "abs": "https://arxiv.org/abs/2505.07857", "title": "Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines", "authors": ["Faiza Hassan", "Summra Saleem", "Kashif Javed", "Muhammad Nabeel Asim", "Abdur Rehman", "Andreas Dengel"], "categories": ["cs.CL", "cs.AI"], "comment": "42 pages, 10 figures(including 6 graphs)", "summary": "Multifarious intent detection predictors are developed for different\nlanguages, including English, Chinese and French, however, the field remains\nunderdeveloped for Urdu, the 10th most spoken language. In the realm of\nwell-known languages, intent detection predictors utilize the strategy of\nfew-shot learning and prediction of unseen classes based on the model training\non seen classes. However, Urdu language lacks few-shot strategy based intent\ndetection predictors and traditional predictors are focused on prediction of\nthe same classes which models have seen in the train set. To empower Urdu\nlanguage specific intent detection, this introduces a unique contrastive\nlearning approach that leverages unlabeled Urdu data to re-train pre-trained\nlanguage models. This re-training empowers LLMs representation learning for the\ndownstream intent detection task. Finally, it reaps the combined potential of\npre-trained LLMs and the prototype-informed attention mechanism to create a\ncomprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm\nof proposed predictive pipeline, it explores the potential of 6 distinct\nlanguage models and 13 distinct similarity computation methods. The proposed\nframework is evaluated on 2 public benchmark datasets, namely ATIS encompassing\n5836 samples and Web Queries having 8519 samples. Across ATIS dataset under\n4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and\n98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,\nrespectively. In an additional case study on the Web Queries dataset under same\nclasses train and test set settings, LLMPIA outperformed state-of-the-art\npredictor by 53.55% F1-Score."}
{"id": "2505.08360", "pdf": "https://arxiv.org/pdf/2505.08360.pdf", "abs": "https://arxiv.org/abs/2505.08360", "title": "A Comparison Between Human and Generative AI Decision-Making Attributes in Complex Health Services", "authors": ["Nandini Doreswamy", "Louise Horstmanshof"], "categories": ["cs.HC", "cs.CY", "68T01 (Primary), 91C99 (Secondary)", "J.4; K.4.1; K.4.2; J.3"], "comment": "10 pages, 2 figures, 1 table", "summary": "A comparison between human and Generative AI decision-making attributes in\ncomplex health services is a knowledge gap in the literature, at present.\nHumans may possess unique attributes beneficial to decision-making in complex\nhealth services such as health policy and health regulation, but are also\nsusceptible to decision-making flaws. The objective is to explore whether\nhumans have unique, and/or helpful attributes that contribute to optimal\ndecision-making in complex health services. This comparison may also shed light\non whether humans are likely to compete, cooperate, or converge with Generative\nAI. The comparison is based on two published reviews: a scoping review of human\nattributes [1] and a rapid review of Generative AI attributes [2]. The analysis\ncategorizes attributes by uniqueness and impact. The results are presented in\ntabular form, comparing the sets and subsets of human and Generative AI\nattributes. Humans and Generative AI decision-making attributes have\ncomplementary strengths. Cooperation between these two entities seems more\nlikely than pure competition. To maintain meaningful decision-making roles,\nhumans could develop their unique attributes, with decision-making systems\nintegrating both human and Generative AI contributions. These entities may also\nconverge, in future."}
{"id": "2505.07858", "pdf": "https://arxiv.org/pdf/2505.07858.pdf", "abs": "https://arxiv.org/abs/2505.07858", "title": "Scaling Laws for Speculative Decoding", "authors": ["Siyuan Yan", "Mo Zhu", "Guo-qing Jiang", "Jianfei Wang", "Jiaxing Chen", "Wentai Zhang", "Xiang Liao", "Xiao Cui", "Chen Zhang", "Zhuoran Song", "Ran Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "The escalating demand for efficient decoding in large language models (LLMs)\nis particularly critical for reasoning-intensive architectures like OpenAI-o3\nand DeepSeek-R1, which depend on extended chain-of-thought reasoning. This\nstudy investigates speculative decoding techniques through dense LLM\narchitectures to establish foundational insights for accelerating reasoning\ntasks. While speculative decoding methods leveraging parallel\ndraft-verification cycles have emerged as promising acceleration techniques,\nthe scaling laws governing decoding efficiency remain under-explored compared\nto conventional backbone LLMs developed through Pretraining->SFT->RLHF training\nparadigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2\nand 1.3) governing draft model acceptance rate (or decoding speed) across three\ndimensions: pretraining token volume, draft model capacity, and decoding batch\nsize. Building on these laws, we achieve Scylla, which coordinates\nmulti-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical\nvalidation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and\n0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on\nsummarization and QA tasks (Figure 2). Industrial inference engine deployments\ndemonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),\nvalidating the transformative potential of systematic scaling for efficient LLM\ninference. Code will be released later."}
{"id": "2505.08375", "pdf": "https://arxiv.org/pdf/2505.08375.pdf", "abs": "https://arxiv.org/abs/2505.08375", "title": "Human-in-the-Loop Optimization for Inclusive Design: Balancing Automation and Designer Expertise", "authors": ["Pascal Jansen"], "categories": ["cs.HC"], "comment": "Position Paper for the CHI 2025 Workshop Access InContext: Futuring\n  Accessible Prototyping Tools and Methods. April 26, 2025. Yokohama, Japan", "summary": "Accessible and inclusive design has gained increased attention in HCI, yet\npractical implementation remains challenging due to resource-intensive\nprototyping methods. Traditional approaches such as workshops, A-B tests, and\nco-design sessions struggle to capture the diverse and complex needs of users\nwith disabilities at scale. This position paper argues for an automated,\naccessible Human-in-the-Loop (HITL) design optimization process that shifts the\ndesigner's role from directly crafting prototypes to curating constraints for\nalgorithmic exploration. By pre-constraining the design space based on specific\nuser interaction needs, integrating adaptive multi-modal feedback channels, and\npersonalizing feedback prompts, the HITL approach could efficiently refine\ndesign parameters, such as text size, color contrast, layout, and interaction\nmodalities, to achieve optimal accessibility. This approach promises scalable,\nindividualized design solutions while raising critical questions about\nconstraint curation, transparency, user agency, and ethical considerations,\nmaking it essential to discuss and refine these ideas collaboratively at the\nworkshop."}
{"id": "2505.07859", "pdf": "https://arxiv.org/pdf/2505.07859.pdf", "abs": "https://arxiv.org/abs/2505.07859", "title": "Boosting Performance on ARC is a Matter of Perspective", "authors": ["Daniel Franzen", "Jan Disselhoff", "David Hartmann"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures, 5 tables", "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU)."}
{"id": "2505.08493", "pdf": "https://arxiv.org/pdf/2505.08493.pdf", "abs": "https://arxiv.org/abs/2505.08493", "title": "BizChat: Scaffolding AI-Powered Business Planning for Small Business Owners Across Digital Skill Levels", "authors": ["Quentin Romero Lauro", "Aakash Gautam", "Yasmine Kotturi"], "categories": ["cs.HC", "H.5.2"], "comment": "4 pages, 1 figure, CHIWORK '25 Adjunct, June 23-25, 2025, Amsterdam,\n  Netherlands", "summary": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment."}
{"id": "2505.07861", "pdf": "https://arxiv.org/pdf/2505.07861.pdf", "abs": "https://arxiv.org/abs/2505.07861", "title": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation", "authors": ["Harry Dong", "Bilge Acun", "Beidi Chen", "Yuejie Chi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Due to long generations, large language model (LLM) math reasoning demands\nsignificant computational resources and time. While many existing efficient\ninference methods have been developed with excellent performance preservation\non language tasks, they often severely degrade math performance. In this paper,\nwe propose Caprese, a low-cost distillation method to recover lost capabilities\nfrom deploying efficient inference methods, focused primarily in feedforward\nblocks. With original weights unperturbed, roughly 1% of additional parameters,\nand only 20K synthetic training samples, we are able to recover much if not all\nof the math capabilities lost from efficient inference for thinking LLMs and\nwithout harm to language tasks for instruct LLMs. Moreover, Caprese slashes the\nnumber of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and\nintegrates cleanly into existing model layers to reduce latency (>11% reduction\nto generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity."}
{"id": "2505.08691", "pdf": "https://arxiv.org/pdf/2505.08691.pdf", "abs": "https://arxiv.org/abs/2505.08691", "title": "VizCV: AI-assisted visualization of researchers' publications tracks", "authors": ["Vladimír Lazárik", "Marco Agus", "Barbora Kozlíková", "Pere-Pau Vázquez"], "categories": ["cs.HC", "cs.AI"], "comment": "11 pages, 9 figures. Subtmitted", "summary": "Analyzing how the publication records of scientists and research groups have\nevolved over the years is crucial for assessing their expertise since it can\nsupport the management of academic environments by assisting with career\nplanning and evaluation. We introduce VizCV, a novel web-based end-to-end\nvisual analytics framework that enables the interactive exploration of\nresearchers' scientific trajectories. It incorporates AI-assisted analysis and\nsupports automated reporting of career evolution. Our system aims to model\ncareer progression through three key dimensions: a) research topic evolution to\ndetect and visualize shifts in scholarly focus over time, b) publication record\nand the corresponding impact, c) collaboration dynamics depicting the growth\nand transformation of a researcher's co-authorship network. AI-driven insights\nprovide automated explanations of career transitions, detecting significant\nshifts in research direction, impact surges, or collaboration expansions. The\nsystem also supports comparative analysis between researchers, allowing users\nto compare topic trajectories and impact growth. Our interactive, multi-tab and\nmultiview system allows for the exploratory analysis of career milestones under\ndifferent perspectives, such as the most impactful articles, emerging research\nthemes, or obtaining a detailed analysis of the contribution of the researcher\nin a subfield. The key contributions include AI/ML techniques for: a) topic\nanalysis, b) dimensionality reduction for visualizing patterns and trends, c)\nthe interactive creation of textual descriptions of facets of data through\nconfigurable prompt generation and large language models, that include key\nindicators, to help understanding the career development of individuals or\ngroups."}
{"id": "2505.07862", "pdf": "https://arxiv.org/pdf/2505.07862.pdf", "abs": "https://arxiv.org/abs/2505.07862", "title": "Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition", "authors": ["Andrew Kiruluta", "Eric Lundy", "Priscilla Burity"], "categories": ["cs.CL"], "comment": null, "summary": "Existing sequence to sequence models for structured language tasks rely\nheavily on the dot product self attention mechanism, which incurs quadratic\ncomplexity in both computation and memory for input length N. We introduce the\nGraph Wavelet Transformer (GWT), a novel architecture that replaces this\nbottleneck with a learnable, multi scale wavelet transform defined over an\nexplicit graph Laplacian derived from syntactic or semantic parses. Our\nanalysis shows that multi scale spectral decomposition offers an interpretable,\nefficient, and expressive alternative to quadratic self attention for graph\nstructured sequence modeling."}
{"id": "2505.07884", "pdf": "https://arxiv.org/pdf/2505.07884.pdf", "abs": "https://arxiv.org/abs/2505.07884", "title": "Development of a WAZOBIA-Named Entity Recognition System", "authors": ["S. E Emedem", "I. E Onyenwe", "E. G Onyedinma"], "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "comment": "6 pages, 3 figures, 1 table", "summary": "Named Entity Recognition NER is very crucial for various natural language\nprocessing applications, including information extraction, machine translation,\nand sentiment analysis. Despite the ever-increasing interest in African\nlanguages within computational linguistics, existing NER systems focus mainly\non English, European, and a few other global languages, leaving a significant\ngap for under-resourced languages. This research presents the development of a\nWAZOBIA-NER system tailored for the three most prominent Nigerian languages:\nHausa, Yoruba, and Igbo. This research begins with a comprehensive compilation\nof annotated datasets for each language, addressing data scarcity and\nlinguistic diversity challenges. Exploring the state-of-the-art machine\nlearning technique, Conditional Random Fields (CRF) and deep learning models\nsuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder\nRepresentation from Transformers (Bert) and fine-tune with a Recurrent Neural\nNetwork (RNN), the study evaluates the effectiveness of these approaches in\nrecognizing three entities: persons, organizations, and locations. The system\nutilizes optical character recognition (OCR) technology to convert textual\nimages into machine-readable text, thereby enabling the Wazobia system to\naccept both input text and textual images for extraction purposes. The system\nachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in\nF1-score, and 0.9301 in accuracy. The model's evaluation was conducted across\nthree languages, with precision, recall, F1-score, and accuracy as key\nassessment metrics. The Wazobia-NER system demonstrates that it is feasible to\nbuild robust NER tools for under-resourced African languages using current NLP\nframeworks and transfer learning."}
{"id": "2505.07863", "pdf": "https://arxiv.org/pdf/2505.07863.pdf", "abs": "https://arxiv.org/abs/2505.07863", "title": "QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction", "authors": ["Ziliang Wang", "Xiaohong Zhang", "Ze Shi Li", "Meng Yan"], "categories": ["cs.CL"], "comment": null, "summary": "Accurate prediction of Quality of Service (QoS) metrics is fundamental for\nselecting and managing cloud based services. Traditional QoS models rely on\nmanual feature engineering and yield only point estimates, offering no insight\ninto the confidence of their predictions. In this paper, we propose QoSBERT,\nthe first framework that reformulates QoS prediction as a semantic regression\ntask based on pre trained language models. Unlike previous approaches relying\non sparse numerical features, QoSBERT automatically encodes user service\nmetadata into natural language descriptions, enabling deep semantic\nunderstanding. Furthermore, we integrate a Monte Carlo Dropout based\nuncertainty estimation module, allowing for trustworthy and risk-aware service\nquality prediction, which is crucial yet underexplored in existing QoS models.\nQoSBERT applies attentive pooling over contextualized embeddings and a\nlightweight multilayer perceptron regressor, fine tuned jointly to minimize\nabsolute error. We further exploit the resulting uncertainty estimates to\nselect high quality training samples, improving robustness in low resource\nsettings. On standard QoS benchmark datasets, QoSBERT achieves an average\nreduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and\n6.9% in MAE for throughput prediction compared to the strongest baselines,\nwhile providing well calibrated confidence intervals for robust and trustworthy\nservice quality estimation. Our approach not only advances the accuracy of\nservice quality prediction but also delivers reliable uncertainty\nquantification, paving the way for more trustworthy, data driven service\nselection and optimization."}
{"id": "2505.08083", "pdf": "https://arxiv.org/pdf/2505.08083.pdf", "abs": "https://arxiv.org/abs/2505.08083", "title": "LLMs to Support K-12 Teachers in Culturally Relevant Pedagogy: An AI Literacy Example", "authors": ["Jiayi Wang", "Ruiwei Xiao", "Xinying Hou", "Hanqi Li", "Ying Jui Tseng", "John Stamper", "Ken Koedinger"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Culturally Relevant Pedagogy (CRP) is vital in K-12 education, yet teachers\nstruggle to implement CRP into practice due to time, training, and resource\ngaps. This study explores how Large Language Models (LLMs) can address these\nbarriers by introducing CulturAIEd, an LLM tool that assists teachers in\nadapting AI literacy curricula to students' cultural contexts. Through an\nexploratory pilot with four K-12 teachers, we examined CulturAIEd's impact on\nCRP integration. Results showed CulturAIEd enhanced teachers' confidence in\nidentifying opportunities for cultural responsiveness in learning activities\nand making culturally responsive modifications to existing activities. They\nvalued CulturAIEd's streamlined integration of student demographic information,\nimmediate actionable feedback, which could result in high implementation\nefficiency. This exploration of teacher-AI collaboration highlights how LLM can\nhelp teachers include CRP components into their instructional practices\nefficiently, especially in global priorities for future-ready education, such\nas AI literacy."}
{"id": "2505.07870", "pdf": "https://arxiv.org/pdf/2505.07870.pdf", "abs": "https://arxiv.org/abs/2505.07870", "title": "Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection", "authors": ["Suavis Giramata", "Madhusudan Srinivasan", "Venkat Naidu Gudivada", "Upulee Kanewala"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in various\napplications, raising critical concerns about fairness and potential biases in\ntheir outputs. This paper explores the prioritization of metamorphic relations\n(MRs) in metamorphic testing as a strategy to efficiently detect fairness\nissues within LLMs. Given the exponential growth of possible test cases,\nexhaustive testing is impractical; therefore, prioritizing MRs based on their\neffectiveness in detecting fairness violations is crucial. We apply a sentence\ndiversity-based approach to compute and rank MRs to optimize fault detection.\nExperimental results demonstrate that our proposed prioritization approach\nimproves fault detection rates by 22% compared to random prioritization and 12%\ncompared to distance-based prioritization, while reducing the time to the first\nfailure by 15% and 8%, respectively. Furthermore, our approach performs within\n5% of fault-based prioritization in effectiveness, while significantly reducing\nthe computational cost associated with fault labeling. These results validate\nthe effectiveness of diversity-based MR prioritization in enhancing fairness\ntesting for LLMs."}
{"id": "2505.08245", "pdf": "https://arxiv.org/pdf/2505.08245.pdf", "abs": "https://arxiv.org/abs/2505.08245", "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement", "authors": ["Haoran Ye", "Jing Jin", "Yuhang Xie", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "63 pages, 482 references", "summary": "The rapid advancement of large language models (LLMs) has outpaced\ntraditional evaluation methodologies. It presents novel challenges, such as\nmeasuring human-like psychological constructs, navigating beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with Psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This survey introduces and synthesizes an emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. We systematically explore the role of Psychometrics in shaping\nbenchmarking principles, broadening evaluation scopes, refining methodologies,\nvalidating results, and advancing LLM capabilities. This paper integrates\ndiverse perspectives to provide a structured framework for researchers across\ndisciplines, enabling a more comprehensive understanding of this nascent field.\nUltimately, we aim to provide actionable insights for developing future\nevaluation paradigms that align with human-level AI and promote the advancement\nof human-centered AI systems for societal benefit. A curated repository of LLM\npsychometric resources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics."}
{"id": "2505.07871", "pdf": "https://arxiv.org/pdf/2505.07871.pdf", "abs": "https://arxiv.org/abs/2505.07871", "title": "Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy", "authors": ["A M Muntasir Rahman", "Ajim Uddin", "Guiling \"Grace\" Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Financial sentiment analysis (FSA) presents unique challenges to LLMs that\nsurpass those in typical sentiment analysis due to the nuanced language used in\nfinancial contexts. The prowess of these models is often undermined by the\ninherent subjectivity of sentiment classifications in existing benchmark\ndatasets like Financial Phrasebank. These datasets typically feature undefined\nsentiment classes that reflect the highly individualized perspectives of\nannotators, leading to significant variability in annotations. This variability\nresults in an unfair expectation for LLMs during benchmarking, where they are\ntasked to conjecture the subjective viewpoints of human annotators without\nsufficient context. In this paper, we introduce the Annotators' Instruction\nAssisted Prompt, a novel evaluation prompt designed to redefine the task\ndefinition of FSA for LLMs. By integrating detailed task instructions\noriginally intended for human annotators into the LLMs' prompt framework, AIAP\naims to standardize the understanding of sentiment across both human and\nmachine interpretations, providing a fair and context-rich foundation for\nsentiment analysis. We utilize a new dataset, WSBS, derived from the\nWallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM\nperformance by aligning machine operations with the refined task definitions.\nExperimental results demonstrate that AIAP enhances LLM performance\nsignificantly, with improvements up to 9.08. This context-aware approach not\nonly yields incremental gains in performance but also introduces an innovative\nsentiment-indexing method utilizing model confidence scores. This method\nenhances stock price prediction models and extracts more value from the\nfinancial sentiment analysis, underscoring the significance of WSB as a\ncritical source of financial text. Our research offers insights into both\nimproving FSA through better evaluation methods."}
{"id": "2505.08515", "pdf": "https://arxiv.org/pdf/2505.08515.pdf", "abs": "https://arxiv.org/abs/2505.08515", "title": "CoVoL: A Cooperative Vocabulary Learning Game for Children with Autism", "authors": ["Pawel Chodkiewicz", "Pragya Verma", "Grischa Liebel"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Children with Autism commonly face difficulties in vocabulary acquisition,\nwhich can have an impact on their social communication. Using digital tools for\nvocabulary learning can prove beneficial for these children, as they can\nprovide a predictable environment and effective individualized feedback. While\nexisting work has explored the use of technology-assisted vocabulary learning\nfor children with Autism, no study has incorporated turn-taking to facilitate\nlearning and use of vocabulary similar to that used in real-world social\ncontexts. To address this gap, we propose the design of a cooperative\ntwo-player vocabulary learning game, CoVoL. CoVoL allows children to engage in\ngame-based vocabulary learning useful for real-world social communication\nscenarios. We discuss our first prototype and its evaluation. Additionally, we\npresent planned features which are based on feedback obtained through ten\ninterviews with researchers and therapists, as well as an evaluation plan for\nthe final release of CoVoL."}
{"id": "2505.07874", "pdf": "https://arxiv.org/pdf/2505.07874.pdf", "abs": "https://arxiv.org/abs/2505.07874", "title": "The Sound of Populism: Distinct Linguistic Features Across Populist Variants", "authors": ["Yu Wang", "Runxi Yu", "Zhongyuan Wang", "Jing He"], "categories": ["cs.CL"], "comment": null, "summary": "This study explores the sound of populism by integrating the classic\nLinguistic Inquiry and Word Count (LIWC) features, which capture the emotional\nand stylistic tones of language, with a fine-tuned RoBERTa model, a\nstate-of-the-art context-aware language model trained to detect nuanced\nexpressions of populism. This approach allows us to uncover the auditory\ndimensions of political rhetoric in U.S. presidential inaugural and State of\nthe Union addresses. We examine how four key populist dimensions (i.e.,\nleft-wing, right-wing, anti-elitism, and people-centrism) manifest in the\nlinguistic markers of speech, drawing attention to both commonalities and\ndistinct tonal shifts across these variants. Our findings reveal that populist\nrhetoric consistently features a direct, assertive ``sound\" that forges a\nconnection with ``the people'' and constructs a charismatic leadership persona.\nHowever, this sound is not simply informal but strategically calibrated.\nNotably, right-wing populism and people-centrism exhibit a more emotionally\ncharged discourse, resonating with themes of identity, grievance, and crisis,\nin contrast to the relatively restrained emotional tones of left-wing and\nanti-elitist expressions."}
{"id": "2505.08588", "pdf": "https://arxiv.org/pdf/2505.08588.pdf", "abs": "https://arxiv.org/abs/2505.08588", "title": "Small but Significant: On the Promise of Small Language Models for Accessible AIED", "authors": ["Yumou Wei", "Paulo Carvalho", "John Stamper"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "This vision paper advocates using small language models (e.g., Phi-2)\n  in AI for education (AIED)", "summary": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches."}
{"id": "2505.07883", "pdf": "https://arxiv.org/pdf/2505.07883.pdf", "abs": "https://arxiv.org/abs/2505.07883", "title": "Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rational decision-making under uncertainty requires coherent degrees of\nbelief in events. However, event probabilities generated by Large Language\nModels (LLMs) have been shown to exhibit incoherence, violating the axioms of\nprobability theory. This raises the question of whether coherent event\nprobabilities can be recovered from the embeddings used by the models. If so,\nthose derived probabilities could be used as more accurate estimates in events\ninvolving uncertainty. To explore this question, we propose enforcing axiomatic\nconstraints, such as the additive rule of probability theory, in the latent\nspace learned by an extended variational autoencoder (VAE) applied to LLM\nembeddings. This approach enables event probabilities to naturally emerge in\nthe latent space as the VAE learns to both reconstruct the original embeddings\nand predict the embeddings of semantically related events. We evaluate our\nmethod on complementary events (i.e., event A and its complement, event not-A),\nwhere the true probabilities of the two events must sum to 1. Experiment\nresults on open-weight language models demonstrate that probabilities recovered\nfrom embeddings exhibit greater coherence than those directly reported by the\ncorresponding models and align closely with the true probabilities."}
{"id": "2505.08628", "pdf": "https://arxiv.org/pdf/2505.08628.pdf", "abs": "https://arxiv.org/abs/2505.08628", "title": "Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach", "authors": ["Yichen Zhao", "Yuhua Wang", "Xi Cheng", "Junhao Fang", "Yang Yang"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Metabolic syndrome (MetS) is a medication condition characterized by\nabdominal obesity, insulin resistance, hypertension and hyperlipidemia. It\nincreases the risk of majority of chronic diseases, including type 2 diabetes\nmellitus, and affects about one quarter of the global population. Therefore,\nearly detection and timely intervention for MetS are crucial. Standard\ndiagnosis for MetS components requires blood tests conducted within medical\ninstitutions. However, it is frequently underestimated, leading to unmet need\nfor care for MetS population. This study aims to use the least physiological\ndata and free texts about exercises related activities, which are obtained\neasily in daily life, to diagnosis MetS. We collected the data from 40\nvolunteers in a nursing home and used data augmentation to reduce the\nimbalance. We propose a deep learning framework for classifying MetS that\nintegrates natural language processing (NLP) and exercise monitoring. The\nresults showed that the best model reported a high positive result (AUROC=0.806\nand REC=76.3%) through 3-fold cross-validation. Feature importance analysis\nrevealed that text and minimum heart rate on a daily basis contribute the most\nin the classification of MetS. This study demonstrates the potential\napplication of data that are easily measurable in daily life for the early\ndiagnosis of MetS, which could contribute to reducing the cost of screening and\nmanagement for MetS population."}
{"id": "2505.07884", "pdf": "https://arxiv.org/pdf/2505.07884.pdf", "abs": "https://arxiv.org/abs/2505.07884", "title": "Development of a WAZOBIA-Named Entity Recognition System", "authors": ["S. E Emedem", "I. E Onyenwe", "E. G Onyedinma"], "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "comment": "6 pages, 3 figures, 1 table", "summary": "Named Entity Recognition NER is very crucial for various natural language\nprocessing applications, including information extraction, machine translation,\nand sentiment analysis. Despite the ever-increasing interest in African\nlanguages within computational linguistics, existing NER systems focus mainly\non English, European, and a few other global languages, leaving a significant\ngap for under-resourced languages. This research presents the development of a\nWAZOBIA-NER system tailored for the three most prominent Nigerian languages:\nHausa, Yoruba, and Igbo. This research begins with a comprehensive compilation\nof annotated datasets for each language, addressing data scarcity and\nlinguistic diversity challenges. Exploring the state-of-the-art machine\nlearning technique, Conditional Random Fields (CRF) and deep learning models\nsuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder\nRepresentation from Transformers (Bert) and fine-tune with a Recurrent Neural\nNetwork (RNN), the study evaluates the effectiveness of these approaches in\nrecognizing three entities: persons, organizations, and locations. The system\nutilizes optical character recognition (OCR) technology to convert textual\nimages into machine-readable text, thereby enabling the Wazobia system to\naccept both input text and textual images for extraction purposes. The system\nachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in\nF1-score, and 0.9301 in accuracy. The model's evaluation was conducted across\nthree languages, with precision, recall, F1-score, and accuracy as key\nassessment metrics. The Wazobia-NER system demonstrates that it is feasible to\nbuild robust NER tools for under-resourced African languages using current NLP\nframeworks and transfer learning."}
{"id": "2505.08648", "pdf": "https://arxiv.org/pdf/2505.08648.pdf", "abs": "https://arxiv.org/abs/2505.08648", "title": "Enhancing Software Development with Context-Aware Conversational Agents: A User Study on Developer Interactions with Chatbots", "authors": ["Glaucia Melo", "Paulo Alencar", "Donald Cowan"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Software development is a cognitively intensive process requiring\nmultitasking, adherence to evolving workflows, and continuous learning. With\nthe rise of large language model (LLM)-based tools, such as conversational\nagents (CAs), there is growing interest in supporting developers through\nnatural language interaction. However, little is known about the specific\nfeatures developers seek in these systems. We conducted a user study with 29\ndevelopers using a prototype text-based chatbot to investigate preferred\nfunctionalities. Our findings reveal strong interest in task automation,\nversion control support, and contextual adaptability, especially the need to\ntailor assistance for both novice and experienced users. We highlight the\nimportance of deep contextual understanding, historical interaction awareness,\nand personalized support in CA design. This study contributes to the\ndevelopment of context-aware chatbots that enhance productivity and\nsatisfaction, and it outlines opportunities for future research on human-AI\ncollaboration in software engineering."}
{"id": "2505.07886", "pdf": "https://arxiv.org/pdf/2505.07886.pdf", "abs": "https://arxiv.org/abs/2505.07886", "title": "PLHF: Prompt Optimization with Few-Shot Human Feedback", "authors": ["Chun-Pai Yang", "Kan Zheng", "Shou-De Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic prompt optimization frameworks are developed to obtain suitable\nprompts for large language models (LLMs) with respect to desired output quality\nmetrics. Although existing approaches can handle conventional tasks such as\nfixed-solution question answering, defining the metric becomes complicated when\nthe output quality cannot be easily assessed by comparisons with standard\ngolden samples. Consequently, optimizing the prompts effectively and\nefficiently without a clear metric becomes a critical challenge. To address the\nissue, we present PLHF (which stands for \"P\"rompt \"L\"earning with \"H\"uman\n\"F\"eedback), a few-shot prompt optimization framework inspired by the\nwell-known RLHF technique. Different from naive strategies, PLHF employs a\nspecific evaluator module acting as the metric to estimate the output quality.\nPLHF requires only a single round of human feedback to complete the entire\nprompt optimization process. Empirical results on both public and industrial\ndatasets show that PLHF outperforms prior output grading strategies for LLM\nprompt optimizations."}
{"id": "2505.08666", "pdf": "https://arxiv.org/pdf/2505.08666.pdf", "abs": "https://arxiv.org/abs/2505.08666", "title": "Claycode: Stylable and Deformable 2D Scannable Codes", "authors": ["Marco Maida", "Alberto Crescini", "Marco Perronet", "Elena Camuffo"], "categories": ["cs.GR", "cs.CG", "cs.CV", "cs.HC", "I.3.0; I.3.5; I.3.6; E.4"], "comment": null, "summary": "This paper introduces Claycode, a novel 2D scannable code designed for\nextensive stylization and deformation. Unlike traditional matrix-based codes\n(e.g., QR codes), Claycodes encode their message in a tree structure. During\nthe encoding process, bits are mapped into a topology tree, which is then\ndepicted as a nesting of color regions drawn within the boundaries of a target\npolygon shape. When decoding, Claycodes are extracted and interpreted in\nreal-time from a camera stream. We detail the end-to-end pipeline and show that\nClaycodes allow for extensive stylization without compromising their\nfunctionality. We then empirically demonstrate Claycode's high tolerance to\nheavy deformations, outperforming traditional 2D scannable codes in scenarios\nwhere they typically fail."}
{"id": "2505.07888", "pdf": "https://arxiv.org/pdf/2505.07888.pdf", "abs": "https://arxiv.org/abs/2505.07888", "title": "Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping", "authors": ["Yusen Wu", "Xiaotie Deng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the challenge in long-text style transfer using\nzero-shot learning of large language models (LLMs), proposing a hierarchical\nframework that combines sentence-level stylistic adaptation with\nparagraph-level structural coherence. We argue that in the process of effective\nparagraph-style transfer, to preserve the consistency of original syntactic and\nsemantic information, it is essential to perform style transfer not only at the\nsentence level but also to incorporate paragraph-level semantic considerations,\nwhile ensuring structural coherence across inter-sentential relationships. Our\nproposed framework, ZeroStylus, operates through two systematic phases:\nhierarchical template acquisition from reference texts and template-guided\ngeneration with multi-granular matching. The framework dynamically constructs\nsentence and paragraph template repositories, enabling context-aware\ntransformations while preserving inter-sentence logical relationships.\nExperimental evaluations demonstrate significant improvements over baseline\nmethods, with structured rewriting achieving 6.90 average score compared to\n6.70 for direct prompting approaches in tri-axial metrics assessing style\nconsistency, content preservation, and expression quality. Ablation studies\nvalidate the necessity of both template hierarchies during style transfer,\nshowing higher content preservation win rate against sentence-only approaches\nthrough paragraph-level structural encoding, as well as direct prompting method\nthrough sentence-level pattern extraction and matching. The results establish\nnew capabilities for coherent long-text style transfer without requiring\nparallel corpora or LLM fine-tuning."}
{"id": "2407.03808", "pdf": "https://arxiv.org/pdf/2407.03808.pdf", "abs": "https://arxiv.org/abs/2407.03808", "title": "Designing Value-Centered Consent Interfaces: A Mixed-Methods Approach to Support Patient Values in Data-Sharing Decisions", "authors": ["David Leimstädtner", "Peter Sörries", "Claudia Müller-Birn"], "categories": ["cs.HC"], "comment": "30 pages", "summary": "In the digital health domain, ethical data collection practices are crucial\nfor ensuring the availability of quality datasets that drive medical\nadvancement. Data donation, allowing patients to share their medical data for\nsecondary research purposes, presents a promising resource for such datasets.\nYet, current consent user interfaces mediating data-sharing decisions are found\nto favor data collectors' values over those of data subjects. This raises\nethical concerns about the use of data collected, as well as concerning the\nquality of the resulting datasets. Seeking to establish value-centered data\ncollection practices in digital health, we investigate the design of consent\nuser interfaces that support end-users in making value-congruent health\ndata-sharing decisions. Focusing our research efforts on the situated context\nof health data donation at the psychosomatic unit of a university hospital, we\ndemonstrate how a human-centered design can ground technology within the\nperspective of a vulnerable group. We employed an exploratory sequential\nmixed-method approach consisting of five phases: Participatory workshops elicit\npatient values, informing the design of a proposed Value-Centered Consent\nInterface. An online experiment demonstrates our interface element's effect,\nincreasing value congruence in data-sharing decisions. Our proposed consent\nuser interface design is then adapted to the research context through a\nco-creation workshop with domain experts and a user interface evaluation with\npatients. Our work contributes to recent discourse in CSCW concerning ethical\nimplications of new data practices within their socio-technological context by\nexploring patient values on medical data-sharing, introducing a novel consent\ninterface leveraging reflection to support value-congruent decision-making, and\nproviding a situated evaluation of the proposed consent user interface with\npatients."}
{"id": "2505.07889", "pdf": "https://arxiv.org/pdf/2505.07889.pdf", "abs": "https://arxiv.org/abs/2505.07889", "title": "BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning", "authors": ["Yuyang Liu", "Liuzhenghao Lv", "Xiancheng Zhang", "Li Yuan", "Yonghong Tian"], "categories": ["cs.CL"], "comment": null, "summary": "Biological protocols are fundamental to reproducible and safe life science\nresearch. While LLMs excel on general tasks, their systematic evaluation on\nthese highly specialized, accuracy-critical, and inherently procedural texts\nremains limited. In this work, we present BioProBench, the first large-scale,\nintegrated multi-task benchmark for biological protocol understanding and\nreasoning. While limited benchmarks have touched upon specific aspects like\nprotocol QA, BioProBench provides a comprehensive suite of five core tasks:\nProtocol Question Answering, Step Ordering, Error Correction, Protocol\nGeneration, and Protocol Reasoning, enabling a holistic evaluation of LLMs on\nprocedural biological texts. Built upon 27K original protocols, it yields\nnearly 556K high-quality structured instances. We evaluate 12 mainstream\nopen/closed-source LLMs on BioProBench. Experimental results reveal that while\ntop models preform well on surface understanding tasks, struggle significantly\nwith deep reasoning and structured generation tasks like ordering and\ngeneration. Furthermore, model comparisons reveal diverse performance: certain\nopen-source models approach closed-source levels on some tasks, yet\nbio-specific small models lag behind general LLMs, indicating limitations on\ncomplex procedural content. Overall, our findings underscore that procedural\nreasoning within biological protocols represents a significant challenge for\ncurrent LLMs. BioProBench serves as a standardized framework to diagnose these\nspecific limitations and guide the development of AI systems better equipped\nfor safely automating complex scientific procedures. The code and data are\navailable at: https://github.com/YuyangSunshine/bioprotocolbench and\nhttps://huggingface.co/datasets/GreatCaptainNemo/BioProBench."}
{"id": "2409.16732", "pdf": "https://arxiv.org/pdf/2409.16732.pdf", "abs": "https://arxiv.org/abs/2409.16732", "title": "Perfectly to a Tee: Understanding User Perceptions of Personalized LLM-Enhanced Narrative Interventions", "authors": ["Ananya Bhattacharjee", "Sarah Yi Xu", "Pranav Rao", "Yuchen Zeng", "Jonah Meyerhoff", "Syed Ishtiaque Ahmed", "David C Mohr", "Michael Liut", "Alex Mariakakis", "Rachel Kornfield", "Joseph Jay Williams"], "categories": ["cs.HC"], "comment": null, "summary": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that personalized\nLLM-enhanced stories were perceived to be better than human-written ones in\nconveying key takeaways, promoting reflection, and reducing belief in negative\nthoughts. These stories were not only seen as more relatable but also similarly\nauthentic to human-written ones, highlighting the potential of LLMs in helping\nyoung adults manage their struggles. The findings of this work provide crucial\ndesign considerations for future narrative-based digital mental health\ninterventions, such as the need to maintain relatability without veering into\nimplausibility and refining the wording and tone of AI-enhanced content."}
{"id": "2505.07890", "pdf": "https://arxiv.org/pdf/2505.07890.pdf", "abs": "https://arxiv.org/abs/2505.07890", "title": "TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks", "authors": ["Kutay Ertürk", "Furkan Altınışık", "İrem Sarıaltın", "Ömer Nezih Gerek"], "categories": ["cs.CL", "eess.IV"], "comment": null, "summary": "This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information.\n  Our approach revisits sign language recognition as sequence-to-sequence\ntranslation, inspired by the linguistic nature of sign languages and the\nsuccess of transformers in natural language processing. Since TSLFormer uses\nthe self-attention mechanism, it effectively captures temporal co-occurrence\nwithin gesture sequences and highlights meaningful motion patterns as words\nunfold.\n  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different\nwords, TSLFormer achieves competitive performance with minimal computational\ncost. These results show that joint-based input is sufficient for enabling\nreal-time, mobile, and assistive communication systems for hearing-impaired\nindividuals."}
{"id": "2412.14190", "pdf": "https://arxiv.org/pdf/2412.14190.pdf", "abs": "https://arxiv.org/abs/2412.14190", "title": "Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships", "authors": ["Julian De Freitas", "Noah Castelo", "Ahmet Uguralp", "Zeliha Uguralp"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Can consumers form especially deep emotional bonds with AI and be vested in\nAI identities over time? We leverage a natural app-update event at Replika AI,\na popular US-based AI companion, to shed light on these questions. We find\nthat, after the app removed its erotic role play (ERP) feature, preventing\nintimate interactions between consumers and chatbots that were previously\npossible, this event triggered perceptions in customers that their AI\ncompanion's identity had discontinued. This in turn predicted negative consumer\nwelfare and marketing outcomes related to loss, including mourning the loss,\nand devaluing the \"new\" AI relative to the \"original\". Experimental evidence\nconfirms these findings. Further experiments find that AI companions users feel\ncloser to their AI companion than even their best human friend, and mourn a\nloss of their AI companion more than a loss of various other inanimate\nproducts. In short, consumers are forming human-level relationships with AI\ncompanions; disruptions to these relationships trigger real patterns of\nmourning as well as devaluation of the offering; and the degree of mourning and\ndevaluation are explained by perceived discontinuity in the AIs identity. Our\nresults illustrate that relationships with AI are truly personal, creating\nunique benefits and risks for consumers and firms alike."}
{"id": "2505.07891", "pdf": "https://arxiv.org/pdf/2505.07891.pdf", "abs": "https://arxiv.org/abs/2505.07891", "title": "TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking", "authors": ["Ching Nam Hang", "Pei-Duo Yu", "Chee Wei Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the age of social media, the rapid spread of misinformation and rumors has\nled to the emergence of infodemics, where false information poses a significant\nthreat to society. To combat this issue, we introduce TrumorGPT , a novel\ngenerative artificial intelligence solution designed for fact-checking in the\nhealth domain. TrumorGPT aims to distinguish \"trumors\", which are\nhealth-related rumors that turn out to be true, providing a crucial tool in\ndifferentiating between mere speculation and verified facts. This framework\nleverages a large language model (LLM) with few-shot learning for semantic\nhealth knowledge graph construction and semantic reasoning. TrumorGPT\nincorporates graph-based retrieval-augmented generation (GraphRAG) to address\nthe hallucination issue common in LLMs and the limitations of static training\ndata. GraphRAG involves accessing and utilizing information from regularly\nupdated semantic health knowledge graphs that consist of the latest medical\nnews and health information, ensuring that fact-checking by TrumorGPT is based\non the most recent data. Evaluating with extensive healthcare datasets,\nTrumorGPT demonstrates superior performance in fact-checking for public health\nclaims. Its ability to effectively conduct fact-checking across various\nplatforms marks a critical step forward in the fight against health-related\nmisinformation, enhancing trust and accuracy in the digital information age."}
{"id": "2412.16766", "pdf": "https://arxiv.org/pdf/2412.16766.pdf", "abs": "https://arxiv.org/abs/2412.16766", "title": "A Protocol for KG Construction Tasks Involving Users", "authors": ["Ademar Crotti Junior", "Christophe Debruyne"], "categories": ["cs.HC", "cs.AI", "cs.DB"], "comment": "For associated repository, see\n  https://github.com/chrdebru/kgc-user-study-protocol", "summary": "Knowledge graph construction (KGC) from (semi-)structured data is\nchallenging, and facilitating user involvement is an issue frequently brought\nup within this community. We cannot deny the progress we have made with respect\nto (declarative) knowledge graph construction languages and tools to help build\nsuch mappings. However, it is surprising that no two studies report on similar\nprotocols. This heterogeneity does not allow for comparing KGC languages,\ntechniques, and tools. This paper first analyses studies involving users to\nidentify the points of comparison. These gaps include a lack of systematic\nconsistency in task design, participant selection, and evaluation metrics.\nMoreover, there needs to be a systematic way of analyzing the data and\nreporting the findings, which is also lacking. We thus propose and introduce a\nuser protocol for KGC designed to address this challenge. Where possible, we\ndraw and take elements from the literature we deem fit for such a protocol. The\nprotocol, as such, allows for the comparison of languages and techniques for\nthe RDF Mapping Language (RML) core functionality, which is covered by most of\nthe other state-of-the-art techniques and tools. We also propose how the\nprotocol can be amended to compare extensions (of RML). This protocol provides\nan important step towards a more comparable evaluation of KGC user studies."}
{"id": "2505.07897", "pdf": "https://arxiv.org/pdf/2505.07897.pdf", "abs": "https://arxiv.org/abs/2505.07897", "title": "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows", "authors": ["Stefano Rando", "Luca Romani", "Alessio Sampieri", "Yuta Kyuragi", "Luca Franco", "Fabio Galasso", "Tatsunori Hashimoto", "John Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Context lengths for models have grown rapidly, from thousands to millions of\ntokens in just a few years. The extreme context sizes of modern long-context\nmodels have made it difficult to construct realistic long-context benchmarks --\nnot only due to the cost of collecting million-context tasks but also in\nidentifying realistic scenarios that require significant contexts. We identify\ncode comprehension and repair as a natural testbed and challenge task for\nlong-context models and introduce LongCodeBench (LCB), a benchmark to test LLM\ncoding abilities in long-context scenarios. Our benchmark tests both the\ncomprehension and repair capabilities of LCLMs in realistic and important\nsettings by drawing from real-world GitHub issues and constructing QA\n(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the\ncomplexity of our benchmark, enabling us to evaluate models across different\nscales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.\nWe find that long-context remains a weakness for all models, with performance\ndrops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for\nQwen2.5."}
{"id": "2501.18493", "pdf": "https://arxiv.org/pdf/2501.18493.pdf", "abs": "https://arxiv.org/abs/2501.18493", "title": "Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline", "authors": ["Shivani Kapania", "Stephanie Ballard", "Alex Kessler", "Jennifer Wortman Vaughan"], "categories": ["cs.HC"], "comment": null, "summary": "Alongside the growth of generative AI, we are witnessing a surge in the use\nof synthetic data across all stages of the AI development pipeline. It is now\ncommon practice for researchers and practitioners to use one large generative\nmodel (which we refer to as an auxiliary model) to generate synthetic data that\nis used to train or evaluate another, reconfiguring AI workflows and reshaping\nthe very nature of data. While scholars have raised concerns over the risks of\nsynthetic data, policy guidance and best practices for its responsible use have\nnot kept up with these rapidly evolving industry trends, in part because we\nlack a clear picture of current practices and challenges. Our work aims to\naddress this gap. Through 29 interviews with AI practitioners and responsible\nAI experts, we examine the expanding role of synthetic data in AI development.\nOur findings reveal how auxiliary models are now widely used across the AI\ndevelopment pipeline. Practitioners describe synthetic data as crucial for\naddressing data scarcity and providing a competitive edge, noting that\nevaluation of generative AI systems at scale would be infeasible without\nauxiliary models. However, they face challenges controlling the outputs of\nauxiliary models, generating data that accurately depict underrepresented\ngroups, and scaling data validation practices that are based primarily on\nmanual inspection. We detail general limitations of and ethical considerations\nfor synthetic data and conclude with a proposal of concrete steps towards the\ndevelopment of best practices for its responsible use."}
{"id": "2505.07899", "pdf": "https://arxiv.org/pdf/2505.07899.pdf", "abs": "https://arxiv.org/abs/2505.07899", "title": "DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise", "authors": ["Ding Cao", "Yuchen Cai", "Rongxi Guo", "Xuesong He", "Guiquan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sequential knowledge editing techniques aim to continuously update the\nknowledge in large language models at a low cost, preventing the models from\ngenerating outdated or incorrect information. However, existing sequential\nediting methods suffer from a significant decline in editing success rates\nafter long-term editing. Through theoretical analysis and experiments, we\nidentify that as the number of edits increases, the model's output increasingly\ndeviates from the desired target, leading to a drop in editing success rates.\nWe refer to this issue as the accumulation of superimposed noise problem. To\naddress this, we identify the factors contributing to this deviation and\npropose DeltaEdit, a novel method that optimizes update parameters through a\ndynamic orthogonal constraints strategy, effectively reducing interference\nbetween edits to mitigate deviation. Experimental results demonstrate that\nDeltaEdit significantly outperforms existing methods in edit success rates and\nthe retention of generalization capabilities, ensuring stable and reliable\nmodel performance even under extensive sequential editing."}
{"id": "2504.20369", "pdf": "https://arxiv.org/pdf/2504.20369.pdf", "abs": "https://arxiv.org/abs/2504.20369", "title": "Perception-aware Sampling for Scatterplot Visualizations", "authors": ["Zafeiria Moumoulidou", "Hamza Elhamdadi", "Ke Yang", "Subrata Mitra", "Cindy Xiong Bearfield", "Alexandra Meliou"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Visualizing data is often a crucial first step in data analytics workflows,\nbut growing data sizes pose challenges due to computational and visual\nperception limitations. As a result, data analysts commonly down-sample their\ndata and work with subsets. Deriving representative samples, however, remains a\nchallenge. This paper focuses on scatterplots, a widely-used visualization\ntype, and introduces a novel sampling objective -- perception-awareness --\naiming to improve sample efficacy by targeting humans' perception of a\nvisualization.\n  We make the following contributions: (1) We propose perception-augmented\ndatabases and design PAwS: a novel perception-aware sampling method for\nscatterplots that leverages saliency maps -- a computer vision tool for\npredicting areas of attention focus in visualizations -- and models\nperception-awareness via saliency, density, and coverage objectives. (2) We\ndesign ApproPAwS: a fast, perception-aware method for approximate\nvisualizations, which exploits the fact that small visual perturbations are\noften imperceptible to humans. (3) We introduce the concept of perceptual\nsimilarity as a metric for sample quality, and present a novel method that\ncompares saliency maps to measure it. (4) Our extensive experimental evaluation\nshows that our methods consistently outperform prior art in producing samples\nwith high perceptual similarity, while ApproPAwS achieves up to 100x speed-ups\nwith minimal loss in visual fidelity. Our user study shows that PAwS is often\npreferred by humans, validating our quantitative findings."}
{"id": "2505.07903", "pdf": "https://arxiv.org/pdf/2505.07903.pdf", "abs": "https://arxiv.org/abs/2505.07903", "title": "SEM: Reinforcement Learning for Search-Efficient Large Language Models", "authors": ["Zeyang Sha", "Shiwen Cui", "Weiqiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models(LLMs) have demonstrated their\ncapabilities not only in reasoning but also in invoking external tools,\nparticularly search engines. However, teaching models to discern when to invoke\nsearch and when to rely on their internal knowledge remains a significant\nchallenge. Existing reinforcement learning approaches often lead to redundant\nsearch behaviors, resulting in inefficiencies and over-cost. In this paper, we\npropose SEM, a novel post-training reinforcement learning framework that\nexplicitly trains LLMs to optimize search usage. By constructing a balanced\ndataset combining MuSiQue and MMLU, we create scenarios where the model must\nlearn to distinguish between questions it can answer directly and those\nrequiring external retrieval. We design a structured reasoning template and\nemploy Group Relative Policy Optimization(GRPO) to post-train the model's\nsearch behaviors. Our reward function encourages accurate answering without\nunnecessary search while promoting effective retrieval when needed.\nExperimental results demonstrate that our method significantly reduces\nredundant search operations while maintaining or improving answer accuracy\nacross multiple challenging benchmarks. This framework advances the model's\nreasoning efficiency and extends its capability to judiciously leverage\nexternal knowledge."}
{"id": "2505.07142", "pdf": "https://arxiv.org/pdf/2505.07142.pdf", "abs": "https://arxiv.org/abs/2505.07142", "title": "Exploring Anthropomorphism in Conversational Agents for Environmental Sustainability", "authors": ["Mathyas Giudici", "Samuele Scherini", "Pascal Chaussumier", "Stefano Ginocchio", "Franca Garzotto"], "categories": ["cs.HC"], "comment": null, "summary": "The paper investigates the integration of Large Language Models (LLMs) into\nConversational Agents (CAs) to encourage a shift in consumption patterns from a\ndemand-driven to a supply-based paradigm. Specifically, the research examines\nthe role of anthropomorphic design in delivering environmentally conscious\nmessages by comparing two CA designs: a personified agent representing an\nappliance and a traditional, non-personified assistant. A lab study (N=26)\nassessed the impact of these designs on interaction, perceived self-efficacy,\nand engagement. Results indicate that LLM-based CAs significantly enhance\nusers' self-reported eco-friendly behaviors, with participants expressing\ngreater confidence in managing energy consumption. While the anthropomorphic\ndesign did not notably affect self-efficacy, those interacting with the\npersonified agent reported a stronger sense of connection with the system.\nThese findings suggest that although anthropomorphic CAs may improve user\nengagement, both designs hold promise for fostering sustainable behaviors in\nhome energy management."}
{"id": "2505.07920", "pdf": "https://arxiv.org/pdf/2505.07920.pdf", "abs": "https://arxiv.org/abs/2505.07920", "title": "Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions", "authors": ["Daoze Zhang", "Zhijian Bao", "Sihang Du", "Zhiyi Zhao", "Kuangling Zhang", "Dezheng Bao", "Yang Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2 figures, 5 tables", "summary": "Peer review is a critical component of scientific progress in the fields like\nAI, but the rapid increase in submission volume has strained the reviewing\nsystem, which inevitably leads to reviewer shortages and declines review\nquality. Besides the growing research popularity, another key factor in this\noverload is the repeated resubmission of substandard manuscripts, largely due\nto the lack of effective tools for authors to self-evaluate their work before\nsubmission. Large Language Models (LLMs) show great promise in assisting both\nauthors and reviewers, and their performance is fundamentally limited by the\nquality of the peer review data. However, existing peer review datasets face\nthree major limitations: (1) limited data diversity, (2) inconsistent and\nlow-quality data due to the use of revised rather than initial submissions, and\n(3) insufficient support for tasks involving rebuttal and reviewer-author\ninteractions. To address these challenges, we introduce the largest\nconsistency-ensured peer review and rebuttal dataset named Re^2, which\ncomprises 19,926 initial submissions, 70,668 review comments, and 53,818\nrebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the\nrebuttal and discussion stage is framed as a multi-turn conversation paradigm\nto support both traditional static review tasks and dynamic interactive LLM\nassistants, providing more practical guidance for authors to refine their\nmanuscripts and helping alleviate the growing review burden. Our data and code\nare available in https://anonymous.4open.science/r/ReviewBench_anon/."}
{"id": "2505.07736", "pdf": "https://arxiv.org/pdf/2505.07736.pdf", "abs": "https://arxiv.org/abs/2505.07736", "title": "VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections", "authors": ["Eason Chen", "Xinyi Tang", "Aprille Xi", "Chenyu Lin", "Conrad Borchers", "Shivang Gupta", "Jionghao Lin", "Kenneth R Koedinger"], "categories": ["cs.HC"], "comment": "6 pages", "summary": "Hybrid tutoring, where a human tutor supports multiple students in learning\nwith educational technology, is an increasingly common application to deliver\nhigh-impact tutoring at scale. However, past hybrid tutoring applications are\nlimited in guiding tutor attention to students that require support.\nSpecifically, existing conferencing tools, commonly used in hybrid tutoring, do\nnot allow tutors to monitor multiple students' screens while directly\ncommunicating and attending to multiple students simultaneously. To address\nthis issue, this paper introduces VTutor, a web-based platform leveraging\npeer-to-peer screen sharing and virtual avatars to deliver real-time,\ncontext-aware tutoring feedback at scale. By integrating a multi-student\nmonitoring dashboard with AI-powered avatar prompts, VTutor empowers a single\neducator or tutor to rapidly detect off-task or struggling students and\nintervene proactively, thus enhancing the benefits of one-on-one interactions\nin classroom contexts with several students. Drawing on insight from the\nlearning sciences and past research on animated pedagogical agents, we\ndemonstrate how stylized avatars can potentially sustain student engagement\nwhile accommodating varying infrastructure constraints. Finally, we address\nopen questions on refining large-scale, AI-driven tutoring solutions for\nimproved learner outcomes, and how VTutor could help interpret real-time\nlearner interactions to support remote tutors at scale. The VTutor platform can\nbe accessed at https://ls2025.vtutor.ai. The system demo video is at\nhttps://ls2025.vtutor.ai/video."}
{"id": "2505.07968", "pdf": "https://arxiv.org/pdf/2505.07968.pdf", "abs": "https://arxiv.org/abs/2505.07968", "title": "Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models", "authors": ["Weiyi Wu", "Xinwen Xu", "Chongyang Gao", "Xingjian Diao", "Siting Li", "Lucas A. Salas", "Jiang Gui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have great potential in the field of health\ncare, yet they face great challenges in adapting to rapidly evolving medical\nknowledge. This can lead to outdated or contradictory treatment suggestions.\nThis study investigated how LLMs respond to evolving clinical guidelines,\nfocusing on concept drift and internal inconsistencies. We developed the\nDriftMedQA benchmark to simulate guideline evolution and assessed the temporal\nreliability of various LLMs. Our evaluation of seven state-of-the-art models\nacross 4,290 scenarios demonstrated difficulties in rejecting outdated\nrecommendations and frequently endorsing conflicting guidance. Additionally, we\nexplored two mitigation strategies: Retrieval-Augmented Generation and\npreference fine-tuning via Direct Preference Optimization. While each method\nimproved model performance, their combination led to the most consistent and\nreliable results. These findings underscore the need to improve LLM robustness\nto temporal shifts to ensure more dependable applications in clinical practice."}
{"id": "2408.09030", "pdf": "https://arxiv.org/pdf/2408.09030.pdf", "abs": "https://arxiv.org/abs/2408.09030", "title": "Studying the Effects of Collaboration in Interactive Theme Discovery Systems", "authors": ["Alvin Po-Chun Chen", "Dananjay Srinivas", "Alexandra Barry", "Maksim Seniw", "Maria Leonor Pacheco"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "NLP-assisted solutions have gained considerable traction to support\nqualitative data analysis. However, there does not exist a unified evaluation\nframework that can account for the many different settings in which qualitative\nresearchers may employ them. In this paper, we take a first step in this\ndirection by proposing an evaluation framework to study the way in which\ndifferent tools may result in different outcomes depending on the collaboration\nstrategy employed. Specifically, we study the impact of synchronous vs.\nasynchronous collaboration using two different NLP-assisted qualitative\nresearch tools and present a comprehensive analysis of significant differences\nin the consistency, cohesiveness, and correctness of their outputs."}
{"id": "2505.07980", "pdf": "https://arxiv.org/pdf/2505.07980.pdf", "abs": "https://arxiv.org/abs/2505.07980", "title": "Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration", "authors": ["Fupei Guo", "Achintha Wijesinghe", "Songyang Zhang", "Zhi Ding"], "categories": ["cs.CL", "C.2.1; I.4.8"], "comment": null, "summary": "Semantic communications represent a new paradigm of next-generation\nnetworking that shifts bit-wise data delivery to conveying the semantic\nmeanings for bandwidth efficiency. To effectively accommodate various potential\ndownstream tasks at the receiver side, one should adaptively convey the most\ncritical semantic information. This work presents a novel task-adaptive\nsemantic communication framework based on diffusion models that is capable of\ndynamically adjusting the semantic message delivery according to various\ndownstream tasks. Specifically, we initialize the transmission of a\ndeep-compressed general semantic representation from the transmitter to enable\ndiffusion-based coarse data reconstruction at the receiver. The receiver\nidentifies the task-specific demands and generates textual prompts as feedback.\nIntegrated with the attention mechanism, the transmitter updates the semantic\ntransmission with more details to better align with the objectives of the\nintended receivers. Our test results demonstrate the efficacy of the proposed\nmethod in adaptively preserving critical task-relevant information for semantic\ncommunications while preserving high compression efficiency."}
{"id": "2410.13757", "pdf": "https://arxiv.org/pdf/2410.13757.pdf", "abs": "https://arxiv.org/abs/2410.13757", "title": "MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation", "authors": ["Zichen Zhu", "Hao Tang", "Yansi Li", "Dingye Liu", "Hongshen Xu", "Kunyao Lan", "Danyang Zhang", "Yixuan Jiang", "Hao Zhou", "Chenrun Wang", "Situo Zhang", "Liangtai Sun", "Yixiao Wang", "Yuheng Sun", "Lu Chen", "Kai Yu"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC"], "comment": "NAACL 2025 Demo Track [code] https://github.com/OpenDFM/MobA\n  [dataset] https://huggingface.co/datasets/OpenDFM/MobA-MobBench", "summary": "Existing Multimodal Large Language Model (MLLM)-based agents face significant\nchallenges in handling complex GUI (Graphical User Interface) interactions on\ndevices. These challenges arise from the dynamic and structured nature of GUI\nenvironments, which integrate text, images, and spatial relationships, as well\nas the variability in action spaces across different pages and tasks. To\naddress these limitations, we propose MobA, a novel MLLM-based mobile assistant\nsystem. MobA introduces an adaptive planning module that incorporates a\nreflection mechanism for error recovery and dynamically adjusts plans to align\nwith the real environment contexts and action module's execution capacity.\nAdditionally, a multifaceted memory module provides comprehensive memory\nsupport to enhance adaptability and efficiency. We also present MobBench, a\ndataset designed for complex mobile interactions. Experimental results on\nMobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI\nenvironments and perform complex mobile tasks."}
{"id": "2505.08004", "pdf": "https://arxiv.org/pdf/2505.08004.pdf", "abs": "https://arxiv.org/abs/2505.08004", "title": "Large Language Models and Arabic Content: A Review", "authors": ["Haneh Rhel", "Dmitri Roussinov"], "categories": ["cs.CL", "cs.AI"], "comment": "Original language: English This paper has been submitted to the First\n  International Conference on Artificial Intelligence and Generative AI\n  (FICAILY 2025), and it has been accepted for presentation at FICAILY on\n  9-10/July 2025 and for publication in the Springer Nature. Number of pages:\n  16 Publication status Accepted/In press - 7 Apr 2025\n  https://www.gena-ai-libya2025.com/", "summary": "Over the past three years, the rapid advancement of Large Language Models\n(LLMs) has had a profound impact on multiple areas of Artificial Intelligence\n(AI), particularly in Natural Language Processing (NLP) across diverse\nlanguages, including Arabic. Although Arabic is considered one of the most\nwidely spoken languages across 27 countries in the Arabic world and used as a\nsecond language in some other non-Arabic countries as well, there is still a\nscarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face\nvarious challenges due to the complexities of the Arabic language, including\nits rich morphology, intricate structure, and diverse writing standards, among\nother factors. Researchers have been actively addressing these challenges,\ndemonstrating that pre-trained Large Language Models (LLMs) trained on\nmultilingual corpora achieve significant success in various Arabic NLP tasks.\nThis study provides an overview of using large language models (LLMs) for the\nArabic language, highlighting early pre-trained Arabic Language models across\nvarious NLP applications and their ability to handle diverse Arabic content\ntasks and dialects. It also provides an overview of how techniques like\nfinetuning and prompt engineering can enhance the performance of these models.\nAdditionally, the study summarizes common Arabic benchmarks and datasets while\npresenting our observations on the persistent upward trend in the adoption of\nLLMs."}
{"id": "2411.04994", "pdf": "https://arxiv.org/pdf/2411.04994.pdf", "abs": "https://arxiv.org/abs/2411.04994", "title": "Legacy Procurement Practices Shape How U.S. Cities Govern AI: Understanding Government Employees' Practices, Challenges, and Needs", "authors": ["Nari Johnson", "Elise Silva", "Harrison Leon", "Motahhare Eslami", "Beth Schwanke", "Ravit Dotan", "Hoda Heidari"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 2 column format. In proceedings of ACM FAccT 2025", "summary": "Most AI tools adopted by governments are not developed internally, but\ninstead are acquired from third-party vendors in a process called public\nprocurement. In this paper, we conduct the first empirical study of how United\nStates cities' procurement practices shape critical decisions surrounding\npublic sector AI. We conduct semi-structured interviews with 19 city employees\nwho oversee AI procurement across 7 U.S. cities. We found that cities' legacy\nprocurement practices, which are shaped by decades-old laws and norms,\nestablish infrastructure that determines which AI is purchased, and which\nactors hold decision-making power over procured AI. We characterize the\nemerging actions cities have taken to adapt their purchasing practices to\naddress algorithmic harms. From employees' reflections on real-world AI\nprocurements, we identify three key challenges that motivate but are not fully\naddressed by existing AI procurement reform initiatives. Based on these\nfindings, we discuss implications and opportunities for the FAccT community to\nsupport cities in foreseeing and preventing AI harms throughout the public\nprocurement processes."}
{"id": "2505.08037", "pdf": "https://arxiv.org/pdf/2505.08037.pdf", "abs": "https://arxiv.org/abs/2505.08037", "title": "TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation", "authors": ["Yutong Liu", "Feng Xiao", "Ziyue Zhang", "Yongbin Yu", "Cheng Huang", "Fan Gao", "Xiangxiang Wang", "Ma-bao Ban", "Manping Fan", "Thupten Tsering", "Cheng Huang", "Gadeng Luosang", "Renzeng Duojie", "Nyima Tashi"], "categories": ["cs.CL", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Multi-level Tibetan spelling correction addresses errors at both the\ncharacter and syllable levels within a unified model. Existing methods focus\nmainly on single-level correction and lack effective integration of both\nlevels. Moreover, there are no open-source datasets or augmentation methods\ntailored for this task in Tibetan. To tackle this, we propose a data\naugmentation approach using unlabeled text to generate multi-level corruptions,\nand introduce TiSpell, a semi-masked model capable of correcting both\ncharacter- and syllable-level errors. Although syllable-level correction is\nmore challenging due to its reliance on global context, our semi-masked\nstrategy simplifies this process. We synthesize nine types of corruptions on\nclean sentences to create a robust training set. Experiments on both simulated\nand real-world data demonstrate that TiSpell, trained on our dataset,\noutperforms baseline models and matches the performance of state-of-the-art\napproaches, confirming its effectiveness."}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917.pdf", "abs": "https://arxiv.org/abs/2501.14917", "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance."}
{"id": "2505.08054", "pdf": "https://arxiv.org/pdf/2505.08054.pdf", "abs": "https://arxiv.org/abs/2505.08054", "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning", "authors": ["Zhehao Zhang", "Weijie Xu", "Fanyou Wu", "Chandan K. Reddy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Safety alignment approaches in large language models (LLMs) often lead to the\nover-refusal of benign queries, significantly diminishing their utility in\nsensitive scenarios. To address this challenge, we introduce FalseReject, a\ncomprehensive resource containing 16k seemingly toxic queries accompanied by\nstructured responses across 44 safety-related categories. We propose a\ngraph-informed adversarial multi-agent interaction framework to generate\ndiverse and complex prompts, while structuring responses with explicit\nreasoning to aid models in accurately distinguishing safe from unsafe contexts.\nFalseReject includes training datasets tailored for both standard\ninstruction-tuned models and reasoning-oriented models, as well as a\nhuman-annotated benchmark test set. Our extensive benchmarking on 29\nstate-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.\nEmpirical results demonstrate that supervised finetuning with FalseReject\nsubstantially reduces unnecessary refusals without compromising overall safety\nor general language capabilities."}
{"id": "2502.05442", "pdf": "https://arxiv.org/pdf/2502.05442.pdf", "abs": "https://arxiv.org/abs/2502.05442", "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?", "authors": ["Dylan Waldner", "Risto Miikkulainen"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted to CogSci 2025", "summary": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning."}
{"id": "2505.08058", "pdf": "https://arxiv.org/pdf/2505.08058.pdf", "abs": "https://arxiv.org/abs/2505.08058", "title": "HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method", "authors": ["Chris Forrester", "Octavia Sulea"], "categories": ["cs.CL"], "comment": null, "summary": "Compute optimization using token reduction of LLM prompts is an emerging task\nin the fields of NLP and next generation, agentic AI. In this white paper, we\nintroduce a novel (patent pending) text representation scheme and a\nfirst-of-its-kind word-level semantic compression of paragraphs that can lead\nto over 90\\% token reduction, while retaining high semantic similarity to the\nsource text. We explain how this novel compression technique can be lossless\nand how the detail granularity is controllable. We discuss benchmark results\nover open source data (i.e. Bram Stoker's Dracula available through Project\nGutenberg) and show how our results hold at the paragraph level, across\nmultiple genres and models."}
{"id": "2504.10497", "pdf": "https://arxiv.org/pdf/2504.10497.pdf", "abs": "https://arxiv.org/abs/2504.10497", "title": "Exploring Generative AI Techniques in Government: A Case Study", "authors": ["Sunyi Liu", "Mengzhe Geng", "Rebecca Hart"], "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.MA", "cs.SY", "eess.SY"], "comment": "In submission to IEEE Intelligent Systems", "summary": "The swift progress of Generative Artificial intelligence (GenAI), notably\nLarge Language Models (LLMs), is reshaping the digital landscape. Recognizing\nthis transformative potential, the National Research Council of Canada (NRC)\nlaunched a pilot initiative to explore the integration of GenAI techniques into\nits daily operation for performance excellence, where 22 projects were launched\nin May 2024. Within these projects, this paper presents the development of the\nintelligent agent Pubbie as a case study, targeting the automation of\nperformance measurement, data management and insight reporting at the NRC.\nCutting-edge techniques are explored, including LLM orchestration and semantic\nembedding via RoBERTa, while strategic fine-tuning and few-shot learning\napproaches are incorporated to infuse domain knowledge at an affordable cost.\nThe user-friendly interface of Pubbie allows general government users to input\nqueries in natural language and easily upload or download files with a simple\nbutton click, greatly reducing manual efforts and accessibility barriers."}
{"id": "2505.08106", "pdf": "https://arxiv.org/pdf/2505.08106.pdf", "abs": "https://arxiv.org/abs/2505.08106", "title": "Are LLMs complicated ethical dilemma analyzers?", "authors": ["Jiashen", "Du", "Jesse Yao", "Allen Liu", "Zhekai Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "CS194-280 Advanced LLM Agents project. Project page:\n  https://github.com/ALT-JS/ethicaLLM", "summary": "One open question in the study of Large Language Models (LLMs) is whether\nthey can emulate human ethical reasoning and act as believable proxies for\nhuman judgment. To investigate this, we introduce a benchmark dataset\ncomprising 196 real-world ethical dilemmas and expert opinions, each segmented\ninto five structured components: Introduction, Key Factors, Historical\nTheoretical Perspectives, Resolution Strategies, and Key Takeaways. We also\ncollect non-expert human responses for comparison, limited to the Key Factors\nsection due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,\nClaude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric\nframework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine\nsimilarity, and Universal Sentence Encoder similarity. Metric weights are\ncomputed through an inversion-based ranking alignment and pairwise AHP\nanalysis, enabling fine-grained comparison of model outputs to expert\nresponses. Our results show that LLMs generally outperform non-expert humans in\nlexical and structural alignment, with GPT-4o-mini performing most consistently\nacross all sections. However, all models struggle with historical grounding and\nproposing nuanced resolution strategies, which require contextual abstraction.\nHuman responses, while less structured, occasionally achieve comparable\nsemantic similarity, suggesting intuitive moral reasoning. These findings\nhighlight both the strengths and current limitations of LLMs in ethical\ndecision-making."}
{"id": "2505.08120", "pdf": "https://arxiv.org/pdf/2505.08120.pdf", "abs": "https://arxiv.org/abs/2505.08120", "title": "Putting It All into Context: Simplifying Agents with LCLMs", "authors": ["Mingjian Jiang", "Yangjun Ruan", "Luis Lastras", "Pavan Kapanipathi", "Tatsunori Hashimoto"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in language model (LM) agents have demonstrated significant\npotential for automating complex real-world tasks. To make progress on these\ndifficult tasks, LM agent architectures have become increasingly complex, often\nincorporating multi-step retrieval tools, multiple agents, and scaffolding\nadapted to the underlying LM. In this work, we investigate whether all of this\ncomplexity is necessary, or if parts of these scaffolds can be removed on\nchallenging tasks like SWE-bench. We show that in the case of SWE-bench, simply\nputting the entire environment into the context of a long context language\nmodel (LCLM) and properly prompting the model makes it competitive with\ncarefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model\nwithout any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable\nwith approaches using carefully tuned agent scaffolds (32%). While the\nunscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic\narchitectures, we demonstrate that the more capable Gemini-2.5-Pro using the\nsame unscaffolded approach directly attains a 50.8% solve rate. Additionally, a\ntwo-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a\ncompetitive 48.6% solve rate."}
{"id": "2505.08130", "pdf": "https://arxiv.org/pdf/2505.08130.pdf", "abs": "https://arxiv.org/abs/2505.08130", "title": "ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval", "authors": ["Mingxu Tao", "Bowen Tang", "Mingxuan Ma", "Yining Zhang", "Hourun Li", "Feifan Wen", "Hao Ma", "Jia Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in NAACL 2025 Demo Track", "summary": "The rise of Large Language Models~(LLMs) revolutionizes information\nretrieval, allowing users to obtain required answers through complex\ninstructions within conversations. However, publicly available services remain\ninadequate in addressing the needs of faculty and students to search\ncampus-specific information. It is primarily due to the LLM's lack of\ndomain-specific knowledge and the limitation of search engines in supporting\nmultilingual and timely scenarios. To tackle these challenges, we introduce\nALOHA, a multilingual agent enhanced by hierarchical retrieval for university\norientation. We also integrate external APIs into the front-end interface to\nprovide interactive service. The human evaluation and case study show our\nproposed system has strong capabilities to yield correct, timely, and\nuser-friendly responses to the queries in multiple languages, surpassing\ncommercial chatbots and search engines. The system has been deployed and has\nprovided service for more than 12,000 people."}
{"id": "2505.08167", "pdf": "https://arxiv.org/pdf/2505.08167.pdf", "abs": "https://arxiv.org/abs/2505.08167", "title": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage", "authors": ["Ruilin Liu", "Zhixiao Zhao", "Jieqiong Li", "Chang Liu", "Dongbo Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields."}
{"id": "2505.08168", "pdf": "https://arxiv.org/pdf/2505.08168.pdf", "abs": "https://arxiv.org/abs/2505.08168", "title": "Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph", "authors": ["Yuxiang Wang", "Xiao Yan", "Shiyu Jin", "Quanqing Xu", "Chuang Hu", "Yuanyuan Zhu", "Bo Du", "Jia Wu", "Jiawei Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text-attributed graph (TAG) provides a text description for each graph node,\nand few- and zero-shot node classification on TAGs have many applications in\nfields such as academia and social networks. Existing work utilizes various\ngraph-based augmentation techniques to train the node and text embeddings,\nwhile text-based augmentations are largely unexplored. In this paper, we\npropose Text Semantics Augmentation (TSA) to improve accuracy by introducing\nmore text semantic supervision signals. Specifically, we design two\naugmentation techniques, i.e., positive semantics matching and negative\nsemantics contrast, to provide more reference texts for each graph node or text\ndescription. Positive semantic matching retrieves texts with similar embeddings\nto match with a graph node. Negative semantic contrast adds a negative prompt\nto construct a text description with the opposite semantics, which is\ncontrasted with the original node and text. We evaluate TSA on 5 datasets and\ncompare with 13 state-of-the-art baselines. The results show that TSA\nconsistently outperforms all baselines, and its accuracy improvements over the\nbest-performing baseline are usually over 5%."}
{"id": "2505.08200", "pdf": "https://arxiv.org/pdf/2505.08200.pdf", "abs": "https://arxiv.org/abs/2505.08200", "title": "A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs", "authors": ["Artem Shelmanov", "Ekaterina Fadeeva", "Akim Tsvigun", "Ivan Tsvigun", "Zhuohan Xie", "Igor Kiselev", "Nico Daheim", "Caiqi Zhang", "Artem Vazhentsev", "Mrinmaya Sachan", "Preslav Nakov", "Timothy Baldwin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have the tendency to hallucinate, i.e., to\nsporadically generate false or fabricated information. This presents a major\nchallenge, as hallucinations often appear highly convincing and users generally\nlack the tools to detect them. Uncertainty quantification (UQ) provides a\nframework for assessing the reliability of model outputs, aiding in the\nidentification of potential hallucinations. In this work, we introduce\npre-trained UQ heads: supervised auxiliary modules for LLMs that substantially\nenhance their ability to capture uncertainty compared to unsupervised UQ\nmethods. Their strong performance stems from the powerful Transformer\narchitecture in their design and informative features derived from LLM\nattention maps. Experimental evaluation shows that these heads are highly\nrobust and achieve state-of-the-art performance in claim-level hallucination\ndetection across both in-domain and out-of-domain prompts. Moreover, these\nmodules demonstrate strong generalization to languages they were not explicitly\ntrained on. We pre-train a collection of UQ heads for popular LLM series,\nincluding Mistral, Llama, and Gemma 2. We publicly release both the code and\nthe pre-trained heads."}
{"id": "2505.08245", "pdf": "https://arxiv.org/pdf/2505.08245.pdf", "abs": "https://arxiv.org/abs/2505.08245", "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement", "authors": ["Haoran Ye", "Jing Jin", "Yuhang Xie", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "63 pages, 482 references", "summary": "The rapid advancement of large language models (LLMs) has outpaced\ntraditional evaluation methodologies. It presents novel challenges, such as\nmeasuring human-like psychological constructs, navigating beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with Psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This survey introduces and synthesizes an emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. We systematically explore the role of Psychometrics in shaping\nbenchmarking principles, broadening evaluation scopes, refining methodologies,\nvalidating results, and advancing LLM capabilities. This paper integrates\ndiverse perspectives to provide a structured framework for researchers across\ndisciplines, enabling a more comprehensive understanding of this nascent field.\nUltimately, we aim to provide actionable insights for developing future\nevaluation paradigms that align with human-level AI and promote the advancement\nof human-centered AI systems for societal benefit. A curated repository of LLM\npsychometric resources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics."}
{"id": "2505.08261", "pdf": "https://arxiv.org/pdf/2505.08261.pdf", "abs": "https://arxiv.org/abs/2505.08261", "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration", "authors": ["Rishabh Agrawal", "Himanshu Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."}
{"id": "2505.08303", "pdf": "https://arxiv.org/pdf/2505.08303.pdf", "abs": "https://arxiv.org/abs/2505.08303", "title": "Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow", "authors": ["Ziyu Zhou", "Yihang Wu", "Jingyuan Yang", "Zhan Xiao", "Rongjun Li"], "categories": ["cs.CL"], "comment": null, "summary": "Black-Box prompt optimization methods have emerged as a promising strategy\nfor refining input prompts to better align large language models (LLMs),\nthereby enhancing their task performance. Although these methods have\ndemonstrated encouraging results, most studies and experiments have primarily\nfocused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,\nGPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with\nDeepSeek V3 (671B), it remains an open question whether these black-box\noptimization techniques will continue to yield significant performance\nimprovements for models of such scale. In response to this, we select three\nwell-known black-box optimization methods and evaluate them on large-scale LLMs\n(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The\nresults show that these black-box prompt optimization methods offer only\nlimited improvements on these large-scale LLMs. Furthermore, we hypothesize\nthat the scale of the model is the primary factor contributing to the limited\nbenefits observed. To explore this hypothesis, we conducted experiments on LLMs\nof varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an\ninverse scaling law, wherein the effectiveness of black-box optimization\nmethods diminished as the model size increased."}
{"id": "2505.08311", "pdf": "https://arxiv.org/pdf/2505.08311.pdf", "abs": "https://arxiv.org/abs/2505.08311", "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale", "authors": ["Yunjie Ji", "Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\n\\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}."}
{"id": "2505.08348", "pdf": "https://arxiv.org/pdf/2505.08348.pdf", "abs": "https://arxiv.org/abs/2505.08348", "title": "On the Geometry of Semantics in Next-token Prediction", "authors": ["Yize Zhao", "Christos Thrampoulidis"], "categories": ["cs.CL"], "comment": null, "summary": "Modern language models demonstrate a remarkable ability to capture linguistic\nmeaning despite being trained solely through next-token prediction (NTP). We\ninvestigate how this conceptually simple training objective leads models to\nextract and encode latent semantic and grammatical concepts. Our analysis\nreveals that NTP optimization implicitly guides models to encode concepts via\nsingular value decomposition (SVD) factors of a centered data-sparsity matrix\nthat captures next-word co-occurrence patterns. While the model never\nexplicitly constructs this matrix, learned word and context embeddings\neffectively factor it to capture linguistic structure. We find that the most\nimportant SVD factors are learned first during training, motivating the use of\nspectral clustering of embeddings to identify human-interpretable semantics,\nincluding both classical k-means and a new orthant-based method directly\nmotivated by our interpretation of concepts. Overall, our work bridges\ndistributional semantics, neural collapse geometry, and neural network training\ndynamics, providing insights into how NTP's implicit biases shape the emergence\nof meaning representations in language models."}
{"id": "2505.08351", "pdf": "https://arxiv.org/pdf/2505.08351.pdf", "abs": "https://arxiv.org/abs/2505.08351", "title": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring", "authors": ["Mina Almasi", "Ross Deans Kristensen-McLachlan"], "categories": ["cs.CL"], "comment": null, "summary": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants."}
{"id": "2505.08389", "pdf": "https://arxiv.org/pdf/2505.08389.pdf", "abs": "https://arxiv.org/abs/2505.08389", "title": "Towards Contamination Resistant Benchmarks", "authors": ["Rahmatullah Musawi", "Sheng Lu"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of large language models (LLMs) has transformed the\nlandscape of natural language processing. Evaluating LLMs properly is crucial\nfor understanding their potential and addressing concerns such as safety.\nHowever, LLM evaluation is confronted by various factors, among which\ncontamination stands out as a key issue that undermines the reliability of\nevaluations. In this work, we introduce the concept of contamination resistance\nto address this challenge. We propose a benchmark based on Caesar ciphers\n(e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an\nexcellent example of a contamination resistant benchmark. We test this\nbenchmark on widely used LLMs under various settings, and we find that these\nmodels struggle with this benchmark when contamination is controlled. Our\nfindings reveal issues in current LLMs and raise important questions regarding\ntheir true capabilities. Our work contributes to the development of\ncontamination resistant benchmarks, enabling more rigorous LLM evaluation and\noffering insights into the true capabilities and limitations of LLMs."}
{"id": "2505.08392", "pdf": "https://arxiv.org/pdf/2505.08392.pdf", "abs": "https://arxiv.org/abs/2505.08392", "title": "Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping", "authors": ["Ren Zhuang", "Ben Wang", "Shuifa Sun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models leverage Chain-of-Thought (CoT) prompting for complex\ntasks, but their reasoning traces are often excessively verbose and\ninefficient, leading to significant computational costs and latency. Current\nCoT compression techniques typically rely on generic importance metrics and\nstatic compression rates, which may inadvertently remove functionally critical\ntokens or fail to adapt to varying reasoning complexity. To overcome these\nlimitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic\nCoT compression via supervised fine-tuning. This approach introduces two\nsynergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric\naccurately identifying functionally relevant tokens by measuring the gradient\ninfluence of their intermediate representations on the final answer loss, and\n(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the\ncompression rate based on runtime model uncertainty while ensuring local\ncoherence through an adaptive N-token constraint. To our knowledge, this is the\nfirst work unifying a goal-oriented, gradient-based importance metric with\ndynamic, uncertainty-aware skipping for CoT compression. Trained on compressed\nMATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization\nacross diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It\nachieves substantial efficiency gains - reducing CoT token counts by over 45%\non average and delivering 1.6-2.0 times inference speedups - while maintaining\nhigh reasoning accuracy. Notably, it significantly outperforms existing\nbaselines by preserving accuracy even at high effective compression rates,\nadvancing the state of the art in the CoT reasoning efficiency-accuracy\ntrade-off."}
{"id": "2505.08402", "pdf": "https://arxiv.org/pdf/2505.08402.pdf", "abs": "https://arxiv.org/abs/2505.08402", "title": "TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers", "authors": ["Aiyao He", "Sijia Cui", "Shuai Xu", "Yanna Wang", "Bo Xu"], "categories": ["cs.CL"], "comment": "Accepted to ICONIP 2024", "summary": "Recently, large language models(LLMs) have played an increasingly important\nrole in solving a wide range of NLP tasks, leveraging their capabilities of\nnatural language understanding and generating. Integration with external tools\nfurther enhances LLMs' effectiveness, providing more precise, timely, and\nspecialized responses. However, LLMs still encounter difficulties with\nnon-executable actions and improper actions, which are primarily attributed to\nincorrect parameters. The process of generating parameters by LLMs is confined\nto the tool level, employing the coarse-grained strategy without considering\nthe different difficulties of various tools. To address this issue, we propose\nTUMS, a novel framework designed to enhance the tool-use capabilities of LLMs\nby transforming tool-level processing into parameter-level processing.\nSpecifically, our framework consists of four key components: (1) an intent\nrecognizer that identifies the user's intent to help LLMs better understand the\ntask; (2) a task decomposer that breaks down complex tasks into simpler\nsubtasks, each involving a tool call; (3) a subtask processor equipped with\nmulti-structure handlers to generate accurate parameters; and (4) an executor.\nOur empirical studies have evidenced the effectiveness and efficiency of the\nTUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on\neasy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key\ncontribution of each part with ablation experiments, offering more insights and\nstimulating future research on Tool-augmented LLMs."}
{"id": "2505.08435", "pdf": "https://arxiv.org/pdf/2505.08435.pdf", "abs": "https://arxiv.org/abs/2505.08435", "title": "Hakim: Farsi Text Embedding Model", "authors": ["Mehran Sarmadi", "Morteza Alikhani", "Erfan Zinvandi", "Zahra Pourbahman"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding."}
{"id": "2505.08439", "pdf": "https://arxiv.org/pdf/2505.08439.pdf", "abs": "https://arxiv.org/abs/2505.08439", "title": "A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court", "authors": ["Matteo Marulli", "Glauco Panattoni", "Marco Bertini"], "categories": ["cs.CL"], "comment": "51 pages", "summary": "Topic modeling in Italian legal research is hindered by the lack of public\ndatasets, limiting the analysis of legal themes in Supreme Court judgments. To\naddress this, we developed a document processing pipeline that produces an\nanonymized dataset optimized for topic modeling.\n  The pipeline integrates document layout analysis (YOLOv8x), optical character\nrecognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964\nand a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and\nthe text recognizer (TrOCR) obtained a character error rate of 0.0047 and a\nword error rate of 0.0248. Compared to OCR-only methods, our dataset improved\ntopic modeling with a diversity score of 0.6198 and a coherence score of\n0.6638.\n  We applied BERTopic to extract topics and used large language models to\ngenerate labels and summaries. Outputs were evaluated against domain expert\ninterpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for\nlabeling and 0.9130 for summarization."}
{"id": "2505.08450", "pdf": "https://arxiv.org/pdf/2505.08450.pdf", "abs": "https://arxiv.org/abs/2505.08450", "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation", "authors": ["Kazuki Hayashi", "Hidetaka Kamigaito", "Shinya Kouda", "Taro Watanabe"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."}
{"id": "2505.08463", "pdf": "https://arxiv.org/pdf/2505.08463.pdf", "abs": "https://arxiv.org/abs/2505.08463", "title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models", "authors": ["Fujun Zhang", "XiangDong Su"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines."}
{"id": "2505.08464", "pdf": "https://arxiv.org/pdf/2505.08464.pdf", "abs": "https://arxiv.org/abs/2505.08464", "title": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions", "authors": ["Lata Pangtey", "Anukriti Bhatnagar", "Shubhi Bansal", "Shahid Shafi Dar", "Nagendra Kumar"], "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models."}
{"id": "2505.08468", "pdf": "https://arxiv.org/pdf/2505.08468.pdf", "abs": "https://arxiv.org/abs/2505.08468", "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Ahmed Masry", "Mizanur Rahman", "Amran Bhuiyan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025 Industry Track", "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist."}
{"id": "2505.08498", "pdf": "https://arxiv.org/pdf/2505.08498.pdf", "abs": "https://arxiv.org/abs/2505.08498", "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models", "authors": ["Takumi Shibata", "Yuichi Miyamura"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES."}
{"id": "2505.08504", "pdf": "https://arxiv.org/pdf/2505.08504.pdf", "abs": "https://arxiv.org/abs/2505.08504", "title": "Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding", "authors": ["Jeongwoo Kang", "Maximin Coavoux", "Cédric Lopez", "Didier Schwab"], "categories": ["cs.CL"], "comment": "published at Insights from Negative Results in NLP (workshop EMNLP\n  2025)", "summary": "Sequence-to-sequence models are widely used to train Abstract Meaning\nRepresentation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR\ngraphs have to be linearized into a one-line text format. While Penman encoding\nis typically used for this purpose, we argue that it has limitations: (1) for\ndeep graphs, some closely related nodes are located far apart in the linearized\ntext (2) Penman's tree-based encoding necessitates inverse roles to handle node\nre-entrancy, doubling the number of relation types to predict. To address these\nissues, we propose a triple-based linearization method and compare its\nefficiency with Penman linearization. Although triples are well suited to\nrepresent a graph, our results suggest room for improvement in triple encoding\nto better compete with Penman's concise and explicit representation of a nested\ngraph structure."}
{"id": "2505.08546", "pdf": "https://arxiv.org/pdf/2505.08546.pdf", "abs": "https://arxiv.org/abs/2505.08546", "title": "Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation", "authors": ["Chiara Manna", "Afra Alishahi", "Frédéric Blain", "Eva Vanmassenhove"], "categories": ["cs.CL"], "comment": null, "summary": "While gender bias in modern Neural Machine Translation (NMT) systems has\nreceived much attention, traditional evaluation metrics do not to fully capture\nthe extent to which these systems integrate contextual gender cues. We propose\na novel evaluation metric called Minimal Pair Accuracy (MPA), which measures\nthe reliance of models on gender cues for gender disambiguation. MPA is\ndesigned to go beyond surface-level gender accuracy metrics by focusing on\nwhether models adapt to gender cues in minimal pairs -- sentence pairs that\ndiffer solely in the gendered pronoun, namely the explicit indicator of the\ntarget's entity gender in the source language (EN). We evaluate a number of NMT\nmodels on the English-Italian (EN--IT) language pair using this metric, we show\nthat they ignore available gender cues in most cases in favor of (statistical)\nstereotypical gender interpretation. We further show that in anti-stereotypical\ncases, these models tend to more consistently take masculine gender cues into\naccount while ignoring the feminine cues. Furthermore, we analyze the attention\nhead weights in the encoder component and show that while all models encode\ngender information to some extent, masculine cues elicit a more diffused\nresponse compared to the more concentrated and specialized responses to\nfeminine gender cues."}
{"id": "2505.08588", "pdf": "https://arxiv.org/pdf/2505.08588.pdf", "abs": "https://arxiv.org/abs/2505.08588", "title": "Small but Significant: On the Promise of Small Language Models for Accessible AIED", "authors": ["Yumou Wei", "Paulo Carvalho", "John Stamper"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "This vision paper advocates using small language models (e.g., Phi-2)\n  in AI for education (AIED)", "summary": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches."}
{"id": "2505.08590", "pdf": "https://arxiv.org/pdf/2505.08590.pdf", "abs": "https://arxiv.org/abs/2505.08590", "title": "Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models", "authors": ["Hussien Al-Asi", "Jordan P Reynolds", "Shweta Agarwal", "Bryan J Dangott", "Aziza Nassar", "Zeynettin Akkus"], "categories": ["cs.CL", "q-bio.QM"], "comment": null, "summary": "Advancements in artificial intelligence (AI) are transforming pathology by\nintegrat-ing large language models (LLMs) with retrieval-augmented generation\n(RAG) and domain-specific foundation models. This study explores the\napplication of RAG-enhanced LLMs coupled with pathology foundation models for\nthyroid cytology diagnosis, addressing challenges in cytological\ninterpretation, standardization, and diagnostic accuracy. By leveraging a\ncurated knowledge base, RAG facilitates dy-namic retrieval of relevant case\nstudies, diagnostic criteria, and expert interpreta-tion, improving the\ncontextual understanding of LLMs. Meanwhile, pathology foun-dation models,\ntrained on high-resolution pathology images, refine feature extrac-tion and\nclassification capabilities. The fusion of these AI-driven approaches en-hances\ndiagnostic consistency, reduces variability, and supports pathologists in\ndis-tinguishing benign from malignant thyroid lesions. Our results demonstrate\nthat integrating RAG with pathology-specific LLMs significantly improves\ndiagnostic efficiency and interpretability, paving the way for AI-assisted\nthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for\ncorrect prediction of surgi-cal pathology diagnosis from thyroid cytology\nsamples."}
{"id": "2505.08600", "pdf": "https://arxiv.org/pdf/2505.08600.pdf", "abs": "https://arxiv.org/abs/2505.08600", "title": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding", "authors": ["Danying Ge", "Jianhua Gao", "Qizhi Jiang", "Yifei Feng", "Weixing Ji"], "categories": ["cs.CL", "I.2.7"], "comment": "10 pages, 10 figures, 2 tables", "summary": "Speculative decoding, which combines a draft model with a target model, has\nemerged as an effective approach to accelerate large language model (LLM)\ninference. However, existing methods often face a trade-off between the\nacceptance rate and decoding speed in downstream tasks due to the limited\ncapacity of the draft model, making it difficult to ensure efficiency across\ndiverse tasks. To address this problem, we propose a speculative decoding\nalgorithm tailored for downstream task optimization. It includes an automatic\ntask partitioning and assigning method, which automatically categorizes\ndownstream tasks into different sub-tasks and assigns them to a set of\nheterogeneous draft models. Each draft model is aligned with the target model\nusing task-specific data, thereby enhancing the consistency of inference\nresults. In addition, our proposed method incorporates an online lightweight\nprompt classifier to dynamically route prompts to the appropriate draft model.\nExperimental results demonstrate that the proposed method improves draft\naccuracy by 6% to 50% over vanilla speculative decoding, while achieving a\nspeedup of 1.10x to 2.64x in LLM inference."}
{"id": "2505.08651", "pdf": "https://arxiv.org/pdf/2505.08651.pdf", "abs": "https://arxiv.org/abs/2505.08651", "title": "Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing", "authors": ["Chen Wu", "Yin Song"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 6 figures, ACL 2025 (Industry Track)", "summary": "We present MegaBeam-Mistral-7B, a language model that supports 512K-token\ncontext length. Our work addresses practical limitations in long-context\ntraining, supporting real-world tasks such as compliance monitoring and\nverification. Evaluated on three long-context benchmarks, our 7B-parameter\nmodel demonstrates superior in-context learning performance on HELMET and\nrobust retrieval and tracing capability on RULER. It is currently the only open\nmodel to achieve competitive long-range reasoning on BABILong at 512K context\nlength without RAG or targeted fine-tuning. Released as fully open source under\nthe Apache 2.0 license, the model has been downloaded over 100,000 times on\nHugging Face. Model available at:\nhttps://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k"}
{"id": "2505.08662", "pdf": "https://arxiv.org/pdf/2505.08662.pdf", "abs": "https://arxiv.org/abs/2505.08662", "title": "Revealing economic facts: LLMs know more than they say", "authors": ["Marcus Buckmann", "Quynh Anh Nguyen", "Edward Hill"], "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC", "I.2.7"], "comment": "34 pages, 17 figures", "summary": "We investigate whether the hidden states of large language models (LLMs) can\nbe used to estimate and impute economic and financial statistics. Focusing on\ncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,\nwe show that a simple linear model trained on the hidden states of open-source\nLLMs outperforms the models' text outputs. This suggests that hidden states\ncapture richer economic information than the responses of the LLMs reveal\ndirectly. A learning curve analysis indicates that only a few dozen labelled\nexamples are sufficient for training. We also propose a transfer learning\nmethod that improves estimation accuracy without requiring any labelled data\nfor the target variable. Finally, we demonstrate the practical utility of\nhidden-state representations in super-resolution and data imputation tasks."}
{"id": "2505.08690", "pdf": "https://arxiv.org/pdf/2505.08690.pdf", "abs": "https://arxiv.org/abs/2505.08690", "title": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation", "authors": ["Sheng Liang", "Hang Lv", "Zhihao Wen", "Yaxiong Wu", "Yongyue Zhang", "Hao Wang", "Yong Liu"], "categories": ["cs.CL", "I.2.7"], "comment": "15 pages, 3 figures", "summary": "Event extraction (EE) is a fundamental task in natural language processing\n(NLP) that involves identifying and extracting event information from\nunstructured text. Effective EE in real-world scenarios requires two key steps:\nselecting appropriate schemas from hundreds of candidates and executing the\nextraction process. Existing research exhibits two critical gaps: (1) the rigid\nschema fixation in existing pipeline systems, and (2) the absence of benchmarks\nfor evaluating joint schema matching and extraction. Although large language\nmodels (LLMs) offer potential solutions, their schema hallucination tendencies\nand context window limitations pose challenges for practical deployment. In\nresponse, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel\nparadigm combining schema paraphrasing with schema retrieval-augmented\ngeneration. ASEE adeptly retrieves paraphrased schemas and accurately generates\ntargeted structures. To facilitate rigorous evaluation, we construct the\nMulti-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which\nsystematically consolidates 12 datasets across diverse domains, complexity\nlevels, and language settings. Extensive evaluations on MD-SEE show that our\nproposed ASEE demonstrates strong adaptability across various scenarios,\nsignificantly improving the accuracy of event extraction."}
{"id": "2505.08734", "pdf": "https://arxiv.org/pdf/2505.08734.pdf", "abs": "https://arxiv.org/abs/2505.08734", "title": "NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context", "authors": ["Ben Yao", "Qiuchi Li", "Yazhou Zhang", "Siyu Yang", "Bohan Zhang", "Prayag Tiwari", "Jing Qin"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "25 pages, 10 figures, 16 tables", "summary": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues."}
{"id": "2505.08739", "pdf": "https://arxiv.org/pdf/2505.08739.pdf", "abs": "https://arxiv.org/abs/2505.08739", "title": "Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies", "authors": ["Xiaoliang Luo", "Xinyi Xu", "Michael Ramscar", "Bradley C. Love"], "categories": ["cs.CL"], "comment": null, "summary": "Can autoregressive large language models (LLMs) learn consistent probability\ndistributions when trained on sequences in different token orders? We prove\nformally that for any well-defined probability distribution, sequence\nperplexity is invariant under any factorization, including forward, backward,\nor arbitrary permutations. This result establishes a rigorous theoretical\nfoundation for studying how LLMs learn from data and defines principled\nprotocols for empirical evaluation. Applying these protocols, we show that\nprior studies examining ordering effects suffer from critical methodological\nflaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted\norders on scientific text. We find systematic deviations from theoretical\ninvariance across all orderings with arbitrary permutations strongly deviating\nfrom both forward and backward models, which largely (but not completely)\nagreed with one another. Deviations were traceable to differences in\nself-attention, reflecting positional and locality biases in processing. Our\ntheoretical and empirical results provide novel avenues for understanding\npositional biases in LLMs and suggest methods for detecting when LLMs'\nprobability distributions are inconsistent and therefore untrustworthy."}
{"id": "2505.08750", "pdf": "https://arxiv.org/pdf/2505.08750.pdf", "abs": "https://arxiv.org/abs/2505.08750", "title": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models", "authors": ["Yanxi Zhang", "Xin Cong", "Zhong Zhang", "Xiao Liu", "Dongyan Zhao", "Yesai Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains."}
{"id": "2505.08751", "pdf": "https://arxiv.org/pdf/2505.08751.pdf", "abs": "https://arxiv.org/abs/2505.08751", "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "authors": ["Saurabh Dash", "Yiyang Nan", "John Dang", "Arash Ahmadian", "Shivalika Singh", "Madeline Smith", "Bharat Venkitesh", "Vlad Shmyhlo", "Viraat Aryabumi", "Walter Beller-Morales", "Jeremy Pekmez", "Jason Ozuzu", "Pierre Richemond", "Acyr Locatelli", "Nick Frosst", "Phil Blunsom", "Aidan Gomez", "Ivan Zhang", "Marzieh Fadaee", "Manoj Govindassamy", "Sudip Roy", "Matthias Gallé", "Beyza Ermis", "Ahmet Üstün", "Sara Hooker"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance."}
{"id": "2505.08775", "pdf": "https://arxiv.org/pdf/2505.08775.pdf", "abs": "https://arxiv.org/abs/2505.08775", "title": "HealthBench: Evaluating Large Language Models Towards Improved Human Health", "authors": ["Rahul K. Arora", "Jason Wei", "Rebecca Soskin Hicks", "Preston Bowman", "Joaquin Quiñonero-Candela", "Foivos Tsimpourlas", "Michael Sharman", "Meghan Shah", "Andrea Vallone", "Alex Beutel", "Johannes Heidecke", "Karan Singhal"], "categories": ["cs.CL"], "comment": "Blog: https://openai.com/index/healthbench/ Code:\n  https://github.com/openai/simple-evals", "summary": "We present HealthBench, an open-source benchmark measuring the performance\nand safety of large language models in healthcare. HealthBench consists of\n5,000 multi-turn conversations between a model and an individual user or\nhealthcare professional. Responses are evaluated using conversation-specific\nrubrics created by 262 physicians. Unlike previous multiple-choice or\nshort-answer benchmarks, HealthBench enables realistic, open-ended evaluation\nthrough 48,562 unique rubric criteria spanning several health contexts (e.g.,\nemergencies, transforming clinical data, global health) and behavioral\ndimensions (e.g., accuracy, instruction following, communication). HealthBench\nperformance over the last two years reflects steady initial progress (compare\nGPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3\nscores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms\nGPT-4o and is 25 times cheaper. We additionally release two HealthBench\nvariations: HealthBench Consensus, which includes 34 particularly important\ndimensions of model behavior validated via physician consensus, and HealthBench\nHard, where the current top score is 32%. We hope that HealthBench grounds\nprogress towards model development and applications that benefit human health."}
{"id": "2505.07864", "pdf": "https://arxiv.org/pdf/2505.07864.pdf", "abs": "https://arxiv.org/abs/2505.07864", "title": "Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding", "authors": ["Takamitsu Omasa", "Ryo Koshihara", "Masumi Morishige"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "11 pages, 1 figures,", "summary": "Flowcharts are indispensable tools in software design and business-process\nanalysis, yet current vision-language models (VLMs) frequently misinterpret the\ndirectional arrows and graph topology that set these diagrams apart from\nnatural images. We introduce a seven-stage pipeline grouped into three broader\nprocesses: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical\ncharacter recognition (OCR) to extract node text; and (3) construction of a\nstructured prompt that guides the VLMs. Tested on a 90-question benchmark\ndistilled from 30 annotated flowcharts, the method raises overall accuracy from\n80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The\ngain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);\nbranch-result questions improve more modestly, and before-step questions remain\ndifficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same\ntrends, reinforcing the advantage of explicit arrow encoding. Limitations\ninclude dependence on detector and OCR precision, the small evaluation set, and\nresidual errors at nodes with multiple incoming edges. Future work will enlarge\nthe benchmark with synthetic and handwritten flowcharts and assess the approach\non Business Process Model and Notation (BPMN) and Unified Modeling Language\n(UML)."}
{"id": "2505.07865", "pdf": "https://arxiv.org/pdf/2505.07865.pdf", "abs": "https://arxiv.org/abs/2505.07865", "title": "CellVerse: Do Large Language Models Really Understand Cell Biology?", "authors": ["Fan Zhang", "Tianyu Liu", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Donghao Zhou", "Yefeng Zheng", "Kun Wang", "Xian Wu", "Pheng-Ann Heng"], "categories": ["q-bio.QM", "cs.AI", "cs.CL", "q-bio.CB"], "comment": null, "summary": "Recent studies have demonstrated the feasibility of modeling single-cell data\nas natural languages and the potential of leveraging powerful large language\nmodels (LLMs) for understanding cell biology. However, a comprehensive\nevaluation of LLMs' performance on language-driven single-cell analysis tasks\nstill remains unexplored. Motivated by this challenge, we introduce CellVerse,\na unified language-centric question-answering benchmark that integrates four\ntypes of single-cell multi-omics data and encompasses three hierarchical levels\nof single-cell analysis tasks: cell type annotation (cell-level), drug response\nprediction (drug-level), and perturbation analysis (gene-level). Going beyond\nthis, we systematically evaluate the performance across 14 open-source and\nclosed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the\nexperimental results reveal: (1) Existing specialist models (C2S-Pythia) fail\nto make reasonable decisions across all sub-tasks within CellVerse, while\ngeneralist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit\npreliminary understanding capabilities within the realm of cell biology. (2)\nThe performance of current LLMs falls short of expectations and has substantial\nroom for improvement. Notably, in the widely studied drug response prediction\ntask, none of the evaluated LLMs demonstrate significant performance\nimprovement over random guessing. CellVerse offers the first large-scale\nempirical demonstration that significant challenges still remain in applying\nLLMs to cell biology. By introducing CellVerse, we lay the foundation for\nadvancing cell biology through natural languages and hope this paradigm could\nfacilitate next-generation single-cell analysis."}
{"id": "2505.07902", "pdf": "https://arxiv.org/pdf/2505.07902.pdf", "abs": "https://arxiv.org/abs/2505.07902", "title": "Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach", "authors": ["Ruikun Hou", "Babette Bühler", "Tim Fütterer", "Efe Bozkir", "Peter Gerjets", "Ulrich Trautwein", "Enkelejda Kasneci"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "The 18th International Conference on Educational Data Mining (EDM\n  2025)", "summary": "Classroom discourse is an essential vehicle through which teaching and\nlearning take place. Assessing different characteristics of discursive\npractices and linking them to student learning achievement enhances the\nunderstanding of teaching quality. Traditional assessments rely on manual\ncoding of classroom observation protocols, which is time-consuming and costly.\nDespite many studies utilizing AI techniques to analyze classroom discourse at\nthe utterance level, investigations into the evaluation of discursive practices\nthroughout an entire lesson segment remain limited. To address this gap, our\nstudy proposes a novel text-centered multimodal fusion architecture to assess\nthe quality of three discourse components grounded in the Global Teaching\nInSights (GTI) observation protocol: Nature of Discourse, Questioning, and\nExplanations. First, we employ attention mechanisms to capture inter- and\nintra-modal interactions from transcript, audio, and video streams. Second, a\nmulti-task learning approach is adopted to jointly predict the quality scores\nof the three components. Third, we formulate the task as an ordinal\nclassification problem to account for rating level order. The effectiveness of\nthese designed elements is demonstrated through an ablation study on the GTI\nGermany dataset containing 92 videotaped math lessons. Our results highlight\nthe dominant role of text modality in approaching this task. Integrating\nacoustic features enhances the model's consistency with human ratings,\nachieving an overall Quadratic Weighted Kappa score of 0.384, comparable to\nhuman inter-rater reliability (0.326). Our study lays the groundwork for the\nfuture development of automated discourse quality assessment to support teacher\nprofessional development through timely feedback on multidimensional discourse\npractices."}
{"id": "2505.07908", "pdf": "https://arxiv.org/pdf/2505.07908.pdf", "abs": "https://arxiv.org/abs/2505.07908", "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny", "authors": ["Karahan Sarıtaş", "Çağatay Yıldız"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "In this reproduction study, we revisit recent claims that self-attention\nimplements kernel principal component analysis (KPCA) (Teo et al., 2024),\npositing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix\nof the keys, and (ii) that self-attention projects queries onto the principal\ncomponent axes of the key matrix $K$ in a feature space. Our analysis reveals\nthree critical inconsistencies: (1) No alignment exists between learned\nself-attention value vectors and what is proposed in the KPCA perspective, with\naverage similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA\n(Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating\nnegligible correspondence; (2) Reported decreases in reconstruction loss\n$J_\\text{proj}$, arguably justifying the claim that the self-attention\nminimizes the projection error of KPCA, are misinterpreted, as the quantities\ninvolved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix\neigenvalue statistics, introduced to justify that $V$ captures the eigenvector\nof the gram matrix, are irreproducible without undocumented\nimplementation-specific adjustments. Across 10 transformer architectures, we\nconclude that the KPCA interpretation of self-attention lacks empirical\nsupport."}
{"id": "2505.07912", "pdf": "https://arxiv.org/pdf/2505.07912.pdf", "abs": "https://arxiv.org/abs/2505.07912", "title": "SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Niklas Stehr", "Oliver Karras", "Markus Stocker", "Sören Auer"], "categories": ["cs.DL", "cs.CL", "cs.MM"], "comment": "18 pages, 10 figures, submitted to TPDL 2025", "summary": "Democratic societies need accessible, reliable information. Videos and\nPodcasts have established themselves as the medium of choice for civic\ndissemination, but also as carriers of misinformation. The emerging Science\nCommunication Knowledge Infrastructure (SciCom KI) curating non-textual media\nis still fragmented and not adequately equipped to scale against the content\nflood. Our work sets out to support the SciCom KI with a central, collaborative\nplatform, the SciCom Wiki, to facilitate FAIR (findable, accessible,\ninteroperable, reusable) media representation and the fact-checking of their\ncontent, particularly for videos and podcasts. Building an open-source service\nsystem centered around Wikibase, we survey requirements from 53 stakeholders,\nrefine these in 11 interviews, and evaluate our prototype based on these\nrequirements with another 14 participants. To address the most requested\nfeature, fact-checking, we developed a neurosymbolic computational\nfact-checking approach, converting heterogenous media into knowledge graphs.\nThis increases machine-readability and allows comparing statements against\nequally represented ground-truth. Our computational fact-checking tool was\niteratively evaluated through 10 expert interviews, a public user survey with\n43 participants verified the necessity and usability of our tool. Overall, our\nfindings identified several needs to systematically support the SciCom KI. The\nSciCom Wiki, as a FAIR digital library complementing our neurosymbolic\ncomputational fact-checking framework, was found suitable to address the raised\nrequirements. Further, we identified that the SciCom KI is severely\nunderdeveloped regarding FAIR knowledge and related systems facilitating its\ncollaborative creation and curation. Our system can provide a central knowledge\nnode, yet a collaborative effort is required to scale against the imminent\n(mis-)information flood."}
{"id": "2505.08052", "pdf": "https://arxiv.org/pdf/2505.08052.pdf", "abs": "https://arxiv.org/abs/2505.08052", "title": "NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh"], "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study formalizes a computational model to simulate classical Persian\npoets' dynamics of influence through constructing a multi-dimensional\nsimilarity network. Using a rigorously curated dataset based on Ganjoor's\ncorpus, we draw upon semantic, lexical, stylistic, thematic, and metrical\nfeatures to demarcate each poet's corpus. Each is contained within weighted\nsimilarity matrices, which are then appended to generate an aggregate graph\nshowing poet-to-poet influence. Further network investigation is carried out to\nidentify key poets, style hubs, and bridging poets by calculating degree,\ncloseness, betweenness, eigenvector, and Katz centrality measures. Further, for\ntypological insight, we use the Louvain community detection algorithm to\ndemarcate clusters of poets sharing both style and theme coherence, which\ncorrespond closely to acknowledged schools of literature like Sabk-e Hindi,\nSabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a\nnew data-driven view of Persian literature distinguished between canonical\nsignificance and interextual influence, thus highlighting relatively\nlesser-known figures who hold great structural significance. Combining\ncomputational linguistics with literary study, this paper produces an\ninterpretable and scalable model for poetic tradition, enabling retrospective\nreflection as well as forward-looking research within digital humanities."}
{"id": "2505.08080", "pdf": "https://arxiv.org/pdf/2505.08080.pdf", "abs": "https://arxiv.org/abs/2505.08080", "title": "Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders", "authors": ["Dong Shu", "Xuansheng Wu", "Haiyan Zhao", "Mengnan Du", "Ninghao Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 3 figures", "summary": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information."}
{"id": "2505.08137", "pdf": "https://arxiv.org/pdf/2505.08137.pdf", "abs": "https://arxiv.org/abs/2505.08137", "title": "Large Language Models for Computer-Aided Design: A Survey", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Siew-Kei Lam", "Tuan Ngo"], "categories": ["cs.LG", "cs.CL", "cs.GR", "cs.MM"], "comment": null, "summary": "Large Language Models (LLMs) have seen rapid advancements in recent years,\nwith models like ChatGPT and DeepSeek, showcasing their remarkable capabilities\nacross diverse domains. While substantial research has been conducted on LLMs\nin various fields, a comprehensive review focusing on their integration with\nComputer-Aided Design (CAD) remains notably absent. CAD is the industry\nstandard for 3D modeling and plays a vital role in the design and development\nof products across different industries. As the complexity of modern designs\nincreases, the potential for LLMs to enhance and streamline CAD workflows\npresents an exciting frontier. This article presents the first systematic\nsurvey exploring the intersection of LLMs and CAD. We begin by outlining the\nindustrial significance of CAD, highlighting the need for AI-driven innovation.\nNext, we provide a detailed overview of the foundation of LLMs. We also examine\nboth closed-source LLMs as well as publicly available models. The core of this\nreview focuses on the various applications of LLMs in CAD, providing a taxonomy\nof six key areas where these models are making considerable impact. Finally, we\npropose several promising future directions for further advancements, which\noffer vast opportunities for innovation and are poised to shape the future of\nCAD technology. Github:\nhttps://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy"}
{"id": "2505.08148", "pdf": "https://arxiv.org/pdf/2505.08148.pdf", "abs": "https://arxiv.org/abs/2505.08148", "title": "A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem", "authors": ["Sunday Oyinlola Ogundoyin", "Muhammad Ikram", "Hassan Jameel Asghar", "Benjamin Zi Hao Zhao", "Dali Kaafar"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Millions of users leverage generative pretrained transformer (GPT)-based\nlanguage models developed by leading model providers for a wide range of tasks.\nTo support enhanced user interaction and customization, many platforms-such as\nOpenAI-now enable developers to create and publish tailored model instances,\nknown as custom GPTs, via dedicated repositories or application stores. These\ncustom GPTs empower users to browse and interact with specialized applications\ndesigned to meet specific needs. However, as custom GPTs see growing adoption,\nconcerns regarding their security vulnerabilities have intensified. Existing\nresearch on these vulnerabilities remains largely theoretical, often lacking\nempirical, large-scale, and statistically rigorous assessments of associated\nrisks.\n  In this study, we analyze 14,904 custom GPTs to assess their susceptibility\nto seven exploitable threats, such as roleplay-based attacks, system prompt\nleakage, phishing content generation, and malicious code synthesis, across\nvarious categories and popularity tiers within the OpenAI marketplace. We\nintroduce a multi-metric ranking system to examine the relationship between a\ncustom GPT's popularity and its associated security risks.\n  Our findings reveal that over 95% of custom GPTs lack adequate security\nprotections. The most prevalent vulnerabilities include roleplay-based\nvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing\n(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit\ninherent security weaknesses, which are often inherited or amplified in custom\nGPTs. These results highlight the urgent need for enhanced security measures\nand stricter content moderation to ensure the safe deployment of GPT-based\napplications."}
{"id": "2505.08203", "pdf": "https://arxiv.org/pdf/2505.08203.pdf", "abs": "https://arxiv.org/abs/2505.08203", "title": "Not that Groove: Zero-Shot Symbolic Music Editing", "authors": ["Li Zhang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Most work in AI music generation focused on audio, which has seen limited use\nin the music production industry due to its rigidity. To maximize flexibility\nwhile assuming only textual instructions from producers, we are among the first\nto tackle symbolic music editing. We circumvent the known challenge of lack of\nlabeled data by proving that LLMs with zero-shot prompting can effectively edit\ndrum grooves. The recipe of success is a creatively designed format that\ninterfaces LLMs and music, while we facilitate evaluation by providing an\nevaluation dataset with annotated unit tests that highly aligns with musicians'\njudgment."}
{"id": "2505.08445", "pdf": "https://arxiv.org/pdf/2505.08445.pdf", "abs": "https://arxiv.org/abs/2505.08445", "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency", "authors": ["Adel Ammar", "Anis Koubaa", "Omer Nacar", "Wadii Boulila"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models achieve high task performance yet often hallucinate or\nrely on outdated knowledge. Retrieval-augmented generation (RAG) addresses\nthese gaps by coupling generation with external search. We analyse how\nhyperparameters influence speed and quality in RAG systems, covering Chroma and\nFaiss vector stores, chunking policies, cross-encoder re-ranking, and\ntemperature, and we evaluate six metrics: faithfulness, answer correctness,\nanswer relevancy, context precision, context recall, and answer similarity.\nChroma processes queries 13% faster, whereas Faiss yields higher retrieval\nprecision, revealing a clear speed-accuracy trade-off. Naive fixed-length\nchunking with small windows and minimal overlap outperforms semantic\nsegmentation while remaining the quickest option. Re-ranking provides modest\ngains in retrieval quality yet increases runtime by roughly a factor of 5, so\nits usefulness depends on latency constraints. These results help practitioners\nbalance computational cost and accuracy when tuning RAG systems for\ntransparent, up-to-date responses. Finally, we re-evaluate the top\nconfigurations with a corrective RAG workflow and show that their advantages\npersist when the model can iteratively request additional evidence. We obtain a\nnear-perfect context precision (99%), which demonstrates that RAG systems can\nachieve extremely high retrieval accuracy with the right combination of\nhyperparameters, with significant implications for applications where retrieval\nquality directly impacts downstream task performance, such as clinical decision\nsupport in healthcare."}
{"id": "2505.08622", "pdf": "https://arxiv.org/pdf/2505.08622.pdf", "abs": "https://arxiv.org/abs/2505.08622", "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models", "authors": ["Donghoon Kim", "Minji Bae", "Kyuhong Shim", "Byonghyo Shim"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models."}
{"id": "2505.08638", "pdf": "https://arxiv.org/pdf/2505.08638.pdf", "abs": "https://arxiv.org/abs/2505.08638", "title": "TRAIL: Trace Reasoning and Agentic Issue Localization", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Jitin Krishnan", "Anand Kannappan", "Rebecca Qian"], "categories": ["cs.AI", "cs.CL"], "comment": "Dataset link: https://huggingface.co/datasets/PatronusAI/TRAIL", "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."}
{"id": "2505.08704", "pdf": "https://arxiv.org/pdf/2505.08704.pdf", "abs": "https://arxiv.org/abs/2505.08704", "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs", "authors": ["K M Sajjadul Islam", "Ayesha Siddika Nipu", "Jiawei Wu", "Praveen Madiraju"], "categories": ["cs.AI", "cs.CL"], "comment": "IEEE 26th International Conference on Information Reuse and\n  Integration for Data Science (IRI 2025), San Jose, CA, USA", "summary": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting."}
{"id": "2505.08727", "pdf": "https://arxiv.org/pdf/2505.08727.pdf", "abs": "https://arxiv.org/abs/2505.08727", "title": "Memorization-Compression Cycles Improve Generalization", "authors": ["Fangyuan Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": "12 pages, 6 figures", "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation."}
{"id": "2505.08783", "pdf": "https://arxiv.org/pdf/2505.08783.pdf", "abs": "https://arxiv.org/abs/2505.08783", "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation", "authors": ["Shanda Li", "Tanya Marwah", "Junhong Shen", "Weiwei Sun", "Andrej Risteski", "Yiming Yang", "Ameet Talwalkar"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": null, "summary": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE."}
{"id": "2310.12059", "pdf": "https://arxiv.org/pdf/2310.12059.pdf", "abs": "https://arxiv.org/abs/2310.12059", "title": "Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education", "authors": ["Duc-Vu Nguyen", "Quoc-Nam Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at SoICT 2023", "summary": "In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only."}
{"id": "2408.06787", "pdf": "https://arxiv.org/pdf/2408.06787.pdf", "abs": "https://arxiv.org/abs/2408.06787", "title": "Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions", "authors": ["Bo Xue", "Yi Xu", "Yunchong Song", "Yiming Pang", "Yuyang Ren", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large\ncorpora with powerful context modeling, making them promising for mitigating\nthe limitations of previous methods. Directly fine-tuning LLMs offers great\ncapability but comes at the cost of huge time and memory consumption, while\nutilizing frozen LLMs yields suboptimal results.In this work, we aim to\nleverage LLMs for KGC effectively and efficiently. We capture the context-aware\nhidden states of knowledge triples by employing prompts to stimulate the\nintermediate layers of LLMs. We then train a data-efficient classifier on these\nhidden states to harness the inherent capabilities of frozen LLMs in KGC.\nAdditionally, to reduce ambiguity and enrich knowledge representation, we\ngenerate detailed entity descriptions through subgraph sampling on KGs.\nExtensive experiments on standard benchmarks demonstrate the efficiency and\neffectiveness of our approach. We outperform traditional KGC methods across\nmost datasets and, notably, achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $13.48\\times$."}
{"id": "2408.09030", "pdf": "https://arxiv.org/pdf/2408.09030.pdf", "abs": "https://arxiv.org/abs/2408.09030", "title": "Studying the Effects of Collaboration in Interactive Theme Discovery Systems", "authors": ["Alvin Po-Chun Chen", "Dananjay Srinivas", "Alexandra Barry", "Maksim Seniw", "Maria Leonor Pacheco"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "NLP-assisted solutions have gained considerable traction to support\nqualitative data analysis. However, there does not exist a unified evaluation\nframework that can account for the many different settings in which qualitative\nresearchers may employ them. In this paper, we take a first step in this\ndirection by proposing an evaluation framework to study the way in which\ndifferent tools may result in different outcomes depending on the collaboration\nstrategy employed. Specifically, we study the impact of synchronous vs.\nasynchronous collaboration using two different NLP-assisted qualitative\nresearch tools and present a comprehensive analysis of significant differences\nin the consistency, cohesiveness, and correctness of their outputs."}
{"id": "2409.04168", "pdf": "https://arxiv.org/pdf/2409.04168.pdf", "abs": "https://arxiv.org/abs/2409.04168", "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks", "authors": ["Andreas Stephan", "Dawei Zhu", "Matthias Aßenmacher", "Xiaoyu Shen", "Benjamin Roth"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. The\nperformance of LLM judges is typically evaluated by measuring the correlation\nwith human judgments on generative tasks such as summarization or machine\ntranslation. In contrast, we study LLM judges on mathematical reasoning tasks.\nThese tasks require multi-step reasoning, and the correctness of their\nsolutions is verifiable, enabling a more objective evaluation. We perform a\ndetailed performance analysis and find that easy samples are easy to judge, and\ndifficult samples are difficult to judge. Our analysis uncovers a strong\ncorrelation between judgment performance and the candidate model task\nperformance, indicating that judges tend to favor higher-quality models even if\ntheir answer is incorrect. As a consequence, we test whether we can predict the\nbehavior of LLM judges using simple features such as part-of-speech tags and\nfind that we can correctly predict 70%-75% of judgments. We conclude this study\nby analyzing practical use cases, showing that LLM judges consistently detect\nthe on-average better model but largely fail if we use them to improve task\nperformance."}
{"id": "2410.06205", "pdf": "https://arxiv.org/pdf/2410.06205.pdf", "abs": "https://arxiv.org/abs/2410.06205", "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?", "authors": ["Federico Barbero", "Alex Vitvitskyi", "Christos Perivolaropoulos", "Razvan Pascanu", "Petar Veličković"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths."}
{"id": "2410.07002", "pdf": "https://arxiv.org/pdf/2410.07002.pdf", "abs": "https://arxiv.org/abs/2410.07002", "title": "CursorCore: Assist Programming through Aligning Anything", "authors": ["Hao Jiang", "Qi Liu", "Rui Li", "Shengyu Ye", "Shijin Wang"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore."}
{"id": "2412.05342", "pdf": "https://arxiv.org/pdf/2412.05342.pdf", "abs": "https://arxiv.org/abs/2412.05342", "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation", "authors": ["Xiaoyu Wang", "Ningyuan Xi", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Xiaokai Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe."}
{"id": "2412.20299", "pdf": "https://arxiv.org/pdf/2412.20299.pdf", "abs": "https://arxiv.org/abs/2412.20299", "title": "No Preference Left Behind: Group Distributional Preference Optimization", "authors": ["Binwei Yao", "Zefan Cai", "Yun-Shiuan Chuang", "Shanglin Yang", "Ming Jiang", "Diyi Yang", "Junjie Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Preferences within a group of people are not uniform but follow a\ndistribution. While existing alignment methods like Direct Preference\nOptimization (DPO) attempt to steer models to reflect human preferences, they\nstruggle to capture the distributional pluralistic preferences within a group.\nThese methods often skew toward dominant preferences, overlooking the diversity\nof opinions, especially when conflicting preferences arise. To address this\nissue, we propose Group Distributional Preference Optimization (GDPO), a novel\nframework that aligns language models with the distribution of preferences\nwithin a group by incorporating the concept of beliefs that shape individual\npreferences. GDPO calibrates a language model using statistical estimation of\nthe group's belief distribution and aligns the model with belief-conditioned\npreferences, offering a more inclusive alignment framework than traditional\nmethods. In experiments using both synthetic controllable opinion generation\nand real-world movie review datasets, we show that DPO fails to align with the\ntargeted belief distributions, while GDPO consistently reduces this alignment\ngap during training. Moreover, our evaluation metrics demonstrate that GDPO\noutperforms existing approaches in aligning with group distributional\npreferences, marking a significant advance in pluralistic alignment."}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917.pdf", "abs": "https://arxiv.org/abs/2501.14917", "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance."}
{"id": "2502.01597", "pdf": "https://arxiv.org/pdf/2502.01597.pdf", "abs": "https://arxiv.org/abs/2502.01597", "title": "FutureVision: A methodology for the investigation of future cognition", "authors": ["Tiago Timponi Torrent", "Mark Turner", "Nicolás Hinrichs", "Frederico Belcavello", "Igor Lourenço", "Arthur Lorenzi Almeida", "Marcelo Viridiano", "Ely Edison Matos"], "categories": ["cs.CL"], "comment": "Paper accepted at CogSci 2025", "summary": "This paper presents a methodology combining multimodal semantic analysis with\nan eye-tracking experimental protocol to investigate the cognitive effort\ninvolved in understanding the communication of future scenarios. To demonstrate\nthe methodology, we conduct a pilot study examining how visual fixation\npatterns vary during the evaluation of valence and counterfactuality in\nfictional ad pieces describing futuristic scenarios, using a portable eye\ntracker. Participants eye movements are recorded while evaluating the stimuli\nand describing them to a conversation partner. Gaze patterns are analyzed\nalongside semantic representations of the stimuli and participants\ndescriptions, constructed from a frame semantic annotation of both linguistic\nand visual modalities. Preliminary results show that far-future and pessimistic\nscenarios are associated with longer fixations and more erratic saccades,\nsupporting the hypothesis that fractures in the base spaces underlying the\ninterpretation of future scenarios increase cognitive load for comprehenders."}
{"id": "2502.04066", "pdf": "https://arxiv.org/pdf/2502.04066.pdf", "abs": "https://arxiv.org/abs/2502.04066", "title": "SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals", "authors": ["Changhao Jiang", "Ming Zhang", "Junjie Ye", "Xiaoran Fan", "Yifei Cao", "Jiajun Sun", "Zhiheng Xi", "Shihan Dou", "Yi Dong", "Yujiong Shen", "Jingqi Tong", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The GPT-4 technical report highlights the possibility of predicting model\nperformance on downstream tasks using only pre-training signals, though\ndetailed methodologies are absent. Such predictive capabilities are essential\nfor resource-efficient pre-training and the construction of task-aligned\ndatasets. In this paper, we aim to predict performance in closed-book question\nanswering (QA), a vital downstream task indicative of a model's internal\nknowledge. We address three primary challenges: (1) limited access to and\nunderstanding of pre-training corpora, (2) limitations of current evaluation\nmethods for pre-trained models, and (3) limitations of frequency-based metrics\nin predicting model performance. In response to these challenges, we conduct\nlarge-scale retrieval and semantic analysis across the pre-training corpora of\n21 publicly available and 3 custom-trained large language models. Subsequently,\nwe develop a multi-template QA evaluation framework incorporating paraphrased\nquestion variants. Building on these foundations, we propose Size-dependent\nMutual Information (SMI), an information-theoretic metric that linearly\ncorrelates pre-training data characteristics, model size, and QA accuracy,\nwithout requiring any additional training. The experimental results demonstrate\nthat SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on\nmodels with over one billion parameters. Theoretical analysis further reveals\nthe marginal benefits of scaling model size and optimizing data, indicating\nthat the upper limit of specific QA task accuracy is approximately 80%. Our\nproject is available at https://github.com/yuhui1038/SMI."}
{"id": "2502.18679", "pdf": "https://arxiv.org/pdf/2502.18679.pdf", "abs": "https://arxiv.org/abs/2502.18679", "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data", "authors": ["Siqi Guo", "Ilgee Hong", "Vicente Balmaseda", "Changlong Yu", "Liang Qiu", "Xin Liu", "Haoming Jiang", "Tuo Zhao", "Tianbao Yang"], "categories": ["cs.CL"], "comment": "18 pages, 7 figures", "summary": "Supervised fine-tuning (SFT) has become a crucial step for aligning\npretrained large language models (LLMs) using supervised datasets of\ninput-output pairs. However, despite being supervised, SFT is inherently\nlimited by its generative training objective. To address its limitations, the\nexisting common strategy is to follow SFT with a separate phase of preference\noptimization (PO), which relies on either human-labeled preference data or a\nstrong reward model to guide the learning process. In this paper, we address\nthe limitations of SFT by exploring one of the most successful techniques in\nconventional supervised learning: discriminative learning. We introduce\nDiscriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates\nthe burden of collecting human-labeled preference data or training strong\nreward models. Unlike SFT that employs a generative approach and overlooks\nnegative data, DFT adopts a discriminative paradigm that increases the\nprobability of positive answers while suppressing potentially negative ones,\naiming for data prediction instead of token prediction. Our contributions\ninclude: (i) a discriminative probabilistic framework for fine-tuning LLMs by\nexplicitly modeling the discriminative likelihood of an answer among all\npossible outputs given an input; (ii) efficient algorithms to optimize this\ndiscriminative likelihood; and (iii) extensive experiments demonstrating DFT's\neffectiveness, achieving performance better than SFT and comparable to if not\nbetter than SFT$\\rightarrow$PO. The code can be found at\nhttps://github.com/Optimization-AI/DFT."}
{"id": "2503.01844", "pdf": "https://arxiv.org/pdf/2503.01844.pdf", "abs": "https://arxiv.org/abs/2503.01844", "title": "Can (A)I Change Your Mind?", "authors": ["Miriam Havin", "Timna Wharton Kleinman", "Moran Koren", "Yaniv Dover", "Ariel Goldstein"], "categories": ["cs.CL"], "comment": "Accetped to CogSci 2025", "summary": "The increasing integration of large language models (LLMs) based\nconversational agents into everyday life raises critical cognitive and social\nquestions about their potential to influence human opinions. Although previous\nstudies have shown that LLM-based agents can generate persuasive content, these\ntypically involve controlled English-language settings. Addressing this, our\npreregistered study explored LLMs' persuasive capabilities in more ecological,\nunconstrained scenarios, examining both static (written paragraphs) and dynamic\n(conversations via Telegram) interaction types. Conducted entirely in Hebrew\nwith 200 participants, the study assessed the persuasive effects of both LLM\nand human interlocutors on controversial civil policy topics. Results indicated\nthat participants adopted LLM and human perspectives similarly, with\nsignificant opinion changes evident across all conditions, regardless of\ninterlocutor type or interaction mode. Confidence levels increased\nsignificantly in most scenarios. These findings demonstrate LLM-based agents'\nrobust persuasive capabilities across diverse sources and settings,\nhighlighting their potential impact on shaping public opinions."}
{"id": "2503.04830", "pdf": "https://arxiv.org/pdf/2503.04830.pdf", "abs": "https://arxiv.org/abs/2503.04830", "title": "Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents", "authors": ["Jingying Zeng", "Hui Liu", "Zhenwei Dai", "Xianfeng Tang", "Chen Luo", "Samarth Varshney", "Zhen Li", "Qi He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the advancement of conversational large language models (LLMs), several\nLLM-based Conversational Shopping Agents (CSA) have been developed to help\ncustomers smooth their online shopping. The primary objective in building an\nengaging and trustworthy CSA is to ensure the agent's responses about product\nfactoids are accurate and factually grounded. However, two challenges remain.\nFirst, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk\nspreading misinformation and diminishing customer trust. Second, without\nproviding knowledge source attribution in CSA response, customers struggle to\nverify LLM-generated information. To address both challenges, we present an\neasily productionized solution that enables a ''citation experience'' to our\ncustomers. We build auto-evaluation metrics to holistically evaluate LLM's\ngrounding and attribution capabilities, suggesting that citation generation\nparadigm substantially improves grounding performance by 13.83%. To deploy this\ncapability at scale, we introduce Multi-UX-Inference system, which appends\nsource citations to LLM outputs while preserving existing user experience\nfeatures and supporting scalable inference. Large-scale online A/B tests show\nthat grounded CSA responses improves customer engagement by 3% - 10%, depending\non UX variations."}
{"id": "2503.13517", "pdf": "https://arxiv.org/pdf/2503.13517.pdf", "abs": "https://arxiv.org/abs/2503.13517", "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning", "authors": ["Hao Cui", "Zahra Shamsi", "Gowoon Cheon", "Xuejian Ma", "Shutong Li", "Maria Tikhanovskaya", "Peter Norgaard", "Nayantara Mudur", "Martyna Plomecka", "Paul Raccuglia", "Yasaman Bahri", "Victor V. Albert", "Pranesh Srinivasan", "Haining Pan", "Philippe Faist", "Brian Rohr", "Ekin Dogus Cubuk", "Muratahan Aykol", "Amil Merchant", "Michael J. Statt", "Dan Morris", "Drew Purves", "Elise Kleeman", "Ruth Alcantara", "Matthew Abraham", "Muqthar Mohammad", "Ean Phing VanLee", "Chenfei Jiang", "Elizabeth Dorfman", "Eun-Ah Kim", "Michael P Brenner", "Viren Jain", "Sameera Ponda", "Subhashini Venugopalan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICLR 2025 main conference", "summary": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie"}
{"id": "2503.24027", "pdf": "https://arxiv.org/pdf/2503.24027.pdf", "abs": "https://arxiv.org/abs/2503.24027", "title": "Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes", "authors": ["Florian Carichon", "Romain Rampa", "Golnoosh Farnadi"], "categories": ["cs.CL"], "comment": "Updated to match the version accepted at ACM FAccT 2025. Includes\n  revised text and results", "summary": "Novelty modeling and detection is a core topic in Natural Language Processing\n(NLP), central to numerous tasks such as recommender systems and automatic\nsummarization. It involves identifying pieces of text that deviate in some way\nfrom previously known information. However, novelty is also a crucial\ndeterminant of the unique perception of relevance and quality of an experience,\nas it rests upon each individual's understanding of the world. Social factors,\nparticularly cultural background, profoundly influence perceptions of novelty\nand innovation. Cultural novelty arises from differences in salience and\nnovelty as shaped by the distance between distinct communities. While cultural\ndiversity has garnered increasing attention in artificial intelligence (AI),\nthe lack of robust metrics for quantifying cultural novelty hinders a deeper\nunderstanding of these divergences. This gap limits quantifying and\nunderstanding cultural differences within computational frameworks. To address\nthis, we propose an interdisciplinary framework that integrates knowledge from\nsociology and management. Central to our approach is GlobalFusion, a novel\ndataset comprising 500 dishes and approximately 100,000 cooking recipes\ncapturing cultural adaptation from over 150 countries. By introducing a set of\nJensen-Shannon Divergence metrics for novelty, we leverage this dataset to\nanalyze textual divergences when recipes from one community are modified by\nanother with a different cultural background. The results reveal significant\ncorrelations between our cultural novelty metrics and established cultural\nmeasures based on linguistic, religious, and geographical distances. Our\nfindings highlight the potential of our framework to advance the understanding\nand measurement of cultural diversity in AI."}
{"id": "2504.02732", "pdf": "https://arxiv.org/pdf/2504.02732.pdf", "abs": "https://arxiv.org/abs/2504.02732", "title": "Why do LLMs attend to the first token?", "authors": ["Federico Barbero", "Álvaro Arroyo", "Xiangming Gu", "Christos Perivolaropoulos", "Michael Bronstein", "Petar Veličković", "Razvan Pascanu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."}
{"id": "2504.02870", "pdf": "https://arxiv.org/pdf/2504.02870.pdf", "abs": "https://arxiv.org/abs/2504.02870", "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening", "authors": ["Frank P. -W. Lo", "Jianing Qiu", "Zeyu Wang", "Haibao Yu", "Yeming Chen", "Gao Zhang", "Benny Lo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by CVPR 2025 Workshop", "summary": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows."}
{"id": "2504.04717", "pdf": "https://arxiv.org/pdf/2504.04717.pdf", "abs": "https://arxiv.org/abs/2504.04717", "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models", "authors": ["Yubo Li", "Xiaobin Shen", "Xinyu Yao", "Xueying Ding", "Yidi Miao", "Ramayya Krishnan", "Rema Padman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."}
{"id": "2504.07128", "pdf": "https://arxiv.org/pdf/2504.07128.pdf", "abs": "https://arxiv.org/abs/2504.07128", "title": "DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning", "authors": ["Sara Vera Marjanović", "Arkil Patel", "Vaibhav Adlakha", "Milad Aghajohari", "Parishad BehnamGhader", "Mehar Bhatia", "Aditi Khandelwal", "Austin Kraft", "Benno Krojer", "Xing Han Lù", "Nicholas Meade", "Dongchan Shin", "Amirhossein Kazemnejad", "Gaurav Kamath", "Marius Mosbach", "Karolina Stańczak", "Siva Reddy"], "categories": ["cs.CL"], "comment": "142 pages, pre-print", "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs."}
{"id": "2504.16408", "pdf": "https://arxiv.org/pdf/2504.16408.pdf", "abs": "https://arxiv.org/abs/2504.16408", "title": "LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation", "authors": ["Jiahao Yuan", "Xingzhe Sun", "Xing Yu", "Jingwen Wang", "Dehui Du", "Zhiqing Cui", "Zixiang Di"], "categories": ["cs.CL"], "comment": "XLLM @ ACL 2025 Shared Task-III: LLM for Structural Reasoning\n  (LLM-SR)", "summary": "The LLMSR@XLLM25 formulates a low-resource structural reasoning task that\nchallenges LLMs to generate interpretable, step-by-step rationales with minimal\nlabeled data. We present Less is More, the third-place winning approach in the\nLLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled\nexamples. Our approach leverages a multi-agent framework with reverse-prompt\ninduction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage\nreward-guided filtering to distill high-quality supervision across three\nsubtasks: question parsing, CoT parsing, and step-level verification. All\nmodules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+\nsetup. By combining structure validation with reward filtering across few-shot\nand zero-shot prompts, our pipeline consistently improves structure reasoning\nquality. These results underscore the value of controllable data distillation\nin enhancing structured inference under low-resource constraints. Our code is\navailable at https://github.com/JhCircle/Less-is-More."}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565.pdf", "abs": "https://arxiv.org/abs/2504.17565", "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}"}
{"id": "2504.21625", "pdf": "https://arxiv.org/pdf/2504.21625.pdf", "abs": "https://arxiv.org/abs/2504.21625", "title": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-following Ability", "authors": ["Jiaming Wang", "Yunke Zhao", "Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose \\textbf{Meeseeks} (named after Mr. Meeseeks from \\textit{Rick\nand Morty}\\footnote{Rick and Morty is an American adult animated science\nfiction sitcom created by Justin Roiland and Dan Harmon for Cartoon Network's\nnighttime programming block Adult Swim.}.) Meeseeks simulates realistic\nhuman-LLM interactions through an iterative feedback framework, which enables\nmodels to self-correct based on specific requirement failures in each turn,\nbetter reflecting real-world user-end usage patterns. Meanwhile, the benchmark\nimplements a comprehensive evaluation system with 38 capability tags organized\nacross three dimensions: Intent Recognition, Granular Content Validation, and\nOutput Structure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios."}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039.pdf", "abs": "https://arxiv.org/abs/2505.00039", "title": "Graph RAG for Legal Norms: A Hierarchical and Temporal Approach", "authors": ["Hudson de Martim"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nadvance the field of Artificial Intelligence applied to Law, creating\nopportunities for more effective systems in legal research, legislative\nanalysis, and decision support."}
{"id": "2505.01731", "pdf": "https://arxiv.org/pdf/2505.01731.pdf", "abs": "https://arxiv.org/abs/2505.01731", "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models", "authors": ["Chuan Sun", "Han Yu", "Lizhen Cui", "Xiaoxiao Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pruning large language models (LLMs) is a promising solution for reducing\nmodel sizes and computational complexity while preserving performance.\nTraditional layer-wise pruning methods often adopt a uniform sparsity approach\nacross all layers, which leads to suboptimal performance due to the varying\nsignificance of individual transformer layers within the model not being\naccounted for. To this end, we propose the Shapley Value-based Non-Uniform\nPruning (SV-NUP) method for LLMs. This approach quantifies the contribution of\neach transformer layer to the overall model performance, enabling the\nassignment of tailored pruning budgets to different layers to retain critical\nparameters. To further improve efficiency, we design the Sliding Window-based\nShapley Value approximation method. It substantially reduces computational\noverhead compared to exact SV calculation methods. Extensive experiments on\nvarious LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness\nof the proposed approach. The results reveal that non-uniform pruning\nsignificantly enhances the performance of pruned models. Notably, SV-NUP\nachieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and\nLLaMA-13B, respectively, compared to SparseGPT at 70% sparsity."}
{"id": "2505.03688", "pdf": "https://arxiv.org/pdf/2505.03688.pdf", "abs": "https://arxiv.org/abs/2505.03688", "title": "IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages", "authors": ["Sharvi Endait", "Ruturaj Ghatage", "Aditya Kulkarni", "Rajlaxmi Patil", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress in question-answering (QA) systems has predominantly\nbenefited high-resource languages, leaving Indic languages largely\nunderrepresented despite their vast native speaker base. In this paper, we\npresent IndicSQuAD, a comprehensive multi-lingual extractive QA dataset\ncovering nine major Indic languages, systematically derived from the SQuAD\ndataset. Building on previous work with MahaSQuAD for Marathi, our approach\nadapts and extends translation techniques to maintain high linguistic fidelity\nand accurate answer-span alignment across diverse languages. IndicSQuAD\ncomprises extensive training, validation, and test sets for each language,\nproviding a robust foundation for model development. We evaluate baseline\nperformances using language-specific monolingual BERT models and the\nmultilingual MuRIL-BERT. The results indicate some challenges inherent in\nlow-resource settings. Moreover, our experiments suggest potential directions\nfor future work, including expanding to additional languages, developing\ndomain-specific datasets, and incorporating multimodal data. The dataset and\nmodels are publicly shared at https://github.com/l3cube-pune/indic-nlp"}
{"id": "2505.06186", "pdf": "https://arxiv.org/pdf/2505.06186.pdf", "abs": "https://arxiv.org/abs/2505.06186", "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies", "authors": ["Massimiliano Pronesti", "Joao Bettencourt-Silva", "Paul Flanagan", "Alessandra Pascale", "Oisin Redmond", "Anya Belz", "Yufang Hou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems."}
{"id": "2505.07672", "pdf": "https://arxiv.org/pdf/2505.07672.pdf", "abs": "https://arxiv.org/abs/2505.07672", "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit", "authors": ["Arun S. Maiya"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages", "summary": "We present OnPrem$.$LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,\nvLLM, and Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem$.$LLM also supports integration with a wide range of cloud\nLLM providers when permitted, enabling hybrid deployments that balance\nperformance with data control. A no-code web interface extends accessibility to\nnon-technical users."}
{"id": "2505.07705", "pdf": "https://arxiv.org/pdf/2505.07705.pdf", "abs": "https://arxiv.org/abs/2505.07705", "title": "Codifying Character Logic in Role-Playing", "authors": ["Letian Peng", "Jingbo Shang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces Codified Profiles for role-playing, a novel approach\nthat represents character logic as structured, executable functions for\nbehavioral decision-making. Each profile defines a set of functions\nparse_by_scene(scene) that outputs a list of logic-grounded assertions\ntriggered_statements, using both explicit control structures (e.g.,\nif-then-else) and condition checks like check_condition(scene, question), where\neach question is a semantically meaningful prompt about the scene (e.g., \"Is\nthe character in danger?\") discriminated by the role-playing LLM as true,\nfalse, or unknown. This explicit representation offers three key advantages\nover traditional prompt-based profiles, which append character descriptions\ndirectly into text prompts: (1) Persistence, by enforcing complete and\nconsistent execution of character logic, rather than relying on the model's\nimplicit reasoning; (2) Updatability, through systematic inspection and\nrevision of behavioral logic, which is difficult to track or debug in\nprompt-only approaches; (3) Controllable Randomness, by supporting stochastic\nbehavior directly within the logic, enabling fine-grained variability that\nprompting alone struggles to achieve. To validate these advantages, we\nintroduce a new benchmark constructed from 83 characters and 5,141 scenes\ncurated from Fandom, using NLI-based scoring to compare character responses\nagainst ground-truth actions. Our experiments demonstrate the significant\nbenefits of codified profiles in improving persistence, updatability, and\nbehavioral diversity. Notably, by offloading a significant portion of reasoning\nto preprocessing, codified profiles enable even 1B-parameter models to perform\nhigh-quality role-playing, providing a scalable and efficient foundation for\nlocal deployment of role-play agents."}
{"id": "2410.13757", "pdf": "https://arxiv.org/pdf/2410.13757.pdf", "abs": "https://arxiv.org/abs/2410.13757", "title": "MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation", "authors": ["Zichen Zhu", "Hao Tang", "Yansi Li", "Dingye Liu", "Hongshen Xu", "Kunyao Lan", "Danyang Zhang", "Yixuan Jiang", "Hao Zhou", "Chenrun Wang", "Situo Zhang", "Liangtai Sun", "Yixiao Wang", "Yuheng Sun", "Lu Chen", "Kai Yu"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC"], "comment": "NAACL 2025 Demo Track [code] https://github.com/OpenDFM/MobA\n  [dataset] https://huggingface.co/datasets/OpenDFM/MobA-MobBench", "summary": "Existing Multimodal Large Language Model (MLLM)-based agents face significant\nchallenges in handling complex GUI (Graphical User Interface) interactions on\ndevices. These challenges arise from the dynamic and structured nature of GUI\nenvironments, which integrate text, images, and spatial relationships, as well\nas the variability in action spaces across different pages and tasks. To\naddress these limitations, we propose MobA, a novel MLLM-based mobile assistant\nsystem. MobA introduces an adaptive planning module that incorporates a\nreflection mechanism for error recovery and dynamically adjusts plans to align\nwith the real environment contexts and action module's execution capacity.\nAdditionally, a multifaceted memory module provides comprehensive memory\nsupport to enhance adaptability and efficiency. We also present MobBench, a\ndataset designed for complex mobile interactions. Experimental results on\nMobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI\nenvironments and perform complex mobile tasks."}
{"id": "2501.00958", "pdf": "https://arxiv.org/pdf/2501.00958.pdf", "abs": "https://arxiv.org/abs/2501.00958", "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "authors": ["Wenqi Zhang", "Hang Zhang", "Xin Li", "Jiashuo Sun", "Yongliang Shen", "Weiming Lu", "Deli Zhao", "Yueting Zhuang", "Lidong Bing"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Under review", "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook."}
{"id": "2501.02423", "pdf": "https://arxiv.org/pdf/2501.02423.pdf", "abs": "https://arxiv.org/abs/2501.02423", "title": "Scaling Laws for Floating Point Quantization Training", "authors": ["Xingwu Sun", "Shuaipeng Li", "Ruobing Xie", "Weidong Han", "Kan Wu", "Zhen Yang", "Yixing Li", "An Wang", "Shuai Li", "Jinbao Xue", "Yu Cheng", "Yangyu Tao", "Zhanhui Kang", "Chengzhong Xu", "Di Wang", "Jie Jiang"], "categories": ["cs.LG", "cs.AR", "cs.CL"], "comment": null, "summary": "Low-precision training is considered an effective strategy for reducing both\ntraining and downstream inference costs. Previous scaling laws for precision\nmainly focus on integer quantization, which pay less attention to the\nconstituents in floating-point (FP) quantization, and thus cannot well fit the\nLLM losses in this scenario. In contrast, while FP quantization training is\nmore commonly implemented in production, it's research has been relatively\nsuperficial. In this paper, we thoroughly explore the effects of FP\nquantization targets, exponent bits, mantissa bits, and the calculation\ngranularity of the scaling factor in FP quantization training performance of\nLLM models. In addition to an accurate FP quantization unified scaling law, we\nalso provide valuable suggestions for the community: (1) Exponent bits\ncontribute slightly more to the model performance than mantissa bits. We\nprovide the optimal exponent-mantissa bit ratio for different bit numbers,\nwhich is available for future reference by hardware manufacturers; (2) We\ndiscover the formation of the critical data size in low-precision LLM training.\nToo much training data exceeding the critical data size will inversely bring in\ndegradation of LLM performance; (3) The optimal FP quantization precision is\ndirectly proportional to the computational power, but within a wide\ncomputational power range. We estimate that the best cost-performance precision\nshould lie between 4-8 bits."}
{"id": "2501.09425", "pdf": "https://arxiv.org/pdf/2501.09425.pdf", "abs": "https://arxiv.org/abs/2501.09425", "title": "Vision-Language Models Do Not Understand Negation", "authors": ["Kumail Alhamoud", "Shaden Alshammari", "Yonglong Tian", "Guohao Li", "Philip Torr", "Yoon Kim", "Marzyeh Ghassemi"], "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025; project page: https://negbench.github.io", "summary": "Many practical vision-language applications require models that understand\nnegation, e.g., when using natural language to retrieve images which contain\ncertain objects but not others. Despite advancements in vision-language models\n(VLMs) through large-scale training, their ability to comprehend negation\nremains underexplored. This study addresses the question: how well do current\nVLMs understand negation? We introduce NegBench, a new benchmark designed to\nevaluate negation understanding across 18 task variations and $79$k examples\nspanning image, video, and medical datasets. The benchmark consists of two core\ntasks designed to evaluate negation understanding in diverse multimodal\nsettings: Retrieval with Negation and Multiple Choice Questions with Negated\nCaptions. Our evaluation reveals that modern VLMs struggle significantly with\nnegation, often performing at chance level. To address these shortcomings, we\nexplore a data-centric approach wherein we finetune CLIP models on large-scale\nsynthetic datasets containing millions of negated captions. We show that this\napproach can result in a 10% increase in recall on negated queries and a 28%\nboost in accuracy on multiple-choice questions with negated captions."}
{"id": "2501.15857", "pdf": "https://arxiv.org/pdf/2501.15857.pdf", "abs": "https://arxiv.org/abs/2501.15857", "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "authors": ["Yutong Yin", "Zhaoran Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing."}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742.pdf", "abs": "https://arxiv.org/abs/2503.22742", "title": "Adaptive Integrated Layered Attention (AILA)", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice."}
{"id": "2503.23083", "pdf": "https://arxiv.org/pdf/2503.23083.pdf", "abs": "https://arxiv.org/abs/2503.23083", "title": "Efficient Adaptation For Remote Sensing Visual Grounding", "authors": ["Hasan Moughnieh", "Mohamad Chalhoub", "Hasan Nasrallah", "Cristiano Nattero", "Paolo Campanella", "Giovanni Nico", "Ali J. Ghandour"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training."}
{"id": "2504.13989", "pdf": "https://arxiv.org/pdf/2504.13989.pdf", "abs": "https://arxiv.org/abs/2504.13989", "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."}
{"id": "2504.14361", "pdf": "https://arxiv.org/pdf/2504.14361.pdf", "abs": "https://arxiv.org/abs/2504.14361", "title": "Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction", "authors": ["Till Rossner", "Ziteng Li", "Jonas Balke", "Nikoo Salehfard", "Tom Seifert", "Ming Tang"], "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "8 pages, 6 figures", "summary": "AI-driven drug response prediction holds great promise for advancing\npersonalized cancer treatment. However, the inherent heterogenity of cancer and\nhigh cost of data generation make accurate prediction challenging. In this\nstudy, we investigate whether incorporating the pretrained foundation model\nscGPT can enhance the performance of existing drug response prediction\nframeworks. Our approach builds on the DeepCDR framework, which encodes drug\nrepresentations from graph structures and cell representations from multi-omics\nprofiles. We adapt this framework by leveraging scGPT to generate enriched cell\nrepresentations using its pretrained knowledge to compensate for limited amount\nof data. We evaluate our modified framework using IC$_{50}$ values on Pearson\ncorrelation coefficient (PCC) and a leave-one-drug out validation strategy,\ncomparing it against the original DeepCDR framework and a prior\nscFoundation-based approach. scGPT not only outperforms previous approaches but\nalso exhibits greater training stability, highlighting the value of leveraging\nscGPT-derived knowledge in this domain."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435.pdf", "abs": "https://arxiv.org/abs/2504.21435", "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\nstandalone videos and mainly assess \"visual elements\" like human actions and\nobject states. In reality, contemporary videos often encompass complex and\ncontinuous narratives, typically presented as a series. To address this\nchallenge, we propose SeriesBench, a benchmark consisting of 105 carefully\ncurated narrative-driven series, covering 28 specialized tasks that require\ndeep narrative understanding. Specifically, we first select a diverse set of\ndrama series spanning various genres. Then, we introduce a novel long-span\nnarrative annotation method, combined with a full-information transformation\napproach to convert manual annotations into diverse task formats. To further\nenhance model capacity for detailed analysis of plot structures and character\nrelationships within series, we propose a novel narrative reasoning framework,\nPC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still\nface significant challenges in understanding narrative-driven series, while\nPC-DCoT enables these MLLMs to achieve performance improvements. Overall, our\nSeriesBench and PC-DCoT highlight the critical necessity of advancing model\ncapabilities to understand narrative-driven series, guiding the future\ndevelopment of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2505.00759", "pdf": "https://arxiv.org/pdf/2505.00759.pdf", "abs": "https://arxiv.org/abs/2505.00759", "title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "authors": ["Jiahui Chen", "Candace Ross", "Reyhane Askari-Hemmat", "Koustuv Sinha", "Melissa Hall", "Michal Drozdzal", "Adriana Romero-Soriano"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The steady improvements of text-to-image (T2I) generative models lead to slow\ndeprecation of automatic evaluation benchmarks that rely on static datasets,\nmotivating researchers to seek alternative ways to evaluate the T2I progress.\nIn this paper, we explore the potential of multi-modal large language models\n(MLLMs) as evaluator agents that interact with a T2I model, with the objective\nof assessing prompt-generation consistency and image aesthetics. We present\nMultimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively\ngenerates prompts for evaluation, scores generated images and matches T2I\nevaluation of existing benchmarks with a fraction of the prompts used in\nexisting static benchmarks. Moreover, we show that MT2IE's prompt-generation\nconsistency scores have higher correlation with human judgment than scores\npreviously introduced in the literature. MT2IE generates prompts that are\nefficient at probing T2I model performance, producing the same relative T2I\nmodel rankings as existing benchmarks while using only 1/80th the number of\nprompts for evaluation."}
{"id": "2505.03054", "pdf": "https://arxiv.org/pdf/2505.03054.pdf", "abs": "https://arxiv.org/abs/2505.03054", "title": "BLAB: Brutally Long Audio Bench", "authors": ["Orevaoghene Ahia", "Martijn Bartelds", "Kabir Ahuja", "Hila Gonen", "Valentin Hofmann", "Siddhant Arora", "Shuyue Stella Li", "Vishal Puttagunta", "Mofetoluwa Adeyemi", "Charishma Buchireddy", "Ben Walls", "Noah Bennett", "Shinji Watanabe", "Noah A. Smith", "Yulia Tsvetkov", "Sachin Kumar"], "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing large audio language models (LMs) capable of understanding diverse\nspoken interactions is essential for accommodating the multimodal nature of\nhuman communication and can increase the accessibility of language technologies\nacross different user populations. Recent work on audio LMs has primarily\nevaluated their performance on short audio segments, typically under 30\nseconds, with limited exploration of long-form conversational speech segments\nthat more closely reflect natural user interactions with these models. We\nintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audio\nbenchmark that evaluates audio LMs on localization, duration estimation,\nemotion, and counting tasks using audio segments averaging 51 minutes in\nlength. BLAB consists of 833+ hours of diverse, full-length audio clips, each\npaired with human-annotated, text-based natural language questions and answers.\nOur audio data were collected from permissively licensed sources and underwent\na human-assisted filtering process to ensure task compliance. We evaluate six\nopen-source and proprietary audio LMs on BLAB and find that all of them,\nincluding advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the\ntasks in BLAB. Our comprehensive analysis reveals key insights into the\ntrade-offs between task difficulty and audio duration. In general, we find that\naudio LMs struggle with long-form speech, with performance declining as\nduration increases. They perform poorly on localization, temporal reasoning,\ncounting, and struggle to understand non-phonemic information, relying more on\nprompts than audio content. BLAB serves as a challenging evaluation framework\nto develop audio LMs with robust long-form audio understanding capabilities."}
{"id": "2505.04806", "pdf": "https://arxiv.org/pdf/2505.04806.pdf", "abs": "https://arxiv.org/abs/2505.04806", "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs", "authors": ["Chetan Pathade"], "categories": ["cs.CR", "cs.CL"], "comment": "7 Pages, 6 Figures", "summary": "Large Language Models (LLMs) are increasingly integrated into consumer and\nenterprise applications. Despite their capabilities, they remain susceptible to\nadversarial attacks such as prompt injection and jailbreaks that override\nalignment safeguards. This paper provides a systematic investigation of\njailbreak strategies against various state-of-the-art LLMs. We categorize over\n1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,\nMistral 7B, and Vicuna, and examine their generalizability and construction\nlogic. We further propose layered mitigation strategies and recommend a hybrid\nred-teaming and sandboxing approach for robust LLM security."}
