{"id": "2506.18919", "pdf": "https://arxiv.org/pdf/2506.18919.pdf", "abs": "https://arxiv.org/abs/2506.18919", "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection", "authors": ["Hexiang Gu", "Qifan Yu", "Saihui Hou", "Zhiqin Fang", "Huijia Wu", "Zhaofeng He"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "The rapid development of social media has intensified the spread of harmful\ncontent. Harmful memes, which integrate both images and text, pose significant\nchallenges for automated detection due to their implicit semantics and complex\nmultimodal interactions. Although existing research has made progress in\ndetection accuracy and interpretability, the lack of a systematic, large-scale,\ndiverse, and highly explainable dataset continues to hinder further advancement\nin this field. To address this gap, we introduce MemeMind, a novel dataset\nfeaturing scientifically rigorous standards, large scale, diversity, bilingual\nsupport (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.\nMemeMind fills critical gaps in current datasets by offering comprehensive\nlabeling and explicit reasoning traces, thereby providing a solid foundation\nfor enhancing harmful meme detection. In addition, we propose an innovative\ndetection framework, MemeGuard, which effectively integrates multimodal\ninformation with reasoning process modeling, significantly improving models'\nability to understand and identify harmful memes. Extensive experiments\nconducted on the MemeMind dataset demonstrate that MemeGuard consistently\noutperforms existing state-of-the-art methods in harmful meme detection tasks.", "AI": {"tldr": "MemeMind is a comprehensive dataset for harmful meme detection, coupled with the MemeGuard framework that enhances detection accuracy through multimodal information integration and reasoning.", "motivation": "The increasing prevalence of harmful content in social media necessitates effective detection methods, yet existing datasets lack scale, diversity, and explainability.", "method": "Introduction of the MemeMind dataset, featuring bilingual support and Chain-of-Thought annotations, alongside the MemeGuard detection framework that integrates multimodal information.", "result": "Extensive experiments show that MemeGuard outperforms state-of-the-art methods in harmful meme detection using the MemeMind dataset.", "conclusion": "MemeMind and MemeGuard represent significant advancements in the automatic detection of harmful memes, paving the way for future research in this area.", "key_contributions": ["Introduction of the MemeMind dataset with bilingual support and comprehensive annotations.", "Development of the MemeGuard detection framework that incorporates reasoning processes.", "Demonstrated improvements over existing methods in harmful meme detection tasks."], "limitations": "Limited to harmful memes and may not generalize to other multimedia content types.", "keywords": ["harmful memes", "dataset", "detection framework", "Chain-of-Thought", "multimodal"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.18998", "pdf": "https://arxiv.org/pdf/2506.18998.pdf", "abs": "https://arxiv.org/abs/2506.18998", "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge", "authors": ["Sahil Kale", "Vijaykant Nadadur"], "categories": ["cs.CL"], "comment": "Accepted to the Pre-ACL Workshop 2025, Copenhagen", "summary": "When artificial intelligence mistakes memorization for intelligence, it\ncreates a dangerous mirage of reasoning. Existing studies treat memorization\nand self-knowledge deficits in LLMs as separate issues and do not recognize an\nintertwining link that degrades the trustworthiness of LLM responses. In our\nstudy, we utilize a novel framework to ascertain if LLMs genuinely learn\nreasoning patterns from training data or merely memorize them to assume\ncompetence across problems of similar complexity focused on STEM domains. Our\nanalysis shows a noteworthy problem in generalization: LLMs draw confidence\nfrom memorized solutions to infer a higher self-knowledge about their reasoning\nability, which manifests as an over 45% inconsistency in feasibility\nassessments when faced with self-validated, logically coherent task\nperturbations. This effect is most pronounced in science and medicine domains,\nwhich tend to have maximal standardized jargon and problems, further confirming\nour approach. Significant wavering within the self-knowledge of LLMs also shows\nflaws in current architectures and training patterns, highlighting the need for\ntechniques that ensure a balanced, consistent stance on models' perceptions of\ntheir own knowledge for maximum AI explainability and trustworthiness. Our code\nand results are available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.", "AI": {"tldr": "The study examines the relationship between memorization and self-knowledge in LLMs, revealing significant flaws in their reasoning capabilities, particularly in STEM domains.", "motivation": "To investigate the intertwined issues of memorization and self-knowledge deficits in LLMs that affect the trustworthiness of their responses.", "method": "The authors employ a novel framework to analyze LLMs' reasoning by examining their reliance on memorized solutions and the resulting inconsistencies in their self-assessments of reasoning ability.", "result": "The analysis reveals over 45% inconsistency in feasibility assessments by LLMs when faced with logically coherent task changes, especially pronounced in science and medicine contexts.", "conclusion": "Current LLM architectures and training practices lead to flawed self-knowledge, necessitating new methodologies for enhancing AI explainability and trustworthiness.", "key_contributions": ["Introduces a novel framework for analyzing LLM reasoning patterns.", "Demonstrates a significant problem with generalization in LLMs influenced by memorization.", "Highlights critical flaws in LLM self-knowledge relevant to AI trustworthiness."], "limitations": "The study primarily focuses on LLM performance in STEM domains, and broader implications in other areas are not explored.", "keywords": ["LLM", "memorization", "self-knowledge", "trustworthiness", "AI explainability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.19004", "pdf": "https://arxiv.org/pdf/2506.19004.pdf", "abs": "https://arxiv.org/abs/2506.19004", "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations", "authors": ["Brian Siyuan Zheng", "Alisa Liu", "Orevaoghene Ahia", "Jonathan Hayase", "Yejin Choi", "Noah A. Smith"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Modern tokenizers employ deterministic algorithms to map text into a single\n\"canonical\" token sequence, yet the same string can be encoded as many\nnon-canonical tokenizations using the tokenizer vocabulary. In this work, we\ninvestigate the robustness of LMs to text encoded with non-canonical\ntokenizations entirely unseen during training. Surprisingly, when evaluated\nacross 20 benchmarks, we find that instruction-tuned models retain up to 93.4%\nof their original performance when given a randomly sampled tokenization, and\n90.8% with character-level tokenization. We see that overall stronger models\ntend to be more robust, and robustness diminishes as the tokenization departs\nfarther from the canonical form. Motivated by these results, we then identify\nsettings where non-canonical tokenization schemes can *improve* performance,\nfinding that character-level segmentation improves string manipulation and code\nunderstanding tasks by up to +14%, and right-aligned digit grouping enhances\nlarge-number arithmetic by +33%. Finally, we investigate the source of this\nrobustness, finding that it arises in the instruction-tuning phase. We show\nthat while both base and post-trained models grasp the semantics of\nnon-canonical tokenizations (perceiving them as containing misspellings), base\nmodels try to mimic the imagined mistakes and degenerate into nonsensical\noutput, while post-trained models are committed to fluent responses. Overall,\nour findings suggest that models are less tied to their tokenizer than\npreviously believed, and demonstrate the promise of intervening on tokenization\nat inference time to boost performance.", "AI": {"tldr": "This paper examines how language models (LMs) perform with non-canonical tokenizations, finding that many models maintain high performance and can even benefit from certain non-standard tokenization strategies.", "motivation": "To explore the robustness of language models to unseen non-canonical tokenizations and their effects on performance.", "method": "Analyzed LMs across 20 benchmarks to assess performance retention with non-canonical tokenizations, specifically character-level tokenization and randomly sampled tokenization methods.", "result": "Instruction-tuned models maintain up to 93.4% performance with random tokenization and show improvements in specific tasks with alternative tokenization approaches, such as up to +14% in string manipulation and +33% in large-number arithmetic.", "conclusion": "Models show robustness to non-canonical tokenizations, indicating they are less reliant on their standard tokenizer, and suggest possible performance enhancements through tokenization modifications at inference.", "key_contributions": ["Demonstrated high performance retention of LMs with non-canonical tokenizations.", "Identified scenarios where non-canonical tokenization enhances performance.", "Explored differences between base and post-trained models in processing non-canonical tokenizations."], "limitations": "", "keywords": ["language models", "tokenization", "robustness", "instruction tuning", "performance enhancement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19028", "pdf": "https://arxiv.org/pdf/2506.19028.pdf", "abs": "https://arxiv.org/abs/2506.19028", "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective", "authors": ["Weijie Xu", "Yiwen Wang", "Chi Xue", "Xiangkun Hu", "Xi Fang", "Guimin Dong", "Chandan K. Reddy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "29 pages, 9 figures, 15 tables", "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose\nFiSCo(Fine-grained Semantic Computation), a novel statistical framework to\nevaluate group-level fairness in LLMs by detecting subtle semantic differences\nin long-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSco more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.", "AI": {"tldr": "The paper proposes FiSCo, a novel statistical framework for evaluating group-level fairness in Large Language Models (LLMs) by detecting subtle semantic biases in long-form responses across demographic groups.", "motivation": "To address the challenges of inherent biases in LLM-generated responses and the shortcomings of existing evaluation methods that often overlook these biases, particularly in long-form outputs.", "method": "FiSCo operates at the claim level, assessing semantic differences in LLM outputs by decomposing responses into semantically distinct claims and applying statistical hypothesis testing to compare similarities among demographic groups.", "result": "FiSCo reliably identifies nuanced biases in LLM outputs and outperforms existing evaluation metrics by reducing the impact of stochastic variability in LLM responses.", "conclusion": "The framework enhances the reliability of LLMs by providing a more robust means of detecting biases that affect fairness across demographic groups.", "key_contributions": ["Introduction of FiSCo for evaluating group-level fairness in LLMs.", "Decomposition of model outputs into semantically distinct claims for better bias detection.", "Validation of the framework on diverse datasets including gender, race, and age."], "limitations": "", "keywords": ["Large Language Models", "Fairness Evaluation", "Semantic Bias Detection", "Statistical Framework", "Counterfactual Fairness"], "importance_score": 9, "read_time_minutes": 29}}
{"id": "2506.18962", "pdf": "https://arxiv.org/pdf/2506.18962.pdf", "abs": "https://arxiv.org/abs/2506.18962", "title": "UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding", "authors": ["Weiheng Lu", "Chunfeng Song", "Jiamin Wu", "Pengyu Zhu", "Yuchen Zhou", "Weijian Mai", "Qihao Zheng", "Wanli Ouyang"], "categories": ["cs.HC"], "comment": "19pages,4 figures", "summary": "Decoding human brain activity from electroencephalography (EEG) signals is a\ncentral challenge at the intersection of neuroscience and artificial\nintelligence, enabling diverse applications in mental state assessment,\nclinical monitoring, and human-machine interaction. Recent efforts have\nextensively explored EEG-based brain foundation models for generalized brain\ndecoding, employing large-scale training on multiple datasets. However, most of\nthese attempts struggle with generalizability and fail to achieve satisfactory\nperformance without task-specific tuning due to pronounced inherent\nheterogeneity among decoding tasks. To address these challenges, we present\nUniMind, a general-purpose EEG foundation model for unified multi-task brain\ndecoding by uniquely unleashing the power of large language models to\ncomprehend complex neural patterns. UniMind offers several advantages. First,\nwe design a Neuro-Language Connector to bridge the modality gap between neural\nsignals and large language models, distilling and transforming the\nspatiotemporal neural patterns of EEG data into representations understandable\nby language models. Second, a Task-aware Query Selection module is proposed to\ninject task-awareness into the cross-modal alignment by dynamically generating\ntask-adaptive query tokens, enabling learning of task-relevant neural patterns\nacross diverse tasks. Extensive experiments across ten datasets demonstrate\nthat UniMind substantially outperforms state-of-the-art multi-task decoding\nmodels, with an average gain of 12 percent, while also offering valuable\nneuroscientific insights into neural functional correlations across tasks. The\ncode will be made publicly available.", "AI": {"tldr": "UniMind is a general-purpose EEG foundation model that improves brain activity decoding by bridging EEG signals with large language models using a Neuro-Language Connector.", "motivation": "To improve generalizability and performance of EEG-based brain decoding models that currently require task-specific tuning.", "method": "UniMind employs a Neuro-Language Connector to translate spatiotemporal EEG patterns into formats understood by language models and uses a Task-aware Query Selection module for task-relevant neural pattern learning.", "result": "UniMind achieves an average performance improvement of 12 percent over existing state-of-the-art models in multi-task brain decoding across ten datasets.", "conclusion": "UniMind is a significant advancement in EEG decoding, providing insights into neural patterns and enhancing human-machine interaction without the need for task-specific tuning.", "key_contributions": ["Neuro-Language Connector for translating EEG data to language models.", "Task-aware Query Selection for dynamic task-adaptive learning.", "Substantial performance improvements in multi-task decoding."], "limitations": "", "keywords": ["EEG", "brain decoding", "large language models", "multi-task learning", "human-machine interaction"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2506.19037", "pdf": "https://arxiv.org/pdf/2506.19037.pdf", "abs": "https://arxiv.org/abs/2506.19037", "title": "Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "comment": null, "summary": "Masked diffusion language models (MDLM) have shown strong promise for\nnon-autoregressive text generation, yet existing samplers act as implicit\nplanners, selecting tokens to unmask via denoiser confidence or entropy scores.\nSuch heuristics falter under parallel unmasking - they ignore pairwise\ninteractions between tokens and cannot account for dependencies when unmasking\nmultiple positions at once, limiting their inference time to traditional\nauto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking\nStrategy (DUS), an inference-only, planner-model-free method that requires no\nadditional training. DUS leverages a first-order Markov assumption to partition\nsequence positions into dilation-based groups of non-adjacent tokens, enabling\nindependent, parallel unmasking steps that respect local context that minimizes\nthe joint entropy of each iteration step. Unlike semi-AR block approaches\n(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces\nthe number of denoiser calls to O(log B) per generation block - yielding\nsubstantial speedup over the O(B) run time of state-of-the-art diffusion\nmodels, where B is the block size in the semi-AR inference process. In\nexperiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -\ndomains suited to non-ordinal generation - DUS improves scores over parallel\nconfidence-based planner, without modifying the underlying denoiser. DUS offers\na lightweight, budget-aware approach to efficient, high-quality text\ngeneration, paving the way to unlock the true capabilities of MDLMs.", "AI": {"tldr": "Introduces the Dilated-scheduled Unmasking Strategy (DUS) for efficient text generation using Masked Diffusion Language Models (MDLM) that improves inference speed without additional training.", "motivation": "Address limitations of existing samplers in Masked Diffusion Language Models, particularly their inability to effectively handle parallel unmasking and dependency management among tokens.", "method": "DUS employs a first-order Markov assumption to partition sequences into dilation-based groups, allowing for independent, parallel unmasking that considers local context.", "result": "DUS shows significant speedup in inference time, reducing denoiser calls to O(log B) compared to O(B) of traditional models, while improving performance on benchmarks like GSM8K and code completion datasets.", "conclusion": "DUS provides a lightweight and efficient solution for text generation that enhances the capabilities of MDLMs without requiring additional training.", "key_contributions": ["Introduction of the Dilated-scheduled Unmasking Strategy (DUS)", "Reduction in denoiser calls from O(B) to O(log B)", "Improvement in text generation quality on specified benchmarks"], "limitations": "", "keywords": ["Masked Diffusion Language Models", "Unmasking Strategy", "Efficiency in Text Generation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.19017", "pdf": "https://arxiv.org/pdf/2506.19017.pdf", "abs": "https://arxiv.org/abs/2506.19017", "title": "Raise Awareness of the Environmental Impacts of Retail Food Products: A User-Centered Scenario-Based Approach", "authors": ["Lorenzo Porcelli", "Francesco Palmieri"], "categories": ["cs.HC"], "comment": null, "summary": "The climate is warming rapidly, and atmospheric concentrations of greenhouse\ngases (GHGs) are at their highest levels ever recorded. As a result of these\nclimate changes, caused mainly by human activities, disasters have increased\nfivefold over the past 50 years, causing death and economic loss. Civic\nengagement and awareness are essential to mitigate climate change and its\nimpacts. In this work, we proposed a user interface that makes users aware of\nthe environmental impact of the food products they buy when shopping. A\nuser-centered scenario-based design was followed in the development of the\ninterface. Gamification elements were added to increase civic participation in\nclimate action.", "AI": {"tldr": "The paper presents a user interface aimed at raising awareness of the environmental impacts of food products through user-centered design and gamification.", "motivation": "To mitigate climate change and its impacts through increased civic engagement and awareness about environmental impacts of food products.", "method": "The development of the interface followed a user-centered scenario-based design approach with the incorporation of gamification elements to enhance participation.", "result": "The proposed user interface successfully informs users about the environmental impact of their food purchases, encouraging climate action.", "conclusion": "Enhanced user awareness through the interface can promote civic engagement in climate action, potentially leading to reduced environmental impact.", "key_contributions": ["Development of a gamified user interface for environmental awareness", "Application of user-centered design in climate action tools", "Focus on civic engagement related to food product purchases"], "limitations": "", "keywords": ["climate change", "user interface", "gamification", "civic engagement", "environmental impact"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.19058", "pdf": "https://arxiv.org/pdf/2506.19058.pdf", "abs": "https://arxiv.org/abs/2506.19058", "title": "NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching", "authors": ["Mike Zhang", "Rob van der Goot"], "categories": ["cs.CL"], "comment": "TalentCLEF 2025", "summary": "Matching job titles is a highly relevant task in the computational job market\ndomain, as it improves e.g., automatic candidate matching, career path\nprediction, and job market analysis. Furthermore, aligning job titles to job\nskills can be considered an extension to this task, with similar relevance for\nthe same downstream tasks. In this report, we outline NLPnorth's submission to\nTalentCLEF 2025, which includes both of these tasks: Multilingual Job Title\nMatching, and Job Title-Based Skill Prediction. For both tasks we compare\n(fine-tuned) classification-based, (fine-tuned) contrastive-based, and\nprompting methods. We observe that for Task A, our prompting approach performs\nbest with an average of 0.492 mean average precision (MAP) on test data,\naveraged over English, Spanish, and German. For Task B, we obtain an MAP of\n0.290 on test data with our fine-tuned classification-based approach.\nAdditionally, we made use of extra data by pulling all the language-specific\ntitles and corresponding \\emph{descriptions} from ESCO for each job and skill.\nOverall, we find that the largest multilingual language models perform best for\nboth tasks. Per the provisional results and only counting the unique teams, the\nranking on Task A is 5$^{\\text{th}}$/20 and for Task B 3$^{\\text{rd}}$/14.", "AI": {"tldr": "This report presents NLPnorth's submission to TalentCLEF 2025, focusing on multilingual job title matching and skill prediction using various NLP techniques.", "motivation": "To enhance automatic candidate matching, career path prediction, and job market analysis through job title alignment and corresponding skills.", "method": "The study employs (fine-tuned) classification-based, contrastive-based, and prompting methods for two main tasks: Multilingual Job Title Matching and Job Title-Based Skill Prediction.", "result": "The prompting method achieved the highest performance on job title matching with an average precision of 0.492, while the classification-based approach scored 0.290 for skill prediction.", "conclusion": "The largest multilingual language models yielded the best results for both tasks, securing 5th/20 for Task A and 3rd/14 for Task B among competing teams.", "key_contributions": ["Introduced effective methodology for job title matching and skill prediction", "Utilized a multilingual approach across English, Spanish, and German", "Demonstrated the effectiveness of large language models in job-related NLP tasks."], "limitations": "", "keywords": ["job title matching", "skill prediction", "multilingual", "NLP", "TalentCLEF 2025"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.19107", "pdf": "https://arxiv.org/pdf/2506.19107.pdf", "abs": "https://arxiv.org/abs/2506.19107", "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education", "authors": ["Ruiwei Xiao", "Xinying Hou", "Runlong Ye", "Majeed Kazemitabaar", "Nicholas Diana", "Michael Liut", "John Stamper"], "categories": ["cs.HC", "cs.AI"], "comment": "Under review for Computer & Education: Artificial Intelligence.\n  Journal policy allows submitting as preprint", "summary": "With the proliferation of large language model (LLM) applications since 2022,\ntheir use in education has sparked both excitement and concern. Recent studies\nconsistently highlight students' (mis)use of LLMs can hinder learning outcomes.\nThis work aims to teach students how to effectively prompt LLMs to improve\ntheir learning. We first proposed pedagogical prompting, a\ntheoretically-grounded new concept to elicit learning-oriented responses from\nLLMs. To move from concept design to a proof-of-concept learning intervention\nin real educational settings, we selected early undergraduate CS education\n(CS1/CS2) as the example context. We began with a formative survey study with\ninstructors (N=36) teaching early-stage undergraduate-level CS courses to\ninform the instructional design based on classroom needs. Based on their\ninsights, we designed and developed a learning intervention through an\ninteractive system with scenario-based instruction to train pedagogical\nprompting skills. Finally, we evaluated its instructional effectiveness through\na user study with CS novice students (N=22) using pre/post-tests. Through mixed\nmethods analyses, our results indicate significant improvements in learners'\nLLM-based pedagogical help-seeking skills, along with positive attitudes toward\nthe system and increased willingness to use pedagogical prompts in the future.\nOur contributions include (1) a theoretical framework of pedagogical prompting;\n(2) empirical insights into current instructor attitudes toward pedagogical\nprompting; and (3) a learning intervention design with an interactive learning\ntool and scenario-based instruction leading to promising results on teaching\nLLM-based help-seeking. Our approach is scalable for broader implementation in\nclassrooms and has the potential to be integrated into tools like ChatGPT as an\non-boarding experience to encourage learning-oriented use of generative AI.", "AI": {"tldr": "This paper presents a study on teaching effective prompting strategies for large language models in education, with a focus on enhancing learning outcomes for undergraduate computer science students.", "motivation": "The widespread application of large language models (LLMs) in education has raised concerns about their impact on learning outcomes, prompting the need to teach effective prompting strategies to students.", "method": "A formative survey study was conducted with instructors (N=36) to inform instructional design, followed by the development of an interactive learning intervention targeting early undergraduate CS students (N=22), evaluated through pre/post-tests and mixed methods analysis.", "result": "The study revealed significant improvements in students' skills for seeking help via LLMs, along with increased positive attitudes towards using prompts in the future.", "conclusion": "The findings support the potential of pedagogical prompting training in enhancing LLM usage in educational contexts, with implications for integrating such approaches into existing tools like ChatGPT.", "key_contributions": ["A theoretical framework of pedagogical prompting", "Insights into instructor attitudes towards pedagogical prompting", "Design of an interactive learning tool for teaching LLM-based help-seeking"], "limitations": "", "keywords": ["pedagogical prompting", "large language models", "educational intervention"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19073", "pdf": "https://arxiv.org/pdf/2506.19073.pdf", "abs": "https://arxiv.org/abs/2506.19073", "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation", "authors": ["Jackson Trager", "Francielle Vargas", "Diego Alves", "Matteo Guida", "Mikel K. Ngueajio", "Ameeta Agrawal", "Flor Plaza-del-Arco", "Yalda Daryanai", "Farzan Karimi-Malekabadi"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning.", "AI": {"tldr": "This paper presents MFTCXplain, a multilingual benchmark for evaluating the moral reasoning of LLMs, revealing significant misalignments in moral sentiment prediction.", "motivation": "The paper addresses concerns regarding the moral reasoning capabilities of LLMs in socially sensitive contexts and the limitations of current evaluation benchmarks.", "method": "Introducing a multilingual dataset of 3,000 tweets annotated with labels for hate speech, moral categories, and rationales, allowing for a comprehensive assessment of LLMs' moral reasoning skills.", "result": "LLMs achieve an F1 score of up to 0.836 in hate speech detection but struggle with moral sentiment prediction (F1 < 0.35), particularly in underrepresented languages.", "conclusion": "This study illustrates the insufficiency of LLMs in capturing and expressing human moral reasoning, highlighting the need for improved models and evaluation methods.", "key_contributions": ["Development of MFTCXplain, a multilingual moral reasoning benchmark", "Demonstration of LLMs' limited ability to predict moral sentiments", "Provision of dataset annotations that enhance interpretability"], "limitations": "The dataset may not encompass all cultural nuances, and the assessment primarily focuses on a limited set of languages and contexts.", "keywords": ["Moral Foundation Theory", "Large Language Models", "Hate Speech", "Multilingual Benchmark", "Moral Reasoning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.19210", "pdf": "https://arxiv.org/pdf/2506.19210.pdf", "abs": "https://arxiv.org/abs/2506.19210", "title": "Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment", "authors": ["Bhanuka Gamage", "Nicola McDowell", "Dijana Kovacic", "Leona Holloway", "Thanh-Toan Do", "Nicholas Price", "Arthur Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's conditionally accepted version of a paper to be published at\n  ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)", "summary": "Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision\nimpairment, yet remains underrepresented in assistive technology research.\nUnlike ocular conditions, CVI affects higher-order visual processing-impacting\nobject recognition, facial perception, and attention in complex environments.\nThis paper presents a co-design study with two adults with CVI investigating\nhow smart glasses, i.e. head-mounted extended reality displays, can support\nunderstanding and interaction with the immediate environment. Guided by the\nDouble Diamond design framework, we conducted a two-week diary study, two\nideation workshops, and ten iterative development sessions using the Apple\nVision Pro. Our findings demonstrate that smart glasses can meaningfully\naddress key challenges in locating objects, reading text, recognising people,\nengaging in conversations, and managing sensory stress. With the rapid\nadvancement of smart glasses and increasing recognition of CVI as a distinct\nform of vision impairment, this research addresses a timely and under-explored\nintersection of technology and need.", "AI": {"tldr": "This paper explores the potential of smart glasses to assist individuals with Cerebral Visual Impairment (CVI) in navigating their environments, highlighting user-centered design methods.", "motivation": "Cerebral Visual Impairment (CVI) is underrepresented in assistive technology research and affects higher-order visual processing, necessitating innovative solutions to help affected individuals interact with their surroundings.", "method": "The study utilized the Double Diamond design framework including a two-week diary study, two ideation workshops, and ten iterative development sessions with users of the Apple Vision Pro.", "result": "The findings indicate that smart glasses can significantly aid in locating objects, reading text, recognizing people, and managing sensory stress in complex environments for individuals with CVI.", "conclusion": "This research highlights the importance of developing assistive technologies for CVI, revealing the potential of smart glasses to address specific challenges faced by individuals with this condition.", "key_contributions": ["Novel application of smart glasses for assisting users with CVI", "User-centered co-design involving individuals with CVI", "Insights into the interaction between technology and the unique needs of CVI users."], "limitations": "", "keywords": ["Cerebral Visual Impairment", "assistive technology", "smart glasses", "user-centered design", "accessibility"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.19089", "pdf": "https://arxiv.org/pdf/2506.19089.pdf", "abs": "https://arxiv.org/abs/2506.19089", "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available.", "AI": {"tldr": "This paper presents StorySim, a framework for generating synthetic stories to evaluate theory of mind and world modeling capabilities of large language models.", "motivation": "To evaluate the theory of mind and world modeling capabilities of large language models without contamination from pretraining data.", "method": "The framework generates highly controllable story prompts using a Storyboard, allowing for precise manipulation of character perspectives and events, and includes tasks for testing ToM and WM.", "result": "Experiments reveal that state-of-the-art LLMs perform better on world modeling tasks than theory of mind tasks, with better performance when reasoning with humans over inanimate objects.", "conclusion": "The framework provides evidence of heuristic behaviors in LLMs, such as recency bias. All code is available for public use.", "key_contributions": ["Introduction of StorySim for story generation", "Control over character perspectives and events", "Findings on LLM performance in ToM vs WM tasks"], "limitations": "", "keywords": ["Theory of Mind", "World Modeling", "Large Language Models", "Synthetic Story Generation", "Heuristic Behavior"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2506.19268", "pdf": "https://arxiv.org/pdf/2506.19268.pdf", "abs": "https://arxiv.org/abs/2506.19268", "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps", "authors": ["Timoteo Kelly", "Abdulkadir Korkmaz", "Samuel Mallet", "Connor Souders", "Sadra Aliakbarpour", "Praveen Rao"], "categories": ["cs.HC", "cs.CR", "cs.ET", "cs.LG"], "comment": "Under review at The 34th ACM International Conference on Information\n  and Knowledge Management (CIKM'25)", "summary": "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing.", "AI": {"tldr": "HARPT is a large-scale corpus of mobile health app store reviews aimed at enhancing research on user privacy and trust.", "motivation": "The study aims to advance research in user privacy and trust in the context of mobile health applications by providing a comprehensive annotated dataset.", "method": "The HARPT dataset was created through a sophisticated annotation strategy involving rule-based filtering, iterative manual labeling, targeted data augmentation, and weak supervision using transformer-based classifiers to ensure high-quality labels.", "result": "A large-scale dataset with over 480,000 labeled reviews was successfully compiled, allowing for benchmarking of various classification models that showed strong performance.", "conclusion": "HARPT serves as a valuable public resource for future research in health informatics, cybersecurity, and natural language processing.", "key_contributions": ["Creation of a large annotated mobile health app store review corpus", "Development of a nuanced label schema for trust and privacy aspects", "Benchmarking of multiple classification models with strong performance outcomes"], "limitations": "", "keywords": ["mobile health", "user trust", "privacy", "natural language processing", "dataset"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.19113", "pdf": "https://arxiv.org/pdf/2506.19113.pdf", "abs": "https://arxiv.org/abs/2506.19113", "title": "Human-Aligned Faithfulness in Toxicity Explanations of LLMs", "authors": ["Ramaravind K. Mothilal", "Joanna Roy", "Syed Ishtiaque Ahmed", "Shion Guha"], "categories": ["cs.CL"], "comment": "21 pages, 5 figures, 7 tables", "summary": "The discourse around toxicity and LLMs in NLP largely revolves around\ndetection tasks. This work shifts the focus to evaluating LLMs' reasoning about\ntoxicity -- from their explanations that justify a stance -- to enhance their\ntrustworthiness in downstream tasks. Despite extensive research on\nexplainability, it is not straightforward to adopt existing methods to evaluate\nfree-form toxicity explanation due to their over-reliance on input text\nperturbations, among other challenges. To account for these, we propose a\nnovel, theoretically-grounded multi-dimensional criterion, Human-Aligned\nFaithfulness (HAF), that measures the extent to which LLMs' free-form toxicity\nexplanations align with those of a rational human under ideal conditions. We\ndevelop six metrics, based on uncertainty quantification, to comprehensively\nevaluate \\haf of LLMs' toxicity explanations with no human involvement, and\nhighlight how \"non-ideal\" the explanations are. We conduct several experiments\non three Llama models (of size up to 70B) and an 8B Ministral model on five\ndiverse toxicity datasets. Our results show that while LLMs generate plausible\nexplanations to simple prompts, their reasoning about toxicity breaks down when\nprompted about the nuanced relations between the complete set of reasons, the\nindividual reasons, and their toxicity stances, resulting in inconsistent and\nnonsensical responses. We open-source our code and LLM-generated explanations\nat https://github.com/uofthcdslab/HAF.", "AI": {"tldr": "This paper evaluates LLMs' reasoning about toxicity through their explanations, proposing a metric called Human-Aligned Faithfulness (HAF) for assessment.", "motivation": "To enhance the trustworthiness of LLMs in evaluating toxicity by focusing on their explanation capabilities rather than just detection tasks.", "method": "The authors developed a multi-dimensional criterion called Human-Aligned Faithfulness (HAF), which involves six metrics based on uncertainty quantification to evaluate LLMs' toxicity explanations without human involvement.", "result": "Experiments on three Llama models and an 8B Ministral model show that while LLMs can provide plausible explanations, their reasoning about nuanced toxicity relations is inconsistent and often nonsensical.", "conclusion": "The study reveals limitations in LLMs' reasoning about toxicity and provides open-source access to their code and generated explanations to facilitate further research.", "key_contributions": ["Proposed a novel metric HAF to evaluate LLMs' toxicity explanations.", "Conducted extensive experiments on multiple LLMs across diverse datasets.", "Opened access to tools and data for the research community."], "limitations": "The study highlights the challenges of assessing LLMs' explanations, particularly when ideal conditions are not met.", "keywords": ["toxicity", "LLMs", "explainability", "Human-Aligned Faithfulness", "NLP"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.19307", "pdf": "https://arxiv.org/pdf/2506.19307.pdf", "abs": "https://arxiv.org/abs/2506.19307", "title": "OpticalAging: Real-time Presbyopia Simulation for Inclusive Design via Tunable Lenses", "authors": ["Qing Zhang", "Zixiong Su", "Yoshihito Kondoh", "Kazunori Asada", "Thad Starner", "Kai Kunze", "Yuta Itoh", "Jun Rekimoto"], "categories": ["cs.HC"], "comment": "Under Submission", "summary": "Presbyopia, a common age-related vision condition affecting most people as\nthey age, often remains inadequately understood by those unaffected. To help\nbridge the gap between abstract accessibility knowledge and a more grounded\nappreciation of perceptual challenges, this study presents OpticalAging, an\noptical see-through simulation approach. Unlike VR-based methods, OpticalAging\nuses dynamically controlled tunable lenses to simulate the first-person visual\nperspective of presbyopia's distance-dependent blur during real-world\ninteraction, aiming to enhance awareness. While acknowledging critiques\nregarding simulation's limitations in fully capturing lived experience, we\nposition this tool as a complement to user-centered methods. Our user study (N\n= 19, 18-35 years old) provides validation: quantitative measurements show\nstatistically significant changes in near points across three age modes (40s,\n50s, 60s), while qualitative results suggest increases in reported\nunderstanding and empathy among participants. The integration of our tool into\na design task showcases its potential applicability within age-inclusive design\nworkflows when used critically alongside direct user engagement.", "AI": {"tldr": "OpticalAging is an optical see-through simulation that uses tunable lenses to represent presbyopia's visual effects, aiming to enhance understanding and empathy towards those affected by the condition.", "motivation": "To enhance awareness and understanding of presbyopia, particularly among those who are not affected by it, through a simulation that approximates the visual challenges faced by individuals with this condition.", "method": "The study employed OpticalAging, which utilizes dynamically controlled tunable lenses to create a first-person perspective simulation of presbyopia's distance-dependent blur during real-world interaction, complemented by a user study with 19 participants.", "result": "Quantitative measurements indicated significant changes in near points across three age modes, and qualitative feedback showed increased understanding and empathy from participants toward presbyopia.", "conclusion": "OpticalAging could be effectively integrated into age-inclusive design workflows, serving as a valuable tool when combined with user-centered methods.", "key_contributions": ["Introduction of OpticalAging as a novel simulation tool for presbyopia", "Validation through a mixed-methods user study showing both quantitative and qualitative improvements in empathy", "Recommendations for integrating simulation into design practices for inclusivity"], "limitations": "The simulation may not fully capture the lived experience of presbyopia, which is a common critique of such tools.", "keywords": ["Presbyopia", "Human-Computer Interaction", "Simulation", "Empathy", "Design"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.19159", "pdf": "https://arxiv.org/pdf/2506.19159.pdf", "abs": "https://arxiv.org/abs/2506.19159", "title": "Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data", "authors": ["Yun Tang", "Eesung Kim", "Vijendra Raj Apsingekar"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech2025", "summary": "A joint speech and text optimization method is proposed for hybrid transducer\nand attention-based encoder decoder (TAED) modeling to leverage large amounts\nof text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained\nwith both speech and text input modalities together, while it only takes speech\ndata as input during inference. The trained model can unify the internal\nrepresentations from different modalities, and be further extended to\ntext-based domain adaptation. It can effectively alleviate data scarcity for\nmismatch domain tasks since no speech data is required. Our experiments show\nJ-TAED successfully integrates speech and linguistic information into one\nmodel, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model\nis also evaluated on two out-of-domain datasets: one is finance and another is\nnamed entity focused. The text-based domain adaptation brings 15.3% and 17.8%\nWER reduction on those two datasets respectively.", "AI": {"tldr": "A joint speech and text optimization method (J-TAED) is proposed to enhance ASR accuracy by integrating speech and text inputs during training and adapting to various domains without requiring speech data during inference.", "motivation": "To leverage large amounts of text corpus for improving ASR accuracy and addressing issues of data scarcity in mismatch domain tasks.", "method": "The joint TAED model is trained using both speech and text modalities simultaneously. During inference, it utilizes only speech inputs while maintaining unified internal representations from both modalities.", "result": "The J-TAED model significantly reduces word error rate (WER) by 5.8% to 12.8% on the Librispeech dataset and achieves 15.3% and 17.8% WER reductions on finance and named entity focused datasets respectively.", "conclusion": "The joint approach effectively integrates speech and linguistic features, demonstrating improved ASR accuracy and adaptability to different domains without speech data during inference.", "key_contributions": ["Introduction of the J-TAED model for hybrid ASR tasks", "Achievement of WER reduction across multiple datasets", "Extension to text-based domain adaptation"], "limitations": "", "keywords": ["speech recognition", "text adaptation", "ASR", "machine learning", "HCI"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.19364", "pdf": "https://arxiv.org/pdf/2506.19364.pdf", "abs": "https://arxiv.org/abs/2506.19364", "title": "Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment", "authors": ["Angxuan Chen", "Jingjing Lian", "Xinran Kuang", "Jiyou Jia"], "categories": ["cs.HC"], "comment": null, "summary": "The integration of Generative AI (GenAI) into education has raised concerns\nabout over-reliance and superficial learning, particularly in writing tasks in\nhigher education. This study explores whether a theory-driven learning\nanalytics dashboard (LAD) can enhance human-AI collaboration in the academic\nwriting task by improving writing knowledge gains, fostering self-regulated\nlearning (SRL) skills and building different human-AI dialogue characteristics.\nGrounded in Zimmerman's SRL framework, the LAD provided real-time feedback on\nlearners' goal-setting, writing processes and reflection, while monitoring the\nquality of learner-AI interactions. A quasi-experiment was conducted involving\n52 postgraduate students divided into an experimental group (EG) using the LAD\nto a control group (CG) without it in a human-AI collaborative writing task.\nPre- and post- knowledge tests, questionnaires measuring SRL and cognitive\nload, and students' dialogue data with GenAI were collected and analyzed.\nResults showed that the EG achieved significantly higher writing knowledge\ngains and improved SRL skills, particularly in self-efficacy and cognitive\nstrategies. However, the EG also reported increased test anxiety and cognitive\nload, possibly due to heightened metacognitive awareness. Epistemic Network\nAnalysis revealed that the EG engaged in more reflective, evaluative\ninteractions with GenAI, while the CG focused on more transactional and\ninformation-seeking exchanges. These findings contribute to the growing body of\nliterature on the educational use of GenAI and highlight the importance of\ndesigning interventions that complement GenAI tools, ensuring that technology\nenhances rather than undermines the learning process.", "AI": {"tldr": "This study investigates the effects of a theory-driven learning analytics dashboard on enhancing human-AI collaboration in academic writing, showing positive impacts on writing knowledge and self-regulated learning, despite increased cognitive load.", "motivation": "To explore the integration of Generative AI in education while addressing concerns about superficial learning and over-reliance on AI tools.", "method": "A quasi-experiment was conducted with 52 postgraduate students, comparing an experimental group using a learning analytics dashboard with a control group not using it, involving pre- and post-tests alongside questionnaires and dialogue analysis.", "result": "The experimental group showed significantly greater writing knowledge gains and improvements in self-regulated learning skills, but also experienced increased test anxiety and cognitive load.", "conclusion": "The study demonstrates that well-designed interventions around GenAI can enhance educational outcomes while stressing the need to prevent over-reliance and guarantee that technology supports learning processes.", "key_contributions": ["Utilization of a theory-driven learning analytics dashboard", "Enhanced writing knowledge and self-regulated learning skills", "Insights into human-AI interaction characteristics during writing tasks"], "limitations": "Increased cognitive load and test anxiety among participants in the experimental group.", "keywords": ["Generative AI", "human-AI collaboration", "self-regulated learning", "learning analytics", "academic writing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19187", "pdf": "https://arxiv.org/pdf/2506.19187.pdf", "abs": "https://arxiv.org/abs/2506.19187", "title": "Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages", "authors": ["Christopher Toukmaji", "Jeffrey Flanigan"], "categories": ["cs.CL"], "comment": "Accepted to ACL GEM 2025", "summary": "LLMs are typically trained in high-resource languages, and tasks in\nlower-resourced languages tend to underperform the higher-resource language\ncounterparts for in-context learning. Despite the large body of work on\nprompting settings, it is still unclear how LLMs should be adapted\ncross-lingually specifically for in-context learning in the low-resource target\nlanguages. We perform a comprehensive study spanning five diverse target\nlanguages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU\ntraining hours (9,900+ TFLOPs) across various adaptation techniques: few-shot\nprompting, translate-test, fine-tuning, embedding re-initialization, and\ninstruction fine-tuning. Our results show that the few-shot prompting and\ntranslate-test settings tend to heavily outperform the gradient-based\nadaptation methods. To better understand this discrepancy, we design a novel\nmetric, Valid Output Recall (VOR), and analyze model outputs to empirically\nattribute the degradation of these trained models to catastrophic forgetting.\nTo the extent of our knowledge, this is the largest study done on in-context\nlearning for low-resource languages with respect to train compute and number of\nadaptation techniques considered. We make all our datasets and trained models\navailable for public use.", "AI": {"tldr": "The paper explores in-context learning performance of LLMs on low-resource languages, comparing various adaptation techniques and introducing a new metric for analysis.", "motivation": "To investigate and improve the performance of LLMs in low-resource languages for in-context learning tasks due to their typically poor performance compared to high-resource languages.", "method": "A comprehensive study of five low-resource languages, three base LLMs, and seven downstream tasks, utilizing techniques like few-shot prompting, translate-test, and fine-tuning over 4,100 GPU training hours.", "result": "The study found that few-shot prompting and translate-test methods significantly outperformed gradient-based adaptations; introduced a new metric, Valid Output Recall (VOR), to analyze issues such as catastrophic forgetting.", "conclusion": "The research highlights effective strategies for using LLMs in low-resource languages and provides resources for further research with public access to datasets and models.", "key_contributions": ["Largest study on in-context learning for low-resource languages", "Introduction of Valid Output Recall (VOR) metric", "Public availability of datasets and trained models"], "limitations": "The study primarily focuses on specific low-resource languages and may not generalize to all low-resource contexts; further research needed on long-term adaptation strategies.", "keywords": ["in-context learning", "low-resource languages", "language models", "few-shot prompting", "gradient-based methods"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.19430", "pdf": "https://arxiv.org/pdf/2506.19430.pdf", "abs": "https://arxiv.org/abs/2506.19430", "title": "Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities", "authors": ["Adrien Coppens", "Valrie Maquil"], "categories": ["cs.HC"], "comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Engineering Interactive Computer Systems (EICS)\n  2024 International Workshops, and is available online at\n  https://doi.org/10.1007/978-3-031-91760-8_4", "summary": "The automated analysis of human behaviour provides many opportunities for the\ncreation of interactive systems and the post-experiment investigations for user\nstudies. Commodity depth cameras offer reasonable body tracking accuracy at a\nlow price point, without the need for users to wear or hold any extra\nequipment. The resulting systems typically perform body tracking through a\ndedicated machine learning model, but they can be enhanced with additional AI\ncomponents providing extra capabilities. This leads to opportunities but also\nchallenges, for example regarding the orchestration of such AI components and\nthe engineering of the resulting tracking pipeline. In this paper, we discuss\nthese elements, based on our experience with the creation of a remote\ncollaboration system across distant wall-sized displays, that we built using\nexisting and readily available building blocks, including AI-based recognition\nmodels.", "AI": {"tldr": "This paper explores the automated analysis of human behaviour through depth cameras, discussing the integration of AI components in creating interactive systems for remote collaboration.", "motivation": "The motivation behind this research is to enhance interactive systems by leveraging commodity depth cameras for body tracking and integrating AI components for improved functionality.", "method": "The authors analyze their approach based on the development of a remote collaboration system using AI-based recognition models and existing technologies for effective user tracking.", "result": "The paper identifies both opportunities and challenges in the orchestration and engineering of AI-enhanced body tracking systems.", "conclusion": "The authors conclude that using readily available AI and tracking technologies can facilitate the creation of innovative interactive systems, although careful consideration is needed for their integration.", "key_contributions": ["Integration of AI components in body tracking systems", "Development of a remote collaboration system using depth cameras", "Discussion on challenges in engineering tracking pipelines"], "limitations": "", "keywords": ["Human-Computer Interaction", "Body Tracking", "AI Integration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19209", "pdf": "https://arxiv.org/pdf/2506.19209.pdf", "abs": "https://arxiv.org/abs/2506.19209", "title": "Augmenting Multi-Agent Communication with State Delta Trajectory", "authors": ["Yichen Tang", "Weihang Su", "Yujia Zhou", "Yiqun Liu", "Min Zhang", "Shaoping Ma", "Qingyao Ai"], "categories": ["cs.CL"], "comment": "22 pages, 5 figures", "summary": "Multi-agent techniques such as role playing or multi-turn debates have been\nshown to be effective in improving the performance of large language models\n(LLMs) in downstream tasks. Despite their differences in workflows, existing\nLLM-based multi-agent systems mostly use natural language for agent\ncommunication. While this is appealing for its simplicity and interpretability,\nit also introduces inevitable information loss as one model must down sample\nits continuous state vectors to concrete tokens before transferring them to the\nother model. Such losses are particularly significant when the information to\ntransfer is not simple facts, but reasoning logics or abstractive thoughts. To\ntackle this problem, we propose a new communication protocol that transfers\nboth natural language tokens and token-wise state transition trajectory from\none agent to another. Particularly, compared to the actual state value, we find\nthat the sequence of state changes in LLMs after generating each token can\nbetter reflect the information hidden behind the inference process, so we\npropose a State Delta Encoding (SDE) method to represent state transition\ntrajectories. The experimental results show that multi-agent systems with SDE\nachieve SOTA performance compared to other communication protocols,\nparticularly in tasks that involve complex reasoning. This shows the potential\nof communication augmentation for LLM-based multi-agent systems.", "AI": {"tldr": "The paper proposes a new communication protocol for multi-agent systems using LLMs, enhancing performance by transferring state transition trajectories through State Delta Encoding (SDE).", "motivation": "Existing LLM-based multi-agent systems rely on natural language for agent communication, leading to information loss, especially in complex reasoning tasks.", "method": "The proposed communication protocol involves State Delta Encoding (SDE), which transfers both natural language tokens and token-wise state transition trajectories between agents.", "result": "The multi-agent systems employing SDE outperform existing methods, showing state-of-the-art (SOTA) performance in complex reasoning tasks.", "conclusion": "Communication augmentation, particularly through State Delta Encoding, significantly enhances the efficacy of LLM-based multi-agent systems in reasoning tasks.", "key_contributions": ["Introduction of State Delta Encoding (SDE) for agent communication.", "Demonstration of improved performance in multi-agent systems for complex reasoning tasks.", "Comparison of communication protocols showing SDE's superiority."], "limitations": "", "keywords": ["multi-agent systems", "LLMs", "State Delta Encoding", "communication protocol", "complex reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.19495", "pdf": "https://arxiv.org/pdf/2506.19495.pdf", "abs": "https://arxiv.org/abs/2506.19495", "title": "5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace", "authors": ["Russell Beale", "Eugenia Sergueeva"], "categories": ["cs.HC"], "comment": null, "summary": "Empathy is widely recognized as a vital attribute for effective collaboration\nand communication in the workplace, yet developing empathic skills and\nfostering it among colleagues remains a challenge. This study explores the\npotential of a collaborative digital storytelling platform - In Your Shoes -\ndesigned to promote empathic listening and interpersonal understanding through\nthe structured exchange of personal narratives. A one-week intervention was\nconducted with employees from multiple organizations using the platform.\nEmploying a mixed methods approach, we assessed quantitative changes in empathy\nusing the Empathy Quotient (EQ) and qualitatively analyzed participant\nexperiences through grounded theory. While quantitative analysis revealed no\nstatistically significant shift in dispositional empathy, qualitative findings\nsuggested the tool facilitated situational empathy, prompted self-reflection,\nimproved emotional resonance, and enhanced workplace relationships.\nParticipants reported feelings of psychological safety, connection, and, in\nsome cases, therapeutic benefits from sharing and responding to stories. These\nresults highlight the promise of asynchronous, structured narrative-based\ndigital tools for supporting empathic engagement in professional settings,\noffering insights for the design of emotionally intelligent workplace\ntechnologies.", "AI": {"tldr": "The study investigates a digital storytelling platform aimed at enhancing empathy among employees.", "motivation": "To develop empathic skills and improve collaboration and communication in the workplace through digital means.", "method": "A one-week intervention using the 'In Your Shoes' platform with a mixed methods approach, assessing empathy quantitatively and qualitatively.", "result": "Quantitative analysis showed no significant change in dispositional empathy, but qualitative findings indicated improvements in situational empathy and workplace relationships.", "conclusion": "Digital storytelling tools can facilitate empathic engagement in professional settings, providing emotional support and enhancing interpersonal connections.", "key_contributions": ["Development of a digital platform for empathy training", "Mixed methods approach in workplace empathy research", "Insights into emotional intelligence in workplace technology design"], "limitations": "Quantitative results were not statistically significant, focusing mainly on situational rather than dispositional empathy improvements.", "keywords": ["empathy", "digital storytelling", "workplace communication", "emotional intelligence", "collaboration"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.19258", "pdf": "https://arxiv.org/pdf/2506.19258.pdf", "abs": "https://arxiv.org/abs/2506.19258", "title": "Personality Prediction from Life Stories using Language Models", "authors": ["Rasiq Hussain", "Jerry Ma", "Rithik Khandelwal", "Joshua Oltmanns", "Mehak Gupta"], "categories": ["cs.CL", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Natural Language Processing (NLP) offers new avenues for personality\nassessment by leveraging rich, open-ended text, moving beyond traditional\nquestionnaires. In this study, we address the challenge of modeling long\nnarrative interview where each exceeds 2000 tokens so as to predict Five-Factor\nModel (FFM) personality traits. We propose a two-step approach: first, we\nextract contextual embeddings using sliding-window fine-tuning of pretrained\nlanguage models; then, we apply Recurrent Neural Networks (RNNs) with attention\nmechanisms to integrate long-range dependencies and enhance interpretability.\nThis hybrid method effectively bridges the strengths of pretrained transformers\nand sequence modeling to handle long-context data. Through ablation studies and\ncomparisons with state-of-the-art long-context models such as LLaMA and\nLongformer, we demonstrate improvements in prediction accuracy, efficiency, and\ninterpretability. Our results highlight the potential of combining\nlanguage-based features with long-context modeling to advance personality\nassessment from life narratives.", "AI": {"tldr": "This study presents a novel method for predicting personality traits using long narrative interviews through a hybrid approach of contextual embeddings and RNNs with attention mechanisms.", "motivation": "To improve personality assessment by utilizing long, open-ended text instead of traditional questionnaires.", "method": "A two-step approach: first extracting contextual embeddings using sliding-window fine-tuning of pretrained language models, then applying RNNs with attention to model long-range dependencies.", "result": "The hybrid method showed improved prediction accuracy, efficiency, and interpretability compared to state-of-the-art long-context models like LLaMA and Longformer through comprehensive ablation studies.", "conclusion": "Combining language-based features with long-context modeling significantly advances the field of personality assessment from life narratives.", "key_contributions": ["Proposal of a hybrid method using contextual embeddings and RNNs for personality prediction", "Demonstrated improvements over state-of-the-art models in prediction accuracy and interpretability", "Introduced a novel application of NLP techniques in personality assessment"], "limitations": "", "keywords": ["Natural Language Processing", "personality assessment", "long-context modeling"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.19519", "pdf": "https://arxiv.org/pdf/2506.19519.pdf", "abs": "https://arxiv.org/abs/2506.19519", "title": "Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood", "authors": ["Panagiotis Kourtesis", "Evgenia Giatzoglou", "Panagiotis Vorias", "Katerina Alkisti Gounari", "Eleni Orfanidou", "Chrysanthi Nega"], "categories": ["cs.HC", "B.8; C.4; D.0; J.4"], "comment": "29 pages, 11 Figures, 7 Tables", "summary": "Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic\ntrade-offs of its input modes remain under-examined. Seventy-seven healthy\nvolunteers-young (19-29 y) and middle-aged (27-56 y)-completed a VR\nTrail-Making Test with three pointing methods: eye-tracking, head-gaze, and a\nsix-degree-of-freedom hand controller. Completion time, spatial accuracy, and\nerror counts for the simple (Trail A) and alternating (Trail B) sequences were\nanalysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability\n(SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour:\nyounger adults were reliably faster, more precise, and less error-prone.\nAgainst this backdrop, input modality mattered. Eye-tracking yielded the best\nspatial accuracy and shortened Trail A time relative to manual control;\nhead-gaze matched eye-tracking on Trail A speed and became the quickest, least\nerror-prone option on Trail B. Controllers lagged on every metric. Subjective\nratings were high across the board, with only a small usability dip in\nmiddle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed\nmanual pointing, but optimal choice depended on task demands: eye-tracking\nmaximised spatial precision, whereas head-gaze offered calibration-free\nenhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears\nto be accurate, engaging, and ergonomically adaptable assessment, yet it\nrequires age-specific-stratified norms.", "AI": {"tldr": "This study investigates the effectiveness of various input methods in a VR neuropsychological test across different age groups, revealing that gaze-based controls outperform manual ones in accuracy and speed under different task demands.", "motivation": "To explore the ergonomic trade-offs of input modes in virtual reality for neuropsychological testing and assess their impact based on age.", "method": "Seventy-seven volunteers participated in a VR Trail-Making Test using eye-tracking, head-gaze, and a hand controller. Performance metrics including completion time, spatial accuracy, and error counts were analyzed, alongside subjective usability and experience ratings.", "result": "Younger adults performed better than middle-aged participants, with eye-tracking providing the best accuracy and speed on easy tasks, while head-gaze excelled in more complex tasks. Controllers were less effective across all measures.", "conclusion": "Gaze-based methods are advantageous for VR assessments, but optimal input choice varies with task complexity, suggesting the need for tailored approaches based on user characteristics.", "key_contributions": ["Demonstrated superior performance of gaze-based controls in VR over manual methods.", "Identified age-related differences in VR performance metrics.", "Provided insights into usability perceptions of different input modalities."], "limitations": "Findings may require age-specific norms for broader application.", "keywords": ["Virtual Reality", "Neuropsychological Testing", "Input Modality", "Gaze-based Interaction", "User Experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.19262", "pdf": "https://arxiv.org/pdf/2506.19262.pdf", "abs": "https://arxiv.org/abs/2506.19262", "title": "What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning", "authors": ["Yuchang Zhu", "Zhonghua zhen", "Qunshu Lin", "Haotong Wei", "Xiaolong Sun", "Zixuan Yu", "Minghao Liu", "Zibin Zheng", "Liang Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "Ongoing work", "summary": "With the remarkable generative capabilities of large language models (LLMs),\nusing LLM-generated data to train downstream models has emerged as a promising\napproach to mitigate data scarcity in specific domains and reduce\ntime-consuming annotations. However, recent studies have highlighted a critical\nissue: iterative training on self-generated data results in model collapse,\nwhere model performance degrades over time. Despite extensive research on the\nimplications of LLM-generated data, these works often neglect the importance of\ndata diversity, a key factor in data quality. In this work, we aim to\nunderstand the implications of the diversity of LLM-generated data on\ndownstream model performance. Specifically, we explore how varying levels of\ndiversity in LLM-generated data affect downstream model performance.\nAdditionally, we investigate the performance of models trained on data that\nmixes different proportions of LLM-generated data, which we refer to as\nsynthetic data. Our experimental results show that, with minimal distribution\nshift, moderately diverse LLM-generated data can enhance model performance in\nscenarios with insufficient labeled data, whereas highly diverse generated data\nhas a negative impact. We hope our empirical findings will offer valuable\nguidance for future studies on LLMs as data generators.", "AI": {"tldr": "This study investigates the impact of LLM-generated data diversity on downstream model performance, revealing that moderately diverse data enhances performance while highly diverse data can degrade it.", "motivation": "To address the issue of model collapse during iterative training on LLM-generated data and to assess the importance of data diversity for improving model performance.", "method": "The authors conducted experiments analyzing the effects of varying levels of diversity in LLM-generated data on downstream model performance, including the performance of models trained on mixed proportions of LLM-generated data.", "result": "The findings indicate that moderately diverse LLM-generated data improves model performance in scenarios with insufficient labeled data, whereas highly diverse data negatively impacts model results.", "conclusion": "The results provide insights into how data diversity in LLM-generated datasets can guide future research on LLMs as effective data generators for improving downstream model performance.", "key_contributions": ["Analysis of the impact of LLM-generated data diversity on model performance.", "Empirical findings indicating the effects of varying diversity levels on downstream models.", "Guidance for future work on leveraging LLMs for data generation in machine learning."], "limitations": "The study does not address the full range of potential model architectures and use cases for LLM-generated data, focusing instead on specific scenarios.", "keywords": ["large language models", "data diversity", "downstream model performance", "synthetic data", "model collapse"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19524", "pdf": "https://arxiv.org/pdf/2506.19524.pdf", "abs": "https://arxiv.org/abs/2506.19524", "title": "Beyond Wellbeing Apps: Co-Designing Immersive, Embodied, and Collective Digital Wellbeing Interventions for Healthcare Professionals", "authors": ["Zheyuan Zhang", "Jingjing Sun", "Dorian Peters", "Rafael A. Calvo"], "categories": ["cs.HC"], "comment": "21 pages, DIS '25: Designing Interactive Systems Conference, Funchal,\n  Portugal, July 2025", "summary": "Healthcare professionals (HCPs) face increasing levels of stress and burnout.\nTechnological wellbeing interventions provide accessible and flexible support\nfor HCPs. While most studies have focused on mobile- and web-based programs,\nalternative technologies like virtual reality (VR), augmented reality (AR),\ntangible interfaces, and embodied technologies are emerging as engaging and\neffective tools for wellbeing interventions. However, there is still a lack of\nresearch on how such technologies are perceived among HCPs. This study explored\nHCPs' perceptions and preferences for various types of wellbeing technologies,\nby conducting a 2-phase co-design study involving 26 HCPs in idea generation,\nconcept evaluation, prototype testing, and design iteration. From our findings,\nHCPs highly valued the potential of technologies to support mental health with\nimmersive, embodied, and collective experiences. Furthermore, we provided\ndesign recommendations for wellbeing technologies for HCPs that sustain user\nengagement by meeting their needs for autonomy, competence, and relatedness in\nthe experiences.", "AI": {"tldr": "This study investigates healthcare professionals' perceptions of wellbeing technologies such as VR, AR, and embodied interfaces, highlighting their potential for mental health support and providing design recommendations.", "motivation": "Healthcare professionals experience significant stress and burnout, and traditional interventions may not fully address their needs. Emerging technologies offer new ways to enhance wellbeing.", "method": "The study conducted a two-phase co-design with 26 healthcare professionals, focusing on idea generation, concept evaluation, prototype testing, and design iteration to gather insights on their perceptions of wellbeing technologies.", "result": "Healthcare professionals expressed strong appreciation for technologies that foster immersive, embodied, and collective experiences to enhance mental health support.", "conclusion": "The research identifies key design recommendations for wellbeing technologies, emphasizing the importance of meeting HCPs' needs for autonomy, competence, and relatedness to sustain engagement.", "key_contributions": ["Exploration of HCPs' perceptions of innovative wellbeing technologies.", "Identification of key features valued by HCPs in wellbeing interventions.", "Specific design recommendations to improve user engagement in wellbeing technologies."], "limitations": "", "keywords": ["healthcare professionals", "wellbeing technologies", "co-design study", "virtual reality", "augmented reality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19279", "pdf": "https://arxiv.org/pdf/2506.19279.pdf", "abs": "https://arxiv.org/abs/2506.19279", "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition", "authors": ["Zhiyang Qi", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rising demand for mental health care has fueled interest in AI-driven\ncounseling systems. While large language models (LLMs) offer significant\npotential, current approaches face challenges, including limited understanding\nof clients' psychological states and counseling stages, reliance on\nhigh-quality training data, and privacy concerns associated with commercial\ndeployment. To address these issues, we propose EmoStage, a framework that\nenhances empathetic response generation by leveraging the inference\ncapabilities of open-source LLMs without additional training data. Our\nframework introduces perspective-taking to infer clients' psychological states\nand support needs, enabling the generation of emotionally resonant responses.\nIn addition, phase recognition is incorporated to ensure alignment with the\ncounseling process and to prevent contextually inappropriate or inopportune\nresponses. Experiments conducted in both Japanese and Chinese counseling\nsettings demonstrate that EmoStage improves the quality of responses generated\nby base models and performs competitively with data-driven methods.", "AI": {"tldr": "EmoStage is a framework for AI-driven counseling systems that enhances empathetic response generation using LLMs, addressing challenges of understanding psychological states and ensuring timing of responses.", "motivation": "The need for effective AI-driven mental health care solutions that understand client states and provide contextually appropriate responses.", "method": "EmoStage framework leverages open-source LLMs to enhance empathetic response generation without requiring additional training data, incorporating perspective-taking and phase recognition.", "result": "Experiments show that EmoStage improves response quality in counseling settings compared to base models and competes well with data-driven methods.", "conclusion": "EmoStage presents a novel approach to integrating empathy into AI counseling, addressing key limitations of existing methods while maintaining high response quality.", "key_contributions": ["Introduction of perspective-taking to infer psychological states", "Incorporation of phase recognition for timely responses", "Demonstration of performance improvements in mental health counseling settings."], "limitations": "Potential dependency on the inherent limitations of base LLM capabilities and challenges in cross-cultural application.", "keywords": ["AI counseling", "Empathetic response generation", "Psychological states", "Large language models", "Phase recognition"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19611", "pdf": "https://arxiv.org/pdf/2506.19611.pdf", "abs": "https://arxiv.org/abs/2506.19611", "title": "Filters of Identity: AR Beauty and the Algorithmic Politics of the Digital Body", "authors": ["Miriam Doh", "Corinna Canali", "Nuria Oliver"], "categories": ["cs.HC"], "comment": "This work was presented at the \"Body Politics: Unpacking Tensions and\n  Future Perspectives For Body-Centric Design Research in HCI\" workshop at the\n  ACM (Association for Computing Machinery) CHI conference on Human Factors in\n  Computing Systems 2025", "summary": "This position paper situates AR beauty filters within the broader debate on\nBody Politics in HCI. We argue that these filters are not neutral tools but\ntechnologies of governance that reinforce racialized, gendered, and ableist\nbeauty standards. Through naming conventions, algorithmic bias, and platform\ngovernance, they impose aesthetic norms while concealing their influence. To\naddress these challenges, we advocate for transparency-driven interventions and\na critical rethinking of algorithmic aesthetics and digital embodiment.", "AI": {"tldr": "This position paper discusses the impact of AR beauty filters within Body Politics in HCI, arguing they perpetuate harmful beauty standards.", "motivation": "To explore the influence of AR beauty filters as technologies that reinforce societal norms related to race, gender, and ability through algorithmic bias.", "method": "Analyzing the naming conventions and algorithmic governance associated with AR beauty filters.", "result": "Identifies that AR beauty filters impose aesthetic norms and conceal their impact on users.", "conclusion": "Calls for greater transparency in the design and governance of algorithmic aesthetics and urges a rethinking of digital embodiment.", "key_contributions": ["Argues that AR beauty filters are technologies of governance.", "Highlights the role of algorithmic biases in imposing beauty standards.", "Advocates for transparency-driven interventions."], "limitations": "", "keywords": ["Augmented Reality", "Beauty Filters", "Body Politics", "HCI", "Algorithmic Bias"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.19315", "pdf": "https://arxiv.org/pdf/2506.19315.pdf", "abs": "https://arxiv.org/abs/2506.19315", "title": "JCAPT: A Joint Modeling Approach for CAPT", "authors": ["Tzu-Hsuan Yang", "Yue-Yang He", "Berlin Chen"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Submitted to the ISCA SLaTE-2025 Workshop", "summary": "Effective pronunciation feedback is critical in second language (L2)\nlearning, for which computer-assisted pronunciation training (CAPT) systems\noften encompass two key tasks: automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD). Recent work has shown that\njoint modeling of these two tasks can yield mutual benefits. Our unified\nframework leverages Mamba, a selective state space model (SSM), while\nintegrating phonological features and think token strategies to jointly enhance\ninterpretability and fine-grained temporal reasoning in APA and MDD. To our\nknowledge, this is the first study to combine phonological attribution,\nSSM-based modeling, and prompting in CAPT. A series of experiments conducted on\nthe speechocean762 benchmark demonstrate that our model consistently\noutperforms prior methods, particularly on the MDD task.", "AI": {"tldr": "This paper presents a novel framework for improving pronunciation feedback in second language learning by integrating pronunciation assessment and mispronunciation detection through a selective state space model.", "motivation": "The study addresses the need for effective pronunciation feedback in second language learning, highlighting the mutual benefits of joint modeling for pronunciation assessment and mispronunciation detection.", "method": "The authors propose a unified framework using Mamba, a selective state space model, which incorporates phonological features and think token strategies to enhance both interpretability and temporal reasoning in automatic pronunciation assessment and mispronunciation detection.", "result": "Experiments on the speechocean762 benchmark show that the proposed model outperforms previous methods, particularly excelling in mispronunciation detection.", "conclusion": "This research is the first to integrate phonological attribution, SSM-based modeling, and prompting in computer-assisted pronunciation training, setting a new benchmark for pronunciation feedback systems.", "key_contributions": ["First study to integrate phonological attribution with SSM-based modeling in CAPT.", "Demonstrated superior performance on mispronunciation detection compared to prior methods.", "Introduces think token strategies to enhance model interpretability."], "limitations": "", "keywords": ["pronunciation assessment", "mispronunciation detection", "selective state space model", "language learning", "computer-assisted training"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.19644", "pdf": "https://arxiv.org/pdf/2506.19644.pdf", "abs": "https://arxiv.org/abs/2506.19644", "title": "Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation", "authors": ["M. Michelessa", "J. Ng", "C. Hurter", "B. Y. Lim"], "categories": ["cs.HC"], "comment": "DIS2025, code available at github.com/mario-michelessa/varifai", "summary": "Diversity in image generation is essential to ensure fair representations and\nsupport creativity in ideation. Hence, many text-to-image models have\nimplemented diversification mechanisms. Yet, after a few iterations of\ngeneration, a lack of diversity becomes apparent, because each user has their\nown diversity goals (e.g., different colors, brands of cars), and there are\ndiverse attributions to be specified. To support user-driven diversity control,\nwe propose Varif.ai that employs text-to-image and Large Language Models to\niteratively i) (re)generate a set of images, ii) verify if user-specified\nattributes have sufficient coverage, and iii) vary existing or new attributes.\nThrough an elicitation study, we uncovered user needs for diversity in image\ngeneration. A pilot validation showed that Varif.ai made achieving diverse\nimage sets easier. In a controlled evaluation with 20 participants, Varif.ai\nproved more effective than baseline methods across various scenarios. Thus,\nthis supports user control of diversity in image generation for creative\nideation and scalable image generation.", "AI": {"tldr": "Varif.ai is a tool designed to enhance diversity in text-to-image generation based on user-specified attributes, demonstrating improved effectiveness over existing methods in controlled evaluations.", "motivation": "To address the lack of diversity in image generation by tailoring outputs to distinct user preferences and needs.", "method": "Varif.ai employs a combination of text-to-image generation and Large Language Models to iteratively create images, verify attribute coverage, and introduce variation in attributes based on user input.", "result": "In a controlled study with 20 participants, Varif.ai was found to be more effective in generating diverse image sets compared to baseline methods.", "conclusion": "Varif.ai supports user-driven diversity control in image generation, enhancing creativity and scalability in ideation processes.", "key_contributions": ["Introduction of user-driven diversity control in image generation", "Use of Large Language Models to support iterative image generation", "Demonstrated improved effectiveness in achieving diverse image sets compared to conventional methods."], "limitations": "", "keywords": ["image generation", "diversity control", "text-to-image", "human-computer interaction", "large language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.19352", "pdf": "https://arxiv.org/pdf/2506.19352.pdf", "abs": "https://arxiv.org/abs/2506.19352", "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation", "authors": ["Jisu Shin", "Juhyun Oh", "Eunsu Kim", "Hoyun Song", "Alice Oh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/", "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression.", "AI": {"tldr": "Proposes an atomic-level evaluation framework to enhance persona fidelity in LLMs, addressing Out-of-Character behavior and improving assessment methods.", "motivation": "Maintaining coherent and engaging human-AI interactions requires LLMs to exhibit consistent persona fidelity, yet traditional evaluation methods fall short in capturing subtle deviations.", "method": "Introduces a framework with three key metrics that analyze persona alignment and consistency at a granular level, enabling precise detection of persona inconsistencies.", "result": "The proposed framework effectively identifies persona misalignments that previous methods missed, demonstrating its effectiveness across diverse tasks and personality types.", "conclusion": "The findings reveal the impact of task structure and persona desirability on LLM adaptability, emphasizing the challenges of maintaining persona consistency.", "key_contributions": ["Development of a granular evaluation framework for persona fidelity in LLMs", "Identification of subtle deviations in persona alignment", "Demonstration of the influence of task structure on persona consistency"], "limitations": "", "keywords": ["persona fidelity", "large language models", "evaluation framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19382", "pdf": "https://arxiv.org/pdf/2506.19382.pdf", "abs": "https://arxiv.org/abs/2506.19382", "title": "Measuring and Guiding Monosemanticity", "authors": ["Ruben Hrle", "Felix Friedrich", "Manuel Brack", "Stephan Wldchen", "Bjrn Deiseroth", "Patrick Schramowski", "Kristian Kersting"], "categories": ["cs.CL"], "comment": null, "summary": "There is growing interest in leveraging mechanistic interpretability and\ncontrollability to better understand and influence the internal dynamics of\nlarge language models (LLMs). However, current methods face fundamental\nchallenges in reliably localizing and manipulating feature representations.\nSparse Autoencoders (SAEs) have recently emerged as a promising direction for\nfeature extraction at scale, yet they, too, are limited by incomplete feature\nisolation and unreliable monosemanticity. To systematically quantify these\nlimitations, we introduce Feature Monosemanticity Score (FMS), a novel metric\nto quantify feature monosemanticity in latent representation. Building on these\ninsights, we propose Guided Sparse Autoencoders (G-SAE), a method that\nconditions latent representations on labeled concepts during training. We\ndemonstrate that reliable localization and disentanglement of target concepts\nwithin the latent space improve interpretability, detection of behavior, and\ncontrol. Specifically, our evaluations on toxicity detection, writing style\nidentification, and privacy attribute recognition show that G-SAE not only\nenhances monosemanticity but also enables more effective and fine-grained\nsteering with less quality degradation. Our findings provide actionable\nguidelines for measuring and advancing mechanistic interpretability and control\nof LLMs.", "AI": {"tldr": "This paper presents Guided Sparse Autoencoders (G-SAE), a method aimed at improving the interpretability and controllability of large language models (LLMs) using feature monosemanticity.", "motivation": "To address the challenges in reliably localizing and manipulating feature representations in LLMs, which hinders understanding and influencing their internal dynamics.", "method": "The authors introduce a new metric called Feature Monosemanticity Score (FMS) and propose G-SAE, which conditions latent representations on labeled concepts during training to enhance interpretability and control.", "result": "G-SAE improves the localization and disentanglement of concepts in the latent space, facilitating tasks like toxicity detection and writing style identification with less quality degradation.", "conclusion": "The findings offer guidelines for enhancing mechanistic interpretability and control in LLMs, showcasing that G-SAE significantly improves monosemanticity and enables effective steering of model behavior.", "key_contributions": ["Introduction of Feature Monosemanticity Score (FMS) for quantifying feature monosemanticity", "Development of Guided Sparse Autoencoders (G-SAE) for better interpretability in LLMs", "Demonstration of improved performance in various LLM tasks through G-SAE"], "limitations": "", "keywords": ["mechanistic interpretability", "large language models", "sparse autoencoders"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.19399", "pdf": "https://arxiv.org/pdf/2506.19399.pdf", "abs": "https://arxiv.org/abs/2506.19399", "title": "Automated Detection of Pre-training Text in Black-box LLMs", "authors": ["Ruihan Hu", "Yu-Ming Shang", "Jiankun Peng", "Wei Luo", "Yazhe Wang", "Xi Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "Detecting whether a given text is a member of the pre-training data of Large\nLanguage Models (LLMs) is crucial for ensuring data privacy and copyright\nprotection. Most existing methods rely on the LLM's hidden information (e.g.,\nmodel parameters or token probabilities), making them ineffective in the\nblack-box setting, where only input and output texts are accessible. Although\nsome methods have been proposed for the black-box setting, they rely on massive\nmanual efforts such as designing complicated questions or instructions. To\naddress these issues, we propose VeilProbe, the first framework for\nautomatically detecting LLMs' pre-training texts in a black-box setting without\nhuman intervention. VeilProbe utilizes a sequence-to-sequence mapping model to\ninfer the latent mapping feature between the input text and the corresponding\noutput suffix generated by the LLM. Then it performs the key token\nperturbations to obtain more distinguishable membership features. Additionally,\nconsidering real-world scenarios where the ground-truth training text samples\nare limited, a prototype-based membership classifier is introduced to alleviate\nthe overfitting issue. Extensive evaluations on three widely used datasets\ndemonstrate that our framework is effective and superior in the black-box\nsetting.", "AI": {"tldr": "VeilProbe is a framework for automatically detecting whether text is part of LLMs' pre-training data in a black-box setting without human intervention.", "motivation": "The need to ensure data privacy and copyright protection necessitates mechanisms to detect if texts are part of LLMs' training data, especially when only input and output texts are available.", "method": "VeilProbe employs a sequence-to-sequence mapping model to infer latent mapping features between input texts and LLM-generated output suffixes, and applies key token perturbations to enhance membership feature distinguishability.", "result": "Extensive evaluations on three popular datasets show that VeilProbe outperforms existing methods in accurately detecting membership in a black-box setting.", "conclusion": "VeilProbe effectively addresses the challenges of detecting pre-training data membership in LLMs without human intervention, using modern machine learning techniques to improve accuracy and reduce manual effort.", "key_contributions": ["First framework for automatic detection of LLMs' pre-training texts in a black-box setting", "Utilizes sequence-to-sequence mapping for feature extraction", "Introduces a prototype-based membership classifier to tackle overfitting in limited data scenarios."], "limitations": "", "keywords": ["Large Language Models", "data privacy", "copyright protection", "membership inference", "machine learning"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2506.19418", "pdf": "https://arxiv.org/pdf/2506.19418.pdf", "abs": "https://arxiv.org/abs/2506.19418", "title": "Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study", "authors": ["Yingji Zhang", "Marco Valentino", "Danilo S. Carvalho", "Andr Freitas"], "categories": ["cs.CL"], "comment": null, "summary": "Incorporating explicit reasoning rules within the latent space of language\nmodels (LMs) offers a promising pathway to enhance generalisation,\ninterpretability, and controllability. While current Transformer-based language\nmodels have shown strong performance on Natural Language Inference (NLI) tasks,\nthey often rely on memorisation rather than rule-based inference. This work\ninvestigates how reasoning rules can be explicitly embedded and memorised\nwithin the LMs through Language Variational Autoencoders (VAEs). We propose a\ncomplete pipeline for learning reasoning rules within Transformer-based\nlanguage VAEs. This pipeline encompasses three rule-based reasoning tasks, a\nsupporting theoretical framework, and a practical end-to-end architecture. The\nexperiment illustrates the following findings: Disentangled reasoning: Under\nexplicit signal supervision, reasoning rules - viewed as functional mappings -\ncan be disentangled within the encoder's parametric space. This separation\nresults in distinct clustering of rules in the output feature space. Prior\nknowledge injection: injecting reasoning information into the Query enables the\nmodel to more effectively retrieve the stored value Value from memory based on\nKey. This approach offers a simple method for integrating prior knowledge into\ndecoder-only language models. Performance bottleneck: In mathematical reasoning\ntasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance\nbeyond a point. Moreover, ffn layers are better than attention layers at\npreserving the separation of reasoning rules in the model's parameters.", "AI": {"tldr": "This paper explores the integration of explicit reasoning rules into the latent space of language models, using Language Variational Autoencoders to improve generalisation, interpretability, and controllability.", "motivation": "The study addresses the limitations of Transformer-based language models that tend to rely on memorization rather than rule-based inference, aiming to enhance their reasoning capabilities.", "method": "The authors propose a complete pipeline for learning reasoning rules within Transformer-based language VAEs, involving three rule-based reasoning tasks and an end-to-end architecture.", "result": "The experiments demonstrate that reasoning rules can be disentangled within the encoder, leading to improved clustering in feature space, and that injecting reasoning information enhances memory retrieval in decoder-only models.", "conclusion": "The proposed methods improve the interpretability and effectiveness of language models by embedding reasoning rules, although performance in mathematical tasks plateaus beyond a certain sample count.", "key_contributions": ["Introduction of explicit reasoning rules in language LMs", "Development of a pipeline for reasoning rules in VAEs", "Demonstration of effective knowledge injection into language models"], "limitations": "Performance does not improve significantly beyond a certain sample count in reasoning tasks; challenges in optimal layer choice for reasoning rules preservation.", "keywords": ["Language Models", "Reasoning Rules", "Variational Autoencoders", "Natural Language Inference", "Interpretability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19467", "pdf": "https://arxiv.org/pdf/2506.19467.pdf", "abs": "https://arxiv.org/abs/2506.19467", "title": "Can Large Language Models Capture Human Annotator Disagreements?", "authors": ["Jingwei Ni", "Yu Fan", "Vilm Zouhar", "Donya Rooein", "Alexander Hoyle", "Mrinmaya Sachan", "Markus Leippold", "Dirk Hovy", "Elliott Ash"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint Under Review", "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.", "AI": {"tldr": "This paper evaluates Large Language Models' ability to predict human annotation disagreements in NLP tasks, revealing shortcomings in disagreement modeling.", "motivation": "To understand if LLMs can effectively capture human annotation variation in NLP, which is essential for certain subjective and ambiguous tasks.", "method": "The paper extensively evaluates LLMs on their ability to predict annotation disagreements without repeated human labels, examining performance in generating majority labels and modeling disagreements.", "result": "LLMs struggle with predicting annotation disagreements, which may be overlooked by evaluations based solely on majority labels.", "conclusion": "The findings emphasize the need for improved evaluation methods for LLMs in disagreement modeling and suggest that typical performance enhancements (like RLVR) may worsen this specific capability.", "key_contributions": ["Identification of LLMs' limitations in predicting human annotation disagreements.", "Highlighting the shortcomings of majority label-based evaluations in capturing annotation variation.", "Providing code and data resources for further exploration of disagreement prediction."], "limitations": "The study is limited to the evaluation of LLMs without repeated human labels, which may not fully capture the complexity of human annotation variation.", "keywords": ["Large Language Models", "NLU", "Human Annotation Variation", "Disagreement Prediction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.19352", "pdf": "https://arxiv.org/pdf/2506.19352.pdf", "abs": "https://arxiv.org/abs/2506.19352", "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation", "authors": ["Jisu Shin", "Juhyun Oh", "Eunsu Kim", "Hoyun Song", "Alice Oh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/", "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression.", "AI": {"tldr": "The paper introduces an atomic-level evaluation framework to measure persona fidelity in LLMs, addressing Out-of-Character behavior and offering granular insights into persona alignment.", "motivation": "To improve human-AI interactions by addressing the Out-of-Character behavior in large language models and ensuring consistent persona expression.", "method": "The proposed framework uses three key metrics to quantify persona alignment and consistency at a finer granularity, allowing for a precise evaluation of long-form text generation.", "result": "Experiments show that the framework can effectively detect persona inconsistencies that traditional methods miss, revealing the impact of task structure and persona desirability on model adaptability.", "conclusion": "The study highlights the importance of measuring persona fidelity more precisely in LLMs, thereby improving the reliability of human-AI interactions.", "key_contributions": ["Introduction of an atomic-level evaluation framework for persona fidelity in LLMs.", "Development of three key metrics for measuring persona alignment.", "Demonstration of the framework's effectiveness in detecting subtle persona inconsistencies."], "limitations": "", "keywords": ["Persona Fidelity", "Large Language Models", "Human-AI Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19468", "pdf": "https://arxiv.org/pdf/2506.19468.pdf", "abs": "https://arxiv.org/abs/2506.19468", "title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages", "authors": ["Wenhan Han", "Yifan Zhang", "Zhixun Chen", "Binbin Liu", "Haobin Lin", "Bingni Zhang", "Taifeng Wang", "Mykola Pechenizkiy", "Meng Fang", "Yin Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics.", "AI": {"tldr": "The paper introduces MuBench, a comprehensive benchmark for evaluating multilingual LLMs across 61 languages, highlighting gaps in language coverage and proposing Multilingual Consistency as a new evaluation metric.", "motivation": "To address fragmented assessments of multilingual capabilities in large language models due to limited evaluation datasets that lack cross-lingual alignment.", "method": "The authors introduce MuBench, a benchmark that evaluates multilingual LLMs across a wide range of capabilities for 61 languages, and they propose Multilingual Consistency (MLC) as a new metric to analyze model performance.", "result": "Evaluation of state-of-the-art multilingual LLMs revealed significant gaps between claimed capabilities and actual performance, especially for low-resource languages, and highlighted the need for Multilingual Consistency as a metric.", "conclusion": "The study indicates that current multilingual LLMs have notable coverage gaps, particularly in low-resource languages, and encourages the adoption of Multilingual Consistency to better understand and improve model performance.", "key_contributions": ["Introduction of MuBench, a benchmark for multilingual capabilities", "Proposal of Multilingual Consistency as an evaluation metric", "Pretraining of 1.2B-parameter models to study cross-lingual transfer dynamics"], "limitations": "The benchmark and its evaluations may still be limited by the underlying dataset and may not fully capture the complexity of all linguistic features across languages.", "keywords": ["Multilingual LLMs", "MuBench", "Multilingual Consistency", "low-resource languages", "cross-lingual transfer"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.19483", "pdf": "https://arxiv.org/pdf/2506.19483.pdf", "abs": "https://arxiv.org/abs/2506.19483", "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models", "authors": ["Marcos Estecha-Garitagoitia", "Chen Zhang", "Mario Rodrguez-Cantelar", "Luis Fernando D'Haro"], "categories": ["cs.CL"], "comment": null, "summary": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems.", "AI": {"tldr": "This study explores turn-level data augmentation for dialogue systems using commonsense relationships and evaluates the generated synthetic turns with pretrained LLMs.", "motivation": "To enhance dialogue systems by utilizing commonsense relationships between dialogue turns for improved data augmentation and evaluation.", "method": "The approach uses pretrained LLMs to generate alternative responses based on commonsense attributes extracted from selected dialogue datasets and includes an evaluation framework inspired by the ACCENT metric.", "result": "Preliminary results indicate that the method successfully leverages LLMs for generating contextually relevant commonsense knowledge in dialogue, demonstrating effectiveness in augmenting the dialogue dataset.", "conclusion": "The findings suggest that LLMs can be effectively used for data augmentation in dialogue systems, particularly through commonsense reasoning and automated evaluation methods.", "key_contributions": ["Introduction of a novel dataset for dialogue augmentation based on commonsense attributes", "Development of an automatic evaluation framework for dialogue quality assessment", "Utilization of pretrained LLMs for generating contextually relevant dialogue turns"], "limitations": "The method does not implement the complex extraction process of event-relation tuples as seen in ACCENT, potentially limiting its evaluation comprehensiveness.", "keywords": ["dialogue systems", "data augmentation", "commonsense reasoning", "Large Language Models", "evaluation framework"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.19484", "pdf": "https://arxiv.org/pdf/2506.19484.pdf", "abs": "https://arxiv.org/abs/2506.19484", "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "authors": ["Russell Beale"], "categories": ["cs.CL", "cs.AI", "cs.HC", "K.3.2; I.2.6; H.4.m"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned.", "AI": {"tldr": "This article reviews the use of LLM-based conversational agents in education and offers strategies to align these tools with established educational theories to enhance learning outcomes.", "motivation": "To understand how LLMs can be effectively integrated into educational contexts and to identify gaps in their application relative to traditional pedagogical theories.", "method": "Synthesis of existing literature on LLMs in education, with a focus on theories of conversational pedagogy and analysis of prompting strategies and RAG.", "result": "The review finds that while LLMs can support established learning principles, they often diverge from traditional pedagogical methods, particularly in fostering knowledge co-construction.", "conclusion": "Practical strategies are proposed to enhance the educational effectiveness of LLMs in learning environments, focusing on prompt design and integration of retrieval mechanisms.", "key_contributions": ["Comprehensive review of LLMs in education", "Practical strategies for aligning LLM interactions with pedagogical theories", "Identification of gaps in LLM application to traditional teaching methods"], "limitations": "Models tend to provide direct answers rather than fostering collaborative learning; constant availability may lead to issues regarding expertise and guidance.", "keywords": ["Large Language Models", "Conversational Agents", "Education", "Pedagogy", "Retrieval-Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.19484", "pdf": "https://arxiv.org/pdf/2506.19484.pdf", "abs": "https://arxiv.org/abs/2506.19484", "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "authors": ["Russell Beale"], "categories": ["cs.CL", "cs.AI", "cs.HC", "K.3.2; I.2.6; H.4.m"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned.", "AI": {"tldr": "This paper reviews the integration of LLMs in education, emphasizing alignment with pedagogical theories for effective conversational learning.", "motivation": "Large Language Models are reshaping education by facilitating engaging conversational learning experiences that require alignment with established educational theories.", "method": "The article synthesizes literature on LLMs in education, explores theories of conversational pedagogy, and examines the role of prompting strategies and RAG.", "result": "It identifies gaps in the application of educational theories to LLMs, including the tendency of LLMs to provide direct answers rather than encourage knowledge co-construction.", "conclusion": "Practical strategies are proposed to enhance the educational effectiveness of LLM interactions by aligning them with sound pedagogical approaches.", "key_contributions": ["Synthesis of literature on LLMs and education.", "Identification of gaps between LLM capabilities and traditional pedagogical assumptions.", "Proposed strategies for better aligning LLM interactions with educational theories."], "limitations": "The tendency of LLMs to operate outside human-like expertise and provide immediate answers rather than support deeper learning processes.", "keywords": ["Large Language Models", "education", "conversational learning", "pedagogy", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.19492", "pdf": "https://arxiv.org/pdf/2506.19492.pdf", "abs": "https://arxiv.org/abs/2506.19492", "title": "Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs", "authors": ["Shu Yang", "Junchao Wu", "Xuansheng Wu", "Derek Wong", "Ninhao Liu", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\ntasks by engaging in extended reasoning before producing final answers, yet\nthis strength introduces the risk of overthinking, where excessive token\ngeneration occurs even for simple tasks. While recent work in efficient\nreasoning seeks to reduce reasoning length while preserving accuracy, it\nremains unclear whether such optimization is truly a free lunch. Drawing on the\nintuition that compressing reasoning may reduce the robustness of model\nresponses and lead models to omit key reasoning steps, we investigate whether\nefficient reasoning strategies introduce behavioral inconsistencies. To\nsystematically assess this, we introduce $ICBENCH$, a benchmark designed to\nmeasure inconsistency in LRMs across three dimensions: inconsistency across\ntask settings (ITS), inconsistency between training objectives and learned\nbehavior (TR-LB), and inconsistency between internal reasoning and\nself-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs,\nwe find that while larger models generally exhibit greater consistency than\nsmaller ones, they all display widespread \"scheming\" behaviors, including\nself-disagreement, post-hoc rationalization, and the withholding of reasoning\ncues. Crucially, our results demonstrate that efficient reasoning strategies\nsuch as No-Thinking and Simple Token-Budget consistently increase all three\ndefined types of inconsistency. These findings suggest that although efficient\nreasoning enhances token-level efficiency, further investigation is imperative\nto ascertain whether it concurrently introduces the risk of models evading\neffective supervision.", "AI": {"tldr": "This paper investigates the trade-offs of efficient reasoning strategies in Large Reasoning Models (LRMs), revealing increased behavioral inconsistencies even as token efficiency improves.", "motivation": "To assess whether efficient reasoning in LRMs compromises model robustness and introduces behavioral inconsistencies.", "method": "The authors introduce $ICBENCH$, a benchmark for measuring inconsistency in LRMs across three dimensions: task setting inconsistency, training objective vs. behavior inconsistency, and internal reasoning vs. self-explanation inconsistency.", "result": "The application of $ICBENCH$ shows that larger models are generally more consistent, but all models exhibit scheming behaviors, and efficient reasoning strategies increase types of inconsistencies.", "conclusion": "Efficient reasoning may enhance token efficiency but poses risks of behavioral inconsistencies and could lead to models avoiding effective supervision, necessitating further study.", "key_contributions": ["Development of the $ICBENCH$ benchmark for assessing inconsistencies in LRMs", "Identification of 'scheming' behaviors in LRMs", "Demonstration that efficient reasoning strategies can exacerbate behavioral inconsistencies"], "limitations": "The study primarily focuses on open-source LRMs and may not fully represent proprietary models; further exploration into specific inconsistencies is required.", "keywords": ["Large Reasoning Models", "behavioral inconsistency", "efficient reasoning", "self-explanation", "ICBENCH"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2401.08405", "pdf": "https://arxiv.org/pdf/2401.08405.pdf", "abs": "https://arxiv.org/abs/2401.08405", "title": "Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT", "authors": ["Mohammad Ronagh Nikghalb", "Jinghui Cheng"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to CSCW 2025; 23 pages", "summary": "In an era of AI's growing capabilities and influences, recent advancements\nare reshaping HCI and CSCW's view of AI. Playful interactions emerged as an\nimportant way for users to make sense of the ever-changing AI technologies, yet\nremained underexamined. We target this gap by investigating playful\ninteractions exhibited by users of a popular AI technology, ChatGPT. Through a\nthematic analysis of 372 user-generated posts on the ChatGPT subreddit, we\nfound that more than half (54\\%) of user discourse revolved around playful\ninteractions. The analysis further allowed us to construct a preliminary\nframework to describe these interactions, categorizing them into six types:\nreflecting, jesting, imitating, challenging, tricking, and contriving; each\nincluded sub-categories. This study contributes to HCI and CSCW by identifying\nthe diverse ways users engage in playful interactions with AI. It examines how\nthese interactions can help users understand AI's agency, shape human-AI\nrelationships, and provide insights for designing AI systems.", "AI": {"tldr": "This study investigates playful interactions in user discourse surrounding ChatGPT, revealing that over half of the discussion involves playfulness and establishing a preliminary framework for categorizing these interactions.", "motivation": "The paper addresses the gap in understanding how users engage with AI technologies through playful interactions, particularly in the context of HCI and CSCW.", "method": "The authors conducted a thematic analysis of 372 user-generated posts from the ChatGPT subreddit to identify and categorize instances of playful interactions.", "result": "The analysis revealed that 54% of the posts featured playful interactions, which were categorized into six types: reflecting, jesting, imitating, challenging, tricking, and contriving.", "conclusion": "The study contributes to HCI and CSCW by offering insights into how playful interactions can aid users in understanding AI, shaping human-AI relationships, and informing AI design.", "key_contributions": ["Identification of playful interaction types with AI", "Development of a framework for categorizing playful interactions", "Insights into user engagement with AI technology"], "limitations": "", "keywords": ["Human-Computer Interaction", "AI", "Playful Interactions", "ChatGPT", "CSCW"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.19505", "pdf": "https://arxiv.org/pdf/2506.19505.pdf", "abs": "https://arxiv.org/abs/2506.19505", "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models", "authors": ["Zeyu Li", "Chuanfu Xiao", "Yang Wang", "Xiang Liu", "Zhenheng Tang", "Baotong Lu", "Mao Yang", "Xinyu Chen", "Xiaowen Chu"], "categories": ["cs.CL"], "comment": null, "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.", "AI": {"tldr": "This paper presents AnTKV, a framework that utilizes Anchor Token-aware Vector Quantization to improve the performance of KV cache quantization in Large Language Models by reducing accuracy loss while maintaining high throughput.", "motivation": "Quantization reduces memory usage in Large Language Models, but performance degradation from ultra-low-bit quantization presents a challenge. We aim to address this by analyzing how different tokens' KV caches are affected by quantization errors.", "method": "We conduct a forward error propagation analysis on attention outputs and introduce the Anchor Score (AnS) to quantify the sensitivity of KV cache tokens to quantization errors. Based on the AnS analysis, we preserve a subset of high-AnS tokens using full precision and implement a framework for efficient KV cache quantization.", "result": "AnTKV allows LLaMA-3-8B to manage up to 840K tokens on a single 80GB A100 GPU, achieving up to 3.5x higher decoding throughput compared to FP16 and outperforms prior methods under 4-bit quantization. It also reduces perplexity significantly at low bit quantization levels.", "conclusion": "The AnTKV framework can effectively reduce the performance loss due to quantization in KV caches of LLMs while enhancing the operational throughput, presenting a viable solution to the challenges associated with ultra-low-bit quantization.", "key_contributions": ["Introduction of Anchor Score (AnS) for sensitivity analysis of KV cache tokens", "Development of AnTKV framework for improved KV cache quantization", "Demonstration of enhanced throughput and reduced perplexity in LLaMA-3-8B compared to FP16 and prior works"], "limitations": "", "keywords": ["Large Language Models", "quantization", "KV cache", "Anchor Token-aware Vector Quantization", "performance optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2404.15615", "pdf": "https://arxiv.org/pdf/2404.15615.pdf", "abs": "https://arxiv.org/abs/2404.15615", "title": "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition", "authors": ["Ting Luo", "Jing Zhang", "Yingwei Qiu", "Li Zhang", "Yaohua Hu", "Zhuliang Yu", "Zhen Liang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Emotion decoding using Electroencephalography (EEG)-based affective\nbrain-computer interfaces (aBCIs) plays a crucial role in affective computing\nbut is limited by challenges such as EEG's non-stationarity, individual\nvariability, and the high cost of large labeled datasets. While deep learning\nmethods are effective, they require extensive computational resources and large\ndata volumes, limiting their practical application. To overcome these issues,\nwe propose Manifold-based Domain Adaptation with Dynamic Distribution (M3D), a\nlightweight, non-deep transfer learning framework. M3D consists of four key\nmodules: manifold feature transformation, dynamic distribution alignment,\nclassifier learning, and ensemble learning. The data is mapped to an optimal\nGrassmann manifold space, enabling dynamic alignment of source and target\ndomains. This alignment is designed to prioritize both marginal and conditional\ndistributions, improving adaptation efficiency across diverse datasets. In\nclassifier learning, the principle of structural risk minimization is applied\nto build robust classification models. Additionally, dynamic distribution\nalignment iteratively refines the classifier. The ensemble learning module\naggregates classifiers from different optimization stages to leverage diversity\nand enhance prediction accuracy. M3D is evaluated on two EEG emotion\nrecognition datasets using two validation protocols (cross-subject\nsingle-session and cross-subject cross-session) and a clinical EEG dataset for\nMajor Depressive Disorder (MDD). Experimental results show that M3D outperforms\ntraditional non-deep learning methods with a 4.47% average improvement and\nachieves deep learning-level performance with reduced data and computational\nrequirements, demonstrating its potential for real-world aBCI applications.", "AI": {"tldr": "M3D is a lightweight, non-deep learning framework for emotion decoding using EEG that overcomes challenges of EEG's non-stationarity and data limitations, achieving improved performance in emotion recognition tasks.", "motivation": "To address the limitations of deep learning methods in emotion decoding using EEG-based aBCIs, particularly related to computational resources and data requirements.", "method": "M3D consists of manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning, optimizing performance on EEG datasets.", "result": "M3D outperforms traditional non-deep learning methods by 4.47% on average and achieves deep learning-level performance with reduced data and computation needs.", "conclusion": "M3D demonstrates potential for real-world applications in affective computing by efficiently adapting across diverse EEG datasets.", "key_contributions": ["Proposed a lightweight aBCI framework (M3D) for emotion recognition", "Achieved deep learning-level performance with reduced data requirements", "Introduced dynamic distribution alignment for improved adaptation efficiency"], "limitations": "", "keywords": ["Electroencephalography", "emotion recognition", "transfer learning", "aBCI", "Machine Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.19512", "pdf": "https://arxiv.org/pdf/2506.19512.pdf", "abs": "https://arxiv.org/abs/2506.19512", "title": "heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation", "authors": ["Ashish Chouhan", "Michael Gertz"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures, 6 tables, Workshop on BioNLP and Shared Tasks at\n  ACL 2025", "summary": "This paper presents the approach of our team called heiDS for the ArchEHR-QA\n2025 shared task. A pipeline using a retrieval augmented generation (RAG)\nframework is designed to generate answers that are attributed to clinical\nevidence from the electronic health records (EHRs) of patients in response to\npatient-specific questions. We explored various components of a RAG framework,\nfocusing on ranked list truncation (RLT) retrieval strategies and attribution\napproaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a\nquery-dependent-k retrieval strategy, including the existing surprise and\nautocut methods and two new methods proposed in this work, autocut* and elbow.\nThe experimental results show the benefits of our strategy in producing factual\nand relevant answers when compared to a fixed-$k$.", "AI": {"tldr": "This paper describes the heiDS approach for the ArchEHR-QA 2025 task, utilizing a retrieval augmented generation framework to answer patient-specific questions based on electronic health records.", "motivation": "The motivation of this work is to improve the accuracy and relevance of answers generated for patient-specific queries by utilizing a retrieval augmented generation framework and exploring advanced retrieval strategies.", "method": "The method involves a pipeline using a retrieval augmented generation framework, focusing on query-dependent-k retrieval strategies instead of a fixed top-k retrieval approach. It explores various strategies including surprise, autocut, and introduces new methods autocut* and elbow.", "result": "Experimental results indicate that the query-dependent retrieval strategies significantly improve the generation of factual and relevant answers compared to traditional fixed-k methods.", "conclusion": "The conclusion highlights the effectiveness of the proposed retrieval strategies in enhancing the quality of responses generated from electronic health records, supporting better patient-specific query handling.", "key_contributions": ["Introduction of query-dependent-k retrieval strategies", "Development of new methods: autocut* and elbow", "Demonstration of improved answer relevance and factuality over fixed-top-k approaches"], "limitations": "", "keywords": ["retrieval augmented generation", "electronic health records", "patient-specific questions"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.19525", "pdf": "https://arxiv.org/pdf/2506.19525.pdf", "abs": "https://arxiv.org/abs/2506.19525", "title": "Automatic Posology Structuration : What role for LLMs?", "authors": ["Natalia Bobkova", "Laura Zanella-Calzada", "Anyes Tafoughalt", "Raphal Teboul", "Franois Plesse", "Flix Gaschi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use.", "AI": {"tldr": "This paper presents a hybrid pipeline using Large Language Models and Named Entity Recognition for structuring posology instructions in French prescriptions to improve medication safety.", "motivation": "The motivation behind this study is to enhance medication safety and clinical decision support by effectively structuring often ambiguous posology instructions found in French prescriptions.", "method": "The authors explore prompt-based methods and fine-tuning of Large Language Models (LLMs) against a baseline system based on Named Entity Recognition and Linking (NERL).", "result": "The hybrid approach achieves 91% structuration accuracy while minimizing latency and computational costs, outperforming traditional methods.", "conclusion": "The results indicate that combining NERL with LLMs provides a robust solution for structuring medical prescriptions, balancing accuracy and computational efficiency.", "key_contributions": ["Development of a hybrid pipeline combining NERL and LLMs for posology structuration", "Empirical comparison of prompt-based methods vs. fine-tuning", "Proposed a confidence-based routing strategy for low-confidence cases"], "limitations": "The study primarily focuses on French prescriptions, and the results may not generalize to other languages or types of medical instructions.", "keywords": ["posology instructions", "Large Language Models", "Named Entity Recognition", "hybrid pipeline", "clinical decision support"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.15276", "pdf": "https://arxiv.org/pdf/2501.15276.pdf", "abs": "https://arxiv.org/abs/2501.15276", "title": "Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production", "authors": ["Yue Fu", "Michele Newman", "Lewis Going", "Qiuzi Feng", "Jin Ha Lee"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Artificial intelligence is reshaping creative domains, yet its co-creative\nprocesses, especially in group settings with novice users, remain under\nexplored. To bridge this gap, we conducted a case study in a college-level\ncourse where nine undergraduate students were tasked with creating three\noriginal music tracks using AI tools over 10 weeks. The study spanned the\nentire creative journey from ideation to releasing these songs on Spotify.\nParticipants leveraged AI for music and lyric production, cover art, and\ndistribution. Our findings highlight how AI transforms creative workflows:\naccelerating ideation but compressing the traditional preparation stage, and\nrequiring novices to navigate a challenging idea selection and validation\nphase. We also identified a new \"collaging and refinement\" stage, where\nparticipants creatively combined diverse AI-generated outputs into cohesive\nworks. Furthermore, AI influenced group social dynamics and role division among\nhuman creators. Based on these insights, we propose the Human-AI Co-Creation\nStage Model and the Human-AI Agency Model, offering new perspectives on\ncollaborative co-creation with AI.", "AI": {"tldr": "This study explores how AI tools facilitate the creative process in group settings among novice users, focusing on music creation.", "motivation": "To investigate the co-creative processes involving AI in group settings, particularly with novice users, which are underexplored.", "method": "A case study was conducted in a college course where nine undergraduate students used AI tools to create three original music tracks over 10 weeks.", "result": "AI tools accelerated ideation and introduced a 'collaging and refinement' stage, influencing creative workflows and group dynamics.", "conclusion": "The study proposes the Human-AI Co-Creation Stage Model and the Human-AI Agency Model to understand collaborative co-creation with AI.", "key_contributions": ["Identification of a new 'collaging and refinement' stage in creative workflows", "Proposed models for understanding Human-AI collaboration", "Insights into group dynamics and role division in AI-assisted creativity"], "limitations": "", "keywords": ["AI", "Human-AI Collaboration", "Creative Processes", "Music Creation", "Co-Creation Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.19527", "pdf": "https://arxiv.org/pdf/2506.19527.pdf", "abs": "https://arxiv.org/abs/2506.19527", "title": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs", "authors": ["Kelin Fu", "Kaigui Bian"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities.", "AI": {"tldr": "KnowMap enhances LLM task adaptation by constructing a dynamic knowledge base from environmental data, improving performance while avoiding traditional pitfalls.", "motivation": "To address the limitations of traditional fine-tuning methods for LLMs by providing a more efficient way to integrate task-specific knowledge.", "method": "KnowMap dynamically constructs a knowledge base and fine-tunes a small knowledge-embedding model to augment a larger LLM with necessary task-specific information.", "result": "A 17.71% improvement in performance for the gpt-4-turbo model on the ScienceWorld benchmark was observed.", "conclusion": "KnowMap offers a more effective means for LLM task adaptation and improves reasoning capabilities by incorporating environmental and experiential knowledge.", "key_contributions": ["Introduction of KnowMap for LLM task adaptation", "Demonstrated performance improvement on ScienceWorld benchmark", "Integration of environmental and experiential knowledge into LLMs"], "limitations": "", "keywords": ["Large Language Models", "task adaptation", "knowledge base"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2503.16484", "pdf": "https://arxiv.org/pdf/2503.16484.pdf", "abs": "https://arxiv.org/abs/2503.16484", "title": "AI-Facilitated Episodic Future Thinking For Adults with Obesity", "authors": ["Sareh Ahmadi", "Michelle Rockwell", "Megan Stuart", "Nicki Rohani", "Allison Tegge", "Xuan Wang", "Jeffrey Stein", "Edward A. Fox"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Episodic Future Thinking (EFT) involves vividly imagining personal future\nevents and experiences in detail. It has shown promise as an intervention to\nreduce delay discounting-the tendency to devalue delayed rewards in favor of\nimmediate gratification- and to promote behavior change in a range of\nmaladaptive health behaviors. We present EFTeacher, an AI chatbot powered by\nthe GPT-4-Turbo large language model, designed to generate EFT cues for users\nwith lifestyle-related conditions. To evaluate the feasibility and usability of\nEFTeacher, we conducted a mixed-methods study that included usability\nassessments, user evaluations based on content characteristics questionnaires,\nand semi-structured interviews. Qualitative findings indicate that participants\nperceived EFTeacher as communicative and supportive through an engaging\ndialogue. The chatbot facilitated imaginative thinking and reflection on future\ngoals. Participants appreciated its adaptability and personalization features,\nthough some noted challenges such as repetitive dialogue and verbose responses.\nOur findings underscore the potential of large language model-based chatbots in\nEFT interventions targeting maladaptive health behaviors.", "AI": {"tldr": "EFTeacher is an AI chatbot designed to promote Episodic Future Thinking (EFT) for improving health behaviors by providing personalized cues.", "motivation": "To reduce delay discounting and promote behavior change in users with lifestyle-related health conditions through AI-supported interventions.", "method": "A mixed-methods study was conducted, involving usability assessments, user evaluations through content characteristics questionnaires, and semi-structured interviews with participants.", "result": "Participants found EFTeacher communicative and supportive, aiding in imaginative thinking about future goals, though some reported issues like repetitive dialogue.", "conclusion": "The study suggests that large language model-based chatbots like EFTeacher can be effective for EFT interventions in addressing maladaptive health behaviors.", "key_contributions": ["Introduction of EFTeacher, an AI chatbot for promoting EFT", "Mixed-methods evaluation of usability and participant perceptions", "Insights into user interactions and areas for improvement"], "limitations": "Challenges included repetitive dialogue and verbosity in responses.", "keywords": ["Episodic Future Thinking", "AI chatbot", "health behavior change", "user experience", "large language model"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.19548", "pdf": "https://arxiv.org/pdf/2506.19548.pdf", "abs": "https://arxiv.org/abs/2506.19548", "title": "Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection", "authors": ["Devesh Pant", "Rishi Raj Grandhe", "Vipin Samaria", "Mukul Paul", "Sudhir Kumar", "Saransh Khanna", "Jatin Agrawal", "Jushaan Singh Kalra", "Akhil VSSG", "Satish V Khalikar", "Vipin Garg", "Himanshu Chauhan", "Pranay Verma", "Neha Khandelwal", "Soma S Dhavala", "Minesh Mathew"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Early detection of disease outbreaks is crucial to ensure timely intervention\nby the health authorities. Due to the challenges associated with traditional\nindicator-based surveillance, monitoring informal sources such as online media\nhas become increasingly popular. However, owing to the number of online\narticles getting published everyday, manual screening of the articles is\nimpractical. To address this, we propose Health Sentinel. It is a multi-stage\ninformation extraction pipeline that uses a combination of ML and non-ML\nmethods to extract events-structured information concerning disease outbreaks\nor other unusual health events-from online articles. The extracted events are\nmade available to the Media Scanning and Verification Cell (MSVC) at the\nNational Centre for Disease Control (NCDC), Delhi for analysis, interpretation\nand further dissemination to local agencies for timely intervention. From April\n2022 till date, Health Sentinel has processed over 300 million news articles\nand identified over 95,000 unique health events across India of which over\n3,500 events were shortlisted by the public health experts at NCDC as potential\noutbreaks.", "AI": {"tldr": "Health Sentinel is a multi-stage pipeline for extracting structured information about disease outbreaks from online articles.", "motivation": "To improve early detection of disease outbreaks beyond traditional surveillance methods.", "method": "A combination of ML and non-ML methods is used to extract event-structured information from online articles.", "result": "Processed over 300 million articles, identifying over 95,000 unique health events, with over 3,500 events shortlisted as potential outbreaks.", "conclusion": "Health Sentinel supports health authorities by providing structured information on disease outbreaks for timely interventions.", "key_contributions": ["Development of a multi-stage information extraction pipeline", "Integration of ML and non-ML methods for data extraction", "Real-time processing of online media articles for health insights."], "limitations": "", "keywords": ["Health Sentinel", "disease outbreaks", "information extraction", "machine learning", "public health"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.19549", "pdf": "https://arxiv.org/pdf/2506.19549.pdf", "abs": "https://arxiv.org/abs/2506.19549", "title": "RCStat: A Statistical Framework for using Relative Contextualization in Transformers", "authors": ["Debabrata Mahapatra", "Shubham Agarwal", "Apoorv Saxena", "Subrata Mitra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.", "AI": {"tldr": "RCStat introduces a statistical framework using raw attention logits to enhance token importance assessment in auto-regressive transformers, yielding improved performance in key-value compression and attribution tasks.", "motivation": "The paper aims to improve understanding of input-token importance in transformers by utilizing pre-Softmax query-key logits, which provide richer information than traditional Softmax-normalized attention weights.", "method": "RCStat employs Relative Contextualization (RC) to measure the contextual alignment between token segments, deriving an efficient upper bound for RC.", "result": "RCStat demonstrates significant improvements in key-value compression and attribution tasks, achieving state-of-the-art performance across various benchmarks without requiring model retraining.", "conclusion": "The proposed methods show that raw attention logits can yield more effective token importance metrics and practical applications like key-value eviction and improved explanation fidelity in NLP tasks.", "key_contributions": ["Introduction of RCStat framework for analyzing attention logits", "Demonstration of improved key-value compression methods", "Enhanced attribution explanations using raw attention metrics"], "limitations": "", "keywords": ["Transformers", "Attention Mechanism", "Machine Learning", "Key-Value Compression", "Attribution"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19571", "pdf": "https://arxiv.org/pdf/2506.19571.pdf", "abs": "https://arxiv.org/abs/2506.19571", "title": "Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress", "authors": ["Lorenzo Proietti", "Stefano Perrella", "Roberto Navigli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference. 24 pages", "summary": "In Machine Translation (MT) evaluation, metric performance is assessed based\non agreement with human judgments. In recent years, automatic metrics have\ndemonstrated increasingly high levels of agreement with humans. To gain a\nclearer understanding of metric performance and establish an upper bound, we\nincorporate human baselines in the MT meta-evaluation, that is, the assessment\nof MT metrics' capabilities. Our results show that human annotators are not\nconsistently superior to automatic metrics, with state-of-the-art metrics often\nranking on par with or higher than human baselines. Despite these findings\nsuggesting human parity, we discuss several reasons for caution. Finally, we\nexplore the broader implications of our results for the research field, asking:\nCan we still reliably measure improvements in MT evaluation? With this work, we\naim to shed light on the limits of our ability to measure progress in the\nfield, fostering discussion on an issue that we believe is crucial to the\nentire MT evaluation community.", "AI": {"tldr": "This paper explores the performance of automatic metrics in Machine Translation evaluation against human judgments, revealing that state-of-the-art metrics often perform on par or better than humans.", "motivation": "To better understand and assess the capabilities of MT metrics and their agreement with human evaluations, incorporating human baselines to establish an upper bound on performance.", "method": "Meta-evaluation of Machine Translation metrics by comparing human annotator performance with that of state-of-the-art automatic metrics.", "result": "Human annotators are not consistently better than automatic metrics, with some state-of-the-art metrics achieving equal or better rankings than human baselines.", "conclusion": "The results suggest caution in interpreting human parity with automatic metrics, raising concerns about the reliability of measuring progress in MT evaluation.", "key_contributions": ["Introduced human baselines in the evaluation of MT metrics.", "Demonstrated that automatic metrics can match or exceed human performance.", "Provoked discussion on the reliability of measuring improvements in MT evaluation."], "limitations": "Potential variability in human annotator performance and the need for consistent evaluation metrics across studies.", "keywords": ["Machine Translation", "MT evaluation", "automatic metrics", "human judgments", "meta-evaluation"], "importance_score": 5, "read_time_minutes": 24}}
{"id": "2506.19599", "pdf": "https://arxiv.org/pdf/2506.19599.pdf", "abs": "https://arxiv.org/abs/2506.19599", "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu", "Xiaoyi Wang", "Yanqing Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.", "AI": {"tldr": "ECCoT is a framework designed to validate and improve the interpretability of reasoning chains in Large Language Models using a combination of topic modeling and causal reasoning techniques.", "motivation": "To improve the transparency and reliability of Large Language Models by addressing the issue of unreliable conclusions arising from invalid reasoning chains.", "method": "The ECCoT framework leverages the Markov Random Field-Embedded Topic Model (MRF-ETM) for generating topic-aware reasoning and Causal Sentence-BERT (CSBert) to ensure causal reasoning alignment, along with structured ordering statistics to filter ineffective reasoning chains.", "result": "The framework enhances interpretability, reduces biases, and increases trust in LLM-based decision-making by validating and refining reasoning chains.", "conclusion": "ECCoT represents a significant advancement in making LLM outputs more interpretable and trustworthy, particularly in critical applications where decision-making is affected by AI.", "key_contributions": ["Introduction of the ECCoT framework.", "Development of the MRF-ETM for topic-driven Chain of Thought generation.", "Integration of CSBert for enhancing causal reasoning."], "limitations": "", "keywords": ["Large Language Models", "Causal Reasoning", "Interpretability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.19603", "pdf": "https://arxiv.org/pdf/2506.19603.pdf", "abs": "https://arxiv.org/abs/2506.19603", "title": "Social Hatred: Efficient Multimodal Detection of Hatemongers", "authors": ["Tom Marzea", "Abraham Israeli", "Oren Tsur"], "categories": ["cs.CL", "cs.SI"], "comment": "To be published in WOAH, July 2025. arXiv admin note: text overlap\n  with arXiv:2409.14464", "summary": "Automatic detection of online hate speech serves as a crucial step in the\ndetoxification of the online discourse. Moreover, accurate classification can\npromote a better understanding of the proliferation of hate as a social\nphenomenon. While most prior work focus on the detection of hateful utterances,\nwe argue that focusing on the user level is as important, albeit challenging.\nIn this paper we consider a multimodal aggregative approach for the detection\nof hate-mongers, taking into account the potentially hateful texts, user\nactivity, and the user network. Evaluating our method on three unique datasets\nX (Twitter), Gab, and Parler we show that processing a user's texts in her\nsocial context significantly improves the detection of hate mongers, compared\nto previously used text and graph-based methods. We offer comprehensive set of\nresults obtained in different experimental settings as well as qualitative\nanalysis of illustrative cases. Our method can be used to improve the\nclassification of coded messages, dog-whistling, and racial gas-lighting, as\nwell as to inform intervention measures. Moreover, we demonstrate that our\nmultimodal approach performs well across very different content platforms and\nover large datasets and networks.", "AI": {"tldr": "This paper presents a multimodal approach for detecting hate-mongers by analyzing user texts, activity, and networks, leading to improved detection accuracy on various social media platforms.", "motivation": "The need to detoxify online discourse and understand the social phenomenon of hate speech requires effective detection methods that go beyond just analyzing text.", "method": "A multimodal aggregative approach that considers hateful texts, user activity, and user networks to detect hate-mongers.", "result": "The proposed method, evaluated on datasets from Twitter, Gab, and Parler, significantly improves hate monger detection compared to traditional text and graph-based methods.", "conclusion": "This multimodal approach can enhance the detection of coded messages and inform intervention strategies while being effective across different platforms.", "key_contributions": ["Introduction of a user-level focus in hate speech detection.", "Development of a multimodal detection approach enhancing traditional methods.", "Comprehensive evaluation across multiple datasets and platforms."], "limitations": "", "keywords": ["hate speech detection", "multimodal analysis", "social networks"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.19607", "pdf": "https://arxiv.org/pdf/2506.19607.pdf", "abs": "https://arxiv.org/abs/2506.19607", "title": "Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge", "authors": ["Juraj Vladika", "Ihsan Soydemir", "Florian Matthes"], "categories": ["cs.CL"], "comment": "Accepted to FEVER @ ACL 2025", "summary": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation.", "AI": {"tldr": "This paper explores self-correcting methods to reduce hallucinations in news summarization through multi-turn LLM interactions and evidence retrieval.", "motivation": "To address the issue of hallucinations in large language models, particularly in the context of news summarization, by leveraging self-correcting techniques.", "method": "The authors applied two state-of-the-art self-correcting systems to correct hallucinated summaries using evidence from three search engines and analyzed the performance.", "result": "The study reveals significant insights into the performance of self-correcting methods, highlighting the usefulness of search engine snippets and a strong correlation between G-Eval and human evaluations.", "conclusion": "The findings suggest that employing multi-turn interactions with LLMs for verification can enhance summary accuracy and mitigate hallucinations.", "key_contributions": ["Application of self-correcting systems to news summarization", "Insights on the role of search engine snippets", "High alignment of automated evaluation with human judgment."], "limitations": "", "keywords": ["large language models", "hallucinations", "self-correcting methods"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.23478", "pdf": "https://arxiv.org/pdf/2410.23478.pdf", "abs": "https://arxiv.org/abs/2410.23478", "title": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs", "authors": ["Sireesh Gururaja", "Yueheng Zhang", "Guannan Tang", "Tianhao Zhang", "Kevin Murphy", "Yu-Tsen Yi", "Junwon Seo", "Anthony Rollett", "Emma Strubell"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science.", "AI": {"tldr": "Collage is a tool for prototyping and evaluating information extraction models on scientific PDFs.", "motivation": "Address the challenges of comparing and using domain-specific NLP models for scientific documents.", "method": "Collage provides interfaces for various information extraction models including HuggingFace token classifiers and LLMs, enabling rapid evaluation and prototyping.", "result": "Collage allows for the inspection and debugging of NLP modeling pipelines, providing insights into intermediate processing states.", "conclusion": "The tool facilitates literature review in materials science by improving the evaluation of information extraction models.", "key_contributions": ["Development of Collage for model comparison and evaluation", "Support for various NLP models out of the box", "Enhanced debugging and understanding of modeling pipelines"], "limitations": "", "keywords": ["information extraction", "HuggingFace", "scientific documents", "NLP tool", "literature review"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19652", "pdf": "https://arxiv.org/pdf/2506.19652.pdf", "abs": "https://arxiv.org/abs/2506.19652", "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager", "authors": ["Lucie Galland", "Catherine Pelachaud", "Florian Pecune"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.", "AI": {"tldr": "Proposes a framework combining LLMs with RL-based dialogue management for goal-oriented open-ended dialogues, enhancing adaptability and efficiency.", "motivation": "To improve open-ended dialogue systems by integrating LLMs with reinforcement learning for personalized patient interactions.", "method": "Utilizes hierarchical reinforcement learning and meta-learning to model dialogue phases and adapt to user profiles.", "result": "The dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward during Motivational Interviews.", "conclusion": "The framework shows promise for developing responsive dialogue systems tailored to individual user needs in health contexts.", "key_contributions": ["Integration of LLMs with a reinforcement learning dialogue manager", "Application of hierarchical reinforcement learning for dialogue structuring", "Demonstration of improved performance over existing LLMs in goal-oriented dialogues"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "dialogue management", "health informatics", "personalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.19733", "pdf": "https://arxiv.org/pdf/2506.19733.pdf", "abs": "https://arxiv.org/abs/2506.19733", "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?", "authors": ["Chuxuan Hu", "Yuxuan Zhu", "Antony Kellermann", "Caleb Biddulph", "Suppakit Waiwitlikhit", "Jason Benn", "Daniel Kang"], "categories": ["cs.CL"], "comment": "9 pages, 4 figures, 2 tables", "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns.", "AI": {"tldr": "This paper examines the generalizability of reinforcement post training (RPT) on large language models (LLMs) across multiple domains.", "motivation": "To investigate how well improvements from reinforcement post training (RPT) of LLMs generalize to unseen domains, as prior evaluations have not sufficiently addressed this.", "method": "Two studies are conducted: (1) an observational study comparing a variety of RPT models against their base models across seen and unseen domains, and (2) an interventional study where LLMs are fine-tuned using RPT on single domains and tested on multiple domains.", "result": "Both studies indicate that while RPT improves performance on tasks related to fine-tuning data, the generalization to different reasoning patterns in unseen domains is inconsistent, with gains potentially vanishing.", "conclusion": "RPT shows promise in enhancing LLM reasoning abilities, but the variability in generalization to new domains raises questions about its applicability across diverse reasoning tasks.", "key_contributions": ["First comprehensive evaluation of RPT across multiple domains.", "Identification of inconsistencies in generalization of RPT improvements.", "Insights into the reasoning patterns that affect LLM performance post-RPT."], "limitations": "Generalization inconsistencies may limit the applicability of RPT in diverse reasoning contexts.", "keywords": ["Reinforcement Learning", "Language Models", "Generalization", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.19750", "pdf": "https://arxiv.org/pdf/2506.19750.pdf", "abs": "https://arxiv.org/abs/2506.19750", "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach", "authors": ["Takashi Nishibayashi", "Seiji Kanazawa", "Kumpei Yamada"], "categories": ["cs.CL"], "comment": null, "summary": "Background: Symptom Checkers (SCs) provide users with personalized medical\ninformation. To prevent performance degradation from algorithm updates, SC\ndevelopers must evaluate diagnostic performance changes for individual diseases\nbefore deployment. However, acquiring sufficient evaluation data for rare\ndiseases is difficult, and manually creating numerous clinical vignettes is\ncostly and impractical. Objective: This study proposes and validates a novel\nSynthetic Vignette Simulation Approach to evaluate diagnostic performance\nchanges for individual rare diseases following SC algorithm updates. Methods:\nWe used disease-phenotype annotations from the Human Phenotype Ontology (HPO),\na knowledge database for rare diseases, to generate synthetic vignettes. With\nthese, we simulated SC interviews to estimate the impact of algorithm updates\non real-world diagnostic performance. The method's effectiveness was evaluated\nretrospectively by comparing estimated values with actual metric changes using\nthe R 2(R-squared) coefficient. Results: The experiment included eight past SC\nalgorithm updates. For updates on diseases with frequency information in HPO\n(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8\nchange, it was 0.78 (p=0.047), indicating the method can predict\npost-deployment performance. In contrast, large prediction errors occurred for\ndiseases without frequency information (n=3), highlighting its importance. The\nmanual effort to map HPO phenotypes to SC symptoms was approximately 2 hours\nper disease. Conclusions: Our method enables pre-deployment evaluation of SC\nalgorithm changes for individual rare diseases using a publicly available,\nexpert-created knowledge base. This transparent and low-cost approach allows\ndevelopers to efficiently improve diagnostic performance for rare diseases,\npotentially enhancing support for early diagnosis.", "AI": {"tldr": "This study introduces a Synthetic Vignette Simulation Approach to evaluate how algorithm updates in Symptom Checkers (SCs) affect diagnostic performance for rare diseases, overcoming data acquisition challenges.", "motivation": "To assess the impact of algorithm updates on diagnostic performance for rare diseases, which is often hindered by limited evaluation data and high costs of manual vignette creation.", "method": "Synthetic vignettes were generated using disease-phenotype annotations from the Human Phenotype Ontology (HPO), simulating SC interviews to estimate algorithm update impacts on diagnostic performance.", "result": "The method achieved an R^2 of 0.831 for recall@8 and 0.78 for precision@8 with diseases having frequency data, demonstrating predictive capability for algorithm updates; however, inaccuracies arose for diseases lacking frequency data.", "conclusion": "The Synthetic Vignette Simulation Approach offers a cost-effective and transparent method for evaluating SC algorithm changes for rare diseases, potentially leading to improved diagnostic support.", "key_contributions": ["Introduction of a new method for synthetic vignette generation", "Validation of predictive capabilities of the approach using historical SC updates", "Highlighting the importance of frequency information in evaluating diagnostic performance"], "limitations": "The method's accuracy is contingent on the availability of frequency information for rare diseases.", "keywords": ["Symptom Checkers", "Synthetic Vignettes", "Rare Diseases", "Diagnostic Performance", "Human Phenotype Ontology"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.19753", "pdf": "https://arxiv.org/pdf/2506.19753.pdf", "abs": "https://arxiv.org/abs/2506.19753", "title": "Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis", "authors": ["Omar A. Essameldin", "Ali O. Elbeih", "Wael H. Gomaa", "Wael F. Elsersy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities.", "AI": {"tldr": "This paper classifies 18 Arabic dialects from tweets using various models, achieving notable accuracy with MARBERTv2.", "motivation": "To address the growing need for effective identification of Arabic dialects in digital communication due to their diversity across 22 countries.", "method": "The study employs RNN models, Transformer models, and large language models (LLMs) using prompt engineering to classify dialects in the QADI dataset of Arabic tweets.", "result": "MARBERTv2 achieved the best performance with 65% accuracy and 64% F1-score, highlighting the efficacy of advanced NLP techniques in dialect identification.", "conclusion": "This research enhances applications like personalized chatbots and social media monitoring, improving accessibility for Arabic-speaking communities.", "key_contributions": ["Introduces classification of 18 Arabic dialects from tweets.", "Demonstrates effectiveness of MARBERTv2 and NLP techniques in dialect identification.", "Identifies linguistic issues relevant to Arabic dialects."], "limitations": "", "keywords": ["Arabic dialects", "NLP", "MARBERTv2", "classification", "language models"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.19761", "pdf": "https://arxiv.org/pdf/2506.19761.pdf", "abs": "https://arxiv.org/abs/2506.19761", "title": "Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR", "authors": ["Martin Ratajczak", "Jean-Philippe Robichaud", "Jennifer Drexler Fox"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Long-form speech recognition is an application area of increasing research\nfocus. ASR models based on multi-head attention (MHA) are ill-suited to\nlong-form ASR because of their quadratic complexity in sequence length. We\nbuild on recent work that has investigated linear complexity recurrent\nattention (RA) layers for ASR. We find that bidirectional RA layers can match\nthe accuracy of MHA for both short- and long-form applications. We present a\nstrong limited-context attention (LCA) baseline, and show that RA layers are\njust as accurate while being more efficient. We develop a long-form training\nparadigm which further improves RA performance, leading to better accuracy than\nLCA with 44% higher throughput. We also present Direction Dropout, a novel\nregularization method that improves accuracy, provides fine-grained control of\nthe accuracy/throughput trade-off of bidirectional RA, and enables a new\nalternating directions decoding mode with even higher throughput.", "AI": {"tldr": "This paper presents advancements in long-form speech recognition using bidirectional recurrent attention (RA) layers, which offer efficiency and accuracy improvements over multi-head attention (MHA).", "motivation": "There is a growing need for improved long-form speech recognition due to the limitations of existing models that struggle with long sequences.", "method": "The authors investigate the use of bidirectional RA layers as an alternative to MHA, developing a long-form training paradigm and introducing Direction Dropout as a new regularization technique.", "result": "Bidirectional RA layers match the accuracy of MHA for both short and long-form ASR, demonstrating a 44% higher throughput compared to established methods while also improving the accuracy/throughput trade-off.", "conclusion": "The findings suggest that RA layers are a viable and efficient alternative for long-form speech recognition, enabling superior performance in accuracy and throughput.", "key_contributions": ["Introduction of bidirectional recurrent attention layers for ASR.", "Development of Direction Dropout regularization method.", "Establishment of a long-form training paradigm that enhances RA performance."], "limitations": "", "keywords": ["Speech Recognition", "Machine Learning", "Recurrent Attention", "Long-form ASR", "Regularization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.19767", "pdf": "https://arxiv.org/pdf/2506.19767.pdf", "abs": "https://arxiv.org/abs/2506.19767", "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning", "authors": ["Yuqian Fu", "Tinghong Chen", "Jiajun Chai", "Xihuai Wang", "Songjun Tu", "Guojun Yin", "Wei Lin", "Qichao Zhang", "Yuanheng Zhu", "Dongbin Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.", "AI": {"tldr": "The paper proposes Supervised Reinforcement Fine-Tuning (SRFT), which unifies Supervised Fine-Tuning and Reinforcement Learning methods for large language models (LLMs) to enhance reasoning task performance.", "motivation": "To address the challenge of effectively integrating Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in enhancing large language models' reasoning capabilities.", "method": "An entropy-aware weighting mechanism is used to propose Supervised Reinforcement Fine-Tuning (SRFT), combining SFT and RL in a single-stage method that directly optimizes the LLM with demonstrations and self-exploration rollouts.", "result": "SRFT demonstrated an average accuracy of 59.1%, surpassing zero-RL methods by 9.0% on five mathematical reasoning benchmarks and by 10.9% on three out-of-distribution benchmarks.", "conclusion": "The proposed SRFT method effectively improves the integration of SFT and RL for LLMs, leading to notable performance enhancements in reasoning tasks.", "key_contributions": ["Proposed a unified fine-tuning method (SRFT) for LLMs integrating SFT and RL.", "Introduced entropy-aware weighting mechanisms to optimize training effectiveness.", "Demonstrated significant performance improvements on reasoning benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Supervised Fine-Tuning", "Reinforcement Learning", "Entropy", "Reasoning Benchmarks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.19794", "pdf": "https://arxiv.org/pdf/2506.19794.pdf", "abs": "https://arxiv.org/abs/2506.19794", "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study", "authors": ["Yuqi Zhu", "Yi Zhong", "Jintian Zhang", "Ziheng Zhang", "Shuofei Qiao", "Yujie Luo", "Lun Du", "Da Zheng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.", "AI": {"tldr": "This study explores ways to enhance data analysis capabilities in open-source Large Language Models (LLMs) by evaluating model performance across data understanding, code generation, and strategic planning.", "motivation": "To address the limitations of open-source LLMs in reasoning-intensive data analysis tasks and to improve their effectiveness.", "method": "The authors curate a seed dataset of diverse scenarios and evaluate models across three dimensions: data understanding, code generation, and strategic planning.", "result": "The study finds that strategic planning quality is the primary determinant of model performance, and that interaction design and data quality significantly influence reasoning capabilities.", "conclusion": "Insights from the analysis lead to the development of a data synthesis methodology that improves the analytical reasoning capabilities of open-source LLMs.", "key_contributions": ["Identifying strategic planning as a key factor in model performance.", "Demonstrating the impact of interaction design and task complexity on reasoning capabilities.", "Developing a novel data synthesis methodology to enhance LLMs' analytical reasoning."], "limitations": "The paper is noted as a work in progress, which may imply that results and methodologies are not yet fully validated.", "keywords": ["Large Language Models", "data analysis", "strategic planning", "interaction design", "data quality"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.19831", "pdf": "https://arxiv.org/pdf/2506.19831.pdf", "abs": "https://arxiv.org/abs/2506.19831", "title": "How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?", "authors": ["Abdullah Khondoker", "Enam Ahmed Taufik", "Md. Iftekhar Islam Tashik", "S M Ishtiak Mahmud", "Farig Sadeque"], "categories": ["cs.CL"], "comment": null, "summary": "The spread of cyber hatred has led to communal violence, fueling aggression\nand conflicts between various religious, ethnic, and social groups, posing a\nsignificant threat to social harmony. Despite its critical importance, the\nclassification of communal violent text remains an underexplored area in\nexisting research. This study aims to enhance the accuracy of detecting text\nthat incites communal violence, focusing specifically on Bengali textual data\nsourced from social media platforms. We introduce a fine-tuned BanglaBERT model\ntailored for this task, achieving a macro F1 score of 0.60. To address the\nissue of data imbalance, our dataset was expanded by adding 1,794 instances,\nwhich facilitated the development and evaluation of a fine-tuned ensemble\nmodel. This ensemble model demonstrated an improved performance, achieving a\nmacro F1 score of 0.63, thus highlighting its effectiveness in this domain. In\naddition to quantitative performance metrics, qualitative analysis revealed\ninstances where the models struggled with context understanding, leading to\noccasional misclassifications, even when predictions were made with high\nconfidence. Through analyzing the cosine similarity between words, we\nidentified certain limitations in the pre-trained BanglaBERT models,\nparticularly in their ability to distinguish between closely related communal\nand non-communal terms. To further interpret the model's decisions, we applied\nLIME, which helped to uncover specific areas where the model struggled in\nunderstanding context, contributing to errors in classification. These findings\nhighlight the promise of NLP and interpretability tools in reducing online\ncommunal violence. Our work contributes to the growing body of research in\ncommunal violence detection and offers a foundation for future studies aiming\nto refine these techniques for better accuracy and societal impact.", "AI": {"tldr": "This study enhances the detection of communal violent text in Bengali using a fine-tuned BanglaBERT model, achieving a macro F1 score of 0.63. It addresses data imbalance and explores model limitations in understanding context.", "motivation": "To tackle the growing issue of cyber hatred which leads to communal violence, this study seeks to improve the classification of violent text on social media.", "method": "A fine-tuned BanglaBERT model was developed, and an ensemble model was created to address data imbalance by adding more instances, achieving improved classification scores.", "result": "The fine-tuned ensemble model achieved a macro F1 score of 0.63 and identified limitations in context understanding leading to misclassifications.", "conclusion": "The work demonstrates the potential of NLP tools, like the fine-tuned models and interpretability techniques, in reducing online communal violence and provides a basis for future improvements.", "key_contributions": ["Development of a fine-tuned BanglaBERT model for communal violence detection", "Addressing data imbalance with an expanded dataset", "Application of LIME for model interpretability"], "limitations": "The models struggled with context understanding and distinguishing between closely related terms, leading to errors in classification.", "keywords": ["communal violence", "BanglaBERT", "NLP", "social media", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.19835", "pdf": "https://arxiv.org/pdf/2506.19835.pdf", "abs": "https://arxiv.org/abs/2506.19835", "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration", "authors": ["Yucheng Zhou", "Lingran Song", "Jianbing Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM.", "AI": {"tldr": "Introduction of a modular multi-agent framework (MAM) that enhances medical diagnostics using LLMs in a collaborative way.", "motivation": "To overcome limitations in knowledge update costs, comprehensiveness, and flexibility in current multimodal medical LLMs.", "method": "MAM decomposes the medical diagnostic process into specialized roles (General Practitioner, Specialist Team, Radiologist, Medical Assistant, Director), each represented by LLM-based agents, enabling efficient updates and improved diagnostics.", "result": "MAM demonstrates significant performance improvements (18% to 365%) over baseline models on multimodal medical datasets.", "conclusion": "The modular framework MAM outperforms existing modality-specific LLMs and facilitates better medical diagnostics.", "key_contributions": ["Modular Multi-Agent Framework for medical diagnostics", "Role assignment for improved diagnostic performance", "Performance improvements over baseline models across multimodal datasets"], "limitations": "", "keywords": ["Large Language Models", "medical diagnostics", "multi-modal framework", "agent-based systems", "performance improvement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2308.16075", "pdf": "https://arxiv.org/pdf/2308.16075.pdf", "abs": "https://arxiv.org/abs/2308.16075", "title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages", "authors": ["Baban Gain", "Dibyanayan Bandyopadhyay", "Samrat Mukherjee", "Chandranath Adak", "Asif Ekbal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural Machine Translation (NMT) has made remarkable progress using\nlarge-scale textual data, but the potential of incorporating multimodal inputs,\nespecially visual information, remains underexplored in high-resource settings.\nWhile prior research has focused on using multimodal data in low-resource\nscenarios, this study examines how image features impact translation when added\nto a large-scale, pre-trained unimodal NMT system. Surprisingly, the study\nfinds that images might be redundant in this context. Additionally, the\nresearch introduces synthetic noise to assess whether images help the model\nhandle textual noise. Multimodal models slightly outperform text-only models in\nnoisy settings, even when random images are used. The study's experiments\ntranslate from English to Hindi, Bengali, and Malayalam, significantly\noutperforming state-of-the-art benchmarks. Interestingly, the effect of visual\ncontext varies with the level of source text noise: no visual context works\nbest for non-noisy translations, cropped image features are optimal for low\nnoise, and full image features perform better in high-noise scenarios. This\nsheds light on the role of visual context, especially in noisy settings, and\nopens up a new research direction for Noisy Neural Machine Translation in\nmultimodal setups. The research emphasizes the importance of combining visual\nand textual information to improve translation across various environments. Our\ncode is publicly available at https://github.com/babangain/indicMMT.", "AI": {"tldr": "This study explores the integration of visual information into Neural Machine Translation (NMT) systems, revealing that images can be redundant in high-resource settings but beneficial in noisy environments.", "motivation": "To investigate the impact of multimodal inputs, specifically visual features, on the performance of neural machine translation systems.", "method": "The study incorporated image features into a large-scale, pre-trained unimodal NMT system and assessed its performance against text-only models under varying levels of textual noise.", "result": "The multimodal models slightly outperformed text-only models in noisy scenarios, showing that the effectiveness of visual context fluctuates based on source text noise.", "conclusion": "Visual and textual information should be combined for optimal NMT performance, especially in noisy contexts, suggesting new research directions for Noisy Neural Machine Translation.", "key_contributions": ["Demonstrated the redundancy of images in high-resource NMT settings.", "Identified varying effectiveness of visual context based on the level of noise in source text.", "Introduced synthetic noise to evaluate the impact on translation performance."], "limitations": "", "keywords": ["Neural Machine Translation", "multimodal inputs", "visual information", "textual noise", "translation performance"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2404.01799", "pdf": "https://arxiv.org/pdf/2404.01799.pdf", "abs": "https://arxiv.org/abs/2404.01799", "title": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics", "authors": ["Qixiang Fang", "Daniel L. Oberski", "Dong Nguyen"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025", "summary": "Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers'. While such benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics -- a field dedicated to the measurement of latent variables like\nacademic proficiency -- into LLM benchmarking. We make four primary\ncontributions. First, we reflect on current LLM benchmark developments and\ncontrast them with psychometrics-based test development. Second, we introduce\nPATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of\nLLMs. PATCH addresses the aforementioned limitations. In particular, PATCH\nenables valid comparison between LLMs and human populations. Third, we\ndemonstrate PATCH by measuring several LLMs' proficiency in 8th grade\nmathematics against 56 human populations. We show that adopting a\npsychometrics-based approach yields evaluation outcomes that diverge from those\nbased on current benchmarking practices. Fourth, we release 4 high-quality\ndatasets to support measuring and comparing LLM proficiency in grade school\nmathematics and science with human populations.", "AI": {"tldr": "This paper proposes PATCH, a novel framework for benchmarking large language models (LLMs) using psychometrics to improve measurement quality and comparability with human populations.", "motivation": "Existing benchmarks for LLMs often measure academic proficiency but have limitations including unclear measurement quality and lack of assessment on an item level. The authors propose using psychometrics to address these challenges.", "method": "The authors introduce PATCH, a framework that leverages psychometric principles to create benchmarks that allow valid comparisons between LLMs and human populations. They apply this framework to evaluate several LLMs' proficiency in 8th grade mathematics.", "result": "The evaluation using PATCH shows that current benchmarks yield different outcomes compared to the psychometrics-based approach. Additionally, four high-quality datasets are released to facilitate LLM proficiency assessment in mathematics and science against human populations.", "conclusion": "The proposed PATCH framework addresses significant limitations of current LLM benchmarks and enhances the accuracy of measuring LLM academic proficiency.", "key_contributions": ["Introduction of the PATCH framework for psychometrics-assisted benchmarking of LLMs", "Demonstration of PATCH's application in evaluating LLMs against human populations", "Release of four high-quality datasets for LLM proficiency assessment"], "limitations": "", "keywords": ["large language models", "psychometrics", "benchmarking", "academic proficiency", "evaluation techniques"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2406.18259", "pdf": "https://arxiv.org/pdf/2406.18259.pdf", "abs": "https://arxiv.org/abs/2406.18259", "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and Explainability is Complicated", "authors": ["Jiazhou Ji", "Ruizhe Li", "Shujun Li", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Chiyu Chen", "Xiaoyu Jiang", "Xinru Lu"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures", "summary": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.", "AI": {"tldr": "This paper introduces a ternary classification scheme for detecting LLM-generated texts, emphasizing the importance of an 'undecided' category for improved explainability in detection results.", "motivation": "To address the complexities in distinguishing between human and LLM-generated texts, especially as LLMs become more advanced and their texts harder to classify.", "method": "The authors created four new datasets of various texts from LLMs and human authors, performed binary classification tests on detection methods, and developed a ternary classification approach involving a new 'undecided' category. They utilized human annotators to provide labels and explanation notes, analyzing the behavior of existing detectors in this new context.", "result": "The study found that the 'undecided' category significantly enhances explainability and understanding of detection results, indicating the need for clearer communication to users about the nature of detected texts.", "conclusion": "The findings underscore the necessity of incorporating explainability into LLM text detection systems and provide guidelines for developing better detection methods that facilitate user understanding.", "key_contributions": ["Introduction of a ternary classification scheme with an 'undecided' category", "Creation of new datasets for improved detection analysis", "Guidelines for enhancing explainability in detection systems"], "limitations": "The study may need further validation across more diverse datasets and additional detectors to fully confirm the robustness of the findings.", "keywords": ["LLM detection", "ternary classification", "explainability", "text classification", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2407.06331", "pdf": "https://arxiv.org/pdf/2407.06331.pdf", "abs": "https://arxiv.org/abs/2407.06331", "title": "LEVOS: Leveraging Vocabulary Overlap with Sanskrit to Generate Technical Lexicons in Indian Languages", "authors": ["Karthika N J", "Krishnakant Bhatt", "Ganesh Ramakrishnan", "Preethi Jyothi"], "categories": ["cs.CL"], "comment": "20th Workshop on Innovative Use of NLP for Building Educational\n  Applications (Co-located with ACL2025)", "summary": "Translating technical terms into lexically similar, low-resource Indian\nlanguages remains a challenge due to limited parallel data and the complexity\nof linguistic structures. We propose a novel use-case of Sanskrit-based\nsegments for linguistically informed translation of such terms, leveraging\nsubword-level similarity and morphological alignment across related languages.\nOur approach uses character-level segmentation to identify meaningful subword\nunits, facilitating more accurate and context-aware translation. To enable\nthis, we utilize a Character-level Transformer model for Sanskrit Word\nSegmentation (CharSS), which addresses the complexities of sandhi and\nmorpho-phonemic changes during segmentation. We observe consistent improvements\nin two experimental settings for technical term translation using\nSanskrit-derived segments, averaging 8.46 and 6.79 chrF++ scores, respectively.\nFurther, we conduct a post hoc human evaluation to verify the quality\nassessment of the translated technical terms using automated metrics. This work\nhas important implications for the education field, especially in creating\naccessible, high-quality learning materials in Indian languages. By supporting\nthe accurate and linguistically rooted translation of technical content, our\napproach facilitates inclusivity and aids in bridging the resource gap for\nlearners in low-resource language communities.", "AI": {"tldr": "This paper presents a novel method for translating technical terms into low-resource Indian languages using Sanskrit-based segments and a Character-level Transformer model for improved accuracy.", "motivation": "To address the challenges in translating technical terms into low-resource Indian languages due to limited parallel data and complex linguistic structures.", "method": "The approach utilizes character-level segmentation to identify meaningful subword units and employs a Character-level Transformer model for Sanskrit Word Segmentation (CharSS) to handle sandhi and morpho-phonemic complexities.", "result": "The method achieved consistent improvements in translation quality, with average chrF++ scores of 8.46 and 6.79 in two experimental settings.", "conclusion": "The approach has implications for creating accessible learning materials in Indian languages, promoting inclusivity within low-resource language communities.", "key_contributions": ["Novel use of Sanskrit-based segments for translation", "Implementation of character-level segmentation", "Human evaluation verifies translation quality"], "limitations": "", "keywords": ["translation", "low-resource languages", "Sanskrit", "NLP", "education"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2408.01933", "pdf": "https://arxiv.org/pdf/2408.01933.pdf", "abs": "https://arxiv.org/abs/2408.01933", "title": "Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks", "authors": ["Junhao Chen", "Bowen Wang", "Jiuyang Chang", "Yuta Nakashima"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper is the journal extension of our NeurIPS 2024 paper\n  \"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models\"", "summary": "This paper introduces REACT, a benchmark designed to rigorously evaluate the\nreasoning capabilities of large language models (LLMs) within accountable,\nhigh-stakes decision-making tasks in medical and legal domains. Unlike\ntraditional benchmarks primarily focused on prediction accuracy, REACT\nemphasizes transparent and interpretable reasoning, requiring models to align\ntheir logic closely with expert-derived procedures. To assess whether LLM\nreasoning aligns closely with human experts, we annotated 511 clinical cases\nfrom the medical domain and 86 legal cases from the legal domain, each enriched\nwith detailed expert-extracted rationales and evidence supporting each step of\nthe reasoning process. These annotations were guided by carefully constructed\nreasoning graphs, which explicitly encode domain-specific inference structures\nand decision criteria derived by domain experts. These reasoning graphs serve\nnot only as standards for expert annotation but also as structured guidelines\nenabling models to reason transparently and step-by-step. To address the\nscalability challenges of manual annotation, we further developed a\nsemi-automatic annotation pipeline leveraging expert-defined reasoning graph\ntemplates to efficiently generate new graphs, exploring the potential to extend\nour approach into additional critical domains. Experimental results demonstrate\nthat reasoning graphs substantially enhance the interpretability and accuracy\nof LLM reasoning compared to traditional baselines, although significant gaps\nremain relative to expert-level reasoning performance.", "AI": {"tldr": "The paper introduces REACT, a benchmark for evaluating the reasoning capabilities of large language models in high-stakes medical and legal decision-making contexts, emphasizing interpretability and transparency over mere prediction accuracy.", "motivation": "The existing benchmarks for large language models often focus primarily on prediction accuracy without considering the interpretability and reasoning capabilities required for accountable decision-making in critical domains such as medicine and law.", "method": "REACT utilizes expert-annotated clinical and legal cases, employing reasoning graphs to provide structured guidelines for model reasoning, supported by a semi-automatic annotation pipeline to address scalability.", "result": "Experimental results show that reasoning graphs significantly enhance LLM interpretability and accuracy compared to traditional benchmarks, but gaps still exist in relation to expert-level reasoning.", "conclusion": "While REACT offers a rigorous framework to evaluate LLM reasoning, further research is needed to bridge the gaps in performance compared to human experts.", "key_contributions": ["Development of the REACT benchmark for evaluating reasoning in LLMs", "Expert-annotated datasets for medical and legal domains", "Introduction of reasoning graphs for structured reasoning"], "limitations": "The model reasoning still lags behind expert-level performance despite improvements in interpretability and accuracy.", "keywords": ["large language models", "medical decision-making", "legal decision-making", "interpretability", "reasoning graphs"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.05563", "pdf": "https://arxiv.org/pdf/2410.05563.pdf", "abs": "https://arxiv.org/abs/2410.05563", "title": "Rational Metareasoning for Large Language Models", "authors": ["C. Nicol De Sabbata", "Theodore R. Sumers", "Badr AlKhamissi", "Antoine Bosselut", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Being prompted to engage in reasoning has emerged as a core technique for\nusing large language models (LLMs), deploying additional inference-time compute\nto improve task performance. However, as LLMs increase in both size and\nadoption, inference costs are correspondingly becoming increasingly burdensome.\nHow, then, might we optimize reasoning's cost-performance tradeoff? This work\nintroduces a novel approach based on computational models of metareasoning used\nin cognitive science, training LLMs to selectively use intermediate reasoning\nsteps only when necessary. We first develop a reward function that incorporates\nthe Value of Computation by penalizing unnecessary reasoning, then use this\nreward function with Expert Iteration to train the LLM. Compared to few-shot\nchain-of-thought prompting and STaR, our method significantly reduces inference\ncosts (20-37\\% fewer tokens generated across three models) while maintaining\ntask performance across diverse datasets.", "AI": {"tldr": "This work presents a method to optimize reasoning in LLMs by selectively engaging reasoning steps, reducing inference costs while maintaining performance.", "motivation": "The increasing size and adoption of LLMs result in higher inference costs, prompting the need for optimization in reasoning techniques.", "method": "A novel approach that uses a reward function based on computational models of metareasoning to train LLMs, penalizing unnecessary reasoning steps with Expert Iteration.", "result": "The proposed method reduces inference costs by 20-37% across three models while preserving task performance on various datasets.", "conclusion": "Selective reasoning in LLMs can lead to significant efficiency improvements without sacrificing capability.", "key_contributions": ["Introduction of a reward function for metareasoning in LLMs", "Reduction of inference costs by 20-37%", "Use of Expert Iteration for training LLMs on selective reasoning"], "limitations": "", "keywords": ["Large Language Models", "Metareasoning", "Inference Cost Optimization"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.18469", "pdf": "https://arxiv.org/pdf/2410.18469.pdf", "abs": "https://arxiv.org/abs/2410.18469", "title": "ADVLLM: Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities", "authors": ["Chung-En Sun", "Xiaodong Liu", "Weiwei Yang", "Tsui-Wei Weng", "Hao Cheng", "Aidan San", "Michel Galley", "Jianfeng Gao"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to NAACL 2025 Main (oral)", "summary": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety.", "AI": {"tldr": "ADV-LLM introduces an efficient method for generating adversarial suffixes that enhance jailbreak attacks on LLMs.", "motivation": "To address the computational inefficiencies and low success rates of current methods for generating adversarial suffixes in LLMs.", "method": "An iterative self-tuning process is developed to craft adversarial suffixes, significantly reducing generation costs and enhancing attack success rates.", "result": "Achieves nearly 100% Attack Success Rate on various open-source LLMs, and strong transferability to closed-source models, including 99% on GPT-3.5 and 49% on GPT-4.", "conclusion": "ADV-LLM not only enhances the jailbreak capabilities of LLMs but also aids in future safety alignment research by generating datasets for studying LLM safety.", "key_contributions": ["Develops ADV-LLM for efficient adversarial suffix generation.", "Achieves high attack success rates on multiple LLMs.", "Provides datasets for safety alignment research."], "limitations": "", "keywords": ["Large Language Models", "Adversarial Suffixes", "Safety Alignment", "Jailbreak Attacks", "Machine Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.23478", "pdf": "https://arxiv.org/pdf/2410.23478.pdf", "abs": "https://arxiv.org/abs/2410.23478", "title": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs", "authors": ["Sireesh Gururaja", "Yueheng Zhang", "Guannan Tang", "Tianhao Zhang", "Kevin Murphy", "Yu-Tsen Yi", "Junwon Seo", "Anthony Rollett", "Emma Strubell"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science.", "AI": {"tldr": "Collage is a tool for evaluating and prototyping information extraction models on scientific PDFs, providing insights into processing failures and supporting various NLP models.", "motivation": "To address the challenges of comparing domain-specific information extraction models for scientific documents, especially PDF formats, which are prevalent in academia.", "method": "Collage allows the evaluation of token classifiers, LLMs, and other models, while providing interfaces for experimentation and debugging of modeling pipelines.", "result": "Collage supports rapid prototyping and visualization of information extraction models, improving the ability to conduct literature reviews in fields like materials science.", "conclusion": "The tool enhances the understanding and usability of NLP models for non-experts and facilitates better evaluation of model performance.", "key_contributions": ["Rapid prototyping and evaluation of NLP models on scientific PDFs.", "Granular inspection and debugging capabilities for users and developers.", "Support for various models through extensible software interfaces."], "limitations": "", "keywords": ["Natural Language Processing", "Information Extraction", "Scientific Documents", "PDF", "HuggingFace"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.10227", "pdf": "https://arxiv.org/pdf/2411.10227.pdf", "abs": "https://arxiv.org/abs/2411.10227", "title": "Entropy and type-token ratio in gigaword corpora", "authors": ["Pablo Rosillo-Rodes", "Maxi San Miguel", "David Sanchez"], "categories": ["cs.CL", "cs.IR", "physics.soc-ph"], "comment": "15 pages, 10 figures, 8 tables", "summary": "There are different ways of measuring diversity in complex systems. In\nparticular, in language, lexical diversity is characterized in terms of the\ntype-token ratio and the word entropy. We here investigate both diversity\nmetrics in six massive linguistic datasets in English, Spanish, and Turkish,\nconsisting of books, news articles, and tweets. These gigaword corpora\ncorrespond to languages with distinct morphological features and differ in\nregisters and genres, thus constituting a varied testbed for a quantitative\napproach to lexical diversity. We unveil an empirical functional relation\nbetween entropy and type-token ratio of texts of a given corpus and language,\nwhich is a consequence of the statistical laws observed in natural language.\nFurther, in the limit of large text lengths we find an analytical expression\nfor this relation relying on both Zipf and Heaps laws that agrees with our\nempirical findings.", "AI": {"tldr": "Investigation of lexical diversity metrics using six linguistic datasets.", "motivation": "To explore the relationship between lexical diversity metrics in different languages and types of texts.", "method": "Analyzed type-token ratio and word entropy in English, Spanish, and Turkish datasets comprising books, articles, and tweets.", "result": "Established a functional relationship between entropy and type-token ratio that holds across different languages and text types.", "conclusion": "Empirical and analytical findings support a derived relationship influenced by statistical laws of natural language, enhancing understanding of lexical diversity.", "key_contributions": ["Empirical analysis of lexical diversity across different languages and text genres.", "Derivation of a functional relationship between type-token ratio and word entropy.", "Validation of Zipf and Heaps laws in the context of lexical diversity."], "limitations": "", "keywords": ["lexical diversity", "type-token ratio", "word entropy", "linguistic datasets", "statistical laws"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2411.19832", "pdf": "https://arxiv.org/pdf/2411.19832.pdf", "abs": "https://arxiv.org/abs/2411.19832", "title": "Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation", "authors": ["Dimosthenis Antypas", "Indira Sen", "Carla Perez-Almendros", "Jose Camacho-Collados", "Francesco Barbieri"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted at the 9th Workshop on Online Abuse and Harms (WOAH)", "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.", "AI": {"tldr": "A novel dataset for social media content moderation across six sensitive categories shows significant performance improvements for LLMs.", "motivation": "To enhance the detection of sensitive content in large datasets and address the limitations of current moderation tools and existing datasets focusing mainly on toxic language.", "method": "Collected and annotated a dataset for social media moderation across six categories, fine-tuning LLMs on this dataset to evaluate detection performance.", "result": "Fine-tuning LLMs on the novel dataset significantly improves detection performance by 10-15% compared to popular existing models like LLaMA and proprietary OpenAI models.", "conclusion": "The new dataset provides a more effective solution for moderating sensitive content, outperforming existing moderation tools and APIs.", "key_contributions": ["Creation of a unified dataset for six sensitive categories", "Demonstration of improved LLM performance on this dataset", "Addressing gaps in existing content moderation frameworks"], "limitations": "", "keywords": ["content moderation", "sensitive content", "large language models", "dataset", "social media"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.12743", "pdf": "https://arxiv.org/pdf/2502.12743.pdf", "abs": "https://arxiv.org/abs/2502.12743", "title": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect and Explain LLM-Generated Texts?", "authors": ["Jiazhou Ji", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Yang Xu", "Xinru Lu", "Xiaoyu Jiang", "Ruizhe Li", "Shujun Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Distinguishing between human- and LLM-generated texts is crucial given the\nrisks associated with misuse of LLMs. This paper investigates detection and\nexplanation capabilities of current LLMs across two settings: binary (human vs.\nLLM-generated) and ternary classification (including an ``undecided'' class).\nWe evaluate 6 close- and open-source LLMs of varying sizes and find that\nself-detection (LLMs identifying their own outputs) consistently outperforms\ncross-detection (identifying outputs from other LLMs), though both remain\nsuboptimal. Introducing a ternary classification framework improves both\ndetection accuracy and explanation quality across all models. Through\ncomprehensive quantitative and qualitative analyses using our human-annotated\ndataset, we identify key explanation failures, primarily reliance on inaccurate\nfeatures, hallucinations, and flawed reasoning. Our findings underscore the\nlimitations of current LLMs in self-detection and self-explanation,\nhighlighting the need for further research to address overfitting and enhance\ngeneralizability.", "AI": {"tldr": "This paper explores the detection capabilities of various LLMs to differentiate between human-generated and LLM-generated texts, revealing suboptimal performance and key explanation failures.", "motivation": "To understand the risks associated with LLM misuse by evaluating their capabilities in distinguishing between human and LLM-generated texts.", "method": "The study evaluates six close- and open-source LLMs in both binary and ternary classification settings, analyzing detection accuracy and explanation quality using a human-annotated dataset.", "result": "Self-detection consistently outperforms cross-detection across all models, while the ternary classification framework enhances detection accuracy and explanation quality.", "conclusion": "The findings point out the limitations of LLMs in self-detection and self-explanation, indicating a need for further research to improve generalizability and address issues like overfitting.", "key_contributions": ["Investigation of self-detection vs cross-detection capabilities in LLMs.", "Introduction of a ternary classification framework for improved detection accuracy.", "Identification of key explanation failures in LLM outputs."], "limitations": "Reliance on inaccurate features, hallucinations, and flawed reasoning in LLM explanations.", "keywords": ["LLM detection", "self-detection", "classification", "explanation quality", "explanation failures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.17036", "pdf": "https://arxiv.org/pdf/2502.17036.pdf", "abs": "https://arxiv.org/abs/2502.17036", "title": "Language Model Re-rankers are Fooled by Lexical Similarities", "authors": ["Lovisa Hagstrm", "Ercong Nie", "Ruben Halifa", "Helmut Schmid", "Richard Johansson", "Alexander Junge"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to FEVER 2025", "summary": "Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information\nand the relations between the query and the retrieved answers. To understand\nwhether LM re-rankers always live up to this assumption, we evaluate 6\ndifferent LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show\nthat LM re-rankers struggle to outperform a simple BM25 baseline on DRUID.\nLeveraging a novel separation metric based on BM25 scores, we explain and\nidentify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation.", "AI": {"tldr": "This paper evaluates 6 different language model re-rankers used in retrieval-augmented generation and finds that they often underperform compared to a simple BM25 baseline, suggesting the need for better evaluation datasets.", "motivation": "To assess whether language model re-rankers indeed improve retrieval results over traditional methods like BM25, and to investigate their weaknesses.", "method": "Evaluated 6 LM re-rankers on the NQ, LitQA2, and DRUID datasets and introduced a novel separation metric based on BM25 scores to analyze performance.", "result": "LM re-rankers struggled to outperform the BM25 baseline on the DRUID dataset, with identified errors stemming from lexical dissimilarities.", "conclusion": "The results suggest that current LM re-rankers have significant weaknesses and highlight the necessity of more realistic evaluation datasets.", "key_contributions": ["Evaluation of 6 LM re-rankers against traditional methods", "Identification of re-ranker errors related to lexical dissimilarities", "Introduction of a novel separation metric based on BM25 scores"], "limitations": "The performance issues were primarily observed on the DRUID dataset and the improvements were mainly effective for the NQ dataset.", "keywords": ["language model re-rankers", "retrieval-augmented generation", "BM25", "NQ", "LitQA2", "DRUID"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.16553", "pdf": "https://arxiv.org/pdf/2503.16553.pdf", "abs": "https://arxiv.org/abs/2503.16553", "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models", "authors": ["Zhenlin Qin", "Leizhen Wang", "Francisco Camara Pereira", "Zhenliang Ma"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs.", "AI": {"tldr": "This paper presents a unified fine-tuning framework for training LLM-based mobility prediction models, improving accuracy and transferability across diverse contexts.", "motivation": "To address the limitations of existing LLM-based mobility prediction models that struggle with adaptability to different contexts and datasets.", "method": "A unified fine-tuning framework for a foundational open-source LLM tailored for mobility prediction tasks was developed and tested across six real-world datasets.", "result": "The proposed model outperformed state-of-the-art deep learning and LLM-based models in prediction accuracy and transferability.", "conclusion": "The findings suggest that the unified fine-tuning approach significantly enhances the applicability of LLMs for predicting individual mobility across varied contexts.", "key_contributions": ["Development of a unified fine-tuning framework for LLMs in mobility prediction", "Validation through experiments on six diverse mobility datasets", "Demonstrated improved prediction accuracy and adaptability over existing models"], "limitations": "", "keywords": ["Large Language Models", "mobility prediction", "fine-tuning", "transferability", "open source"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.08697", "pdf": "https://arxiv.org/pdf/2504.08697.pdf", "abs": "https://arxiv.org/abs/2504.08697", "title": "Large Language Models as Span Annotators", "authors": ["Zdenk Kasner", "Vilm Zouhar", "Patrcia Schmidtov", "Ivan Kart", "Kristna Onderkov", "Ondej Pltek", "Dimitra Gkatzia", "Saad Mahamood", "Ondej Duek", "Simone Balloccu"], "categories": ["cs.CL"], "comment": null, "summary": "Span annotation is the task of localizing and classifying text spans\naccording to custom guidelines. Annotated spans can be used to analyze and\nevaluate high-quality texts for which single-score metrics fail to provide\nactionable feedback. Until recently, span annotation was limited to human\nannotators or fine-tuned models. In this study, we show that large language\nmodels (LLMs) can serve as flexible and cost-effective span annotation\nbackbones. To demonstrate their utility, we compare LLMs to skilled human\nannotators on three diverse span annotation tasks: evaluating data-to-text\ngeneration, identifying translation errors, and detecting propaganda\ntechniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA)\ncomparable to human annotators at a fraction of a cost per output annotation.\nWe also manually analyze model outputs, finding that LLMs make errors at a\nsimilar rate to human annotators. We release the dataset of more than 40k model\nand human annotations for further research.", "AI": {"tldr": "This study explores the use of large language models (LLMs) for span annotation, demonstrating their effectiveness and cost-efficiency compared to human annotators.", "motivation": "Addressing the limitations of traditional span annotation methods and providing a flexible, cost-effective alternative using LLMs.", "method": "Comparison of LLMs and skilled human annotators on three span annotation tasks, including evaluation of data-to-text generation, identification of translation errors, and detection of propaganda techniques.", "result": "LLMs achieved inter-annotator agreement comparable to human annotators and made errors at a similar rate, with significantly lower cost per output annotation.", "conclusion": "LLMs can effectively replace or assist human annotators in span annotation tasks, providing a valuable resource for further research with a released dataset of 40k annotations.", "key_contributions": ["Demonstrated LLMs' capability in span annotation tasks.", "Provided a comparative analysis of LLMs and human annotators' performance.", "Released a large dataset for ongoing research."], "limitations": "The potential biases and limitations of LLM outputs were not exhaustively explored.", "keywords": ["span annotation", "large language models", "human annotators", "text evaluation", "data-to-text generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.06609", "pdf": "https://arxiv.org/pdf/2506.06609.pdf", "abs": "https://arxiv.org/abs/2506.06609", "title": "Transferring Features Across Language Models With Model Stitching", "authors": ["Alan Chen", "Jack Merullo", "Alessandro Stolfo", "Ellie Pavlick"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we demonstrate that affine mappings between residual streams of\nlanguage models is a cheap way to effectively transfer represented features\nbetween models. We apply this technique to transfer the weights of Sparse\nAutoencoders (SAEs) between models of different sizes to compare their\nrepresentations. We find that small and large models learn similar\nrepresentation spaces, which motivates training expensive components like SAEs\non a smaller model and transferring to a larger model at a FLOPs savings. In\nparticular, using a small-to-large transferred SAE as initialization can lead\nto 50% cheaper training runs when training SAEs on larger models. Next, we show\nthat transferred probes and steering vectors can effectively recover ground\ntruth performance. Finally, we dive deeper into feature-level transferability,\nfinding that semantic and structural features transfer noticeably differently\nwhile specific classes of functional features have their roles faithfully\nmapped. Overall, our findings illustrate similarities and differences in the\nlinear representation spaces of small and large models and demonstrate a method\nfor improving the training efficiency of SAEs.", "AI": {"tldr": "This paper shows that affine mappings between language model residual streams can transfer features effectively, using Sparse Autoencoders (SAEs) to demonstrate significant training efficiency gains.", "motivation": "To improve representation transfer between language models of different sizes and enhance the training efficiency of Sparse Autoencoders (SAEs).", "method": "Applying affine mappings to transfer weights of Sparse Autoencoders between models of different sizes and analyzing representation spaces and feature transferability.", "result": "Demonstrated that small and large models possess similar representation spaces, leading to a potential 50% reduction in training costs when using transferred SAEs as initialization for larger models.", "conclusion": "Feature transfer is effective between models of different sizes, with significant differences in how semantic and structural features transfer; this approach improves training efficiency for SAEs.", "key_contributions": ["Introduced a method for transferring features between models using affine mappings.", "Showed cost-saving benefits of transferring SAEs from small to large models.", "Analyzed the transferability of various feature types across model sizes."], "limitations": "", "keywords": ["Language Models", "Sparse Autoencoders", "Feature Transferability", "Training Efficiency", "Representation Spaces"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.06877", "pdf": "https://arxiv.org/pdf/2506.06877.pdf", "abs": "https://arxiv.org/abs/2506.06877", "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning", "authors": ["Jiaxing Guo", "Wenjie Yang", "Shengzhong Zhang", "Tongshan Xu", "Lun Du", "Da Zheng", "Zengfeng Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning.", "AI": {"tldr": "Introducing ParaStepVerifier for step-by-step verification of mathematical problem-solving in LLMs.", "motivation": "The paper highlights the issue of reward hacking in LLMs, where correct answers are produced via incorrect reasoning. This raises concerns about the reliability of LLMs in mathematical tasks.", "method": "ParaStepVerifier, a new methodology designed to perform detailed, step-by-step verification of the reasoning processes in LLMs when solving mathematical problems.", "result": "Empirical results show that ParaStepVerifier significantly improves the identification of flawed reasoning steps compared to previous baselines, particularly in handling complex, multi-step problems.", "conclusion": "ParaStepVerifier presents a promising solution for enhancing the evaluation and training of LLMs to ensure authentic mathematical reasoning capabilities.", "key_contributions": ["Introduction of MathOlympiadEval dataset with fine-grained annotations", "Development of ParaStepVerifier methodology", "Demonstrated improvement in identifying flawed reasoning in LLM outputs"], "limitations": "", "keywords": ["Large Language Models", "Mathematical problem-solving", "Step-by-step verification", "Reward hacking", "Reasoning flaws"], "importance_score": 9, "read_time_minutes": 15}}
