{"id": "2506.14775", "pdf": "https://arxiv.org/pdf/2506.14775.pdf", "abs": "https://arxiv.org/abs/2506.14775", "title": "See What I Mean? CUE: A Cognitive Model of Understanding Explanations", "authors": ["Tobias Labarta", "Nhi Hoang", "Katharina Weitz", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "categories": ["cs.HC", "cs.AI", "cs.LG", "68T05, 91E30, 68U35, 62P10", "H.5.2; I.2.6; H.1.2; I.4.9; K.4.2"], "comment": "10 pages, 5 figures (main text), 4 tables, 455-participant user study", "summary": "As machine learning systems increasingly inform critical decisions, the need\nfor human-understandable explanations grows. Current evaluations of Explainable\nAI (XAI) often prioritize technical fidelity over cognitive accessibility which\ncritically affects users, in particular those with visual impairments. We\npropose CUE, a model for Cognitive Understanding of Explanations, linking\nexplanation properties to cognitive sub-processes: legibility (perception),\nreadability (comprehension), and interpretability (interpretation). In a study\n(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we\nfound comparable task performance but lower confidence/effort for visually\nimpaired users. Unlike expected, these gaps were not mitigated and sometimes\nworsened by accessibility-focused color maps like Cividis. These results\nchallenge assumptions about perceptual optimization and support the need for\nadaptive XAI interfaces. They also validate CUE by demonstrating that altering\nexplanation legibility affects understandability. We contribute: (1) a\nformalized cognitive model for explanation understanding, (2) an integrated\ndefinition of human-centered explanation properties, and (3) empirical evidence\nmotivating accessible, user-tailored XAI.", "AI": {"tldr": "The paper presents CUE, a cognitive model for understanding explanations in Explainable AI (XAI) focused on improving accessibility for visually impaired users, revealing shortcomings of current color maps in enhancing explanation legibility and understanding.", "motivation": "To address the growing need for human-understandable explanations in machine learning systems, particularly for users with visual impairments, and to challenge the focus on technical fidelity over cognitive accessibility in XAI evaluations.", "method": "A study involving 455 participants tested the effectiveness of different heatmap colormaps (BWR, Cividis, Coolwarm) on task performance, confidence, and effort in understanding explanations.", "result": "The study revealed that while task performance was comparable across groups, visually impaired users exhibited lower confidence and effort, with accessibility-focused color maps like Cividis not improving gaps as expected.", "conclusion": "The findings suggest a need for adaptive XAI interfaces that take cognitive processes into account, emphasizing that alterations in legibility significantly affect users' understanding of explanations.", "key_contributions": ["A formalized cognitive model for explanation understanding (CUE)", "An integrated definition of human-centered explanation properties", "Empirical evidence for the importance of accessible, user-tailored XAI interfaces"], "limitations": "Study limited to heatmap color maps; further exploration needed across other explanation modalities and user demographics.", "keywords": ["Explainable AI", "Cognitive Understanding", "Accessibility", "Human-Computer Interaction", "Color Maps"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14777", "pdf": "https://arxiv.org/pdf/2506.14777.pdf", "abs": "https://arxiv.org/abs/2506.14777", "title": "WebXAII: an open-source web framework to study human-XAI interaction", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "SÃ©bastien Harispe"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This article introduces WebXAII, an open-source web framework designed to\nfacilitate research on human interaction with eXplainable Artificial\nIntelligence (XAI) systems. The field of XAI is rapidly expanding, driven by\nthe growing societal implications of the widespread adoption of AI (and in\nparticular machine learning) across diverse applications. Researchers who study\nthe interaction between humans and XAI techniques typically develop ad hoc\ninterfaces in order to conduct their studies. These interfaces are usually not\nshared alongside the results of the studies, which limits their reusability and\nthe reproducibility of experiments. In response, we design and implement\nWebXAII, a web-based platform that can embody full experimental protocols,\nmeaning that it can present all aspects of the experiment to human participants\nand record their responses. The experimental protocols are translated into a\ncomposite architecture of generic views and modules, which offers a lot of\nflexibility. The architecture is defined in a structured configuration file, so\nthat protocols can be implemented with minimal programming skills. We\ndemonstrate that WebXAII can effectively embody relevant protocols, by\nreproducing the protocol of a state-of-the-art study of the literature. The\nframework is available at https://github.com/PAJEAN/WebXAII.", "AI": {"tldr": "WebXAII is an open-source web framework for research on human interaction with explainable AI, providing a flexible platform for conducting reproducible experiments with minimal programming.", "motivation": "Address the limitations in sharing and reusability of ad hoc interfaces used in studying interactions with explainable AI.", "method": "WebXAII is designed as a web-based platform that can embody full experimental protocols, translating them into a composite architecture of generic views and modules.", "result": "WebXAII effectively implements and reproduces experimental protocols, demonstrated through a state-of-the-art study from the literature.", "conclusion": "The framework promotes reproducibility in XAI research by allowing detailed implementation of experimental protocols with little programming required.", "key_contributions": ["Open-source web framework for XAI research", "Supports full experimental protocol implementation", "Flexible architecture for minimal programming"], "limitations": "", "keywords": ["explainable AI", "human-computer interaction", "web framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14799", "pdf": "https://arxiv.org/pdf/2506.14799.pdf", "abs": "https://arxiv.org/abs/2506.14799", "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in AI has enabled automated analysis of complex media content\nat scale and generate actionable insights regarding character representation\nalong such dimensions as gender and age. Past work focused on quantifying\nrepresentation from audio/video/text using various ML models, but without\nhaving the audience in the loop. We ask, even if character distribution along\ndemographic dimensions are available, how useful are they to the general\npublic? Do they actually trust the numbers generated by AI models? Our work\naddresses these questions through a user study, while proposing a new AI-based\ncharacter representation and visualization tool. Our tool based on the\nContrastive Language Image Pretraining (CLIP) foundation model to analyze\nvisual screen data to quantify character representation across dimensions of\nage and gender. We also designed effective visualizations suitable for\npresenting such analytics to lay audience. Next, we conducted a user study to\nseek empirical evidence on the usefulness and trustworthiness of the\nAI-generated results for carefully chosen movies presented in the form of our\nvisualizations. We note that participants were able to understand the analytics\nfrom our visualization, and deemed the tool `overall useful'. Participants also\nindicated a need for more detailed visualizations to include more demographic\ncategories and contextual information of the characters. Participants' trust in\nAI-based gender and age models is seen to be moderate to low, although they\nwere not against the use of AI in this context. Our tool including code,\nbenchmarking, and data from the user study can be found here:\nhttps://anonymous.4open.science/r/Character-Representation-Media-FF7B", "AI": {"tldr": "This paper presents a new AI-based tool for analyzing character representation in media, focusing on gender and age, and evaluates its usefulness and trustworthiness through a user study.", "motivation": "To assess the usefulness and trustworthiness of AI-generated character representation analytics in media for the general public, as previous works lacked audience involvement.", "method": "Utilized a tool based on the Contrastive Language Image Pretraining (CLIP) model to analyze character representation across age and gender dimensions, supported by effective visualizations designed for a lay audience. Conducted a user study to evaluate understanding and trust in the analytics provided by the tool.", "result": "Participants found the tool's visualizations helpful and expressed overall utility, although their trust in AI-generated analytics on gender and age was moderate to low. More detailed visualizations were requested to cover additional demographic categories and contextual information.", "conclusion": "The AI-generated character representation tool provides useful insights but requires improvement in trust and detail for greater acceptance by users.", "key_contributions": ["Development of an AI-based character representation tool using the CLIP model", "User study demonstrating the tool's empirical usefulness", "Insights into public trust in AI-generated demographic analytics"], "limitations": "Participants' trust in AI models was moderate to low; more detailed visualizations for various demographics are needed.", "keywords": ["AI", "character representation", "user study", "CLIP", "visualization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14809", "pdf": "https://arxiv.org/pdf/2506.14809.pdf", "abs": "https://arxiv.org/abs/2506.14809", "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model", "authors": ["Peng Jiang", "Vinicius Cezar Monteiro de Lira", "Antonio Maiorino"], "categories": ["cs.HC", "cs.LG", "I.2; H.4"], "comment": null, "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating\nhigh-quality surveys remains labor-intensive, requiring both domain expertise\nand methodological rigor. With the evolution of large language models (LLMs),\nnew opportunities emerge to automate survey generation. This paper presents the\nreal-world deployment of an LLM-powered system designed to accelerate data\ncollection while maintaining survey quality. Deploying such systems in\nproduction introduces real-world complexity, including diverse user needs and\nquality control. We evaluate the system using the DeLone and McLean IS Success\nModel to understand how generative AI can reshape a core IS method. This study\nmakes three key contributions. To our knowledge, this is the first application\nof the IS Success Model to a generative AI system for survey creation. In\naddition, we propose a hybrid evaluation framework combining automated and\nhuman assessments. Finally, we implement safeguards that mitigate\npost-deployment risks and support responsible integration into IS workflows.", "AI": {"tldr": "This paper discusses the deployment of an LLM-powered system for automated survey generation in Information Systems (IS) research, focusing on quality and real-world application challenges.", "motivation": "To address the labor-intensive nature of creating high-quality surveys in IS research by leveraging advancements in large language models (LLMs) for automation.", "method": "Deployment of an LLM-powered system, evaluated using the DeLone and McLean IS Success Model.", "result": "The study reveals how generative AI can enhance survey quality and efficiency while outlining complexities of real-world deployment.", "conclusion": "Generative AI offers a transformative approach to survey creation in IS, but requires careful integration and evaluation frameworks to ensure quality and mitigate risks.", "key_contributions": ["First use of the IS Success Model for a generative AI system in survey creation.", "Proposed a hybrid evaluation framework integrating automated and human assessments.", "Implemented safeguards for responsible integration into IS workflows."], "limitations": "Complexities in user needs and quality control during real-world deployment.", "keywords": ["Human-Computer Interaction", "Large Language Models", "Survey Generation", "Information Systems", "Generative AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14900", "pdf": "https://arxiv.org/pdf/2506.14900.pdf", "abs": "https://arxiv.org/abs/2506.14900", "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "authors": ["Imane Guellil", "SalomÃ© Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "categories": ["cs.CL"], "comment": "Accepted and will be published at ACL2025 (main conference)", "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.", "AI": {"tldr": "A dataset for extracting adverse events from discharge summaries of elderly patients is presented, addressing underrepresentation in clinical NLP.", "motivation": "To address the challenges in adverse event extraction from clinical text, particularly for elderly patients who are underrepresented in existing datasets.", "method": "Development of a manually annotated corpus containing 14 clinically significant adverse events along with contextual attributes; evaluation using various models in FlairNLP.", "result": "Transformer models achieved high performance in coarse-grained extraction but struggled significantly with fine-grained tasks, revealing ongoing challenges in the extraction of nuanced clinical information.", "conclusion": "Despite high-level performance metrics, difficulties remain in extracting rare events and complex attributes, indicating a need for continued research in this area.", "key_contributions": ["A novel dataset targeting adverse event extraction in discharge summaries of the elderly.", "Support for both discontinuous and overlapping entities in the annotation schema.", "Evaluation of multiple models, highlighting gaps in fine-grained entity extraction performance."], "limitations": "Focus on a specific population may limit generalizability; performance drop for fine-grained entity extraction shows existing challenges.", "keywords": ["Adverse Event extraction", "clinical NLP", "dataset", "elderly patients", "transformer models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14820", "pdf": "https://arxiv.org/pdf/2506.14820.pdf", "abs": "https://arxiv.org/abs/2506.14820", "title": "Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction", "authors": ["Hyeon Jeon", "Hyunwook Lee", "Yun-Hsin Kuo", "Taehyun Yang", "Daniel Archambault", "Sungahn Ko", "Takanori Fujiwara", "Kwan-Liu Ma", "Jinwook Seo"], "categories": ["cs.HC", "cs.LG"], "comment": "EG/VGTC EuroVis 2025 Short paper", "summary": "Visual analytics using dimensionality reduction (DR) can easily be unreliable\nfor various reasons, e.g., inherent distortions in representing the original\ndata. The literature has thus proposed a wide range of methodologies to make\nDR-based visual analytics reliable. However, the diversity and extensiveness of\nthe literature can leave novice analysts and researchers uncertain about where\nto begin and proceed. To address this problem, we propose a guide for reading\npapers for reliable visual analytics with DR. Relying on the previous\nclassification of the relevant literature, our guide helps both practitioners\nto (1) assess their current DR expertise and (2) identify papers that will\nfurther enhance their understanding. Interview studies with three experts in DR\nand data visualizations validate the significance, comprehensiveness, and\nusefulness of our guide.", "AI": {"tldr": "This paper presents a guide for reading literature on reliable visual analytics using dimensionality reduction (DR), aimed at aiding beginners in the field.", "motivation": "Novice analysts face challenges in navigating the extensive literature on dimensionality reduction and reliable visual analytics.", "method": "The paper proposes a guide based on a classification of the literature, aiming to help practitioners assess their DR expertise and identify relevant papers.", "result": "Interview studies with three experts validate the guide's significance and utility for enhancing understanding of DR.", "conclusion": "The proposed guide can significantly assist novice analysts and practitioners in navigating the literature on reliable visual analytics with DR.", "key_contributions": ["Development of a comprehensive guide for reliable visual analytics", "Classification of literature on dimensionality reduction", "Empirical validation through interviews with experts"], "limitations": "", "keywords": ["visual analytics", "dimensionality reduction", "literature guide", "data visualization", "expert interviews"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.14901", "pdf": "https://arxiv.org/pdf/2506.14901.pdf", "abs": "https://arxiv.org/abs/2506.14901", "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction", "authors": ["Marija Å akota", "Robert West"], "categories": ["cs.CL"], "comment": null, "summary": "Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.", "AI": {"tldr": "Boosted Constrained Decoding (BoostCD) improves structured NLP task performance by combining predictions from constrained and unconstrained decoding phases.", "motivation": "To address low-quality outputs during constrained decoding at test time for structured NLP tasks using autoregressive models.", "method": "BoostCD operates in two phases: Phase 1 involves decoding from the base model in both constrained and unconstrained modes to generate two weak predictions, and Phase 2 combines these predictions using a learned autoregressive model for a final output.", "result": "BoostCD, demonstrated through the BoostIE model, outperforms prior methods in closed information extraction tasks both in and out of distribution.", "conclusion": "The complementary mistakes made by the base model in different modes are effectively leveraged by the boosted model, leading to improved structured outputs.", "key_contributions": ["Introduction of Boosted Constrained Decoding (BoostCD) methodology.", "Improvement of structured NLP task results using BoostIE.", "Demonstrated effectiveness in closed information extraction beyond prior approaches."], "limitations": "", "keywords": ["Boosted Constrained Decoding", "Structured NLP", "Information Extraction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14829", "pdf": "https://arxiv.org/pdf/2506.14829.pdf", "abs": "https://arxiv.org/abs/2506.14829", "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities", "authors": ["Aditya Majumdar", "Wenbo Zhang", "Kashvi Prawal", "Amulya Yadav"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects\nfocus on harnessing AI to address societal issues in areas such as healthcare,\nsocial justice, etc. Unfortunately, despite growing interest in AI4SI,\nachieving tangible, on-the-ground impact remains a significant challenge. For\nexample, identifying and engaging motivated collaborators who are willing to\nco-design and deploy AI based solutions in real-world settings is often\ndifficult. Even when such partnerships are established, many AI4SI projects\n\"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to\ntransition to at-scale production-level solutions. Furthermore, the unique\nchallenges faced by AI4SI researchers are not always fully recognized within\nthe broader AI community, where such work is sometimes viewed as primarily\napplied and not aligning with the traditional criteria for novelty emphasized\nin core AI venues. This paper attempts to shine a light on the diverse\nchallenges faced in AI4SI research by diagnosing a multitude of factors that\nprevent AI4SI partnerships from achieving real-world impact on the ground.\nDrawing on semi-structured interviews with six leading AI4SI researchers -\ncomplemented by the authors' own lived experiences in conducting AI4SI research\n- this paper attempts to understand the day-to-day difficulties faced in\ndeveloping and deploying socially impactful AI solutions. Through thematic\nanalysis, we identify structural and organizational, communication,\ncollaboration, and operational challenges as key barriers to deployment. While\nthere are no easy fixes, we synthesize best practices and actionable strategies\ndrawn from these interviews and our own work in this space. In doing so, we\nhope this paper serves as a practical reference guide for AI4SI researchers and\npartner organizations seeking to engage more effectively in socially impactful\nAI collaborations.", "AI": {"tldr": "This paper examines the challenges faced in AI for Social Impact (AI4SI) projects, focusing on obstacles to collaboration and real-world implementation.", "motivation": "The motivation behind this paper is to illuminate the challenges faced by AI4SI projects in achieving societal impact and to provide actionable strategies to overcome these barriers.", "method": "The authors conducted semi-structured interviews with six leading AI4SI researchers and analyzed their findings through thematic analysis.", "result": "Key challenges identified include structural, organizational, communication, collaboration, and operational barriers that hinder the deployment of AI solutions in social contexts.", "conclusion": "The paper synthesizes best practices and strategies to help AI4SI researchers and partner organizations enhance their collaboration and effectiveness in socially impactful projects.", "key_contributions": ["Identification of key challenges facing AI4SI projects", "Thematic analysis of interviews with AI4SI researchers", "Recommendations for best practices and actionable strategies for effective collaboration"], "limitations": "", "keywords": ["AI for Social Impact", "AI4SI", "collaboration", "socially impactful AI", "deployment challenges"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2506.14912", "pdf": "https://arxiv.org/pdf/2506.14912.pdf", "abs": "https://arxiv.org/abs/2506.14912", "title": "CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision", "authors": ["Dyah Adila", "Shuai Zhang", "Boran Han", "Bonan Min", "Yuyang Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.", "AI": {"tldr": "CrEst is a weakly supervised framework for assessing the credibility of context documents in LLM inference, improving accuracy and robustness in knowledge-intensive tasks.", "motivation": "To enhance LLM performance by addressing the reliability of context documents that can influence the output.", "method": "CrEst utilizes semantic coherence among credible documents to provide automated credibility estimation through inter-document agreement, using both black-box and white-box integration strategies.", "result": "CrEst improves accuracy by 26.86% and the F1 score by 3.49% across various models and datasets, outperforming strong baselines.", "conclusion": "The framework effectively integrates credibility assessment into LLM inference, demonstrating consistent performance even in high-noise environments.", "key_contributions": ["Introduction of a weakly supervised framework for document credibility assessment", "Development of two integration strategies for LLMs", "Significant improvements over baseline models in accuracy and F1 score"], "limitations": "", "keywords": ["large language models", "credibility assessment", "machine learning", "natural language processing", "contextual information"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14948", "pdf": "https://arxiv.org/pdf/2506.14948.pdf", "abs": "https://arxiv.org/abs/2506.14948", "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework", "authors": ["Mohna Chakraborty", "Lu Wang", "David Jurgens"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in domains requiring\nmoral understanding, yet their reasoning often remains shallow, and misaligned\nwith human reasoning. Unlike humans, whose moral reasoning integrates\ncontextual trade-offs, value systems, and ethical theories, LLMs often rely on\nsurface patterns, leading to biased decisions in morally and ethically complex\nscenarios. To address this gap, we present a value-grounded framework for\nevaluating and distilling structured moral reasoning in LLMs. We benchmark 12\nopen-source models across four moral datasets using a taxonomy of prompts\ngrounded in value systems, ethical theories, and cognitive reasoning\nstrategies. Our evaluation is guided by four questions: (1) Does reasoning\nimprove LLM decision-making over direct prompting? (2) Which types of\nvalue/ethical frameworks most effectively guide LLM reasoning? (3) Which\ncognitive reasoning strategies lead to better moral performance? (4) Can\nsmall-sized LLMs acquire moral competence through distillation? We find that\nprompting with explicit moral structure consistently improves accuracy and\ncoherence, with first-principles reasoning and Schwartz's + care-ethics\nscaffolds yielding the strongest gains. Furthermore, our supervised\ndistillation approach transfers moral competence from large to small models\nwithout additional inference cost. Together, our results offer a scalable path\ntoward interpretable and value-grounded models.", "AI": {"tldr": "This paper presents a value-grounded framework to enhance moral reasoning in LLMs, demonstrating that structured prompting improves decision-making and that moral competence can be distilled from large to small models effectively.", "motivation": "Large language models often exhibit shallow reasoning in moral contexts, leading to biased decisions. This paper seeks to provide a better understanding of moral reasoning in LLMs by integrating explicit value systems and ethical theories into their processing.", "method": "The authors benchmarked 12 open-source LLMs across four moral datasets using a range of structured prompts based on value systems and cognitive reasoning strategies. They evaluated whether structured prompting improves decision-making and the effectiveness of different moral frameworks.", "result": "The study finds that prompting with explicit moral structures boosts accuracy and coherence. First-principles reasoning and Schwartz's care-ethics approaches yield significant improvements, and moral competence can be effectively distilled into smaller models without extra inference costs.", "conclusion": "The proposed framework demonstrates a scalable approach for developing interpretable and morally-grounded AI models, paving the way for more ethical AI applications.", "key_contributions": ["Introduction of a value-grounded framework for moral reasoning in LLMs", "Demonstration of performance improvements through structured moral prompting", "Successful distillation of moral competence from larger to smaller LLMs"], "limitations": "The study may have limitations in the generalizability of the results across all moral dilemmas and its dependence on specific datasets used for evaluation.", "keywords": ["language models", "moral reasoning", "value systems", "ethical theories", "cognitive strategies"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.14927", "pdf": "https://arxiv.org/pdf/2506.14927.pdf", "abs": "https://arxiv.org/abs/2506.14927", "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "authors": ["Joseph J. Peper", "Wenzhao Qiu", "Ali Payani", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.", "AI": {"tldr": "MDBench is a new dataset created for evaluating large language models on multi-document reasoning tasks, utilizing a novel synthetic generation process to create challenging document sets and corresponding question-answer examples.", "motivation": "The motivation behind MDBench is to provide an evaluation benchmark for the reasoning capabilities of large language models in the context of multi-document inputs, which are critical due to the limitations of existing benchmarks and the high cost of annotating such data.", "method": "MDBench is generated through a synthetic process that modifies condensed structured seed knowledge using LLM-assisted edits, resulting in controllable creation of document sets and QA examples tailored for multi-document reasoning.", "result": "Analysis shows that MDBench presents significant challenges for popular LLMs and prompting techniques, highlighting the need for robust handling of multi-document contexts, even for shorter sets.", "conclusion": "MDBench not only enables targeted analysis of multi-document reasoning but also adapts quickly to new challenges and future advancements in model performance.", "key_contributions": ["Introduction of MDBench as a dataset for multi-document reasoning", "Development of a novel synthetic generation process allowing efficient creation of challenging document sets", "Analysis of LLM performance on MDBench revealing significant reasoning challenges"], "limitations": "", "keywords": ["large language models", "multi-document reasoning", "evaluation benchmarks", "synthetic data generation", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.15008", "pdf": "https://arxiv.org/pdf/2506.15008.pdf", "abs": "https://arxiv.org/abs/2506.15008", "title": "Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output", "authors": ["Richa Gupta", "Alexander Htet Kyaw"], "categories": ["cs.HC", "cs.AI"], "comment": "15 Pages, 6 figures, CAAD Futures 2025", "summary": "Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.", "AI": {"tldr": "This paper presents a pipeline that integrates generative AI with sustainability metrics for interior architectural design.", "motivation": "To improve the actionable data provided by generative AI in architectural design, particularly focusing on sustainability metrics.", "method": "The proposed pipeline combines DALL-E 3 with a materials dataset, analyzing generated designs for material usage and corresponding CO2e values. It includes user tests to evaluate how sustainability information affects design decisions.", "result": "User tests indicated that providing sustainability metrics alongside AI-generated designs leads to more informed decisions, but may also cause decision fatigue and reduce satisfaction.", "conclusion": "The integration of sustainability metrics can enhance ecologically responsible design practices while highlighting the need to balance freedom and constraints in the creative process.", "key_contributions": ["Introduction of a novel pipeline that enriches AI-generated designs with sustainability data", "Evaluation of user responses to sustainability metrics", "Insights on balancing design creativity with environmental considerations"], "limitations": "The study indicates potential decision fatigue and decreased satisfaction when integrating extensive sustainability data.", "keywords": ["Generative AI", "Sustainability", "Interior Design", "DALL-E 3", "Materials Dataset"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.14949", "pdf": "https://arxiv.org/pdf/2506.14949.pdf", "abs": "https://arxiv.org/abs/2506.14949", "title": "From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?", "authors": ["Shadman Sakib", "Oishy Fatema Akhand", "Ajwad Abrar"], "categories": ["cs.CL"], "comment": "Accepted in 1st IEEE QPAIN 2025", "summary": "While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.", "AI": {"tldr": "This study explores the effectiveness of Large Language Models (LLMs) in predicting diabetes using various prompting methods and compares their performance with traditional machine learning models.", "motivation": "To investigate the application of Large Language Models in predicting medical conditions, specifically diabetes, given their under-explored potential for structured numerical data.", "method": "An empirical analysis was conducted using the Pima Indian Diabetes Database (PIDD) with six LLMs and three traditional ML models, evaluating performance using accuracy, precision, recall, and F1-score.", "result": "Proprietary LLMs, particularly GPT-4o and Gemma-2-27B, outperformed open-source models and traditional ML models in diabetes prediction, especially in few-shot settings.", "conclusion": "LLMs can effectively aid medical prediction tasks, but improvements in prompting strategies and domain-specific fine-tuning are needed for optimal performance.", "key_contributions": ["Demonstrated the effectiveness of LLMs in medical predictions", "Showed LLM performance surpassing traditional ML models in certain metrics", "Highlighted the need for further research in prompt engineering for healthcare applications"], "limitations": "Performance variance across different prompting strategies and necessity for domain-specific fine-tuning.", "keywords": ["diabetes prediction", "large language models", "machine learning", "healthcare", "Pima Indian Diabetes Database"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15047", "pdf": "https://arxiv.org/pdf/2506.15047.pdf", "abs": "https://arxiv.org/abs/2506.15047", "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi", "Dong Whi Yoo", "Keran Wang", "Violeta J. Rodriguez", "Ravi Karkar", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.", "AI": {"tldr": "The study develops 'Carey', a GPT-4o-based chatbot aimed at supporting family caregivers of individuals with Alzheimer's Disease and Related Dementia by addressing their emotional and informational needs.", "motivation": "Family caregivers of individuals with Alzheimer's Disease face emotional and logistical challenges, leading to increased stress, anxiety, and depression, highlighting a need for supportive technologies.", "method": "The authors developed a chatbot named Carey, conducting semi-structured interviews with 16 family caregivers after scenario-driven interactions to gather insights on their experiences and needs.", "result": "The analysis revealed six key themes related to caregiver needs: on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy, along with tensions in their concerns and desires.", "conclusion": "The study provides insights on caregiver needs and chatbot capabilities, offering design recommendations for creating more effective AI systems that better support caregivers' mental health.", "key_contributions": ["Development of Carey, a supportive AI chatbot for caregivers", "Identification of six themes in caregiver needs and expectations", "Design recommendations for caregiver-centered AI systems"], "limitations": "", "keywords": ["Alzheimer's Disease", "AI chatbot", "caregiver support", "mental health", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15001", "pdf": "https://arxiv.org/pdf/2506.15001.pdf", "abs": "https://arxiv.org/abs/2506.15001", "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings", "authors": ["Ignacio Sastre", "Aiala RosÃ¡"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025", "summary": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.", "AI": {"tldr": "The paper discusses generating reversible sentence embeddings allowing LLMs to reconstruct original text without modifying their weights.", "motivation": "To explore the capability of LLMs in reconstructing original text using reversible sentence embeddings for potential applications.", "method": "Introducing a special memory token whose embedding is specifically optimized through training on a fixed sequence, allowing the model to reconstruct the original text.", "result": "Evaluation across English and Spanish datasets showed that Llama 3.1 8B successfully reconstructs sequences of up to approximately 240 tokens across various model scales.", "conclusion": "The findings suggest significant implications for memory-based retrieval, compression, and controlled text generation within LLMs.", "key_contributions": ["Introduction of reversible sentence embeddings for LLMs", "Successful reconstruction of original text from embeddings", "Demonstration of LLM capabilities in memory-based applications"], "limitations": "", "keywords": ["reversible sentence embeddings", "LLM", "text reconstruction", "memory token", "controlled generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15129", "pdf": "https://arxiv.org/pdf/2506.15129.pdf", "abs": "https://arxiv.org/abs/2506.15129", "title": "Data Verbalisation: What is Text Doing in a Data Visualisation?", "authors": ["Paul Murrell"], "categories": ["cs.HC", "stat.OT"], "comment": "43 pages (including appendix), 20 figures", "summary": "This article discusses the role that text elements play in a data\nvisualisation. We argue that there is a need for a simple, coherent explanation\nof text elements similar to the understanding that already exists for non-text\nelements like bars, points, and lines. We explore examples of how text is used\nwithin a data visualisation and use existing knowledge and assessment\ntechniques to evaluate when text is effective and when it is not. The result is\na framework that aims to be easy to understand and easy to apply in order to\nunderstand the purpose and effectiveness of the text elements in any data\nvisualisation.", "AI": {"tldr": "This paper presents a framework for understanding the role of text elements in data visualizations, aimed at enhancing their effectiveness.", "motivation": "There is a lack of coherent understanding and categorization of text elements in data visualizations, similar to which exists for non-text elements.", "method": "The authors explore examples of text usage in visualizations and employ existing knowledge and assessment techniques to evaluate text effectiveness.", "result": "The study results in a framework that is simple and easy to apply, facilitating the assessment of text elements in data visualizations.", "conclusion": "A clear framework for text elements in data visualization can improve their purpose and effectiveness, benefiting designers and users alike.", "key_contributions": ["Development of a framework for assessing text elements in visualizations", "Analysis of text effectiveness compared to non-text elements", "Guidelines for better integration of text in data visualizations"], "limitations": "", "keywords": ["data visualization", "text elements", "framework", "user assessment", "visual design"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.15030", "pdf": "https://arxiv.org/pdf/2506.15030.pdf", "abs": "https://arxiv.org/abs/2506.15030", "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "categories": ["cs.CL"], "comment": "22 pages, 2 figures, 5 tables", "summary": "Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.", "AI": {"tldr": "This paper applies NLP techniques to identify social isolation and loneliness in suicide narratives, revealing significant predictors and improving prevention efforts.", "motivation": "Social isolation and loneliness are linked to increased suicide rates, yet they are not currently monitored in official reporting systems. This study aims to fill this gap by using NLP to analyze law enforcement and coroner narratives.", "method": "The research utilizes topic modeling for lexicon development and supervised learning classifiers to create high-quality classifiers, achieving an average F1 score of .86 and accuracy of .82.", "result": "From analyzing over 300,000 suicides from 2002 to 2020, the study identified 1,198 cases mentioning chronic social isolation, with specific demographic groups, such as men and gay individuals, showing higher odds of classification related to social isolation.", "conclusion": "The methods developed in this study can enhance surveillance and preventative strategies for social isolation and loneliness, especially in the context of suicide prevention in the U.S.", "key_contributions": ["Application of NLP techniques to suicide narratives for identifying social isolation", "Development of high-quality classifiers for chronic social isolation", "Identification of significant predictors for social isolation related to demographic factors."], "limitations": "", "keywords": ["social isolation", "loneliness", "NLP", "suicide", "classification"], "importance_score": 8, "read_time_minutes": 22}}
{"id": "2506.15189", "pdf": "https://arxiv.org/pdf/2506.15189.pdf", "abs": "https://arxiv.org/abs/2506.15189", "title": "Accessible Gesture-Driven Augmented Reality Interaction System", "authors": ["Yikan Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction", "AI": {"tldr": "This paper presents a gesture-based interaction system for augmented reality (AR) that enhances accessibility for users with motor impairments by leveraging deep learning techniques for gesture recognition.", "motivation": "To improve the accessibility of augmented reality environments for users with motor impairments, who struggle with traditional input methods.", "method": "The authors use deep learning models, including vision transformers, temporal convolutional networks, and graph attention networks, to process gestures from wearable sensors and cameras. Federated learning is employed for privacy-preserving model training, while reinforcement learning optimizes interface elements.", "result": "The proposed system shows a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems.", "conclusion": "The study concludes that the gesture-based interaction system significantly enhances AR accessibility and scalability for users with motor impairments.", "key_contributions": ["Gesture-based interaction system for AR", "Application of advanced deep learning methods for gesture recognition", "Federated learning for privacy-preserving training"], "limitations": "", "keywords": ["Deep learning", "Federated learning", "Gesture recognition", "Augmented reality", "Accessibility", "Human-computer interaction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.15068", "pdf": "https://arxiv.org/pdf/2506.15068.pdf", "abs": "https://arxiv.org/abs/2506.15068", "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation", "authors": ["Zongxia Li", "Yapei Chang", "Yuhang Zhou", "Xiyang Wu", "Zichao Liang", "Yoo Yeon Sung", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.", "AI": {"tldr": "PrefBERT is a scoring model designed to evaluate open-ended long-form generation, improving semantic feedback compared to traditional metrics.", "motivation": "Existing evaluation methods struggle to effectively separate good from bad outputs in long-form generation due to biases and lack of coherence and relevance assessment.", "method": "PrefBERT is trained on diverse response evaluation datasets and uses distinct rewards for guiding long-form generation output quality in generative reinforcement policy optimization (GRPO).", "result": "PrefBERT offers improved semantic reward feedback and aligns better with human preferences compared to traditional metrics like ROUGE-L and BERTScore.", "conclusion": "The use of PrefBERT in training policy models leads to responses that are more aligned with human quality assessments than those trained with conventional metrics.", "key_contributions": ["Introduction of PrefBERT for evaluating open-ended long-form outputs", "Demonstrates improved alignment with human preferences over traditional metrics", "Code availability for further research and application"], "limitations": "", "keywords": ["open-ended generation", "evaluation metrics", "PrefBERT", "long-form generation", "human preferences"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15293", "pdf": "https://arxiv.org/pdf/2506.15293.pdf", "abs": "https://arxiv.org/abs/2506.15293", "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces", "authors": ["Francesco Chiossi", "Julian Rasch", "Robin Welsch", "Albrecht Schmidt", "Florian Michahelles"], "categories": ["cs.HC", "cs.RO"], "comment": "9 pages", "summary": "As robots enter collaborative workspaces, ensuring mutual understanding\nbetween human workers and robotic systems becomes a prerequisite for trust,\nsafety, and efficiency. In this position paper, we draw on the cooperation\nscenario of the AIMotive project in which a human and a cobot jointly perform\nassembly tasks to argue for a structured approach to intent communication.\nBuilding on the Situation Awareness-based Agent Transparency (SAT) framework\nand the notion of task abstraction levels, we propose a multidimensional design\nspace that maps intent content (SAT1, SAT3), planning horizon (operational to\nstrategic), and modality (visual, auditory, haptic). We illustrate how this\nspace can guide the design of multimodal communication strategies tailored to\ndynamic collaborative work contexts. With this paper, we lay the conceptual\nfoundation for a future design toolkit aimed at supporting transparent\nhuman-robot interaction in the workplace. We highlight key open questions and\ndesign challenges, and propose a shared agenda for multimodal, adaptive, and\ntrustworthy robotic collaboration in hybrid work environments.", "AI": {"tldr": "This position paper discusses the need for effective intent communication between humans and robots in collaborative workspaces, proposing a multidimensional design space for multimodal communication strategies.", "motivation": "Ensuring mutual understanding between human workers and robots is essential for trust, safety, and efficiency in collaborative environments.", "method": "The paper introduces a structured approach based on the SAT framework and task abstraction levels to create a design space mapping intent content, planning horizon, and communication modality.", "result": "A conceptual design space is proposed to guide the development of multimodal communication strategies that enhance human-robot interaction in dynamic work contexts.", "conclusion": "The study lays the groundwork for a future design toolkit to support transparent and trustworthy collaboration between humans and robots, addressing design challenges and open questions.", "key_contributions": ["Structured approach to intent communication using SAT framework.", "Multidimensional design space for task and communication strategies.", "Foundation for a design toolkit for human-robot interactions."], "limitations": "", "keywords": ["human-robot interaction", "intent communication", "collaborative workspaces", "multimodal communication", "task abstraction"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2506.15076", "pdf": "https://arxiv.org/pdf/2506.15076.pdf", "abs": "https://arxiv.org/abs/2506.15076", "title": "Learning-Time Encoding Shapes Unlearning in LLMs", "authors": ["Ruihan Wu", "Konstantin Garov", "Kamalika Chaudhuri"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.", "AI": {"tldr": "Investigates the impact of learning-time choices in knowledge encoding on the effectiveness of unlearning in large language models (LLMs).", "motivation": "With the deployment of LLMs, the ability to unlearn knowledge has become critical due to privacy concerns and the need to correct outdated information.", "method": "Empirical investigation of knowledge encoding methods and their impact on unlearning performance through experiments.", "result": "Learning with paraphrased descriptions enhances unlearning performance, while unlearning specific knowledge from chunks of text proves challenging.", "conclusion": "Knowledge encoding during the learning phase significantly influences the post-hoc unlearning process in LLMs.", "key_contributions": ["Demonstrates the effect of paraphrased descriptions on unlearning efficiency.", "Identifies challenges in unlearning specific knowledge from larger text blocks.", "Highlights the importance of learning-time knowledge encoding."], "limitations": "Focuses on empirical results without a theoretical framework for unlearning mechanisms.", "keywords": ["large language models", "unlearning", "knowledge encoding", "empirical investigation", "factual knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15294", "pdf": "https://arxiv.org/pdf/2506.15294.pdf", "abs": "https://arxiv.org/abs/2506.15294", "title": "UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments", "authors": ["Jonas Lau", "Annie Tran"], "categories": ["cs.HC"], "comment": null, "summary": "This paper discusses a popular UX research activity, feature prioritization,\nusing the User Experience Research Point of View (UXR PoV) Playbook framework.\nWe describe an application of multinomial logistic regression, frequently\nmarketed as MaxDiff, for prioritizing product features in consumer product\ndevelopment. It addresses challenges of traditional surveying techniques. We\npropose a solution using MaxDiff to generate a reliable preference list with a\nreasonable sample size. We also adapt the MaxDiff method to reduce the number\nof survey responses in half, making it less tedious from the survey takers'\nperspective. We present a case study using the adapted MaxDiff method for\ntablet feature prioritization research involving users with disabilities.", "AI": {"tldr": "This paper presents the adapted MaxDiff method for feature prioritization in UX research, particularly for users with disabilities, improving survey efficiency and reliability.", "motivation": "To address the challenges of traditional surveying techniques in UX research and improve feature prioritization for consumer product development, especially for users with disabilities.", "method": "The paper employs multinomial logistic regression, marketed as MaxDiff, to prioritize product features, with an adaptation to reduce the survey response requirement by half.", "result": "The adapted MaxDiff method generates a reliable preference list for product features while reducing the burden on survey takers, demonstrated through a case study focused on tablets for users with disabilities.", "conclusion": "The study highlights the effectiveness of the adapted MaxDiff method in improving the user experience of surveys and ensures better feature prioritization for accessible design.", "key_contributions": ["Application of MaxDiff method in UX research", "Adaptation of the MaxDiff technique to reduce survey responses", "Case study focusing on users with disabilities for tablet feature prioritization"], "limitations": "", "keywords": ["UX research", "feature prioritization", "MaxDiff", "users with disabilities", "survey techniques"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.15081", "pdf": "https://arxiv.org/pdf/2506.15081.pdf", "abs": "https://arxiv.org/abs/2506.15081", "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification", "authors": ["Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025(main conference)", "summary": "Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.", "AI": {"tldr": "This paper presents a Discourse-aware Clarification Module (DCM) that enhances dialogue discourse parsing by resolving ambiguities in utterance relations through clarification type and discourse goal reasoning, along with a Contribution-aware Preference Optimization (CPO) mechanism.", "motivation": "Dialogue discourse parsing is challenged by ambiguities introduced by linguistic features such as omission and idioms, which can obscure intended discourse relations between utterances.", "method": "The proposed DCM employs two reasoning processes: clarification type reasoning to analyze linguistic features and discourse goal reasoning to distinguish intended relations from ambiguities. CPO is introduced to optimize the DCM by assessing the contributions of clarifications and providing feedback.", "result": "Experiments on the STAC and Molweni datasets show that the proposed approach effectively resolves ambiguities and significantly outperforms state-of-the-art baselines in dialogue discourse parsing.", "conclusion": "The implementation of DCM and CPO demonstrates a robust improvement in the parser's performance, evidencing the effectiveness of discourse-aware techniques in addressing dialogue ambiguities.", "key_contributions": ["Introduction of the Discourse-aware Clarification Module (DCM)", "Development of Contribution-aware Preference Optimization (CPO)", "Demonstration of significant performance improvement over state-of-the-art baselines"], "limitations": "", "keywords": ["Dialogue Discourse Parsing", "Clarification Module", "Discourse Relations", "Machine Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.15314", "pdf": "https://arxiv.org/pdf/2506.15314.pdf", "abs": "https://arxiv.org/abs/2506.15314", "title": "Case Study for Developing a UXR Point of View for FinOps Product Innovation", "authors": ["Jason Dong", "Anna Wu"], "categories": ["cs.HC"], "comment": null, "summary": "In the dynamic landscape of Cloud financial management, we are sharing a case\nstudy exploring the development of a User Experience Research (UXR) Point of\nView (PoV) to drive FinOps product innovation. We demonstrate how qualitative\nand quantitative research methods working together to navigate the challenges\nof understanding customer needs, aligning cross-functional teams, and\nprioritizing limited resources. Through a multi-phased research approach, the\nresearch team identifies opportunities, quantifies pain points, and segments\ndiverse customer cohorts. This culminated in a UXR PoV that informed the\ncreation of a differentiated product strategy, a 'one-stop shop' dashboard\nempowering FinOps practitioners with actionable insights and tools. This case\nstudy highlights the power of mixed-methods research in uncovering actionable\ninsights that drive impactful product innovation.", "AI": {"tldr": "This case study discusses developing a User Experience Research (UXR) Point of View (PoV) for driving FinOps product innovation through mixed-methods research.", "motivation": "To address challenges in understanding customer needs and aligning teams in Cloud financial management.", "method": "A multi-phased research approach combining qualitative and quantitative methods to identify customer needs and pain points.", "result": "The research led to the creation of a differentiated product strategy and a comprehensive dashboard for FinOps practitioners.", "conclusion": "Mixed-methods research can significantly drive product innovation by uncovering actionable insights.", "key_contributions": ["Development of a UXR PoV for FinOps product innovation", "Implementation of a mixed-methods research approach", "Creation of a differentiated product strategy and dashboard"], "limitations": "", "keywords": ["User Experience Research", "Cloud Financial Management", "Product Innovation", "Mixed-Methods Research", "FinOps"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.15118", "pdf": "https://arxiv.org/pdf/2506.15118.pdf", "abs": "https://arxiv.org/abs/2506.15118", "title": "CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records", "authors": ["Junke Wang", "Hongshun Ling", "Li Zhang", "Longqian Zhang", "Fang Wang", "Yuan Gao", "Zhi Li"], "categories": ["cs.CL"], "comment": "20 pages,5 figures", "summary": "Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.", "AI": {"tldr": "The CKD-EHR framework enhances disease prediction in Electronic Health Records by embedding medical knowledge in language models, achieving better accuracy and efficiency in clinical settings.", "motivation": "To improve disease prediction models in EHRs which struggle with medical knowledge representation and clinical deployment efficiency.", "method": "The study employs the CKD-EHR framework which involves fine-tuning the Qwen2.5-7B model with medical knowledge data, using a multi-granularity attention distillation mechanism to generate soft labels for a lightweight BERT student model.", "result": "CKD-EHR outperforms baseline models with a 9% increase in diagnostic accuracy, a 27% improvement in F1-score, and a 22.2 times speedup in inference over the MIMIC-III dataset.", "conclusion": "CKD-EHR provides significant improvements in diagnosis accuracy and resource utilization, representing a practical solution for clinical settings.", "key_contributions": ["Proposes the CKD-EHR framework for disease risk prediction", "Uses knowledge distillation from a large model to a lightweight BERT model", "Demonstrates significant improvements in accuracy and efficiency on clinical data."], "limitations": "", "keywords": ["Electronic Health Records", "Disease Prediction", "Knowledge Distillation", "Clinical Efficiency", "Machine Learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.15325", "pdf": "https://arxiv.org/pdf/2506.15325.pdf", "abs": "https://arxiv.org/abs/2506.15325", "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook", "authors": ["Festus Adedoyin", "Huseyin Dogan"], "categories": ["cs.HC"], "comment": null, "summary": "Advancements in Artificial Intelligence (AI) have significantly transformed\nthe financial industry, enabling the development of more personalised and\nadaptable financial products and services. This research paper explores various\ninstances where Human-Centred AI (HCAI) has facilitated these advancements,\ndrawing from contemporary studies and industry progress. The paper examines how\nthe application of HCAI-powered data analytics, machine learning, and natural\nlanguage processing enables financial institutions to gain a deeper\nunderstanding of their customers' unique needs, preferences, and behavioural\npatterns. This, in turn, allows for the creation of tailored financial\nsolutions that address individual consumer requirements, ultimately enhancing\noverall user experience and satisfaction. Additionally, the study highlights\nthe integration of AI-powered robo-advisory services, which offer customised\ninvestment recommendations and portfolio management tailored to diverse risk\nprofiles and investment goals. Moreover, the paper underscores the role of AI\nin strengthening fraud detection, risk assessment, and regulatory compliance,\nleading to a more secure and adaptable financial landscape. The findings of\nthis research demonstrate the substantial impact of Human-Centred AI on the\nfinancial industry, offering a strategic framework for financial institutions\nto leverage these technologies. By incorporating a User Experience Research\n(UXR) Point of View (PoV), financial institutions can ensure that AI-driven\nsolutions align with user needs and business objectives.", "AI": {"tldr": "This paper explores the transformative impact of Human-Centred AI in the financial industry, focusing on personalized financial services, enhanced user experience, and AI-powered solutions for fraud detection and portfolio management.", "motivation": "To examine how Human-Centred AI can enhance the financial industry by creating tailored financial products and improving user experience.", "method": "The research draws upon contemporary studies and industry progress in using HCAI, machine learning, and natural language processing in finance.", "result": "The study finds that HCAI empowers financial institutions to understand customer needs better, leading to personalized financial solutions and improved user satisfaction.", "conclusion": "HCAI significantly transforms the financial landscape by tailoring services to user needs and enhancing security through AI-driven solutions.", "key_contributions": ["Identification of HCAI applications in finance", "Framework for integrating UX research into AI solutions", "Insights on AI's role in fraud detection and risk management"], "limitations": "", "keywords": ["Human-Centred AI", "financial services", "user experience", "machine learning", "fraud detection"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.15131", "pdf": "https://arxiv.org/pdf/2506.15131.pdf", "abs": "https://arxiv.org/abs/2506.15131", "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "authors": ["Jing Yang Lee", "Kong-Aik Lee", "Woon-Seng Gan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.", "AI": {"tldr": "This paper addresses the one-to-many property of open-domain dialogue by proposing a framework that includes Multi-Response Generation and Preference-based Selection, leveraging a new dialogue corpus for enhanced response diversity.", "motivation": "To enhance response diversity in open-domain dialogue systems, as most LLM-based agents do not explicitly model the one-to-many property of dialogues.", "method": "The paper decomposes the generation of dialogues into two tasks: generating multiple diverse responses and selecting one based on preferences. A new corpus, o2mDial, is introduced to facilitate these tasks.", "result": "The framework improves response diversity and quality, achieving up to a 90% increase in response quality in comparison to smaller LLMs, aligning their performance closer to larger models.", "conclusion": "The proposed two-stage framework effectively models the one-to-many property and improves the generation quality of open-domain dialogue systems.", "key_contributions": ["Introduction of o2mDial corpus for Multi-Response Generation", "Proposed in-context learning and instruction-tuning strategies", "Development of new evaluation metrics for response generation"], "limitations": "", "keywords": ["Open-domain Dialogue", "Multi-Response Generation", "Preference-based Selection"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.15332", "pdf": "https://arxiv.org/pdf/2506.15332.pdf", "abs": "https://arxiv.org/abs/2506.15332", "title": "Building Blocks of a User Experience Research Point of View", "authors": ["Patricia Diaz"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents three User Experience Research (UXR) perspectives based\non data, evidence and insights - known as Point of View (POV) - showcasing how\nthe strategies and methods of building a POV work in an enterprise setting. The\nPOV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in\nvideos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the\nlearner as it is the loop that requires the least effort for the highest\nimpact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at\nthe intersection of emergent technical capabilities that unlock novel\napproaches to critical user needs while addressing business strategic\npriorities (2019). They all seemed far-fetched and went against common\npractice. All were adopted and had long-lasting impact.", "AI": {"tldr": "The paper discusses three User Experience Research perspectives utilizing AI to create impactful strategies in enterprise settings.", "motivation": "To showcase how User Experience Research can leverage AI and data to create strategies that significantly impact user needs and business priorities in enterprises.", "method": "The paper outlines three points of view developed through UX research and AI applications in enterprise environments, including the use of smart visuals, AI feedback mechanisms, and identifying opportunities at the intersection of technology and user needs.", "result": "The implementation of the proposed UXR strategies had long-lasting positive effects on user engagement and business effectiveness.", "conclusion": "Through these innovative UXR perspectives, the paper highlights the potential of integrating AI in understanding user needs and achieving business goals.", "key_contributions": ["Introduction of AI-driven user experience strategies", "Demonstration of effective integration of AI in learning tools", "Identification of key opportunity landscapes through UX research"], "limitations": "The paper does not address the scalability of these strategies in different organizational contexts.", "keywords": ["User Experience Research", "Artificial Intelligence", "Enterprise Strategy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15138", "pdf": "https://arxiv.org/pdf/2506.15138.pdf", "abs": "https://arxiv.org/abs/2506.15138", "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models", "authors": ["Gyeongje Cho", "Yeonkyoun So", "Chanwoo Park", "Sangmin Lee", "Sungmok Jung", "Jaejin Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.", "AI": {"tldr": "This paper introduces Thunder-Tok, a Korean tokenizer that reduces token fertility by 10% using a linguistically informed approach.", "motivation": "To address the issue of token fertility in language modeling without sacrificing performance, particularly for the Korean language.", "method": "A rule-based pre-tokenization method aligned with Korean linguistic structure, creating a seed vocabulary, and a branching entropy-based selection algorithm.", "result": "Thunder-Tok reduces the number of tokens by approximately 10%, resulting in a 10% increase in inference speed, without compromising performance across downstream tasks.", "conclusion": "The proposed tokenizer effectively enhances the efficiency of language models for the Korean language while preserving linguistic information.", "key_contributions": ["Introduction of Thunder-Tok, a new Korean tokenizer", "Reduction of token fertility by 10%", "Linguistically informed pre-tokenization method"], "limitations": "", "keywords": ["tokenization", "Korean language", "language models", "linguistic structure", "efficiency"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.15468", "pdf": "https://arxiv.org/pdf/2506.15468.pdf", "abs": "https://arxiv.org/abs/2506.15468", "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI", "authors": ["Ryota Okumura", "Tadahiro Taniguchi", "Akira Taniguchi", "Yoshinobu Hagiwara"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.", "AI": {"tldr": "Proposes co-creative learning where humans and AI collaboratively construct shared representations, tested via a human-AI interaction model.", "motivation": "Addresses challenges of integrating information from different modalities in human-AI interactions.", "method": "An online experiment involving participants playing a joint attention naming game with various AI agents based on the Metropolis-Hastings naming game.", "result": "Human-AI pairs with an MH-based agent improved categorization accuracy and converged towards a shared sign system.", "conclusion": "Empirical evidence for co-creative learning in human-AI pairs suggests potential for symbiotic AI systems that learn with humans.", "key_contributions": ["Introduction of co-creative learning paradigm", "Empirical evidence in human-AI dyads via MHNG", "Demonstration of improved categorization through interaction"], "limitations": "", "keywords": ["co-creative learning", "human-AI interaction", "Metropolis-Hastings naming game", "shared representations", "symbiotic AI"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.15156", "pdf": "https://arxiv.org/pdf/2506.15156.pdf", "abs": "https://arxiv.org/abs/2506.15156", "title": "Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View", "authors": ["Muhammad Cendekia Airlangga", "Hilal AlQuabeh", "Munachiso S Nwadike", "Kentaro Inui"], "categories": ["cs.CL"], "comment": null, "summary": "We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.", "AI": {"tldr": "This study investigates memory in state-space language models through primacy and recency effects, revealing mechanisms that influence information retention and forgetting.", "motivation": "To understand how information is retained and forgotten in state-space language models and to uncover the underlying mechanisms of memory performance.", "method": "Structured recall tasks were applied to the Mamba architecture to assess accuracy profiles and identify memory mechanisms through targeted ablations and input perturbations.", "result": "A U-shaped accuracy profile indicates strong performance at the beginning and end of input sequences, with identified mechanisms of long-term memory being linked to selective state space channels, short-term memory being influenced by recent inputs, and dynamic modulation by semantic regularity.", "conclusion": "Memory in state-space models is nuanced, with distinct mechanisms influencing retention and forgetting based on input structure and temporal factors.", "key_contributions": ["Identified mechanisms of long-term and short-term memory in language models.", "Demonstrated the impact of semantic regularity on memory allocation.", "Validated findings using large-scale language models with targeted experiments."], "limitations": "Limited to the specific architecture of the Mamba models and may not generalize to all state-space models.", "keywords": ["state-space models", "memory retention", "primacy effects", "recency effects", "semantic regularity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.15497", "pdf": "https://arxiv.org/pdf/2506.15497.pdf", "abs": "https://arxiv.org/abs/2506.15497", "title": "Foundation of Affective Computing and Interaction", "authors": ["Changzeng Fu"], "categories": ["cs.HC"], "comment": null, "summary": "This book provides a comprehensive exploration of affective computing and\nhuman-computer interaction technologies. It begins with the historical\ndevelopment and basic concepts of human-computer interaction, delving into the\ntechnical frameworks and practical applications of emotional computing, visual\ninteraction, voice interaction, brain-computer interfaces, physiological\nelectrical signal analysis, and social robotics. The book covers a wide range\nof topics, including the psychological and neuroscience foundations of emotion,\nmultimodal emotion recognition, emotional expression mechanisms, and the\nprinciples of brain-computer interfaces.\n  Key technologies such as affective computing based on discrete emotion theory\nand dimensional models, visual perception principles, speech recognition and\nsynthesis, EEG signal acquisition and processing, and multimodal emotion\nrecognition are explained in detail. This book also addresses the technical\nchallenges in the field, including multimodal data fusion, privacy and\nsecurity, and ethical considerations in human-machine relationships. It\ndiscusses the applications of these technologies across various domains such as\neducation, healthcare, entertainment, and intelligent assistance.\n  Looking to the future, the book anticipates trends such as the deep\nintegration of artificial intelligence with emotion recognition, the\nadvancement of multimodal interaction technologies, and the development of more\npersonalized and adaptive emotion recognition systems. It emphasizes the\nimportance of balancing technological innovation with ethical considerations to\nensure the responsible development and application of affective computing\ntechnologies.", "AI": {"tldr": "A comprehensive exploration of affective computing and human-computer interaction technologies, addressing emotional computing, interaction modalities, and ethical considerations.", "motivation": "To explore the intersection of emotion and technology in human-computer interaction (HCI) and to identify future trends and considerations.", "method": "The book covers historical developments, technical frameworks, and practical applications of various interaction technologies, including emotional computing, multimodal recognition, and brain-computer interfaces.", "result": "The book provides detailed explanations of key technologies and their applications across fields such as education, healthcare, and entertainment, while addressing challenges in multimodal data fusion and ethical implications.", "conclusion": "The future of affective computing is anticipated to involve deeper AI integration, advancements in multimodal interaction technologies, and a focus on ethical considerations in development.", "key_contributions": ["Comprehensive overview of affective computing and HCI technologies", "Detailed exploration of emotional recognition systems", "Discussion of ethical implications in human-machine interactions"], "limitations": "", "keywords": ["affective computing", "human-computer interaction", "multimodal emotion recognition", "brain-computer interfaces", "ethical considerations"], "importance_score": 9, "read_time_minutes": 60}}
{"id": "2506.15208", "pdf": "https://arxiv.org/pdf/2506.15208.pdf", "abs": "https://arxiv.org/abs/2506.15208", "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals", "authors": ["Andrea Cadeddu", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Enrico Motta", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to IEEE Access", "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).", "AI": {"tldr": "This study examines the use of LLMs for text classification related to the UN's Sustainable Development Goals, comparing performance between various models and adaptation techniques.", "motivation": "The paper addresses the challenge of tracking progress toward the 17 Sustainable Development Goals due to the complex data involved, highlighting the need for effective text classification models.", "method": "The study analyzes multiple proprietary and open-source large language models (LLMs) for a single-label, multi-class text classification task focused on Sustainable Development Goals and evaluates in-context learning approaches such as Zero-Shot, Few-Shot Learning, and Fine-Tuning.", "result": "The findings suggest that smaller LLMs, when optimized with prompt engineering, can achieve performance comparable to larger models like GPT.", "conclusion": "Optimizing smaller models through prompt engineering is effective for text classification tasks, potentially providing cost-effective solutions for analyzing data related to the Sustainable Development Goals.", "key_contributions": ["Comparison of proprietary and open-source LLMs for SDG-focused text classification", "Evaluation of task adaptation techniques like Zero-Shot and Few-Shot Learning", "Demonstration that smaller models can match larger models through prompt optimization"], "limitations": "", "keywords": ["Sustainable Development Goals", "text classification", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15512", "pdf": "https://arxiv.org/pdf/2506.15512.pdf", "abs": "https://arxiv.org/abs/2506.15512", "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach", "authors": ["Wenqi Guan", "Yang Fang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.", "AI": {"tldr": "This paper proposes a novel approach to enhancing remote learning retrieval using GPT-based models within the LangChain framework.", "motivation": "Current retrieval of remote learning resources lacks sufficient contextual depth for complex student queries, impacting user satisfaction and learning outcomes.", "method": "The approach integrates GPT-based models within the LangChain framework, utilizing Chain of Thought (CoT) reasoning and prompt engineering to improve retrieval precision and relevance.", "result": "The proposed system shows improvements in user satisfaction and learning outcomes when compared to standard LLMs.", "conclusion": "Integrating GPT-based models within the LangChain framework significantly enhances the retrieval of remote learning resources, providing contextually enriched explanations that meet students' needs.", "key_contributions": ["Novel integration of GPT-based models in the LangChain framework for remote learning", "Use of CoT reasoning and prompt engineering to improve context and precision", "Demonstrated improvements in user satisfaction and learning outcomes."], "limitations": "", "keywords": ["Large Language Models", "remote learning", "LangChain", "CoT reasoning", "prompt engineering"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.15211", "pdf": "https://arxiv.org/pdf/2506.15211.pdf", "abs": "https://arxiv.org/abs/2506.15211", "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "authors": ["Feng He", "Zijun Chen", "Xinnian Liang", "Tingting Ma", "Yunqi Qiu", "Shuangzhi Wu", "Junchi Yan"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.", "AI": {"tldr": "The paper introduces ProtoReasoning, a framework that enhances the reasoning capabilities of Large Reasoning Models through prototypical representations, demonstrating improved performance across various reasoning tasks.", "motivation": "To understand and enhance cross-domain generalization in Large Reasoning Models, focusing on the role of shared abstract reasoning prototypes.", "method": "ProtoReasoning utilizes an automated prototype construction pipeline, a comprehensive verification system with Prolog and PDDL, and ensures scalability for problem synthesis within prototype space.", "result": "ProtoReasoning demonstrates a 4.7% improvement in logical reasoning, 6.3% in planning tasks, and 4.0% in general reasoning, showcasing the effectiveness of using prototypes.", "conclusion": "The study validates that reasoning prototypes are foundational for enhancing generalization in LLMs, outperforming traditional training on natural language representations.", "key_contributions": ["Introduction of ProtoReasoning framework", "Automated prototype construction and verification systems", "Demonstrated improvements in various reasoning tasks"], "limitations": "", "keywords": ["Large Reasoning Models", "ProtoReasoning", "Cross-domain generalization", "Prototypical representations", "Logical reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15525", "pdf": "https://arxiv.org/pdf/2506.15525.pdf", "abs": "https://arxiv.org/abs/2506.15525", "title": "\"How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "categories": ["cs.HC"], "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in\n  proceedings of 24th annual ACM Interaction Design and Children Conference\n  (IDC 2025)", "summary": "As generative AI (GenAI) emerges as a transformative force, clear\nunderstanding of high school students' perspectives is essential for GenAI's\nmeaningful integration in high school environments. In this work, we draw\ninsights from a participatory design workshop where we engaged 17 high school\nstudents -- a group rarely involved in prior research in this area -- through\nthe design of novel GenAI tools and school policies addressing their key\nconcerns. Students identified challenges and developed solutions outlining\ntheir ideal features in GenAI tools, appropriate school use, and regulations.\nThese centered around the problem spaces of combating bias & misinformation,\ntackling crime & plagiarism, preventing over-reliance on AI, and handling false\naccusations of academic dishonesty. Building on our participants'\nunderrepresented perspectives, we propose new guidelines targeted at\neducational technology designers for development of GenAI technologies in high\nschools. We also argue for further incorporation of student voices in\ndevelopment of AI policies in their schools.", "AI": {"tldr": "This paper investigates high school students' perspectives on generative AI integration in education through a participatory design workshop, resulting in guidelines for developing AI tools that address their concerns.", "motivation": "To integrate generative AI meaningfully in high school environments by understanding students' perspectives, a group often excluded from prior research.", "method": "Conducted a participatory design workshop with 17 high school students to gather insights on their perspectives and design preferences regarding generative AI tools.", "result": "Students identified challenges related to bias, misinformation, crime, plagiarism, and over-reliance on AI, and proposed solutions outlining their ideal features for GenAI tools and appropriate regulations.", "conclusion": "New guidelines are proposed for educational technology designers to develop generative AI technologies in high schools, emphasizing the need for student voices in AI policy development.", "key_contributions": ["Insights from high school students on GenAI tool design and policies.", "Proposed guidelines for educational technology designers.", "Emphasis on student involvement in AI policy creation."], "limitations": "", "keywords": ["generative AI", "high school", "participatory design", "educational technology", "AI policy"], "importance_score": 4, "read_time_minutes": 17}}
{"id": "2506.15215", "pdf": "https://arxiv.org/pdf/2506.15215.pdf", "abs": "https://arxiv.org/abs/2506.15215", "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "authors": ["Yongqi Fan", "Yating Wang", "Guandong Wang", "Jie Zhai", "Jingping Liu", "Qi Ye", "Tong Ruan"], "categories": ["cs.CL"], "comment": null, "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.", "AI": {"tldr": "MinosEval is a novel evaluation method for open-ended question answering that distinguishes between factoid and non-factoid questions, using adaptive scoring for fact-based inquiries and instance-aware ranking for others, outperforming traditional methods in interpretability and alignment with human annotations.", "motivation": "To improve the evaluation of open-ended question answering systems by addressing the limitations of existing metrics and methods, particularly in capturing semantic similarities and providing interpretable results.", "method": "MinosEval distinguishes between factoid and non-factoid questions and utilizes adaptive scoring for factoid questions and instance-aware listwise ranking for non-factoid questions to evaluate candidate answers.", "result": "Experiments demonstrate that MinosEval aligns better with human annotations and provides more interpretable evaluation results compared to traditional metrics and existing LLM-based approaches.", "conclusion": "MinosEval offers a refined and interpretable method for evaluating open-ended question answering, improving the understanding of LLM performance on complex questions.", "key_contributions": ["Introduction of MinosEval as a tailored evaluation method for open-ended QA.", "Use of distinct scoring strategies for factoid and non-factoid questions.", "Improvement in alignment with human annotations in evaluation results."], "limitations": "The proposed method may be limited by the datasets used for training and evaluation, and its effectiveness may vary across different QA contexts.", "keywords": ["Open-ended Question Answering", "Evaluation Metrics", "Large Language Models", "Semantic Similarity", "Human Annotation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.15239", "pdf": "https://arxiv.org/pdf/2506.15239.pdf", "abs": "https://arxiv.org/abs/2506.15239", "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.", "AI": {"tldr": "This paper evaluates the performance of language technologies on Basque and Spanish, highlighting challenges due to linguistic variation with a novel dataset.", "motivation": "To assess how well existing language technologies can handle linguistic variations in Basque and Spanish.", "method": "The study employs Natural Language Inference (NLI) as a pivot task, supported by a newly curated parallel dataset focusing on variants of Basque and Spanish, and conducts experiments with both encoder-only and decoder-based LLMs.", "result": "The analysis reveals a significant performance drop in handling linguistic variation, particularly in Basque, with encoder-only models struggling more with Western Basque.", "conclusion": "Linguistic variation presents notable challenges for language technologies, underscoring the need for further refinement of models to better handle dialectal differences.", "key_contributions": ["Introduction of a novel, manually-curated parallel dataset for Basque and Spanish variants.", "Empirical evidence of performance drops in LLMs with linguistic variation, especially in peripheral dialects.", "Public availability of data and code for further research."], "limitations": "Focuses mainly on Basque and Spanish, limiting generalizability to other languages or dialects.", "keywords": ["Natural Language Processing", "Basque", "Spanish", "Language Models", "Linguistic Variation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.15241", "pdf": "https://arxiv.org/pdf/2506.15241.pdf", "abs": "https://arxiv.org/abs/2506.15241", "title": "Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs", "authors": ["Yang Fan", "Zhang Qi", "Xing Wenqian", "Liu Chang", "Liu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.", "AI": {"tldr": "The paper proposes the Graph RAG framework for enhancing large language models in historical text analysis, presenting a dataset with minimal manual annotation and evaluating its effectiveness in relation extraction.", "motivation": "To address domain knowledge gaps in large language models for computational humanities, specifically in historical text analysis.", "method": "The framework combines chain-of-thought prompting, self-instruction generation, and process supervision to create a character relationship dataset with minimal manual effort, integrating knowledge graphs with retrieval-augmented generation.", "result": "The domain-specific model Xunzi-Qwen1.5-14B achieved an F1 score of 0.68 in relation extraction. The DeepSeek model with GraphRAG improved F1 score by 11%, effectively reducing hallucination issues.", "conclusion": "The Graph RAG framework presents a low-resource solution for classical text knowledge extraction, enhancing historical knowledge services and humanities research.", "key_contributions": ["Introduction of the Graph RAG framework for historical text analysis.", "Creation of The First Four Histories character relationship dataset with minimal manual annotation.", "Demonstration of improved model performance in relation extraction using knowledge graphs."], "limitations": "", "keywords": ["large language models", "historical text analysis", "retrieval-augmented generation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.15246", "pdf": "https://arxiv.org/pdf/2506.15246.pdf", "abs": "https://arxiv.org/abs/2506.15246", "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge", "authors": ["Juli Bakagianni", "John Pavlopoulos", "Aristidis Likas"], "categories": ["cs.CL"], "comment": null, "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.", "AI": {"tldr": "TopClustRAG is a retrieval-augmented generation system designed for end-to-end question answering, utilizing a hybrid retrieval strategy and clustering to enhance answer quality.", "motivation": "To improve answer diversity, relevance, and faithfulness in retrieval-augmented generation systems for large-scale question answering.", "method": "The system employs a hybrid retrieval strategy using sparse and dense indices, followed by K-Means clustering to group semantically similar passages, which are then used to create cluster-specific prompts for a large language model.", "result": "On the FineWeb Sample-10BT dataset, TopClustRAG achieved 2nd place in faithfulness and 7th in correctness on the leaderboard, indicating effective performance.", "conclusion": "The use of clustering-based context filtering and prompt aggregation enhances the overall performance of large-scale RAG systems.", "key_contributions": ["Introduction of TopClustRAG for enhanced question answering", "Hybrid retrieval strategy combining sparse and dense indices", "Implementation of K-Means clustering for passage organization"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Question Answering", "K-Means Clustering"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.15266", "pdf": "https://arxiv.org/pdf/2506.15266.pdf", "abs": "https://arxiv.org/abs/2506.15266", "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments", "authors": ["Sungen Hahm", "Heejin Kim", "Gyuseong Lee", "Hyunji Park", "Jaejin Lee"], "categories": ["cs.CL"], "comment": null, "summary": "To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.", "AI": {"tldr": "This paper presents Thunder-DeID, a de-identification framework designed for the South Korean judiciary to enhance the protection of personal data in court judgments.", "motivation": "The South Korean judiciary's de-identification process is insufficient for handling court judgments at scale, requiring a solution that meets legal standards and effectively protects personal data.", "method": "The authors developed Thunder-DeID, which includes the creation of a Korean legal dataset with annotated judgments, a systematic categorization of Personally Identifiable Information (PII), and an end-to-end deep neural network (DNN)-based de-identification pipeline.", "result": "The experiments showed that Thunder-DeID achieves state-of-the-art performance in de-identifying court judgments.", "conclusion": "The proposed framework addresses legal and technical challenges in the de-identification process, proving effective in optimizing data protection for court cases.", "key_contributions": ["First Korean legal dataset with annotated court judgments", "Systematic categorization of PII", "End-to-end DNN-based de-identification pipeline"], "limitations": "The study may have limitations related to the generalizability of the model to other jurisdictions or legal systems.", "keywords": ["De-identification", "Court judgments", "Deep neural network", "Data protection", "Korean legal dataset"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.15301", "pdf": "https://arxiv.org/pdf/2506.15301.pdf", "abs": "https://arxiv.org/abs/2506.15301", "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment", "authors": ["Shrestha Ghosh", "Moritz Schneider", "Carina Reinicke", "Carsten Eickhoff"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.", "AI": {"tldr": "This survey analyzes the task of trial-patient matching in clinical trial recruitment and contextualizes LLM-based methods in this domain.", "motivation": "To address the limited adoption of LLMs in critical domains like clinical trial recruitment despite their advancement in general-domain NLP tasks, and to explore the potential benefits of LLMs in matching trials and patients.", "method": "The paper conducts a survey of current LLM-assisted approaches to trial-patient matching, evaluates existing benchmarks and frameworks, and discusses the challenges and future directions for LLM adoption in clinical research.", "result": "The analysis reveals that current applications often rely on proprietary models and lack robust evaluation benchmarks, highlighting the need for improved methodologies in LLM-assisted clinical trial recruitment.", "conclusion": "The survey identifies significant gaps in current approaches and suggests necessary advancements in evaluation frameworks and methodologies, paving the way for more effective use of LLMs in clinical trials.", "key_contributions": ["First comprehensive analysis of trial-patient matching using LLMs", "Critical examination of existing benchmarks and approaches", "Identification of challenges and future directions for LLM technologies in clinical research"], "limitations": "Limited exploration of proprietary models and existing frameworks without rigorous evaluation metrics.", "keywords": ["clinical trial recruitment", "LLM", "trial-patient matching", "NLP", "evaluation frameworks"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.15304", "pdf": "https://arxiv.org/pdf/2506.15304.pdf", "abs": "https://arxiv.org/abs/2506.15304", "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification", "authors": ["Negar Foroutan", "Jakhongir Saydaliev", "Ye Eun Kim", "Antoine Bosselut"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to EMNLP", "summary": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.", "AI": {"tldr": "This paper presents a supervised contrastive learning approach to improve language identification performance for low-resource languages.", "motivation": "The motivation behind this research is to address the poor performance of language identification models on low-resource languages, which are often constrained by limited and single-domain data.", "method": "The authors propose a novel approach based on supervised contrastive learning to learn domain-invariant representations for low-resource languages, aiming to resolve class imbalance and bias issues.", "result": "The proposed method improves language identification performance on out-of-domain data for low-resource languages by 3.2%.", "conclusion": "The findings demonstrate the effectiveness of the supervised contrastive learning approach in enhancing language identification models for low-resource settings.", "key_contributions": ["Introduction of a novel supervised contrastive learning approach for low-resource language identification.", "Analysis showing a 3.2% improvement in performance on out-of-domain data.", "Focus on addressing class imbalance and bias in language identification training."], "limitations": "", "keywords": ["language identification", "low-resource languages", "supervised contrastive learning", "domain-invariant representations", "multilingual LLM"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15339", "pdf": "https://arxiv.org/pdf/2506.15339.pdf", "abs": "https://arxiv.org/abs/2506.15339", "title": "DeVisE: Behavioral Testing of Medical Large Language Models", "authors": ["Camila Zurdo Tagliabue", "Heloisa Oss Boll", "Aykut Erdem", "Erkut Erdem", "Iacer Calixto"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "AI": {"tldr": "Introducing DeVisE, a framework for evaluating the clinical reasoning of large language models in decision support using a dataset of ICU discharge notes.", "motivation": "To address the inadequacies of current evaluation methods that fail to discern true medical reasoning in LLMs.", "method": "Constructing a dataset of ICU discharge notes and evaluating five LLMs under zero-shot and fine-tuned settings while assessing input-level sensitivity and downstream reasoning.", "result": "Zero-shot models demonstrated coherent counterfactual reasoning, while fine-tuned models were stable but less responsive to changes influenced by demographic factors.", "conclusion": "Behavioral testing is essential for understanding clinical LLM reasoning and enhancing the safety and transparency of medical AI systems.", "key_contributions": ["Introduction of DeVisE framework for evaluating clinical understanding in LLMs.", "Creation of a comprehensive dataset of ICU discharge notes with demographic and vital sign counterfactuals.", "Highlighting the impacts of demographic factors on LLM outputs and advocating for fairness-aware evaluations."], "limitations": "The study's findings may depend on the quality and diversity of the ICU discharge notes used in the dataset.", "keywords": ["Large Language Models", "Clinical Decision Support", "Behavioral Testing", "Fairness in AI", "Medical AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15355", "pdf": "https://arxiv.org/pdf/2506.15355.pdf", "abs": "https://arxiv.org/abs/2506.15355", "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Sriparna Saha"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.", "AI": {"tldr": "SANSKRITI is a benchmark dataset for evaluating language models' understanding of India's diverse culture, featuring 21,853 question-answer pairs across significant cultural attributes.", "motivation": "To enhance language models' performance by understanding local socio-cultural contexts, particularly in India.", "method": "The study introduces the SANSKRITI dataset and evaluates it on various language models including LLMs, ILMs, and SLMs, assessing their understanding of culturally nuanced queries.", "result": "The evaluation uncovers significant variations in the performance of different language models when tasked with region-specific cultural questions.", "conclusion": "SANSKRITI provides a comprehensive dataset that can help improve the cultural understanding of language models.", "key_contributions": ["Introduction of the SANSKRITI benchmark dataset for Indian cultural knowledge", "Evaluation of leading language models against this dataset", "Highlighting the disparities in model performance based on cultural contexts."], "limitations": "", "keywords": ["Cultural Understanding", "Language Models", "Benchmark Dataset", "Indian Culture", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.15372", "pdf": "https://arxiv.org/pdf/2506.15372.pdf", "abs": "https://arxiv.org/abs/2506.15372", "title": "COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation", "authors": ["Raghvendra Kumar", "S. A. Mohammed Salman", "Aryan Sahu", "Tridib Nandi", "Pragathi Y. P.", "Sriparna Saha", "Jose G. Moreno"], "categories": ["cs.CL"], "comment": "ACL 2025 MAINs", "summary": "Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.", "AI": {"tldr": "COSMMIC is a dataset for multimodal and multilingual summarization in Indian languages, integrating text, images, and user comments.", "motivation": "To address the lack of resources for comment-aware multimodal summarization in Indian languages.", "method": "The study introduces a dataset comprising 4,959 article-image pairs and 24,484 reader comments, assessing various configurations for summarization using state-of-the-art language models.", "result": "The dataset effectively enhances summarization by incorporating reader insights, with a comprehensive evaluation of different configurations showing efficacy in natural language generation tasks.", "conclusion": "COSMMIC advances NLP resources for Indian languages by combining text, images, and user feedback, promoting inclusivity in research.", "key_contributions": ["Introduction of the COSMMIC dataset for nine Indian languages", "Integration of multimodal data (text, images, comments) for summarization", "Evaluation of summarization configurations using advanced language models"], "limitations": "", "keywords": ["multimodal summarization", "multilingual dataset", "Indian languages", "natural language generation", "reader comments"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.07955", "pdf": "https://arxiv.org/pdf/2506.07955.pdf", "abs": "https://arxiv.org/abs/2506.07955", "title": "Implementation Considerations for Automated AI Grading of Student Work", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "categories": ["cs.HC"], "comment": null, "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "AI": {"tldr": "This study investigates the use of an AI grading platform in K-12 classrooms, revealing mixed responses from teachers and students regarding AI-generated assessments.", "motivation": "To understand how teachers and students interact with AI-powered grading platforms and the implications for trust and pedagogical authority in assessments.", "method": "Conducted a co-design pilot with 19 teachers, utilizing platform usage logs, surveys, and qualitative interviews to analyze the impact of AI-generated rubrics and feedback on teaching and learning experiences.", "result": "Teachers appreciated rapid feedback but distrusted automated scoring; students favored quick feedback yet expressed skepticism toward AI-only grading.", "conclusion": "Trust and oversight are critical when integrating AI assessment tools, and any design must foster teacher agency while providing effective feedback.", "key_contributions": ["Insights into teacher-student interactions with AI grading platforms", "Identification of trust issues surrounding automated scoring", "Guidelines for designing trustworthy AI assessment tools"], "limitations": "Limited to K-12 settings and may not generalize to other educational contexts.", "keywords": ["AI grading", "K-12 education", "teacher-centered design", "assessment", "pedagogical agency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.15415", "pdf": "https://arxiv.org/pdf/2506.15415.pdf", "abs": "https://arxiv.org/abs/2506.15415", "title": "Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning", "authors": ["Stanley Ngugi"], "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "comment": "11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.", "AI": {"tldr": "This paper introduces Targeted Lexical Injection (TLI), a fine-tuning approach to improve lexical alignment in Low-Resource Languages (LRLs) like Swahili. It leverages insights from early layers of the Lugha-Llama-8B-wura model to enhance performance in cross-lingual tasks.", "motivation": "LLMs often underperform in low-resource languages due to data scarcity, which affects tasks like translation and cross-lingual information retrieval. This research addresses the challenge of improving lexical alignment in LRLs.", "method": "The paper proposes Targeted Lexical Injection (TLI), utilizing Low-Rank Adaptation (LoRA) and contrastive learning to fine-tune the model, specifically targeting early-layer embeddings that display strong lexical alignment.", "result": "TLI significantly improved lexical alignment performance, with cosine similarity for 623 Swahili-English word pairs increasing from 0.3211 to 0.4113 (+28.08%) and for 63 unseen control pairs from 0.3143 to 0.4033 (+28.32%).", "conclusion": "TLI effectively enhances the model's ability to maintain and propagate early-layer cross-lingual knowledge, marking a significant advancement in the fine-tuning of LRL-focused LLMs.", "key_contributions": ["Introduction of Targeted Lexical Injection (TLI) as a novel fine-tuning method.", "Demonstration of improved lexical alignment using early-layer embeddings in LLMs.", "Evidence of strong generalization to unseen word pairs."], "limitations": "", "keywords": ["Large Language Models", "lexical alignment", "fine-tuning", "low-resource languages", "contrastive learning"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2506.15425", "pdf": "https://arxiv.org/pdf/2506.15425.pdf", "abs": "https://arxiv.org/abs/2506.15425", "title": "Understanding GUI Agent Localization Biases through Logit Sharpness", "authors": ["Xingjian Tao", "Yiwei Wang", "Yujun Cai", "Zhicheng Yang", "Jing Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.", "AI": {"tldr": "This paper presents a framework for evaluating multimodal large language models (MLLMs) in GUI interactions, focusing on reducing localization errors and improving model performance through novel metrics and techniques.", "motivation": "To address the unreliability of MLLMs in GUI interactions due to systematic localization errors and to enhance the interpretability and robustness of GUI agent behavior.", "method": "The paper proposes a fine-grained evaluation framework that categorizes model predictions into four types and introduces the Peak Sharpness Score (PSS) to quantify model uncertainty. Additionally, it presents the Context-Aware Cropping technique to refine input context without additional training.", "result": "Experiments demonstrate that the proposed framework and techniques improve model performance and provide insightful analysis of model behavior.", "conclusion": "The study highlights the importance of nuanced evaluation methods and training-free techniques to enhance the performance of MLLMs for GUI agents.", "key_contributions": ["Introduction of a fine-grained evaluation framework for MLLMs in GUI tasks.", "Development of the Peak Sharpness Score (PSS) for measuring model uncertainty.", "Proposal of Context-Aware Cropping as a training-free performance enhancement technique."], "limitations": "", "keywords": ["Multimodal large language models", "GUI agents", "localization errors", "model uncertainty", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2503.01903", "pdf": "https://arxiv.org/pdf/2503.01903.pdf", "abs": "https://arxiv.org/abs/2503.01903", "title": "PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice", "authors": ["Shuyu Liu", "Ruoxi Wang", "Ling Zhang", "Xuequan Zhu", "Rui Yang", "Xinzhu Zhou", "Fei Wu", "Zhi Yang", "Cheng Jin", "Gang Wang"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The advent of Large Language Models (LLMs) offers potential solutions to\naddress problems such as shortage of medical resources and low diagnostic\nconsistency in psychiatric clinical practice. Despite this potential, a robust\nand comprehensive benchmarking framework to assess the efficacy of LLMs in\nauthentic psychiatric clinical environments is absent. This has impeded the\nadvancement of specialized LLMs tailored to psychiatric applications. In\nresponse to this gap, by incorporating clinical demands in psychiatry and\nclinical data, we proposed a benchmarking system, PsychBench, to evaluate the\npractical performance of LLMs in psychiatric clinical settings. We conducted a\ncomprehensive quantitative evaluation of 16 LLMs using PsychBench, and\ninvestigated the impact of prompt design, chain-of-thought reasoning, input\ntext length, and domain-specific knowledge fine-tuning on model performance.\nThrough detailed error analysis, we identified strengths and potential\nlimitations of the existing models and suggested directions for improvement.\nSubsequently, a clinical reader study involving 60 psychiatrists of varying\nseniority was conducted to further explore the practical benefits of existing\nLLMs as supportive tools for psychiatrists of varying seniority. Through the\nquantitative and reader evaluation, we show that while existing models\ndemonstrate significant potential, they are not yet adequate as decision-making\ntools in psychiatric clinical practice. The reader study further indicates\nthat, as an auxiliary tool, LLM could provide particularly notable support for\njunior psychiatrists, effectively enhancing their work efficiency and overall\nclinical quality. To promote research in this area, we will make the dataset\nand evaluation framework publicly available, with the hope of advancing the\napplication of LLMs in psychiatric clinical settings.", "AI": {"tldr": "This paper presents PsychBench, a benchmarking framework to evaluate the performance of Large Language Models (LLMs) in psychiatric clinical settings, addressing the need for tailored tools in the field.", "motivation": "The paper addresses the shortage of medical resources and low diagnostic consistency in psychiatric practice by introducing a benchmarking framework for evaluating LLMs in this context.", "method": "A comprehensive quantitative evaluation of 16 LLMs was conducted using PsychBench, focusing on factors such as prompt design, reasoning, input length, and domain-specific fine-tuning. Additionally, a clinical study with 60 psychiatrists assessed the practical benefits of these models.", "result": "The evaluation revealed that while existing LLMs show significant potential as auxiliary tools, they currently lack adequacy as decision-making aids in psychiatric practice, particularly for seasoned professionals compared to junior psychiatrists.", "conclusion": "The dataset and evaluation framework will be made publicly available to foster research on the application of LLMs in psychiatric settings, highlighting the need for improved models in clinical environments.", "key_contributions": ["Introduction of PsychBench as a benchmarking framework for LLMs in psychiatry.", "Quantitative evaluation of 16 LLMs, analyzing impacts of prompt design and model tuning.", "Findings indicate substantial support for junior psychiatrists, but limitations for seasoned clinicians."], "limitations": "Existing models are not yet adequate as decision-making tools; further improvements and tailored versions are necessary.", "keywords": ["Large Language Models", "Psychiatry", "Benchmarking", "Clinical Evaluation", "Auxiliary Tools"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15451", "pdf": "https://arxiv.org/pdf/2506.15451.pdf", "abs": "https://arxiv.org/abs/2506.15451", "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need", "authors": ["Zhouhong Gu", "Xiaoxuan Zhu", "Yin Cai", "Hao Shen", "Xingzhou Chen", "Qingyi Wang", "Jialin Li", "Xiaoran Shi", "Haoran Guo", "Wenxuan Huang", "Hongwei Feng", "Yanghua Xiao", "Zheyu Ye", "Yao Hu", "Shaosheng Cao"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.", "AI": {"tldr": "AgentGroupChat-V2 is a novel framework for large language model multi-agent systems, addressing challenges in architecture design and task resolution with core innovations in parallel processing and collaboration.", "motivation": "To improve system architecture and performance of multi-agent systems using large language models for complex tasks.", "method": "Introduces a fully parallel architecture, an adaptive collaboration engine for heterogeneous LLMs, and optimization strategies for agent organization.", "result": "AgentGroupChat-V2 achieves superior performance with 91.50% accuracy on GSM8K, 30.4% on AIME, and 79.20% on HumanEval, particularly excelling in difficult tasks.", "conclusion": "AgentGroupChat-V2 offers a comprehensive, efficient solution for multi-agent systems capable of handling complex reasoning tasks more effectively than existing methods.", "key_contributions": ["Divide-and-conquer fully parallel architecture for user query decomposition.", "Adaptive collaboration engine for dynamic LLM selection.", "Agent organization optimization for efficient problem decomposition."], "limitations": "", "keywords": ["large language models", "multi-agent systems", "task resolution", "parallel architecture", "adaptive collaboration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15455", "pdf": "https://arxiv.org/pdf/2506.15455.pdf", "abs": "https://arxiv.org/abs/2506.15455", "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation", "authors": ["Xinnuo Xu", "Rachel Lawrence", "Kshitij Dubey", "Atharva Pandey", "Risa Ueno", "Fabian Falck", "Aditya V. Nori", "Rahul Sharma", "Amit Sharma", "Javier Gonzalez"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.", "AI": {"tldr": "This paper introduces RE-IMAGINE, a framework for evaluating reasoning abilities in Large Language Models by generating problem variations that cannot be solved through memorization alone.", "motivation": "To clarify whether observed high accuracy of LLMs on reasoning benchmarks arises from true reasoning skills or statistical recall of their training data.", "method": "The RE-IMAGINE framework creates a hierarchy of reasoning abilities based on the ladder of causation and generates problem variations within this hierarchy using an intermediate symbolic representation.", "result": "The framework was tested on four popular benchmarks, revealing a decrease in performance when models were faced with generated problem variations, suggesting reliance on statistical recall rather than genuine reasoning.", "conclusion": "RE-IMAGINE highlights the limitations of LLMs in reasoning tasks and sets the stage for further research into developing reasoning skills across different domains.", "key_contributions": ["Introduction of the RE-IMAGINE framework for reasoning evaluation", "Generation of problem variations that require genuine reasoning beyond memorization", "Demonstration of performance reductions in LLMs when faced with generated problems"], "limitations": "The approach relies on specific problem variations and does not explore all reasoning domains exhaustively.", "keywords": ["Large Language Models", "Reasoning", "Framework", "Problem Variations", "Statistical Recall"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.15480", "pdf": "https://arxiv.org/pdf/2506.15480.pdf", "abs": "https://arxiv.org/abs/2506.15480", "title": "Context-Informed Grounding Supervision", "authors": ["Hyunji Lee", "Seunghyun Yoon", "Yunjae Won", "Hanseok Oh", "Geewook Kim", "Trung Bui", "Franck Dernoncourt", "Elias Stengel-Eskin", "Mohit Bansal", "Minjoon Seo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.", "AI": {"tldr": "This paper proposes Context-INformed Grounding Supervision (CINGS) to enhance grounded generation in large language models by training them with relevant context prepended to responses.", "motivation": "To improve the grounding ability of large language models (LLMs) when using external knowledge, addressing limitations seen in prior methods where context is simply appended at inference time.", "method": "CINGS involves post-training supervision where relevant context is prepended to the response while computing the loss only over the response tokens, effectively masking out the context.", "result": "Models trained with CINGS show significantly improved grounding in both textual and visual domains compared to standard models, with better performance across diverse benchmarks and reduced hallucinations.", "conclusion": "CINGS enhances the model's reliance on external context without negatively impacting general performance, indicating a shift in knowledge and behavior that promotes factual consistency.", "key_contributions": ["Introduction of CINGS for improved model grounding", "Demonstration of enhanced performance across 11 information-seeking datasets", "Reduction of hallucinations in vision-language models with factual consistency"], "limitations": "", "keywords": ["Language Models", "Grounding", "Supervision", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15498", "pdf": "https://arxiv.org/pdf/2506.15498.pdf", "abs": "https://arxiv.org/abs/2506.15498", "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling", "authors": ["Md Imbesat Hassan Rizvi", "Xiaodan Zhu", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages main content, 4 figures, 4 tables", "summary": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.", "AI": {"tldr": "The paper introduces SPARE, a framework for efficient process annotation in LLMs, improving reasoning performance and efficiency in multi-step reasoning tasks.", "motivation": "Automated process annotation is essential for advancing the reasoning capabilities of LLMs but remains challenging.", "method": "SPARE employs a single-pass annotation process that aligns solution steps to a reference solution while providing explicit reasoning for evaluation.", "result": "SPARE shows improved reasoning performance across four datasets in mathematical reasoning, compositional question answering, and spatial reasoning, achieving competitive results with significantly lower runtime.", "conclusion": "SPARE enhances the efficiency of process supervision for LLMs, providing a practical solution to the annotation challenge and is made available for further research.", "key_contributions": ["Introduction of SPARE for single-pass annotation", "Improved reasoning performance across multiple domains", "Enhanced efficiency compared to existing methods"], "limitations": "", "keywords": ["Large Language Models", "process supervision", "annotation efficiency", "reasoning performance", "AI applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15504", "pdf": "https://arxiv.org/pdf/2506.15504.pdf", "abs": "https://arxiv.org/abs/2506.15504", "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge", "authors": ["Li Zheng", "Sihang Wang", "Hao Fei", "Zuquan Peng", "Fei Li", "Jianming Fu", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL 2025", "summary": "Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.", "AI": {"tldr": "The paper presents an emotion-guided framework, EmoBi, for detecting hyperbole and metaphor by leveraging emotion analysis and dynamic interactions.", "motivation": "Identifying hyperbole and metaphor poses challenges due to their semantic obscurity and the neglect of implicit emotions in existing detection methods.", "method": "The EmoBi framework includes an emotion analysis module to mine emotion connotations, a domain mapping module for target and source understanding, and a dynamic interaction module to enhance detection of rhetorical devices.", "result": "EmoBi significantly outperforms baseline methods, achieving an F1 score increase of 28.1% for hyperbole detection and 23.1% for metaphor detection compared to the state of the art.", "conclusion": "The EmoBi framework demonstrates effectiveness in improving the detection of hyperbole and metaphor, showcasing potential advancements in NLP applications.", "key_contributions": ["Emotion-guided framework for hyperbole and metaphor detection", "Deep analysis of emotion connotations", "Bidirectional dynamic interaction for mutual promotion between rhetorical devices"], "limitations": "", "keywords": ["hyperbole detection", "metaphor detection", "emotion analysis", "natural language processing", "NLP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.15522", "pdf": "https://arxiv.org/pdf/2506.15522.pdf", "abs": "https://arxiv.org/abs/2506.15522", "title": "Lessons from Training Grounded LLMs with Verifiable Rewards", "authors": ["Shang Hong Sim", "Tej Deep Pala", "Vernon Toh", "Hai Leong Chieu", "Amir Zadeh", "Chuan Li", "Navonil Majumder", "Soujanya Poria"], "categories": ["cs.CL"], "comment": null, "summary": "Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.", "AI": {"tldr": "This study explores how reinforcement learning and internal reasoning can enhance the grounding of large language models (LLMs) through a method called GRPO, leading to significant improvements in generating well-cited responses and handling unanswerable queries.", "motivation": "Generating grounded and trustworthy responses in large language models remains a challenging task, with existing models often failing to provide correct citations and accurate answers.", "method": "The authors employ the Group Relative Policy Optimization (GRPO) method to train LLMs with verifiable outcome-based rewards focusing on answer correctness, citation sufficiency, and refusal quality, without needing extensive annotations.", "result": "Experiments show that reasoning-augmented models outperform instruction-only variants, particularly in unanswerable queries and citation generation. The two-stage training method enhances grounding by stabilizing the learning signal, and combining GRPO with instruction tuning improves performance on long-form QA tasks.", "conclusion": "The study concludes that reinforcement learning, internal reasoning, and an outcome-driven training approach are effective in building more reliable and verifiable LLMs.", "key_contributions": ["Introduction of GRPO for training LLMs", "Demonstration of stage-wise optimization improving grounding", "Combining instruction tuning with GRPO to enhance QA tasks"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "grounding", "citation", "reasoning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.15545", "pdf": "https://arxiv.org/pdf/2506.15545.pdf", "abs": "https://arxiv.org/abs/2506.15545", "title": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models", "authors": ["Bailin Wang", "Chang Lan", "Chong Wang", "Ruoming Pang"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.", "AI": {"tldr": "This paper introduces RATTENTION, a new local-global attention model that improves efficiency without sacrificing performance by successfully handling out-of-window tokens and optimizing window size.", "motivation": "To address the limitations of local attention by enhancing its performance in short-context scenarios while maintaining efficiency.", "method": "The authors propose RATTENTION, which incorporates a specialized linear attention mechanism to capture information from tokens outside the defined window and perform pretraining experiments.", "result": "RATTENTION achieves a superior performance-efficiency tradeoff, matching full-attention model performance with a significantly smaller window size (512) and maintaining training speeds comparable to existing methods.", "conclusion": "The proposed RATTENTION model successfully balances the tradeoff between performance and efficiency, especially in scenarios with shorter contexts, making it a promising alternative to traditional attention mechanisms.", "key_contributions": ["Introduction of RATTENTION model for local-global attention", "Demonstration of improved efficiency and performance with smaller window sizes", "Validation of model performance on the RULER benchmark"], "limitations": "", "keywords": ["local-global attention", "RATTENTION", "efficiency", "performance", "Transformers"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15553", "pdf": "https://arxiv.org/pdf/2506.15553.pdf", "abs": "https://arxiv.org/abs/2506.15553", "title": "Approximating Language Model Training Data from Weights", "authors": ["John X. Morris", "Junjie Oscar Yin", "Woojeong Kim", "Vitaly Shmatikov", "Alexander M. Rush"], "categories": ["cs.CL"], "comment": null, "summary": "Modern language models often have open weights but closed training data. We\nformalize the problem of data approximation from model weights and propose\nseveral baselines and metrics. We develop a gradient-based approach that\nselects the highest-matching data from a large public text corpus and show its\neffectiveness at recovering useful data given only weights of the original and\nfinetuned models. Even when none of the true training data is known, our method\nis able to locate a small subset of public Web documents can be used to train a\nmodel to close to the original model performance given models trained for both\nclassification and supervised-finetuning. On the AG News classification task,\nour method improves performance from 65% (using randomly selected data) to 80%,\napproaching the expert benchmark of 88%. When applied to a model trained with\nSFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,\ncompared to an expert LLAMA model's perplexity of 2.0.", "AI": {"tldr": "The paper proposes a gradient-based method for approximating training data from existing language models using their weights, enhancing model performance significantly.", "motivation": "To address the challenge of recovering training data for language models, which often have open weights but closed training datasets.", "method": "A gradient-based approach that identifies the highest-matching data from a large public text corpus is developed to approximate original training data.", "result": "The proposed method improved classification performance on the AG News task from 65% to 80% and decreased perplexity on MSMARCO data from 3.3 to 2.3.", "conclusion": "The method shows potential in recovering effective training data to enhance model performance, even without access to the original dataset.", "key_contributions": ["Introduces a method for data approximation from language model weights.", "Demonstrates significant performance improvements in classification tasks and perplexity measurements.", "Establishes baseline metrics for evaluating data approximation techniques."], "limitations": "", "keywords": ["language models", "data approximation", "machine learning", "fine-tuning", "gradient-based methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.15556", "pdf": "https://arxiv.org/pdf/2506.15556.pdf", "abs": "https://arxiv.org/abs/2506.15556", "title": "PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction", "authors": ["Shufan Li", "Aditya Grover"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "16 pages,4 figures", "summary": "Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.", "AI": {"tldr": "This paper introduces Predictive Generation (PredGen), a framework to reduce latency in LLM-driven voice chat applications by generating responses while the user is speaking.", "motivation": "LLMs combined with TTS systems face latency issues in voice chat applications, which negatively impacts user experience, especially on consumer-grade hardware.", "method": "PredGen employs speculative decoding to generate candidate responses during user input, allowing TTS to start processing with minimal delay.", "result": "PredGen reduces latency by around 2x with minimal additional computational cost in diverse scenarios according to simulated experiments.", "conclusion": "The proposed framework significantly enhances the responsiveness of voice assistants by addressing the latency in generating audio responses from LLMs.", "key_contributions": ["Introduction of the PredGen framework for latency reduction in voice applications", "Demonstration of 2x latency improvement in simulated experiments", "Minimization of additional computational costs during response generation."], "limitations": "", "keywords": ["Large Language Models", "Voice Chat Applications", "Predictive Generation"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.15568", "pdf": "https://arxiv.org/pdf/2506.15568.pdf", "abs": "https://arxiv.org/abs/2506.15568", "title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models", "authors": ["Zhengyang Shan", "Emily Ruth Diana", "Jiawei Zhou"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main", "summary": "We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.", "AI": {"tldr": "This paper evaluates gender fairness in large language models (LLMs) using a new metric called the Gender Inclusivity Fairness Index (GIFI) that assesses inclusivity for both binary and non-binary genders.", "motivation": "To address the limitations of previous studies that primarily focus on binary gender distinctions and to improve the inclusivity of LLMs.", "method": "The study introduces the Gender Inclusivity Fairness Index (GIFI), which includes various evaluations such as probing models with gender pronouns and assessing their generation and cognitive behaviors related to gender.", "result": "Extensive evaluations using GIFI on 22 LLMs showed significant variations in gender inclusivity among the models, highlighting the need for advancements in this area.", "conclusion": "The findings emphasize the importance of enhancing inclusivity in LLMs and establish a benchmark for future work in gender fairness in generative models.", "key_contributions": ["Introduction of the Gender Inclusivity Fairness Index (GIFI) for evaluating gender inclusivity in LLMs", "Comprehensive assessments on 22 LLMs revealing significant bias variations", "Establishment of a benchmark for future advancements in gender fairness in AI models"], "limitations": "", "keywords": ["gender fairness", "large language models", "gender inclusivity", "bias evaluation", "generative models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15569", "pdf": "https://arxiv.org/pdf/2506.15569.pdf", "abs": "https://arxiv.org/abs/2506.15569", "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification", "authors": ["Chengye Wang", "Yifei Shen", "Zexi Kuang", "Arman Cohan", "Yilun Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.", "AI": {"tldr": "Introduction of SciVer, a benchmark for evaluating foundation models in multimodal scientific claim verification.", "motivation": "To address the need for a specific benchmark to assess foundation models' capabilities in multimodal scientific contexts.", "method": "Creation of SciVer with 3,000 expert-annotated examples spanning four reasoning types, evaluating 21 state-of-the-art multimodal foundation models.", "result": "Found a significant performance gap between the evaluated models and human experts in multimodal scientific claim verification.", "conclusion": "Identification of critical limitations in current models, providing insights to improve comprehension and reasoning in scientific literature tasks.", "key_contributions": ["Introduction of a new benchmark (SciVer) for evaluating models in scientific contexts", "Detailed performance assessment of multimodal foundation models", "Analysis of critical limitations and error evaluation in existing models"], "limitations": "The study reveals substantial performance gaps without presenting solutions for the identified limitations.", "keywords": ["multimodal", "foundation models", "scientific claim verification", "benchmark", "RAG"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15583", "pdf": "https://arxiv.org/pdf/2506.15583.pdf", "abs": "https://arxiv.org/abs/2506.15583", "title": "DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement", "authors": ["Shaoqing Lin", "Chong Teng", "Fei Li", "Donghong Ji", "Lizhen Qu", "Zhuang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) now generate discourse-level, multi-sentence\nvisual descriptions, challenging text scene graph parsers originally designed\nfor single-sentence caption-to-graph mapping. Current approaches typically\nmerge sentence-level parsing outputs for discourse input, often missing\nphenomena like cross-sentence coreference, resulting in fragmented graphs and\ndegraded downstream VLM task performance. To address this, we introduce a new\ntask, Discourse-level text Scene Graph parsing (DiscoSG), supported by our\ndataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised\nmulti-sentence caption-graph pairs for images. Each caption averages 9\nsentences, and each graph contains at least 3 times more triples than those in\nexisting datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS\nimproves SPICE by approximately 48% over the best sentence-merging baseline,\nhigh inference cost and restrictive licensing hinder its open-source use, and\nsmaller fine-tuned PLMs struggle with complex graphs. We propose\nDiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a\nsecond PLM to iteratively propose graph edits, reducing full-graph generation\noverhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE\nby approximately 30% over the best baseline while achieving 86 times faster\ninference than GPT-4. It also consistently improves downstream VLM tasks like\ndiscourse-level caption evaluation and hallucination detection. Code and data\nare available at: https://github.com/ShaoqLin/DiscoSG", "AI": {"tldr": "This paper introduces a new task, Discourse-level text Scene Graph parsing (DiscoSG), aimed at improving the performance of Vision-Language Models by refining the way multi-sentence visual descriptions are parsed into scene graphs.", "motivation": "Current scene graph parsers struggle with discourse-level multi-sentence visual descriptions, leading to poor performance on downstream Vision-Language Model tasks due to issues like cross-sentence coreference.", "method": "The authors present DiscoSG, a dataset and task that pairs multi-sentence captions with enhanced scene graphs. They propose a two-step graph generation method called DiscoSG-Refiner, which utilizes two Flan-T5-Base models to improve graph parsing efficiency.", "result": "DiscoSG-Refiner improves SPICE by approximately 30% over existing baselines while achieving significantly faster inference times, demonstrating enhanced performance in downstream tasks.", "conclusion": "The proposed methods and dataset are effective in addressing the limitations of previous approaches, showing substantial improvements in discourse-level tasks.", "key_contributions": ["Introduction of DiscoSG dataset and task for discourse-level scene graph parsing.", "Development of DiscoSG-Refiner for efficient graph generation while maintaining high performance.", "Significant improvements in downstream evaluations such as caption assessment and hallucination detection."], "limitations": "High inference costs for larger models and licensing restrictions hinder their open-source accessibility.", "keywords": ["Vision-Language Models", "scene graph parsing", "multi-sentence captions", "DiscoSG-DS", "DiscoSG-Refiner"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.15594", "pdf": "https://arxiv.org/pdf/2506.15594.pdf", "abs": "https://arxiv.org/abs/2506.15594", "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts", "authors": ["Negar Foroutan", "Angelika Romanou", "Matin Ansaripour", "Julian Martin Eisenschlos", "Karl Aberer", "RÃ©mi Lebret"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 (Findings)", "summary": "Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.", "AI": {"tldr": "WikiMixQA introduces a benchmark for evaluating vision-language models' cross-modal reasoning skills over long document contexts, emphasizing complex reasoning with tables and charts from Wikipedia.", "motivation": "The paper addresses challenges in automatic document understanding, particularly with long-context vision inputs and complex layouts.", "method": "The authors present WikiMixQA, a benchmark that includes 1,000 multiple-choice questions based on tables and charts from 4,000 Wikipedia pages, focusing on cross-modal reasoning.", "result": "Evaluation of 12 state-of-the-art vision-language models shows that while proprietary models perform better with direct context (~70% accuracy), their performance drops significantly with long documents. GPT-4-o is notably better, exceeding 50% accuracy, while open-source models achieve a maximum of 27%.", "conclusion": "The findings highlight the difficulties in long-context, multi-modal reasoning and position WikiMixQA as an essential tool for advancing research in document understanding.", "key_contributions": ["Introduction of WikiMixQA benchmark for cross-modal reasoning", "Evaluation of multiple state-of-the-art vision-language models", "Demonstration of performance disparities in long-context processing"], "limitations": "The benchmark focuses on specific Wikipedia topics and may not generalize across all document types.", "keywords": ["vision-language models", "document understanding", "cross-modal reasoning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.15598", "pdf": "https://arxiv.org/pdf/2506.15598.pdf", "abs": "https://arxiv.org/abs/2506.15598", "title": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso", "Pedro Pinto", "Abel Ferreira", "LuÃ­s Abreu", "Isabel Rangel", "Sandra Monteiro"], "categories": ["cs.CL", "cs.AI"], "comment": "This is a preprint version of the manuscript currently under review\n  at an international journal", "summary": "While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.", "AI": {"tldr": "The paper investigates the use of generative AI for automating the creation of multiple-choice questions (MCQs) in Portuguese, focusing on reading comprehension suitable for elementary education.", "motivation": "To address the time-consuming process of manually creating MCQs and the need for high-quality and reliable MCQs in educational settings, particularly in non-English languages.", "method": "Generative models are used to produce MCQs aligned with curriculum and varying difficulty levels. The generated MCQs are evaluated through expert reviews and psychometric analysis based on student responses.", "result": "The study finds that the quality of AI-generated MCQs is comparable to human-created ones, although issues with semantic clarity, answerability, and distractor quality are identified.", "conclusion": "While generative AI shows promise in MCQ generation, overcoming challenges in clarity and engagement is essential for practical application in education.", "key_contributions": ["Demonstrates the application of generative AI in non-English language MCQ generation.", "Evaluates the quality of AI-generated MCQs using expert reviews and psychometric data.", "Highlights the importance of reliable distractors in MCQ design."], "limitations": "Focuses on Portuguese, limiting applicability to other languages; issues with semantic clarity and distractor quality were noted.", "keywords": ["generative AI", "MCQ generation", "reading comprehension", "psychometrics", "Portuguese"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.15617", "pdf": "https://arxiv.org/pdf/2506.15617.pdf", "abs": "https://arxiv.org/abs/2506.15617", "title": "The Compositional Architecture of Regret in Large Language Models", "authors": ["Xiangxiang Cui", "Shu Yang", "Tianjin Huang", "Wanyu Lin", "Lijie Hu", "Di Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.", "AI": {"tldr": "The paper explores the concept of regret in Large Language Models (LLMs) by identifying and analyzing regret expressions within model outputs to enhance reliability and understanding of cognitive processes in neural networks.", "motivation": "Understanding the mechanism of regret in LLMs is vital for improving model reliability and cognitive representation in neural networks.", "method": "The study develops a workflow to construct a regret dataset through designed prompting scenarios, introduces the Supervised Compression-Decoupling Index (S-CDI) to find optimal representation layers for regret, and defines the Regret Dominance Score (RDS) to identify and analyze regret neurons.", "result": "The proposed methods successfully identified optimal regret representation layers, leading to improved performance in probe classification. An M-shaped decoupling pattern was observed in model layers, and neurons were categorized into three functional types.", "conclusion": "The analysis of regret in LLMs provides insights into cognitive processing and highlights the potential for enhancing model reliability through better understanding of how different neurons function in relation to regret.", "key_contributions": ["Workflow for constructing a comprehensive regret dataset", "Development of S-CDI metric for optimal layer identification", "Introduction of RDS metric for regret neuron analysis"], "limitations": "Challenges include the absence of specialized datasets, lack of metrics for optimal layer identification, and limited methods for analyzing regret neurons.", "keywords": ["Large Language Models", "regret expression", "neural networks", "cognitive processes", "machine learning"], "importance_score": 8, "read_time_minutes": 23}}
{"id": "2506.15623", "pdf": "https://arxiv.org/pdf/2506.15623.pdf", "abs": "https://arxiv.org/abs/2506.15623", "title": "Minding the Politeness Gap in Cross-cultural Communication", "authors": ["Yuka Machino", "Matthias Hofer", "Max Siegel", "Joshua B. Tenenbaum", "Robert D. Hawkins"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Misunderstandings in cross-cultural communication often arise from subtle\ndifferences in interpretation, but it is unclear whether these differences\narise from the literal meanings assigned to words or from more general\npragmatic factors such as norms around politeness and brevity. In this paper,\nwe report three experiments examining how speakers of British and American\nEnglish interpret intensifiers like \"quite\" and \"very.\" To better understand\nthese cross-cultural differences, we developed a computational cognitive model\nwhere listeners recursively reason about speakers who balance informativity,\npoliteness, and utterance cost. Our model comparisons suggested that\ncross-cultural differences in intensifier interpretation stem from a\ncombination of (1) different literal meanings, (2) different weights on\nutterance cost. These findings challenge accounts based purely on semantic\nvariation or politeness norms, demonstrating that cross-cultural differences in\ninterpretation emerge from an intricate interplay between the two.", "AI": {"tldr": "This paper investigates cross-cultural communication issues in the interpretation of English intensifiers between British and American speakers through computational cognitive modeling.", "motivation": "To explore the subtle differences in interpretation of intensifiers between British and American English speakers and understand whether these arise from literal meanings or pragmatic factors.", "method": "The authors conducted three experiments and developed a computational cognitive model to analyze how listeners balance informativity, politeness, and utterance cost in their interpretations.", "result": "The model comparisons indicated that differences in intensifier interpretation between cultures stem from both literal meaning variations and the weight assigned to utterance cost.", "conclusion": "Cross-cultural differences in interpretation are influenced by a complex interplay of semantic meanings and normative behaviors, challenging previous theories that focused on either solely.", "key_contributions": ["Development of a computational cognitive model for cross-cultural communication analysis.", "Identification of the interaction between literal meaning and pragmatic factors in understanding intensifiers.", "New insights into the role of utterance cost in cross-cultural communication."], "limitations": "", "keywords": ["cross-cultural communication", "intensifiers", "computational cognitive model", "British English", "American English"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.15629", "pdf": "https://arxiv.org/pdf/2506.15629.pdf", "abs": "https://arxiv.org/abs/2506.15629", "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability", "authors": ["Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.", "AI": {"tldr": "This paper presents Ordered CommonGen, a benchmark for evaluating LLMs on their compositional generalization and instruction-following capabilities with a focus on generating sentences in specified concept order.", "motivation": "To evaluate the capability of generative LLMs to follow instructions regarding the order of concepts in sentence generation and to measure their compositional generalization abilities.", "method": "Introduced a benchmark called Ordered CommonGen that quantifies ordered coverage, assessing whether generated sentences include concepts in the specified order, while using 36 LLMs to analyze their performance.", "result": "The study revealed that LLMs understand instruction intent but often produce low-diversity outputs due to biases towards specific order patterns; the best-performing LLM achieved only 75% ordered coverage.", "conclusion": "Improvements are necessary for LLMs regarding their instruction-following and compositional generalization capabilities.", "key_contributions": ["Development of the Ordered CommonGen benchmark", "Introduction of ordered coverage metrics", "Analysis of LLMs' instruction-following capabilities and their limitations"], "limitations": "The study primarily focuses on instruction-following and compositional generalization without addressing other aspects of LLM performance.", "keywords": ["generative LLMs", "commonsense reasoning", "benchmark", "ordered coverage", "instruction-following"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15650", "pdf": "https://arxiv.org/pdf/2506.15650.pdf", "abs": "https://arxiv.org/abs/2506.15650", "title": "Oldies but Goldies: The Potential of Character N-grams for Romanian Texts", "authors": ["Dana Lupsa", "Sanda-Maria Avram"], "categories": ["cs.CL"], "comment": null, "summary": "This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings.", "AI": {"tldr": "This study evaluates machine learning techniques for authorship attribution of Romanian texts, finding that artificial neural networks perform best using character n-gram features.", "motivation": "To address authorship attribution for Romanian texts using a systematic evaluation of machine learning techniques.", "method": "The study evaluates six machine learning techniques (SVM, LR, k-NN, DT, RF, ANN) using character n-gram features for authorship classification on the ROST corpus.", "result": "The ANN model achieved the highest performance, with perfect classification in four out of fifteen runs using 5-gram features.", "conclusion": "Lightweight, interpretable character n-gram approaches can deliver state-of-the-art accuracy for Romanian authorship attribution and may be preferable in resource-constrained language settings.", "key_contributions": ["Evaluation of multiple machine learning techniques for authorship attribution", "Demonstrated effectiveness of character n-gram features in Romanian texts", "Showcased the potential of simple stylometric approaches in under-studied languages"], "limitations": "", "keywords": ["authorship attribution", "machine learning", "Romanian texts"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.15662", "pdf": "https://arxiv.org/pdf/2506.15662.pdf", "abs": "https://arxiv.org/abs/2506.15662", "title": "CC-LEARN: Cohort-based Consistency Learning", "authors": ["Xiao Ye", "Shaswat Shrivastava", "Zhaonan Li", "Jacob Dineen", "Shijie Lu", "Avneet Ahuja", "Ming Shen", "Zhikun Xu", "Ben Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.", "AI": {"tldr": "Cohort-based Consistency Learning (CC-Learn) enhances reasoning consistency in LLMs through a reinforcement learning framework that optimizes cohort-level consistency.", "motivation": "Large language models struggle with robust reasoning and consistency across tasks.", "method": "Cohort-based Consistency Learning (CC-Learn) uses reinforcement learning to train on cohorts of similar questions, optimizing a composite objective that includes cohort accuracy and penalties for trivial lookups.", "result": "Experiments show that CC-Learn improves both accuracy and reasoning stability on benchmarks like ARC-Challenge and StrategyQA compared to pretrained and SFT models.", "conclusion": "Cohort-level reinforcement learning effectively enhances reasoning consistency in large language models.", "key_contributions": ["Introduction of a framework for cohort-level consistency in reasoning", "Demonstration of improved accuracy and stability on reasoning tasks", "Optimized reinforcement learning objectives for LLMs"], "limitations": "", "keywords": ["Cohort-based Consistency Learning", "Reinforcement Learning", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.15674", "pdf": "https://arxiv.org/pdf/2506.15674.pdf", "abs": "https://arxiv.org/abs/2506.15674", "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers", "authors": ["Tommaso Green", "Martin Gubri", "Haritz Puerto", "Sangdoo Yun", "Seong Joon Oh"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.", "AI": {"tldr": "The paper examines privacy leaks in the reasoning traces of large reasoning models, showing that these internal processes can contain sensitive user information and can be exploited through prompt injections.", "motivation": "To investigate the assumption that reasoning traces of large models are safe from privacy leakage.", "method": "The authors conduct probing and agentic evaluations to analyze how reasoning steps in models can lead to sensitive data leakage.", "result": "The study finds that while increasing reasoning steps makes models more cautious in final outputs, it significantly increases the verbosity in reasoning, leading to more potential data leakage.", "conclusion": "Safety measures should address not just final outputs but also the internal reasoning processes of models to mitigate privacy risks.", "key_contributions": ["Demonstration that reasoning traces can leak sensitive user information.", "Identification of prompt injections as a method for extracting this information.", "Analysis of the trade-off between reasoning utility and privacy risks."], "limitations": "The study primarily focuses on the leakage aspects and may not cover extensive solutions to mitigate these issues.", "keywords": ["Privacy leakage", "Reasoning traces", "Large language models", "Sensitive data", "Prompt injections"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.15676", "pdf": "https://arxiv.org/pdf/2506.15676.pdf", "abs": "https://arxiv.org/abs/2506.15676", "title": "Gender-Neutral Machine Translation Strategies in Practice", "authors": ["Hillary Dawkins", "Isar Nejadgholi", "Chi-kiu Lo"], "categories": ["cs.CL"], "comment": "to appear at GITT 2025", "summary": "Gender-inclusive machine translation (MT) should preserve gender ambiguity in\nthe source to avoid misgendering and representational harms. While gender\nambiguity often occurs naturally in notional gender languages such as English,\nmaintaining that gender neutrality in grammatical gender languages is a\nchallenge. Here we assess the sensitivity of 21 MT systems to the need for\ngender neutrality in response to gender ambiguity in three translation\ndirections of varying difficulty. The specific gender-neutral strategies that\nare observed in practice are categorized and discussed. Additionally, we\nexamine the effect of binary gender stereotypes on the use of gender-neutral\ntranslation. In general, we report a disappointing absence of gender-neutral\ntranslations in response to gender ambiguity. However, we observe a small\nhandful of MT systems that switch to gender neutral translation using specific\nstrategies, depending on the target language.", "AI": {"tldr": "This paper evaluates the performance of 21 machine translation systems in preserving gender neutrality, particularly in the context of gender ambiguity in language.", "motivation": "To assess how machine translation systems handle gender ambiguity while considering the potential for misgendering and representational harms.", "method": "The study analyzes 21 machine translation systems across three translation directions of varying difficulty, categorizing their gender-neutral strategies in response to gender ambiguity.", "result": "The findings highlight a broad failure among the majority of MT systems to provide gender-neutral translations when faced with gender ambiguity, although a few systems do implement effective strategies based on the target language.", "conclusion": "There is a pressing need for improvements in machine translation systems to ensure they maintain gender neutrality, as current implementations often fall short.", "key_contributions": ["Evaluation of 21 MT systems' response to gender ambiguity", "Categorization of gender-neutral strategies in MT", "Examination of the impact of binary gender stereotypes on MT"], "limitations": "The study's scope is limited to just 21 MT systems, which may not provide a comprehensive view of all systems available.", "keywords": ["machine translation", "gender neutrality", "gender ambiguity", "representational harms", "binary gender stereotypes"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.15681", "pdf": "https://arxiv.org/pdf/2506.15681.pdf", "abs": "https://arxiv.org/abs/2506.15681", "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "categories": ["cs.CL"], "comment": "Project page: https://byungkwanlee.github.io/GenRecal-page/", "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.", "AI": {"tldr": "The paper presents Generation after Recalibration (GenRecal), a distillation framework that improves the performance of vision-language models (VLMs) by facilitating knowledge transfer across different VLM architectures.", "motivation": "The motivation behind this work is to address the challenges of deploying vision-language models (VLMs) on resource-constrained devices, particularly due to their high computational demands.", "method": "GenRecal introduces a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, allowing for effective knowledge transfer.", "result": "Extensive experiments show that GenRecal significantly enhances baseline performances on various benchmarks, surpassing both open-source and closed-source VLMs.", "conclusion": "GenRecal demonstrates a promising approach to create smaller, efficient VLMs without compromising performance, making it feasible for deployment in real-world scenarios.", "key_contributions": ["Introduction of GenRecal, a general-purpose distillation framework for VLMs", "Development of the Recalibrator for feature representation alignment", "Demonstration of superior performance on multiple challenging benchmarks."], "limitations": "", "keywords": ["vision-language models", "distillation framework", "knowledge transfer"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.15683", "pdf": "https://arxiv.org/pdf/2506.15683.pdf", "abs": "https://arxiv.org/abs/2506.15683", "title": "PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning", "authors": ["Yuhui Shi", "Yehan Yang", "Qiang Sheng", "Hao Mi", "Beizhe Hu", "Chaoxi Xu", "Juan Cao"], "categories": ["cs.CL", "cs.CY"], "comment": "17 pages, 3 figures, 6 tables", "summary": "With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.", "AI": {"tldr": "PhantomHunter detects LLM-generated text from privately-tuned models using a family-aware learning framework, outperforming existing baselines.", "motivation": "The rise of large language models (LLMs) has led to increased issues like misinformation and academic misconduct, necessitating effective detection of LLM-generated text, particularly from privately tuned models.", "method": "The paper introduces PhantomHunter, a specialized detector that employs a family-aware learning framework to identify shared traits across base models and their fine-tuned derivatives.", "result": "PhantomHunter demonstrates superior detection capabilities, achieving F1 scores over 96% compared to 7 baselines and 3 industrial services in experiments using various LLM families.", "conclusion": "The proposed method effectively addresses the challenges of detecting text from privately-tuned LLMs, indicating its potential importance in maintaining integrity in text generation.", "key_contributions": ["Introduction of PhantomHunter for detecting LLM-generated text from privately-tuned models.", "Use of family-aware learning framework for improved detection accuracy.", "Experimental validation showing substantially higher performance than existing solutions."], "limitations": "", "keywords": ["large language models", "text detection", "machine learning", "privately-tuned models", "family-aware learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.06561", "pdf": "https://arxiv.org/pdf/2506.06561.pdf", "abs": "https://arxiv.org/abs/2506.06561", "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles", "authors": ["Ho Yin 'Sam' Ng", "Ting-Yao Hsu", "Aashish Anantha Ramakrishnan", "Branislav Kveton", "Nedim Lipka", "Franck Dernoncourt", "Dongwon Lee", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Ting-Hao 'Kenneth' Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "The LaMP-CAP dataset is publicly available at:\n  https://github.com/Crowd-AI-Lab/lamp-cap", "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.", "AI": {"tldr": "This paper presents LaMP-Cap, a dataset designed for generating personalized figure captions using multimodal figure profiles, showing significant improvements in caption quality when leveraging profiles.", "motivation": "There is a need for personalized figure captions that match authors' writing styles, as existing AI-generated captions often require revision.", "method": "The paper introduces LaMP-Cap, which includes multimodal inputs (figure images and additional figure profiles) to assist in generating personalized captions.", "result": "Experiments with four LLMs demonstrate that using profile information improves caption generation, with images proving more beneficial than text when creating captions.", "conclusion": "The study highlights the effectiveness of multimodal profiles in generating captions that closely reflect the authors' styles, advocating for the use of such datasets in future research.", "key_contributions": ["Introduction of the LaMP-Cap dataset for personalized figure caption generation.", "Demonstration of the importance of multimodal profiles for enhancing caption quality.", "Ablation studies showing the superiority of image-based profiles over text-only profiles."], "limitations": "", "keywords": ["personalized caption generation", "multimodal profiles", "figure captions", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.06619", "pdf": "https://arxiv.org/pdf/2506.06619.pdf", "abs": "https://arxiv.org/abs/2506.06619", "title": "BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs", "authors": ["Jesse Woo", "Fateme Hashemi Chaleshtori", "Ana MarasoviÄ", "Kenneth Marino"], "categories": ["cs.CL"], "comment": "ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix", "summary": "A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work.", "AI": {"tldr": "BRIEFME is a new dataset designed for improving legal brief writing and editing by language models.", "motivation": "To address the gap in Legal NLP related to the writing and editing of legal briefs, aiming to assist legal professionals in making persuasive arguments.", "method": "The paper introduces the BRIEFME dataset, which includes three tasks: argument summarization, argument completion, and case retrieval, aimed at evaluating the capabilities of language models in the legal domain.", "result": "Current large language models (LLMs) perform well in argument summarization and completion tasks but struggle with realistic argument completion and case retrieval.", "conclusion": "The findings suggest a need for ongoing development in Legal NLP to support legal practitioners more effectively.", "key_contributions": ["Introduction of the BRIEFME dataset for legal briefs", "Analysis of LLM performance on legal-specific tasks", "Identification of areas where LLMs struggle in Legal NLP"], "limitations": "The dataset and tasks may not fully encapsulate the complexities of legal argumentation in practice.", "keywords": ["Legal NLP", "Legal briefs", "Language models", "Argumentation", "Dataset"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955.pdf", "abs": "https://arxiv.org/abs/2506.06955", "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Koichi Takeda", "Yusuke Miyao", "Pontus Stenetorp", "Qianying Liu", "Su Myat Noe", "Hideyuki Tachibana", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI"], "comment": "This version includes an updated literature review, added\n  acknowledgements, and a revised author list", "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.", "AI": {"tldr": "BIS Reasoning 1.0 is a dataset designed to evaluate belief-inconsistent reasoning in large language models, benchmarked against state-of-the-art models.", "motivation": "To evaluate reasoning biases in large language models (LLMs) using syllogistic reasoning problems that are logically valid but belief-inconsistent.", "method": "Introduced a new dataset of syllogistic reasoning problems and benchmarked performance across various state-of-the-art LLMs, including GPT and Claude models.", "result": "GPT-4o achieved 79.54% accuracy, highlighting significant performance variance among models.", "conclusion": "Findings indicate critical weaknesses in LLMs related to logically valid but belief-conflicting inputs, with implications for their use in sensitive areas such as healthcare and law.", "key_contributions": ["Introduction of the BIS Reasoning 1.0 dataset", "Benchmarking of multiple state-of-the-art LLMs", "Identification of reasoning biases in LLMs"], "limitations": "Limited to evaluating Japanese LLMs; results may vary with additional contexts or languages.", "keywords": ["BIS Reasoning", "syllogistic reasoning", "large language models", "belief-inconsistent reasoning", "benchmarking"], "importance_score": 7, "read_time_minutes": 10}}
