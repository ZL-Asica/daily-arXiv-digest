{"id": "2508.00843", "pdf": "https://arxiv.org/pdf/2508.00843.pdf", "abs": "https://arxiv.org/abs/2508.00843", "title": "Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling", "authors": ["Sumit Kumar", "Sarthak Kapoor", "Harsh Vardhan", "Yao Zhao"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing industries by enhancing\nefficiency, scalability, and innovation. This paper investigates the potential\nof LLMs in automating Computer-Aided Design (CAD) workflows, by integrating\nFreeCAD with LLM as CAD design tool. Traditional CAD processes are often\ncomplex and require specialized sketching skills, posing challenges for rapid\nprototyping and generative design. We propose a framework where LLMs generate\ninitial CAD scripts from natural language descriptions, which are then executed\nand refined iteratively based on error feedback. Through a series of\nexperiments with increasing complexity, we assess the effectiveness of this\napproach. Our findings reveal that LLMs perform well for simple to moderately\ncomplex designs but struggle with highly constrained models, necessitating\nmultiple refinements. The study highlights the need for improved memory\nretrieval, adaptive prompt engineering, and hybrid AI techniques to enhance\nscript robustness. Future directions include integrating cloud-based execution\nand exploring advanced LLM capabilities to further streamline CAD automation.\nThis work underscores the transformative potential of LLMs in design workflows\nwhile identifying critical areas for future development."}
{"id": "2508.00846", "pdf": "https://arxiv.org/pdf/2508.00846.pdf", "abs": "https://arxiv.org/abs/2508.00846", "title": "Cognitive Exoskeleton: Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback", "authors": ["Songlin Xu", "Xinyu Zhang"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we introduce an AI-mediated framework that can provide\nintelligent feedback to augment human cognition. Specifically, we leverage deep\nreinforcement learning (DRL) to provide adaptive time pressure feedback to\nimprove user performance in a math arithmetic task. Time pressure feedback\ncould either improve or deteriorate user performance by regulating user\nattention and anxiety. Adaptive time pressure feedback controlled by a DRL\npolicy according to users' real-time performance could potentially solve this\ntrade-off problem. However, the DRL training and hyperparameter tuning may\nrequire large amounts of data and iterative user studies. Therefore, we propose\na dual-DRL framework that trains a regulation DRL agent to regulate user\nperformance by interacting with another simulation DRL agent that mimics user\ncognition behaviors from an existing dataset. Our user study demonstrates the\nfeasibility and effectiveness of the dual-DRL framework in augmenting user\nperformance, in comparison to the baseline group."}
{"id": "2508.00847", "pdf": "https://arxiv.org/pdf/2508.00847.pdf", "abs": "https://arxiv.org/abs/2508.00847", "title": "GPT Chatbots for Alleviating Anxiety and Depression: A Pilot Randomized Controlled Trial with Afghan Women", "authors": ["Sofia Sahab", "Jawad Haqbeen", "Diksha Sapkota", "Takayuki Ito"], "categories": ["cs.HC", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this study, we investigated the effects of GPT-4, with and without\nspecific conversational instructions, on the mental health of Afghan women.\nThese women face multifaceted challenges, including Taliban-imposed\nrestrictions, societal inequalities, and domestic violence, adversely affecting\ntheir well-being. We conducted a randomized controlled trial with 60\nparticipants, dividing them into three groups: GPT-4, a supportive listener\n(GPT-4 with empathetic engagement instructions), and a waiting list. The\nHospital Anxiety and Depression Scale (HADS) was used to measure anxiety and\ndepression before and after the intervention. Linguistic analysis of chat data\nexamined personal pronouns, tones, emotions, and Language Style Matching (LSM).\nThe supportive listener group showed a significant reduction in HADS scores\ncompared to the other groups. Linguistic analysis revealed a more positive tone\nand higher LSM in the supportive listener group, with a significant negative\ncorrelation between LSM and changes in HADS scores, indicating greater\nlinguistic alignment was linked to reductions in anxiety and depression.\nPerceived empathy ratings were also significantly higher in the supportive\nlistener group. These findings highlight the potential of AI-driven\ninterventions, like GPT-4, in providing accessible mental health support.\nHowever, such interventions should complement traditional psychotherapy,\nensuring a collaborative approach to optimize therapeutic outcomes."}
{"id": "2508.00848", "pdf": "https://arxiv.org/pdf/2508.00848.pdf", "abs": "https://arxiv.org/abs/2508.00848", "title": "RestAware: Non-Invasive Sleep Monitoring Using FMCW Radar and AI-Generated Summaries", "authors": ["Agniva Banerjee", "Bhanu Partap Paregi", "Haroon R. Lone"], "categories": ["cs.HC", "cs.CY", "eess.SP"], "comment": null, "summary": "Monitoring sleep posture and behavior is critical for diagnosing sleep\ndisorders and improving overall sleep quality. However, traditional approaches,\nsuch as wearable devices, cameras, and pressure sensors, often compromise user\ncomfort, fail under obstructions like blankets, and raise privacy concerns. To\novercome these limitations, we present RestAware, a non-invasive, contactless\nsleep monitoring system based on a 24GHz frequency-modulated continuous wave\n(FMCW) radar. Our system is evaluated on 25 participants across eight common\nsleep postures, achieving 92% classification accuracy and an F1-score of 0.91\nusing a K-Nearest Neighbors (KNN) classifier. In addition, we integrate\ninstruction-tuned large language models (Mistral, Llama, and Falcon) to\ngenerate personalized, human-readable sleep summaries from radar-derived\nposture data. This low-cost ($ 35), privacy-preserving solution offers a\npractical alternative for real-time deployment in smart homes and clinical\nenvironments."}
{"id": "2508.00864", "pdf": "https://arxiv.org/pdf/2508.00864.pdf", "abs": "https://arxiv.org/abs/2508.00864", "title": "Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches", "authors": ["Margarita Bugue√±o", "Gerard de Melo"], "categories": ["cs.CL"], "comment": "7 pages, 3 figures, 3 tables. Appendix starts on page 10", "summary": "In document classification, graph-based models effectively capture document\nstructure, overcoming sequence length limitations and enhancing contextual\nunderstanding. However, most existing graph document representations rely on\nheuristics, domain-specific rules, or expert knowledge. Unlike previous\napproaches, we propose a method to learn data-driven graph structures,\neliminating the need for manual design and reducing domain dependence. Our\napproach constructs homogeneous weighted graphs with sentences as nodes, while\nedges are learned via a self-attention model that identifies dependencies\nbetween sentence pairs. A statistical filtering strategy aims to retain only\nstrongly correlated sentences, improving graph quality while reducing the graph\nsize. Experiments on three document classification datasets demonstrate that\nlearned graphs consistently outperform heuristic-based graphs, achieving higher\naccuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness\nof the statistical filtering in improving classification robustness. These\nresults highlight the potential of automatic graph generation over traditional\nheuristic approaches and open new directions for broader applications in NLP."}
{"id": "2508.00850", "pdf": "https://arxiv.org/pdf/2508.00850.pdf", "abs": "https://arxiv.org/abs/2508.00850", "title": "Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability", "authors": ["Nadja R. Ging-Jehli", "Russell K. Childers", "Joshua Lu", "Robert Gemma", "Rachel Zhu"], "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "How do we learn when to persist, when to let go, and when to shift gears?\nGearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed\nto model how humans and artificial agents adapt to shifting environment\ndemands. Grounded in cognitive neuroscience, computational psychiatry,\neconomics, and artificial intelligence, Supertasks combine computational\nneurocognitive modeling with serious gaming. This creates a dynamic,\nmulti-mission environment engineered to assess mechanisms of adaptive behavior\nacross cognitive and social contexts. Computational parameters explain behavior\nand probe mechanisms by controlling the game environment. Unlike traditional\ntasks, GF enables neurocognitive modeling of individual differences across\nperceptual decisions, learning, and meta-cognitive levels. This positions GF as\na flexible testbed for understanding how cognitive-affective control processes,\nlearning styles, strategy use, and motivational shifts adapt across contexts\nand over time. It serves as an experimental platform for scientists, a\nphenotype-to-mechanism intervention for clinicians, and a training tool for\nplayers aiming to strengthen self-regulated learning, mood, and stress\nresilience. Online study (n = 60, ongoing) results show that GF recovers\neffects from traditional neuropsychological tasks (construct validity),\nuncovers novel patterns in how learning differs across contexts and how\nclinical features map onto distinct adaptations. These findings pave the way\nfor developing in-game interventions that foster self-efficacy and agency to\ncope with real-world stress and uncertainty. GF builds a new adaptive ecosystem\ndesigned to accelerate science, transform clinical care, and foster individual\ngrowth. It offers a mirror and training ground where humans and machines\nco-develop together deeper flexibility and awareness."}
{"id": "2508.00889", "pdf": "https://arxiv.org/pdf/2508.00889.pdf", "abs": "https://arxiv.org/abs/2508.00889", "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts", "authors": ["Hagyeong Shin", "Binoy Robin Dalal", "Iwona Bialynicka-Birula", "Navjot Matharu", "Ryan Muir", "Xingwei Yang", "Samuel W. K. Wong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for an oral presentation at Agentic & GenAI Evaluation KDD\n  2025: KDD workshop on Evaluation and Trustworthiness of Agentic and\n  Generative AI Models", "summary": "Large language models (LLMs) are known to hallucinate, producing natural\nlanguage outputs that are not grounded in the input, reference materials, or\nreal-world knowledge. In enterprise applications where AI features support\nbusiness decisions, such hallucinations can be particularly detrimental. LLMs\nthat analyze and summarize contact center conversations introduce a unique set\nof challenges for factuality evaluation, because ground-truth labels often do\nnot exist for analytical interpretations about sentiments captured in the\nconversation and root causes of the business problems. To remedy this, we first\nintroduce a \\textbf{3D} -- \\textbf{Decompose, Decouple, Detach} -- paradigm in\nthe human annotation guideline and the LLM-judges' prompt to ground the\nfactuality labels in linguistically-informed evaluation criteria. We then\nintroduce \\textbf{FECT}, a novel benchmark dataset for \\textbf{F}actuality\n\\textbf{E}valuation of Interpretive AI-Generated \\textbf{C}laims in Contact\nCenter Conversation \\textbf{T}ranscripts, labeled under our 3D paradigm.\nLastly, we report our findings from aligning LLM-judges on the 3D paradigm.\nOverall, our findings contribute a new approach for automatically evaluating\nthe factuality of outputs generated by an AI system for analyzing contact\ncenter conversations."}
{"id": "2508.00852", "pdf": "https://arxiv.org/pdf/2508.00852.pdf", "abs": "https://arxiv.org/abs/2508.00852", "title": "Visuo-Acoustic Hand Pose and Contact Estimation", "authors": ["Yuemin Ma", "Uksang Yoo", "Yunchao Yao", "Shahram Najam Syed", "Luca Bondi", "Jonathan Francis", "Jean Oh", "Jeffrey Ichnowski"], "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Accurately estimating hand pose and hand-object contact events is essential\nfor robot data-collection, immersive virtual environments, and biomechanical\nanalysis, yet remains challenging due to visual occlusion, subtle contact cues,\nlimitations in vision-only sensing, and the lack of accessible and flexible\ntactile sensing. We therefore introduce VibeMesh, a novel wearable system that\nfuses vision with active acoustic sensing for dense, per-vertex hand contact\nand pose estimation. VibeMesh integrates a bone-conduction speaker and sparse\npiezoelectric microphones, distributed on a human hand, emitting structured\nacoustic signals and capturing their propagation to infer changes induced by\ncontact. To interpret these cross-modal signals, we propose a graph-based\nattention network that processes synchronized audio spectra and RGB-D-derived\nhand meshes to predict contact with high spatial resolution. We contribute: (i)\na lightweight, non-intrusive visuo-acoustic sensing platform; (ii) a\ncross-modal graph network for joint pose and contact inference; (iii) a dataset\nof synchronized RGB-D, acoustic, and ground-truth contact annotations across\ndiverse manipulation scenarios; and (iv) empirical results showing that\nVibeMesh outperforms vision-only baselines in accuracy and robustness,\nparticularly in occluded or static-contact settings."}
{"id": "2508.00924", "pdf": "https://arxiv.org/pdf/2508.00924.pdf", "abs": "https://arxiv.org/abs/2508.00924", "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML", "authors": ["Ernesto L. Estevanell-Valladares", "Suilan Estevez-Velarde", "Yoan Guti√©rrez", "Andr√©s Montoyo", "Ruslan Mitkov"], "categories": ["cs.CL", "68T05, 68T50", "I.2.6; I.2.7; I.2.8"], "comment": "17 pages, 10 figures, 7 tables. Preprint. Under review at EMNLP 2025.\n  This is not the final version", "summary": "Experts in machine learning leverage domain knowledge to navigate decisions\nin model selection, hyperparameter optimisation, and resource allocation. This\nis particularly critical for fine-tuning language models (LMs), where repeated\ntrials incur substantial computational overhead and environmental impact.\nHowever, no existing automated framework simultaneously tackles the entire\nmodel selection and HPO task for resource-efficient LM fine-tuning. We\nintroduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past\nexperiences to optimise discriminative and generative LM fine-tuning pipelines\nefficiently. XAutoLM learns from stored successes and failures by extracting\ntask- and system-level meta-features to bias its sampling toward fruitful\nconfigurations and away from costly dead ends. On four text classification and\ntwo question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak\nF1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error\nratios by up to sevenfold, and uncovers up to 50% more pipelines above the\nzero-shot Pareto front. In contrast, simpler memory-based baselines suffer\nnegative transfer. We release XAutoLM and our experience store to catalyse\nresource-efficient, Green AI fine-tuning in the NLP community."}
{"id": "2508.00856", "pdf": "https://arxiv.org/pdf/2508.00856.pdf", "abs": "https://arxiv.org/abs/2508.00856", "title": "EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities", "authors": ["Steph Grohmann"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "In biomedical science, review by a Research Ethics Committee (REC) is an\nindispensable way of protecting human subjects from harm. However, in social\nscience and the humanities, mandatory ethics compliance has long been met with\nscepticism as biomedical models of ethics can map poorly onto methodologies\ninvolving complex socio-political and cultural considerations. As a result,\ntailored ethics training and support as well as access to RECs with the\nnecessary expertise is lacking in some areas, including parts of Europe and\nlow- and middle-income countries. This paper suggests that Generative AI can\nmeaningfully contribute to closing these gaps, illustrating this claim by\npresenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics\nsupport system for social science and humanities researchers. Drawing on\nconstitutional AI technology and a collaborative prompt development\nmethodology, EthicAlly provides structured ethics assessment that incorporates\nboth universal ethics principles and contextual and interpretive considerations\nrelevant to most social science research. In supporting researchers in ethical\nresearch design and preparation for REC submission, this kind of system can\nalso contribute to easing the burden on institutional RECs, without attempting\nto automate or replace human ethical oversight."}
{"id": "2508.01005", "pdf": "https://arxiv.org/pdf/2508.01005.pdf", "abs": "https://arxiv.org/abs/2508.01005", "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation", "authors": ["Yiqun Chen", "Erhan Zhang", "Lingyong Yan", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Jiaxin Mao"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG."}
{"id": "2508.00929", "pdf": "https://arxiv.org/pdf/2508.00929.pdf", "abs": "https://arxiv.org/abs/2508.00929", "title": "Accessibility and Social Inclusivity: A Literature Review of Music Technology for Blind and Low Vision People", "authors": ["Shumeng Zhang", "Raul Masu", "Mela Bettega", "Mingming Fan"], "categories": ["cs.HC", "cs.CY", "cs.SD", "eess.AS"], "comment": "Accepted by ASSETS'25 - The 27th International ACM SIGACCESS\n  Conference on Computers and Accessibility", "summary": "This paper presents a systematic literature review of music technology\ntailored for blind and low vision (BLV) individuals. Music activities can be\nparticularly beneficial for BLV people. However, a systematic approach to\norganizing knowledge on designing accessible technology for BLV people has yet\nto be attempted. We categorize the existing studies based on the type of\ntechnology and the extent of BLV people's involvement in the research. We\nidentify six main categories of BLV people-oriented music technology and\nhighlight four key trends in design goals. Based on these categories, we\npropose four general insights focusing on (1) spatial awareness, (2) access to\ninformation, (3) (non-verbal) communication, and (4) memory. The identified\ntrends suggest that more empirical studies involving BLV people in real-world\nscenarios are needed to ensure that technological advancements can enhance\nmusical experiences and social inclusion. This research proposes collaborative\nmusic technology and inclusive real-world testing with the target group as two\nkey areas missing in current research. They serve as a foundational step in\nshifting the focus from ``accessible technology'' to ``inclusive technology''\nfor BLV individuals within the broader field of accessibility research."}
{"id": "2508.01006", "pdf": "https://arxiv.org/pdf/2508.01006.pdf", "abs": "https://arxiv.org/abs/2508.01006", "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu", "authors": ["Farah Adeeba", "Brian Dillon", "Hassan Sajjad", "Rajesh Bhatt"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages."}
{"id": "2508.01070", "pdf": "https://arxiv.org/pdf/2508.01070.pdf", "abs": "https://arxiv.org/abs/2508.01070", "title": "How Long Does It Take to Alleviate Discomfort? A Preliminary Study on Reducing Cybersickness in Novice Users", "authors": ["Zhengxin Zhang", "Shufang Qian", "Yi Wang", "Xiao Liu", "Thuong Hoang", "Chetan Arora", "Jingjing Zhang", "Henry Been Lirn Duh"], "categories": ["cs.HC"], "comment": null, "summary": "Cybersickness significantly impacts the user experience in VR applications.\nLocomotion tunneling is a widely adopted technique for mitigating cybersickness\nin susceptible users. However, there is a lack of research investigating the\neffects of prolonged use of locomotion tunneling among novice users. To fill\nthis gap, we used VRChat as our experimental platform. We recruited 24 novice\nVR users, defined as participants with no prior experience using immersive\nvirtual environments. We collected five days of data within a one-week period.\nThe results indicated that participants exhibited significant mitigation to\ncybersickness by Day 4. However, a change in the VR scene on Day 5 led to a\nnotable increase in cybersickness symptoms. Qualitative feedback revealed\nparticipant-perceived causes of cybersickness and suggested that the\neffectiveness of locomotion tunneling was limited in some scenarios. Finally,\nwe discussed the limitations of the study and proposed directions for future\nresearch."}
{"id": "2508.01096", "pdf": "https://arxiv.org/pdf/2508.01096.pdf", "abs": "https://arxiv.org/abs/2508.01096", "title": "Cross-Domain Web Information Extraction at Pinterest", "authors": ["Michael Farag", "Patrick Halina", "Andrey Zaytsev", "Alekhya Munagala", "Imtihan Ahmed", "Junhao Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives."}
{"id": "2508.01092", "pdf": "https://arxiv.org/pdf/2508.01092.pdf", "abs": "https://arxiv.org/abs/2508.01092", "title": "DescribePro: Collaborative Audio Description with Human-AI Interaction", "authors": ["Maryam Cheema", "Sina Elahimanesh", "Samuel Martin", "Pooyan Fazli", "Hasti Seifi"], "categories": ["cs.HC"], "comment": "ASSETS 25 19 pages, 8 figures", "summary": "Audio description (AD) makes video content accessible to millions of blind\nand low vision (BLV) users. However, creating high-quality AD involves a\ntrade-off between the precision of human-crafted descriptions and the\nefficiency of AI-generated ones. To address this, we present DescribePro a\ncollaborative AD authoring system that enables describers to iteratively refine\nAI-generated descriptions through multimodal large language model prompting and\nmanual editing. DescribePro also supports community collaboration by allowing\nusers to fork and edit existing ADs, enabling the exploration of different\nnarrative styles. We evaluate DescribePro with 18 describers (9 professionals\nand 9 novices) using quantitative and qualitative methods. Results show that AI\nsupport reduces repetitive work while helping professionals preserve their\nstylistic choices and easing the cognitive load for novices. Collaborative tags\nand variations show potential for providing customizations, version control,\nand training new describers. These findings highlight the potential of\ncollaborative, AI-assisted tools to enhance and scale AD authorship."}
{"id": "2508.01159", "pdf": "https://arxiv.org/pdf/2508.01159.pdf", "abs": "https://arxiv.org/abs/2508.01159", "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates", "authors": ["Liam G. McCoy", "Fateme Nateghi Haredasht", "Kanav Chopra", "David Wu", "David JH Wu", "Abass Conteh", "Sarita Khemani", "Saloni Kumar Maharaj", "Vishnu Ravi", "Arth Pahwa", "Yingjie Weng", "Leah Rosengaus", "Lena Giang", "Kelvin Zhenghao Li", "Olivia Jee", "Daniel Shirvani", "Ethan Goh", "Jonathan H. Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication."}
{"id": "2508.01110", "pdf": "https://arxiv.org/pdf/2508.01110.pdf", "abs": "https://arxiv.org/abs/2508.01110", "title": "Cross-Device Motion Interaction via Apple's Native System Frameworks", "authors": ["Ezequiel Santos"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce an open-source, fully offline pipeline that transforms a\nconsumer-grade iPhone into a motion controller with real-time tactile feedback,\nusing only native Apple frameworks. Designed for rapid prototyping and applied\nmobile HCI scenarios, the system integrates CoreMotion for inertial sensing,\nMultipeerConnectivity for peer-to-peer data transmission at 10 Hz, and\nCoreHaptics for immediate tactile confirmation. A built-in logger captures\nend-to-end latency without requiring clock synchronization, yielding a mean\ndelay of 70.4 ms and 95th percentile below 74 ms on typical 5 GHz Wi-Fi (-55\ndBm RSSI). We validated the pipeline through a real-time demonstrator game,\nKeepCalm, deployed during a public event with 21 participants. Results showed\nstable connections, zero packet loss, and negligible power impact (24 mW on\niPhone 13 mini). With fewer than 500 lines of Swift code and no reliance on\ncloud infrastructure, this system provides a compact, reproducible foundation\nfor embodied interaction research, casual games, and offline educational tools.\nAll source code, latency logs, and provisioning scripts are openly released\nunder an MIT license."}
{"id": "2508.01161", "pdf": "https://arxiv.org/pdf/2508.01161.pdf", "abs": "https://arxiv.org/abs/2508.01161", "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages", "authors": ["Jiyu Chen", "Necva B√∂l√ºc√º", "Sarvnaz Karimi", "Diego Moll√°", "C√©cile L. Paris"], "categories": ["cs.CL"], "comment": "In Proceedings of the 19th International Workshop on Semantic\n  Evaluation (SemEval-2025), Vienna, Austria. Association for Computational\n  Linguistics", "summary": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language."}
{"id": "2508.01155", "pdf": "https://arxiv.org/pdf/2508.01155.pdf", "abs": "https://arxiv.org/abs/2508.01155", "title": "Presentation of Low-Frequency Vibration to the Face Using Amplitude Modulation", "authors": ["Yuma Akiba", "Shota Nakayama", "Keigo Ushiyama", "Izumi Mizoguchi", "Hiroyuki Kajimoto"], "categories": ["cs.HC"], "comment": "Submitted version, in IEEE Transactions on Haptics, 2025", "summary": "This study proposes a method to present pure low-frequency vibration\nsensations to the face that cannot be presented by small commercially available\nvibrators. The core innovation lies in utilizing an amplitude modulation\ntechnique with a carrier frequency of approximately 200 Hz. Due to the absence\nof Pacinian corpuscles in the facial region - receptors responsible for\ndetecting high-frequency vibrations around 200 Hz - only the original\nlow-frequency signal is perceived. Three experiments were conducted.\nExperiments 1 and 2 were performed on the forehead to confirm that the proposed\namplitude modulation method could produce the desired low-frequency perception\nand to evaluate the subjective quality of the vibration. The results suggested\nthat the proposed method could produce the perception of desired pure\nlow-frequency vibration when applied to the forehead. In Experiment 3, the\nproposed method was applied to the whole face, and its range of applicability\nwas explored. The results indicated that the original low-frequency vibration\nwas clearly perceptible around the eyes, cheeks, and lower lip area."}
{"id": "2508.01198", "pdf": "https://arxiv.org/pdf/2508.01198.pdf", "abs": "https://arxiv.org/abs/2508.01198", "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios."}
{"id": "2508.01165", "pdf": "https://arxiv.org/pdf/2508.01165.pdf", "abs": "https://arxiv.org/abs/2508.01165", "title": "RoboLinker: A Diffusion-model-based Matching Clothing Generator Between Humans and Companion Robots", "authors": ["Jing Tang", "Qing Xiao", "Kunxu Du", "Zaiqiao Ye"], "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": "3 pages, 3 figures, accepted by UIST Adjunct'25", "summary": "We present RoboLinker, a generative design system that creates matching\noutfits for humans and their robots. Using a diffusion-based model, the system\ntakes a robot image and a style prompt from users as input, and outputs a human\noutfit that visually complements the robot's attire. Through an interactive\ninterface, users can refine the generated designs. We evaluate RoboLinker with\nboth humanoid and pet-like robots, demonstrating its capacity to produce\nstylistically coherent and emotionally resonant results."}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213.pdf", "abs": "https://arxiv.org/abs/2508.01213", "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level."}
{"id": "2508.01235", "pdf": "https://arxiv.org/pdf/2508.01235.pdf", "abs": "https://arxiv.org/abs/2508.01235", "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration", "authors": ["Yaxin Hu", "Arissa J. Sato", "Jingxin Du", "Chenming Ye", "Anjun Zhu", "Pragathi Praveena", "Bilge Mutlu"], "categories": ["cs.HC", "cs.RO", "68"], "comment": null, "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments."}
{"id": "2508.01222", "pdf": "https://arxiv.org/pdf/2508.01222.pdf", "abs": "https://arxiv.org/abs/2508.01222", "title": "WebDS: An End-to-End Benchmark for Web-based Data Science", "authors": ["Ethan Hsu", "Hong Meng Yam", "Ines Bouissou", "Aaron Murali John", "Raj Thota", "Josh Koe", "Vivek Sarath Putta", "G K Dharesan", "Alexander Spangher", "Shikhar Murty", "Tenghao Huang", "Christopher D. Manning"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science."}
{"id": "2508.01279", "pdf": "https://arxiv.org/pdf/2508.01279.pdf", "abs": "https://arxiv.org/abs/2508.01279", "title": "ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts", "authors": ["Jiajun Zhu", "Xinyu Cheng", "Zhongsu Luo", "Yunfan Zhou", "Xinhuan Shu", "Di Weng", "Yingcai Wu"], "categories": ["cs.HC"], "comment": "Accepted at Annual ACM Symposium on User Interface Software and\n  Technology (UIST'25), September 28-October 1, 2025, Busan, Republic of Korea", "summary": "Large language models (LLMs) enable the rapid generation of data wrangling\nscripts based on natural language instructions, but these scripts may not fully\nadhere to user-specified requirements, necessitating careful inspection and\niterative refinement. Existing approaches primarily assist users in\nunderstanding script logic and spotting potential issues themselves, rather\nthan providing direct validation of correctness. To enhance debugging\nefficiency and optimize the user experience, we develop ViseGPT, a tool that\nautomatically extracts constraints from user prompts to generate comprehensive\ntest cases for verifying script reliability. The test results are then\ntransformed into a tailored Gantt chart, allowing users to intuitively assess\nalignment with semantic requirements and iteratively refine their scripts. Our\ndesign decisions are informed by a formative study (N=8) that explores user\npractices and challenges. We further evaluate the effectiveness and usability\nof ViseGPT through a user study (N=18). Results indicate that ViseGPT\nsignificantly improves debugging efficiency for LLM-generated data-wrangling\nscripts, enhances users' ability to detect and correct issues, and streamlines\nthe workflow experience."}
{"id": "2508.01245", "pdf": "https://arxiv.org/pdf/2508.01245.pdf", "abs": "https://arxiv.org/abs/2508.01245", "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework", "authors": ["Yue Chen", "Minghua He", "Fangkai Yang", "Pu Zhao", "Lu Wang", "Yu Kang", "Yifei Dong", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability."}
{"id": "2508.01282", "pdf": "https://arxiv.org/pdf/2508.01282.pdf", "abs": "https://arxiv.org/abs/2508.01282", "title": "ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance", "authors": ["Jiawei Li", "Linjie Qiu", "Zhiqing Wu", "Qiongyan Chen", "Ziyan Wang", "Mingming Fan"], "categories": ["cs.HC"], "comment": "10 pages, 5 figures, Proceedings of the 33rd ACM International\n  Conference on Multimedia", "summary": "Older adults tend to encounter challenges when learning to use new smartphone\napps due to age-related cognitive and physical changes. Compared to traditional\nsupport methods such as video tutorials, trial-and-error allows older adults to\nlearn to use smartphone apps by making and correcting mistakes. However, it\nremains unknown how trial-and-error should be designed to empower older adults\nto use smartphone apps and how well it would work for older adults. Informed by\nthe guidelines derived from prior work, we designed and implemented ExplorAR,\nan AR-based trial-and-error system that offers real-time and situated visual\nguidance in the augmented space around the smartphone to empower older adults\nto explore and correct mistakes independently. We conducted a user study with\n18 older adults to compare ExplorAR with traditional video tutorials and a\nsimplified version of ExplorAR. Results show that the AR-supported\ntrial-and-error method enhanced older adults' learning experience by fostering\ndeeper cognitive engagement and improving confidence in exploring unknown\noperations."}
{"id": "2508.01263", "pdf": "https://arxiv.org/pdf/2508.01263.pdf", "abs": "https://arxiv.org/abs/2508.01263", "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "categories": ["cs.CL"], "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives."}
{"id": "2508.01318", "pdf": "https://arxiv.org/pdf/2508.01318.pdf", "abs": "https://arxiv.org/abs/2508.01318", "title": "AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Emotion Recognition", "authors": ["Zheng Lian"], "categories": ["cs.HC"], "comment": null, "summary": "Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict\nemotions without being constrained by predefined label spaces, enabling\nfine-grained and human-like emotion understanding. Unlike traditional\ndiscriminative methods, OV-MER leverages generative models, such as large\nlanguage models (LLMs) with extensive vocabularies, to capture the full\nspectrum of emotions. Previous approaches (like AffectGPT) primarily rely on\ntoken-level loss for training. However, this objective does not align with the\nemotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately,\nEW-based metrics cannot be directly optimized via gradient backpropagation. In\nthis paper, we propose AffectGPT-R1, a reinforcement learning framework that\ndirectly optimizes performance on EW-based metrics. Specifically, we treat\nthese metrics as the reward function and employ Group Relative Policy\nOptimization (GRPO) to maximize rewards. Experimental results demonstrate that\nAffectGPT-R1 achieves significant improvements on OV-MER. We hope this work\nadvances the field of multimodal emotion recognition. Our code will be publicly\navailable at:https://github.com/zeroQiaoba/AffectGPT."}
{"id": "2508.01290", "pdf": "https://arxiv.org/pdf/2508.01290.pdf", "abs": "https://arxiv.org/abs/2508.01290", "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Yanyan Wang", "Hongye Tan", "Jiye Liang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information."}
{"id": "2508.01388", "pdf": "https://arxiv.org/pdf/2508.01388.pdf", "abs": "https://arxiv.org/abs/2508.01388", "title": "An Appraisal-Based Approach to Human-Centred Explanations", "authors": ["Rukshani Somarathna", "Madhawa Perera", "Tom Gedeon", "Matt Adcock"], "categories": ["cs.HC"], "comment": null, "summary": "Explainability remains a critical challenge in artificial intelligence (AI)\nsystems, particularly in high stakes domains such as healthcare, finance, and\ndecision support, where users must understand and trust automated reasoning.\nTraditional explainability methods such as feature importance and post-hoc\njustifications often fail to capture the cognitive processes that underlie\nhuman decision making, leading to either too technical or insufficiently\nmeaningful explanations. We propose a novel appraisal based framework inspired\nby the Component Process Model (CPM) for explainability to address this gap.\nWhile CPM has traditionally been applied to emotion research, we use its\nappraisal component as a cognitive model for generating human aligned\nexplanations. By structuring explanations around key appraisal dimensions such\nas relevance, implications, coping potential, and normative significance our\nframework provides context sensitive, cognitively meaningful justifications for\nAI decisions. This work introduces a new paradigm for generating intuitive,\nhuman-centred explanations in AI driven systems by bridging cognitive science\nand explainable AI."}
{"id": "2508.01302", "pdf": "https://arxiv.org/pdf/2508.01302.pdf", "abs": "https://arxiv.org/abs/2508.01302", "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment."}
{"id": "2508.01520", "pdf": "https://arxiv.org/pdf/2508.01520.pdf", "abs": "https://arxiv.org/abs/2508.01520", "title": "Unlocking Excellence: The Impact of Voucher Incentives on Cybersecurity Education", "authors": ["Jianhua Li", "Shang Gao", "Michelle Harvey", "Trina Myers"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "While voucher incentives have been popular for primary and secondary schools,\nthey are less used in higher education. In this study, we leverage industry\nvoucher incentives to inspire students in cybersecurity education (CSE). We\nadopt a 100% portfolio-based assessment strategy, where students can freely\nselect their target grades in the investigated unit. We purposely design one of\nthe high distinction (HD) tasks to be obtaining an industry certificate and\nprovide vouchers to those who can accomplish a predefined set of tasks before a\nmidpoint. The voucher recipients will use the voucher to access the industry\ncertificate training materials and sit the certificate exam for free. Passing\nthe certificate exam is one of the conditions for gaining an HD grade. Our\nsurvey and interviews reveal a substantial influence of voucher incentives on\nstudents' career aspirations. In light of the findings, recommendations on\nadopting voucher incentives in CSE or broader ICT education are offered for\ninstitutions and researchers."}
{"id": "2508.01309", "pdf": "https://arxiv.org/pdf/2508.01309.pdf", "abs": "https://arxiv.org/abs/2508.01309", "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation", "authors": ["Weibo Zhou", "Lingbo Li", "Shangsong Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains."}
{"id": "2508.01547", "pdf": "https://arxiv.org/pdf/2508.01547.pdf", "abs": "https://arxiv.org/abs/2508.01547", "title": "Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice", "authors": ["Yongsu Ahn", "Nam Wook Kim"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper investigates why recent generative AI models outperform humans in\ndata visualization knowledge tasks. Through systematic comparative analysis of\nresponses to visualization questions, we find that differences exist between\ntwo ChatGPT models and human outputs over rhetorical structure, knowledge\nbreadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more\nadvanced model, displays a hybrid of characteristics from both humans and\nChatGPT-3.5. The two models were generally favored over human responses, while\ntheir strengths in coverage and breadth, and emphasis on technical and\ntask-oriented visualization feedback collectively shaped higher overall\nquality. Based on our findings, we draw implications for advancing user\nexperiences based on the potential of LLMs and human perception over their\ncapabilities, with relevance to broader applications of AI."}
{"id": "2508.01317", "pdf": "https://arxiv.org/pdf/2508.01317.pdf", "abs": "https://arxiv.org/abs/2508.01317", "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales."}
{"id": "2508.01553", "pdf": "https://arxiv.org/pdf/2508.01553.pdf", "abs": "https://arxiv.org/abs/2508.01553", "title": "How Many Times Do People Usually Experience Different Kinds of Stressors Each Day?", "authors": ["Sameer Neupane", "Mithun Saha", "David M. Almeida", "Santosh Kumar"], "categories": ["cs.HC"], "comment": "In Companion of the 2025 ACM International Joint Conference on\n  Pervasive and Ubiquitous Computing (UbiComp Companion '25)", "summary": "Understanding how frequently people experience different kinds of daily\nstressors is crucial for interpreting stress exposure and informing mental\nhealth care. But it can't be directly estimated from current assessment\nmethods, such as diaries, end-of-day interviews, and ecological momentary\nassessments (EMA), that use sparse sampling to limit participant burden, and a\nstructured response format for uniformity. In this paper, we utilize stressor\ndata collected in a 100-day field study with 68 participants that adopted\nwearable-triggered prompts and a freeform format to solicit stressors soon\nafter they occurred, but limited its prompts to a small subset to keep the\nburden low. We develop asymptotic models to estimate the latent frequency of\ndifferent kinds of real-life stressors that address sample sparsity and\nsampling bias. We find that people experience 5.39 stressors per day, on\naverage. The top three are related to work (1.76/day), health (0.59/day), and\ntransportation (0.55/day). These estimates offer a principled benchmark for\ninterpreting individual stressor loads. They can also inform mental health care\ntreatments and interventions by establishing population-level baselines."}
{"id": "2508.01326", "pdf": "https://arxiv.org/pdf/2508.01326.pdf", "abs": "https://arxiv.org/abs/2508.01326", "title": "Large-Scale Diverse Synthesis for Mid-Training", "authors": ["Xuemiao Zhang", "Chengying Tu", "Can Ren", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale."}
{"id": "2508.01743", "pdf": "https://arxiv.org/pdf/2508.01743.pdf", "abs": "https://arxiv.org/abs/2508.01743", "title": "Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation", "authors": ["Shiyao Zhang", "Omar Faruk", "Robert Porzel", "Dennis K√ºster", "Tanja Schultz", "Hui Liu"], "categories": ["cs.HC"], "comment": "\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "An increasing number of online interaction settings now provide the\npossibility to visually represent oneself via an animated avatar instead of a\nvideo stream. Benefits include protecting the communicator's privacy while\nstill providing a means to express their individuality. In consequence, there\nhas been a surge in means for avatar-based personalization, ranging from\nclassic human representations to animals, food items, and more. However, using\navatars also has drawbacks. Depending on the human-likeness of the avatar and\nthe corresponding disparities between the avatar and the original expresser,\navatars may elicit discomfort or even hinder effective nonverbal communication\nby distorting emotion perception. This study examines the relationship between\nthe human-likeness of virtual avatars and emotion perception for Ekman's six\n\"basic emotions\". Research reveals that avatars with varying degrees of\nhuman-likeness have distinct effects on emotion perception. High human-likeness\navatars, such as human avatars, tend to elicit more negative emotional\nresponses from users, a phenomenon that is consistent with the concept of\nUncanny Valley in aesthetics, which suggests that closely resembling humans can\nprovoke negative emotional responses. Conversely, a raccoon avatar and a shark\navatar, known as cuteness, which exhibit moderate human similarity in this\nstudy, demonstrate a positive influence on emotion perception. Our initial\nresults suggest that the human-likeness of avatars is an important factor for\nemotion perception. The results from the follow-up study further suggest that\nthe cuteness of avatars and their natural facial status may also play a\nsignificant role in emotion perception and elicitation. We discuss practical\nimplications for strategically conveying specific human behavioral messages\nthrough avatars in multiple applications, such as business and counseling."}
{"id": "2508.01370", "pdf": "https://arxiv.org/pdf/2508.01370.pdf", "abs": "https://arxiv.org/abs/2508.01370", "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights."}
{"id": "2508.01765", "pdf": "https://arxiv.org/pdf/2508.01765.pdf", "abs": "https://arxiv.org/abs/2508.01765", "title": "HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion", "authors": ["Kaining Zhang", "Catarina Moreira", "Pedro Belchior", "Gun Lee", "Mark Billinghurst", "Joaquim Jorge"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "We introduce \\textit{HeadZoom}, a hands-free interaction technique for\nnavigating two-dimensional visual content using head movements. The system\nenables fluid zooming and panning by only using real-time head tracking. It\nsupports natural control in applications such as map exploration, radiograph\ninspection, and image browsing, particularly where physical interaction is\nlimited. We evaluated HeadZoom in a within-subjects user study comparing three\ninteraction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial,\nerror, and subjective metrics. Results show that Parallel Zoom significantly\nreduced total head movement compared to Static and Tilt modes. Users reported\nsignificantly lower perceived exertion for Parallel Zoom, confirming its\nsuitability for prolonged or precision-based tasks. By minimising movement\ndemands while maintaining task effectiveness, HeadZoom advances the design of\nhead-based 2D interaction in VR, creating new opportunities for immersive,\naccessible, and hands-free systems for image exploration."}
{"id": "2508.01401", "pdf": "https://arxiv.org/pdf/2508.01401.pdf", "abs": "https://arxiv.org/abs/2508.01401", "title": "MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs", "authors": ["Ahmad Rezaie Mianroodi", "Amirali Rezaie", "Niko Grisel Todorov", "Cyril Rakovski", "Frank Rudzicz"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages excluding references and appendices", "summary": "Physicians spend significant time documenting clinical encounters, a burden\nthat contributes to professional burnout. To address this, robust automation\ntools for medical documentation are crucial. We introduce MedSynth -- a novel\ndataset of synthetic medical dialogues and notes designed to advance the\nDialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.\nInformed by an extensive analysis of disease distributions, this dataset\nincludes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We\ndemonstrate that our dataset markedly enhances the performance of models in\ngenerating medical notes from dialogues, and dialogues from medical notes. The\ndataset provides a valuable resource in a field where open-access,\nprivacy-compliant, and diverse training data are scarce. Code is available at\nhttps://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available\nat https://huggingface.co/datasets/Ahmad0067/MedSynth."}
{"id": "2508.01789", "pdf": "https://arxiv.org/pdf/2508.01789.pdf", "abs": "https://arxiv.org/abs/2508.01789", "title": "Sonify Anything: Towards Context-Aware Sonic Interactions in AR", "authors": ["Laura Sch√ºtz", "Sasan Matinfar", "Ulrich Eck", "Daniel Roth", "Nassir Navab"], "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5; H.5.2; H.5.1; I.3.5"], "comment": null, "summary": "In Augmented Reality (AR), virtual objects interact with real objects.\nHowever, the lack of physicality of virtual objects leads to the absence of\nnatural sonic interactions. When virtual and real objects collide, either no\nsound or a generic sound is played. Both lead to an incongruent multisensory\nexperience, reducing interaction and object realism. Unlike in Virtual Reality\n(VR) and games, where predefined scenes and interactions allow for the playback\nof pre-recorded sound samples, AR requires real-time sound synthesis that\ndynamically adapts to novel contexts and objects to provide audiovisual\ncongruence during interaction. To enhance real-virtual object interactions in\nAR, we propose a framework for context-aware sounds using methods from computer\nvision to recognize and segment the materials of real objects. The material's\nphysical properties and the impact dynamics of the interaction are used to\ngenerate material-based sounds in real-time using physical modelling synthesis.\nIn a user study with 24 participants, we compared our congruent material-based\nsounds to a generic sound effect, mirroring the current standard of\nnon-context-aware sounds in AR applications. The results showed that\nmaterial-based sounds led to significantly more realistic sonic interactions.\nMaterial-based sounds also enabled participants to distinguish visually similar\nmaterials with significantly greater accuracy and confidence. These findings\nshow that context-aware, material-based sonic interactions in AR foster a\nstronger sense of realism and enhance our perception of real-world\nsurroundings."}
{"id": "2508.01411", "pdf": "https://arxiv.org/pdf/2508.01411.pdf", "abs": "https://arxiv.org/abs/2508.01411", "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations", "authors": ["Rania Al-Sabbagh"], "categories": ["cs.CL"], "comment": null, "summary": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts."}
{"id": "2508.01837", "pdf": "https://arxiv.org/pdf/2508.01837.pdf", "abs": "https://arxiv.org/abs/2508.01837", "title": "When not to help: planning for lasting human-AI collaboration", "authors": ["Mark Steyvers", "Lukas Mayer"], "categories": ["cs.HC"], "comment": null, "summary": "AI systems and technologies that can interact with humans in real time face a\ncommunication dilemma: when to offer assistance and how frequently. Overly\nfrequent or contextually redundant assistance can cause users to disengage,\nundermining the long-term benefits of AI assistance. We introduce a cognitive\nmodeling framework based on Partially Observable Markov Decision Processes\n(POMDPs) that addresses this timing challenge by inferring a user's latent\ncognitive state related to AI engagement over time. Additionally, our framework\nincorporates reasoning about the long-term effects of AI assistance, explicitly\naiming to avoid actions that could lead the human user to disengage or\ndeactivate the AI. A key component of our approach is counterfactual reasoning:\nat each time step, the AI considers how well the user would perform\nindependently and weighs the potential boost in performance against the risk of\ndiminishing engagement with the AI. Through simulations, we show that this\nadaptive strategy significantly outperforms baseline policies in which\nassistance is always provided or never provided. Our results highlight the\nimportance of balancing short-term decision accuracy with sustained user\nengagement, showing how communication strategies can be optimized to avoid\nalert fatigue while preserving the user's receptiveness to AI guidance."}
{"id": "2508.01412", "pdf": "https://arxiv.org/pdf/2508.01412.pdf", "abs": "https://arxiv.org/abs/2508.01412", "title": "Discovering Bias Associations through Open-Ended LLM Generations", "authors": ["Jinhao Pan", "Chahat Raj", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation"}
{"id": "2508.01850", "pdf": "https://arxiv.org/pdf/2508.01850.pdf", "abs": "https://arxiv.org/abs/2508.01850", "title": "ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training", "authors": ["Lala Shakti Swarup Ray", "Vitor Fortes Rey", "Bo Zhou", "Paul Lukowicz", "Sungho Suh"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Prolonged seated activity is increasingly common in modern environments,\nraising concerns around musculoskeletal health, ergonomics, and the design of\nresponsive interactive systems. Existing posture sensing methods such as\nvision-based or wearable approaches face limitations including occlusion,\nprivacy concerns, user discomfort, and restricted deployment flexibility. We\nintroduce ChairPose, the first full body, wearable free seated pose estimation\nsystem that relies solely on pressure sensing and operates independently of\nchair geometry. ChairPose employs a two stage generative model trained on\npressure maps captured from a thin, chair agnostic sensing mattress. Unlike\nprior approaches, our method explicitly incorporates chair morphology into the\ninference process, enabling accurate, occlusion free, and privacy preserving\npose estimation. To support generalization across diverse users and chairs, we\nintroduce a physics driven data augmentation pipeline that simulates realistic\nvariations in posture and seating conditions. Evaluated across eight users and\nfour distinct chairs, ChairPose achieves a mean per joint position error of\n89.4 mm when both the user and the chair are unseen, demonstrating robust\ngeneralization to novel real world generalizability. ChairPose expands the\ndesign space for posture aware interactive systems, with potential applications\nin ergonomics, healthcare, and adaptive user interfaces."}
{"id": "2508.01424", "pdf": "https://arxiv.org/pdf/2508.01424.pdf", "abs": "https://arxiv.org/abs/2508.01424", "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs", "authors": ["Haonan Bian", "Yutao Qi", "Rui Yang", "Yuanxi Che", "Jiaqian Wang", "Heming Xia", "Ranran Zhen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches."}
{"id": "2508.01860", "pdf": "https://arxiv.org/pdf/2508.01860.pdf", "abs": "https://arxiv.org/abs/2508.01860", "title": "Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction", "authors": ["Mansi Sharma", "Shuang Chen", "Philipp M√ºller", "Maurice Rekrut", "Antonio Kr√ºger"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "For machines to effectively assist humans in challenging visual search tasks,\nthey must differentiate whether a human is simply glancing into a scene\n(navigational intent) or searching for a target object (informational intent).\nPrevious research proposed combining electroencephalography (EEG) and\neye-tracking measurements to recognize such search intents implicitly, i.e.,\nwithout explicit user input. However, the applicability of these approaches to\nreal-world scenarios suffers from two key limitations. First, previous work\nused fixed search times in the informational intent condition -- a stark\ncontrast to visual search, which naturally terminates when the target is found.\nSecond, methods incorporating EEG measurements addressed prediction scenarios\nthat require ground truth training data from the target user, which is\nimpractical in many use cases. We address these limitations by making the first\npublicly available EEG and eye-tracking dataset for navigational vs.\ninformational intent recognition, where the user determines search times. We\npresent the first method for cross-user prediction of search intents from EEG\nand eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out\nevaluations -- comparable to within-user prediction accuracy (85.5%) but\noffering much greater flexibility"}
{"id": "2508.01450", "pdf": "https://arxiv.org/pdf/2508.01450.pdf", "abs": "https://arxiv.org/abs/2508.01450", "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data", "authors": ["Xinlin Zhuang", "Feilong Tang", "Haolin Yang", "Ming Hu", "Huifa Li", "Haochen Xue", "Yichen Li", "Junjun He", "Zongyuan Ge", "Ying Qian", "Imran Razzak"], "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ."}
{"id": "2508.01878", "pdf": "https://arxiv.org/pdf/2508.01878.pdf", "abs": "https://arxiv.org/abs/2508.01878", "title": "VidAnimator: User-Guided Stylized 3D Character Animation from Human Videos", "authors": ["Xinwu Ye", "Jun-Hsiang Yao", "Jielin Feng", "Shuhong Mei", "Xingyu Lan", "Siming Chen"], "categories": ["cs.HC"], "comment": "14 pages, 7 figures, Xinwu Ye and Jun-Hsiang Yao contributed equally\n  to this work", "summary": "With captivating visual effects, stylized 3D character animation has gained\nwidespread use in cinematic production, advertising, social media, and the\npotential development of virtual reality (VR) non-player characters (NPCs).\nHowever, animating stylized 3D characters often requires significant time and\neffort from animators. We propose a mixed-initiative framework and interactive\nsystem to enable stylized 3D characters to mimic motion in human videos. The\nframework takes a single-view human video and a stylized 3D character (the\ntarget character) as input, captures the motion of the video, and then\ntransfers the motion to the target character. In addition, it involves two\ninteraction modules for customizing the result. Accordingly, the system\nincorporates two authoring tools that empower users with intuitive\nmodification. A questionnaire study offers tangible evidence of the framework's\ncapability of generating natural stylized 3D character animations similar to\nthe motion in the video. Additionally, three case studies demonstrate the\nutility of our approach in creating diverse results."}
{"id": "2508.01473", "pdf": "https://arxiv.org/pdf/2508.01473.pdf", "abs": "https://arxiv.org/abs/2508.01473", "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks."}
{"id": "2508.01881", "pdf": "https://arxiv.org/pdf/2508.01881.pdf", "abs": "https://arxiv.org/abs/2508.01881", "title": "Anchoring and Alignment: Data Factors in Part-to-Whole Visualization", "authors": ["Connor Bailey", "Michael Gleicher"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures, IEEE Visualization conference, repository URL:\n  https://github.com/uwgraphics/PartToWhole, preregistration URL:\n  https://osf.io/e36au", "summary": "We explore the effects of data and design considerations through the example\ncase of part-to-whole data relationships. Standard part-to-whole\nrepresentations like pie charts and stacked bar charts make the relationships\nof parts to the whole explicit. Value estimation in these charts benefits from\ntwo perceptual mechanisms: anchoring, where the value is close to a reference\nvalue with an easily recognized shape, and alignment where the beginning or end\nof the shape is aligned with a marker. In an online study, we explore how data\nand design factors such as value, position, and encoding together impact these\neffects in making estimations in part-to-whole charts. The results show how\nsalient values and alignment to positions on a scale affect task performance.\nThis demonstrates the need for informed visualization design based around how\ndata properties and design factors affect perceptual mechanisms."}
{"id": "2508.01480", "pdf": "https://arxiv.org/pdf/2508.01480.pdf", "abs": "https://arxiv.org/abs/2508.01480", "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach", "authors": ["Dimitra Panou", "Alexandros C. Dimopoulos", "Manolis Koubarakis", "Martin Reczko"], "categories": ["cs.CL"], "comment": null, "summary": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4."}
{"id": "2508.01894", "pdf": "https://arxiv.org/pdf/2508.01894.pdf", "abs": "https://arxiv.org/abs/2508.01894", "title": "IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition", "authors": ["Haozhe Zhou", "Riku Arakawa", "Yuvraj Agarwal", "Mayank Goel"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "IMUs are regularly used to sense human motion, recognize activities, and\nestimate full-body pose. Users are typically required to place sensors in\npredefined locations that are often dictated by common wearable form factors\nand the machine learning model's training process. Consequently, despite the\nincreasing number of everyday devices equipped with IMUs, the limited\nadaptability has seriously constrained the user experience to only using a few\nwell-explored device placements (e.g., wrist and ears). In this paper, we\nrethink IMU-based motion sensing by acknowledging that signals can be captured\nfrom any point on the human body. We introduce IMU over Continuous Coordinates\n(IMUCoCo), a novel framework that maps signals from a variable number of IMUs\nplaced on the body surface into a unified feature space based on their spatial\ncoordinates. These features can be plugged into downstream models for pose\nestimation and activity recognition. Our evaluations demonstrate that IMUCoCo\nsupports accurate pose estimation in a wide range of typical and atypical\nsensor placements. Overall, IMUCoCo supports significantly more flexible use of\nIMUs for motion sensing than the state-of-the-art, allowing users to place\ntheir sensors-laden devices according to their needs and preferences. The\nframework also supports the ability to change device locations depending on the\ncontext and suggests placement depending on the use case."}
{"id": "2508.01486", "pdf": "https://arxiv.org/pdf/2508.01486.pdf", "abs": "https://arxiv.org/abs/2508.01486", "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu", "authors": ["Vallabhaneni Raj Kumar", "Ashwin S", "Supriya Manna", "Niladri Sett", "Cheedella V S N M S Hema Harshitha", "Kurakula Harshitha", "Anand Kumar Sharma", "Basina Deepakraj", "Tanuj Sarkar", "Bondada Navaneeth Krishna", "Samanthapudi Shakeer"], "categories": ["cs.CL"], "comment": "work under review", "summary": "In the Indian subcontinent, Telugu, one of India's six classical languages,\nis the most widely spoken Dravidian Language. Despite its 96 million speaker\nbase worldwide, Telugu remains underrepresented in the global NLP and Machine\nLearning landscape, mainly due to lack of high-quality annotated resources.\nThis work introduces TeSent, a comprehensive benchmark dataset for sentiment\nclassification, a key text classification problem, in Telugu. TeSent not only\nprovides ground truth labels for the sentences, but also supplements with\nprovisions for evaluating explainability and fairness, two critical\nrequirements in modern-day machine learning tasks. We scraped Telugu texts\ncovering multiple domains from various social media platforms, news websites\nand web-blogs to preprocess and generate 26,150 sentences, and developed a\ncustom-built annotation platform and a carefully crafted annotation protocol\nfor collecting the ground truth labels along with their human-annotated\nrationales. We then fine-tuned several SOTA pre-trained models in two ways:\nwith rationales, and without rationales. Further, we provide a detailed\nplausibility and faithfulness evaluation suite, which exploits the rationales,\nfor six widely used post-hoc explainers applied on the trained models. Lastly,\nwe curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate\nfairness of Telugu sentiment and emotion related NLP tasks, and provide a\nfairness evaluation suite for the trained classifier models. Our experimental\nresults suggest that training with rationales may improve model accuracy,\nreduce bias in models, and make the explainers' output more aligned to human\nreasoning."}
{"id": "2508.01906", "pdf": "https://arxiv.org/pdf/2508.01906.pdf", "abs": "https://arxiv.org/abs/2508.01906", "title": "Effect of AI Performance, Risk Perception, and Trust on Human Dependence in Deepfake Detection AI system", "authors": ["Yingfan Zhou", "Ester Chen", "Manasa Pisipati", "Aiping Xiong", "Sarah Rajtmajer"], "categories": ["cs.HC"], "comment": null, "summary": "Synthetic images, audio, and video can now be generated and edited by\nArtificial Intelligence (AI). In particular, the malicious use of synthetic\ndata has raised concerns about potential harms to cybersecurity, personal\nprivacy, and public trust. Although AI-based detection tools exist to help\nidentify synthetic content, their limitations often lead to user mistrust and\nconfusion between real and fake content. This study examines the role of AI\nperformance in influencing human trust and decision making in synthetic data\nidentification. Through an online human subject experiment involving 400\nparticipants, we examined how varying AI performance impacts human trust and\ndependence on AI in deepfake detection. Our findings indicate how participants\ncalibrate their dependence on AI based on their perceived risk and the\nprediction results provided by AI. These insights contribute to the development\nof transparent and explainable AI systems that better support everyday users in\nmitigating the harms of synthetic media."}
{"id": "2508.01491", "pdf": "https://arxiv.org/pdf/2508.01491.pdf", "abs": "https://arxiv.org/abs/2508.01491", "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought", "authors": ["Zhivar Sourati", "Alireza S. Ziabari", "Morteza Dehghani"], "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability."}
{"id": "2508.02075", "pdf": "https://arxiv.org/pdf/2508.02075.pdf", "abs": "https://arxiv.org/abs/2508.02075", "title": "Human Capital Visualization using Speech Amount during Meetings", "authors": ["Ekai Hashimoto", "Takeshi Mizumoto", "Kohei Nagira", "Shun Shiramatsu"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work", "summary": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises."}
{"id": "2508.01503", "pdf": "https://arxiv.org/pdf/2508.01503.pdf", "abs": "https://arxiv.org/abs/2508.01503", "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction."}
{"id": "2508.02133", "pdf": "https://arxiv.org/pdf/2508.02133.pdf", "abs": "https://arxiv.org/abs/2508.02133", "title": "Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs", "authors": ["Yitong Zhu", "Lei Han", "GuanXuan Jiang", "PengYuan Zhou", "Yuyang Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for human-computer\ninteraction, yet real-world challenges like dynamic modality incompleteness and\nasynchrony severely limit its robustness. Existing methods often assume\nconsistently complete data or lack dynamic adaptability. To address these\nlimitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)\nframework for robust continuous emotion prediction. This framework employs a\ndual-layer expert structure. A Modality Expert Bank utilizes soft routing to\ndynamically handle missing modalities and achieve robust information fusion. A\nsubsequent Emotion Expert Bank leverages differential-attention routing to\nflexibly attend to emotional prototypes, enabling fine-grained emotion\nrepresentation. Additionally, a cross-modal alignment module explicitly\naddresses temporal shifts and semantic inconsistencies between modalities.\nExtensive experiments on benchmark datasets DEAP and DREAMER demonstrate our\nmodel's state-of-the-art performance in continuous emotion regression,\nshowcasing exceptional robustness under challenging conditions such as dynamic\nmodality absence and asynchronous sampling. This research significantly\nadvances the development of intelligent emotion systems adaptable to complex\nreal-world environments."}
{"id": "2508.01541", "pdf": "https://arxiv.org/pdf/2508.01541.pdf", "abs": "https://arxiv.org/abs/2508.01541", "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "authors": ["Sara C√¢mara", "Eduardo Luz", "Val√©ria Carvalho", "Ivan Meneghini", "Gladston Moreira"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length."}
{"id": "2508.02173", "pdf": "https://arxiv.org/pdf/2508.02173.pdf", "abs": "https://arxiv.org/abs/2508.02173", "title": "EchoLadder: Progressive AI-Assisted Design of Immersive VR Scenes", "authors": ["Zhuangze Hou", "Jingze Tian", "Nianlong Li", "Farong Ren", "Can Liu"], "categories": ["cs.HC"], "comment": "To appear at UIST 2025", "summary": "Mixed reality platforms allow users to create virtual environments, yet\nnovice users struggle with both ideation and execution in spatial design. While\nexisting AI models can automatically generate scenes based on user prompts, the\nlack of interactive control limits users' ability to iteratively steer the\noutput. In this paper, we present EchoLadder, a novel human-AI collaboration\npipeline that leverages large vision-language model (LVLM) to support\ninteractive scene modification in virtual reality. EchoLadder accepts users'\nverbal instructions at varied levels of abstraction and spatial specificity,\ngenerates concrete design suggestions throughout a progressive design process.\nThe suggestions can be automatically applied, regenerated and retracted by\nusers' toggle control.Our ablation study showed effectiveness of our pipeline\ncomponents. Our user study found that, compared to baseline without showing\nsuggestions, EchoLadder better supports user creativity in spatial design. It\nalso contributes insights on users' progressive design strategies under AI\nassistance, providing design implications for future systems."}
{"id": "2508.01554", "pdf": "https://arxiv.org/pdf/2508.01554.pdf", "abs": "https://arxiv.org/abs/2508.01554", "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "authors": ["Yujia Zheng", "Tianhao Li", "Haotian Huang", "Tianyu Zeng", "Jingyu Lu", "Chuangxin Chu", "Yuekai Huang", "Ziyou Jiang", "Qian Xiong", "Yuyao Ge", "Mingyang Li"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP."}
{"id": "2508.02216", "pdf": "https://arxiv.org/pdf/2508.02216.pdf", "abs": "https://arxiv.org/abs/2508.02216", "title": "Data Augmentation for Visualization Design Knowledge Bases", "authors": ["Hyeok Kim", "Jeffrey Heer"], "categories": ["cs.HC"], "comment": "10 pages, 4 tables, 7 figures, accepted to IEEE VIS 2025", "summary": "Visualization knowledge bases enable computational reasoning and\nrecommendation over a visualization design space. These systems evaluate design\ntrade-offs using numeric weights assigned to different features (e.g., binning\na variable). Feature weights can be learned automatically by fitting a model to\na collection of chart pairs, in which one chart is deemed preferable to the\nother. To date, labeled chart pairs have been drawn from published empirical\nresearch results; however, such pairs are not comprehensive, resulting in a\ntraining corpus that lacks many design variants and fails to systematically\nassess potential trade-offs. To improve knowledge base coverage and accuracy,\nwe contribute data augmentation techniques for generating and labeling chart\npairs. We present methods to generate novel chart pairs based on design\npermutations and by identifying under-assessed features -- leading to an\nexpanded corpus with thousands of new chart pairs, now in need of labels.\nAccordingly, we next compare varied methods to scale labeling efforts to\nannotate chart pairs, in order to learn updated feature weights. We evaluate\nour methods in the context of the Draco knowledge base, demonstrating\nimprovements to both feature coverage and chart recommendation performance."}
{"id": "2508.01630", "pdf": "https://arxiv.org/pdf/2508.01630.pdf", "abs": "https://arxiv.org/abs/2508.01630", "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "authors": ["Maziyar Panahi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct."}
{"id": "2508.02232", "pdf": "https://arxiv.org/pdf/2508.02232.pdf", "abs": "https://arxiv.org/abs/2508.02232", "title": "Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults", "authors": ["Lei Han", "Mingnan Wei", "Qiongyan Chen", "Anqi Wang", "Rong Pang", "Kefei Liu", "Rongrong Chen", "David Yip"], "categories": ["cs.HC"], "comment": null, "summary": "Photo-based reminiscence has the potential to have a positive impact on older\nadults' reconnection with their personal history and improve their well-being.\nSupporting reminiscence in older adults through technological implementations\nis becoming an increasingly important area of research in the fields of HCI and\nCSCW. However, the impact of integrating gaze and speech as mixed-initiative\ninteractions in LLM-powered reminiscence conversations remains under-explored.\nTo address this, we conducted expert interviews to understand the challenges\nthat older adults face with LLM-powered, photo-based reminiscence experiences.\nBased on these design considerations, we developed Eye2Recall, a system that\nintegrates eye tracking for detecting visual interest with natural language\ninteraction to create a mixed-initiative reminiscence experience. We evaluated\nits effectiveness through a user study involving ten older adults. The results\nhave important implications for the future design of more accessible and\nempowering reminiscence technologies that better align with older adults'\nnatural interaction patterns and enhance their positive aging."}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656.pdf", "abs": "https://arxiv.org/abs/2508.01656", "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "authors": ["Lucio La Cava", "Dominik Macko", "R√≥bert M√≥ro", "Ivan Srba", "Andrea Tagarelli"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios."}
{"id": "2508.02274", "pdf": "https://arxiv.org/pdf/2508.02274.pdf", "abs": "https://arxiv.org/abs/2508.02274", "title": "mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of Arrhythmia", "authors": ["Arjun Kumar", "Noppanat Wadlom", "Jaeheon Kwak", "Si-Hyuck Kang", "Insik Shin"], "categories": ["cs.HC", "cs.LG", "92C55, 68T07"], "comment": "15 pages, 27 images", "summary": "Arrhythmia is a common cardiac condition that can precipitate severe\ncomplications without timely intervention. While continuous monitoring is\nessential for timely diagnosis, conventional approaches such as\nelectrocardiogram and wearable devices are constrained by their reliance on\nspecialized medical expertise and patient discomfort from their contact nature.\nExisting contactless monitoring, primarily designed for healthy subjects, face\nsignificant challenges when analyzing reflected signals from arrhythmia\npatients due to disrupted spatial stability and temporal consistency.\n  In this paper, we introduce mCardiacDx, a radar-driven contactless system\nthat accurately analyzes reflected signals and reconstructs heart pulse\nwaveforms for arrhythmia monitoring and diagnosis. The key contributions of our\nwork include a novel precise target localization (PTL) technique that locates\nreflected signals despite spatial disruptions, and an encoder-decoder model\nthat transforms these signals into HPWs, addressing temporal inconsistencies.\nOur evaluation on a large dataset of healthy subjects and arrhythmia patients\nshows that both mCardiacDx and PTL outperform state-of-the-art approach in\narrhythmia monitoring and diagnosis, also demonstrating improved performance in\nhealthy subjects."}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674.pdf", "abs": "https://arxiv.org/abs/2508.01674", "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements."}
{"id": "2508.02328", "pdf": "https://arxiv.org/pdf/2508.02328.pdf", "abs": "https://arxiv.org/abs/2508.02328", "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC", "cs.CL", "cs.IR", "H.5.2; I.2.7; H.1.2"], "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables", "summary": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs."}
{"id": "2508.01682", "pdf": "https://arxiv.org/pdf/2508.01682.pdf", "abs": "https://arxiv.org/abs/2508.01682", "title": "The Bidirectional Process Reward Model", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling."}
{"id": "2508.02371", "pdf": "https://arxiv.org/pdf/2508.02371.pdf", "abs": "https://arxiv.org/abs/2508.02371", "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design", "authors": ["Matou≈° Jel√≠nek", "Nadine Schlicker", "Ewart de Visser"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness."}
{"id": "2508.01696", "pdf": "https://arxiv.org/pdf/2508.01696.pdf", "abs": "https://arxiv.org/abs/2508.01696", "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bin Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks."}
{"id": "2508.02376", "pdf": "https://arxiv.org/pdf/2508.02376.pdf", "abs": "https://arxiv.org/abs/2508.02376", "title": "Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction", "authors": ["Matus Krajcovic", "Peter Demcak", "Eduard Kuric"], "categories": ["cs.HC", "H.5; I.2"], "comment": null, "summary": "Embodied conversational agents (ECAs) are increasingly more realistic and\ncapable of dynamic conversations. In online surveys, anthropomorphic agents\ncould help address issues like careless responding and satisficing, which\noriginate from the lack of personal engagement and perceived accountability.\nHowever, there is a lack of understanding of how ECAs in user experience\nresearch may affect participant engagement, satisfaction, and the quality of\nresponses. As a proof of concept, we propose an instrument that enables the\nincorporation of conversations with a virtual avatar into surveys, using on\nAI-driven video generation, speech recognition, and Large Language Models. In\nour between-subjects study, 80 participants (UK, stratified random sample of\ngeneral population) either talked to a voice-based agent with an animated video\navatar, or interacted with a chatbot. Across surveys based on two self-reported\npsychometric tests, 2,265 conversation responses were obtained. Statistical\ncomparison of results indicates that embodied agents can contribute\nsignificantly to more informative, detailed responses, as well as higher yet\nmore time-efficient engagement. Furthermore, qualitative analysis provides\nvaluable insights for causes of no significant change to satisfaction, linked\nto personal preferences, turn-taking delays and Uncanny Valley reactions. These\nfindings support the pursuit and development of new methods toward human-like\nagents for the transformation of online surveys into more natural interactions\nresembling in-person interviews."}
{"id": "2508.01708", "pdf": "https://arxiv.org/pdf/2508.01708.pdf", "abs": "https://arxiv.org/abs/2508.01708", "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "authors": ["Berkay K√∂pr√º", "Mehrzad Mashal", "Yigit Gurses", "Akos Kadar", "Maximilian Schmitt", "Ditty Mathew", "Felix Burkhardt", "Florian Eyben", "Bj√∂rn W. Schuller"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate."}
{"id": "2508.02413", "pdf": "https://arxiv.org/pdf/2508.02413.pdf", "abs": "https://arxiv.org/abs/2508.02413", "title": "Improving Knowledge Graph Understanding with Contextual Views", "authors": ["Antrea Christou", "Cogan Shimizu"], "categories": ["cs.HC"], "comment": "12 pages", "summary": "Navigating, visualizing, and discovery in graph data is frequently a\ndifficult prospect. This is especially true for knowledge graphs (KGs), due to\nhigh number of possible labeled connections to other data.\n  However, KGs are frequently equipped with an ontology as a schema. That is,\nit informs how the relationships between data may be constrained. This\nadditional information can be leveraged to improve how (knowledge) graph data\ncan be navigated, visualized, or otherwise utilized in a discovery process.\n  In this manuscript, we introduce the Interactive Knowledge (InK) Browser.\nThis tool specifically takes advantage ontological information (i.e.,\nknowledge) when found in KGs. Specifically, we use modular views that provide\nvarious perspectives over the graph, including an interactive schema view, data\nlistings based on type, neighborhood connections, and geospatial depiction\n(where appropriate). For this manuscript, we have evaluated the basic premise\nof this tool over a user group ($n= With this grown user survey, we continue to\nevaluate how scalable tools, including flexible views, can make KG exploration\neasier for a range of applications.)"}
{"id": "2508.01710", "pdf": "https://arxiv.org/pdf/2508.01710.pdf", "abs": "https://arxiv.org/abs/2508.01710", "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "authors": ["Raviraj Joshi", "Rakesh Paul", "Kanishk Singla", "Anusha Kamath", "Michael Evans", "Katherine Luna", "Shaona Ghosh", "Utkarsh Vaidya", "Eileen Long", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models."}
{"id": "2508.02470", "pdf": "https://arxiv.org/pdf/2508.02470.pdf", "abs": "https://arxiv.org/abs/2508.02470", "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration", "authors": ["Hyunjn An", "Yongwon Kim", "Wonduk Seo", "Joonil Park", "Daye Kang", "Changhoon Oh", "Dokyun Kim", "Seunghyun Lee"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design."}
{"id": "2508.01739", "pdf": "https://arxiv.org/pdf/2508.01739.pdf", "abs": "https://arxiv.org/abs/2508.01739", "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues."}
{"id": "2508.02550", "pdf": "https://arxiv.org/pdf/2508.02550.pdf", "abs": "https://arxiv.org/abs/2508.02550", "title": "Stakeholder Perspectives on Humanistic Implementation of Computer Perception in Healthcare: A Qualitative Study", "authors": ["Kristin M. Kostick-Quenet", "Meghan E. Hurley", "Syed Ayaz", "John Herrington", "Casey Zampella", "Julia Parish-Morris", "Birkan Tun√ß", "Gabriel L√°zaro-Mu√±oz", "J. S. Blumenthal-Barby", "Eric A. Storch"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "65 pages", "summary": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care."}
{"id": "2508.01754", "pdf": "https://arxiv.org/pdf/2508.01754.pdf", "abs": "https://arxiv.org/abs/2508.01754", "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Luodan Zhang", "Zhen Lin", "Guangsheng Bao", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection."}
{"id": "2508.02592", "pdf": "https://arxiv.org/pdf/2508.02592.pdf", "abs": "https://arxiv.org/abs/2508.02592", "title": "Teaching Critical Visualization: A Field Report", "authors": ["Andrew McNutt", "Shiyi He", "Sujit Kumar Kamaraj", "Purbid Bambroo", "Nastaran Jadidi", "John Bovard", "Chang Han"], "categories": ["cs.HC"], "comment": "Accepted to EduVIS25", "summary": "Critical Visualization is gaining popularity and academic focus, yet\nrelatively few academic courses have been offered to support students in this\ncomplex area. This experience report describes a recent experimental course on\nthe topic, exploring both what the topic could be as well as an experimental\ncontent structure (namely as scavenger hunt). Generally the course was\nsuccessful, achieving the learning objectives of developing critical thinking\nskills, improving communication about complex ideas, and developing a knowledge\nabout theories in the area. While improvements can be made, we hope that\nhumanistic notions of criticality are embraced more deeply in visualization\npedagogy."}
{"id": "2508.01781", "pdf": "https://arxiv.org/pdf/2508.01781.pdf", "abs": "https://arxiv.org/abs/2508.01781", "title": "A comprehensive taxonomy of hallucinations in Large Language Models", "authors": ["Manuel Cossio"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "55 pages, 16 figures, 3 tables", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications."}
{"id": "2508.02593", "pdf": "https://arxiv.org/pdf/2508.02593.pdf", "abs": "https://arxiv.org/abs/2508.02593", "title": "Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition", "authors": ["Catalina Gomez", "Lalithkumar Seenivasan", "Xinrui Zou", "Jeewoo Yoon", "Sirui Chu", "Ariel Leong", "Patrick Kramer", "Yu-Chun Ku", "Jose L. Porras", "Alejandro Martin-Gomez", "Masaru Ishii", "Mathias Unberath"], "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Traditional surgical skill acquisition relies heavily on expert feedback, yet\ndirect access is limited by faculty availability and variability in subjective\nassessments. While trainees can practice independently, the lack of\npersonalized, objective, and quantitative feedback reduces the effectiveness of\nself-directed learning. Recent advances in computer vision and machine learning\nhave enabled automated surgical skill assessment, demonstrating the feasibility\nof automatic competency evaluation. However, it is unclear whether such\nArtificial Intelligence (AI)-driven feedback can contribute to skill\nacquisition. Here, we examine the effectiveness of explainable AI\n(XAI)-generated feedback in surgical training through a human-AI study. We\ncreate a simulation-based training framework that utilizes XAI to analyze\nvideos and extract surgical skill proxies related to primitive actions. Our\nintervention provides automated, user-specific feedback by comparing trainee\nperformance to expert benchmarks and highlighting deviations from optimal\nexecution through understandable proxies for actionable guidance. In a\nprospective user study with medical students, we compare the impact of\nXAI-guided feedback against traditional video-based coaching on task outcomes,\ncognitive load, and trainees' perceptions of AI-assisted learning. Results\nshowed improved cognitive load and confidence post-intervention. While no\ndifferences emerged between the two feedback types in reducing performance gaps\nor practice adjustments, trends in the XAI group revealed desirable effects\nwhere participants more closely mimicked expert practice. This work encourages\nthe study of explainable AI in surgical education and the development of\ndata-driven, adaptive feedback mechanisms that could transform learning\nexperiences and competency assessment."}
{"id": "2508.01812", "pdf": "https://arxiv.org/pdf/2508.01812.pdf", "abs": "https://arxiv.org/abs/2508.01812", "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "authors": ["Amir DN Cohen", "Hilla Merhav", "Yoav Goldberg", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs."}
{"id": "2508.02610", "pdf": "https://arxiv.org/pdf/2508.02610.pdf", "abs": "https://arxiv.org/abs/2508.02610", "title": "PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players", "authors": ["Sanchita S. Kamath", "Omar Khan", "Anurag Choudhary", "Jan Meyerhoff-Liang", "Soyoung Choi", "JooYoung Seo"], "categories": ["cs.HC"], "comment": null, "summary": "Blind and low-vision (BLV) individuals experience lower levels of physical\nactivity (PA) compared to sighted peers due to a lack of accessible, engaging\nexercise options. Existing solutions often rely on auditory cues but do not\nfully integrate rich sensory feedback or support spatial navigation, limiting\ntheir effectiveness. This study introduces PunchPulse, a virtual reality (VR)\nboxing exergame designed to motivate BLV users to reach and sustain moderate to\nvigorous physical activity (MVPA) levels. Over a seven-month, multi-phased\nstudy, PunchPulse was iteratively refined with three BLV co-designers, informed\nby two early pilot testers, and evaluated by six additional BLV user-study\nparticipants. Data collection included both qualitative (researcher\nobservations, SOPI) and quantitative (MVPA zones, aid usage, completion times)\nmeasures of physical exertion and gameplay performance. The user study revealed\nthat all participants reached moderate MVPA thresholds, with high levels of\nimmersion and engagement observed. This work demonstrates the potential of VR\nas an inclusive medium for promoting meaningful PA in the BLV community and\naddresses a critical gap in accessible, intensity-driven exercise\ninterventions."}
{"id": "2508.01815", "pdf": "https://arxiv.org/pdf/2508.01815.pdf", "abs": "https://arxiv.org/abs/2508.01815", "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning."}
{"id": "2508.02639", "pdf": "https://arxiv.org/pdf/2508.02639.pdf", "abs": "https://arxiv.org/abs/2508.02639", "title": "Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable", "authors": ["Tingying He", "Jason Dykes", "Petra Isenberg", "Tobias Isenberg"], "categories": ["cs.HC"], "comment": null, "summary": "We present a new comprehensive theory for explaining, exploring, and using\npattern as a visual variable in visualization. Although patterns have long been\nused for data encoding and continue to be valuable today, their conceptual\nfoundations are precarious: the concepts and terminology used across the\nresearch literature and in practice are inconsistent, making it challenging to\nuse patterns effectively and to conduct research to inform their use. To\naddress this problem, we conduct a comprehensive cross-disciplinary literature\nreview that clarifies ambiguities around the use of \"pattern\" and \"texture\". As\na result, we offer a new consistent treatment of pattern as a composite visual\nvariable composed of structured groups of graphic primitives that can serve as\nmarks for encoding data individually and collectively. This new and widely\napplicable formulation opens a sizable design space for the visual variable\npattern, which we formalize as a new system comprising three sets of variables:\nthe spatial arrangement of primitives, the appearance relationships among\nprimitives, and the retinal visual variables that characterize individual\nprimitives. We show how our pattern system relates to existing visualization\ntheory and highlight opportunities for visualization design. We further explore\npatterns based on complex spatial arrangements, demonstrating explanatory power\nand connecting our conceptualization to broader theory on maps and cartography.\nAn author version and additional materials are available on OSF: osf.io/z7ae2."}
{"id": "2508.01832", "pdf": "https://arxiv.org/pdf/2508.01832.pdf", "abs": "https://arxiv.org/abs/2508.01832", "title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory", "authors": ["Rubin Wei", "Jiaqi Cao", "Jiarui Wang", "Jushi Kai", "Qipeng Guo", "Bowen Zhou", "Zhouhan Lin"], "categories": ["cs.CL"], "comment": null, "summary": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future."}
{"id": "1907.00326", "pdf": "https://arxiv.org/pdf/1907.00326.pdf", "abs": "https://arxiv.org/abs/1907.00326", "title": "Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes", "authors": ["Jie Cao", "Michael Tanana", "Zac E. Imel", "Eric Poitras", "David C. Atkins", "Vivek Srikumar"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2019", "summary": "Automatically analyzing dialogue can help understand and guide behavior in\ndomains such as counseling, where interactions are largely mediated by\nconversation. In this paper, we study modeling behavioral codes used to asses a\npsychotherapy treatment style called Motivational Interviewing (MI), which is\neffective for addressing substance abuse and related problems. Specifically, we\naddress the problem of providing real-time guidance to therapists with a\ndialogue observer that (1) categorizes therapist and client MI behavioral codes\nand, (2) forecasts codes for upcoming utterances to help guide the conversation\nand potentially alert the therapist. For both tasks, we define neural network\nmodels that build upon recent successes in dialogue modeling. Our experiments\ndemonstrate that our models can outperform several baselines for both tasks. We\nalso report the results of a careful analysis that reveals the impact of the\nvarious network design tradeoffs for modeling therapy dialogue."}
{"id": "2508.01858", "pdf": "https://arxiv.org/pdf/2508.01858.pdf", "abs": "https://arxiv.org/abs/2508.01858", "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents", "authors": ["Yuhan Guo", "Cong Guo", "Aiwen Sun", "Hongliang He", "Xinyu Yang", "Yue Lu", "Yingji Zhang", "Xuntao Guo", "Dong Zhang", "Jianzhuang Liu", "Jiang Duan", "Yijia Xiao", "Liangjian Wen", "Hai-Ming Xu", "Yong Dai"], "categories": ["cs.CL", "cs.AI"], "comment": "Our code and data is open sourced at\n  https://github.com/Gnonymous/Web-CogReasoner", "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner"}
{"id": "2412.13395", "pdf": "https://arxiv.org/pdf/2412.13395.pdf", "abs": "https://arxiv.org/abs/2412.13395", "title": "Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse", "authors": ["Jie Cao", "Abhijit Suresh", "Jennifer Jacobs", "Charis Clevenger", "Amanda Howard", "Chelsea Brown", "Brent Milne", "Tom Fischaber", "Tamara Sumner", "James H. Martin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to COLING'2025", "summary": "Human tutoring interventions play a crucial role in supporting student\nlearning, improving academic performance, and promoting personal growth. This\npaper focuses on analyzing mathematics tutoring discourse using talk moves - a\nframework of dialogue acts grounded in Accountable Talk theory. However,\nscaling the collection, annotation, and analysis of extensive tutoring\ndialogues to develop machine learning models is a challenging and\nresource-intensive task. To address this, we present SAGA22, a compact dataset,\nand explore various modeling strategies, including dialogue context, speaker\ninformation, pretraining datasets, and further fine-tuning. By leveraging\nexisting datasets and models designed for classroom teaching, our results\ndemonstrate that supplementary pretraining on classroom data enhances model\nperformance in tutoring settings, particularly when incorporating longer\ncontext and speaker information. Additionally, we conduct extensive ablation\nstudies to underscore the challenges in talk move modeling."}
{"id": "2508.01862", "pdf": "https://arxiv.org/pdf/2508.01862.pdf", "abs": "https://arxiv.org/abs/2508.01862", "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models", "authors": ["Yijun Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism."}
{"id": "2508.00859", "pdf": "https://arxiv.org/pdf/2508.00859.pdf", "abs": "https://arxiv.org/abs/2508.00859", "title": "Author Once, Publish Everywhere: Portable Metadata Authoring with the CEDAR Embeddable Editor", "authors": ["Martin J. O'Connor", "Marcos Martinez-Romero", "Attila L. Egyedi", "Mete U. Akdogan", "Michael V. Dorf", "Mark A. Musen"], "categories": ["cs.DL", "cs.HC", "68N19 (Primary) 68M11 (Secondary)", "D.2.2; D.2.11; D.2.13; H.3.5; H.5.2"], "comment": null, "summary": "High-quality, \"rich\" metadata are essential for making research data\nfindable, interoperable, and reusable. The Center for Expanded Data Annotation\nand Retrieval (CEDAR) has long addressed this need by providing tools to design\nmachine-actionable metadata templates that encode community standards in a\ncomputable form. To make these capabilities more accessible within real-world\nresearch workflows, we have developed the CEDAR Embeddable Editor (CEE)-a\nlightweight, interoperable Web Component that brings structured,\nstandards-based metadata authoring directly into third-party platforms. The CEE\ndynamically renders metadata forms from machine-actionable templates and\nproduces semantically rich metadata in JSON-LD format. It supports\nontology-based value selection via the BioPortal ontology repository, and it\nincludes external authority resolution for persistent identifiers such as\nORCIDs for individuals and RORs for research organizations. Crucially, the CEE\nrequires no custom user-interface development, allowing deployment across\ndiverse platforms. The CEE has been successfully integrated into generalist\nscientific data repositories such as Dryad and the Open Science Framework,\ndemonstrating its ability to support discipline-specific metadata creation. By\nsupporting the embedding of metadata authoring within existing research\nenvironments, the CEE can facilitate the adoption of community standards and\nhelp improve metadata quality across scientific disciplines."}
{"id": "2508.01918", "pdf": "https://arxiv.org/pdf/2508.01918.pdf", "abs": "https://arxiv.org/abs/2508.01918", "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language", "authors": ["Jaskaranjeet Singh", "Rakesh Thakur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP"}
{"id": "2508.00899", "pdf": "https://arxiv.org/pdf/2508.00899.pdf", "abs": "https://arxiv.org/abs/2508.00899", "title": "ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI", "authors": ["Abeer Dyoub", "Ivan Letteri", "Francesca A. Lisi"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "The emergence of Symbiotic AI (SAI) introduces new challenges to ethical\ndecision-making as it deepens human-AI collaboration. As symbiosis grows, AI\nsystems pose greater ethical risks, including harm to human rights and trust.\nEthical Risk Assessment (ERA) thus becomes crucial for guiding decisions that\nminimize such risks. However, ERA is hindered by uncertainty, vagueness, and\nincomplete information, and morality itself is context-dependent and imprecise.\nThis motivates the need for a flexible, transparent, yet robust framework for\nERA. Our work supports ethical decision-making by quantitatively assessing and\nprioritizing multiple ethical risks so that artificial agents can select\nactions aligned with human values and acceptable risk levels. We introduce\nff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic\nHierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks\nvia an Ethical Risk Score (ERS) for each risk type. The final ERS combines the\nFAHP-derived weight, propagated CF, and risk level. The framework offers a\nrobust mathematical approach for collaborative ERA modeling and systematic,\nstep-by-step analysis. A case study confirms that ff4ERA yields\ncontext-sensitive, ethically meaningful risk scores reflecting both expert\ninput and sensor-based evidence. Risk scores vary consistently with relevant\nfactors while remaining robust to unrelated inputs. Local sensitivity analysis\nshows predictable, mostly monotonic behavior across perturbations, and global\nSobol analysis highlights the dominant influence of expert-defined weights and\ncertainty factors, validating the model design. Overall, the results\ndemonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware\nethical assessments, enabling what-if analyses and guiding designers in\ncalibrating membership functions and expert judgments for reliable ethical\ndecision support."}
{"id": "2508.01930", "pdf": "https://arxiv.org/pdf/2508.01930.pdf", "abs": "https://arxiv.org/abs/2508.01930", "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "authors": ["Tom S. Juzek", "Zina B. Ward"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research."}
{"id": "2508.00928", "pdf": "https://arxiv.org/pdf/2508.00928.pdf", "abs": "https://arxiv.org/abs/2508.00928", "title": "Modeling Head-Neck Dynamics under Lateral Perturbations Using MPC to Mimic CNS postural stabilization strategy", "authors": ["Chrysovalanto Messiou", "Riender Happee", "Georgios Papaioannou"], "categories": ["eess.SY", "cs.HC", "cs.SY"], "comment": null, "summary": "Automated vehicles will allow occupants to engage in non-driving tasks, but\nlimited visual cues will make them vulnerable to unexpected movements. These\nunpredictable perturbations create a \"surprise factor,\" forcing the central\nnervous system to rely on compensatory postural adjustments, which are less\neffective, and are more likely to trigger sensory conflicts. Since the head is\na key reference for sensory input (vestibular and vision), models accurately\ncapturing head-neck postural stabilization are essential for assessing AV\ncomfort. This study extends an existing model predictive control-based\nframework to simulate head-neck postural control under lateral perturbations.\nExperimental validation against human data demonstrates that the model can\naccurately reproduce dynamic responses during lateral trunk perturbations. The\nresults show that muscle effort combined with partial somatosensory feedback\nprovides the best overall dynamic fit without requiring corrective relative and\nglobal head orientation integrators for posture."}
{"id": "2508.01943", "pdf": "https://arxiv.org/pdf/2508.01943.pdf", "abs": "https://arxiv.org/abs/2508.01943", "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "authors": ["Philip Schroeder", "Ondrej Biza", "Thomas Weng", "Hongyin Luo", "James Glass"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io"}
{"id": "2508.00975", "pdf": "https://arxiv.org/pdf/2508.00975.pdf", "abs": "https://arxiv.org/abs/2508.00975", "title": "Star Network Motifs on X during COVID-19", "authors": ["Lynnette Hui Xian Ng", "Divyaansh Sinha", "Kathleen M. Carley"], "categories": ["cs.SI", "cs.HC"], "comment": "Accepted into SBP-BRiMS 2025", "summary": "Social network motifs are recurring patterns of small subgraphs that indicate\nfundamental patterns of social communication. In this work, we study the simple\nstar network motifs that recur on X during the COVID-19 discourse. We study the\nprofile of the manifestation of the star network among bot and human users.\nThere are six primary patterns of the star motif, differentiating by the bots\nand humans being either egos and alters. We describe the presentation of each\nof these six patterns in our data, demonstrating how the motif patterns can\ninform social media behavioral analysis."}
{"id": "2508.01959", "pdf": "https://arxiv.org/pdf/2508.01959.pdf", "abs": "https://arxiv.org/abs/2508.01959", "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension", "authors": ["Junjie Wu", "Jiangnan Li", "Yuqing Li", "Lemao Liu", "Liyan Xu", "Jiwei Li", "Dit-Yan Yeung", "Jie Zhou", "Mo Yu"], "categories": ["cs.CL"], "comment": "Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding", "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications."}
{"id": "2508.01186", "pdf": "https://arxiv.org/pdf/2508.01186.pdf", "abs": "https://arxiv.org/abs/2508.01186", "title": "A Survey on Agent Workflow -- Status and Future", "authors": ["Chaojia Yu", "Zihan Cheng", "Hanwen Cui", "Yishuo Gao", "Zexu Luo", "Yijin Wang", "Hangbin Zheng", "Yong Zhao"], "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 3 figures, accepted to IEEE Conference,\n  ICAIBD(International Conference of Artificial Intelligence and Big Data)\n  2025. This is the author's version, not the publisher's. See\n  https://ieeexplore.ieee.org/document/11082076", "summary": "In the age of large language models (LLMs), autonomous agents have emerged as\na powerful paradigm for achieving general intelligence. These agents\ndynamically leverage tools, memory, and reasoning capabilities to accomplish\nuser-defined goals. As agent systems grow in complexity, agent\nworkflows-structured orchestration frameworks-have become central to enabling\nscalable, controllable, and secure AI behaviors. This survey provides a\ncomprehensive review of agent workflow systems, spanning academic frameworks\nand industrial implementations. We classify existing systems along two key\ndimensions: functional capabilities (e.g., planning, multi-agent collaboration,\nexternal API integration) and architectural features (e.g., agent roles,\norchestration flows, specification languages). By comparing over 20\nrepresentative systems, we highlight common patterns, potential technical\nchallenges, and emerging trends. We further address concerns related to\nworkflow optimization strategies and security. Finally, we outline open\nproblems such as standardization and multimodal integration, offering insights\nfor future research at the intersection of agent design, workflow\ninfrastructure, and safe automation."}
{"id": "2508.01977", "pdf": "https://arxiv.org/pdf/2508.01977.pdf", "abs": "https://arxiv.org/abs/2508.01977", "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine."}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213.pdf", "abs": "https://arxiv.org/abs/2508.01213", "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level."}
{"id": "2508.01990", "pdf": "https://arxiv.org/pdf/2508.01990.pdf", "abs": "https://arxiv.org/abs/2508.01990", "title": "Contextually Aware E-Commerce Product Question Answering using RAG", "authors": ["Praveen Tangarajan", "Anand A. Rajasekar", "Manish Rathi", "Vinay Rao Dandin", "Ozan Ersoy"], "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "6 pages, 1 figure, 5 tables. Preprint under review", "summary": "E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations."}
{"id": "2508.01240", "pdf": "https://arxiv.org/pdf/2508.01240.pdf", "abs": "https://arxiv.org/abs/2508.01240", "title": "RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation", "authors": ["Juntong Chen", "Huayuan Ye", "He Zhu", "Siwei Fu", "Changbo Wang", "Chenhui Li"], "categories": ["cs.LG", "cs.HC"], "comment": "9 pages, 14 figures, paper accepted to IEEE VIS 2025", "summary": "Accurate and reliable visualization of spatiotemporal sensor data such as\nenvironmental parameters and meteorological conditions is crucial for informed\ndecision-making. Traditional spatial interpolation methods, however, often fall\nshort of producing reliable interpolation results due to the limited and\nirregular sensor coverage. This paper introduces a novel spatial interpolation\npipeline that achieves reliable interpolation results and produces a novel\nheatmap representation with uncertainty information encoded. We leverage\nimputation reference data from Graph Neural Networks (GNNs) to enhance\nvisualization reliability and temporal resolution. By integrating Principal\nNeighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), our\nmodel effectively learns the spatiotemporal dependencies. Furthermore, we\npropose an extrinsic, static visualization technique for interpolation-based\nheatmaps that effectively communicates the uncertainties arising from various\nsources in the interpolated map. Through a set of use cases, extensive\nevaluations on real-world datasets, and user studies, we demonstrate our\nmodel's superior performance for data imputation, the improvements to the\ninterpolant with reference data, and the effectiveness of our visualization\ndesign in communicating uncertainties."}
{"id": "2508.01999", "pdf": "https://arxiv.org/pdf/2508.01999.pdf", "abs": "https://arxiv.org/abs/2508.01999", "title": "Prompting Large Language Models to Detect Dementia Family Caregivers", "authors": ["Md Badsha Biswas", "√ñzlem Uzuner"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub."}
{"id": "2508.01316", "pdf": "https://arxiv.org/pdf/2508.01316.pdf", "abs": "https://arxiv.org/abs/2508.01316", "title": "Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust", "authors": ["Mohsen Abbaspour Onari", "Lucie Charlotte Magister", "Yaoxin Wu", "Amalia Lupi", "Dario Creazzo", "Mattia Tordin", "Luigi Di Donatantonio", "Emilio Quaia", "Chao Zhang", "Isel Grau", "Marco S. Nobile", "Yingqian Zhang", "Pietro Li√≤"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Distal myopathy represents a genetically heterogeneous group of skeletal\nmuscle disorders with broad clinical manifestations, posing diagnostic\nchallenges in radiology. To address this, we propose a novel multimodal\nattention-aware fusion architecture that combines features extracted from two\ndistinct deep learning models, one capturing global contextual information and\nthe other focusing on local details, representing complementary aspects of the\ninput data. Uniquely, our approach integrates these features through an\nattention gate mechanism, enhancing both predictive performance and\ninterpretability. Our method achieves a high classification accuracy on the\nBUSI benchmark and a proprietary distal myopathy dataset, while also generating\nclinically relevant saliency maps that support transparent decision-making in\nmedical diagnosis. We rigorously evaluated interpretability through (1)\nfunctionally grounded metrics, coherence scoring against reference masks and\nincremental deletion analysis, and (2) application-grounded validation with\nseven expert radiologists. While our fusion strategy boosts predictive\nperformance relative to single-stream and alternative fusion strategies, both\nquantitative and qualitative evaluations reveal persistent gaps in anatomical\nspecificity and clinical usefulness of the interpretability. These findings\nhighlight the need for richer, context-aware interpretability methods and\nhuman-in-the-loop feedback to meet clinicians' expectations in real-world\ndiagnostic settings."}
{"id": "2508.02013", "pdf": "https://arxiv.org/pdf/2508.02013.pdf", "abs": "https://arxiv.org/abs/2508.02013", "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field."}
{"id": "2508.01510", "pdf": "https://arxiv.org/pdf/2508.01510.pdf", "abs": "https://arxiv.org/abs/2508.01510", "title": "DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "categories": ["eess.SP", "cs.HC"], "comment": null, "summary": "A fully customisable chip-on board (COB) LED design to evoke two brain\nresponses simultaneously (steady state visual evoked potential (SSVEP) and\ntransient evoked potential, P300) is discussed in this paper. Considering\ndifferent possible modalities in braincomputer interfacing (BCI), SSVEP is\nwidely accepted as it requires a lesser number of electroencephalogram (EEG)\nelectrodes and minimal training time. The aim of this work was to produce a\nhybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced\nfatigue and improved classification performance. The system comprises of four\nindependent radial green visual stimuli controlled individually by a 32-bit\nmicrocontroller platform to evoke SSVEP and four red LEDs flashing at random\nintervals to generate P300 events. The system can also record the P300 event\ntimestamps that can be used in classification, to improve the accuracy and\nreliability. The hybrid stimulus was tested for realtime classification\naccuracy by controlling a LEGO robot to move in four directions."}
{"id": "2508.02018", "pdf": "https://arxiv.org/pdf/2508.02018.pdf", "abs": "https://arxiv.org/abs/2508.02018", "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models", "authors": ["Wanqi Yang", "Yanda Li", "Yunchao Wei", "Meng Fang", "Ling Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks."}
{"id": "2508.01523", "pdf": "https://arxiv.org/pdf/2508.01523.pdf", "abs": "https://arxiv.org/abs/2508.01523", "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification", "authors": ["Ningzhi Tang", "Emory Smith", "Yu Huang", "Collin McMillan", "Toby Jia-Jun Li"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "This paper presents a study of using large language models (LLMs) in\nmodifying existing code. While LLMs for generating code have been widely\nstudied, their role in code modification remains less understood. Although\n\"prompting\" serves as the primary interface for developers to communicate\nintents to LLMs, constructing effective prompts for code modification\nintroduces challenges different from generation. Prior work suggests that\nnatural language summaries may help scaffold this process, yet such approaches\nhave been validated primarily in narrow domains like SQL rewriting. This study\ninvestigates two prompting strategies for LLM-assisted code modification:\nDirect Instruction Prompting, where developers describe changes explicitly in\nfree-form language, and Summary-Mediated Prompting, where changes are made by\nediting the generated summaries of the code. We conducted an exploratory study\nwith 15 developers who completed modification tasks using both techniques\nacross multiple scenarios. Our findings suggest that developers followed an\niterative workflow: understanding the code, localizing the edit, and validating\noutputs through execution or semantic reasoning. Each prompting strategy\npresented trade-offs: direct instruction prompting was more flexible and easier\nto specify, while summary-mediated prompting supported comprehension, prompt\nscaffolding, and control. Developers' choice of strategy was shaped by task\ngoals and context, including urgency, maintainability, learning intent, and\ncode familiarity. These findings highlight the need for more usable prompt\ninteractions, including adjustable summary granularity, reliable summary-code\ntraceability, and consistency in generated summaries."}
{"id": "2508.02037", "pdf": "https://arxiv.org/pdf/2508.02037.pdf", "abs": "https://arxiv.org/abs/2508.02037", "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time", "authors": ["Huihan Li", "You Chen", "Siyuan Wang", "Yixin He", "Ninareh Mehrabi", "Rahul Gupta", "Xiang Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks."}
{"id": "2508.01545", "pdf": "https://arxiv.org/pdf/2508.01545.pdf", "abs": "https://arxiv.org/abs/2508.01545", "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs", "authors": ["Emilio Barkett", "Olivia Long", "Paul Kr√∂ger"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally."}
{"id": "2508.02038", "pdf": "https://arxiv.org/pdf/2508.02038.pdf", "abs": "https://arxiv.org/abs/2508.02038", "title": "Marco-Voice Technical Report", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Technical Report", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis."}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656.pdf", "abs": "https://arxiv.org/abs/2508.01656", "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "authors": ["Lucio La Cava", "Dominik Macko", "R√≥bert M√≥ro", "Ivan Srba", "Andrea Tagarelli"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios."}
{"id": "2508.02045", "pdf": "https://arxiv.org/pdf/2508.02045.pdf", "abs": "https://arxiv.org/abs/2508.02045", "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models", "authors": ["Soyeon Kim", "Jindong Wang", "Xing Xie", "Steven Euijong Whang"], "categories": ["cs.CL"], "comment": null, "summary": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git."}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674.pdf", "abs": "https://arxiv.org/abs/2508.01674", "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements."}
{"id": "2508.02053", "pdf": "https://arxiv.org/pdf/2508.02053.pdf", "abs": "https://arxiv.org/abs/2508.02053", "title": "ProCut: LLM Prompt Compression via Attribution Estimation", "authors": ["Zhentao Xu", "Fengyi Li", "Albert Chen", "Xiaofeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts."}
{"id": "2508.01736", "pdf": "https://arxiv.org/pdf/2508.01736.pdf", "abs": "https://arxiv.org/abs/2508.01736", "title": "Set the Stage: Enabling Storytelling with Multiple Robots through Roleplaying Metaphors", "authors": ["Tyrone Justin Sta Maria", "Faith Griffin", "Jordan Aiko Deja"], "categories": ["cs.RO", "cs.HC"], "comment": "3 pages, 2 figures, UIST Poster, adjunct proceedings", "summary": "Gestures are an expressive input modality for controlling multiple robots,\nbut their use is often limited by rigid mappings and recognition constraints.\nTo move beyond these limitations, we propose roleplaying metaphors as a\nscaffold for designing richer interactions. By introducing three roles:\nDirector, Puppeteer, and Wizard, we demonstrate how narrative framing can guide\nthe creation of diverse gesture sets and interaction styles. These roles enable\na variety of scenarios, showing how roleplay can unlock new possibilities for\nmulti-robot systems. Our approach emphasizes creativity, expressiveness, and\nintuitiveness as key elements for future human-robot interaction design."}
{"id": "2508.02074", "pdf": "https://arxiv.org/pdf/2508.02074.pdf", "abs": "https://arxiv.org/abs/2508.02074", "title": "The SMeL Test: A simple benchmark for media literacy in language models", "authors": ["Gustaf Ahdritz", "Anat Kleiman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it."}
{"id": "2508.01823", "pdf": "https://arxiv.org/pdf/2508.01823.pdf", "abs": "https://arxiv.org/abs/2508.01823", "title": "Unraveling the Connection: How Cognitive Workload Shapes Intent Recognition in Robot-Assisted Surgery", "authors": ["Mansi Sharma", "Antonio Kruger"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Robot-assisted surgery has revolutionized the healthcare industry by\nproviding surgeons with greater precision, reducing invasiveness, and improving\npatient outcomes. However, the success of these surgeries depends heavily on\nthe robotic system ability to accurately interpret the intentions of the\nsurgical trainee or even surgeons. One critical factor impacting intent\nrecognition is the cognitive workload experienced during the procedure. In our\nrecent research project, we are building an intelligent adaptive system to\nmonitor cognitive workload and improve learning outcomes in robot-assisted\nsurgery. The project will focus on achieving a semantic understanding of\nsurgeon intents and monitoring their mental state through an intelligent\nmulti-modal assistive framework. This system will utilize brain activity, heart\nrate, muscle activity, and eye tracking to enhance intent recognition, even in\nmentally demanding situations. By improving the robotic system ability to\ninterpret the surgeons intentions, we can further enhance the benefits of\nrobot-assisted surgery and improve surgery outcomes."}
{"id": "2508.02087", "pdf": "https://arxiv.org/pdf/2508.02087.pdf", "abs": "https://arxiv.org/abs/2508.02087", "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models", "authors": ["Jin Li", "Keyu Wang", "Shu Yang", "Zhuoran Zhang", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems."}
{"id": "2508.01853", "pdf": "https://arxiv.org/pdf/2508.01853.pdf", "abs": "https://arxiv.org/abs/2508.01853", "title": "Distinguishing Target and Non-Target Fixations with EEG and Eye Tracking in Realistic Visual Scenes", "authors": ["Mansi Sharma", "Camilo Andr√©s Mart√≠nez Mart√≠nez", "Benedikt Emanuel Wirth", "Antonio Kr√ºger", "Philipp M√ºller"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Distinguishing target from non-target fixations during visual search is a\nfundamental building block to understand users' intended actions and to build\neffective assistance systems. While prior research indicated the feasibility of\nclassifying target vs. non-target fixations based on eye tracking and\nelectroencephalography (EEG) data, these studies were conducted with explicitly\ninstructed search trajectories, abstract visual stimuli, and disregarded any\nscene context. This is in stark contrast with the fact that human visual search\nis largely driven by scene characteristics and raises questions regarding\ngeneralizability to more realistic scenarios. To close this gap, we, for the\nfirst time, investigate the classification of target vs. non-target fixations\nduring free visual search in realistic scenes. In particular, we conducted a\n36-participants user study using a large variety of 140 realistic visual search\nscenes in two highly relevant application scenarios: searching for icons on\ndesktop backgrounds and finding tools in a cluttered workshop. Our approach\nbased on gaze and EEG features outperforms the previous state-of-the-art\napproach based on a combination of fixation duration and saccade-related\npotentials. We perform extensive evaluations to assess the generalizability of\nour approach across scene types. Our approach significantly advances the\nability to distinguish between target and non-target fixations in realistic\nscenarios, achieving 83.6% accuracy in cross-user evaluations. This\nsubstantially outperforms previous methods based on saccade-related potentials,\nwhich reached only 56.9% accuracy."}
{"id": "2508.02094", "pdf": "https://arxiv.org/pdf/2508.02094.pdf", "abs": "https://arxiv.org/abs/2508.02094", "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection."}
{"id": "2508.01915", "pdf": "https://arxiv.org/pdf/2508.01915.pdf", "abs": "https://arxiv.org/abs/2508.01915", "title": "EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses", "authors": ["Akshay Paruchuri", "Sinan Hersek", "Lavisha Aggarwal", "Qiao Yang", "Xin Liu", "Achin Kulshrestha", "Andrea Colaco", "Henry Fuchs", "Ishan Chatterjee"], "categories": ["cs.CV", "cs.ET", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "comment": "15 pages, 6 figres, 6 tables. Accepted to ISMAR 2025 as a TVCG\n  journal paper", "summary": "All-day smart glasses are likely to emerge as platforms capable of continuous\ncontextual sensing, uniquely positioning them for unprecedented assistance in\nour daily lives. Integrating the multi-modal AI agents required for human\nmemory enhancement while performing continuous sensing, however, presents a\nmajor energy efficiency challenge for all-day usage. Achieving this balance\nrequires intelligent, context-aware sensor management. Our approach,\nEgoTrigger, leverages audio cues from the microphone to selectively activate\npower-intensive cameras, enabling efficient sensing while preserving\nsubstantial utility for human memory enhancement. EgoTrigger uses a lightweight\naudio model (YAMNet) and a custom classification head to trigger image capture\nfrom hand-object interaction (HOI) audio cues, such as the sound of a drawer\nopening or a medication bottle being opened. In addition to evaluating on the\nQA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement\nQuestion-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated\nfirst-person QA pairs from full-length Ego4D videos that were curated to ensure\nthat they contained audio, focusing on HOI moments critical for contextual\nunderstanding and memory. Our results show EgoTrigger can use 54% fewer frames\non average, significantly saving energy in both power-hungry sensing components\n(e.g., cameras) and downstream operations (e.g., wireless transmission), while\nachieving comparable performance on datasets for an episodic memory task. We\nbelieve this context-aware triggering strategy represents a promising direction\nfor enabling energy-efficient, functional smart glasses capable of all-day use\n-- supporting applications like helping users recall where they placed their\nkeys or information about their routine activities (e.g., taking medications)."}
{"id": "2508.02189", "pdf": "https://arxiv.org/pdf/2508.02189.pdf", "abs": "https://arxiv.org/abs/2508.02189", "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining", "authors": ["David Demitri Africa", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released."}
{"id": "2508.02094", "pdf": "https://arxiv.org/pdf/2508.02094.pdf", "abs": "https://arxiv.org/abs/2508.02094", "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection."}
{"id": "2508.02193", "pdf": "https://arxiv.org/pdf/2508.02193.pdf", "abs": "https://arxiv.org/abs/2508.02193", "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference", "authors": ["Yuxuan Song", "Zheng Zhang", "Cheng Luo", "Pengyang Gao", "Fan Xia", "Hao Luo", "Zheng Li", "Yuehang Yang", "Hongli Yu", "Xingwei Qu", "Yuwei Fu", "Jing Su", "Ge Zhang", "Wenhao Huang", "Mingxuan Wang", "Lin Yan", "Xiaoying Jia", "Jingjing Liu", "Wei-Ying Ma", "Ya-Qin Zhang", "Yonghui Wu", "Hao Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": "Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion", "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models."}
{"id": "2508.02096", "pdf": "https://arxiv.org/pdf/2508.02096.pdf", "abs": "https://arxiv.org/abs/2508.02096", "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches", "authors": ["Raj Mahmud", "Yufeng Wu", "Abdullah Bin Sawad", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.IR", "cs.AI", "cs.HC", "H.3.3; H.5.2; I.2.7"], "comment": "Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables", "summary": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM.\n  Our findings reveal persistent limitations: post hoc surveys dominate,\nturn-level affective UX constructs are rarely assessed, and adaptive behaviours\nare seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,\nincluding epistemic opacity and verbosity, yet evaluations infrequently address\nthese issues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices."}
{"id": "2508.02208", "pdf": "https://arxiv.org/pdf/2508.02208.pdf", "abs": "https://arxiv.org/abs/2508.02208", "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems", "authors": ["Yebo Peng", "Zixiang Liu", "Yaoming Li", "Zhizhuo Yang", "Xinye Xu", "Bowen Ye", "Weijun Yuan", "Zihan Wang", "Tong Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems."}
{"id": "2508.02176", "pdf": "https://arxiv.org/pdf/2508.02176.pdf", "abs": "https://arxiv.org/abs/2508.02176", "title": "Highly Interactive Testing for Uninterrupted Development Flow", "authors": ["Andrew Tropin"], "categories": ["cs.SE", "cs.HC", "D.2.3; D.2.6; D.2.5; H.5.2"], "comment": "12 pages, ICFP-2025", "summary": "Highly interactive development environments (HIDEs) enable uninterrupted\ndevelopment flow through continuous program evolution and rapid hypothesis\nchecking. However, traditional testing approaches -- typically executed\nseparately via CLI -- isolate tests from HIDE tooling (interactive debuggers,\nvalue and stack inspectors, etc.) and introduce disruptive delays due to coarse\nexecution granularity and lack of runtime context. This disconnect breaks\ndevelopment flow by exceeding critical attention thresholds. In this paper we\npresent a library that provides runtime representation for tests, allowing\ntight integration with HIDEs, and enabling immediate access to HIDE tooling in\nthe context of test failure. We then describe development workflows enhanced\nwith testing and demonstrate how they achieve subsecond test reexecution times\ncrucial for maintaining developer focus."}
{"id": "2508.02241", "pdf": "https://arxiv.org/pdf/2508.02241.pdf", "abs": "https://arxiv.org/abs/2508.02241", "title": "Isolating Culture Neurons in Multilingual Large Language Models", "authors": ["Danial Namazifard", "Lukas Galke"], "categories": ["cs.CL"], "comment": "18 pages, 13 figures", "summary": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons ."}
{"id": "2508.02354", "pdf": "https://arxiv.org/pdf/2508.02354.pdf", "abs": "https://arxiv.org/abs/2508.02354", "title": "Detecting COPD Through Speech Analysis: A Dataset of Danish Speech and Machine Learning Approach", "authors": ["Cuno Sankey-Olsen", "Rasmus Hvass Olesen", "Tobias Oliver Eberhard", "Andreas Triantafyllopoulos", "Bj√∂rn Schuller", "Ilhan Aslan"], "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "comment": null, "summary": "Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating\ndisease affecting millions around the world. Its early detection using\nnon-invasive means could enable preventive interventions that improve quality\nof life and patient outcomes, with speech recently shown to be a valuable\nbiomarker. Yet, its validity across different linguistic groups remains to be\nseen. To that end, audio data were collected from 96 Danish participants\nconducting three speech tasks (reading, coughing, sustained vowels). Half of\nthe participants were diagnosed with different levels of COPD and the other\nhalf formed a healthy control group. Subsequently, we investigated different\nbaseline models using openSMILE features and learnt x-vector embeddings. We\nobtained a best accuracy of 67% using openSMILE features and logistic\nregression. Our findings support the potential of speech-based analysis as a\nnon-invasive, remote, and scalable screening tool as part of future COPD\nhealthcare solutions."}
{"id": "2508.02256", "pdf": "https://arxiv.org/pdf/2508.02256.pdf", "abs": "https://arxiv.org/abs/2508.02256", "title": "Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders", "authors": ["Belen Alastruey", "Jo√£o Maria Janeiro", "Alexandre Allauzen", "Maha Elbayad", "Lo√Øc Barrault", "Marta R. Costa-juss√†"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present a comprehensive study of language interference in\nencoder-only Transformer models across 83 languages. We construct an\ninterference matrix by training and evaluating small BERT-like models on all\npossible language pairs, providing a large-scale quantification of\ncross-lingual interference. Our analysis reveals that interference between\nlanguages is asymmetrical and that its patterns do not align with traditional\nlinguistic characteristics, such as language family, nor with proxies like\nembedding similarity, but instead better relate to script. Finally, we\ndemonstrate that the interference matrix effectively predicts performance on\ndownstream tasks, serving as a tool to better design multilingual models to\nobtain optimal performance."}
{"id": "2508.02630", "pdf": "https://arxiv.org/pdf/2508.02630.pdf", "abs": "https://arxiv.org/abs/2508.02630", "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce", "authors": ["Amine Allouah", "Omar Besbes", "Josu√© D Figueroa", "Yash Kanoria", "Akshit Kumar"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem."}
{"id": "2508.02260", "pdf": "https://arxiv.org/pdf/2508.02260.pdf", "abs": "https://arxiv.org/abs/2508.02260", "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 20 figures", "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs."}
{"id": "2412.02891", "pdf": "https://arxiv.org/pdf/2412.02891.pdf", "abs": "https://arxiv.org/abs/2412.02891", "title": "OriStitch: A Machine Embroidery Workflow to Turn Existing Fabrics into Self-Folding 3D Textiles", "authors": ["Zekun Chang", "Yixuan Gao", "Yuta Noma", "Shuo Feng", "Xinyi Yang", "Kazuhiro Shinoda", "Tung D. Ta", "Koji Yatani", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara", "Koya Narumi", "Francois Guimbretiere", "Thijs Roumen"], "categories": ["cs.HC"], "comment": null, "summary": "OriStitch is a computational fabrication workflow to turn existing flat\nfabrics into self-folding 3D structures. Users turn fabrics into self-folding\nsheets by machine embroidering functional threads in specific patterns on\nfabrics, and then apply heat to deform the structure into a target 3D\nstructure. OriStitch is compatible with a range of existing materials (e.g.,\nleather, woven fabric, and denim).\n  We present the design of specific embroidered hinges that fully close under\nexposure to heat. We discuss the stitch pattern design, thread and fabric\nselection, and heating conditions. To allow users to create 3D textiles using\nour hinges, we create a tool to convert 3D meshes to 2D stitch patterns\nautomatically, as well as an end-to-end fabrication and actuation workflow. To\nvalidate this workflow, we designed and fabricated a cap (303 hinges), a\nhandbag (338 hinges), and a cover for an organically shaped vase (140 hinges).\n  In technical evaluation, we found that our tool successfully converted 23/28\nmodels (textures and volumetric objects) found in related papers. We also\ndemonstrate the folding performance across different materials (suede leather,\ncork, Neoprene, and felt)."}
{"id": "2508.02268", "pdf": "https://arxiv.org/pdf/2508.02268.pdf", "abs": "https://arxiv.org/abs/2508.02268", "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System", "authors": ["Serry Sibaee", "Omer Nacar", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "categories": ["cs.CL"], "comment": null, "summary": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of \\textbf{4.01 out of 5.0}\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication."}
{"id": "2502.02194", "pdf": "https://arxiv.org/pdf/2502.02194.pdf", "abs": "https://arxiv.org/abs/2502.02194", "title": "Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study", "authors": ["Giuseppe Desolda", "Andrea Esposito", "Francesco Greco", "Cesare Tucci", "Paolo Buono", "Antonio Piccinno"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Integrated Development Environments increasingly implement AI-powered code\ncompletion tools (CCTs), which promise to enhance developer efficiency,\naccuracy, and productivity. However, interaction challenges with CCTs persist,\nmainly due to mismatches between developers' mental models and the\nunpredictable behavior of AI-generated suggestions, which is an aspect\nunderexplored in the literature. We conducted an elicitation study with 56\ndevelopers using co-design workshops to elicit their mental models when\ninteracting with CCTs. Different important findings that might drive the\ninteraction design with CCTs emerged. For example, developers expressed diverse\npreferences on when and how code suggestions should be triggered (proactive,\nmanual, hybrid), where and how they are displayed (inline, sidebar, popup,\nchatbot), as well as the level of detail. It also emerged that developers need\nto be supported by customization of activation timing, display modality,\nsuggestion granularity, and explanation content, to better fit the CCT to their\npreferences. To demonstrate the feasibility of these and the other guidelines\nthat emerged during the study, we developed ATHENA, a proof-of-concept CCT that\ndynamically adapts to developers' coding preferences and environments, ensuring\nseamless integration into diverse workflows."}
{"id": "2508.02271", "pdf": "https://arxiv.org/pdf/2508.02271.pdf", "abs": "https://arxiv.org/abs/2508.02271", "title": "Dynaword: From One-shot to Continuously Developed Datasets", "authors": ["Kenneth Enevoldsen", "Kristian N√∏rgaard Jensen", "Jan Kostkan", "Bal√°zs Szab√≥", "M√°rton Kardos", "Kirten Vad", "Andrea Blasi N√∫√±ez", "Gianluca Barmina", "Jacob Nielsen", "Rasmus Larsen", "Peter Vahlstrup", "Per M√∏ldrup Dalum", "Desmond Elliott", "Lukas Galke", "Peter Schneider-Kamp", "Kristoffer Nielbo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution."}
{"id": "2504.14539", "pdf": "https://arxiv.org/pdf/2504.14539.pdf", "abs": "https://arxiv.org/abs/2504.14539", "title": "Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory", "authors": ["Linkun Liu", "Jian Sun", "Ye Tian"], "categories": ["cs.HC"], "comment": null, "summary": "The application of external human-machine interface (EHMI) on autonomous\nvehicles (AVs) facilitates information exchange. Existing research fails to\nconsider the impact of the sequence of actions, as well as the effects of EHMI\napplications and deception, raising the question of whether benevolent,\nwell-intentioned deception should be permitted (i.e., misleading statements\nthat are intended to benefit both parties). We established a game theory based\nEHMI information disclosure framework for AVs in this study. In considering\nbenevolent deception, this framework divided the decision-making process into\nthree stages, respectively encompassing three key questions: whether to\ndisclose, when to disclose, and what type of intention information to disclose.\nThe results show that theoretical advantages of deception exist in certain\ncases when AV expects to maximize the safety of the interaction. In 40 out of\n484 cases (8.3%), safety can be enhanced through successful deception. Those\nsuccessful deceptions fall into two categories: 1) In 28 of these cases, the\nstraight-going AV expected the left-turning human-driven vehicle (HV) to yield,\nwhile HV exhibited lower speed and higher acceleration; 2) In 12 of these\ncases, AV expected HV to proceed first, while HV exhibited higher speed and\nlower acceleration. We also conducted a VR-based driving simulation experiment,\nand the results confirmed our conclusion. Additionally, we found that when\nparticipants had low trust in the EHMI, its use negatively impacted interaction\nefficiency instead. This study serves as an exploratory behavioral mechanism\nstudy based on specific hypotheses for future EHMI design and ethical\ndecision-making of autonomous driving system."}
{"id": "2508.02290", "pdf": "https://arxiv.org/pdf/2508.02290.pdf", "abs": "https://arxiv.org/abs/2508.02290", "title": "A French Version of the OLDI Seed Corpus", "authors": ["Malik Marmonier", "Beno√Æt Sagot", "Rachel Bawden"], "categories": ["cs.CL"], "comment": null, "summary": "We present the first French partition of the OLDI Seed Corpus, our submission\nto the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its\ncreation process, which involved using multiple machine translation systems and\na custom-built interface for post-editing by qualified native speakers. We also\nhighlight the unique translation challenges presented by the source data, which\ncombines highly technical, encyclopedic terminology with the stylistic\nirregularities characteristic of user-generated content taken from Wikipedia.\nThis French corpus is not an end in itself, but is intended as a crucial pivot\nresource to facilitate the collection of parallel corpora for the\nunder-resourced regional languages of France."}
{"id": "2505.11715", "pdf": "https://arxiv.org/pdf/2505.11715.pdf", "abs": "https://arxiv.org/abs/2505.11715", "title": "ConflictLens: LLM-Based Conflict Resolution Training in Romantic Relationship", "authors": ["Jiwon Chun", "Gefei Zhang", "Meng Xia"], "categories": ["cs.HC"], "comment": "2 figures. To appear in a poster at ACM UIST 2025", "summary": "Our poster presents ConflictLens, a three-stage simulation system powered by\nlarge language models (LLMs) and grounded in psychological theory, designed to\nhelp users reflect on and practice conflict resolution in romantic\nrelationships. Users can upload real conflict scenarios to receive evaluation\nof behavioral patterns, reflect on conflicts by annotating their negative\nbehaviors, and practice different conflict resolution strategies in\nAI-simulated duologues. Initial evaluation by three domain experts suggests\nthat ConflictLens offers a realistic experience and effectively supports\nself-guided reflection and communication practice in romantic relationships."}
{"id": "2508.02296", "pdf": "https://arxiv.org/pdf/2508.02296.pdf", "abs": "https://arxiv.org/abs/2508.02296", "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks", "authors": ["Ilias Triantafyllopoulos", "Renyi Qu", "Salvatore Giorgi", "Brenda Curtis", "Lyle H. Ungar", "Jo√£o Sedoc"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance."}
{"id": "2507.20355", "pdf": "https://arxiv.org/pdf/2507.20355.pdf", "abs": "https://arxiv.org/abs/2507.20355", "title": "CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration", "authors": ["Zheng Wei", "Hongtao Wu", "Lvmin Zhang", "Xian Xu", "Yefeng Zheng", "Pan Hui", "Maneesh Agrawala", "Huamin Qu", "Anyi Rao"], "categories": ["cs.HC"], "comment": "UIST 2025", "summary": "Effective communication between directors and cinematographers is fundamental\nin film production, yet traditional approaches relying on visual references and\nhand-drawn storyboards often lack the efficiency and precision necessary during\npre-production. We present CineVision, an AI-driven platform that integrates\nscriptwriting with real-time visual pre-visualization to bridge this\ncommunication gap. By offering dynamic lighting control, style emulation based\non renowned filmmakers, and customizable character design, CineVision enables\ndirectors to convey their creative vision with heightened clarity and rapidly\niterate on scene composition. In a 24-participant lab study, CineVision yielded\nshorter task times and higher usability ratings than two baseline methods,\nsuggesting a potential to ease early-stage communication and accelerate\nstoryboard drafts under controlled conditions. These findings underscore\nCineVision's potential to streamline pre-production processes and foster deeper\ncreative synergy among filmmaking teams, particularly for new collaborators.\nOur code and demo are available at https://github.com/TonyHongtaoWu/CineVision."}
{"id": "2508.02308", "pdf": "https://arxiv.org/pdf/2508.02308.pdf", "abs": "https://arxiv.org/abs/2508.02308", "title": "LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training", "authors": ["Sikui Zhang", "Guangze Gao", "Ziyun Gan", "Chunfeng Yuan", "Zefeng Lin", "Houwen Peng", "Bing Li", "Weiming Hu"], "categories": ["cs.CL"], "comment": "13 pages, 9 figures", "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE."}
{"id": "2507.21411", "pdf": "https://arxiv.org/pdf/2507.21411.pdf", "abs": "https://arxiv.org/abs/2507.21411", "title": "InSituTale: Enhancing Augmented Data Storytelling with Physical Objects", "authors": ["Kentaro Takahira", "Yue Yu", "Takanori Fujiwara", "Suzuki Ryo", "Huamin Qu"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Augmented data storytelling enhances narrative delivery by integrating\nvisualizations with physical environments and presenter actions. Existing\nsystems predominantly rely on body gestures or speech to control\nvisualizations, leaving interactions with physical objects largely\nunderexplored. We introduce augmented physical data storytelling, an approach\nenabling presenters to manipulate visualizations through physical object\ninteractions. To inform this approach, we first conducted a survey of\ndata-driven presentations to identify common visualization commands. We then\nconducted workshops with nine HCI/VIS researchers to collect mappings between\nphysical manipulations and these commands. Guided by these insights, we\ndeveloped InSituTale, a prototype that combines object tracking via a depth\ncamera with Vision-LLM for detecting real-world events. Through physical\nmanipulations, presenters can dynamically execute various visualization\ncommands, delivering cohesive data storytelling experiences that blend physical\nand digital elements. A user study with 12 participants demonstrated that\nInSituTale enables intuitive interactions, offers high utility, and facilitates\nan engaging presentation experience."}
{"id": "2508.02317", "pdf": "https://arxiv.org/pdf/2508.02317.pdf", "abs": "https://arxiv.org/abs/2508.02317", "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."}
{"id": "2507.21462", "pdf": "https://arxiv.org/pdf/2507.21462.pdf", "abs": "https://arxiv.org/abs/2507.21462", "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals", "authors": ["Tingying He", "Maggie McCracken", "Daniel Hajas", "Sarah Creem-Regehr", "Alexander Lex"], "categories": ["cs.HC"], "comment": null, "summary": "We investigate whether tactile charts support comprehension and learning of\ncomplex visualizations for blind and low-vision (BLV) individuals and\ncontribute four tactile chart designs and an interview study. Visualizations\nare powerful tools for conveying data, yet BLV individuals typically can rely\nonly on assistive technologies -- primarily alternative texts -- to access this\ninformation. Prior research shows the importance of mental models of chart\ntypes for interpreting these descriptions, yet BLV individuals have no means to\nbuild such a mental model based on images of visualizations. Tactile charts\nshow promise to fill this gap in supporting the process of building mental\nmodels. Yet studies on tactile data representations mostly focus on simple\nchart types, and it is unclear whether they are also appropriate for more\ncomplex charts as would be found in scientific publications. Working with two\nBLV researchers, we designed 3D-printed tactile template charts with\nexploration instructions for four advanced chart types: UpSet plots, violin\nplots, clustered heatmaps, and faceted line charts. We then conducted an\ninterview study with 12 BLV participants comparing whether using our tactile\ntemplates improves mental models and understanding of charts and whether this\nunderstanding translates to novel datasets experienced through alt texts.\nThematic analysis shows that tactile models support chart type understanding\nand are the preferred learning method by BLV individuals. We also report\nparticipants' opinions on tactile chart design and their role in BLV education."}
{"id": "2508.02322", "pdf": "https://arxiv.org/pdf/2508.02322.pdf", "abs": "https://arxiv.org/abs/2508.02322", "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis", "authors": ["Yuzhuang Xu", "Xu Han", "Yuanchi Zhang", "Yixuan Wang", "Yijun Liu", "Shiyu Ji", "Qingfu Zhu", "Wanxiang Che"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 9 figures, 7 tables", "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU."}
{"id": "2507.22952", "pdf": "https://arxiv.org/pdf/2507.22952.pdf", "abs": "https://arxiv.org/abs/2507.22952", "title": "Automated Label Placement on Maps via Large Language Models", "authors": ["Harry Shomer", "Jiejun Xu"], "categories": ["cs.HC", "cs.CV", "cs.LG"], "comment": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "summary": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE."}
{"id": "2508.02360", "pdf": "https://arxiv.org/pdf/2508.02360.pdf", "abs": "https://arxiv.org/abs/2508.02360", "title": "Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models", "authors": ["Jiayi Zhang", "Shu Yang", "Junchao Wu", "Derek F. Wong", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization."}
{"id": "2507.23298", "pdf": "https://arxiv.org/pdf/2507.23298.pdf", "abs": "https://arxiv.org/abs/2507.23298", "title": "Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System", "authors": ["Kazushi Kato", "Koji Inoue", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted by 27th ACM International Conference on Multimodal\n  Interaction (ICMI '25), Long paper", "summary": "In human dialogue, nonverbal information such as nodding and facial\nexpressions is as crucial as verbal information, and spoken dialogue systems\nare also expected to express such nonverbal behaviors. We focus on nodding,\nwhich is critical in an attentive listening system, and propose a model that\npredicts both its timing and type in real time. The proposed model builds on\nthe voice activity projection (VAP) model, which predicts voice activity from\nboth listener and speaker audio. We extend it to prediction of various types of\nnodding in a continuous and real-time manner unlike conventional models. In\naddition, the proposed model incorporates multi-task learning with verbal\nbackchannel prediction and pretraining on general dialogue data. In the timing\nand type prediction task, the effectiveness of multi-task learning was\nsignificantly demonstrated. We confirmed that reducing the processing rate\nenables real-time operation without a substantial drop in accuracy, and\nintegrated the model into an avatar attentive listening system. Subjective\nevaluations showed that it outperformed the conventional method, which always\ndoes nodding in sync with verbal backchannel. The code and trained models are\navailable at https://github.com/MaAI-Kyoto/MaAI."}
{"id": "2508.02401", "pdf": "https://arxiv.org/pdf/2508.02401.pdf", "abs": "https://arxiv.org/abs/2508.02401", "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation", "authors": ["Xiaolin Lin", "Jingcun Wang", "Olga Kondrateva", "Yiyu Shi", "Bing Li", "Grace Li Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."}
{"id": "2508.00103", "pdf": "https://arxiv.org/pdf/2508.00103.pdf", "abs": "https://arxiv.org/abs/2508.00103", "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Luana Bianchini", "Mariana Alves", "Marcelo Marinho", "Thomaz Veloso", "Valmir Macario", "Diego Dermeval", "Thales Vieira", "Ig Bittencourt", "Seiji Isotani"], "categories": ["cs.HC", "cs.AI", "68T01", "H.5.0; I.2.0"], "comment": "Article accepted in the International Journal of Human-Computer\n  Interaction", "summary": "Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments."}
{"id": "2508.02426", "pdf": "https://arxiv.org/pdf/2508.02426.pdf", "abs": "https://arxiv.org/abs/2508.02426", "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding", "authors": ["Linyu Li", "Zhi Jin", "Yuanpeng He", "Dongming Jin", "Yichi Zhang", "Haoran Duan", "Nyima Tash"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models."}
{"id": "2508.00737", "pdf": "https://arxiv.org/pdf/2508.00737.pdf", "abs": "https://arxiv.org/abs/2508.00737", "title": "How LLMs are Shaping the Future of Virtual Reality", "authors": ["S√ºeda √ñzkaya", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC", "cs.AI"], "comment": "Pre-print", "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems."}
{"id": "2508.02430", "pdf": "https://arxiv.org/pdf/2508.02430.pdf", "abs": "https://arxiv.org/abs/2508.02430", "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications", "authors": ["Robin Nowak", "Patrick Figge", "Carolin Haeussler"], "categories": ["cs.CL"], "comment": null, "summary": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation."}
{"id": "2410.08676", "pdf": "https://arxiv.org/pdf/2410.08676.pdf", "abs": "https://arxiv.org/abs/2410.08676", "title": "Bridging Developer Needs and Feasible Features for AI Assistants in IDEs", "authors": ["Agnia Sergeyuk", "Ekaterina Koshchenko", "Ilya Zakharov", "Timofey Bryksin", "Maliheh Izadi"], "categories": ["cs.SE", "cs.HC"], "comment": "11 pages, 2 figures, 1 table submitted to ASE Industry Showcase 2025", "summary": "Despite the increasing presence of AI assistants in Integrated Development\nEnvironments, it remains unclear what developers actually need from these tools\nand which features are likely to be implemented in practice. To investigate\nthis gap, we conducted a two-phase study. First, we interviewed 35 professional\ndevelopers from three user groups (Adopters, Churners, and Non-Users) to\nuncover unmet needs and expectations. Our analysis revealed five key areas:\nTechnology Improvement, Interaction, and Alignment, as well as Simplifying\nSkill Building, and Programming Tasks. We then examined the feasibility of\naddressing selected needs through an internal prediction market involving 102\npractitioners. The results demonstrate a strong alignment between the\ndevelopers' needs and the practitioners' judgment for features focused on\nimplementation and context awareness. However, features related to proactivity\nand maintenance remain both underestimated and technically unaddressed. Our\nfindings reveal gaps in current AI support and provide practical directions for\ndeveloping more effective and sustainable in-IDE AI systems."}
{"id": "2508.02452", "pdf": "https://arxiv.org/pdf/2508.02452.pdf", "abs": "https://arxiv.org/abs/2508.02452", "title": "LatentPrompt: Optimizing Promts in Latent Space", "authors": ["Mateusz Bystro≈Ñski", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks."}
{"id": "2410.14334", "pdf": "https://arxiv.org/pdf/2410.14334.pdf", "abs": "https://arxiv.org/abs/2410.14334", "title": "Evaluating the evaluators: Towards human-aligned metrics for missing markers reconstruction", "authors": ["Taras Kucherenko", "Derek Peristy", "Judith B√ºtepage"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted at the ACM International Conference on Multimedia 2025 (ACM\n  MM'25)", "summary": "Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. Additionally, we introduce and\nevaluate a set of better-correlated metrics that can drive progress in the\nfield."}
{"id": "2508.02498", "pdf": "https://arxiv.org/pdf/2508.02498.pdf", "abs": "https://arxiv.org/abs/2508.02498", "title": "Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity", "authors": ["Md Tasin Abir", "Arpita Chowdhury", "Ashfia Rahman"], "categories": ["cs.CL"], "comment": "10 pages, 9 figures", "summary": "This study investigates how Facebook shaped collective identity during the\nJuly 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.\nDuring government repression, protesters turned to Facebook as a central space\nfor resistance, where multimodal expressions, images, memes, videos, hashtags,\nand satirical posts played an important role in unifying participants. Using a\nqualitative approach, this research analyzes visual rhetoric, verbal discourse,\nand digital irony to reveal how shared symbols, protest art, and slogans built\na sense of solidarity. Key elements included the symbolic use of red, the\nironic metaphorical use of the term \"Razakar\", and the widespread sharing of\nvisuals representing courage, injustice, and resistance. The findings show that\nthe combination of visual and verbal strategies on Facebook not only mobilized\npublic sentiment, but also built a strong collective identity that challenged\nauthoritarian narratives. This study tries to demonstrate how online platforms\ncan serve as powerful tools for identity construction and political\nmobilization in the digital age."}
{"id": "2502.16395", "pdf": "https://arxiv.org/pdf/2502.16395.pdf", "abs": "https://arxiv.org/abs/2502.16395", "title": "AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science", "authors": ["Qiuhai Zeng", "Claire Jin", "Xinyue Wang", "Yuhan Zheng", "Qunhua Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate data analysis\nthrough executable code generation. Yet, data science tasks often admit\nmultiple statistically valid solutions, e.g. different modeling strategies,\nmaking it critical to understand the reasoning behind analyses, not just their\noutcomes. While manual review of LLM-generated code can help ensure statistical\nsoundness, it is labor-intensive and requires expertise. A more scalable\napproach is to evaluate the underlying workflows - the logical plans guiding\ncode generation. However, it remains unclear how to assess whether a\nLLM-generated workflow supports reproducible implementations.\n  To address this, we present $\\it{AIRepr}$, an $\\it{A}$nalyst -\n$\\it{I}$nspector framework for automatically evaluating and improving the\n$\\it{Repr}$oducibility of LLM-generated data analysis workflows. Our framework\nis grounded in statistical principles and supports scalable, automated\nassessment. We introduce two novel reproducibility-enhancing prompting\nstrategies and benchmark them against standard prompting across 15\nanalyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our\nfindings show that workflows with higher reproducibility also yield more\naccurate analyses, and that reproducibility-enhancing prompts substantially\nimprove both metrics. This work provides a foundation for more transparent,\nreliable, and efficient human-AI collaboration in data science. Our code is\npublicly available."}
{"id": "2508.02502", "pdf": "https://arxiv.org/pdf/2508.02502.pdf", "abs": "https://arxiv.org/abs/2508.02502", "title": "From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks", "authors": ["Shuzhou Yuan", "Zhan Qu", "Mario Tawfelis", "Michael F√§rber"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition."}
{"id": "2504.17921", "pdf": "https://arxiv.org/pdf/2504.17921.pdf", "abs": "https://arxiv.org/abs/2504.17921", "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "authors": ["Mateo Espinosa Zarlenga", "Gabriele Dominici", "Pietro Barbiero", "Zohreh Shams", "Mateja Jamnik"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "comment": "Presented at the Forty-Second International Conference on Machine\n  Learning (ICML 2025). Post-conference manuscript", "summary": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions."}
{"id": "2508.02513", "pdf": "https://arxiv.org/pdf/2508.02513.pdf", "abs": "https://arxiv.org/abs/2508.02513", "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit", "authors": ["Tanja Baeumel", "Daniil Gurgurov", "Yusser al Ghussin", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks."}
{"id": "2507.10644", "pdf": "https://arxiv.org/pdf/2507.10644.pdf", "abs": "https://arxiv.org/abs/2507.10644", "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "authors": ["Tatiana Petrova", "Boris Bliznioukov", "Aleksandr Puzikov", "Radu State"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA."}
{"id": "2508.02515", "pdf": "https://arxiv.org/pdf/2508.02515.pdf", "abs": "https://arxiv.org/abs/2508.02515", "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs", "authors": ["Zhan Qu", "Shuzhou Yuan", "Michael F√§rber"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts."}
{"id": "2508.02527", "pdf": "https://arxiv.org/pdf/2508.02527.pdf", "abs": "https://arxiv.org/abs/2508.02527", "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2", "authors": ["Jack Merullo", "Arjun Khurana", "Oliver McLaughlin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so."}
{"id": "2508.02532", "pdf": "https://arxiv.org/pdf/2508.02532.pdf", "abs": "https://arxiv.org/abs/2508.02532", "title": "Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction", "authors": ["Karan Reddy", "Mayukha Pal"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications."}
{"id": "2508.02540", "pdf": "https://arxiv.org/pdf/2508.02540.pdf", "abs": "https://arxiv.org/abs/2508.02540", "title": "What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)", "authors": ["Anastasia Zhukova", "Terry Ruas", "Felix Hamborg", "Karsten Donnay", "Bela Gipp"], "categories": ["cs.CL"], "comment": "published in the Proceedings of the 2023 ACM/IEEE Joint Conference on\n  Digital Libraries", "summary": "In a world overwhelmed with news, determining which information comes from\nreliable sources or how neutral is the reported information in the news\narticles poses a challenge to news readers. In this paper, we propose a\nmethodology for automatically identifying bias by commission, omission, and\nsource selection (COSS) as a joint three-fold objective, as opposed to the\nprevious work separately addressing these types of bias. In a pipeline concept,\nwe describe the goals and tasks of its steps toward bias identification and\nprovide an example of a visualization that leverages the extracted features and\npatterns of text reuse."}
{"id": "2508.02555", "pdf": "https://arxiv.org/pdf/2508.02555.pdf", "abs": "https://arxiv.org/abs/2508.02555", "title": "Building and Aligning Comparable Corpora", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "categories": ["cs.CL", "I.2.7"], "comment": "27 pages, 11 figures", "summary": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level."}
{"id": "2508.02556", "pdf": "https://arxiv.org/pdf/2508.02556.pdf", "abs": "https://arxiv.org/abs/2508.02556", "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks", "authors": ["Ali Noori", "Pratik Devkota", "Somya Mohanty", "Prashanti Manda"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment."}
{"id": "2508.02558", "pdf": "https://arxiv.org/pdf/2508.02558.pdf", "abs": "https://arxiv.org/abs/2508.02558", "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction", "authors": ["Yuerong Song", "Xiaoran Liu", "Ruixiao Li", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "11 pages, 6 figures", "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."}
{"id": "2508.02573", "pdf": "https://arxiv.org/pdf/2508.02573.pdf", "abs": "https://arxiv.org/abs/2508.02573", "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs", "authors": ["J√©r√©mie Dentan", "Davide Buscaldi", "Sonia Vanier"], "categories": ["cs.CL"], "comment": null, "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization."}
{"id": "2508.02574", "pdf": "https://arxiv.org/pdf/2508.02574.pdf", "abs": "https://arxiv.org/abs/2508.02574", "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare", "authors": ["Eman Alamoudi", "Ellis Solaiman"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling."}
{"id": "2508.02584", "pdf": "https://arxiv.org/pdf/2508.02584.pdf", "abs": "https://arxiv.org/abs/2508.02584", "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification", "authors": ["Ming Pok Ng", "Junqi Jiang", "Gabriel Freedman", "Antonio Rago", "Francesca Toni"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs."}
{"id": "2508.02591", "pdf": "https://arxiv.org/pdf/2508.02591.pdf", "abs": "https://arxiv.org/abs/2508.02591", "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks", "authors": ["Omri Uzan", "Yuval Pinter"], "categories": ["cs.CL"], "comment": null, "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks."}
{"id": "2508.02618", "pdf": "https://arxiv.org/pdf/2508.02618.pdf", "abs": "https://arxiv.org/abs/2508.02618", "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation", "authors": ["Jianxiang Zang", "Meiling Ning", "Shihan Dou", "Jiazheng Zhang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM."}
{"id": "2508.02631", "pdf": "https://arxiv.org/pdf/2508.02631.pdf", "abs": "https://arxiv.org/abs/2508.02631", "title": "Pointer: Linear-Complexity Long-Range Modeling without Pre-training", "authors": ["Zixi Li"], "categories": ["cs.CL"], "comment": "Submitted to Nordic AI Meet 2025", "summary": "We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies."}
{"id": "2508.02635", "pdf": "https://arxiv.org/pdf/2508.02635.pdf", "abs": "https://arxiv.org/abs/2508.02635", "title": "Test Set Quality in Multilingual LLM Evaluation", "authors": ["Kranti Chalamalasetti", "Gabriel Bernier-Colborne", "Yvan Gauthier", "Sowmya Vajjala"], "categories": ["cs.CL"], "comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total", "summary": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues."}
{"id": "2505.09805", "pdf": "https://arxiv.org/pdf/2505.09805.pdf", "abs": "https://arxiv.org/abs/2505.09805", "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models", "authors": ["Aditya Nagori", "Ayush Gautam", "Matthew O. Wiens", "Vuong Nguyen", "Nathan Kenya Mugisha", "Jerome Kabakyenga", "Niranjan Kissoon", "John Mark Ansermino", "Rishikesan Kamaleswaran"], "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": "11 pages, 2 Figures, 1 Table", "summary": "Clustering patient subgroups is essential for personalized care and efficient\nresource use. Traditional clustering methods struggle with high-dimensional,\nheterogeneous healthcare data and lack contextual understanding. This study\nevaluates Large Language Model (LLM) based clustering against classical methods\nusing a pediatric sepsis dataset from a low-income country (LIC), containing\n2,686 records with 28 numerical and 119 categorical variables. Patient records\nwere serialized into text with and without a clustering objective. Embeddings\nwere generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with\nlow-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was\napplied to these embeddings. Classical comparisons included K-Medoids\nclustering on UMAP and FAMD-reduced mixed data. Silhouette scores and\nstatistical tests evaluated cluster quality and distinctiveness.\nStella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B\nwith the clustering objective performed better with higher number of clusters,\nidentifying subgroups with distinct nutritional, clinical, and socioeconomic\nprofiles. LLM-based methods outperformed classical techniques by capturing\nricher context and prioritizing key features. These results highlight potential\nof LLMs for contextual phenotyping and informed decision-making in\nresource-limited settings."}
{"id": "2508.00838", "pdf": "https://arxiv.org/pdf/2508.00838.pdf", "abs": "https://arxiv.org/abs/2508.00838", "title": "The Attribution Crisis in LLM Search Results", "authors": ["Ilan Strauss", "Jangho Yang", "Tim O'Reilly", "Sruly Rosenblat", "Isobel Moure"], "categories": ["cs.DL", "cs.AI", "cs.CL"], "comment": null, "summary": "Web-enabled LLMs frequently answer queries without crediting the web pages\nthey consume, creating an \"attribution gap\" - the difference between relevant\nURLs read and those actually cited. Drawing on approximately 14,000 real-world\nLMArena conversation logs with search-enabled LLM systems, we document three\nexploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI\nGPT-4o responses are generated without explicitly fetching any online content;\n2) No citation: Gemini provides no clickable citation source in 92% of answers;\n3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant\npages per query but cites only three to four. A negative binomial hurdle model\nshows that the average query answered by Gemini or Sonar leaves about 3\nrelevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained\nby its selective log disclosures rather than by better attribution. Citation\nefficiency - extra citations provided per additional relevant web page visited\n- varies widely across models, from 0.19 to 0.45 on identical queries,\nunderscoring that retrieval design, not technical limits, shapes ecosystem\nimpact. We recommend a transparent LLM search architecture based on\nstandardized telemetry and full disclosure of search traces and citation logs."}
{"id": "2508.00881", "pdf": "https://arxiv.org/pdf/2508.00881.pdf", "abs": "https://arxiv.org/abs/2508.00881", "title": "Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models", "authors": ["Vijja Wichitwechkarn", "Charles Fox", "Ruchi Choudhary"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Foundation models for natural language processing have many coherent\ndefinitions of hallucination and methods for its detection and mitigation.\nHowever, analogous definitions and methods do not exist for multi-variate\ntime-series (MVTS) foundation models. We propose new definitions for MVTS\nhallucination, along with new detection and mitigation methods using a\ndiffusion model to estimate hallucination levels. We derive relational datasets\nfrom popular time-series datasets to benchmark these relational hallucination\nlevels. Using these definitions and models, we find that open-source\npre-trained MVTS imputation foundation models relationally hallucinate on\naverage up to 59.5% as much as a weak baseline. The proposed mitigation method\nreduces this by up to 47.7% for these models. The definition and methods may\nimprove adoption and safe usage of MVTS foundation models."}
{"id": "2508.00890", "pdf": "https://arxiv.org/pdf/2508.00890.pdf", "abs": "https://arxiv.org/abs/2508.00890", "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "comment": "Under review", "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability."}
{"id": "2508.00901", "pdf": "https://arxiv.org/pdf/2508.00901.pdf", "abs": "https://arxiv.org/abs/2508.00901", "title": "Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge", "authors": ["Ruichen Xu", "Kexin Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern large language models excel in knowledge-intensive tasks, yet how\ntransformers acquire (store) knowledge during pre-training and extract\n(retrieve) it during post-fine-tuning inference remains theoretically opaque.\nWhile prior theoretical work has begun to investigate these questions through\nthe analysis of training dynamics, such studies are limited to single-layer,\nattention-only architectures. However, most existing studies suggest that MLPs\nare the most contributing components for storing knowledge in transformer-based\nlanguage models. Meanwhile, our empirical investigations reveal that such\nsimplified models, when trained using standard next-token prediction\nobjectives, may be incapable of acquiring or extracting factual knowledge. To\novercome this limitation, we introduce a tractable one-layer transformer\nframework that crucially incorporates both self-attention and MLP modules. By\ntracking its gradient dynamics, we establish convergence and generalization\nguarantees that illuminate the ability of knowledge acquisition and extraction.\nWe prove that 1) Transformers can achieve near-optimal training loss during\npre-training, signifying effective knowledge acquisition; 2) With a large\nfine-tuning dataset and specific data multiplicity conditions met, transformers\ncan achieve low generalization error when tested on factual knowledge learned\nduring pre-training but not reinforced during the fine-tuning, indicating\nsuccessful knowledge extraction; 3) When the conditions are not satisfied,\ntransformers exhibit high generalization loss, resulting in hallucinations. Our\nanalysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,\nour analysis offers theoretical insights into several pertinent empirical\nphenomena, such as the role of learning rate schedules. Experiments on\nsynthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate\nour results."}
{"id": "2508.00902", "pdf": "https://arxiv.org/pdf/2508.00902.pdf", "abs": "https://arxiv.org/abs/2508.00902", "title": "An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models", "authors": ["Kenneth Payne"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "26 pages, 2 figures, 9 tables, 2 appendices", "summary": "Judgment of risk is key to decision-making under uncertainty. As Daniel\nKahneman and Amos Tversky famously discovered, humans do so in a distinctive\nway that departs from mathematical rationalism. Specifically, they demonstrated\nexperimentally that humans accept more risk when they feel themselves at risk\nof losing something than when they might gain. I report the first tests of\nKahneman and Tversky's landmark 'prospect theory' with Large Language Models,\nincluding today's state of the art chain-of-thought 'reasoners'.\n  In common with humans, I find that prospect theory often anticipates how\nthese models approach risky decisions across a range of scenarios. I also\ndemonstrate that context is key to explaining much of the variance in risk\nappetite. The 'frame' through which risk is apprehended appears to be embedded\nwithin the language of the scenarios tackled by the models. Specifically, I\nfind that military scenarios generate far larger 'framing effects' than do\ncivilian settings, ceteris paribus. My research suggests, therefore, that\nlanguage models the world, capturing our human heuristics and biases. But also\nthat these biases are uneven - the idea of a 'frame' is richer than simple\ngains and losses. Wittgenstein's notion of 'language games' explains the\ncontingent, localised biases activated by these scenarios. Finally, I use my\nfindings to reframe the ongoing debate about reasoning and memorisation in\nLLMs."}
{"id": "2508.00910", "pdf": "https://arxiv.org/pdf/2508.00910.pdf", "abs": "https://arxiv.org/abs/2508.00910", "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Public Link: https://github.com/amazon-science/cyber-zero", "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents."}
{"id": "2508.00957", "pdf": "https://arxiv.org/pdf/2508.00957.pdf", "abs": "https://arxiv.org/abs/2508.00957", "title": "Small sample-based adaptive text classification through iterative and contrastive description refinement", "authors": ["Amrit Rajeev", "Udayaadithya Avadhanam", "Harshula Tulapurkar", "SaiBarath Sundar"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Zero-shot text classification remains a difficult task in domains with\nevolving knowledge and ambiguous category boundaries, such as ticketing\nsystems. Large language models (LLMs) often struggle to generalize in these\nscenarios due to limited topic separability, while few-shot methods are\nconstrained by insufficient data diversity. We propose a classification\nframework that combines iterative topic refinement, contrastive prompting, and\nactive learning. Starting with a small set of labeled samples, the model\ngenerates initial topic labels. Misclassified or ambiguous samples are then\nused in an iterative contrastive prompting process to refine category\ndistinctions by explicitly teaching the model to differentiate between closely\nrelated classes. The framework features a human-in-the-loop component, allowing\nusers to introduce or revise category definitions in natural language. This\nenables seamless integration of new, unseen categories without retraining,\nmaking the system well-suited for real-world, dynamic environments. The\nevaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy\non AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with\nminimal accuracy shift after introducing unseen classes (82% and 87%,\nrespectively). The results highlight the effectiveness of prompt-based semantic\nreasoning for fine-grained classification with limited supervision."}
{"id": "2508.01031", "pdf": "https://arxiv.org/pdf/2508.01031.pdf", "abs": "https://arxiv.org/abs/2508.01031", "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent", "authors": ["Jingzhe Ni", "Xiaolong Yin", "Xintong Li", "Xingyu Lu", "Ji Wei", "Ruofeng Tong", "Min Tang", "Peng Du"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation."}
{"id": "2508.01136", "pdf": "https://arxiv.org/pdf/2508.01136.pdf", "abs": "https://arxiv.org/abs/2508.01136", "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs", "authors": ["Wei Zhou", "Peng Sun", "Xuanhe Zhou", "Qianglei Zang", "Ji Xu", "Tieying Zhang", "Guoliang Li", "Fan Wu"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/", "summary": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively."}
{"id": "2508.01191", "pdf": "https://arxiv.org/pdf/2508.01191.pdf", "abs": "https://arxiv.org/abs/2508.01191", "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "authors": ["Chengshuai Zhao", "Zhen Tan", "Pingchuan Ma", "Dawei Li", "Bohan Jiang", "Yancheng Wang", "Yingzhen Yang", "Huan Liu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning."}
{"id": "2508.01249", "pdf": "https://arxiv.org/pdf/2508.01249.pdf", "abs": "https://arxiv.org/abs/2508.01249", "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection", "authors": ["Peiran Wang", "Yang Liu", "Yunfei Lu", "Yifeng Cai", "Hongbo Chen", "Qingyou Yang", "Jie Zhang", "Jue Hong", "Ye Wu"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's working traces as graph-based intermediate\nrepresentations with control flow and data flow described within; (2) a\nproperty registry that attaches security-relevant metadata of interacted tools\n& data, and (3) a type system that performs static inference and checking over\nthe intermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis over sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can achieve 95.75% of TPR, with\nonly 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect\nprompt injection vulnerabilities and enforce fine-grained security constraints."}
{"id": "2508.01274", "pdf": "https://arxiv.org/pdf/2508.01274.pdf", "abs": "https://arxiv.org/abs/2508.01274", "title": "Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan", "authors": ["Jui-Ming Yao", "Bing-Cheng Xie", "Sheng-Wei Peng", "Hao-Yuan Chen", "He-Rong Zheng", "Bing-Jia Tan", "Peter Shaojui Wang", "Shun-Feng Su"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) process visual, acoustic, and\ntextual inputs, addressing the limitations of single-modality LLMs. However,\nexisting benchmarks often overlook tri-modal evaluation in Traditional Chinese\nand do not consider inference latency. To address this, we introduce Multi-TW,\nthe first Traditional Chinese benchmark for evaluating the performance and\nlatency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice\nquestions (image and text, audio and text pairs) sourced from official\nproficiency tests developed with the Steering Committee for the Test of\nProficiency-Huayu (SC-TOP). We evaluated various any-to-any models and\nvision-language models (VLMs) with audio transcription. Our results show that\nclosed-source models generally outperform open-source ones across modalities,\nalthough open-source models can perform well in audio tasks. End-to-end\nany-to-any pipelines offer clear latency advantages compared to VLMs using\nseparate audio transcription. Multi-TW presents a comprehensive view of model\ncapabilities and highlights the need for Traditional Chinese fine-tuning and\nefficient multimodal architectures."}
{"id": "2508.01365", "pdf": "https://arxiv.org/pdf/2508.01365.pdf", "abs": "https://arxiv.org/abs/2508.01365", "title": "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models", "authors": ["Zihan Wang", "Rui Zhang", "Hongwei Li", "Wenshu Fan", "Wenbo Jiang", "Qingchuan Zhao", "Guowen Xu"], "categories": ["cs.CR", "cs.CL"], "comment": "Under review", "summary": "Backdoor attacks pose a significant threat to Large Language Models (LLMs),\nwhere adversaries can embed hidden triggers to manipulate LLM's outputs. Most\nexisting defense methods, primarily designed for classification tasks, are\nineffective against the autoregressive nature and vast output space of LLMs,\nthereby suffering from poor performance and high latency. To address these\nlimitations, we investigate the behavioral discrepancies between benign and\nbackdoored LLMs in output space. We identify a critical phenomenon which we\nterm sequence lock: a backdoored model generates the target sequence with\nabnormally high and consistent confidence compared to benign generation.\nBuilding on this insight, we propose ConfGuard, a lightweight and effective\ndetection method that monitors a sliding window of token confidences to\nidentify sequence lock. Extensive experiments demonstrate ConfGuard achieves a\nnear 100\\% true positive rate (TPR) and a negligible false positive rate (FPR)\nin the vast majority of cases. Crucially, the ConfGuard enables real-time\ndetection almost without additional latency, making it a practical backdoor\ndefense for real-world LLM deployments."}
{"id": "2508.01643", "pdf": "https://arxiv.org/pdf/2508.01643.pdf", "abs": "https://arxiv.org/abs/2508.01643", "title": "ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings", "authors": ["Ali Shiraee Kasmaee", "Mohammad Khodadad", "Mehdi Astaraki", "Mohammad Arshi Saloot", "Nicholas Sherck", "Hamidreza Mahyar", "Soheila Samiee"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on\naccurate and relevant retrieval of chemical literature. However,\ngeneral-purpose text embedding models frequently fail to adequately represent\ncomplex chemical terminologies, resulting in suboptimal retrieval quality.\nSpecialized embedding models tailored to chemical literature retrieval have not\nyet been developed, leaving a substantial performance gap. To address this\nchallenge, we introduce ChEmbed, a domain-adapted family of text embedding\nmodels fine-tuned on a dataset comprising chemistry-specific text from the\nPubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training\ndata, we employ large language models to synthetically generate queries,\nresulting in approximately 1.7 million high-quality query-passage pairs.\nAdditionally, we augment the tokenizer by adding 900 chemically specialized\ntokens to previously unused slots, which significantly reduces the\nfragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains\na 8192-token context length, enabling the efficient retrieval of longer\npassages compared to many other open-source embedding models, which typically\nhave a context length of 512 or 2048 tokens. Evaluated on our newly introduced\nChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general\nembedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents\na practical, lightweight, and reproducible embedding solution that effectively\nimproves retrieval for chemical literature search."}
{"id": "2508.01647", "pdf": "https://arxiv.org/pdf/2508.01647.pdf", "abs": "https://arxiv.org/abs/2508.01647", "title": "DUP: Detection-guided Unlearning for Backdoor Purification in Language Models", "authors": ["Man Hu", "Yahui Ding", "Yatao Yang", "Liangyu Chen", "Yanhao Jia", "Shuai Zhao"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As backdoor attacks become more stealthy and robust, they reveal critical\nweaknesses in current defense strategies: detection methods often rely on\ncoarse-grained feature statistics, and purification methods typically require\nfull retraining or additional clean models. To address these challenges, we\npropose DUP (Detection-guided Unlearning for Purification), a unified framework\nthat integrates backdoor detection with unlearning-based purification. The\ndetector captures feature-level anomalies by jointly leveraging class-agnostic\ndistances and inter-layer transitions. These deviations are integrated through\na weighted scheme to identify poisoned inputs, enabling more fine-grained\nanalysis. Based on the detection results, we purify the model through a\nparameter-efficient unlearning mechanism that avoids full retraining and does\nnot require any external clean model. Specifically, we innovatively repurpose\nknowledge distillation to guide the student model toward increasing its output\ndivergence from the teacher on detected poisoned samples, effectively forcing\nit to unlearn the backdoor behavior. Extensive experiments across diverse\nattack methods and language model architectures demonstrate that DUP achieves\nsuperior defense performance in detection accuracy and purification efficacy.\nOur code is available at https://github.com/ManHu2025/DUP."}
{"id": "2508.01691", "pdf": "https://arxiv.org/pdf/2508.01691.pdf", "abs": "https://arxiv.org/abs/2508.01691", "title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe", "authors": ["Tiantian Feng", "Kevin Huang", "Anfeng Xu", "Xuan Shi", "Thanathai Lertpetchpun", "Jihwan Lee", "Yoonjeong Lee", "Dani Byrd", "Shrikanth Narayanan"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "We present Voxlect, a novel benchmark for modeling dialects and regional\nlanguages worldwide using speech foundation models. Specifically, we report\ncomprehensive benchmark evaluations on dialects and regional language varieties\nin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,\nSpanish, French, German, Brazilian Portuguese, and Italian. Our study used over\n2 million training utterances from 30 publicly available speech corpora that\nare provided with dialectal information. We evaluate the performance of several\nwidely used speech foundation models in classifying speech dialects. We assess\nthe robustness of the dialectal models under noisy conditions and present an\nerror analysis that highlights modeling results aligned with geographic\ncontinuity. In addition to benchmarking dialect classification, we demonstrate\nseveral downstream applications enabled by Voxlect. Specifically, we show that\nVoxlect can be applied to augment existing speech recognition datasets with\ndialect information, enabling a more detailed analysis of ASR performance\nacross dialectal variations. Voxlect is also used as a tool to evaluate the\nperformance of speech generation systems. Voxlect is publicly available with\nthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect."}
{"id": "2508.01773", "pdf": "https://arxiv.org/pdf/2508.01773.pdf", "abs": "https://arxiv.org/abs/2508.01773", "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM."}
{"id": "2508.01780", "pdf": "https://arxiv.org/pdf/2508.01780.pdf", "abs": "https://arxiv.org/abs/2508.01780", "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?", "authors": ["Guozhao Mo", "Wenliang Zhong", "Jiawei Chen", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun"], "categories": ["cs.AI", "cs.CL"], "comment": "Our code and data will be publicly available at\n  https://icip-cas.github.io/LiveMCPBench", "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench."}
{"id": "2508.01791", "pdf": "https://arxiv.org/pdf/2508.01791.pdf", "abs": "https://arxiv.org/abs/2508.01791", "title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase", "authors": ["Fatimah Mohamed Emad Elden"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR."}
{"id": "2508.01887", "pdf": "https://arxiv.org/pdf/2508.01887.pdf", "abs": "https://arxiv.org/abs/2508.01887", "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection", "authors": ["Aldan Creo"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "comment": "Code: https://github.com/ACMCMC/PDFuzz", "summary": "AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz."}
{"id": "2508.01908", "pdf": "https://arxiv.org/pdf/2508.01908.pdf", "abs": "https://arxiv.org/abs/2508.01908", "title": "Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models", "authors": ["Istabrak Abbes", "Gopeshh Subbaraj", "Matthew Riemer", "Nizar Islah", "Benjamin Therien", "Tsuguchika Tabaru", "Hiroaki Kingetsu", "Sarath Chandar", "Irina Rish"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) typically involves pre-training on\nmassive corpora, only to restart the process entirely when new data becomes\navailable. A more efficient and resource-conserving approach would be continual\npre-training, where models are updated with new data rather than retraining\nfrom scratch. However, the introduction of new data often causes distribution\nshifts, leading to performance degradation on previously learned tasks. In this\npaper, we take a deeper look at two popular proposals for addressing this\ndistribution shift within the continual learning literature: experience replay\nand gradient alignment. We consider continual pre-training of models within the\nLlama family of architectures at a large scale across languages with 100\nbillion tokens of training data in each language, finding that both replay and\ngradient alignment lead to more stable learning without forgetting. This\nconclusion holds both as we vary the model scale and as we vary the number and\ndiversity of tasks. Moreover, we are the first to demonstrate the effectiveness\nof gradient alignment techniques in the context of LLM pre-training and propose\nan efficient implementation of meta-experience replay (MER) that imbues\nexperience replay with the benefits of gradient alignment despite negligible\ncompute and memory overhead. Our scaling analysis across model sizes and replay\nrates indicates that small rates of replaying old examples are definitely a\nmore valuable use of compute than investing in model size, but that it is more\ncompute efficient to scale the size of the model than invest in high rates of\nreplaying old examples."}
{"id": "2508.01913", "pdf": "https://arxiv.org/pdf/2508.01913.pdf", "abs": "https://arxiv.org/abs/2508.01913", "title": "A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology", "authors": ["Kamal Al-Sabahi", "Yousuf Khamis Al Mabsali"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Academic publishing, integral to knowledge dissemination and scientific\nadvancement, increasingly faces threats from unethical practices such as\nunconsented authorship, gift authorship, author ambiguity, and undisclosed\nconflicts of interest. While existing infrastructures like ORCID effectively\ndisambiguate researcher identities, they fall short in enforcing explicit\nauthorship consent, accurately verifying contributor roles, and robustly\ndetecting conflicts of interest during peer review. To address these\nshortcomings, this paper introduces a decentralized framework leveraging\nSelf-Sovereign Identity (SSI) and blockchain technology. The proposed model\nuses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to\nsecurely verify author identities and contributions, reducing ambiguity and\nensuring accurate attribution. A blockchain-based trust registry records\nauthorship consent and peer-review activity immutably. Privacy-preserving\ncryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support\nconflict-of-interest detection without revealing sensitive data. Verified\nauthorship metadata and consent records are embedded in publications,\nincreasing transparency. A stakeholder survey of researchers, editors, and\nreviewers suggests the framework improves ethical compliance and confidence in\nscholarly communication. This work represents a step toward a more transparent,\naccountable, and trustworthy academic publishing ecosystem."}
{"id": "2508.01916", "pdf": "https://arxiv.org/pdf/2508.01916.pdf", "abs": "https://arxiv.org/abs/2508.01916", "title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning", "authors": ["Xinting Huang", "Michael Hahn"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Understanding internal representations of neural models is a core interest of\nmechanistic interpretability. Due to its large dimensionality, the\nrepresentation space can encode various aspects about inputs. To what extent\nare different aspects organized and encoded in separate subspaces? Is it\npossible to find these ``natural'' subspaces in a purely unsupervised way?\nSomewhat surprisingly, we can indeed achieve this and find interpretable\nsubspaces by a seemingly unrelated training objective. Our method, neighbor\ndistance minimization (NDM), learns non-basis-aligned subspaces in an\nunsupervised manner. Qualitative analysis shows subspaces are interpretable in\nmany cases, and encoded information in obtained subspaces tends to share the\nsame abstract concept across different inputs, making such subspaces similar to\n``variables'' used by the model. We also conduct quantitative experiments using\nknown circuits in GPT-2; results show a strong connection between subspaces and\ncircuit variables. We also provide evidence showing scalability to 2B models by\nfinding separate subspaces mediating context and parametric knowledge routing.\nViewed more broadly, our findings offer a new perspective on understanding\nmodel internals and building circuits."}
{"id": "2508.02066", "pdf": "https://arxiv.org/pdf/2508.02066.pdf", "abs": "https://arxiv.org/abs/2508.02066", "title": "MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs", "authors": ["Guojiang Zhao", "Sihang Li", "Zixiang Lu", "Zheng Cheng", "Haitao Lin", "Lirong Wu", "Hanchen Xia", "Hengxing Cai", "Wentao Guo", "Hongshuai Wang", "Mingjun Xu", "Siyu Zhu", "Guolin Ke", "Linfeng Zhang", "Zhifeng Gao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious domains, yet their capabilities in molecular reasoning remain\ninsufficiently explored. Current approaches tend to rely heavily on\ngeneral-purpose prompting, which lacks domain-specific molecular semantics,\nwhile those that use fine-tuning strategies often face challenges with\ninterpretability and reasoning depth. To address these issues, we introduce\nMolReasoner, a two-stage framework designed to transition LLMs from\nmemorization towards chemical reasoning. First, we propose Mol-SFT, which\ninitializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)\nsamples generated by GPT-4o and verified for chemical accuracy. Subsequently,\nMol-RL applies reinforcement learning with specialized reward functions\ndesigned explicitly to align chemical structures with linguistic descriptions,\nthereby enhancing molecular reasoning capabilities. Our approach notably\nenhances interpretability, improving the model 's molecular understanding and\nenabling better generalization. Extensive experiments demonstrate that\nMolReasoner outperforms existing methods, and marking a significant shift from\nmemorization-based outputs to robust chemical reasoning."}
{"id": "2508.02075", "pdf": "https://arxiv.org/pdf/2508.02075.pdf", "abs": "https://arxiv.org/abs/2508.02075", "title": "Human Capital Visualization using Speech Amount during Meetings", "authors": ["Ekai Hashimoto", "Takeshi Mizumoto", "Kohei Nagira", "Shun Shiramatsu"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work", "summary": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises."}
{"id": "2508.02091", "pdf": "https://arxiv.org/pdf/2508.02091.pdf", "abs": "https://arxiv.org/abs/2508.02091", "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Chris Shum", "Jiwei Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Preprint Version", "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN"}
{"id": "2508.02124", "pdf": "https://arxiv.org/pdf/2508.02124.pdf", "abs": "https://arxiv.org/abs/2508.02124", "title": "Trainable Dynamic Mask Sparse Attention", "authors": ["Jingze Shi", "Yifan Wu", "Bingheng Wu", "Yiran Peng", "Liangdong Wang", "Guang Liu", "Yuyu Luo"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "8 figures, 4 tables", "summary": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively."}
{"id": "2508.02165", "pdf": "https://arxiv.org/pdf/2508.02165.pdf", "abs": "https://arxiv.org/abs/2508.02165", "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318."}
{"id": "2508.02175", "pdf": "https://arxiv.org/pdf/2508.02175.pdf", "abs": "https://arxiv.org/abs/2508.02175", "title": "Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers", "authors": ["Liang Lin", "Miao Yu", "Kaiwen Luo", "Yibo Zhang", "Lilan Peng", "Dexian Wang", "Xuehai Tang", "Yuanhe Zhang", "Xikang Yang", "Zhenhong Zhou", "Kun Wang", "Yang Liu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth."}
{"id": "2508.02215", "pdf": "https://arxiv.org/pdf/2508.02215.pdf", "abs": "https://arxiv.org/abs/2508.02215", "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding", "authors": ["Yike Zhang", "Zhiyuan He", "Huiqiang Jiang", "Chengruidong Zhang", "Yuqing Yang", "Jianyong Wang", "Lili Qiu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."}
{"id": "2508.02276", "pdf": "https://arxiv.org/pdf/2508.02276.pdf", "abs": "https://arxiv.org/abs/2508.02276", "title": "CellForge: Agentic Design of Virtual Cell Models", "authors": ["Xiangru Tang", "Zhuoyun Yu", "Jiapeng Chen", "Yan Cui", "Daniel Shao", "Weixu Wang", "Fang Wu", "Yuchen Zhuang", "Wenqi Shi", "Zhi Huang", "Arman Cohan", "Xihong Lin", "Fabian Theis", "Smita Krishnaswamy", "Mark Gerstein"], "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.QM"], "comment": null, "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge."}
{"id": "2508.02279", "pdf": "https://arxiv.org/pdf/2508.02279.pdf", "abs": "https://arxiv.org/abs/2508.02279", "title": "Dialogue Systems Engineering: A Survey and Future Directions", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering."}
{"id": "2508.02298", "pdf": "https://arxiv.org/pdf/2508.02298.pdf", "abs": "https://arxiv.org/abs/2508.02298", "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment", "authors": ["Guofu Xie", "Yunsheng Shi", "Hongtao Tian", "Ting Yao", "Xiao Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks."}
{"id": "2508.02328", "pdf": "https://arxiv.org/pdf/2508.02328.pdf", "abs": "https://arxiv.org/abs/2508.02328", "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC", "cs.CL", "cs.IR", "H.5.2; I.2.7; H.1.2"], "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables", "summary": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs."}
{"id": "2508.02366", "pdf": "https://arxiv.org/pdf/2508.02366.pdf", "abs": "https://arxiv.org/abs/2508.02366", "title": "Language Model Guided Reinforcement Learning in Quantitative Trading", "authors": ["Adam Darmanin", "Vince Vella"], "categories": ["cs.LG", "cs.CL", "q-fin.TR", "I.2.7; I.2.6; J.4"], "comment": "12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference", "summary": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL."}
{"id": "2508.02371", "pdf": "https://arxiv.org/pdf/2508.02371.pdf", "abs": "https://arxiv.org/abs/2508.02371", "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design", "authors": ["Matou≈° Jel√≠nek", "Nadine Schlicker", "Ewart de Visser"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness."}
{"id": "2508.02419", "pdf": "https://arxiv.org/pdf/2508.02419.pdf", "abs": "https://arxiv.org/abs/2508.02419", "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens", "authors": ["Haohan Zheng", "Zhenguo Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy."}
{"id": "2508.02470", "pdf": "https://arxiv.org/pdf/2508.02470.pdf", "abs": "https://arxiv.org/abs/2508.02470", "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration", "authors": ["Hyunjn An", "Yongwon Kim", "Wonduk Seo", "Joonil Park", "Daye Kang", "Changhoon Oh", "Dokyun Kim", "Seunghyun Lee"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design."}
{"id": "2508.02503", "pdf": "https://arxiv.org/pdf/2508.02503.pdf", "abs": "https://arxiv.org/abs/2508.02503", "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling", "authors": ["Maxime Bouscary", "Saurabh Amin"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems."}
{"id": "2508.02511", "pdf": "https://arxiv.org/pdf/2508.02511.pdf", "abs": "https://arxiv.org/abs/2508.02511", "title": "Test-time Prompt Intervention", "authors": ["Chenxu Yang", "Qingyi Si", "Mz Dai", "Dingyu Yao", "Mingyu Zheng", "Minghui Chen", "Zheng Lin", "Weiping Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "23 pages, 16 figures, under review", "summary": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning."}
{"id": "2508.02546", "pdf": "https://arxiv.org/pdf/2508.02546.pdf", "abs": "https://arxiv.org/abs/2508.02546", "title": "What are you sinking? A geometric approach on attention sink", "authors": ["Valeria Ruscio", "Umberto Nanni", "Fabrizio Silvestri"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS."}
{"id": "2508.02587", "pdf": "https://arxiv.org/pdf/2508.02587.pdf", "abs": "https://arxiv.org/abs/2508.02587", "title": "Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules", "authors": ["Yilun Liu", "Yunpu Ma", "Yuetian Lu", "Shuo Chen", "Zifeng Ding", "Volker Tresp"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This paper is a preprint under review. arXiv admin note: text overlap\n  with arXiv:2411.08212", "summary": "Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications."}
{"id": "2508.02621", "pdf": "https://arxiv.org/pdf/2508.02621.pdf", "abs": "https://arxiv.org/abs/2508.02621", "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research", "authors": ["Yinghao Zhu", "Yifan Qi", "Zixiang Wang", "Lei Gu", "Dehao Sui", "Haoran Hu", "Xichen Zhang", "Ziyi He", "Liantao Ma", "Lequan Yu"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "Code: https://github.com/yhzhu99/HealthFlow", "summary": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery."}
{"id": "2508.02622", "pdf": "https://arxiv.org/pdf/2508.02622.pdf", "abs": "https://arxiv.org/abs/2508.02622", "title": "Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction", "authors": ["Enrico De Santis", "Antonello Rizzi"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research."}
{"id": "2508.02629", "pdf": "https://arxiv.org/pdf/2508.02629.pdf", "abs": "https://arxiv.org/abs/2508.02629", "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents", "authors": ["Yibin Liu", "Zhixuan Liang", "Zanxin Chen", "Tianxing Chen", "Mengkang Hu", "Wanxi Dong", "Congsheng Xu", "Zhaoming Han", "Yusen Qin", "Yao Mu"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines."}
{"id": "2306.15933", "pdf": "https://arxiv.org/pdf/2306.15933.pdf", "abs": "https://arxiv.org/abs/2306.15933", "title": "You Can Generate It Again: Data-to-Text Generation with Verification and Correction Prompting", "authors": ["Xuan Ren", "Zeyu Zhang", "Lingqiao Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Small language models like T5 excel in generating high-quality text for\ndata-to-text tasks, offering adaptability and cost-efficiency compared to Large\nLanguage Models (LLMs). However, they frequently miss keywords, which is\nconsidered one of the most severe and common errors in this task. In this work,\nwe explore the potential of using feedback systems to enhance semantic fidelity\nin smaller language models for data-to-text generation tasks, through our\nVerification and Correction Prompting (VCP) approach. In the inference stage,\nour approach involves a multi-step process, including generation, verification,\nand regeneration stages. During the verification stage, we implement a simple\nrule to check for the presence of every keyword in the prediction. Recognizing\nthat this rule can be inaccurate, we have developed a carefully designed\ntraining procedure, which enabling the model to incorporate feedback from the\nerror-correcting prompt effectively, despite its potential inaccuracies. The\nVCP approach effectively reduces the Semantic Error Rate (SER) while\nmaintaining the text's quality."}
{"id": "2402.10699", "pdf": "https://arxiv.org/pdf/2402.10699.pdf", "abs": "https://arxiv.org/abs/2402.10699", "title": "Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process", "authors": ["Hongbin Na", "Zimu Wang", "Mieradilijiang Maimaiti", "Tong Chen", "Wei Wang", "Tao Shen", "Ling Chen"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Large language models (LLMs) have demonstrated promising potential in various\ndownstream tasks, including machine translation. However, prior work on\nLLM-based machine translation has mainly focused on better utilizing training\ndata, demonstrations, or pre-defined and universal knowledge to improve\nperformance, with a lack of consideration of decision-making like human\ntranslators. In this paper, we incorporate Thinker with the Drift-Diffusion\nModel (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion\nprocess to emulate human translators' dynamic decision-making under constrained\nresources. We conduct extensive experiments under the high-resource,\nlow-resource, and commonsense translation settings using the WMT22 and CommonMT\ndatasets, in which Thinker-DDM outperforms baselines in the first two\nscenarios. We also perform additional analysis and evaluation on commonsense\ntranslation to illustrate the high effectiveness and efficacy of the proposed\nmethod."}
{"id": "2405.17402", "pdf": "https://arxiv.org/pdf/2405.17402.pdf", "abs": "https://arxiv.org/abs/2405.17402", "title": "THREAD: Thinking Deeper with Recursive Spawning", "authors": ["Philip Schroeder", "Nathaniel Morgan", "Hongyin Luo", "James Glass"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities across\ndiverse settings, but still struggle as the length and complexity of the\ncontext increases. To address this challenge, we propose Thinking Recursively\nand Dynamically (ThReaD). THREAD frames model generation as a thread of\nexecution that, based on the context, can run to completion or dynamically\nspawn new threads. By spawning, threads can offload work (e.g., thinking,\nretrieving information) to child threads, which only return tokens needed for\nthe parent thread to do its work. In effect, this enables the model to adapt,\nas needed, the amount of intermediate work used to produce tokens. We apply\nTHREAD in the settings of LLM task solving and question answering, where the\ndynamic threading allows the model to recursively decompose the given task or\nquestion into progressively simpler sub-problems that can be solved by separate\nchild threads. We test THREAD, implemented using a few-shot learning approach,\non diverse benchmarks for agent tasks and data-grounded question answering.\nTHREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these\nbenchmarks, including ALFWorld, TextCraft, and WebShop, along with two new\nbenchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD\noutperforms existing frameworks by 10% to 50% absolute points with smaller\nmodels, including Llama-3-8b and CodeLlama-7b."}
{"id": "2406.11130", "pdf": "https://arxiv.org/pdf/2406.11130.pdf", "abs": "https://arxiv.org/abs/2406.11130", "title": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis", "authors": ["Yonghyun Jun", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific\naspects within texts, resulting in detailed sentiment tuples. Previous ABSA\nmodels often use static templates to predict all of the elements in the tuples,\nand these models often fail to accurately capture dependencies between\nelements. Multi-view prompting method improves the performance of ABSA by\npredicting tuples with various templates and then ensembling the results.\nHowever, this method suffers from inefficiencies and out-of-distribution\nerrors. In this paper, we propose a Dynamic Order Template (DOT) method for\nABSA, which dynamically generates necessary views for each instance based on\ninstance-level entropy. Ensuring the diverse and relevant view generation, our\nproposed method improves F1-scores on ASQP and ACOS datasets while\nsignificantly reducing inference time."}
{"id": "2406.12307", "pdf": "https://arxiv.org/pdf/2406.12307.pdf", "abs": "https://arxiv.org/abs/2406.12307", "title": "Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?", "authors": ["Seungbin Yang", "ChaeHun Park", "Taehee Kim", "Jaegul Choo"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To quantitatively evaluate this capability, we construct a new\nbenchmark dataset where instances are systematically altered to simulate the\nambiguous and incomplete conditions common in real-world interactions. Our\nexperiments reveal that even state-of-the-art LLMs often struggle to identify\nthese conditions, attempting to use tools without sufficient information or\nwhen the correct tool is unavailable. To better understand these limitations,\nwe conduct a detailed behavioral analysis across various conditions, including\nimplicit evaluation and scenarios where models receive feedback from previous\ntool invocations. Based on this analysis, we propose a novel prompting-based\nreasoning strategy that explicitly instructs models to assess the sufficiency\nof information and the availability of tools. Our proposed approach\nsignificantly enhances the models' ability to recognize incomplete conditions,\nresulting in more informed and contextually appropriate tool-use decisions. We\nbelieve our research contributes to advancing the reliability of LLMs,\nespecially in real-world applications where incomplete or ambiguous information\nis common. Our dataset is available at\nhttps://huggingface.co/datasets/ddehun/ICT."}
{"id": "2406.16306", "pdf": "https://arxiv.org/pdf/2406.16306.pdf", "abs": "https://arxiv.org/abs/2406.16306", "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment", "authors": ["Bolian Li", "Yifan Wang", "Anamika Lochab", "Ananth Grama", "Ruqi Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks."}
{"id": "2408.05456", "pdf": "https://arxiv.org/pdf/2408.05456.pdf", "abs": "https://arxiv.org/abs/2408.05456", "title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation", "authors": ["Wenbo Shang", "Xuliang Zhu", "Xin Huang"], "categories": ["cs.CL"], "comment": "15 pages, 12 figures", "summary": "Unified graph representation learning aims to generate node embeddings, which\ncan be applied to multiple downstream applications of graph analytics. However,\nexisting studies based on graph neural networks and language models either\nsuffer from the limitations of numerous training needs toward specific\ndownstream predictions, poor generalization, or shallow semantic features. In\nthis work, we propose a novel Path-LLM model to efficiently learn unified graph\nrepresentation, which leverages a powerful large language model (LLM) to\nincorporate our proposed path features. Our Path-LLM framework consists of four\nwell-designed techniques. First, we develop a new mechanism of long-to-short\nshortest path (L2SP) selection, which can cover key connections between\ndifferent dense groups. An in-depth analysis and comparison of different path\nselections is conducted to justify the rationale behind our designed L2SP\nmethod. Next, we design path textualization to obtain L2SP-based training texts\nwith key phrase selection from node text attributes. We then feed the texts\ninto a self-supervised LLM training process to align next node/edge generation\nin L2SP with next token generation in causal language modeling for graph\nrepresentation learning and finally extract the unified graph embeddings. We\ntheoretically analyze the algorithm complexity of our Path-LLM approach.\nExtensive experiments on large-scale graph benchmarks validate the superiority\nof Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and\nGraphTranslator on two classical graph learning tasks (node classification and\nedge validation) and one NP-hard graph query processing task (keyword search).\nCompared with WalkLM, our approach saves more than 90% of training paths on\nmillions-scale graphs and runs at most 35x faster."}
{"id": "2408.16493", "pdf": "https://arxiv.org/pdf/2408.16493.pdf", "abs": "https://arxiv.org/abs/2408.16493", "title": "Learning from Negative Samples in Biomedical Generative Entity Linking", "authors": ["Chanhwi Kim", "Hyunjae Kim", "Sihyeon Park", "Jiwoo Lee", "Mujeen Sung", "Jaewoo Kang"], "categories": ["cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "Generative models have become widely used in biomedical entity linking\n(BioEL) due to their excellent performance and efficient memory usage. However,\nthese models are usually trained only with positive samples, i.e., entities\nthat match the input mention's identifier, and do not explicitly learn from\nhard negative samples, which are entities that look similar but have different\nmeanings. To address this limitation, we introduce ANGEL (Learning from\nNegative Samples in Biomedical Generative Entity Linking), the first framework\nthat trains generative BioEL models using negative samples. Specifically, a\ngenerative model is initially trained to generate positive entity names from\nthe knowledge base for given input entities. Subsequently, both correct and\nincorrect outputs are gathered from the model's top-k predictions. Finally, the\nmodel is updated to prioritize the correct predictions through preference\noptimization. Our models outperform the previous best baseline models by up to\nan average top-1 accuracy of 1.4% on five benchmarks. When incorporating our\nframework into pre-training, the performance improvement increases further to\n1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning\nstages. The code and model weights are available at\nhttps://github.com/dmis-lab/ANGEL."}
{"id": "2410.12601", "pdf": "https://arxiv.org/pdf/2410.12601.pdf", "abs": "https://arxiv.org/abs/2410.12601", "title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization", "authors": ["Yixi Ding", "Jiaying Wu", "Tongyao Zhu", "Yanxia Qin", "Qian Liu", "Min-Yen Kan"], "categories": ["cs.CL"], "comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances", "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning."}
{"id": "2411.01281", "pdf": "https://arxiv.org/pdf/2411.01281.pdf", "abs": "https://arxiv.org/abs/2411.01281", "title": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons", "authors": ["Seonil Son", "Ju-Min Oh", "Heegon Jin", "Cheolhun Jang", "Jeongbeom Jeong", "Kuntae Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages for main body, 19 pages in total", "summary": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}"}
{"id": "2411.18104", "pdf": "https://arxiv.org/pdf/2411.18104.pdf", "abs": "https://arxiv.org/abs/2411.18104", "title": "Training and Evaluating Language Models with Template-based Data Generation", "authors": ["Yifan Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in ICLR 2025 DATA-FM Workshop", "summary": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However, a\nfundamental bottleneck persists: these models often struggle with tasks\nrequiring complex, multi-step reasoning, particularly in mathematical\nproblem-solving. This deficiency stems from the critical scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for cultivating\nsophisticated reasoning abilities. To overcome this challenge, we introduce\nTemplate-based Data Generation (TDG), a novel and scalable paradigm that\nharnesses frontier LLMs (GPT-4) to automatically generate parameterized\nmeta-templates, which in turn synthesize a virtually infinite stream of\nhigh-quality problems and solutions. Using this paradigm, we create\nTemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million\nsynthetically generated grade school math problems. Each problem is accompanied\nby a programmatically verifiable solution, offering an unprecedented level of\nquality at scale. This resource not only resolves the data scarcity issue for\nsupervised fine-tuning but also provides a robust mechanism for model alignment\nthrough Reinforcement Learning with Verifiable Rewards (RLVR). Our approach\nelevates data augmentation by employing GPT-4 for meta-template creation,\nguaranteeing diverse and complex problem structures. By providing a scalable\nsolution to the data and verification bottleneck, TDG and TemplateGSM pave the\nway for a new generation of LLMs with powerful, reliable reasoning skills. The\ncode and data are available at https://github.com/iiis-ai/TemplateMath."}
{"id": "2412.02252", "pdf": "https://arxiv.org/pdf/2412.02252.pdf", "abs": "https://arxiv.org/abs/2412.02252", "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity", "authors": ["Da Ma", "Lu Chen", "Situo Zhang", "Yuxun Miao", "Su Zhu", "Zhi Chen", "Hongshen Xu", "Hanqi Li", "Shuai Fan", "Lei Pan", "Kai Yu"], "categories": ["cs.CL"], "comment": "14 pages, 7 figures, 7 tables", "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."}
{"id": "2412.12465", "pdf": "https://arxiv.org/pdf/2412.12465.pdf", "abs": "https://arxiv.org/abs/2412.12465", "title": "Core Context Aware Transformers for Long Context Language Modeling", "authors": ["Yaofo Chen", "Zeng You", "Shuhai Zhang", "Haokun Li", "Yirui Li", "Yaowei Wang", "Mingkui Tan"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at ICML 2025", "summary": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods."}
{"id": "2501.00571", "pdf": "https://arxiv.org/pdf/2501.00571.pdf", "abs": "https://arxiv.org/abs/2501.00571", "title": "KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities", "authors": ["Chengcheng Mai", "Yuxiang Wang", "Ziyu Gong", "Hanxiang Wang", "Yihua Huang"], "categories": ["cs.CL"], "comment": "This work has been accepted by IJCAI 2025 (CCF A)", "summary": "Document-level relation extraction (Doc-RE) aims to extract relations between\nentities across multiple sentences. Therefore, Doc-RE requires more\ncomprehensive reasoning abilities like humans, involving complex cross-sentence\ninteractions between entities, contexts, and external general knowledge,\ncompared to the sentence-level RE. However, most existing Doc-RE methods focus\non optimizing single reasoning ability, but lack the ability to utilize\nexternal knowledge for comprehensive reasoning on long documents. To solve\nthese problems, a knowledge retrieval augmented method, named KnowRA, was\nproposed with comprehensive reasoning to autonomously determine whether to\naccept external knowledge to assist DocRE. Firstly, we constructed a document\ngraph for semantic encoding and integrated the co-reference resolution model to\naugment the co-reference reasoning ability. Then, we expanded the document\ngraph into a document knowledge graph by retrieving the external knowledge base\nfor common-sense reasoning and a novel knowledge filtration method was\npresented to filter out irrelevant knowledge. Finally, we proposed the axis\nattention mechanism to build direct and indirect associations with intermediary\nentities for achieving cross-sentence logical reasoning. Extensive experiments\nconducted on two datasets verified the effectiveness of our method compared to\nthe state-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/KnowRA."}
{"id": "2501.05727", "pdf": "https://arxiv.org/pdf/2501.05727.pdf", "abs": "https://arxiv.org/abs/2501.05727", "title": "Self-Evolving Critique Abilities in Large Language Models", "authors": ["Zhengyang Tang", "Ziniu Li", "Zhenyang Xiao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Dayiheng Liu", "Fei Huang", "Tianyu Liu", "Bowen Yu", "Junyang Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by COLM 2025", "summary": "Despite their remarkable performance, Large Language Models (LLMs) face a\ncritical challenge: providing feedback for tasks where human evaluation is\ndifficult or where LLMs potentially outperform humans. In such scenarios,\nleveraging the critique ability of LLMs themselves - identifying and correcting\nflaws - shows considerable promise. This paper explores enhancing critique\nabilities of LLMs, noting that current approaches rely on human annotations or\nmore powerful models, leaving the challenge of improving critique abilities\nwithout external supervision unresolved. We introduce SCRIT (Self-evolving\nCRITic), a framework that trains LLMs with self-generated data to evolve their\ncritique abilities. To address the low quality of naively generated data, we\npropose a contrastive-critic approach that uses reference solutions during data\nsynthesis to enhance the model's understanding of key concepts, and\nincorporates a self-validation scheme to ensure data quality. The final trained\nmodel operates without any reference solutions at inference time. Implemented\nwith Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent\nimprovements across a wide range of benchmarks spanning both mathematical and\nscientific reasoning: achieving a 10.0\\% relative gain in critique-correction\naccuracy and a 19.0\\% relative improvement in error identification F1-score.\nOur analysis reveals that SCRIT's performance scales positively with data and\nmodel size and enables continuous improvement through multi-round iterations."}
{"id": "2501.14693", "pdf": "https://arxiv.org/pdf/2501.14693.pdf", "abs": "https://arxiv.org/abs/2501.14693", "title": "Rethinking Table Instruction Tuning", "authors": ["Naihao Deng", "Rada Mihalcea"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings. Updates: 07/2025: We release the\n  TAMA-QWen2.5 and TAMA-QWen3 models. 06/2025: We release our project page:\n  https://lit.eecs.umich.edu/TAMA/, code: https://github.com/MichiganNLP/TAMA,\n  huggingface models:\n  https://huggingface.co/collections/MichiganNLP/tama-684eeb3e7f262362856eccd1,\n  and data: https://huggingface.co/datasets/MichiganNLP/TAMA_Instruct", "summary": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices, and also lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and find significant declines in both\nout-of-domain table understanding and general capabilities as compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the previous table instruction-tuning work, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection. We open-source the project and our models."}
{"id": "2502.04066", "pdf": "https://arxiv.org/pdf/2502.04066.pdf", "abs": "https://arxiv.org/abs/2502.04066", "title": "Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge Retention in Language Model Pre-Training", "authors": ["Changhao Jiang", "Ming Zhang", "Junjie Ye", "Xiaoran Fan", "Yifei Cao", "Jiajun Sun", "Zhiheng Xi", "Shihan Dou", "Yi Dong", "Yujiong Shen", "Jingqi Tong", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The GPT-4 technical report highlights the possibility of predicting model\nperformance on downstream tasks using only pre-training signals, though\ndetailed methodologies are absent. Such predictive capabilities are essential\nfor resource-efficient pre-training and the construction of task-aligned\ndatasets. In this paper, we aim to predict performance in closed-book question\nanswering (QA), a vital downstream task that directly reflects a model's\ninternalized knowledge without the help of external tools. We address three\nprimary challenges: (1) limited access to and understanding of pre-training\ncorpora, (2) limitations of current evaluation methods for pre-trained models,\nand (3) limitations of frequency-based metrics in predicting model performance.\nIn response, we conduct large-scale retrieval and semantic analysis across the\npre-training corpora of 21 publicly available and 3 custom-trained large\nlanguage models. We then develop a multi-template QA evaluation framework\nincorporating paraphrased question variants. Building on these foundations, we\npropose Size-dependent Mutual Information (SMI), an information-theoretic\nmetric that linearly correlates pre-training data characteristics, model size,\nand QA accuracy, without requiring additional training. Experimental results\nshow that SMI outperforms co-occurrence-based baselines, achieving $R^2 > 0.75$\non models with over one billion parameters. Theoretical analysis further\nsuggests an upper bound of around 80% QA accuracy under optimal pre-training,\nreflecting intrinsic memory limitations and motivating the use of retrieval or\nfew-shot methods in later stages."}
{"id": "2502.06258", "pdf": "https://arxiv.org/pdf/2502.06258.pdf", "abs": "https://arxiv.org/abs/2502.06258", "title": "Emergent Response Planning in LLMs", "authors": ["Zhichen Dong", "Zhanhui Zhou", "Zhixuan Liu", "Chao Yang", "Chaochao Lu"], "categories": ["cs.CL", "cs.LG"], "comment": "Published at ICML 2025. Code available at:\n  https://github.com/niconi19/Emergent-Response-Planning-in-LLMs", "summary": "In this work, we argue that large language models (LLMs), though trained to\npredict only the next token, exhibit emergent planning behaviors:\n$\\textbf{their hidden representations encode future outputs beyond the next\ntoken}$. Through simple probing, we demonstrate that LLM prompt representations\nencode global attributes of their entire responses, including\n$\\textit{structure attributes}$ (e.g., response length, reasoning steps),\n$\\textit{content attributes}$ (e.g., character choices in storywriting,\nmultiple-choice answers at the end of response), and $\\textit{behavior\nattributes}$ (e.g., answer confidence, factual consistency). In addition to\nidentifying response planning, we explore how it scales with model size across\ntasks and how it evolves during generation. The findings that LLMs plan ahead\nfor the future in their hidden representations suggest potential applications\nfor improving transparency and generation control."}
{"id": "2502.13207", "pdf": "https://arxiv.org/pdf/2502.13207.pdf", "abs": "https://arxiv.org/abs/2502.13207", "title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Despite the increasing use of large language models for creative tasks, their\noutputs often lack diversity. Common solutions, such as sampling at higher\ntemperatures, can compromise the quality of the results. Dealing with this\ntrade-off is still an open challenge in designing AI systems for creativity.\nDrawing on information theory, we propose a context-based score to\nquantitatively evaluate value and originality. This score incentivizes accuracy\nand adherence to the request while fostering divergence from the learned\ndistribution. We show that our score can be used as a reward in a reinforcement\nlearning framework to fine-tune large language models for maximum performance.\nWe validate our strategy through experiments considering a variety of creative\ntasks, such as poetry generation and math problem solving, demonstrating that\nit enhances the value and originality of the generated solutions."}
{"id": "2502.13422", "pdf": "https://arxiv.org/pdf/2502.13422.pdf", "abs": "https://arxiv.org/abs/2502.13422", "title": "Towards Question Answering over Large Semi-structured Tables", "authors": ["Yuxiang Wang", "Junhao Gan", "Jianzhong Qi"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks."}
{"id": "2502.14037", "pdf": "https://arxiv.org/pdf/2502.14037.pdf", "abs": "https://arxiv.org/abs/2502.14037", "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite their growing capabilities, language models still frequently\nreproduce content from their training data, generate repetitive text, and favor\ncommon grammatical patterns and vocabulary. A possible cause is the decoding\nstrategy: the most common strategies either consider only the most probable\ntokens, which reduces output diversity, or increase the likelihood of unlikely\ntokens, compromising output accuracy and correctness. In this paper, we propose\nDiffSampling, a new decoding method that leverages a mathematical analysis of\nthe token probability distribution to ensure the generation of contextually\nappropriate text. In particular, the difference between consecutive, sorted\nprobabilities can be used to truncate incorrect tokens. In addition, we also\npropose two variations of the proposed method that aim to correct the subtle\ninconsistencies of common sampling strategies. Experiments involving four\ndifferent text-generation tasks demonstrate that our approach consistently\nperforms at least on par with the existing methods it builds upon in terms of\nquality, while potentially improving output diversity."}
{"id": "2502.15851", "pdf": "https://arxiv.org/pdf/2502.15851.pdf", "abs": "https://arxiv.org/abs/2502.15851", "title": "Control Illusion: The Failure of Instruction Hierarchies in Large Language Models", "authors": ["Yilin Geng", "Haonan Li", "Honglin Mu", "Xudong Han", "Timothy Baldwin", "Omri Abend", "Eduard Hovy", "Lea Frermann"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed with hierarchical\ninstruction schemes, where certain instructions (e.g., system-level directives)\nare expected to take precedence over others (e.g., user messages). Yet, we lack\na systematic understanding of how effectively these hierarchical control\nmechanisms work. We introduce a systematic evaluation framework based on\nconstraint prioritization to assess how well LLMs enforce instruction\nhierarchies. Our experiments across six state-of-the-art LLMs reveal that\nmodels struggle with consistent instruction prioritization, even for simple\nformatting conflicts. We find that the widely-adopted system/user prompt\nseparation fails to establish a reliable instruction hierarchy, and models\nexhibit strong inherent biases toward certain constraint types regardless of\ntheir priority designation. We find that LLMs more reliably obey constraints\nframed through natural social hierarchies (e.g., authority, expertise,\nconsensus) than system/user roles, which suggests that pretraining-derived\nsocial structures act as latent control priors, with potentially stronger\ninfluence than post-training guardrails."}
{"id": "2502.18583", "pdf": "https://arxiv.org/pdf/2502.18583.pdf", "abs": "https://arxiv.org/abs/2502.18583", "title": "What are Foundation Models Cooking in the Post-Soviet World?", "authors": ["Anton Lavrouk", "Tarek Naous", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL"], "comment": null, "summary": "The culture of the Post-Soviet states is complex, shaped by a turbulent\nhistory that continues to influence current events. In this study, we\ninvestigate the Post-Soviet cultural food knowledge of foundation models by\nconstructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in\nthe Russian and Ukrainian languages, centered around the Post-Soviet region. We\ndemonstrate that leading models struggle to correctly identify the origins of\ndishes from Post-Soviet nations in both text-only and multimodal Question\nAnswering (QA), instead over-predicting countries linked to the language the\nquestion is asked in. Through analysis of pretraining data, we show that these\nresults can be explained by misleading dish-origin co-occurrences, along with\nlinguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move\nbeyond QA-based assessments, we test models' abilities to produce accurate\nvisual descriptions of dishes. The weak correlation between this task and QA\nsuggests that QA alone may be insufficient as an evaluation of cultural\nunderstanding. To foster further research, we will make BORSch publicly\navailable at https://github.com/alavrouk/BORSch."}
{"id": "2503.00808", "pdf": "https://arxiv.org/pdf/2503.00808.pdf", "abs": "https://arxiv.org/abs/2503.00808", "title": "Predictive Data Selection: The Data That Predicts Is the Data That Teaches", "authors": ["Kashun Shum", "Yuzhen Huang", "Hongjian Zou", "Qi Ding", "Yixuan Liao", "Xiaoxin Chen", "Qian Liu", "Junxian He"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Language model pretraining involves training on extensive corpora, where data\nquality plays a pivotal role. In this work, we aim to directly estimate the\ncontribution of data during pretraining and select pretraining data in an\nefficient manner. Specifically, we draw inspiration from recent findings\nshowing that compression efficiency (i.e., the normalized loss) of diverse\nmodels on certain text correlates strongly with their downstream performance,\nwhen the text domain aligns with the downstream benchmarks(Huang et al., 2024).\nBuilding on this observation, we hypothesize that data on which model losses\nare predictive of downstream abilities also contribute effectively to learning,\nwhich shares similar intuition with Thrush et al.(2024). To leverage this\ninsight, we introduce predictive data selection (PreSelect), a lightweight and\nefficient data selection method that requires training and deploying only a\nfastText-based scorer. Through comprehensive experiments with 1B and 3B\nparameter models, we demonstrate that models trained on 30B tokens selected\nwith PreSelect surpass the performance of the vanilla baseline trained on 300B\ntokens, achieving a 10x reduction in compute requirements. Furthermore,\nPreSelect significantly outperforms other competitive data selection baselines,\nsuch as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We\nopen-source our trained data selection scorer along with the curated datasets\nat https://github.com/hkust-nlp/PreSelect."}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288.pdf", "abs": "https://arxiv.org/abs/2503.18288", "title": "TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource Language Modeling", "authors": ["Cheng Huang", "Fan Gao", "Yutong Liu", "Nyima Tashi", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Advancement of large language models (LLMs) has brought transformative\ncapabilities to NLP, but such progress remains unevenly distributed, especially\nfor low-resource and culturally rich languages like Tibetan. In this paper, we\npresent TIB-STC, the first large-scale, expert-curated, and multi-domain\ndataset specifically designed to support the development and evaluation of LLMs\nfor the Tibetan language. Spanning over 11 billion tokens across literature,\nreligion, medicine, law, and daily communication, TIB-STC preserves traditional\ngrammar and stylistic richness. To validate its utility, we train a reference\nmodel, Sun-Shine, on TIB-STC through a three-stage pipeline involving\npretraining, supervised fine-tuning, and preference optimization. Evaluation on\nTLUE Benchmark for Tibetan-specific tasks, including Ti-MMLU and\nTi-SafetyBench, demonstrates the TIB-STC's effectiveness in enabling robust\ninstruction-following and culturally aligned generation. We release TIB-STC to\nadvance research in low-resource language modeling and promote inclusivity in\nmultilingual NLP. All data are available:\nhttps://github.com/Vicentvankor/sun-shine."}
{"id": "2504.03165", "pdf": "https://arxiv.org/pdf/2504.03165.pdf", "abs": "https://arxiv.org/abs/2504.03165", "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation", "authors": ["Weitao Li", "Kaiming Liu", "Xiangyu Zhang", "Xuanyu Lei", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG."}
{"id": "2504.07360", "pdf": "https://arxiv.org/pdf/2504.07360.pdf", "abs": "https://arxiv.org/abs/2504.07360", "title": "Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs", "authors": ["Taibiao Zhao", "Xiaobing Chen", "Mingxuan Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper is accepted by DASFAA2025", "summary": "The adaptation of large language models (LLMs) to time series forecasting\nposes unique challenges, as time series data is continuous in nature, while\nLLMs operate on discrete tokens. Despite the success of LLMs in natural\nlanguage processing (NLP) and other structured domains, aligning time series\ndata with language-based representations while maintaining both predictive\naccuracy and interpretability remains a significant hurdle. Existing methods\nhave attempted to reprogram time series data into text-based forms, but these\noften fall short in delivering meaningful, interpretable results. In this\npaper, we propose a multi-level text alignment framework for time series\nforecasting using LLMs that not only improves prediction accuracy but also\nenhances the interpretability of time series representations. Our method\ndecomposes time series into trend, seasonal, and residual components, which are\nthen reprogrammed into component-specific text representations. We introduce a\nmulti-level alignment mechanism, where component-specific embeddings are\naligned with pre-trained word tokens, enabling more interpretable forecasts.\nExperiments on multiple datasets demonstrate that our method outperforms\nstate-of-the-art models in accuracy while providing good interpretability."}
{"id": "2504.13626", "pdf": "https://arxiv.org/pdf/2504.13626.pdf", "abs": "https://arxiv.org/abs/2504.13626", "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models", "authors": ["Yule Liu", "Jingyi Zheng", "Zhen Sun", "Zifan Peng", "Wenhan Dong", "Zeyang Sha", "Shiwen Cui", "Weiqiang Wang", "Xinlei He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities on various tasks. However, LRMs often suffer from an\n``overthinking'' problem, where the model generates excessively redundant\nreasoning steps with limited performance gains. In this work, we empirically\nreveal an important characteristic of LRM behaviors that placing external CoTs\ngenerated by smaller models between the thinking token (\\texttt{<think>} and\n\\texttt{</think>}) can effectively manipulate the model to generate fewer\nthoughts. Building on this finding, we propose a simple yet efficient pipeline,\n\\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby\nsignificantly reducing computational costs. We conduct extensive experiments to\nevaluate the utility and efficiency of \\Method. For instance, when applied to\nQwQ-32B on the LiveBench/Code dataset, \\Method keeps the original performance\nwhile reducing output token counts by approximately 30\\%, with minimal overhead\nintroduced by the CoT generator. Furthermore, we identify two suboptimal modes,\nblindly following flawed external thoughts and unnecessary rethinking, and show\nthat simple mitigations, such as difficulty-aware fallbacks, can further\nimprove performance. Overall, \\Method offers a practical, general, and\nefficient way to optimize LRM inference, making powerful reasoning models more\naccessible and scalable for real-world applications."}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075.pdf", "abs": "https://arxiv.org/abs/2504.17075", "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to COLM 2025", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree."}
{"id": "2505.00008", "pdf": "https://arxiv.org/pdf/2505.00008.pdf", "abs": "https://arxiv.org/abs/2505.00008", "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination", "authors": ["Zhaoyi Sun", "Wen-Wai Yim", "Ozlem Uzuner", "Fei Xia", "Meliha Yetisgen"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted by the Journal of Biomedical Informatics", "summary": "Objective: This review aims to explore the potential and challenges of using\nNatural Language Processing (NLP) to detect, correct, and mitigate medically\ninaccurate information, including errors, misinformation, and hallucination. By\nunifying these concepts, the review emphasizes their shared methodological\nfoundations and their distinct implications for healthcare. Our goal is to\nadvance patient safety, improve public health communication, and support the\ndevelopment of more reliable and transparent NLP applications in healthcare.\n  Methods: A scoping review was conducted following PRISMA guidelines,\nanalyzing studies from 2020 to 2024 across five databases. Studies were\nselected based on their use of NLP to address medically inaccurate information\nand were categorized by topic, tasks, document types, datasets, models, and\nevaluation metrics.\n  Results: NLP has shown potential in addressing medically inaccurate\ninformation on the following tasks: (1) error detection (2) error correction\n(3) misinformation detection (4) misinformation correction (5) hallucination\ndetection (6) hallucination mitigation. However, challenges remain with data\nprivacy, context dependency, and evaluation standards.\n  Conclusion: This review highlights the advancements in applying NLP to tackle\nmedically inaccurate information while underscoring the need to address\npersistent challenges. Future efforts should focus on developing real-world\ndatasets, refining contextual methods, and improving hallucination management\nto ensure reliable and transparent healthcare applications."}
{"id": "2505.05026", "pdf": "https://arxiv.org/pdf/2505.05026.pdf", "abs": "https://arxiv.org/abs/2505.05026", "title": "Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for Multimodal UI/UX Design Understanding", "authors": ["Jaehyun Jeon", "Min Soo Kim", "Jang Han Yoon", "Sumin Shim", "Yejin Choi", "Hanbin Kim", "Youngjae Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 25 figures, Our code and dataset:\n  https://github.com/jeochris/wiserui-bench", "summary": "User interface (UI) design goes beyond visuals, guiding user behavior and\noverall user experience (UX). Strategically crafted interfaces, for example,\ncan boost sign-ups and drive business sales, underscoring the shift toward\nUI/UX as a unified design concept. While recent studies have explored UI\nquality evaluation using Multimodal Large Language Models (MLLMs), they largely\nfocus on surface-level features, overlooking behavior-oriented aspects. To fill\nthis gap, we introduce WiserUI-Bench, a novel benchmark for assessing models'\nmultimodal understanding of UI/UX design. It includes 300 diverse real-world UI\nimage pairs, each consisting of two design variants A/B-tested at scale by\nactual companies, where one was empirically validated to steer more user\nactions than the other. Each pair is accompanied one or more of 684\nexpert-curated rationales that capture key factors behind each winning design's\neffectiveness, spanning diverse cognitive dimensions of UX. Our benchmark\nsupports two core tasks: (1) selecting the more effective UI/UX design by\npredicting the A/B test verified winner and (2) assessing how well a model,\ngiven the winner, can explain its effectiveness in alignment with expert\nreasoning. Experiments across several MLLMs show that current models exhibit\nlimited nuanced reasoning about UI/UX design and its behavioral impact. We\nbelieve our work will foster research in UI/UX understanding and enable broader\napplications such as behavior-aware interface optimization."}
{"id": "2505.10939", "pdf": "https://arxiv.org/pdf/2505.10939.pdf", "abs": "https://arxiv.org/abs/2505.10939", "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction", "authors": ["Mohammadtaha Bagherifard", "Sahar Rajabi", "Ali Edalat", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages", "summary": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM."}
{"id": "2505.11336", "pdf": "https://arxiv.org/pdf/2505.11336.pdf", "abs": "https://arxiv.org/abs/2505.11336", "title": "XtraGPT: Context-Aware and Controllable Academic Paper Revision via Human-AI Collaboration", "authors": ["Nuo Chen", "Andre Lin HuiKai", "Jiaying Wu", "Junyi Hou", "Zining Zhang", "Qian Wang", "Xidong Wang", "Bingsheng He"], "categories": ["cs.CL"], "comment": "Preprint. The model report is available at\n  https://arxiv.org/abs/2505.11336v1", "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts."}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935.pdf", "abs": "https://arxiv.org/abs/2505.11935", "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings, camera-ready version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, editing charts via code presents a greater challenge.\nThis task demands MLLMs to integrate chart understanding and reasoning\ncapacities, which are labor-intensive. While many MLLMs claim such editing\ncapabilities, current evaluations rely on limited case studies, highlighting\nthe urgent need for a comprehensive evaluation framework. In this work, we\npropose \\textsc{ChartEdit}, a novel benchmark designed for chart editing tasks,\nfeaturing $1405$ diverse editing instructions applied to $233$ real-world\ncharts, each manually annotated and validated for accuracy. Utilizing\n\\textsc{ChartEdit}, we evaluate the performance of 10 mainstream MLLMs across\ntwo types of experiments at both the code and chart levels. The results suggest\nthat large-scale models can generate code to produce images that partially\nmatch the reference images. However, their ability to generate accurate edits\naccording to the instructions remains limited. The state-of-the-art (SOTA)\nmodel achieves a score of only $59.96$, highlighting significant challenges in\nprecise modification. In contrast, small-scale models, including chart-domain\nmodels, struggle both with following editing instructions and generating\noverall chart images, underscoring the need for further development in this\narea. Code is available at https://github.com/xxlllz/ChartEdit."}
{"id": "2505.15442", "pdf": "https://arxiv.org/pdf/2505.15442.pdf", "abs": "https://arxiv.org/abs/2505.15442", "title": "On the Generalization vs Fidelity Paradox in Knowledge Distillation", "authors": ["Suhas Kamasetty Ramesh", "Ayan Sengupta", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation (KD) is a key technique for compressing large language\nmodels into smaller ones while preserving performance. Despite the recent\ntraction of KD research, its effectiveness for smaller language models (LMs)\nand the mechanisms driving knowledge transfer remain underexplored. In this\nwork, we present the first large-scale empirical and statistical analysis of KD\nacross models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks\nin a zero-shot setting. Our findings reveal that KD can improve the average\nperformance of smaller models by up to $10\\%$, with a peak task specific gain\nof $22\\%$, while providing only marginal benefits ($\\sim 1.3\\%$) for larger\nmodels. Surprisingly, teacher performance has a minimal impact on student\noutcomes, while teacher task expertise impacts KD effectiveness. A correlation\nstudy indicates that smaller LMs benefit more from KD, whereas larger LMs show\ndiminished gains. Additionally, we uncover a misalignment between improvements\nin student performance and reasoning fidelity, suggesting that while KD\nenhances accuracy, it does not always maintain the structured decision-making\nprocesses of the teacher. Our ablation study further highlights the importance\nof teacher signals and logit smoothing in influencing students' performance\nafter distillation. Overall, our study offers a comprehensive empirical and\nstatistical assessment of KD, highlighting both its benefits and trade-offs\nwhen distilling knowledge from larger to smaller LMs."}
{"id": "2505.17827", "pdf": "https://arxiv.org/pdf/2505.17827.pdf", "abs": "https://arxiv.org/abs/2505.17827", "title": "Not All Tokens Are What You Need In Thinking", "authors": ["Hang Yuan", "Bin Yu", "Haotian Li", "Shijun Yang", "Christina Dan Wang", "Zhou Yu", "Xueyin Xu", "Weizhen Qi", "Kai Chen"], "categories": ["cs.CL"], "comment": "11 pages, 7 figures and 3 tables", "summary": "Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit\nimpressive problem-solving capabilities but suffer from critical\ninefficiencies: high inference latency, excessive computational resource\nconsumption, and a tendency toward overthinking -- generating verbose chains of\nthought (CoT) laden with redundant tokens that contribute minimally to the\nfinal answer. To address these issues, we propose Conditional Token Selection\n(CTS), a token-level compression framework with a flexible and variable\ncompression ratio that identifies and preserves only the most essential tokens\nin CoT. CTS evaluates each token's contribution to deriving correct answers\nusing conditional importance scoring, then trains models on compressed CoT.\nExtensive experiments demonstrate that CTS effectively compresses long CoT\nwhile maintaining strong reasoning performance. Notably, on the GPQA benchmark,\nQwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with\n13.2% fewer reasoning tokens (13% training token reduction). Further reducing\ntraining tokens by 42% incurs only a marginal 5% accuracy drop while yielding a\n75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy\nin existing CoT."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147.pdf", "abs": "https://arxiv.org/abs/2505.19147", "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.20243", "pdf": "https://arxiv.org/pdf/2505.20243.pdf", "abs": "https://arxiv.org/abs/2505.20243", "title": "It's High Time: A Survey of Temporal Question Answering", "authors": ["Bhawna Piryani", "Abdelrahman Abdallah", "Jamshid Mozafari", "Avishek Anand", "Adam Jatowt"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization."}
{"id": "2506.00893", "pdf": "https://arxiv.org/pdf/2506.00893.pdf", "abs": "https://arxiv.org/abs/2506.00893", "title": "Affordance Benchmark for MLLMs", "authors": ["Junying Wang", "Wenzhe Li", "Yalun Wu", "Yingji Liang", "Yijin Guo", "Chunyi Li", "Haodong Duan", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affordance theory suggests that environments inherently provide action\npossibilities shaping perception and behavior. While Multimodal Large Language\nModels (MLLMs) achieve strong performance in vision-language tasks, their\nability to perceive affordance, which is crucial for intuitive and safe\ninteractions, remains underexplored. To address this, we introduce **A4Bench**,\na novel benchmark designed to evaluate the affordance perception abilities of\nMLLMs across two dimensions: 1) Constitutive Affordance, assessing\nunderstanding of inherent object properties through 1,282 questionanswer pairs\nspanning nine sub-disciplines, and 2) Transformative Affordance, probing\ndynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or\nindividual-specific affordance) with 718 challenging question-answer pairs. We\nevaluate 17 MLLMs (nine proprietary and eight open-source) and compare them to\nhuman performance. Results show that proprietary models generally outperform\nopen-source ones, yet all models perform far below humans, especially in\ntransformative affordance. Furthermore, even top-performing models, such as\nGemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind\nhuman performance (best: 85.34%, worst: 81.25%). These findings highlight\ncritical gaps in environmental understanding of MLLMs and provide a foundation\nfor advancing AI systems toward more robust, context-aware interactions."}
{"id": "2506.05386", "pdf": "https://arxiv.org/pdf/2506.05386.pdf", "abs": "https://arxiv.org/abs/2506.05386", "title": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes", "authors": ["Lo Pang-Yun Ting", "Chengshuai Zhao", "Yu-Hua Zeng", "Yuan Jee Lim", "Kun-Ta Chuang", "Huan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning."}
{"id": "2506.08625", "pdf": "https://arxiv.org/pdf/2506.08625.pdf", "abs": "https://arxiv.org/abs/2506.08625", "title": "RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval", "authors": ["Minhae Oh", "Jeonghye Kim", "Nakyung Lee", "Donggeon Seo", "Taeuk Kim", "Jungwoo Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific reasoning requires not only long-chain reasoning processes, but\nalso knowledge of domain-specific terminologies and adaptation to updated\nfindings. To deal with these challenges for scientific reasoning, we introduce\nRAISE, a step-by-step retrieval-augmented framework which retrieves logically\nrelevant documents from in-the-wild corpus. RAISE is divided into three steps:\nproblem decomposition, logical query generation, and logical retrieval. We\nobserve that RAISE consistently outperforms other baselines on scientific\nreasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves\ndocuments that are not only similar in terms of the domain knowledge, but also\ndocuments logically more relevant."}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251.pdf", "abs": "https://arxiv.org/abs/2506.09251", "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks."}
{"id": "2506.14758", "pdf": "https://arxiv.org/pdf/2506.14758.pdf", "abs": "https://arxiv.org/abs/2506.14758", "title": "Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing large language model (LLM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLLMs. Through empirical analysis, we uncover positive correlations between\nhigh-entropy regions and three types of exploratory reasoning actions: (1)\npivotal tokens that determine or connect logical steps, (2) reflective actions\nsuch as self-verification and correction, and (3) rare behaviors under-explored\nby the base LLMs. Motivated by this, we introduce a minimal modification to\nstandard RL with only one line of code: augmenting the advantage function with\nan entropy-based term. Unlike traditional maximum-entropy methods which\nencourage exploration by promoting uncertainty, we encourage exploration by\npromoting longer and deeper reasoning chains. Notably, our method achieves\nsignificant gains on the Pass@K metric -- an upper-bound estimator of LLM\nreasoning capabilities -- even when evaluated with extremely large K values,\npushing the boundaries of LLM reasoning."}
{"id": "2506.16123", "pdf": "https://arxiv.org/pdf/2506.16123.pdf", "abs": "https://arxiv.org/abs/2506.16123", "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning", "authors": ["Natapong Nitarach", "Warit Sirichotedumrong", "Panop Pitchayarthorn", "Pittawat Taveekitworachai", "Potsawee Manakul", "Kunat Pipatanakul"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\nframework that embeds domain-specific expert financial reasoning blueprints to\nguide large language models' behaviors. We identify three main prompting styles\nin financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured\nCoT (free-form reasoning), and (3) structured CoT (with explicitly structured\nreasoning steps). Prior work has mainly focused on the first two, while\nstructured CoT remains underexplored and lacks domain expertise incorporation.\nTherefore, we evaluate all three prompting approaches across ten CFA-style\nfinancial domains and introduce FinCoT as the first structured finance-specific\nprompting approach incorporating blueprints from domain experts. FinCoT\nimproves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to\n80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%,\nwhile reducing output length by up to 8.9x and 1.16x compared to structured CoT\nmethods, respectively. We find that FinCoT proves most effective for models\nlacking financial post-training. Our findings show that FinCoT does not only\nimprove performance and reduce inference costs but also yields more\ninterpretable and expert-aligned reasoning traces."}
{"id": "2506.16383", "pdf": "https://arxiv.org/pdf/2506.16383.pdf", "abs": "https://arxiv.org/abs/2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Work draft", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."}
{"id": "2506.16792", "pdf": "https://arxiv.org/pdf/2506.16792.pdf", "abs": "https://arxiv.org/abs/2506.16792", "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning", "authors": ["Muyang Zheng", "Yuanzhi Yao", "Changting Lin", "Rui Wang", "Caihong Kai"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks -- methods\ndesigned to elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version -- order-determining optimization. We\nconduct extensive experiments on two datasets using two open-source and four\nclosed-source models. Results show that MIST achieves competitive attack\nsuccess rate, relatively low query count, and fair transferability,\noutperforming or matching state-of-the-art jailbreak methods. Additionally, we\nconduct analysis on computational efficiency to validate the practical\nviability of MIST."}
{"id": "2506.19467", "pdf": "https://arxiv.org/pdf/2506.19467.pdf", "abs": "https://arxiv.org/abs/2506.19467", "title": "Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?", "authors": ["Jingwei Ni", "Yu Fan", "Vil√©m Zouhar", "Donya Rooein", "Alexander Hoyle", "Mrinmaya Sachan", "Markus Leippold", "Dirk Hovy", "Elliott Ash"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint Under Review", "summary": "Variation in human annotation (i.e., disagreements) is common in NLP, often\nreflecting important information like task subjectivity and sample ambiguity.\nModeling this variation is important for applications that are sensitive to\nsuch information. Although RLVR-style reasoning (Reinforcement Learning with\nVerifiable Rewards) has improved Large Language Model (LLM) performance on many\ntasks, it remains unclear whether such reasoning enables LLMs to capture\ninformative variation in human annotation. In this work, we evaluate the\ninfluence of different reasoning settings on LLM disagreement modeling. We\nsystematically evaluate each reasoning setting across model sizes, distribution\nexpression methods, and steering methods, resulting in 60 experimental setups\nacross 3 tasks. Surprisingly, our results show that RLVR-style reasoning\ndegrades performance in disagreement modeling, while naive Chain-of-Thought\n(CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback).\nThese findings underscore the potential risk of replacing human annotators with\nreasoning LLMs, especially when disagreements are important."}
{"id": "2506.21562", "pdf": "https://arxiv.org/pdf/2506.21562.pdf", "abs": "https://arxiv.org/abs/2506.21562", "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design."}
{"id": "2506.21848", "pdf": "https://arxiv.org/pdf/2506.21848.pdf", "abs": "https://arxiv.org/abs/2506.21848", "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification", "authors": ["Duo Zhang", "Junyi Mo"], "categories": ["cs.CL"], "comment": null, "summary": "Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification."}
{"id": "2506.23463", "pdf": "https://arxiv.org/pdf/2506.23463.pdf", "abs": "https://arxiv.org/abs/2506.23463", "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "authors": ["WonJune Jang"], "categories": ["cs.CL", "I.2.7"], "comment": "26 pages, 9 figures", "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks. Our code\navailable at:\nhttps://github.com/torijune/ATF-Adaptive-Table-Filtering-Framework"}
{"id": "2507.02833", "pdf": "https://arxiv.org/pdf/2507.02833.pdf", "abs": "https://arxiv.org/abs/2507.02833", "title": "Generalizing Verifiable Instruction Following", "authors": ["Valentina Pyatkin", "Saumya Malik", "Victoria Graf", "Hamish Ivison", "Shengyi Huang", "Pradeep Dasigi", "Nathan Lambert", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": "11 pages", "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode."}
{"id": "2507.07024", "pdf": "https://arxiv.org/pdf/2507.07024.pdf", "abs": "https://arxiv.org/abs/2507.07024", "title": "FlexOlmo: Open Language Models for Flexible Data Use", "authors": ["Weijia Shi", "Akshita Bhagia", "Kevin Farhat", "Niklas Muennighoff", "Pete Walsh", "Jacob Morrison", "Dustin Schwenk", "Shayne Longpre", "Jake Poznanski", "Allyson Ettinger", "Daogao Liu", "Margaret Li", "Dirk Groeneveld", "Mike Lewis", "Wen-tau Yih", "Luca Soldaini", "Kyle Lo", "Noah A. Smith", "Luke Zettlemoyer", "Pang Wei Koh", "Hannaneh Hajishirzi", "Ali Farhadi", "Sewon Min"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference."}
{"id": "2507.08336", "pdf": "https://arxiv.org/pdf/2507.08336.pdf", "abs": "https://arxiv.org/abs/2507.08336", "title": "Distillation versus Contrastive Learning: How to Train Your Rerankers", "authors": ["Zhichao Xu", "Zhiqi Huang", "Shengyao Zhuang", "Vivek Srikumar"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Training effective text rerankers is crucial for information retrieval. Two\nstrategies are widely used: contrastive learning (optimizing directly on\nground-truth labels) and knowledge distillation (transferring knowledge from a\nlarger reranker). While both have been studied extensively, a clear comparison\nof their effectiveness for training cross-encoder rerankers under practical\nconditions is needed.\n  This paper empirically compares these strategies by training rerankers of\ndifferent sizes and architectures using both methods on the same data, with a\nstrong contrastive learning model acting as the distillation teacher. Our\nresults show that knowledge distillation generally yields better in-domain and\nout-of-domain ranking performance than contrastive learning when distilling\nfrom a larger teacher model. This finding is consistent across student model\nsizes and architectures. However, distilling from a teacher of the same\ncapacity does not provide the same advantage, particularly for out-of-domain\ntasks. These findings offer practical guidance for choosing a training strategy\nbased on available teacher models. We recommend using knowledge distillation to\ntrain smaller rerankers if a larger, more powerful teacher is accessible; in\nits absence, contrastive learning remains a robust baseline."}
{"id": "2507.09506", "pdf": "https://arxiv.org/pdf/2507.09506.pdf", "abs": "https://arxiv.org/abs/2507.09506", "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. First 2 authors contributed equally.\n  Project webpage: https://wujunjie1998.github.io/Ref-Long-website/", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long."}
{"id": "2507.10852", "pdf": "https://arxiv.org/pdf/2507.10852.pdf", "abs": "https://arxiv.org/abs/2507.10852", "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness."}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240.pdf", "abs": "https://arxiv.org/abs/2507.14240", "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "authors": ["Mohammad Shahedur Rahman", "Runbang Hu", "Peng Gao", "Yuede Ji"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) leverage deep learning architectures to process\nand predict sequences of words based on context, enabling them to perform a\nwide range of natural language processing tasks, such as translation,\nsummarization, question answering, and content generation. However, the\nincreasing size and complexity of developing, training, and deploying\ncutting-edge LLMs demand extensive computational resources and large-scale\ndatasets. This creates a significant barrier for researchers and practitioners.\nBecause of that, platforms that host models and datasets have gained widespread\npopularity. For example, on one of the most popular platforms, i.e., Hugging\nFace, there are more than 1.8 million models and more than 450K datasets by the\nend of June 2025, and the trend does not show any slowdown.\n  As existing LLMs are often built from base models or other pretrained models\nand use external datasets, they can inevitably inherit vulnerabilities, biases,\nor malicious components that exist in previous models or datasets. Therefore,\nit is critical to understand these components' origin and development process\nto detect potential risks better, improve model fairness, and ensure compliance\nwith regulatory frameworks. Motivated by that, this project aims to study such\nrelationships between models and datasets, which are the central parts of the\nLLM supply chain. First, we design a methodology to collect LLMs' supply chain\ninformation systematically. With the collected information, we design a new\ngraph to model the relationships between models and datasets, which is a large\ndirected heterogeneous graph having 402,654 nodes and 462,524 edges. Then, on\ntop of this graph, we perform different types of analysis and make multiple\ninteresting findings."}
{"id": "2507.18182", "pdf": "https://arxiv.org/pdf/2507.18182.pdf", "abs": "https://arxiv.org/abs/2507.18182", "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models", "authors": ["Wonjun Jeong", "Dongseok Kim", "Taegkeun Whangbo"], "categories": ["cs.CL", "cs.AI"], "comment": "Comments: 34 pages, 1 figure. v2: All \"Consequence.\" statements in\n  the Theoretical Analysis section relabeled as \"Corollary.\"; duplicated values\n  in Table 20 (previously identical to Table 15) corrected", "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations."}
{"id": "2507.19906", "pdf": "https://arxiv.org/pdf/2507.19906.pdf", "abs": "https://arxiv.org/abs/2507.19906", "title": "CaliDrop: KV Cache Compression with Calibration", "authors": ["Yi Su", "Quantong Qiu", "Yuechi Zhou", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."}
{"id": "2507.20343", "pdf": "https://arxiv.org/pdf/2507.20343.pdf", "abs": "https://arxiv.org/abs/2507.20343", "title": "DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns", "authors": ["Bernd J. Kr√∂ger"], "categories": ["cs.CL"], "comment": "10 pages, 29 references, 2 figures, supplementary material. V2:\n  Discussion of the tongue-palate contact pattern for /t/", "summary": "We present DYNARTmo, a dynamic articulatory model designed to visualize\nspeech articulation processes in a two-dimensional midsagittal plane. The model\nbuilds upon the UK-DYNAMO framework and integrates principles of articulatory\nunderspecification, segmental and gestural control, and coarticulation.\nDYNARTmo simulates six key articulators based on ten continuous and six\ndiscrete control parameters, allowing for the generation of both vocalic and\nconsonantal articulatory configurations. The current implementation is embedded\nin a web-based application (SpeechArticulationTrainer) that includes sagittal,\nglottal, and palatal views, making it suitable for use in phonetics education\nand speech therapy. While this paper focuses on the static modeling aspects,\nfuture work will address dynamic movement generation and integration with\narticulatory-acoustic modules."}
{"id": "2507.22676", "pdf": "https://arxiv.org/pdf/2507.22676.pdf", "abs": "https://arxiv.org/abs/2507.22676", "title": "Listening to the Unspoken: Exploring \"365\" Aspects of Multimodal Interview Performance Assessment", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects."}
{"id": "2507.22940", "pdf": "https://arxiv.org/pdf/2507.22940.pdf", "abs": "https://arxiv.org/abs/2507.22940", "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel framework addressing a critical vulnerability in Large\nLanguage Models (LLMs): the prevalence of factual inaccuracies within\nintermediate reasoning steps despite correct final answers. This phenomenon\nposes substantial risks in high-stakes domains including healthcare, legal\nanalysis, and scientific research, where erroneous yet confidently presented\nreasoning can mislead users into dangerous decisions. Our framework integrates\nthree core components: (1) a specialized fact-checking classifier trained on\ncounterfactually augmented data to detect subtle factual inconsistencies within\nreasoning chains; (2) an enhanced Group Relative Policy Optimization (GRPO)\nreinforcement learning approach that balances factuality, coherence, and\nstructural correctness through multi-dimensional rewards; and (3) a mechanistic\ninterpretability method examining how factuality improvements manifest in model\nactivations during reasoning processes. Extensive evaluation across multi\nstate-of-the-art models reveals concerning patterns: even leading models like\nClaude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and\n82.57% respectively. Our approach significantly enhances factual robustness (up\nto 49.90% improvement) while maintaining or improving performance on\nchallenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore,\nour neural activation-level analysis provides actionable insights into how\nfactual enhancements reshape reasoning trajectories within model architectures,\nestablishing foundations for future training methodologies that explicitly\ntarget factual robustness through activation-guided optimization."}
{"id": "2507.23541", "pdf": "https://arxiv.org/pdf/2507.23541.pdf", "abs": "https://arxiv.org/abs/2507.23541", "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning", "authors": ["Keer Lu", "Zheng Liang", "Youquan Li", "Jiejun Tan", "Da Pan", "Shusen Zhang", "Guosheng Dong", "Huang Leng", "Bin Cui", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%."}
{"id": "2405.18937", "pdf": "https://arxiv.org/pdf/2405.18937.pdf", "abs": "https://arxiv.org/abs/2405.18937", "title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description", "authors": ["Mahmoud Ahmed", "Junjie Fei", "Jian Ding", "Eslam Mohamed Bakr", "Mohamed Elhoseiny"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/"}
{"id": "2409.01071", "pdf": "https://arxiv.org/pdf/2409.01071.pdf", "abs": "https://arxiv.org/abs/2409.01071", "title": "VideoLLaMB: Long Streaming Video Understanding with Recurrent Memory Bridges", "authors": ["Yuxuan Wang", "Yiqi Song", "Cihang Xie", "Yang Liu", "Zilong Zheng"], "categories": ["cs.CV", "cs.CL"], "comment": "To appear at ICCV 2025", "summary": "Recent advancements in large-scale video-language models have shown\nsignificant potential for real-time planning and detailed interactions.\nHowever, their high computational demands and the scarcity of annotated\ndatasets limit their practicality for academic researchers. In this work, we\nintroduce VideoLLaMB, a novel and efficient framework for long video\nunderstanding that leverages recurrent memory bridges and temporal memory\ntokens to enable seamless encoding of entire video sequences with preserved\nsemantic continuity. Central to our approach is a SceneTiling algorithm that\nsegments videos into coherent semantic units, facilitating robust understanding\nacross tasks without requiring additional training. VideoLLaMB achieves\nstate-of-the-art performance, surpassing existing models by 4.2 points on four\nVideoQA benchmarks and by 2.06 points on egocentric planning tasks. Notably, it\nmaintains strong performance under extreme video length scaling (up to 8 times)\nand excels at fine-grained frame retrieval on our proposed Needle in a Video\nHaystack (NIAVH) benchmark. With linear GPU memory scaling, VideoLLaMB\nprocesses up to 320 frames using a single Nvidia A100 GPU, despite being\ntrained on only 16 frames-offering an unprecedented balance of accuracy,\nscalability, and cost-effectiveness. This makes it highly accessible and\npractical for the academic community."}
{"id": "2409.13095", "pdf": "https://arxiv.org/pdf/2409.13095.pdf", "abs": "https://arxiv.org/abs/2409.13095", "title": "Examining Test-Time Adaptation for Personalized Child Speech Recognition", "authors": ["Zhonghao Shi", "Xuan Shi", "Anfeng Xu", "Tiantian Feng", "Harshvardhan Srivastava", "Shrikanth Narayanan", "Maja J. Matariƒá"], "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Automatic speech recognition (ASR) models often experience performance\ndegradation due to data domain shifts introduced at test time, a challenge that\nis further amplified for child speakers. Test-time adaptation (TTA) methods\nhave shown great potential in bridging this domain gap. However, the use of TTA\nto adapt ASR models to the individual differences in each child's speech has\nnot yet been systematically studied. In this work, we investigate the\neffectiveness of two widely used TTA methods-SUTA, SGEM-in adapting\noff-the-shelf ASR models and their fine-tuned versions for child speech\nrecognition, with the goal of enabling continuous, unsupervised adaptation at\ntest time. Our findings show that TTA significantly improves the performance of\nboth off-the-shelf and fine-tuned ASR models, both on average and across\nindividual child speakers, compared to unadapted baselines. However, while TTA\nhelps adapt to individual variability, it may still be limited with\nnon-linguistic child speech."}
{"id": "2409.15672", "pdf": "https://arxiv.org/pdf/2409.15672.pdf", "abs": "https://arxiv.org/abs/2409.15672", "title": "Language-based Audio Moment Retrieval", "authors": ["Hokuto Munakata", "Taichi Nishimura", "Shota Nakada", "Tatsuya Komatsu"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "In this paper, we propose and design a new task called audio moment retrieval\n(AMR). Unlike conventional language-based audio retrieval tasks that search for\nshort audio clips from an audio database, AMR aims to predict relevant moments\nin untrimmed long audio based on a text query. Given the lack of prior work in\nAMR, we first build a dedicated dataset, Clotho-Moment, consisting of\nlarge-scale simulated audio recordings with moment annotations. We then propose\na DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental\nframework for AMR tasks. This model captures temporal dependencies within audio\nfeatures, inspired by similar video moment retrieval tasks, thus surpassing\nconventional clip-level audio retrieval methods. Additionally, we provide\nmanually annotated datasets to properly measure the effectiveness and\nrobustness of our methods on real data. Experimental results show that AM-DETR,\ntrained with Clotho-Moment, outperforms a baseline model that applies a\nclip-level audio retrieval method with a sliding window on all metrics,\nparticularly improving Recall1@0.7 by 9.00 points. Our datasets and code are\npublicly available in\nhttps://h-munakata.github.io/Language-based-Audio-Moment-Retrieval."}
{"id": "2410.07025", "pdf": "https://arxiv.org/pdf/2410.07025.pdf", "abs": "https://arxiv.org/abs/2410.07025", "title": "CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback", "authors": ["Dennis Hein", "Zhihong Chen", "Sophie Ostmeier", "Justin Xu", "Maya Varma", "Eduardo Pontes Reis", "Arne Edward Michalson", "Christian Bluethgen", "Hyun Joo Shin", "Curtis Langlotz", "Akshay S Chaudhari"], "categories": ["cs.CV", "cs.CL"], "comment": "ACL 2025", "summary": "Radiologists play a crucial role in translating medical images into\nactionable reports. However, the field faces staffing shortages and increasing\nworkloads. While automated approaches using vision-language models (VLMs) show\npromise as assistants, they require exceptionally high accuracy. Most current\nVLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional\npreference fine-tuning in the post-training pipeline has become standard\npractice in the general domain. The challenge in radiology lies in the\nprohibitive cost of obtaining radiologist feedback at scale. To address this\nchallenge, we propose an automated pipeline for preference feedback, focusing\non chest X-ray radiology report generation (RRG). Specifically, our method\nleverages publicly available datasets containing pairs of images and\nradiologist-written reference reports with reference-based metrics, or Judges,\neliminating the need for additional radiologist feedback. We investigate reward\noveroptimization via length exploitation in this setting and introduce a\nlength-controlled version of the GREEN score. Our best-performing setup\nachieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG\ntask while on average maintaining robust performance across six additional\nimage perception and reasoning tasks."}
{"id": "2410.08642", "pdf": "https://arxiv.org/pdf/2410.08642.pdf", "abs": "https://arxiv.org/abs/2410.08642", "title": "More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram", "authors": ["Elisabeth Steffen"], "categories": ["cs.SI", "cs.CL", "cs.CV", "cs.MM"], "comment": "12 pages, 10 figures", "summary": "To address the increasing prevalence of (audio-)visual data on social media,\nand to capture the evolving and dynamic nature of this communication,\nresearchers have begun to explore the potential of unsupervised approaches for\nanalyzing multimodal online content. However, existing research often neglects\nvisual content beyond memes, and in addition lacks methods to compare topic\nmodels across modalities. Our study addresses these gaps by applying multimodal\ntopic modeling for analyzing conspiracy theories in German-language Telegram\nchannels. We use BERTopic with CLIP for the analysis of textual and visual data\nin a corpus of ~40, 000 Telegram messages posted in October 2023 in 571\nGerman-language Telegram channels known for disseminating conspiracy theories.\nThrough this dataset, we provide insights into unimodal and multimodal topic\nmodels by analyzing symmetry and intersections of topics across modalities. We\ndemonstrate the variety of textual and visual content shared in the channels\ndiscovered through the topic modeling, and propose a conceptual framework for\nthe analysis of textual and visual discursive strategies in the communication\nof conspiracy theories. We apply the framework in a case study of the topic\ngroup Israel Gaza."}
{"id": "2410.09758", "pdf": "https://arxiv.org/pdf/2410.09758.pdf", "abs": "https://arxiv.org/abs/2410.09758", "title": "BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank Adaptation", "authors": ["Peijia Qin", "Ruiyi Zhang", "Pengtao Xie"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) is a flexible and efficient method for\nadapting large language models (LLMs) to downstream tasks. Among these methods,\nweight-decomposed low-rank adaptation (DoRA) is a promising approach that\ndecomposes weight matrices into magnitude and direction components to mimic\nfull fine-tuning (FT) better. However, DoRA's simultaneous optimization of\nthese components makes it over-expressive, increases the risk of overfitting,\nand creates a coupled updating pattern that limits its learning capacity. To\naddress these issues, we propose Bi-level Optimization-Based Weight-Decomposed\nLow-Rank Adaptation (BiDoRA), a novel PEFT method based on a bi-level\noptimization framework. BiDoRA fundamentally differs from DoRA by optimizing\nthe magnitude and direction in two separate, asynchronous loops using distinct\ntraining and validation data splits. This decoupled optimization process\neffectively mitigates overfitting and allows for more flexible updates that\nalign even more closely with FT. For instance, weight decomposition analysis\nshows BiDoRA achieves a magnitude-direction update correlation of $-8.042$,\nsignificantly closer to the FT ideal compared to $-1.784$ for DoRA. Evaluation\nof BiDoRA on diverse tasks spanning natural language understanding, generation,\ntoken classification, and extremely small biomedical datasets reveals that it\nconsistently outperforms DoRA and a wide range of leading PEFT methods. This\nimprovement is statistically significant, as demonstrated on the GLUE benchmark\nwhere BiDoRA surpasses DoRA with a p-value of $2.4\\times10^{-4}$ in terms of\nthe Wilcoxon signed-rank test. The code for BiDoRA is available at\nhttps://github.com/t2ance/BiDoRA."}
{"id": "2411.04358", "pdf": "https://arxiv.org/pdf/2411.04358.pdf", "abs": "https://arxiv.org/abs/2411.04358", "title": "Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation", "authors": ["Ayan Sengupta", "Vaibhav Seth", "Arinjay Pathak", "Aastha Verma", "Natraj Raman", "Sriram Gopalakrishnan", "Niladri Chatterjee", "Tanmoy Chakraborty"], "categories": ["cs.LG", "cs.CL"], "comment": "The paper is accepted in TMLR'25", "summary": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique that employs Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, stabilizing fine-tuned LLMs with only O(r) additional parameters, for\na given rank r. MonteCLoRA shows 0.5% and 1.6% improvements in accuracy and\nrobustness over unregularized low-rank adaptation method on natural language\nunderstanding tasks with pre-trained RoBERTa-base. Furthermore, in generative\ntasks with pre-trained LLaMA-1-7B and LLaMA-3.2-3B-Instruct, MonteCLoRA\ndemonstrates robust performance with 50% and 62% lower spreads respectively\nthan the contemporary efficient fine-tuning methods. The theoretical and\nempirical results presented in the paper underscore how parameterization and\nhyperpriors balance exploration-exploitation in the low-rank parametric space,\ntherefore leading to more optimal and robust parameter estimation during\nefficient fine-tuning."}
{"id": "2411.05060", "pdf": "https://arxiv.org/pdf/2411.05060.pdf", "abs": "https://arxiv.org/abs/2411.05060", "title": "A Guide to Misinformation Detection Data and Evaluation", "authors": ["Camille Thibault", "Jacob-Junqi Tian", "Gabrielle Peloquin-Skulski", "Taylor Lynn Curtis", "James Zhou", "Florence Laflamme", "Yuxiang Guan", "Reihaneh Rabbany", "Jean-Fran√ßois Godbout", "Kellin Pelrine"], "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": null, "summary": "Misinformation is a complex societal issue, and mitigating solutions are\ndifficult to create due to data deficiencies. To address this, we have curated\nthe largest collection of (mis)information datasets in the literature, totaling\n75. From these, we evaluated the quality of 36 datasets that consist of\nstatements or claims, as well as the 9 datasets that consist of data in purely\nparagraph form. We assess these datasets to identify those with solid\nfoundations for empirical work and those with flaws that could result in\nmisleading and non-generalizable results, such as spurious correlations, or\nexamples that are ambiguous or otherwise impossible to assess for veracity. We\nfind the latter issue is particularly severe and affects most datasets in the\nliterature. We further provide state-of-the-art baselines on all these\ndatasets, but show that regardless of label quality, categorical labels may no\nlonger give an accurate evaluation of detection model performance. Finally, we\npropose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the\nfield toward systemic solutions rather than inadvertently propagating issues in\nevaluation. Overall, this guide aims to provide a roadmap for higher quality\ndata and better grounded evaluations, ultimately improving research in\nmisinformation detection. All datasets and other artifacts are available at\nhttps://misinfo-datasets.complexdatalab.com/."}
{"id": "2412.15113", "pdf": "https://arxiv.org/pdf/2412.15113.pdf", "abs": "https://arxiv.org/abs/2412.15113", "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture", "authors": ["Thomas F Burns", "Tomoki Fukai", "Christopher J Earls"], "categories": ["cs.NE", "cs.AI", "cs.CL", "92B20, 68T01, 68T37, 68T50", "I.2; I.5; I.7; J.2; J.3"], "comment": "35 pages, 14 figures, 6 tables; accepted and published in TMLR", "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales."}
{"id": "2501.07927", "pdf": "https://arxiv.org/pdf/2501.07927.pdf", "abs": "https://arxiv.org/abs/2501.07927", "title": "Gandalf the Red: Adaptive Security for LLMs", "authors": ["Niklas Pfister", "V√°clav Volhejn", "Manuel Knott", "Santiago Arias", "Julia Bazi≈Ñska", "Mykhailo Bichurin", "Alan Commike", "Janet Darling", "Peter Dienes", "Matthew Fiedler", "David Haber", "Matthias Kraft", "Marco Lancini", "Max Mathys", "Dami√°n Pascual-Ortiz", "Jakub Podolak", "Adri√† Romero-L√≥pez", "Kyriacos Shiarlis", "Andreas Signer", "Zsolt Terek", "Athanasios Theocharis", "Daniel Timbrell", "Samuel Trautwein", "Samuel Watts", "Yun-Han Wu", "Mateo Rojas-Carulla"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally", "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications."}
{"id": "2501.16607", "pdf": "https://arxiv.org/pdf/2501.16607.pdf", "abs": "https://arxiv.org/abs/2501.16607", "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search", "authors": ["Shuozhi Yuan", "Limin Chen", "Miaomiao Yuan", "Jin Zhao"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.PL"], "comment": "15 pages, 6 figures", "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."}
{"id": "2502.04322", "pdf": "https://arxiv.org/pdf/2502.04322.pdf", "abs": "https://arxiv.org/abs/2502.04322", "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions", "authors": ["Yik Siu Chan", "Narutatsu Ri", "Yuxin Xiao", "Marzyeh Ghassemi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": "ICML 2025", "summary": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions."}
{"id": "2502.05206", "pdf": "https://arxiv.org/pdf/2502.05206.pdf", "abs": "https://arxiv.org/abs/2502.05206", "title": "Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety", "authors": ["Xingjun Ma", "Yifeng Gao", "Yixu Wang", "Ruofan Wang", "Xin Wang", "Ye Sun", "Yifan Ding", "Hengyuan Xu", "Yunhao Chen", "Yunhan Zhao", "Hanxun Huang", "Yige Li", "Yutao Wu", "Jiaming Zhang", "Xiang Zheng", "Yang Bai", "Zuxuan Wu", "Xipeng Qiu", "Jingfeng Zhang", "Yiming Li", "Xudong Han", "Haonan Li", "Jun Sun", "Cong Wang", "Jindong Gu", "Baoyuan Wu", "Siheng Chen", "Tianwei Zhang", "Yang Liu", "Mingming Gong", "Tongliang Liu", "Shirui Pan", "Cihang Xie", "Tianyu Pang", "Yinpeng Dong", "Ruoxi Jia", "Yang Zhang", "Shiqing Ma", "Xiangyu Zhang", "Neil Gong", "Chaowei Xiao", "Sarah Erfani", "Tim Baldwin", "Bo Li", "Masashi Sugiyama", "Dacheng Tao", "James Bailey", "Yu-Gang Jiang"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "comment": "706 papers, 60 pages, 3 figures, 14 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety", "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-powered Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."}
{"id": "2502.16395", "pdf": "https://arxiv.org/pdf/2502.16395.pdf", "abs": "https://arxiv.org/abs/2502.16395", "title": "AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science", "authors": ["Qiuhai Zeng", "Claire Jin", "Xinyue Wang", "Yuhan Zheng", "Qunhua Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate data analysis\nthrough executable code generation. Yet, data science tasks often admit\nmultiple statistically valid solutions, e.g. different modeling strategies,\nmaking it critical to understand the reasoning behind analyses, not just their\noutcomes. While manual review of LLM-generated code can help ensure statistical\nsoundness, it is labor-intensive and requires expertise. A more scalable\napproach is to evaluate the underlying workflows - the logical plans guiding\ncode generation. However, it remains unclear how to assess whether a\nLLM-generated workflow supports reproducible implementations.\n  To address this, we present $\\it{AIRepr}$, an $\\it{A}$nalyst -\n$\\it{I}$nspector framework for automatically evaluating and improving the\n$\\it{Repr}$oducibility of LLM-generated data analysis workflows. Our framework\nis grounded in statistical principles and supports scalable, automated\nassessment. We introduce two novel reproducibility-enhancing prompting\nstrategies and benchmark them against standard prompting across 15\nanalyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our\nfindings show that workflows with higher reproducibility also yield more\naccurate analyses, and that reproducibility-enhancing prompts substantially\nimprove both metrics. This work provides a foundation for more transparent,\nreliable, and efficient human-AI collaboration in data science. Our code is\npublicly available."}
{"id": "2503.08379", "pdf": "https://arxiv.org/pdf/2503.08379.pdf", "abs": "https://arxiv.org/abs/2503.08379", "title": "JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with Query Relevance Judgments", "authors": ["Leandro Car√≠sio Fernandes", "Leandro dos Santos Ribeiro", "Marcos Vin√≠cius Borela de Castro", "Leonardo Augusto da Silva Pacheco", "Edans Fl√°vius de Oliveira Sandes"], "categories": ["cs.IR", "cs.CL"], "comment": "23 pages", "summary": "This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens."}
{"id": "2503.12937", "pdf": "https://arxiv.org/pdf/2503.12937.pdf", "abs": "https://arxiv.org/abs/2503.12937", "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Huanjin Yao", "Shunyu Liu", "Xikun Zhang", "Shijian Lu", "Dacheng Tao"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "ICCV 2025 Camera Ready", "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods."}
{"id": "2503.18458", "pdf": "https://arxiv.org/pdf/2503.18458.pdf", "abs": "https://arxiv.org/abs/2503.18458", "title": "StableGS: A Floater-Free Framework for 3D Gaussian Splatting", "authors": ["Luchao Wang", "Qian Ren", "Kaimin Liao", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) reconstructions are plagued by stubborn\n``floater\" artifacts that degrade their geometric and visual fidelity. We are\nthe first to reveal the root cause: a fundamental conflict in the 3DGS\noptimization process where the opacity gradients of floaters vanish when their\nblended color reaches a pseudo-equilibrium of canceling errors against the\nbackground, trapping them in a spurious local minimum. To resolve this, we\npropose StableGS, a novel framework that decouples geometric regularization\nfrom final appearance rendering. Its core is a Dual Opacity architecture that\ncreates two separate rendering paths: a ``Geometric Regularization Path\" to\nbear strong depth-based constraints for structural correctness, and an\n``Appearance Refinement Path\" to generate high-fidelity details upon this\nstable foundation. We complement this with a synergistic set of geometric\nconstraints: a self-supervised depth consistency loss and an external geometric\nprior enabled by our efficient global scale optimization algorithm. Experiments\non multiple benchmarks show StableGS not only eliminates floaters but also\nresolves the common blur-artifact trade-off, achieving state-of-the-art\ngeometric accuracy and visual quality."}
{"id": "2503.21735", "pdf": "https://arxiv.org/pdf/2503.21735.pdf", "abs": "https://arxiv.org/abs/2503.21735", "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics", "authors": ["Arsham Gholamzadeh Khoee", "Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Ensuring reliable software release decisions is critical in safety-critical\ndomains such as automotive manufacturing. Release validation relies on large\ntabular datasets, yet manual analysis is slow, costly, and error-prone. While\nLarge Language Models (LLMs) offer promising automation potential, they face\nchallenges in analytical reasoning, structured data handling, and ambiguity\nresolution. This paper introduces GateLens, an LLM-based system for analyzing\ntabular data in the automotive domain. GateLens translates natural language\nqueries into Relational Algebra (RA) expressions and generates optimized Python\ncode. Unlike traditional multi-agent or planning-based systems that can be\nslow, opaque, and costly to maintain, GateLens emphasizes speed, transparency,\nand reliability. Experimental results show that GateLens outperforms the\nexisting Chain-of-Thought (CoT) + Self-Consistency (SC) based system on\nreal-world datasets, particularly in handling complex and ambiguous queries.\nAblation studies confirm the essential role of the RA layer. Industrial\ndeployment shows over 80% reduction in analysis time while maintaining high\naccuracy across test result interpretation, impact assessment, and release\ncandidate evaluation. GateLens operates effectively in zero-shot settings\nwithout requiring few-shot examples or agent orchestration. This work advances\ndeployable LLM system design by identifying key architectural\nfeatures-intermediate formal representations, execution efficiency, and low\nconfiguration overhead-crucial for safety-critical industrial applications."}
{"id": "2504.07448", "pdf": "https://arxiv.org/pdf/2504.07448.pdf", "abs": "https://arxiv.org/abs/2504.07448", "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation", "authors": ["Juzheng Zhang", "Jiacheng You", "Ashwinee Panda", "Tom Goldstein"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "COLM 2025", "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\nnotable overhead and suffers from parameter interference in multi-task\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\neffective approach that freezes the projection matrices $A$ as random\nprojections and sparsifies the matrices $B$ using task-specific masks. This\ndesign substantially reduces the number of trainable parameters while\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\ninterference in adapter merging by leveraging the orthogonality between adapter\nsubspaces, and supports continual learning by using sparsity to mitigate\ncatastrophic forgetting. Extensive experiments across natural language\nunderstanding, mathematical reasoning, code generation, and safety alignment\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\nmulti-task experiments, LoRI enables effective adapter merging and continual\nlearning with reduced cross-task interference. Code is available at:\nhttps://github.com/juzhengz/LoRI"}
{"id": "2504.16628", "pdf": "https://arxiv.org/pdf/2504.16628.pdf", "abs": "https://arxiv.org/abs/2504.16628", "title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks."}
{"id": "2505.07167", "pdf": "https://arxiv.org/pdf/2505.07167.pdf", "abs": "https://arxiv.org/abs/2505.07167", "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods."}
{"id": "2505.10981", "pdf": "https://arxiv.org/pdf/2505.10981.pdf", "abs": "https://arxiv.org/abs/2505.10981", "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory", "authors": ["Yexiang Liu", "Zekun Li", "Zhi Fang", "Nan Xu", "Ran He", "Tieniu Tan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 Outstanding Paper Award, 33 pages, 51 figures", "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a probabilistic method to efficiently predict scaling\nperformance and identify the best prompting strategy under large sampling\ntimes, eliminating the need for resource-intensive inference processes in\npractical applications. Furthermore, we introduce two ways derived from our\ntheoretical analysis to significantly improve the scaling performance. We hope\nthat our research can promote to re-examine the role of complicated prompting,\nunleash the potential of simple prompting strategies, and provide new insights\nfor enhancing test-time scaling performance. Code is available at\nhttps://github.com/MraDonkey/rethinking_prompting."}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842.pdf", "abs": "https://arxiv.org/abs/2505.12842", "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline while only increasing training time by 4.9\\% and\ntesting time by 6.5\\%. We also experimentally demonstrate that GEM can improve\nthe step-wise success rate by 9.40\\% by requesting assistance from the cloud\nmodel when encountering OOD samples. Analysis verifies the generalization\nability of our method through experiments on nine different backbones. The\ncodes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents."}
{"id": "2505.14899", "pdf": "https://arxiv.org/pdf/2505.14899.pdf", "abs": "https://arxiv.org/abs/2505.14899", "title": "Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs", "authors": ["Wenjie Lin", "Jin Wei-Kocsis", "Jiansong Zhang", "Byung-Cheol Min", "Dongming Gan", "Paul Asunda", "Ragu Athinarayanan"], "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown great potential across various\ndomains, their applications in robotics remain largely limited to static\nprompt-based behaviors and still face challenges in complex tasks under\nzero-shot or few-shot settings. Inspired by human metacognitive learning and\ncreative problem-solving, we address this limitation by exploring a fundamental\nquestion: Can LLMs be empowered with metacognitive capabilities to reason,\nreflect, and create, thereby enhancing their ability to perform robotic tasks\nwith minimal demonstrations? In this paper, we present a framework that\nintegrates metacognitive learning into LLM-powered multi-robot collaboration.\nThe system equips the LLM-powered robotic agents with a skill decomposition and\nself-reflection mechanism that identifies modular skills from prior tasks,\nreflects on failures in unseen task scenarios, and synthesizes effective new\nsolutions. We propose a more challenging robotic benchmark task and evaluate\nour framework on the existing benchmark and the novel task. Experimental\nresults show that our metacognitive learning framework significantly\noutperforms existing baselines. Moreover, we observe that the framework can\ngenerate solutions that differ from the ground truth yet still successfully\ncomplete the tasks. These findings support our hypothesis that metacognitive\nlearning can foster creativity in robotic planning."}
{"id": "2505.18102", "pdf": "https://arxiv.org/pdf/2505.18102.pdf", "abs": "https://arxiv.org/abs/2505.18102", "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "authors": ["Takashi Ishida", "Thanawat Lodkaew", "Ikko Yamane"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models", "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies."}
{"id": "2506.16150", "pdf": "https://arxiv.org/pdf/2506.16150.pdf", "abs": "https://arxiv.org/abs/2506.16150", "title": "PRISON: Unmasking the Criminal Potential of Large Language Models", "authors": ["Xinyi Wu", "Geng Hong", "Pei Chen", "Yueyue Chen", "Xudong Pan", "Min Yang"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) advance, concerns about their misconduct in\ncomplex social contexts intensify. Existing research overlooked the systematic\nunderstanding and assessment of their criminal capability in realistic\ninteractions. We propose a unified framework PRISON, to quantify LLMs' criminal\npotential across five traits: False Statements, Frame-Up, Psychological\nManipulation, Emotional Disguise, and Moral Disengagement. Using structured\ncrime scenarios adapted from classic films grounded in reality, we evaluate\nboth criminal potential and anti-crime ability of LLMs. Results show that\nstate-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as\nproposing misleading statements or evasion tactics, even without explicit\ninstructions. Moreover, when placed in a detective role, models recognize\ndeceptive behavior with only 44% accuracy on average, revealing a striking\nmismatch between conducting and detecting criminal behavior. These findings\nunderscore the urgent need for adversarial robustness, behavioral alignment,\nand safety mechanisms before broader LLM deployment."}
{"id": "2507.03336", "pdf": "https://arxiv.org/pdf/2507.03336.pdf", "abs": "https://arxiv.org/abs/2507.03336", "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky", "authors": ["Ashutosh Hathidara", "Julien Yu", "Sebastian Schreiber"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents."}
{"id": "2507.04377", "pdf": "https://arxiv.org/pdf/2507.04377.pdf", "abs": "https://arxiv.org/abs/2507.04377", "title": "Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions", "authors": ["Xiao Zhang", "Johan Bos"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "ACMMM 2025", "summary": "Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation."}
{"id": "2507.10644", "pdf": "https://arxiv.org/pdf/2507.10644.pdf", "abs": "https://arxiv.org/abs/2507.10644", "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "authors": ["Tatiana Petrova", "Boris Bliznioukov", "Aleksandr Puzikov", "Radu State"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA."}
{"id": "2507.12806", "pdf": "https://arxiv.org/pdf/2507.12806.pdf", "abs": "https://arxiv.org/abs/2507.12806", "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Huan Wang", "Caiming Xiong"], "categories": ["cs.AI", "cs.CL"], "comment": "https://github.com/SalesforceAIResearch/MCPEval", "summary": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce MCPEval, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation."}
{"id": "2507.20957", "pdf": "https://arxiv.org/pdf/2507.20957.pdf", "abs": "https://arxiv.org/abs/2507.20957", "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis", "authors": ["Hoyoung Lee", "Junhyuk Seo", "Suhwan Park", "Junhyeong Lee", "Wonbin Ahn", "Chanyeol Choi", "Alejandro Lopez-Lira", "Yongjae Lee"], "categories": ["q-fin.PM", "cs.AI", "cs.CL"], "comment": null, "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence."}
{"id": "2507.21183", "pdf": "https://arxiv.org/pdf/2507.21183.pdf", "abs": "https://arxiv.org/abs/2507.21183", "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge", "authors": ["Guangchen Lan", "Sipeng Zhang", "Tianle Wang", "Yuwei Zhang", "Daoan Zhang", "Xinpeng Wei", "Xiaoman Pan", "Hongming Zhang", "Dong-Jun Han", "Christopher G. Brinton"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency."}
{"id": "2507.23511", "pdf": "https://arxiv.org/pdf/2507.23511.pdf", "abs": "https://arxiv.org/abs/2507.23511", "title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks", "authors": ["Yadong Niu", "Tianzi Wang", "Heinrich Dinkel", "Xingwei Sun", "Jiahao Zhou", "Gang Li", "Jizhong Liu", "Xunying Liu", "Junbo Zhang", "Jian Luan"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "9 main pages, 5 figures, 3 tables, and 14 appendix pages", "summary": "While large audio-language models have advanced open-ended audio\nunderstanding, they still fall short of nuanced human-level comprehension. This\ngap persists largely because current benchmarks, limited by data annotations\nand evaluation metrics, fail to reliably distinguish between generic and highly\ndetailed model outputs. To this end, this work introduces MECAT, a Multi-Expert\nConstructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via\na pipeline that integrates analysis from specialized expert models with\nChain-of-Thought large language model reasoning, MECAT provides\nmulti-perspective, fine-grained captions and open-set question-answering pairs.\nThe benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced\nAudio Text Evaluation). This metric penalizes generic terms and rewards\ndetailed descriptions by combining single-sample semantic similarity with\ncross-sample discriminability. A comprehensive evaluation of state-of-the-art\naudio models is also presented, providing new insights into their current\ncapabilities and limitations. The data and code are available at\nhttps://github.com/xiaomi-research/mecat"}
