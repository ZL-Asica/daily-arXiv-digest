{"id": "2505.10648", "pdf": "https://arxiv.org/pdf/2505.10648.pdf", "abs": "https://arxiv.org/abs/2505.10648", "title": "Generative Muscle Stimulation: Physical Assistance by Constraining Multimodal-AI with Biomechanical Knowledge", "authors": ["Yun Ho", "Romain Nith", "Peili Jiang", "Shan-Yuan Teng", "Pedro Lopes"], "categories": ["cs.HC"], "comment": "13 pages, 19 figures", "summary": "Decades of interactive electrical-muscle-stimulation (EMS) revealed its\npromise as a wearable interface for physical assistance-EMS directly\ndemonstrates movements through the users' body (e.g., shaking a spray-can\nbefore painting). However, interactive EMS-systems are highly-specialized\nbecause their feedback is (1) fixed (e.g., one program executes spray-can\ninstructions, another executes piano instructions) and (2) non-contextual\n(e.g., using a spray-can while cooking likely involves cooking oil, not paint,\nand thus shaking is unnecessary). To address this, we explored a more flexible\napproach and engineered a system that generates muscle-stimulation-instructions\ngiven the user's context. Through our examples, we show that such a system is\nflexible: it enables unprecedented EMS-interactions (e.g., opening a\nchild-proof pill bottle cap) but also replicates existing systems (e.g., shake\na spray can)-all without requiring task-specific programming. To achieve this,\nour system takes in user's spoken-requests and images from their point of view.\nIt uses computer vision (e.g., detect objects/handedness) and\nlarge-language-models (e.g., reason about objects/situations) to generate\ntextual-instructions. Finally, these instructions are then constrained by\nbiomechanical-knowledge (e.g., joint limits, kinematic-chain, EMS capabilities)\nto produce suitable muscle-stimulation gestures. We believe our concept marks a\nshift toward more general-purpose EMS-interfaces, enabling more flexible and\ncontext-aware assistance."}
{"id": "2505.10661", "pdf": "https://arxiv.org/pdf/2505.10661.pdf", "abs": "https://arxiv.org/abs/2505.10661", "title": "It's only fair when I think it's fair: How Gender Bias Alignment Undermines Distributive Fairness in Human-AI Collaboration", "authors": ["Domenique Zipperling", "Luca Deck", "Julia Lanzl", "Niklas KÃ¼hl"], "categories": ["cs.HC"], "comment": "Accepted to ACM FAccT 2025", "summary": "Human-AI collaboration is increasingly relevant in consequential areas where\nAI recommendations support human discretion. However, human-AI teams'\neffectiveness, capability, and fairness highly depend on human perceptions of\nAI. Positive fairness perceptions have been shown to foster trust and\nacceptance of AI recommendations. Yet, work on confirmation bias highlights\nthat humans selectively adhere to AI recommendations that align with their\nexpectations and beliefs -- despite not being necessarily correct or fair. This\nraises the question whether confirmation bias also transfers to the alignment\nof gender bias between human and AI decisions. In our study, we examine how\ngender bias alignment influences fairness perceptions and reliance. The results\nof a 2x2 between-subject study highlight the connection between gender bias\nalignment, fairness perceptions, and reliance, demonstrating that merely\nconstructing a ``formally fair'' AI system is insufficient for optimal human-AI\ncollaboration; ultimately, AI recommendations will likely be overridden if\nbiases do not align."}
{"id": "2505.10686", "pdf": "https://arxiv.org/pdf/2505.10686.pdf", "abs": "https://arxiv.org/abs/2505.10686", "title": "NeoLightning: A Modern Reimagination of Gesture-Based Sound Design", "authors": ["Yonghyun Kim", "Sangheon Park", "Marcus Parker", "Donghoon Seu", "Alexandria Smith"], "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "summary": "This paper introduces NeoLightning, a modern reinterpretation of the Buchla\nLightning. NeoLightning preserves the innovative spirit of Don Buchla's \"Buchla\nLightning\" (introduced in the 1990s) while making its gesture-based interaction\naccessible to contemporary users. While the original Buchla Lightning and many\nother historical instruments were groundbreaking in their time, they are now\nlargely unsupported, limiting user interaction to indirect experiences. To\naddress this, NeoLightning leverages MediaPipe for deep learning-based gesture\nrecognition and employs Max/MSP and Processing for real-time multimedia\nprocessing. The redesigned system offers precise, low-latency gesture\nrecognition and immersive 3D interaction. By merging the creative spirit of the\noriginal Lightning with modern advancements, NeoLightning redefines\ngesture-based musical interaction, expanding possibilities for expressive\nperformance and interactive sound design."}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831.pdf", "abs": "https://arxiv.org/abs/2505.10831", "title": "Creating General User Models from Computer Use", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture that user knowledge and preferences. GUMs can infer that a user is\npreparing for a wedding they're attending from messages with a friend. Or\nrecognize that a user is struggling with a collaborator's feedback on a draft\nby observing multiple stalled edits and a switch to reading related work. GUMs\nintroduce an architecture that infers new propositions about a user from\nmultimodal observations, retrieves related propositions for context, and\ncontinuously revises existing propositions. To illustrate the breadth of\napplications that GUMs enable, we demonstrate how they augment chat-based\nassistants with context, manage OS notifications to selectively surface\nimportant information, and enable interactive agents that adapt to preferences\nacross apps. We also instantiate proactive assistants (GUMBOs) that discover\nand execute useful suggestions on a user's behalf using their GUM. In our\nevaluations, we find that GUMs make calibrated and accurate inferences about\nusers, and that assistants built on GUMs proactively identify and perform\nactions that users wouldn't think to request explicitly. Altogether, GUMs\nintroduce methods that leverage multimodal models to understand unstructured\ncontext, enabling long-standing visions of HCI and entirely new interactive\nsystems that anticipate user needs."}
{"id": "2505.10643", "pdf": "https://arxiv.org/pdf/2505.10643.pdf", "abs": "https://arxiv.org/abs/2505.10643", "title": "Artificial Intelligence Bias on English Language Learners in Automatic Scoring", "authors": ["Shuchen Guo", "Yun Wang", "Jichao Yu", "Xuansheng Wu", "Bilgehan Ayik", "Field M. Watts", "Ehsan Latif", "Ninghao Liu", "Lei Liu", "Xiaoming Zhai"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study investigated potential scoring biases and disparities toward\nEnglish Language Learners (ELLs) when using automatic scoring systems for\nmiddle school students' written responses to science assessments. We\nspecifically focus on examining how unbalanced training data with ELLs\ncontributes to scoring bias and disparities. We fine-tuned BERT with four\ndatasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting\nthe real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced\nmixed dataset with equal representation of both groups. The study analyzed 21\nassessment items: 10 items with about 30,000 ELL responses, five items with\nabout 1,000 ELL responses, and six items with about 200 ELL responses. Scoring\naccuracy (Acc) was calculated and compared to identify bias using Friedman\ntests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and\nthen calculated the differences in MSGs generated through both the human and AI\nmodels to identify the scoring disparities. We found that no AI bias and\ndistorted disparities between ELLs and non-ELLs were found when the training\ndataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could\nexist if the sample size is limited (ELL = 200)."}
{"id": "2505.10839", "pdf": "https://arxiv.org/pdf/2505.10839.pdf", "abs": "https://arxiv.org/abs/2505.10839", "title": "Alexandria: A Library of Pluralistic Values for Realtime Re-Ranking of Social Media Feeds", "authors": ["Akaash Kolluri", "Renn Su", "Farnaz Jahanbakhsh", "Dora Zhao", "Tiziano Piccardi", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": null, "summary": "Social media feed ranking algorithms fail when they too narrowly focus on\nengagement as their objective. The literature has asserted a wide variety of\nvalues that these algorithms should account for as well -- ranging from\nwell-being to productive discourse -- far more than can be encapsulated by a\nsingle topic or theory. In response, we present a $\\textit{library of values}$\nfor social media algorithms: a pluralistic set of 78 values as articulated\nacross the literature, implemented into LLM-powered content classifiers that\ncan be installed individually or in combination for real-time re-ranking of\nsocial media feeds. We investigate this approach by developing a browser\nextension, $\\textit{Alexandria}$, that re-ranks the X/Twitter feed in real time\nbased on the user's desired values. Through two user studies, both qualitative\n(N=12) and quantitative (N=257), we found that diverse user needs require a\nlarge library of values, enabling more nuanced preferences and greater user\ncontrol. With this work, we argue that the values criticized as missing from\nsocial media ranking algorithms can be operationalized and deployed today\nthrough end-user tools."}
{"id": "2505.10714", "pdf": "https://arxiv.org/pdf/2505.10714.pdf", "abs": "https://arxiv.org/abs/2505.10714", "title": "GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?", "authors": ["Bowen Jiang", "Yangxinyu Xie", "Xiaomeng Wang", "Jiashu He", "Joshua Bergerson", "John K Hutchison", "Jordan Branham", "Camillo J Taylor", "Tanwi Mallick"], "categories": ["cs.CL"], "comment": null, "summary": "We present GeoGrid-Bench, a benchmark designed to evaluate the ability of\nfoundation models to understand geo-spatial data in the grid structure.\nGeo-spatial datasets pose distinct challenges due to their dense numerical\nvalues, strong spatial and temporal dependencies, and unique multimodal\nrepresentations including tabular data, heatmaps, and geographic\nvisualizations. To assess how foundation models can support scientific research\nin this domain, GeoGrid-Bench features large-scale, real-world data covering 16\nclimate variables across 150 locations and extended time frames. The benchmark\nincludes approximately 3,200 question-answer pairs, systematically generated\nfrom 8 domain expert-curated templates to reflect practical tasks encountered\nby human scientists. These range from basic queries at a single location and\ntime to complex spatiotemporal comparisons across regions and periods. Our\nevaluation reveals that vision-language models perform best overall, and we\nprovide a fine-grained analysis of the strengths and limitations of different\nfoundation models in different geo-spatial tasks. This benchmark offers clearer\ninsights into how foundation models can be effectively applied to geo-spatial\ndata analysis and used to support scientific research."}
{"id": "2505.10863", "pdf": "https://arxiv.org/pdf/2505.10863.pdf", "abs": "https://arxiv.org/abs/2505.10863", "title": "Conversations With The Stressed Body: Facilitating Stress Self-Disclosure Among Adolescent Girls Through An Embodied Approach", "authors": ["Xinglin Sun", "Caroline Claisse", "Runhua Zhang", "Xinyu Wu", "Jialin Yuan", "Qi Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescent girls face significant mental health challenges during their\ntransition to adulthood, often experiencing heightened stress from various\nsources. While various interactive technologies for self-disclosure had been\nexplored to support stress relief, little is known about how to encourage\nstress-related self-disclosure through an embodied approach. This study\npresents a co-design workshop centred on Embodied Probes, a series of artefacts\nand activities incorporating embodied methods and technologies. During the\nworkshop, nine participants aged 15 to 18 engaged with their bodies, expressed\nbodily sensations through tangible means, and designed embodied prototypes\ntailored to their personal needs for stress perception and relief. The workshop\nrevealed insights into somatic symptoms, sources, and coping strategies for\nstress among adolescent girls, as well as how embodied methods can support\ntheir stress self-disclosure. This paper contributes to the HCI community by\noffering design implications on leveraging embodied technologies to support\nself-disclosure for young women's mental well-being."}
{"id": "2505.10717", "pdf": "https://arxiv.org/pdf/2505.10717.pdf", "abs": "https://arxiv.org/abs/2505.10717", "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment", "authors": ["Jean-Philippe Corbeil", "Amin Dada", "Jean-Michel Attendu", "Asma Ben Abacha", "Alessandro Sordoni", "Lucas Caccia", "FranÃ§ois Beaulieu", "Thomas Lin", "Jens Kleesiek", "Paul Vozila"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average."}
{"id": "2505.10864", "pdf": "https://arxiv.org/pdf/2505.10864.pdf", "abs": "https://arxiv.org/abs/2505.10864", "title": "Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign Sensing with Physically Realizable Wearable Oscillators", "authors": ["Md Farhan Tasnim Oshim", "Nigel Doering", "Bashima Islam", "Tsui-Wei Weng", "Tauhidur Rahman"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Recent advancements in Ultra-Wideband (UWB) radar technology have enabled\ncontactless, non-line-of-sight vital sign monitoring, making it a valuable tool\nfor healthcare. However, UWB radar's ability to capture sensitive physiological\ndata, even through walls, raises significant privacy concerns, particularly in\nhuman-robot interactions and autonomous systems that rely on radar for sensing\nhuman presence and physiological functions. In this paper, we present\nAnti-Sensing, a novel defense mechanism designed to prevent unauthorized\nradar-based sensing. Our approach introduces physically realizable\nperturbations, such as oscillatory motion from wearable devices, to disrupt\nradar sensing by mimicking natural cardiac motion, thereby misleading heart\nrate (HR) estimations. We develop a gradient-based algorithm to optimize the\nfrequency and spatial amplitude of these oscillations for maximal disruption\nwhile ensuring physiological plausibility. Through both simulations and\nreal-world experiments with radar data and neural network-based HR sensing\nmodels, we demonstrate the effectiveness of Anti-Sensing in significantly\ndegrading model accuracy, offering a practical solution for privacy\npreservation."}
{"id": "2505.10718", "pdf": "https://arxiv.org/pdf/2505.10718.pdf", "abs": "https://arxiv.org/abs/2505.10718", "title": "AI-enhanced semantic feature norms for 786 concepts", "authors": ["Siddharth Suresh", "Kushin Mukherjee", "Tyler Giallanza", "Xizheng Yu", "Mia Patil", "Jonathan D. Cohen", "Timothy T. Rogers"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures", "summary": "Semantic feature norms have been foundational in the study of human\nconceptual knowledge, yet traditional methods face trade-offs between\nconcept/feature coverage and verifiability of quality due to the\nlabor-intensive nature of norming studies. Here, we introduce a novel approach\nthat augments a dataset of human-generated feature norms with responses from\nlarge language models (LLMs) while verifying the quality of norms against\nreliable human judgments. We find that our AI-enhanced feature norm dataset,\nNOVA: Norms Optimized Via AI, shows much higher feature density and overlap\namong concepts while outperforming a comparable human-only norm dataset and\nword-embedding models in predicting people's semantic similarity judgments.\nTaken together, we demonstrate that human conceptual knowledge is richer than\ncaptured in previous norm datasets and show that, with proper validation, LLMs\ncan serve as powerful tools for cognitive science research."}
{"id": "2505.11056", "pdf": "https://arxiv.org/pdf/2505.11056.pdf", "abs": "https://arxiv.org/abs/2505.11056", "title": "Empowering the Teaching and Learning of Geometry in Basic Education by Combining Extended Reality and Machine Learning", "authors": ["Carlos R. Cunha", "AndrÃ© Moreira", "SÃ­lvia Coelho", "VÃ­tor MendonÃ§a", "JoÃ£o Pedro Gomes"], "categories": ["cs.HC"], "comment": "C. R. Cunha, A. Moreira, S. Coelho, V. Mendon\\c{c}a, and J. P. Gomes,\n  'Empowering the Teaching and Learning of Geometry in Basic Education by\n  Combining Extended Reality and Machine Learning', in Good Practices and New\n  Perspectives in Information Systems and Technologies, 2024, pp. 98-109", "summary": "Technology has helped to innovate in the teaching-learning process. Today's\nstudents are more demanding actors when it comes to the environment, they have\nat their disposal to learn, experiment and develop critical thinking. The area\nof mathematics has successively suffered from students' learning difficulties,\nwhether due to lack of motivation, low abstraction ability, or lack of new\ntools for teachers to bring innovation into the classroom and outside it. While\nit is true that digitalization has entered schools, it often follows a process\nof digital replication of approaches and materials that were previously only\navailable on physical media. This work focuses on the use of Extended Realities\nfor teaching mathematics, and very particularly in the teaching of geometry,\nwith a proposition of a conceptual model that combines the use of Extended\nReality and Machine Learning. The proposed model was subject to prototyping,\nwhich is presented as a form of laboratory validation as a contribution to\ninnovate the way in which the geometry teaching-learning process is developed,\nas well as through the ability to obtain useful insights for teachers and\nstudents throughout the process."}
{"id": "2505.10719", "pdf": "https://arxiv.org/pdf/2505.10719.pdf", "abs": "https://arxiv.org/abs/2505.10719", "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models", "authors": ["TomÃ¡s Vergara-Browne", "Ãlvaro Soto"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Motivated by the surge of large language models, there has been a push to\nformally characterize the symbolic abilities intrinsic to the transformer\narchitecture. A programming language, called RASP, has been proposed, which can\nbe directly compiled into transformer weights to implement these algorithms.\nHowever, the tasks that can be implemented in RASP are often uncommon to learn\nfrom natural unsupervised data, showing a mismatch between theoretical\ncapabilities of the transformer architecture, and the practical learnability of\nthese capabilities from unsupervised data. We propose tracr-injection, a method\nthat allows us to distill algorithms written in RASP directly into a\npre-trained language model. We showcase our method by injecting 3 different\nalgorithms into a language model. We show how our method creates an\ninterpretable subspace within the model's residual stream, which can be decoded\ninto the variables present in the code of the RASP algorithm. Additionally, we\nfound that the proposed method can improve out of distribution performance\ncompared to our baseline, indicating that indeed a more symbolic mechanism is\ntaking place in the inner workings of the model. We release the code used to\nrun our experiments."}
{"id": "2505.11162", "pdf": "https://arxiv.org/pdf/2505.11162.pdf", "abs": "https://arxiv.org/abs/2505.11162", "title": "Sliding Speed Influences Electrovibration-Induced Finger Friction Dynamics on Touchscreens", "authors": ["Jagan K Balasubramanian", "Daan M Pool", "Yasemin Vardar"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "19 pages, 13 figures, journal", "summary": "Electrovibration technology can render tactile textures on capacitive\ntouchscreens by modulating friction between the finger and the screen through\nelectrostatic attraction force generated by applying an alternating voltage\nsignal to the screen. This signal should be carefully calibrated for realistic\nand robust texture rendering. However, this process is challenging due to\nvariations in sliding speed, applied force, and individual skin mechanics,\nwhich affect friction in complex and unpredictable ways. Here, we investigate\nhow exploration conditions affect electrovibration-induced finger friction on\ntouchscreens and the role of skin mechanics in this process. Ten participants\nslid their index fingers across an electrovibration-enabled touchscreen at five\nsliding speeds ($20\\sim100$ mm/s) and applied force levels ($0.2\\sim0.6$ N)\nwhile we measured contact forces and skin accelerations. The touchscreen was\nexcited with amplitude-modulated voltage signals across frequencies relevant to\ntouch. We modeled the finger-touchscreen friction response as a first-order\nsystem and the skin mechanics as a mass-spring-damper system. Our results\nshowed that the sliding speed influenced the cutoff frequency of the friction\nresponse as well as the moving mass and stiffness of the finger for the tested\nexploration ranges. Specifically, for every 1 mm/s increase in speed, the\ncutoff frequency, the finger moving mass, and stiffness increased by $13.8$ Hz,\n$3.23\\times 10^{-5}$ kg, and $4.04$ N/m, respectively. Further correlation\nanalysis revealed that finger stiffness affected the cutoff frequency more than\nthe moving mass. Finally, we developed a practical model for\nelectrovibration-induced finger friction on touchscreens that accounts for\nsliding speed variations, paving the way for delivering consistent haptic\nfeedback through electrovibration."}
{"id": "2505.10736", "pdf": "https://arxiv.org/pdf/2505.10736.pdf", "abs": "https://arxiv.org/abs/2505.10736", "title": "Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization", "authors": ["Ximing Dong", "Shaowei Wang", "Dayi Lin", "Ahmed E. Hassan"], "categories": ["cs.CL"], "comment": null, "summary": "Optimizing Large Language Model (LLM) performance requires well-crafted\nprompts, but manual prompt engineering is labor-intensive and often\nineffective. Automated prompt optimization techniques address this challenge\nbut the majority of them rely on randomly selected evaluation subsets, which\nfail to represent the full dataset, leading to unreliable evaluations and\nsuboptimal prompts. Existing coreset selection methods, designed for LLM\nbenchmarking, are unsuitable for prompt optimization due to challenges in\nclustering similar samples, high data collection costs, and the unavailability\nof performance data for new or private datasets. To overcome these issues, we\npropose IPOMP, an Iterative evaluation data selection for effective Prompt\nOptimization using real-time Model Performance. IPOMP is a two-stage approach\nthat selects representative and diverse samples using semantic clustering and\nboundary analysis, followed by iterative refinement with real-time model\nperformance data to replace redundant samples. Evaluations on the BIG-bench\ndataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by\nat least 57% compared with SOTA baselines, with minimal computational overhead\nbelow 1%. Furthermore, the results demonstrate that our real-time\nperformance-guided refinement approach can be universally applied to enhance\nexisting coreset selection methods."}
{"id": "2505.11406", "pdf": "https://arxiv.org/pdf/2505.11406.pdf", "abs": "https://arxiv.org/abs/2505.11406", "title": "Large Language Model Use Impact Locus of Control", "authors": ["Jenny Xiyu Fu", "Brennan Antone", "Kowe Kadoma", "Malte Jung"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity."}
{"id": "2505.10740", "pdf": "https://arxiv.org/pdf/2505.10740.pdf", "abs": "https://arxiv.org/abs/2505.10740", "title": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "authors": ["Qiwei Peng", "Robert Moro", "Michal Gregor", "Ivan Srba", "Simon Ostermann", "Marian Simko", "Juraj PodrouÅ¾ek", "MatÃºÅ¡ MesarÄÃ­k", "Jaroslav KopÄan", "Anders SÃ¸gaard"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The rapid spread of online disinformation presents a global challenge, and\nmachine learning has been widely explored as a potential solution. However,\nmultilingual settings and low-resource languages are often neglected in this\nfield. To address this gap, we conducted a shared task on multilingual claim\nretrieval at SemEval 2025, aimed at identifying fact-checked claims that match\nnewly encountered claims expressed in social media posts across different\nlanguages. The task includes two subtracks: (1) a monolingual track, where\nsocial posts and claims are in the same language, and (2) a crosslingual track,\nwhere social posts and claims might be in different languages. A total of 179\nparticipants registered for the task contributing to 52 test submissions. 23\nout of 31 teams have submitted their system papers. In this paper, we report\nthe best-performing systems as well as the most common and the most effective\napproaches across both subtracks. This shared task, along with its dataset and\nparticipating systems, provides valuable insights into multilingual claim\nretrieval and automated fact-checking, supporting future research in this\nfield."}
{"id": "2505.11417", "pdf": "https://arxiv.org/pdf/2505.11417.pdf", "abs": "https://arxiv.org/abs/2505.11417", "title": "EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions", "authors": ["Patryk Bartkowiak", "Michal Podstawski"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces a novel dataset and evaluation benchmark designed to\nassess and improve small language models deployable on edge devices, with a\nfocus on user profiling from multi-session natural language interactions in\nsmart home environments. At the core of the dataset are structured user\nprofiles, each defined by a set of routines - context-triggered, repeatable\npatterns of behavior that govern how users interact with their home systems.\nUsing these profiles as input, a large language model (LLM) generates\ncorresponding interaction sessions that simulate realistic, diverse, and\ncontext-aware dialogues between users and their devices.\n  The primary task supported by this dataset is profile reconstruction:\ninferring user routines and preferences solely from interactions history. To\nassess how well current models can perform this task under realistic\nconditions, we benchmarked several state-of-the-art compact language models and\ncompared their performance against large foundation models. Our results show\nthat while small models demonstrate some capability in reconstructing profiles,\nthey still fall significantly short of large models in accurately capturing\nuser behavior. This performance gap poses a major challenge - particularly\nbecause on-device processing offers critical advantages, such as preserving\nuser privacy, minimizing latency, and enabling personalized experiences without\nreliance on the cloud. By providing a realistic, structured testbed for\ndeveloping and evaluating behavioral modeling under these constraints, our\ndataset represents a key step toward enabling intelligent, privacy-respecting\nAI systems that learn and adapt directly on user-owned devices."}
{"id": "2505.10772", "pdf": "https://arxiv.org/pdf/2505.10772.pdf", "abs": "https://arxiv.org/abs/2505.10772", "title": "Ranked Voting based Self-Consistency of Large Language Models", "authors": ["Weiqin Wang", "Yile Wang", "Hui Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Majority voting is considered an effective method to enhance chain-of-thought\nreasoning, as it selects the answer with the highest \"self-consistency\" among\ndifferent reasoning paths (Wang et al., 2023). However, previous\nchain-of-thought reasoning methods typically generate only a single answer in\neach trial, thereby ignoring the possibility of other potential answers. As a\nresult, these alternative answers are often overlooked in subsequent voting\nprocesses. In this work, we propose to generate ranked answers in each\nreasoning process and conduct ranked voting among multiple ranked answers from\ndifferent responses, thereby making the overall self-consistency more reliable.\nSpecifically, we use three ranked voting methods: Instant-runoff voting, Borda\ncount voting, and mean reciprocal rank voting. We validate our methods on six\ndatasets, including three multiple-choice and three open-ended\nquestion-answering tasks, using both advanced open-source and closed-source\nlarge language models. Extensive experimental results indicate that our\nproposed method outperforms the baselines, showcasing the potential of\nleveraging the information of ranked answers and using ranked voting to improve\nreasoning performance. The code is available at\nhttps://github.com/szu-tera/RankedVotingSC."}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472.pdf", "abs": "https://arxiv.org/abs/2505.10472", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools."}
{"id": "2505.10775", "pdf": "https://arxiv.org/pdf/2505.10775.pdf", "abs": "https://arxiv.org/abs/2505.10775", "title": "A Systematic Analysis of Base Model Choice for Reward Modeling", "authors": ["Kian Ahrabian", "Pegah Jandaghi", "Negar Mokhberian", "Sai Praneeth Karimireddy", "Jay Pujara"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 13 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward\nmodeling have become a crucial part of training powerful large language models\n(LLMs). One commonly overlooked factor in training high-quality reward models\n(RMs) is the effect of the base model, which is becoming more challenging to\nchoose given the rapidly growing pool of LLMs. In this work, we present a\nsystematic analysis of the effect of base model selection on reward modeling\nperformance. Our results show that the performance can be improved by up to 14%\ncompared to the most common (i.e., default) choice. Moreover, we showcase the\nstrong statistical relation between some existing benchmarks and downstream\nperformances. We also demonstrate that the results from a small set of\nbenchmarks could be combined to boost the model selection ($+$18% on average in\nthe top 5-10). Lastly, we illustrate the impact of different post-training\nsteps on the final performance and explore using estimated data distributions\nto reduce performance prediction error."}
{"id": "2505.10588", "pdf": "https://arxiv.org/pdf/2505.10588.pdf", "abs": "https://arxiv.org/abs/2505.10588", "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation", "authors": ["Manisha Mehta", "Fausto Giunchiglia"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "I.2; I.2.7; K.4.2"], "comment": "Accepted to ACM FAccT 2025. To be presented in Athens, June 2025, and\n  published in the conference proceedings. Preprint version; final version will\n  appear in the ACM Digital Library", "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges."}
{"id": "2505.10792", "pdf": "https://arxiv.org/pdf/2505.10792.pdf", "abs": "https://arxiv.org/abs/2505.10792", "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "authors": ["Zhan Peng Lee", "Andre Lin", "Calvin Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\nimprove factuality in large language models (LLMs) by grounding their outputs\nin retrieved documents. However, ensuring perfect retrieval of relevant\ninformation remains challenging, and when irrelevant content is passed\ndownstream to an LLM, it can lead to hallucinations. In this work, we propose\nFinetune-RAG, a simple and effective fine-tuning approach that features the\nfirst-of-its-kind RAG training dataset constructed to mimic real-world\nimperfections. Experimental results show that Finetune-RAG improves factual\naccuracy by 21.2% over the base model. We also propose a Bench-RAG, an\nLLM-as-a-judge evaluation pipeline that stress tests models under realistic\nimperfect retrieval scenarios. Our codebase and dataset are fully open sourced\nfor community use."}
{"id": "2505.10681", "pdf": "https://arxiv.org/pdf/2505.10681.pdf", "abs": "https://arxiv.org/abs/2505.10681", "title": "Towards an LLM-powered Social Digital Twinning Platform", "authors": ["Ãnder GÃ¼rcan", "Vanja Falck", "Markus G. Rousseau", "Larissa L. Lima"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "13 pages, 3 figures, 23rd International Conference on Practical\n  applications of Agents and Multi-Agent Systems (PAAMS 2025)", "summary": "We present Social Digital Twinner, an innovative social simulation tool for\nexploring plausible effects of what-if scenarios in complex adaptive social\nsystems. The architecture is composed of three seamlessly integrated parts: a\ndata infrastructure featuring real-world data and a multi-dimensionally\nrepresentative synthetic population of citizens, an LLM-enabled agent-based\nsimulation engine, and a user interface that enable intuitive, natural language\ninteractions with the simulation engine and the artificial agents (i.e.\ncitizens). Social Digital Twinner facilitates real-time engagement and empowers\nstakeholders to collaboratively design, test, and refine intervention measures.\nThe approach is promoting a data-driven and evidence-based approach to societal\nproblem-solving. We demonstrate the tool's interactive capabilities by\naddressing the critical issue of youth school dropouts in Kragero, Norway,\nshowcasing its ability to create and execute a dedicated social digital twin\nusing natural language."}
{"id": "2505.10798", "pdf": "https://arxiv.org/pdf/2505.10798.pdf", "abs": "https://arxiv.org/abs/2505.10798", "title": "Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets", "authors": ["Erica Cai", "Sean McQuade", "Kevin Young", "Brendan O'Connor"], "categories": ["cs.CL"], "comment": null, "summary": "When knowledge graphs (KGs) are automatically extracted from text, are they\naccurate enough for downstream analysis? Unfortunately, current annotated\ndatasets can not be used to evaluate this question, since their KGs are highly\ndisconnected, too small, or overly complex. To address this gap, we introduce\nAffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six\ndatasets that are the first to pair complete book scans with large, labeled\nknowledge graphs. Each dataset features affiliation graphs, which are simple\nKGs that capture Member relationships between Person and Organization entities\n-- useful in studies of migration, community interactions, and other social\nphenomena. In addition, three datasets include expanded KGs with a wider\nvariety of relation types. Our preliminary experiments demonstrate significant\nvariability in model performance across datasets, underscoring AffilKG's\nability to enable two critical advances: (1) benchmarking how extraction errors\npropagate to graph-level analyses (e.g., community structure), and (2)\nvalidating KG extraction methods for real-world social science research."}
{"id": "2505.10695", "pdf": "https://arxiv.org/pdf/2505.10695.pdf", "abs": "https://arxiv.org/abs/2505.10695", "title": "Predicting Human Behavior in Autonomous Systems: A Collaborative Machine Teaching Approach for Reducing Transfer of Control Events", "authors": ["Julian Wolter", "Amr Gomaa"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "As autonomous systems become integral to various industries, effective\nstrategies for fault handling are essential to ensure reliability and\nefficiency. Transfer of Control (ToC), a traditional approach for interrupting\nautomated processes during faults, is often triggered unnecessarily in\nnon-critical situations. To address this, we propose a data-driven method that\nuses human interaction data to train AI models capable of preemptively\nidentifying and addressing issues or assisting users in resolution. Using an\ninteractive tool simulating an industrial vacuum cleaner, we collected data and\ndeveloped an LSTM-based model to predict user behavior. Our findings reveal\nthat even data from non-experts can effectively train models to reduce\nunnecessary ToC events, enhancing the system's robustness. This approach\nhighlights the potential of AI to learn directly from human problem-solving\nbehaviors, complementing sensor data to improve industrial automation and\nhuman-AI collaboration."}
{"id": "2505.10829", "pdf": "https://arxiv.org/pdf/2505.10829.pdf", "abs": "https://arxiv.org/abs/2505.10829", "title": "Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances", "authors": ["Chen-Chi Chang", "Chong-Fu Li", "Chu-Hsuan Lee", "Hung-Shin Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IntelliSys 2025", "summary": "This study investigates the challenges of translating low-resource languages\nby integrating Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG). Various model configurations were tested on Hakka translations, with\nBLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).\nThe best-performing model (Model 4) combined retrieval and advanced language\nmodeling, improving lexical coverage, particularly for specialized or\nculturally nuanced terms, and enhancing grammatical coherence. A two-stage\nmethod (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU\nscore of 26%, highlighting iterative correction's value and the challenges of\ndomain-specific expressions. Static dictionary-based approaches struggled with\ncontext-sensitive content, demonstrating the limitations of relying solely on\npredefined resources. These results emphasize the need for curated resources,\ndomain knowledge, and ethical collaboration with local communities, offering a\nframework that improves translation accuracy and fluency while supporting\ncultural preservation."}
{"id": "2505.10718", "pdf": "https://arxiv.org/pdf/2505.10718.pdf", "abs": "https://arxiv.org/abs/2505.10718", "title": "AI-enhanced semantic feature norms for 786 concepts", "authors": ["Siddharth Suresh", "Kushin Mukherjee", "Tyler Giallanza", "Xizheng Yu", "Mia Patil", "Jonathan D. Cohen", "Timothy T. Rogers"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures", "summary": "Semantic feature norms have been foundational in the study of human\nconceptual knowledge, yet traditional methods face trade-offs between\nconcept/feature coverage and verifiability of quality due to the\nlabor-intensive nature of norming studies. Here, we introduce a novel approach\nthat augments a dataset of human-generated feature norms with responses from\nlarge language models (LLMs) while verifying the quality of norms against\nreliable human judgments. We find that our AI-enhanced feature norm dataset,\nNOVA: Norms Optimized Via AI, shows much higher feature density and overlap\namong concepts while outperforming a comparable human-only norm dataset and\nword-embedding models in predicting people's semantic similarity judgments.\nTaken together, we demonstrate that human conceptual knowledge is richer than\ncaptured in previous norm datasets and show that, with proper validation, LLMs\ncan serve as powerful tools for cognitive science research."}
{"id": "2505.10832", "pdf": "https://arxiv.org/pdf/2505.10832.pdf", "abs": "https://arxiv.org/abs/2505.10832", "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "authors": ["Songjun Tu", "Jiahao Lin", "Qichao Zhang", "Xiangyu Tian", "Linjing Li", "Xiangyuan Lan", "Dongbin Zhao"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Project Page: https://github.com/TU2021/AutoThink", "summary": "Large reasoning models (LRMs) are proficient at generating explicit,\nstep-by-step reasoning sequences before producing final answers. However, such\ndetailed reasoning can introduce substantial computational overhead and\nlatency, particularly for simple problems. To address this over-thinking\nproblem, we explore how to equip LRMs with adaptive thinking capabilities:\nenabling them to dynamically decide whether or not to engage in explicit\nreasoning based on problem complexity. Building on R1-style distilled models,\nwe observe that inserting a simple ellipsis (\"...\") into the prompt can\nstochastically trigger either a thinking or no-thinking mode, revealing a\nlatent controllability in the reasoning behavior. Leveraging this property, we\npropose AutoThink, a multi-stage reinforcement learning (RL) framework that\nprogressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while\ndefaulting to succinct responses for simpler tasks. Experiments on five\nmainstream mathematical benchmarks demonstrate that AutoThink achieves\nfavorable accuracy-efficiency trade-offs compared to recent prompting and\nRL-based pruning methods. It can be seamlessly integrated into any R1-style\nmodel, including both distilled and further fine-tuned variants. Notably,\nAutoThink improves relative accuracy by 6.4 percent while reducing token usage\nby 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and\nadaptive reasoning paradigm for LRMs."}
{"id": "2505.10742", "pdf": "https://arxiv.org/pdf/2505.10742.pdf", "abs": "https://arxiv.org/abs/2505.10742", "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use", "authors": ["Brandon Lepine", "Gawesha Weerantunga", "Juho Kim", "Pamela Mishkin", "Matthew Beane"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes."}
{"id": "2505.10836", "pdf": "https://arxiv.org/pdf/2505.10836.pdf", "abs": "https://arxiv.org/abs/2505.10836", "title": "Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs", "authors": ["Abhishek Dey", "Aabha Bothera", "Samhita Sarikonda", "Rishav Aryan", "Sanjay Kumar Podishetty", "Akshay Havalgi", "Gaurav Singh", "Saurabh Srivastava"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at NLDB 2025", "summary": "In this paper, we study the challenges of detecting events on social media,\nwhere traditional unimodal systems struggle due to the rapid and multimodal\nnature of data dissemination. We employ a range of models, including unimodal\nModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced\ngenerative models like GPT-4o, and LLaVA. Additionally, we also study the\neffect of providing multimodal generative models (such as GPT-4o) with a single\nmodality to assess their efficacy. Our results indicate that while multimodal\napproaches notably outperform unimodal counterparts, generative approaches\ndespite having a large number of parameters, lag behind supervised methods in\nprecision. Furthermore, we also found that they lag behind instruction-tuned\nmodels because of their inability to generate event classes correctly. During\nour error analysis, we discovered that common social media issues such as leet\nspeak, text elongation, etc. are effectively handled by generative approaches\nbut are hard to tackle using supervised approaches."}
{"id": "2505.10786", "pdf": "https://arxiv.org/pdf/2505.10786.pdf", "abs": "https://arxiv.org/abs/2505.10786", "title": "Bridging BCI and Communications: A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling", "authors": ["Jiaheng Wang", "Zhenyu Wang", "Tianheng Xu", "Yuan Si", "Ang Li", "Ting Zhou", "Xi Zhao", "Honglin Hu"], "categories": ["eess.SP", "cs.HC"], "comment": null, "summary": "As a method to connect human brain and external devices, Brain-computer\ninterfaces (BCIs) are receiving extensive research attention. Recently, the\nintegration of communication theory with BCI has emerged as a popular trend,\noffering potential to enhance system performance and shape next-generation\ncommunications.\n  A key challenge in this field is modeling the brain wireless communication\nchannel between intracranial electrocorticography (ECoG) emitting neurons and\nextracranial electroencephalography (EEG) receiving electrodes. However, the\ncomplex physiology of brain challenges the application of traditional channel\nmodeling methods, leaving relevant research in its infancy. To address this\ngap, we propose a frequency-division multiple-input multiple-output (MIMO)\nestimation framework leveraging simultaneous macaque EEG and ECoG recordings,\nwhile employing neurophysiology-informed regularization to suppress noise\ninterference. This approach reveals profound similarities between neural signal\npropagation and multi-antenna communication systems. Experimental results show\nimproved estimation accuracy over conventional methods while highlighting a\ntrade-off between frequency resolution and temporal stability determined by\nsignal duration. This work establish a conceptual bridge between neural\ninterfacing and communication theory, accelerating synergistic developments in\nboth fields."}
{"id": "2505.10862", "pdf": "https://arxiv.org/pdf/2505.10862.pdf", "abs": "https://arxiv.org/abs/2505.10862", "title": "Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?", "authors": ["Tairan Fu", "Miguel GonzÃ¡lez", "Javier Conde", "Elena Merino-GÃ³mez", "Pedro Reviriego"], "categories": ["cs.CL", "I.2.7"], "comment": "6 pages, 5 figures, 2 tables", "summary": "Multimodal Large Language Models which can answer complex questions on an\nimage struggle to tell the time on analog clocks. This is probably due to the\nlack of images with clocks at different times in their training set. In this\nwork we explore this issue with one of the latest MLLMs: GPT-4.1 to understand\nwhy MLLMs fail to tell the time and whether fine-tuning can solve the problem.\nThe results show how models are making progress in reading the time on analog\nclocks. But have they really learned to do it, or have they only learned\npatterns in their training datasets? In this work we put the models to the test\nwith different clocks to illustrate the limitations of MLLMs to abstract and\ngeneralize."}
{"id": "2505.10816", "pdf": "https://arxiv.org/pdf/2505.10816.pdf", "abs": "https://arxiv.org/abs/2505.10816", "title": "mmMirror: Device Free mmWave Indoor NLoS Localization Using Van-Atta-Array IRS", "authors": ["Yihe Yan", "Zhenguo Shi", "Yanxiang Wang", "Cheng Jiang", "Chun Tung Chou", "Wen Hu"], "categories": ["cs.NI", "cs.HC", "cs.RO"], "comment": null, "summary": "Industry 4.0 is transforming manufacturing and logistics by integrating\nrobots into shared human environments, such as factories, warehouses, and\nhealthcare facilities. However, the risk of human-robot collisions, especially\nin Non-Line-of-Sight (NLoS) scenarios like around corners, remains a critical\nchallenge. Existing solutions, such as vision-based and LiDAR systems, often\nfail under occlusion, lighting constraints, or privacy concerns, while RF-based\nsystems are limited by range and accuracy.\n  To address these limitations, we propose mmMirror, a novel system leveraging\na Van Atta Array-based millimeter-wave (mmWave) reconfigurable intelligent\nreflecting surface (IRS) for precise, device-free NLoS localization. mmMirror\nintegrates seamlessly with existing frequency-modulated continuous-wave (FMCW)\nradars and offers: (i) robust NLoS localization with centimeter-level accuracy\nat ranges up to 3 m, (ii) seamless uplink and downlink communication between\nradar and IRS, (iii) support for multi-radar and multi-target scenarios via\ndynamic beam steering, and (iv) reduced scanning latency through adaptive time\nslot allocation. Implemented using commodity 24 GHz radars and a PCB-based IRS\nprototype, mmMirror demonstrates its potential in enabling safe human-robot\ninteractions in dynamic and complex environments."}
{"id": "2505.10870", "pdf": "https://arxiv.org/pdf/2505.10870.pdf", "abs": "https://arxiv.org/abs/2505.10870", "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate", "authors": ["Ziyang Huang", "Wangtao Sun", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "This paper systematically addresses the challenges of rule retrieval, a\ncrucial yet underexplored area. Vanilla retrieval methods using sparse or dense\nretrievers to directly search for relevant rules to support downstream\nreasoning, often suffer from low accuracy. This is primarily due to a\nsignificant semantic gap between the instantiated facts in the queries and the\nabstract representations of the rules. Such misalignment results in suboptimal\nretrieval quality, which in turn negatively impacts reasoning performance. To\novercome these challenges, we propose Self-Induction Augmented Retrieval\n(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce\npotential inferential rules that might offer benefits for reasoning by\nabstracting the underlying knowledge and logical structure in queries. These\ninduced rules are then used for query augmentation to improve retrieval\neffectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a\nmethod that re-estimates the relevance of retrieved rules by assessing whether\nthe abstract knowledge they contain can be instantiated to align with the facts\nin the queries and the helpfulness for reasoning. Extensive experiments across\nvarious settings demonstrate the effectiveness and versatility of our proposed\nmethods."}
{"id": "2505.10869", "pdf": "https://arxiv.org/pdf/2505.10869.pdf", "abs": "https://arxiv.org/abs/2505.10869", "title": "A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination", "authors": ["Go Fukino", "Kanta Tachibana"], "categories": ["cs.CV", "cs.HC"], "comment": "7 pages, 13 figures, 3 tables", "summary": "This study focuses on the velocity patterns of various body parts during\nwalking and proposes a method for evaluating gait symmetry. Traditional motion\nanalysis studies have assessed gait symmetry based on differences in\nelectromyographic (EMG) signals or acceleration between the left and right\nsides. In contrast, this paper models intersegmental coordination using an LTI\nsystem and proposes a dissimilarity metric to evaluate symmetry. The method was\ntested on five subjects with both symmetric and asymmetric gait."}
{"id": "2505.10924", "pdf": "https://arxiv.org/pdf/2505.10924.pdf", "abs": "https://arxiv.org/abs/2505.10924", "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?", "authors": ["Ada Chen", "Yongjiang Wu", "Junyuan Zhang", "Shu Yang", "Jen-tse Huang", "Kun Wang", "Wenxuan Wang", "Shuai Wang"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.SE"], "comment": null, "summary": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents."}
{"id": "2505.10902", "pdf": "https://arxiv.org/pdf/2505.10902.pdf", "abs": "https://arxiv.org/abs/2505.10902", "title": "Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Rong Wang", "Li Zhang"], "categories": ["cs.CV", "cs.HC", "92C50", "I.3.8; I.6.8"], "comment": "34 pages, 24 figures", "summary": "Background and Objective: Precise preoperative planning and effective\nphysician training for coronary interventions are increasingly important.\nDespite advances in medical imaging technologies, transforming static or\nlimited dynamic imaging data into comprehensive dynamic cardiac models remains\nchallenging. Existing training systems lack accurate simulation of cardiac\nphysiological dynamics. This study develops a comprehensive dynamic cardiac\nmodel research framework based on 4D-CTA, integrating digital twin technology,\ncomputer vision, and physical model manufacturing to provide precise,\npersonalized tools for interventional cardiology. Methods: Using 4D-CTA data\nfrom a 60-year-old female with three-vessel coronary stenosis, we segmented\ncardiac chambers and coronary arteries, constructed dynamic models, and\nimplemented skeletal skinning weight computation to simulate vessel deformation\nacross 20 cardiac phases. Transparent vascular physical models were\nmanufactured using medical-grade silicone. We developed cardiac output analysis\nand virtual angiography systems, implemented guidewire 3D reconstruction using\nbinocular stereo vision, and evaluated the system through angiography\nvalidation and CABG training applications. Results: Morphological consistency\nbetween virtual and real angiography reached 80.9%. Dice similarity\ncoefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory\nerrors below 1.1 mm. The transparent model demonstrated advantages in CABG\ntraining, allowing direct visualization while simulating beating heart\nchallenges. Conclusion: Our patient-specific digital-physical twin approach\neffectively reproduces both anatomical structures and dynamic characteristics\nof coronary vasculature, offering a dynamic environment with visual and tactile\nfeedback valuable for education and clinical planning."}
{"id": "2505.10936", "pdf": "https://arxiv.org/pdf/2505.10936.pdf", "abs": "https://arxiv.org/abs/2505.10936", "title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "categories": ["cs.CL"], "comment": "34 pages, 20 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nexecuting complex reasoning tasks. Chain-of-thought effectively enhances\nreasoning capabilities by unlocking the potential of large models, while\nmulti-agent systems provide more comprehensive solutions by integrating\ncollective intelligence of multiple agents. However, both approaches face\nsignificant limitations. Single-agent with chain-of-thought, due to the\ninherent complexity of designing cross-domain prompts, faces collaboration\nchallenges. Meanwhile, multi-agent systems consume substantial tokens and\ninevitably dilute the primary problem, which is particularly problematic in\nbusiness workflow tasks. To address these challenges, we propose Cochain, a\ncollaboration prompting framework that effectively solves business workflow\ncollaboration problem by combining knowledge and prompts at a reduced cost.\nSpecifically, we construct an integrated knowledge graph that incorporates\nknowledge from multiple stages. Furthermore, by maintaining and retrieving a\nprompts tree, we can obtain prompt information relevant to other stages of the\nbusiness workflow. We perform extensive evaluations of Cochain across multiple\ndatasets, demonstrating that Cochain outperforms all baselines in both prompt\nengineering and multi-agent LLMs. Additionally, expert evaluation results\nindicate that the use of a small model in combination with Cochain outperforms\nGPT-4."}
{"id": "2505.10954", "pdf": "https://arxiv.org/pdf/2505.10954.pdf", "abs": "https://arxiv.org/abs/2505.10954", "title": "Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design", "authors": ["Koki Iwai", "Yusuke Kumagae", "Yuki Koyama", "Masahiro Hamasaki", "Masataka Goto"], "categories": ["cs.LG", "cs.AI", "cs.GR", "cs.HC"], "comment": "17 pages, 15 figures", "summary": "Preferential Bayesian optimization (PBO) is a variant of Bayesian\noptimization that observes relative preferences (e.g., pairwise comparisons)\ninstead of direct objective values, making it especially suitable for\nhuman-in-the-loop scenarios. However, real-world optimization tasks often\ninvolve inequality constraints, which existing PBO methods have not yet\naddressed. To fill this gap, we propose constrained preferential Bayesian\noptimization (CPBO), an extension of PBO that incorporates inequality\nconstraints for the first time. Specifically, we present a novel acquisition\nfunction for this purpose. Our technical evaluation shows that our CPBO method\nsuccessfully identifies optimal solutions by focusing on exploring feasible\nregions. As a practical application, we also present a designer-in-the-loop\nsystem for banner ad design using CPBO, where the objective is the designer's\nsubjective preference, and the constraint ensures a target predicted\nclick-through rate. We conducted a user study with professional ad designers,\ndemonstrating the potential benefits of our approach in guiding creative design\nunder real-world constraints."}
{"id": "2505.10937", "pdf": "https://arxiv.org/pdf/2505.10937.pdf", "abs": "https://arxiv.org/abs/2505.10937", "title": "Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of large reasoning models (LRMs) has transformed Natural\nLanguage Processing by excelling in complex tasks such as mathematical\nproblem-solving and code generation. These models leverage chain-of-thought\n(CoT) processes, enabling them to emulate human-like reasoning strategies.\nHowever, the advancement of LRMs is hindered by the lack of comprehensive CoT\ndatasets. Current resources often fail to provide extensive reasoning problems\nwith coherent CoT processes distilled from multiple teacher models and do not\naccount for multifaceted properties describing the internal characteristics of\nCoTs. To address these challenges, we introduce OmniThought, a large-scale\ndataset featuring 2 million CoT processes generated and validated by two\npowerful LRMs as teacher models. Each CoT process in OmniThought is annotated\nwith novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which\ndescribe the appropriateness of CoT verbosity and cognitive difficulty level\nfor models to comprehend these reasoning processes. We further establish a\nself-reliant pipeline to curate this dataset. Extensive experiments using\nQwen2.5 models of various sizes demonstrate the positive impact of our proposed\nscores on LRM training effectiveness. Based on the proposed OmniThought\ndataset, we further train and release a series of high-performing LRMs,\nspecifically equipped with stronger reasoning abilities and optimal CoT output\nlength and difficulty level. Our contributions significantly enhance the\ndevelopment and training of LRMs for solving complex tasks."}
{"id": "2505.11146", "pdf": "https://arxiv.org/pdf/2505.11146.pdf", "abs": "https://arxiv.org/abs/2505.11146", "title": "X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation", "authors": ["Peizhen Li", "Longbing Cao", "Xiao-Ming Wu", "Runze Yang", "Xiaohan Yu"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "The ability to imitate realistic facial expressions is essential for humanoid\nrobots engaged in affective human-robot communication. However, the lack of\ndatasets containing diverse humanoid facial expressions with proper annotations\nhinders progress in realistic humanoid facial expression imitation. To address\nthese challenges, we introduce X2C (Anything to Control), a dataset featuring\nnuanced facial expressions for realistic humanoid imitation. With X2C, we\ncontribute: 1) a high-quality, high-diversity, large-scale dataset comprising\n100,000 (image, control value) pairs. Each image depicts a humanoid robot\ndisplaying a diverse range of facial expressions, annotated with 30 control\nvalues representing the ground-truth expression configuration; 2) X2CNet, a\nnovel human-to-humanoid facial expression imitation framework that learns the\ncorrespondence between nuanced humanoid expressions and their underlying\ncontrol values from X2C. It enables facial expression imitation in the wild for\ndifferent human performers, providing a baseline for the imitation task,\nshowcasing the potential value of our dataset; 3) real-world demonstrations on\na physical humanoid robot, highlighting its capability to advance realistic\nhumanoid facial expression imitation. Code and Data:\nhttps://lipzh5.github.io/X2CNet/"}
{"id": "2505.10938", "pdf": "https://arxiv.org/pdf/2505.10938.pdf", "abs": "https://arxiv.org/abs/2505.10938", "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing", "authors": ["Yi Su", "Yuechi Zhou", "Quantong Qiu", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput."}
{"id": "2505.11200", "pdf": "https://arxiv.org/pdf/2505.11200.pdf", "abs": "https://arxiv.org/abs/2505.11200", "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese", "authors": ["Xihuai Wang", "Ziyi Zhao", "Siyu Ren", "Shao Zhang", "Song Li", "Xiaoyu Li", "Ziwen Wang", "Lin Qiu", "Guanglu Wan", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "comment": "Under Review", "summary": "Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4)."}
{"id": "2505.10939", "pdf": "https://arxiv.org/pdf/2505.10939.pdf", "abs": "https://arxiv.org/abs/2505.10939", "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction", "authors": ["Mohammadtaha Bagherifard", "Sahar Rajabi", "Ali Edalat", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages", "summary": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM."}
{"id": "2505.11366", "pdf": "https://arxiv.org/pdf/2505.11366.pdf", "abs": "https://arxiv.org/abs/2505.11366", "title": "Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space", "authors": ["Ali Rabiee", "Sima Ghafoori", "MH Farhadi", "Robert Beyer", "Xiangyu Bai", "David J Lin", "Sarah Ostadabbas", "Reza Abiri"], "categories": ["cs.RO", "cs.HC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Current invasive assistive technologies are designed to infer\nhigh-dimensional motor control signals from severely paralyzed patients.\nHowever, they face significant challenges, including public acceptance, limited\nlongevity, and barriers to commercialization. Meanwhile, noninvasive\nalternatives often rely on artifact-prone signals, require lengthy user\ntraining, and struggle to deliver robust high-dimensional control for dexterous\ntasks. To address these issues, this study introduces a novel human-centered\nmultimodal AI approach as intelligent compensatory mechanisms for lost motor\nfunctions that could potentially enable patients with severe paralysis to\ncontrol high-dimensional assistive devices, such as dexterous robotic arms,\nusing limited and noninvasive inputs. In contrast to the current\nstate-of-the-art (SoTA) noninvasive approaches, our context-aware, multimodal\nshared-autonomy framework integrates deep reinforcement learning algorithms to\nblend limited low-dimensional user input with real-time environmental\nperception, enabling adaptive, dynamic, and intelligent interpretation of human\nintent for complex dexterous manipulation tasks, such as pick-and-place. The\nresults from our ARAS (Adaptive Reinforcement learning for Amplification of\nlimited inputs in Shared autonomy) trained with synthetic users over 50,000\ncomputer simulation episodes demonstrated the first successful implementation\nof the proposed closed-loop human-in-the-loop paradigm, outperforming the SoTA\nshared autonomy algorithms. Following a zero-shot sim-to-real transfer, ARAS\nwas evaluated on 23 human subjects, demonstrating high accuracy in dynamic\nintent detection and smooth, stable 3D trajectory control for dexterous\npick-and-place tasks. ARAS user study achieved a high task success rate of\n92.88%, with short completion times comparable to those of SoTA invasive\nassistive technologies."}
{"id": "2505.10945", "pdf": "https://arxiv.org/pdf/2505.10945.pdf", "abs": "https://arxiv.org/abs/2505.10945", "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer", "authors": ["Seungyoon Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures."}
{"id": "2311.16027", "pdf": "https://arxiv.org/pdf/2311.16027.pdf", "abs": "https://arxiv.org/abs/2311.16027", "title": "An HCAI Methodological Framework (HCAI-MF): Putting It Into Action to Enable Human-Centered AI", "authors": ["Wei Xu", "Zaifeng Gao", "Marvin Dainoff"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human-centered artificial intelligence (HCAI) is a design philosophy that\nprioritizes humans in the design, development, deployment, and use of AI\nsystems, aiming to maximize AI's benefits while mitigating its negative\nimpacts. Despite its growing prominence in literature, the lack of\nmethodological guidance for its implementation poses challenges to HCAI\npractice. To address this gap, this paper proposes a comprehensive HCAI\nmethodological framework (HCAI-MF) comprising five key components: HCAI\nrequirement hierarchy, approach and method taxonomy, process, interdisciplinary\ncollaboration approach, and multi-level design paradigms. A case study\ndemonstrates HCAI-MF's practical implications, while the paper also analyzes\nimplementation challenges. Actionable recommendations and a \"three-layer\" HCAI\nimplementation strategy are provided to address these challenges and guide\nfuture evolution of HCAI-MF. HCAI-MF is presented as a systematic and\nexecutable methodology capable of overcoming current gaps, enabling effective\ndesign, development, deployment, and use of AI systems, and advancing HCAI\npractice."}
{"id": "2505.10948", "pdf": "https://arxiv.org/pdf/2505.10948.pdf", "abs": "https://arxiv.org/abs/2505.10948", "title": "The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs", "authors": ["Makoto Sato"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs), inspired by neuroscience, exhibit behaviors\nthat often evoke a sense of personality and intelligence-yet the mechanisms\nbehind these effects remain elusive. Here, we operationalize Conceptual\nBlending Theory (CBT) as an experimental framework, using prompt-based methods\nto reveal how LLMs blend and compress meaning. By systematically investigating\nPrompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we\nuncover structural parallels and divergences between artificial and biological\ncognition. Our approach bridges linguistics, neuroscience, and empirical AI\nresearch, demonstrating that human-AI collaboration can serve as a living\nprototype for the future of cognitive science. This work proposes prompt\nengineering not just as a technical tool, but as a scientific method for\nprobing the deep structure of meaning itself."}
{"id": "2404.13274", "pdf": "https://arxiv.org/pdf/2404.13274.pdf", "abs": "https://arxiv.org/abs/2404.13274", "title": "Augmented Object Intelligence with XR-Objects", "authors": ["Mustafa Doga Dogan", "Eric J. Gonzalez", "Karan Ahuja", "Ruofei Du", "Andrea ColaÃ§o", "Johnny Lee", "Mar Gonzalez-Franco", "David Kim"], "categories": ["cs.HC", "cs.AI", "H.5.0; H.5.1; H.5.2"], "comment": "15 pages, 15 figures, 2024 ACM Symposium on User Interface Software\n  and Technology (UIST)", "summary": "Seamless integration of physical objects as interactive digital entities\nremains a challenge for spatial computing. This paper explores Augmented Object\nIntelligence (AOI) in the context of XR, an interaction paradigm that aims to\nblur the lines between digital and physical by equipping real-world objects\nwith the ability to interact as if they were digital, where every object has\nthe potential to serve as a portal to digital functionalities. Our approach\nutilizes real-time object segmentation and classification, combined with the\npower of Multimodal Large Language Models (MLLMs), to facilitate these\ninteractions without the need for object pre-registration. We implement the AOI\nconcept in the form of XR-Objects, an open-source prototype system that\nprovides a platform for users to engage with their physical environment in\ncontextually relevant ways using object-based context menus. This system\nenables analog objects to not only convey information but also to initiate\ndigital actions, such as querying for details or executing tasks. Our\ncontributions are threefold: (1) we define the AOI concept and detail its\nadvantages over traditional AI assistants, (2) detail the XR-Objects system's\nopen-source design and implementation, and (3) show its versatility through\nvarious use cases and a user study."}
{"id": "2505.10975", "pdf": "https://arxiv.org/pdf/2505.10975.pdf", "abs": "https://arxiv.org/abs/2505.10975", "title": "Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio", "authors": ["Xinlu He", "Jacob Whitehill"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "13 pages. Submitted to IEEE/ACM Transaction on Audio Speech and\n  Language Processing (TASLP)", "summary": "Monaural multi-speaker automatic speech recognition (ASR) remains challenging\ndue to data scarcity and the intrinsic difficulty of recognizing and\nattributing words to individual speakers, particularly in overlapping speech.\nRecent advances have driven the shift from cascade systems to end-to-end (E2E)\narchitectures, which reduce error propagation and better exploit the synergy\nbetween speech content and speaker identity. Despite rapid progress in E2E\nmulti-speaker ASR, the field lacks a comprehensive review of recent\ndevelopments. This survey provides a systematic taxonomy of E2E neural\napproaches for multi-speaker ASR, highlighting recent advances and comparative\nanalysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)\nfor pre-segmented audio, analyzing their distinct characteristics and\ntrade-offs; (2) recent architectural and algorithmic improvements based on\nthese two paradigms; (3) extensions to long-form speech, including segmentation\nstrategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate\nand compare methods across standard benchmarks. We conclude with a discussion\nof open challenges and future research directions towards building robust and\nscalable multi-speaker ASR."}
{"id": "2501.11803", "pdf": "https://arxiv.org/pdf/2501.11803.pdf", "abs": "https://arxiv.org/abs/2501.11803", "title": "Automating High Quality RT Planning at Scale", "authors": ["Riqiang Gao", "Mamadou Diallo", "Han Liu", "Anthony Magliari", "Jonathan Sackett", "Wilko Verbakel", "Sandra Meyers", "Rafe Mcbeth", "Masoud Zarepisheh", "Simon Arberet", "Martin Kraus", "Florin C. Ghesu", "Ali Kamen"], "categories": ["cs.HC", "cs.LG", "cs.RO"], "comment": "radiotherapy planning, data for AI training", "summary": "Radiotherapy (RT) planning is complex, subjective, and time-intensive.\nAdvances with artificial intelligence (AI) promise to improve its precision and\nefficiency, but progress is often limited by the scarcity of large,\nstandardized datasets. To address this, we introduce the Automated Iterative RT\nPlanning (AIRTP) system, a scalable solution for generating high-quality\ntreatment plans. This scalable solution is designed to generate substantial\nvolumes of consistently high-quality treatment plans, overcoming a key obstacle\nin the advancement of AI-driven RT planning. Our AIRTP pipeline adheres to\nclinical guidelines and automates essential steps, including organ-at-risk\n(OAR) contouring, helper structure creation, beam setup, optimization, and plan\nquality improvement, using AI integrated with RT planning software like Varian\nEclipse. Furthermore, a novel approach for determining optimization parameters\nto reproduce 3D dose distributions, i.e. a method to convert dose predictions\nto deliverable treatment plans constrained by machine limitations is proposed.\nA comparative analysis of plan quality reveals that our automated pipeline\nproduces treatment plans of quality comparable to those generated manually,\nwhich traditionally require several hours of labor per plan. Committed to\npublic research, the first data release of our AIRTP pipeline includes nine\ncohorts covering head-and-neck and lung cancer sites to support an AAPM 2025\nchallenge. To our best knowledge, this dataset features more than 10 times\nnumber of plans compared to the largest existing well-curated public dataset.\nRepo: https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge."}
{"id": "2505.11004", "pdf": "https://arxiv.org/pdf/2505.11004.pdf", "abs": "https://arxiv.org/abs/2505.11004", "title": "Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning", "authors": ["Jingcheng Niu", "Subhabrata Dutta", "Ahmed Elshabrawy", "Harish Tayyar Madabushi", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale Transformer language models (LMs) trained solely on next-token\nprediction with web-scale data can solve a wide range of tasks after seeing\njust a few examples. The mechanism behind this capability, known as in-context\nlearning (ICL), remains both controversial and poorly understood. Some studies\nargue that it is merely the result of memorizing vast amounts of data, while\nothers contend that it reflects a fundamental, symbolic algorithmic development\nin LMs. In this work, we introduce a suite of investigative tasks and a novel\nmethod to systematically investigate ICL by leveraging the full Pythia scaling\nsuite, including interim checkpoints that capture progressively larger amount\nof training data. By carefully exploring ICL performance on downstream tasks\nand simultaneously conducting a mechanistic analysis of the residual stream's\nsubspace, we demonstrate that ICL extends beyond mere \"memorization\" of the\ntraining corpus, yet does not amount to the implementation of an independent\nsymbolic algorithm. Our results also clarify several aspects of ICL, including\nthe influence of training dynamics, model capabilities, and elements of\nmechanistic interpretability. Overall, our work advances the understanding of\nICL and its implications, offering model developers insights into potential\nimprovements and providing AI security practitioners with a basis for more\ninformed guidelines."}
{"id": "2503.04114", "pdf": "https://arxiv.org/pdf/2503.04114.pdf", "abs": "https://arxiv.org/abs/2503.04114", "title": "Organize, Then Vote: Exploring Cognitive Load in Quadratic Survey Interfaces", "authors": ["Ti-Chung Cheng", "Yutong Zhang", "Yi-Hung Chou", "Vinay Koshy", "Tiffany Wenting Li", "Karrie Karahalios", "Hari Sundaram"], "categories": ["cs.HC", "H.5.2"], "comment": null, "summary": "Quadratic Surveys (QSs) elicit more accurate preferences than traditional\nmethods like Likert-scale surveys. However, the cognitive load associated with\nQSs has hindered their adoption in digital surveys for collective\ndecision-making. We introduce a two-phase \"organize-then-vote\" QS to reduce\ncognitive load. As interface design significantly impacts survey results and\naccuracy, our design scaffolds survey takers' decision-making while managing\nthe cognitive load imposed by QS. In a 2x2 between-subject in-lab study on\npublic resource allotment, we compared our interface with a traditional text\ninterface across a QS with 6 (short) and 24 (long) options. Two-phase interface\nparticipants spent more time per option and exhibited shorter voting edit\ndistances. We qualitatively observed shifts in cognitive effort from mechanical\noperations to constructing more comprehensive preferences. We conclude that\nthis interface promoted deeper engagement, potentially reducing satisficing\nbehaviors caused by cognitive overload in longer QSs. This research clarifies\nhow human-centered design improves preference elicitation tools for collective\ndecision-making."}
{"id": "2505.11008", "pdf": "https://arxiv.org/pdf/2505.11008.pdf", "abs": "https://arxiv.org/abs/2505.11008", "title": "Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs", "authors": ["Ye Kyaw Thu", "Thazin Myint Oo"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "14 pages, 2 figures, 6 tables, 1 listing", "summary": "This paper explores syllable sequence prediction in Abugida languages using\nTransformer-based models, focusing on six languages: Bengali, Hindi, Khmer,\nLao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We\ninvestigate the reconstruction of complete syllable sequences from various\nincomplete input types, including consonant sequences, vowel sequences, partial\nsyllables (with random character deletions), and masked syllables (with fixed\nsyllable deletions). Our experiments reveal that consonant sequences play a\ncritical role in accurate syllable prediction, achieving high BLEU scores,\nwhile vowel sequences present a significantly greater challenge. The model\ndemonstrates robust performance across tasks, particularly in handling partial\nand masked syllable reconstruction, with strong results for tasks involving\nconsonant information and syllable masking. This study advances the\nunderstanding of sequence prediction for Abugida languages and provides\npractical insights for applications such as text prediction, spelling\ncorrection, and data augmentation in these scripts."}
{"id": "2504.14406", "pdf": "https://arxiv.org/pdf/2504.14406.pdf", "abs": "https://arxiv.org/abs/2504.14406", "title": "ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking", "authors": ["Runlong Ye", "Patrick Yung Kang Lee", "Matthew Varona", "Oliver Huang", "Carolina Nobre"], "categories": ["cs.HC", "cs.AI"], "comment": "accepted at CHIWORK '25", "summary": "Synthesizing knowledge from large document collections is a critical yet\nincreasingly complex aspect of qualitative research and knowledge work. While\nAI offers automation potential, effectively integrating it into human-centric\nsensemaking workflows remains challenging. We present ScholarMate, an\ninteractive system designed to augment qualitative analysis by unifying AI\nassistance with human oversight. ScholarMate enables researchers to dynamically\narrange and interact with text snippets on a non-linear canvas, leveraging AI\nfor theme suggestions, multi-level summarization, and evidence-based theme\nnaming, while ensuring transparency through traceability to source documents.\nInitial pilot studies indicated that users value this mixed-initiative\napproach, finding the balance between AI suggestions and direct manipulation\ncrucial for maintaining interpretability and trust. We further demonstrate the\nsystem's capability through a case study analyzing 24 papers. By balancing\nautomation with human control, ScholarMate enhances efficiency and supports\ninterpretability, offering a valuable approach for productive human-AI\ncollaboration in demanding sensemaking tasks common in knowledge work."}
{"id": "2505.11010", "pdf": "https://arxiv.org/pdf/2505.11010.pdf", "abs": "https://arxiv.org/abs/2505.11010", "title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models", "authors": ["Jiangxu Wu", "Cong Wang", "TianHuang Su", "Jun Yang", "Haozhi Lin", "Chao Zhang", "Ming Peng", "Kai Shi", "SongPan Yang", "BinQing Pan", "ZiXian Li", "Ni Yang", "ZhenYu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL2025 Accepted", "summary": "The effectiveness of large language models (LLMs) in conversational AI is\nhindered by their reliance on single-turn supervised fine-tuning (SFT) data,\nwhich limits contextual coherence in multi-turn dialogues. Existing methods for\ngenerating multi-turn dialogue data struggle to ensure both diversity and\nquality in instructions. To address this, we propose Review-Instruct, a novel\nframework that synthesizes multi-turn conversations through an iterative\n\"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple\nReviewers, and a Chairman. The framework iteratively refines instructions by\nincorporating Reviewer feedback, enhancing dialogue diversity and difficulty.\nWe construct a multi-turn dataset using the Alpaca dataset and fine-tune the\nLLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate\nsignificant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\%\non MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.\nAblation studies confirm the critical role of the Review stage and the use of\nmultiple Reviewers in boosting instruction diversity and difficulty. Our work\nhighlights the potential of review-driven, multi-agent frameworks for\ngenerating high-quality conversational data at scale."}
{"id": "2504.20035", "pdf": "https://arxiv.org/pdf/2504.20035.pdf", "abs": "https://arxiv.org/abs/2504.20035", "title": "Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality", "authors": ["Brandon Woodard", "Melvin He", "Mose Sakashita", "Jing Qian", "Zainab Iftikhar", "Joseph J. LaViola Jr"], "categories": ["cs.HC", "H.5.2; H.5.1; H.5.0; H.1.2"], "comment": null, "summary": "Off-the-shelf smartphone-based AR systems typically use a single front-facing\nor rear-facing camera, which restricts user interactions to a narrow field of\nview and small screen size, thus reducing their practicality. We present\nCam-2-Cam, an interaction concept implemented in three smartphone-based AR\napplications with interactions that span both cameras. Results from our\nqualitative analysis conducted on 30 participants presented two major design\nlessons that explore the interaction space of smartphone AR while maintaining\ncritical AR interface attributes like embodiment and immersion: (1) Balancing\nContextual Relevance and Feedback Quality serves to outline a delicate balance\nbetween implementing familiar interactions people do in the real world and the\nquality of multimodal AR responses and (2) Preventing Disorientation using\nSimultaneous Capture and Alternating Cameras which details how to prevent\ndisorientation during AR interactions using the two distinct camera techniques\nwe implemented in the paper. Additionally, we consider observed user\nassumptions or natural tendencies to inform future implementations of\ndual-camera setups for smartphone-based AR. We envision our design lessons as\nan initial pioneering step toward expanding the interaction space of\nsmartphone-based AR, potentially driving broader adoption and overcoming\nlimitations of single-camera AR."}
{"id": "2505.11026", "pdf": "https://arxiv.org/pdf/2505.11026.pdf", "abs": "https://arxiv.org/abs/2505.11026", "title": "StRuCom: A Novel Dataset of Structured Code Comments in Russian", "authors": ["Maria Dziuba", "Valentin Malykh"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Structured code comments in docstring format are essential for code\ncomprehension and maintenance, but existing machine learning models for their\ngeneration perform poorly for Russian compared to English. To bridge this gap,\nwe present StRuCom - the first large-scale dataset (153K examples) specifically\ndesigned for Russian code documentation. Unlike machine-translated English\ndatasets that distort terminology (e.g., technical loanwords vs. literal\ntranslations) and docstring structures, StRuCom combines human-written comments\nfrom Russian GitHub repositories with synthetically generated ones, ensuring\ncompliance with Python, Java, JavaScript, C#, and Go standards through\nautomated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom\nshows statistically significant improvements of chrf++ and BERTScore over\nbaseline models."}
{"id": "2505.09819", "pdf": "https://arxiv.org/pdf/2505.09819.pdf", "abs": "https://arxiv.org/abs/2505.09819", "title": "Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses", "authors": ["Ruichen Yang", "GyÃ¶rgy M. LÃ©vay", "Christopher L. Hunt", "DÃ¡niel Czeiner", "Megan C. Hodgson", "Damini Agarwal", "Rahul R. Kaliki", "Nitish V. Thakor"], "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments."}
{"id": "2505.11031", "pdf": "https://arxiv.org/pdf/2505.11031.pdf", "abs": "https://arxiv.org/abs/2505.11031", "title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning", "authors": ["Xiao Zhang", "Huiyuan Lai", "Qianru Meng", "Johan Bos"], "categories": ["cs.CL"], "comment": "Paper submitted to NeruoIPS 2025 dataset and benchmark track", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing tasks, yet their ability to process\nstructured symbolic knowledge remains underexplored. To address this gap, we\npropose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the\nfirst comprehensive benchmark designed to systematically evaluate LLMs'\nproficiency in handling ontologies -- formal, symbolic representations of\ndomain knowledge through concepts, relationships, and instances. Based on the\nproposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n58,981 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing proficiency in understanding\nontological knowledge but substantial weaknesses in reasoning and learning\ntasks. These findings highlight fundamental limitations in LLMs' capability to\nprocess symbolic knowledge and establish OntoURL as a critical benchmark for\nadvancing the integration of LLMs with formal knowledge representations."}
{"id": "2505.09875", "pdf": "https://arxiv.org/pdf/2505.09875.pdf", "abs": "https://arxiv.org/abs/2505.09875", "title": "Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing", "authors": ["Shuning Zhang", "Jingruo Chen", "Zhiqi Gao", "Jiajing Gao", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The proliferation of Large Language Model (LLM)-based Graphical User\nInterface (GUI) agents in web browsing scenarios present complex unintended\nconsequences (UCs). This paper characterizes three UCs from three perspectives:\nphenomena, influence and mitigation, drawing on social media analysis (N=221\nposts) and semi-structured interviews (N=14). Key phenomenon for UCs include\nagents' deficiencies in comprehending instructions and planning tasks,\nchallenges in executing accurate GUI interactions and adapting to dynamic\ninterfaces, the generation of unreliable or misaligned outputs, and\nshortcomings in error handling and feedback processing. These phenomena\nmanifest as influences from unanticipated actions and user frustration, to\nprivacy violations and security vulnerabilities, and further to eroded trust\nand wider ethical concerns. Our analysis also identifies user-initiated\nmitigation, such as technical adjustments and manual oversight, and provides\nimplications for designing future LLM-based GUI agents that are robust,\nuser-centric, and transparent, fostering a crucial balance between automation\nand human oversight."}
{"id": "2505.11051", "pdf": "https://arxiv.org/pdf/2505.11051.pdf", "abs": "https://arxiv.org/abs/2505.11051", "title": "CAMEO: Collection of Multilingual Emotional Speech Corpora", "authors": ["Iwona Christop", "Maciej Czajka"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Under review at NeurIPS", "summary": "This paper presents CAMEO -- a curated collection of multilingual emotional\nspeech datasets designed to facilitate research in emotion recognition and\nother speech-related tasks. The main objectives were to ensure easy access to\nthe data, to allow reproducibility of the results, and to provide a\nstandardized benchmark for evaluating speech emotion recognition (SER) systems\nacross different emotional states and languages. The paper describes the\ndataset selection criteria, the curation and normalization process, and\nprovides performance results for several models. The collection, along with\nmetadata, and a leaderboard, is publicly available via the Hugging Face\nplatform."}
{"id": "2505.10412", "pdf": "https://arxiv.org/pdf/2505.10412.pdf", "abs": "https://arxiv.org/abs/2505.10412", "title": "Using Virtual Reality in Museums to Bridge the Gap Between Material Heritage and the Interpretation of Its Immaterial Context", "authors": ["Carlos R. Cunha", "VÃ­tor MendonÃ§a", "AndrÃ© Moreira", "JoÃ£o Pedro Gomes", "Aida Carvalho"], "categories": ["cs.HC"], "comment": "C. R. Cunha, V. Mendon\\c{c}a, A. Moreira, J. P. Gomes, and A.\n  Carvalho, 'Using Virtual Reality in Museums to Bridge the Gap Between\n  Material Heritage and the Interpretation of Its Immaterial Context', in\n  Advances in Tourism, Technology and Systems, 2022, pp. 397-408", "summary": "Material heritage typically has a whole set of associated immaterial\nheritage, which is essential to pass on to the visitor as a cultural mission of\nthe destinations and those who manage them. In this sense, the interpretation\nof material heritage is a complex process that is not a fully efficient process\nwith the mere observation of physical artifacts. In this context, it emerges as\nfundamental to provide visitors with a set of tools that allow them to\ncorrectly interpret the artifacts that come to fully understand the cultural\ndimension of the destinations and their heritage. Accordingly, the role of\nvirtual reality can leverage the creation of innovative and immersive solutions\nthat allow the visitor to understand and feel part of their own heritage and\nits ancestral component that defines the sociocultural roots of destinations\nand their civilizational traditions. This article, after dissecting and\nsubstantiating the role of virtual reality in the interpretation of heritage,\npresents a conceptual model, based on the use of virtual reality, which was, in\npart, prototyped in the scenario of the Portuguese Museum in the city of\nMiranda do Douro. This proposal is an ongoing contribution to the creation of\ninnovative and immersive tools for the interpretation of heritage."}
{"id": "2505.11080", "pdf": "https://arxiv.org/pdf/2505.11080.pdf", "abs": "https://arxiv.org/abs/2505.11080", "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following", "authors": ["Yapei Chang", "Yekyung Kim", "Michael Krumdick", "Amir Zadeh", "Chuan Li", "Chris Tanner", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 11 figures, 15 tables", "summary": "Reward models are central to aligning LLMs with human preferences, but they\nare costly to train, requiring large-scale human-labeled preference data and\npowerful pretrained LLM backbones. Meanwhile, the increasing availability of\nhigh-quality synthetic instruction-following datasets raises the question: can\nsimpler, reference-based metrics serve as viable alternatives to reward models\nduring RL-based alignment? In this paper, we show first that BLEU, a basic\nstring-matching metric, surprisingly matches strong reward models in agreement\nwith human preferences on general instruction-following datasets. Based on this\ninsight, we develop BLEUBERI, a method that first identifies challenging\ninstructions and then applies Group Relative Policy Optimization (GRPO) using\nBLEU directly as the reward function. We demonstrate that BLEUBERI-trained\nmodels are competitive with models trained via reward model-guided RL across\nfour challenging instruction-following benchmarks and three different base\nlanguage models. A human evaluation further supports that the quality of\nBLEUBERI model outputs is on par with those from reward model-aligned models.\nMoreover, BLEUBERI models generate outputs that are more factually grounded\nthan competing methods. Overall, we show that given access to high-quality\nreference outputs (easily obtained via existing instruction-following datasets\nor synthetic data generation), string matching-based metrics are cheap yet\neffective proxies for reward models during alignment. We release our code and\ndata at https://github.com/lilakk/BLEUBERI."}
{"id": "2412.09765", "pdf": "https://arxiv.org/pdf/2412.09765.pdf", "abs": "https://arxiv.org/abs/2412.09765", "title": "L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement", "authors": ["Morgan B. Talbot", "Gabriel Kreiman", "James J. DiCarlo", "Guy Gaziv"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "The currently leading artificial neural network models of the visual ventral\nstream - which are derived from a combination of performance optimization and\nrobustification methods - have demonstrated a remarkable degree of behavioral\nalignment with humans on visual categorization tasks. We show that image\nperturbations generated by these models can enhance the ability of humans to\naccurately report the ground truth class. Furthermore, we find that the same\nmodels can also be used out-of-the-box to predict the proportion of correct\nhuman responses to individual images, providing a simple, human-aligned\nestimator of the relative difficulty of each image. Motivated by these\nobservations, we propose to augment visual learning in humans in a way that\nimproves human categorization accuracy at test time. Our learning augmentation\napproach consists of (i) selecting images based on their model-estimated\nrecognition difficulty, and (ii) applying image perturbations that aid\nrecognition for novice learners. We find that combining these model-based\nstrategies leads to categorization accuracy gains of 33-72% relative to control\nsubjects without these interventions, on unmodified, randomly selected held-out\ntest images. Beyond the accuracy gain, the training time for the augmented\nlearning group was also shortened by 20-23%, despite both groups completing the\nsame number of training trials. We demonstrate the efficacy of our approach in\na fine-grained categorization task with natural images, as well as two tasks in\nclinically relevant image domains - histology and dermoscopy - where visual\nlearning is notoriously challenging. To the best of our knowledge, our work is\nthe first application of artificial neural networks to increase visual learning\nperformance in humans by enhancing category-specific image features."}
{"id": "2505.11095", "pdf": "https://arxiv.org/pdf/2505.11095.pdf", "abs": "https://arxiv.org/abs/2505.11095", "title": "Towards Better Evaluation for Generated Patent Claims", "authors": ["Lekang Jiang", "Pascal A Scherz", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. 14 pages, 8 tables", "summary": "Patent claims define the scope of protection and establish the legal\nboundaries of an invention. Drafting these claims is a complex and\ntime-consuming process that usually requires the expertise of skilled patent\nattorneys, which can form a large access barrier for many small enterprises. To\nsolve these challenges, researchers have investigated the use of large language\nmodels (LLMs) for automating patent claim generation. However, existing studies\nhighlight inconsistencies between automated evaluation metrics and human expert\nassessments. To bridge this gap, we introduce Patent-CE, the first\ncomprehensive benchmark for evaluating patent claims. Patent-CE includes\ncomparative claim evaluations annotated by patent experts, focusing on five key\ncriteria: feature completeness, conceptual clarity, terminology consistency,\nlogical linkage, and overall quality. Additionally, we propose PatClaimEval, a\nnovel multi-dimensional evaluation method specifically designed for patent\nclaims. Our experiments demonstrate that PatClaimEval achieves the highest\ncorrelation with human expert evaluations across all assessment criteria among\nall tested metrics. This research provides the groundwork for more accurate\nevaluations of automated patent claim generation systems."}
{"id": "2501.03266", "pdf": "https://arxiv.org/pdf/2501.03266.pdf", "abs": "https://arxiv.org/abs/2501.03266", "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena", "authors": ["Stefan Pasch"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": null, "summary": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. In particular,\nlittle is known about how users respond when models refuse to answer a\nprompt-one of the primary mechanisms used to enforce ethical boundaries in\nLLMs. We address this gap by analyzing nearly 50,000 model comparisons from\nChatbot Arena, a platform where users indicate their preferred LLM response in\npairwise matchups, providing a large-scale setting for studying real-world user\npreferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a\nhand-labeled dataset, we distinguish between refusals due to ethical concerns\nand technical limitations. Our results reveal a substantial refusal penalty:\nethical refusals yield significantly lower win rates than both technical\nrefusals and standard responses, indicating that users are especially\ndissatisfied when models decline a task for ethical reasons. However, this\npenalty is not uniform. Refusals receive more favorable evaluations when the\nunderlying prompt is highly sensitive (e.g., involving illegal content), and\nwhen the refusal is phrased in a detailed and contextually aligned manner.\nThese findings underscore a core tension in LLM design: safety-aligned\nbehaviors may conflict with user expectations, calling for more adaptive\nmoderation strategies that account for context and presentation."}
{"id": "2505.11140", "pdf": "https://arxiv.org/pdf/2505.11140.pdf", "abs": "https://arxiv.org/abs/2505.11140", "title": "Scaling Reasoning can Improve Factuality in Large Language Models", "authors": ["Mike Zhang", "Johannes Bjerva", "Russa Biswas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955.pdf", "abs": "https://arxiv.org/abs/2504.13955", "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "authors": ["Suhas BN", "Andrew M. Sherrill", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "22 pages, 6 figures Updated Appendix with example model responses", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2505.11166", "pdf": "https://arxiv.org/pdf/2505.11166.pdf", "abs": "https://arxiv.org/abs/2505.11166", "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization", "authors": ["Huashan Sun", "Shengyi Liao", "Yansen Han", "Yu Bai", "Yang Gao", "Cheng Fu", "Weizhou Shen", "Fanqi Wan", "Ming Yan", "Ji Zhang", "Fei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency."}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724.pdf", "abs": "https://arxiv.org/abs/2505.09724", "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "authors": ["Gino Carmona-DÃ­az", "William JimÃ©nez-Leal", "MarÃ­a Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo BermÃºdez"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis."}
{"id": "2505.11177", "pdf": "https://arxiv.org/pdf/2505.11177.pdf", "abs": "https://arxiv.org/abs/2505.11177", "title": "Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline", "authors": ["Hrishit Madhavi", "Jacob Cherian", "Yuvraj Khamkar", "Dhananjay Bhagat"], "categories": ["cs.CL", "cs.AI", "68T50 (Natural language processing), 68U10 (Image processing)"], "comment": "8 pages, 7 figures, direct arXiv submission", "summary": "This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments"}
{"id": "2505.11199", "pdf": "https://arxiv.org/pdf/2505.11199.pdf", "abs": "https://arxiv.org/abs/2505.11199", "title": "NoPE: The Counting Power of Transformers with No Positional Encodings", "authors": ["Chris KÃ¶cher", "Alexander Kozachinskiy", "Anthony Widjaja Lin", "Marco SÃ¤lzer", "Georg Zetzsche"], "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": null, "summary": "Positional Encodings (PEs) seem to be indispensable for ensuring\nexpressiveness of transformers; without them attention transformers reduce to a\nbag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard\nattention mechanisms were very recently shown to only be able to express\nregular languages, i.e., with limited counting ability. This paper shows that,\nwith average hard attention mechanisms, NoPE-transformers are still\nsurprisingly expressive: they can express counting languages corresponding to\nnonnegative integer solutions to multivariate polynomial equations (i.e.\nDiophantine equations), reasoning about which is well-known to be undecidable.\nIn fact, we provide a precise characterization of languages expressible by\nAverage Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond\nprecisely to what we call \\emph{semi-algebraic sets}, i.e., finite unions of\nsets of nonnegative integer solutions to systems of multivariate polynomial\ninequations. We obtain several interesting consequences of our\ncharacterization. Firstly, NoPE-transformers can express counting properties\nthat are far more complex than established models like simplified counter\nmachines and Petri nets, but cannot express a very simple counting property of\nPARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,\ne.g., whether a given NoPE transformer classifies all input strings in one\nclass. To complement our results, we exhibit a counting language that is not\nexpressible by average hard attention transformers even with arbitrary PEs but\nis expressible in the circuit complexity class TC$^0$, answering an open\nproblem."}
{"id": "2505.11225", "pdf": "https://arxiv.org/pdf/2505.11225.pdf", "abs": "https://arxiv.org/abs/2505.11225", "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization", "authors": ["Chengyu Huang", "Zhengxin Zhang", "Claire Cardie"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While scaling the length of responses at test-time has been shown to markedly\nimprove the reasoning abilities and performance of large language models\n(LLMs), it often results in verbose outputs and increases inference cost. Prior\napproaches for efficient test-time scaling, typically using universal budget\nconstraints or query-level length optimization, do not leverage historical\ninformation from previous encounters with the same problem during training. We\nhypothesize that this limits their ability to progressively make solutions more\nconcise over time. To address this, we present History-Aware Policy\nOptimization (HAPO), which keeps track of a history state (e.g., the minimum\nlength over previously generated correct responses) for each problem. HAPO\nemploys a novel length reward function based on this history state to\nincentivize the discovery of correct solutions that are more concise than those\npreviously found. Crucially, this reward structure avoids overly penalizing\nshorter incorrect responses with the goal of facilitating exploration towards\nmore efficient solutions. By combining this length reward with a correctness\nreward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to\ntrain DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and\nQwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span\nvarious difficulty levels. Experiment results demonstrate that HAPO effectively\ninduces LLMs' concise reasoning abilities, producing length reductions of\n33-59% with accuracy drops of only 2-5%."}
{"id": "2505.11271", "pdf": "https://arxiv.org/pdf/2505.11271.pdf", "abs": "https://arxiv.org/abs/2505.11271", "title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models", "authors": ["Camille Couturier", "Spyros Mastorakis", "Haiying Shen", "Saravan Rajmohan", "Victor RÃ¼hle"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.7"], "comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings", "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants."}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277.pdf", "abs": "https://arxiv.org/abs/2505.11277", "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "authors": ["Yaorui Shi", "Shihan Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."}
{"id": "2505.11280", "pdf": "https://arxiv.org/pdf/2505.11280.pdf", "abs": "https://arxiv.org/abs/2505.11280", "title": "Temporal fine-tuning for early risk detection", "authors": ["Horacio Thompson", "EsaÃº Villatoro-Tello", "Manuel Montes-y-GÃ³mez", "Marcelo Errecalde"], "categories": ["cs.CL"], "comment": "In: Proceedings of the 53rd JAIIO / 50th CLEI - ASAID, 2024, p. 137.\n  ISSN: 2451-7496", "summary": "Early Risk Detection (ERD) on the Web aims to identify promptly users facing\nsocial and health issues. Users are analyzed post-by-post, and it is necessary\nto guarantee correct and quick answers, which is particularly challenging in\ncritical scenarios. ERD involves optimizing classification precision and\nminimizing detection delay. Standard classification metrics may not suffice,\nresorting to specific metrics such as ERDE(theta) that explicitly consider\nprecision and delay. The current research focuses on applying a multi-objective\napproach, prioritizing classification performance and establishing a separate\ncriterion for decision time. In this work, we propose a completely different\nstrategy, temporal fine-tuning, which allows tuning transformer-based models by\nexplicitly incorporating time within the learning process. Our method allows us\nto analyze complete user post histories, tune models considering different\ncontexts, and evaluate training performance using temporal metrics. We\nevaluated our proposal in the depression and eating disorders tasks for the\nSpanish language, achieving competitive results compared to the best models of\nMentalRiskES 2023. We found that temporal fine-tuning optimized decisions\nconsidering context and time progress. In this way, by properly taking\nadvantage of the power of transformers, it is possible to address ERD by\ncombining precision and speed as a single objective."}
{"id": "2505.11297", "pdf": "https://arxiv.org/pdf/2505.11297.pdf", "abs": "https://arxiv.org/abs/2505.11297", "title": "Probing Subphonemes in Morphology Models", "authors": ["Gal Astrach", "Yuval Pinter"], "categories": ["cs.CL"], "comment": null, "summary": "Transformers have achieved state-of-the-art performance in morphological\ninflection tasks, yet their ability to generalize across languages and\nmorphological rules remains limited. One possible explanation for this behavior\ncan be the degree to which these models are able to capture implicit phenomena\nat the phonological and subphonemic levels. We introduce a language-agnostic\nprobing method to investigate phonological feature encoding in transformers\ntrained directly on phonemes, and perform it across seven morphologically\ndiverse languages. We show that phonological features which are local, such as\nfinal-obstruent devoicing in Turkish, are captured well in phoneme embeddings,\nwhereas long-distance dependencies like vowel harmony are better represented in\nthe transformer's encoder. Finally, we discuss how these findings inform\nempirical strategies for training morphological models, particularly regarding\nthe role of subphonemic feature acquisition."}
{"id": "2505.11336", "pdf": "https://arxiv.org/pdf/2505.11336.pdf", "abs": "https://arxiv.org/abs/2505.11336", "title": "XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision", "authors": ["Nuo Chen", "Andre Lin HuiKai", "Jiaying Wu", "Junyi Hou", "Zining Zhang", "Qian Wang", "Xidong Wang", "Bingsheng He"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts."}
{"id": "2505.11341", "pdf": "https://arxiv.org/pdf/2505.11341.pdf", "abs": "https://arxiv.org/abs/2505.11341", "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models", "authors": ["Banca Calvo Figueras", "Rodrigo Agerri"], "categories": ["cs.CL"], "comment": null, "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose assumptions and\nchallenge the reasoning in arguments. Despite growing interest in this area,\nprogress has been hindered by the lack of suitable datasets and automatic\nevaluation standards. This work presents a comprehensive approach to support\nthe development and benchmarking of systems for this task. We construct the\nfirst large-scale manually-annotated dataset. We also investigate automatic\nevaluation methods and identify a reference-based technique using large\nlanguage models (LLMs) as the strategy that best correlates with human\njudgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline\nwhile showcasing the difficulty of the task. Data, code, and a public\nleaderboard are provided to encourage further research not only in terms of\nmodel performance, but also to explore the practical benefits of CQs-Gen for\nboth automated reasoning and human critical thinking."}
{"id": "2505.11352", "pdf": "https://arxiv.org/pdf/2505.11352.pdf", "abs": "https://arxiv.org/abs/2505.11352", "title": "LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors", "authors": ["Rao Ma", "Tongzhou Chen", "Kartik Audhkhasi", "Bhuvana Ramabhadran"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recently, large-scale pre-trained speech encoders and Large Language Models\n(LLMs) have been released, which show state-of-the-art performance on a range\nof spoken language processing tasks including Automatic Speech Recognition\n(ASR). To effectively combine both models for better performance, continuous\nspeech prompts, and ASR error correction have been adopted. However, these\nmethods are prone to suboptimal performance or are inflexible. In this paper,\nwe propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using\nthe ASR posterior matrices. The speech encoder is trained to generate\nConnectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,\nwhich are used to reconstruct pseudo-audio embeddings by computing a weighted\nsum of the LLM input embeddings. These embeddings are concatenated with text\nembeddings in the LLM input space. Using the well-performing USM and Gemma\nmodels as an example, we demonstrate that our proposed LegoSLM method yields\ngood performance on both ASR and speech translation tasks. By connecting USM\nwith Gemma models, we can get an average of 49% WERR over the USM-CTC baseline\non 8 MLS testsets. The trained model also exhibits modularity in a range of\nsettings -- after fine-tuning the Gemma model weights, the speech encoder can\nbe switched and combined with the LLM in a zero-shot fashion. Additionally, we\npropose to control the decode-time influence of the USM and LLM using a softmax\ntemperature, which shows effectiveness in domain adaptation."}
{"id": "2505.11368", "pdf": "https://arxiv.org/pdf/2505.11368.pdf", "abs": "https://arxiv.org/abs/2505.11368", "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines."}
{"id": "2505.11379", "pdf": "https://arxiv.org/pdf/2505.11379.pdf", "abs": "https://arxiv.org/abs/2505.11379", "title": "A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography", "authors": ["Alicia GonzÃ¡lez MartÃ­nez"], "categories": ["cs.CL"], "comment": null, "summary": "Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic\nnotation that can be traced back to the early stages of Islam, when the Quran\nwas mainly oral in nature and the first written renderings of it served as\nmemory aids for this oral tradition. The early systems of diacritical marks\ncreated on top of the Quranic Consonantal Text (QCT) motivated the creation and\nfurther development of a fine-grained system of phonetic notation that\nrepresented tajwid-the rules of recitation. We explored the systematicity of\nthe rules of tajwid, as they are encountered in the Cairo Quran, using a fully\nand accurately encoded digital edition of the Quranic text. For this purpose,\nwe developed a python module that can remove or add the orthographic layer of\ntajwid from a Quranic text in CQO. The interesting characteristic of these two\nsets of rules is that they address the complete Quranic text of the Cairo\nQuran, so they can be used as precise witnesses to study its phonetic and\nprosodic processes. From a computational point of view, the text of the Cairo\nQuran can be used as a linchpin to align and compare Quranic manuscripts, due\nto its richness and completeness. This will let us create a very powerful\nframework to work with the Arabic script, not just within an isolated text, but\nautomatically exploring a specific textual phenomenon in other connected\nmanuscripts. Having all the texts mapped among each other can serve as a\npowerful tool to study the nature of the notation systems of diacritics added\nto the consonantal skeleton."}
{"id": "2505.11413", "pdf": "https://arxiv.org/pdf/2505.11413.pdf", "abs": "https://arxiv.org/abs/2505.11413", "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs", "authors": ["Sijia Chen", "Xiaomin Li", "Mengxue Zhang", "Eric Hanchen Jiang", "Qingcheng Zeng", "Chen-Hsiang Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in medical contexts,\nraising critical concerns about safety, alignment, and susceptibility to\nadversarial manipulation. While prior benchmarks assess model refusal\ncapabilities for harmful prompts, they often lack clinical specificity, graded\nharmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES\n(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for\nevaluating LLM safety in healthcare. CARES includes over 18,000 prompts\nspanning eight medical safety principles, four harm levels, and four prompting\nstyles: direct, indirect, obfuscated, and role-play, to simulate both malicious\nand benign use cases. We propose a three-way response evaluation protocol\n(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess\nmodel behavior. Our analysis reveals that many state-of-the-art LLMs remain\nvulnerable to jailbreaks that subtly rephrase harmful prompts, while also\nover-refusing safe but atypically phrased queries. Finally, we propose a\nmitigation strategy using a lightweight classifier to detect jailbreak attempts\nand steer models toward safer behavior via reminder-based conditioning. CARES\nprovides a rigorous framework for testing and improving medical LLM safety\nunder adversarial and ambiguous conditions."}
{"id": "2505.11421", "pdf": "https://arxiv.org/pdf/2505.11421.pdf", "abs": "https://arxiv.org/abs/2505.11421", "title": "Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model", "authors": ["Phan Tran Minh Dat", "Vo Hoang Nhat Khang", "Quan Thanh Tho"], "categories": ["cs.CL"], "comment": null, "summary": "This work explores the journey towards achieving Bahnaric-Vietnamese\ntranslation for the sake of culturally bridging the two ethnic groups in\nVietnam. However, translating from Bahnaric to Vietnamese also encounters some\ndifficulties. The most prominent challenge is the lack of available original\nBahnaric resources source language, including vocabulary, grammar, dialogue\npatterns and bilingual corpus, which hinders the data collection process for\ntraining. To address this, we leverage a transfer learning approach using\nsequence-to-sequence pre-training language model. First of all, we leverage a\npre-trained Vietnamese language model to capture the characteristics of this\nlanguage. Especially, to further serve the purpose of machine translation, we\naim for a sequence-to-sequence model, not encoder-only like BERT or\ndecoder-only like GPT. Taking advantage of significant similarity between the\ntwo languages, we continue training the model with the currently limited\nbilingual resources of Vietnamese-Bahnaric text to perform the transfer\nlearning from language model to machine translation. Thus, this approach can\nhelp to handle the problem of imbalanced resources between two languages, while\nalso optimizing the training and computational processes. Additionally, we also\nenhanced the datasets using data augmentation to generate additional resources\nand defined some heuristic methods to help the translation more precise. Our\napproach has been validated to be highly effective for the Bahnaric-Vietnamese\ntranslation model, contributing to the expansion and preservation of languages,\nand facilitating better mutual understanding between the two ethnic people."}
{"id": "2505.11423", "pdf": "https://arxiv.org/pdf/2505.11423.pdf", "abs": "https://arxiv.org/abs/2505.11423", "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "authors": ["Xiaomin Li", "Zhou Yu", "Zhiwei Zhang", "Xupeng Chen", "Ziji Zhang", "Yingying Zhuang", "Narayanan Sadagopan", "Anurag Beniwal"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies."}
{"id": "2505.11436", "pdf": "https://arxiv.org/pdf/2505.11436.pdf", "abs": "https://arxiv.org/abs/2505.11436", "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "69 pages, 66 figures, accepted by ACL 2025", "summary": "Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025."}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441.pdf", "abs": "https://arxiv.org/abs/2505.11441", "title": "Is Compression Really Linear with Code Intelligence?", "authors": ["Xianzhen Luo", "Shijie Xuyang", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain."}
{"id": "2505.11462", "pdf": "https://arxiv.org/pdf/2505.11462.pdf", "abs": "https://arxiv.org/abs/2505.11462", "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios."}
{"id": "2505.11470", "pdf": "https://arxiv.org/pdf/2505.11470.pdf", "abs": "https://arxiv.org/abs/2505.11470", "title": "No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce two reference-free metrics for quality evaluation of taxonomies.\nThe first metric evaluates robustness by calculating the correlation between\nsemantic and taxonomic similarity, covering a type of error not handled by\nexisting metrics. The second uses Natural Language Inference to assess logical\nadequacy. Both metrics are tested on five taxonomies and are shown to correlate\nwell with F1 against gold-standard taxonomies."}
{"id": "2505.11475", "pdf": "https://arxiv.org/pdf/2505.11475.pdf", "abs": "https://arxiv.org/abs/2505.11475", "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "38 pages, 2 figures", "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference"}
{"id": "2505.11480", "pdf": "https://arxiv.org/pdf/2505.11480.pdf", "abs": "https://arxiv.org/abs/2505.11480", "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning", "authors": ["Anjiang Wei", "Tarun Suresh", "Huanmi Tan", "Yinglun Xu", "Gagandeep Singh", "Ke Wang", "Alex Aiken"], "categories": ["cs.CL", "cs.AI", "cs.PF", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance."}
{"id": "2505.11484", "pdf": "https://arxiv.org/pdf/2505.11484.pdf", "abs": "https://arxiv.org/abs/2505.11484", "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "categories": ["cs.CL"], "comment": "14 pages", "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT."}
{"id": "2505.11485", "pdf": "https://arxiv.org/pdf/2505.11485.pdf", "abs": "https://arxiv.org/abs/2505.11485", "title": "Modeling cognitive processes of natural reading with transformer-based Language Models", "authors": ["Bruno Bianchi", "FermÃ­n Travi", "Juan E. Kamienkowski"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers."}
{"id": "2505.10583", "pdf": "https://arxiv.org/pdf/2505.10583.pdf", "abs": "https://arxiv.org/abs/2505.10583", "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models", "authors": ["Diogo Freitas", "Brigt HÃ¥vardstun", "CÃ¨sar Ferri", "DarÃ­o Garigliotti", "Jan Arne Telle", "JosÃ© HernÃ¡ndez-Orallo"], "categories": ["cs.CV", "cs.CL"], "comment": "54 pages (42 pages of appendix)", "summary": "Large language models have become multimodal, and many of them are said to\nintegrate their modalities using common representations. If this were true, a\ndrawing of a car as an image, for instance, should map to the similar area in\nthe latent space as a textual description of the strokes that conform the\ndrawing. To explore this in a black-box access regime to these models, we\npropose the use of machine teaching, a theory that studies the minimal set of\nexamples a teacher needs to choose so that the learner captures the concept. In\nthis paper we evaluate the complexity of teaching visual-language models a\nsubset of objects in the Quick, Draw! dataset using two presentations: raw\nimages as bitmaps and trace coordinates in TikZ format. The results indicate\nthat image-based representations generally require fewer segments and achieve\nhigher accuracy than coordinate-based representations. But, surprisingly, the\nteaching size usually ranks concepts similarly across both modalities, even\nwhen controlling for (a human proxy of) concept priors, suggesting that the\nsimplicity of concepts may be an inherent property that transcends modality\nrepresentations."}
{"id": "2505.10586", "pdf": "https://arxiv.org/pdf/2505.10586.pdf", "abs": "https://arxiv.org/abs/2505.10586", "title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports", "authors": ["Poli A. Nemkova", "Suleyman O. Polat", "Rafid I. Jahan", "Sagnik Ray Choudhury", "Sun-joo Lee", "Shouryadipta Sarkar", "Mark V. Albert"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Timely and accurate situation awareness is vital for decision-making in\nhumanitarian response, conflict monitoring, and early warning and early action.\nHowever, the manual analysis of vast and heterogeneous data sources often\nresults in delays, limiting the effectiveness of interventions. This paper\nintroduces a dynamic Retrieval-Augmented Generation (RAG) system that\nautonomously generates situation awareness reports by integrating real-time\ndata from diverse sources, including news articles, conflict event databases,\nand economic indicators. Our system constructs query-specific knowledge bases\non demand, ensuring timely, relevant, and accurate insights.\n  To ensure the quality of generated reports, we propose a three-level\nevaluation framework that combines semantic similarity metrics, factual\nconsistency checks, and expert feedback. The first level employs automated NLP\nmetrics to assess coherence and factual accuracy. The second level involves\nhuman expert evaluation to verify the relevance and completeness of the\nreports. The third level utilizes LLM-as-a-Judge, where large language models\nprovide an additional layer of assessment to ensure robustness. The system is\ntested across multiple real-world scenarios, demonstrating its effectiveness in\nproducing coherent, insightful, and actionable reports. By automating report\ngeneration, our approach reduces the burden on human analysts and accelerates\ndecision-making processes. To promote reproducibility and further research, we\nopenly share our code and evaluation tools with the community via GitHub."}
{"id": "2505.10588", "pdf": "https://arxiv.org/pdf/2505.10588.pdf", "abs": "https://arxiv.org/abs/2505.10588", "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation", "authors": ["Manisha Mehta", "Fausto Giunchiglia"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "I.2; I.2.7; K.4.2"], "comment": "Accepted to ACM FAccT 2025. To be presented in Athens, June 2025, and\n  published in the conference proceedings. Preprint version; final version will\n  appear in the ACM Digital Library", "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges."}
{"id": "2505.10597", "pdf": "https://arxiv.org/pdf/2505.10597.pdf", "abs": "https://arxiv.org/abs/2505.10597", "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment", "authors": ["Jiazheng Zhang", "Wenqing Jing", "Zizhuo Zhang", "Zhiheng Xi", "Shihan Dou", "Rongxiang Weng", "Jiahuan Li", "Jingang Wang", "MingXu Cai", "Shibo Hong", "Tao Gui", "Qi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models (RMs) are essential for aligning large language models (LLMs)\nwith human values. However, noisy preferences in human feedback often lead to\nreward misgeneralization, where RMs overfit to spurious patterns and provide\nmisleading signals during policy optimization. We systematically analyze the\ntraining dynamics of preference pairs and identify that noisy examples are\nharder to fit and introduce instability. Empirical evidence shows that LLMs\noptimized using reward models trained on full noisy datasets perform worse than\nthose trained on filtered, high-quality preferences. To address this, we\npropose Collaborative Reward Modeling (CRM), an online framework that enhances\nrobustness by combining peer review and curriculum learning. Two reward models\nare trained in parallel and assess each other's data selections to filter out\npotential noise. Curriculum learning structures the preference data from easy\nto hard, ensuring synchronized training and stable feedback. Extensive\nexperiments demonstrate that CRM improves generalization, with up to 9.94\npoints of accuracy gain on RewardBench under 40 percent label noise. CRM is\nalso compatible with implicit-reward alignment methods, offering a practical\nand versatile strategy for robust alignment."}
{"id": "2505.10599", "pdf": "https://arxiv.org/pdf/2505.10599.pdf", "abs": "https://arxiv.org/abs/2505.10599", "title": "UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech", "authors": ["Jiaxuan Liu", "Zhenhua Ling"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recent neural codec language models have made great progress in the field of\ntext-to-speech (TTS), but controllable emotional TTS still faces many\nchallenges. Traditional methods rely on predefined discrete emotion labels to\ncontrol emotion categories and intensities, which can't capture the complexity\nand continuity of human emotional perception and expression. The lack of\nlarge-scale emotional speech datasets with balanced emotion distributions and\nfine-grained emotion annotations often causes overfitting in synthesis models\nand impedes effective emotion control. To address these issues, we propose\nUDDETTS, a neural codec language model unifying discrete and dimensional\nemotions for controllable emotional TTS. This model introduces the\ninterpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion\ndescription and supports emotion control driven by either discrete emotion\nlabels or nonlinearly quantified ADV values. Furthermore, a semi-supervised\ntraining strategy is designed to comprehensively utilize diverse speech\ndatasets with different types of emotion annotations to train the UDDETTS.\nExperiments show that UDDETTS achieves linear emotion control along the three\ndimensions of ADV space, and exhibits superior end-to-end emotional speech\nsynthesis capabilities."}
{"id": "2505.10610", "pdf": "https://arxiv.org/pdf/2505.10610.pdf", "abs": "https://arxiv.org/abs/2505.10610", "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "authors": ["Zhaowei Wang", "Wenhao Yu", "Xiyu Ren", "Jipeng Zhang", "Yu Zhao", "Rohit Saxena", "Liang Cheng", "Ginny Wong", "Simon See", "Pasquale Minervini", "Yangqiu Song", "Mark Steedman"], "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs."}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831.pdf", "abs": "https://arxiv.org/abs/2505.10831", "title": "Creating General User Models from Computer Use", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture that user knowledge and preferences. GUMs can infer that a user is\npreparing for a wedding they're attending from messages with a friend. Or\nrecognize that a user is struggling with a collaborator's feedback on a draft\nby observing multiple stalled edits and a switch to reading related work. GUMs\nintroduce an architecture that infers new propositions about a user from\nmultimodal observations, retrieves related propositions for context, and\ncontinuously revises existing propositions. To illustrate the breadth of\napplications that GUMs enable, we demonstrate how they augment chat-based\nassistants with context, manage OS notifications to selectively surface\nimportant information, and enable interactive agents that adapt to preferences\nacross apps. We also instantiate proactive assistants (GUMBOs) that discover\nand execute useful suggestions on a user's behalf using their GUM. In our\nevaluations, we find that GUMs make calibrated and accurate inferences about\nusers, and that assistants built on GUMs proactively identify and perform\nactions that users wouldn't think to request explicitly. Altogether, GUMs\nintroduce methods that leverage multimodal models to understand unstructured\ncontext, enabling long-standing visions of HCI and entirely new interactive\nsystems that anticipate user needs."}
{"id": "2505.10838", "pdf": "https://arxiv.org/pdf/2505.10838.pdf", "abs": "https://arxiv.org/abs/2505.10838", "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs", "authors": ["Ran Li", "Hao Wang", "Chengzhi Mao"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization."}
{"id": "2505.10844", "pdf": "https://arxiv.org/pdf/2505.10844.pdf", "abs": "https://arxiv.org/abs/2505.10844", "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "authors": ["Simeng Han", "Stephen Xia", "Grant Zhang", "Howard Dai", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "categories": ["cs.AI", "cs.CL"], "comment": "13 Tables; 5 Figures", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs."}
{"id": "2505.10852", "pdf": "https://arxiv.org/pdf/2505.10852.pdf", "abs": "https://arxiv.org/abs/2505.10852", "title": "MatTools: Benchmarking Large Language Models for Materials Science Tools", "authors": ["Siyu Liu", "Jiamin Xu", "Beilin Ye", "Bo Hu", "David J. Srolovitz", "Tongqi Wen"], "categories": ["cond-mat.mtrl-sci", "cs.CL", "cs.DB"], "comment": "27 pages, 23 figures", "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research."}
{"id": "2505.10872", "pdf": "https://arxiv.org/pdf/2505.10872.pdf", "abs": "https://arxiv.org/abs/2505.10872", "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?", "authors": ["Chenxi Jiang", "Chuhao Zhou", "Jianfei Yang"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Submitted to CoRL 2025, under review", "summary": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children."}
{"id": "2505.10981", "pdf": "https://arxiv.org/pdf/2505.10981.pdf", "abs": "https://arxiv.org/abs/2505.10981", "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory", "authors": ["Yexiang Liu", "Zekun Li", "Zhi Fang", "Nan Xu", "Ran He", "Tieniu Tan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 Main", "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a method according to probability theory to quickly\nand accurately predict the scaling performance and select the best strategy\nunder large sampling times without extra resource-intensive inference in\npractice. It can serve as the test-time scaling law for majority voting.\nFurthermore, we introduce two ways derived from our theoretical analysis to\nsignificantly improve the scaling performance. We hope that our research can\npromote to re-examine the role of complicated prompting, unleash the potential\nof simple prompting strategies, and provide new insights for enhancing\ntest-time scaling performance."}
{"id": "2505.11079", "pdf": "https://arxiv.org/pdf/2505.11079.pdf", "abs": "https://arxiv.org/abs/2505.11079", "title": "$\\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection", "authors": ["Hao Gu", "Jiangyan Yi", "Chenglong Wang", "Jianhua Tao", "Zheng Lian", "Jiayi He", "Yong Ren", "Yujie Chen", "Zhengqi Wen"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Audio deepfake detection (ADD) has grown increasingly important due to the\nrise of high-fidelity audio generative models and their potential for misuse.\nGiven that audio large language models (ALLMs) have made significant progress\nin various audio processing tasks, a heuristic question arises: Can ALLMs be\nleveraged to solve ADD?. In this paper, we first conduct a comprehensive\nzero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in\ndetecting fake audio. To enhance their performance, we propose\n$\\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we\nreformulate ADD task as an audio question answering problem, prompting the\nmodel with the question: \"Is this audio fake or real?\". We then perform\nsupervised fine-tuning to enable the ALLM to assess the authenticity of query\naudio. Extensive experiments are conducted to demonstrate that our ALLM-based\nmethod can achieve superior performance in fake audio detection, particularly\nin data-scarce scenarios. As a pioneering study, we anticipate that this work\nwill inspire the research community to leverage ALLMs to develop more effective\nADD systems."}
{"id": "2505.11154", "pdf": "https://arxiv.org/pdf/2505.11154.pdf", "abs": "https://arxiv.org/abs/2505.11154", "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Yu Liu", "Wenbo Jiang", "Wenshu Fan", "Qingchuan Zhao", "Guowen Xu"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem."}
{"id": "2505.11165", "pdf": "https://arxiv.org/pdf/2505.11165.pdf", "abs": "https://arxiv.org/abs/2505.11165", "title": "Maximizing Asynchronicity in Event-based Neural Networks", "authors": ["Haiqing Hao", "Nikola ZubiÄ", "Weihua He", "Zhipeng Sui", "Davide Scaramuzza", "Wenhui Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 figures, 9 tables", "summary": "Event cameras deliver visual data with high temporal resolution, low latency,\nand minimal redundancy, yet their asynchronous, sparse sequential nature\nchallenges standard tensor-based machine learning (ML). While the recent\nasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by\nasynchronously encoding events into learned representations for ML pipelines,\nexisting A2S approaches often sacrifice representation expressivity and\ngeneralizability compared to dense, synchronous methods. This paper introduces\nEVA (EVent Asynchronous representation learning), a novel A2S framework to\ngenerate highly expressive and generalizable event-by-event representations.\nInspired by the analogy between events and language, EVA uniquely adapts\nadvances from language modeling in linear attention and self-supervised\nlearning for its construction. In demonstration, EVA outperforms prior A2S\nmethods on recognition tasks (DVS128-Gesture and N-Cars), and represents the\nfirst A2S framework to successfully master demanding detection tasks, achieving\na remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's\ntransformative potential for advancing real-time event-based vision\napplications."}
{"id": "2505.11178", "pdf": "https://arxiv.org/pdf/2505.11178.pdf", "abs": "https://arxiv.org/abs/2505.11178", "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback", "authors": ["Yixin Wan", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches."}
{"id": "2505.11183", "pdf": "https://arxiv.org/pdf/2505.11183.pdf", "abs": "https://arxiv.org/abs/2505.11183", "title": "On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms", "authors": ["Jacob Trauger", "Ambuj Tewari"], "categories": ["stat.ML", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "Probabilistic next-token prediction trained using cross-entropy loss is the\nbasis of most large language models. Given a sequence of previous values,\nnext-token prediction assigns a probability to each possible next value in the\nvocabulary. There are many ways to use next-token prediction to output token\nsequences. This paper examines a few of these algorithms (greedy, lookahead,\nrandom sampling, and temperature-scaled random sampling) and studies their\nconsistency with respect to various goals encoded as loss functions. Although\nconsistency of surrogate losses with respect to a target loss function is a\nwell researched topic, we are the first to study it in the context of LLMs (to\nthe best of our knowledge). We find that, so long as next-token prediction\nconverges to its true probability distribution, random sampling is consistent\nwith outputting sequences that mimic sampling from the true probability\ndistribution. For the other goals, such as minimizing the 0-1 loss on the\nentire sequence, we show no polynomial-time algorithm is optimal for all\nprobability distributions and all decoding algorithms studied are only optimal\nfor a subset of probability distributions. When analyzing these results, we see\nthat there is a dichotomy created between the goals of information retrieval\nand creative generation for the decoding algorithms. This shows that choosing\nthe correct decoding algorithm based on the desired goal is extremely important\nand many of the ones used are lacking theoretical grounding in numerous\nscenarios."}
{"id": "2505.11200", "pdf": "https://arxiv.org/pdf/2505.11200.pdf", "abs": "https://arxiv.org/abs/2505.11200", "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese", "authors": ["Xihuai Wang", "Ziyi Zhao", "Siyu Ren", "Shao Zhang", "Song Li", "Xiaoyu Li", "Ziwen Wang", "Lin Qiu", "Guanglu Wan", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "comment": "Under Review", "summary": "Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4)."}
{"id": "2505.11274", "pdf": "https://arxiv.org/pdf/2505.11274.pdf", "abs": "https://arxiv.org/abs/2505.11274", "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning", "authors": ["Zheng Li", "Qingxiu Dong", "Jingyuan Ma", "Di Zhang", "Zhifang Sui"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy."}
{"id": "2505.11314", "pdf": "https://arxiv.org/pdf/2505.11314.pdf", "abs": "https://arxiv.org/abs/2505.11314", "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks", "authors": ["Christoph Leiter", "Yuki M. Asano", "Margret Keuper", "Steffen Eger"], "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts."}
{"id": "2505.11365", "pdf": "https://arxiv.org/pdf/2505.11365.pdf", "abs": "https://arxiv.org/abs/2505.11365", "title": "Phare: A Safety Probe for Large Language Models", "authors": ["Pierre Le Jeune", "BenoÃ®t MalÃ©sieux", "Weixuan Xiao", "Matteo Dora"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems."}
{"id": "2505.11405", "pdf": "https://arxiv.org/pdf/2505.11405.pdf", "abs": "https://arxiv.org/abs/2505.11405", "title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "authors": ["Bohao Xing", "Xin Liu", "Guoying Zhao", "Chengyu Liu", "Xiaolan Fu", "Heikki KÃ¤lviÃ¤inen"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Emotion understanding is a critical yet challenging task. Recent advances in\nMultimodal Large Language Models (MLLMs) have significantly enhanced their\ncapabilities in this area. However, MLLMs often suffer from hallucinations,\ngenerating irrelevant or nonsensical content. To the best of our knowledge,\ndespite the importance of this issue, there has been no dedicated effort to\nevaluate emotion-related hallucinations in MLLMs. In this work, we introduce\nEmotionHallucer, the first benchmark for detecting and analyzing emotion\nhallucinations in MLLMs. Unlike humans, whose emotion understanding stems from\nthe interplay of biology and social learning, MLLMs rely solely on data-driven\nlearning and lack innate emotional instincts. Fortunately, emotion psychology\nprovides a solid foundation of knowledge about human emotions. Building on\nthis, we assess emotion hallucinations from two dimensions: emotion psychology\nknowledge and real-world multimodal perception. To support robust evaluation,\nwe utilize an adversarial binary question-answer (QA) framework, which employs\ncarefully crafted basic and hallucinated pairs to assess the emotion\nhallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on\nEmotionHallucer, we reveal that: i) most current models exhibit substantial\nissues with emotion hallucinations; ii) closed-source models outperform\nopen-source ones in detecting emotion hallucinations, and reasoning capability\nprovides additional advantages; iii) existing models perform better in emotion\npsychology knowledge than in multimodal emotion perception. As a byproduct,\nthese findings inspire us to propose the PEP-MEK framework, which yields an\naverage improvement of 9.90% in emotion hallucination detection across selected\nmodels. Resources will be available at\nhttps://github.com/xxtars/EmotionHallucer."}
{"id": "2505.11406", "pdf": "https://arxiv.org/pdf/2505.11406.pdf", "abs": "https://arxiv.org/abs/2505.11406", "title": "Large Language Model Use Impact Locus of Control", "authors": ["Jenny Xiyu Fu", "Brennan Antone", "Kowe Kadoma", "Malte Jung"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity."}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409.pdf", "abs": "https://arxiv.org/abs/2505.11409", "title": "Visual Planning: Let's Think Only with Images", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan VuliÄ"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference."}
{"id": "2305.15099", "pdf": "https://arxiv.org/pdf/2305.15099.pdf", "abs": "https://arxiv.org/abs/2305.15099", "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator", "authors": ["Ziwei He", "Meng Yang", "Minwei Feng", "Jingcheng Yin", "Xinbing Wang", "Jingwen Leng", "Zhouhan Lin"], "categories": ["cs.CL"], "comment": null, "summary": "The transformer model is known to be computationally demanding, and\nprohibitively costly for long sequences, as the self-attention module uses a\nquadratic time and space complexity with respect to sequence length. Many\nresearchers have focused on designing new forms of self-attention or\nintroducing new parameters to overcome this limitation, however a large portion\nof them prohibits the model to inherit weights from large pretrained models. In\nthis work, the transformer's inefficiency has been taken care of from another\nperspective. We propose Fourier Transformer, a simple yet effective approach by\nprogressively removing redundancies in hidden sequence using the ready-made\nFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation\n(DCT). Fourier Transformer is able to significantly reduce computational costs\nwhile retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among\nall transformer-based models on the long-range modeling benchmark LRA with\nsignificant improvement in both speed and space. For generative seq-to-seq\ntasks including CNN/DailyMail and ELI5, by inheriting the BART weights our\nmodel outperforms the standard BART and other efficient models. Our code is\npublicly available at https://github.com/LUMIA-Group/FourierTransformer"}
{"id": "2311.07564", "pdf": "https://arxiv.org/pdf/2311.07564.pdf", "abs": "https://arxiv.org/abs/2311.07564", "title": "Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?", "authors": ["Cristina Aggazzotti", "Nicholas Andrews", "Elizabeth Allyn Smith"], "categories": ["cs.CL", "cs.LG"], "comment": "Published in Transactions of the Association for Computational\n  Linguistics; 1st revision includes additional experiments and evaluations;\n  2nd revision includes minor tweak to TFIDF table numbers", "summary": "Authorship verification is the task of determining if two distinct writing\nsamples share the same author and is typically concerned with the attribution\nof written text. In this paper, we explore the attribution of transcribed\nspeech, which poses novel challenges. The main challenge is that many stylistic\nfeatures, such as punctuation and capitalization, are not informative in this\nsetting. On the other hand, transcribed speech exhibits other patterns, such as\nfiller words and backchannels (e.g., 'um', 'uh-huh'), which may be\ncharacteristic of different speakers. We propose a new benchmark for speaker\nattribution focused on human-transcribed conversational speech transcripts. To\nlimit spurious associations of speakers with topic, we employ both conversation\nprompts and speakers participating in the same conversation to construct\nverification trials of varying difficulties. We establish the state of the art\non this new benchmark by comparing a suite of neural and non-neural baselines,\nfinding that although written text attribution models achieve surprisingly good\nperformance in certain settings, they perform markedly worse as conversational\ntopic is increasingly controlled. We present analyses of the impact of\ntranscription style on performance as well as the ability of fine-tuning on\nspeech transcripts to improve performance."}
{"id": "2402.14889", "pdf": "https://arxiv.org/pdf/2402.14889.pdf", "abs": "https://arxiv.org/abs/2402.14889", "title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "authors": ["Priyanshul Govil", "Hemang Jain", "Vamshi Krishna Bonagiri", "Aman Chadha", "Ponnurangam Kumaraguru", "Manas Gaur", "Sanorita Dey"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias, based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augmented 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65, p = 3.4 * 10^{-60}$)\nand can be used to create reliable benchmarks, which would assist bias\nmitigation works."}
{"id": "2404.10652", "pdf": "https://arxiv.org/pdf/2404.10652.pdf", "abs": "https://arxiv.org/abs/2404.10652", "title": "ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images", "authors": ["Quan Van Nguyen", "Dan Quang Tran", "Huy Quang Pham", "Thang Kien-Bao Nguyen", "Nghia Hieu Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": null, "summary": "Visual Question Answerinng (VQA) is a complicated task that requires the\ncapability of simultaneously processing natural language and images. This task\nwas initially researched with a focus on developing methods to help machines\nunderstand objects and scene contexts in images. However, some scene text that\ncarries explicit information about the full content of the image is not\nmentioned. Along with the continuous development of the AI era, there have been\nmany studies on the reading comprehension ability of VQA models in the world.\nTherefore, we introduce the first large-scale dataset in Vietnamese\nspecializing in the ability to understand scene text, we call it ViTextVQA\n(\\textbf{Vi}etnamese \\textbf{Text}-based \\textbf{V}isual \\textbf{Q}uestion\n\\textbf{A}nswering dataset) which contains \\textbf{over 16,000} images and\n\\textbf{over 50,000} questions with answers. To tackle this task efficiently,\nwe propose ViTextBLIP-2, an novel multimodal feature fusion Method, which\noptimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer,\nSwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal\nfeature fusion. Through experiments with various state-of-the-art models, we\nuncover the significance of the order in which tokens in OCR text are processed\nand selected to formulate answers. This finding helped us significantly improve\nthe performance of the baseline models on the ViTextVQA dataset. Our dataset is\navailable (https://github.com/minhquan6203/ViTextVQA-Dataset) for research\npurposes."}
{"id": "2405.00715", "pdf": "https://arxiv.org/pdf/2405.00715.pdf", "abs": "https://arxiv.org/abs/2405.00715", "title": "Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation", "authors": ["Hanyin Wang", "Chufan Gao", "Bolun Liu", "Qiping Xu", "Guleid Hussein", "Mohamad El Labban", "Kingsley Iheasirim", "Hariprasad Korsapati", "Chuck Outcalt", "Jimeng Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across all three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than\nphysician-authored notes (4.1/5). We highlight key considerations for future\nclinical note-generation tasks, emphasizing the importance of pre-defining a\nbest-practice note format, rather than relying on LLMs to determine this for\nclinical practice."}
{"id": "2406.06326", "pdf": "https://arxiv.org/pdf/2406.06326.pdf", "abs": "https://arxiv.org/abs/2406.06326", "title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching", "authors": ["Xiaoying Zhang", "Baolin Peng", "Ye Tian", "Jingyan Zhou", "Yipeng Zhang", "Haitao Mi", "Helen Meng"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from unseen raw documents through self-teaching. Specifically, we\ndevelop a Self-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection.\nAdditionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate\nan in-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nvarious models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits\nsuperior performance across all knowledge acquisition tasks and excels in\npreserving previous knowledge."}
{"id": "2408.06518", "pdf": "https://arxiv.org/pdf/2408.06518.pdf", "abs": "https://arxiv.org/abs/2408.06518", "title": "Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models", "authors": ["Hila Gonen", "Terra Blevins", "Alisa Liu", "Luke Zettlemoyer", "Noah A. Smith"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior."}
{"id": "2409.09636", "pdf": "https://arxiv.org/pdf/2409.09636.pdf", "abs": "https://arxiv.org/abs/2409.09636", "title": "Towards understanding evolution of science through language model series", "authors": ["Junjie Dong", "Zhuoqi Lyu", "Qing Ke"], "categories": ["cs.CL", "cs.CY", "cs.DL"], "comment": null, "summary": "We introduce AnnualBERT, a series of language models designed specifically to\ncapture the temporal evolution of scientific text. Deviating from the\nprevailing paradigms of subword tokenizations and \"one model to rule them all\",\nAnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model\npretrained from scratch on the full-text of 1.7 million arXiv papers published\nuntil 2008 and a collection of progressively trained models on arXiv papers at\nan annual basis. We demonstrate the effectiveness of AnnualBERT models by\nshowing that they not only have comparable performances in standard tasks but\nalso achieve state-of-the-art performances on domain-specific NLP tasks as well\nas link prediction tasks in the arXiv citation network. We then utilize probing\ntasks to quantify the models' behavior in terms of representation learning and\nforgetting as time progresses. Our approach enables the pretrained models to\nnot only improve performances on scientific text processing tasks but also to\nprovide insights into the development of scientific discourse over time. The\nseries of the models is available at https://huggingface.co/jd445/AnnualBERTs."}
{"id": "2409.20204", "pdf": "https://arxiv.org/pdf/2409.20204.pdf", "abs": "https://arxiv.org/abs/2409.20204", "title": "Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach", "authors": ["Aditi Dutta", "Susan Banducci", "Chico Q. Camargo"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Several computational tools have been developed to detect and identify\nsexism, misogyny, and gender-based hate speech, particularly on online\nplatforms. These tools draw on insights from both social science and computer\nscience. Given the increasing concern over gender-based discrimination in\ndigital spaces, the contested definitions and measurements of sexism, and the\nrise of interdisciplinary efforts to understand its online manifestations, a\nsystematic literature review is essential for capturing the current state and\ntrajectory of this evolving field. In this review, we make four key\ncontributions: (1) we synthesize the literature into five core themes:\ndefinitions of sexism and misogyny, disciplinary divergences, automated\ndetection methods, associated challenges, and design-based interventions; (2)\nwe adopt an interdisciplinary lens, bridging theoretical and methodological\ndivides across disciplines; (3) we highlight critical gaps, including the need\nfor intersectional approaches, the under-representation of non-Western\nlanguages and perspectives, and the limited focus on proactive design\nstrategies beyond text classification; and (4) we offer a methodological\ncontribution by applying a rigorous semi-automated systematic review process\nguided by PRISMA, establishing a replicable standard for future work in this\ndomain. Our findings reveal a clear disciplinary divide in how sexism and\nmisogyny are conceptualized and measured. Through an evidence-based synthesis,\nwe examine how existing studies have attempted to bridge this gap through\ninterdisciplinary collaboration. Drawing on both social science theories and\ncomputational modeling practices, we assess the strengths and limitations of\ncurrent methodologies. Finally, we outline key challenges and future directions\nfor advancing research on the detection and mitigation of online sexism and\nmisogyny."}
{"id": "2410.16392", "pdf": "https://arxiv.org/pdf/2410.16392.pdf", "abs": "https://arxiv.org/abs/2410.16392", "title": "Training of Scaffolded Language Models with Language Supervision: A Survey", "authors": ["Matthieu Lin", "Jenny Sheng", "Andrew Zhao", "Shenzhi Wang", "Yang Yue", "Victor Shea Jay Huang", "Huan Liu", "Jun Liu", "Gao Huang", "Yong-Jin Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This survey organizes the intricate literature on the design and optimization\nof emerging structures around post-trained LMs. We refer to this overarching\nstructure as scaffolded LMs and focus on LMs that are integrated into\nmulti-step processes with tools. We view scaffolded LMs as semi-parametric\nmodels wherein we train non-parametric variables, including the prompt, tools,\nand scaffold's code. In particular, they interpret instructions, use tools, and\nreceive feedback all in language. Recent works use an LM as an optimizer to\ninterpret language supervision and update non-parametric variables according to\nintricate objectives. In this survey, we refer to this paradigm as training of\nscaffolded LMs with language supervision. A key feature of non-parametric\ntraining is the ability to learn from language. Parametric training excels in\nlearning from demonstration (supervised learning), exploration (reinforcement\nlearning), or observations (unsupervised learning), using well-defined loss\nfunctions. Language-based optimization enables rich, interpretable, and\nexpressive objectives, while mitigating issues like catastrophic forgetting and\nsupporting compatibility with closed-source models. Furthermore, agents are\nincreasingly deployed as co-workers in real-world applications such as Copilot\nin Office tools or software development. In these mixed-autonomy settings,\nwhere control and decision-making are shared between human and AI, users point\nout errors or suggest corrections. Accordingly, we discuss agents that\ncontinuously improve by learning from this real-time, language-based feedback\nand refer to this setting as streaming learning from language supervision."}
{"id": "2410.19453", "pdf": "https://arxiv.org/pdf/2410.19453.pdf", "abs": "https://arxiv.org/abs/2410.19453", "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework", "authors": ["Hengyuan Zhang", "Chenming Shang", "Sizhe Wang", "Dongdong Zhang", "Feng Yao", "Renliang Sun", "Yiyao Yu", "Yujiu Yang", "Furu Wei"], "categories": ["cs.CL"], "comment": "23 pages, 11 figures", "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"}
{"id": "2411.05527", "pdf": "https://arxiv.org/pdf/2411.05527.pdf", "abs": "https://arxiv.org/abs/2411.05527", "title": "How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP", "authors": ["Kushal Tatariya", "Artur Kulmizev", "Wessel Poelman", "Esther Ploeger", "Marcel Bollmann", "Johannes Bjerva", "Jiaming Luo", "Heather Lent", "Miryam de Lhoneux"], "categories": ["cs.CL"], "comment": null, "summary": "Wikipedia's perceived high quality and broad language coverage have\nestablished it as a fundamental resource in multilingual NLP. In the context of\nlow-resource languages, however, these quality assumptions are increasingly\nbeing scrutinised. This paper critically examines the data quality of Wikipedia\nin a non-English setting by subjecting it to various quality filtering\ntechniques, revealing widespread issues such as a high percentage of one-line\narticles and duplicate articles. We evaluate the downstream impact of quality\nfiltering on Wikipedia and find that data quality pruning is an effective means\nfor resource-efficient training without hurting performance, especially for\nlow-resource languages. Moreover, we advocate for a shift in perspective from\nseeking a general definition of data quality towards a more language- and\ntask-specific one. Ultimately, we aim for this study to serve as a guide to\nusing Wikipedia for pretraining in a multilingual setting."}
{"id": "2411.07019", "pdf": "https://arxiv.org/pdf/2411.07019.pdf", "abs": "https://arxiv.org/abs/2411.07019", "title": "UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction", "authors": ["Zhiqiang Liu", "Yin Hua", "Mingyang Chen", "Zhuo Chen", "Ziqi Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Beyond-triple fact representations including hyper-relational facts with\nauxiliary key-value pairs, temporal facts with additional timestamps, and\nnested facts implying relationships between facts, are gaining significant\nattention. However, constrained by complex fact representation forms, existing\nlink prediction models for beyond-triple facts have difficulty achieving\nhierarchical fact modeling and generalizing the modules for one specific facts\nto other fact types. To overcome this limitation, we propose a Unified\nHierarchical Representation learning framework (UniHR) for unified knowledge\ngraph link prediction. It consists of a unified Hierarchical Data\nRepresentation (HiDR) module and a unified Hierarchical Structure Learning\n(HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs,\ntemporal KGs, and nested factual KGs into triple-based representations. Then\nHiSL incorporates intra-fact and inter-fact message passing, focusing on\nenhancing the semantic information within individual facts and enriching the\nstructural information between facts. Empirical results demonstrate the\neffectiveness of UniHR and highlight the strong potential of unified\nrepresentations. Code and data are available at\nhttps://github.com/Lza12a/UniHR."}
{"id": "2411.08135", "pdf": "https://arxiv.org/pdf/2411.08135.pdf", "abs": "https://arxiv.org/abs/2411.08135", "title": "On the Role of Speech Data in Reducing Toxicity Detection Bias", "authors": ["Samuel J. Bell", "Mariano Coria Meglioli", "Megan Richards", "Eduardo SÃ¡nchez", "Christophe Ropers", "Skyler Wang", "Adina Williams", "Levent Sagun", "Marta R. Costa-jussÃ "], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at NAACL 2025", "summary": "Text toxicity detection systems exhibit significant biases, producing\ndisproportionate rates of false positives on samples mentioning demographic\ngroups. But what about toxicity detection in speech? To investigate the extent\nto which text-based biases are mitigated by speech-based systems, we produce a\nset of high-quality group annotations for the multilingual MuTox dataset, and\nthen leverage these annotations to systematically compare speech- and\ntext-based toxicity classifiers. Our findings indicate that access to speech\ndata during inference supports reduced bias against group mentions,\nparticularly for ambiguous and disagreement-inducing samples. Our results also\nsuggest that improving classifiers, rather than transcription pipelines, is\nmore helpful for reducing group bias. We publicly release our annotations and\nprovide recommendations for future toxicity dataset construction."}
{"id": "2412.11142", "pdf": "https://arxiv.org/pdf/2412.11142.pdf", "abs": "https://arxiv.org/abs/2412.11142", "title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection", "authors": ["Tiankai Yang", "Yi Nian", "Shawn Li", "Ruiyao Xu", "Yuangang Li", "Jiaqi Li", "Zhuo Xiao", "Xiyang Hu", "Ryan Rossi", "Kaize Ding", "Xia Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is an important machine learning task with many\nreal-world uses, including fraud detection, medical diagnosis, and industrial\nmonitoring. Within natural language processing (NLP), AD helps detect issues\nlike spam, misinformation, and unusual user activity. Although large language\nmodels (LLMs) have had a strong impact on tasks such as text generation and\nsummarization, their potential in AD has not been studied enough. This paper\nintroduces AD-LLM, the first benchmark that evaluates how LLMs can help with\nNLP anomaly detection. We examine three key tasks: (i) zero-shot detection,\nusing LLMs' pre-trained knowledge to perform AD without tasks-specific\ntraining; (ii) data augmentation, generating synthetic data and category\ndescriptions to improve AD models; and (iii) model selection, using LLMs to\nsuggest unsupervised AD models. Through experiments with different datasets, we\nfind that LLMs can work well in zero-shot AD, that carefully designed\naugmentation methods are useful, and that explaining model selection for\nspecific datasets remains challenging. Based on these results, we outline six\nfuture research directions on LLMs for AD."}
{"id": "2412.12527", "pdf": "https://arxiv.org/pdf/2412.12527.pdf", "abs": "https://arxiv.org/abs/2412.12527", "title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention", "authors": ["Hyuhng Joon Kim", "Youna Kim", "Sang-goo Lee", "Taeuk Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 (main)", "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\ndiverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e.,\ncontextual) knowledge. While substantial efforts have been made to enhance the\nutilization of both forms of knowledge, situations in which models lack\nrelevant information remain underexplored. To investigate this challenge, we\nfirst present a controlled testbed featuring four distinct knowledge access\nscenarios, including the aforementioned edge case, revealing that conventional\nLLM usage exhibits insufficient robustness in handling all instances.\nAddressing this limitation, we propose Contrastive Decoding with Abstention\n(CDA), a novel training-free decoding method that allows LLMs to generate\nresponses when relevant knowledge is available and to abstain otherwise. CDA\nestimates the relevance of both knowledge sources for a given input, adaptively\ndeciding which type of information to prioritize and which to exclude. Through\nextensive experiments, we demonstrate that CDA can effectively perform accurate\ngeneration and abstention simultaneously, enhancing reliability and preserving\nuser trust."}
{"id": "2412.12632", "pdf": "https://arxiv.org/pdf/2412.12632.pdf", "abs": "https://arxiv.org/abs/2412.12632", "title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context", "authors": ["Zhiyuan Chang", "Mingyang Li", "Xiaojun Jia", "Junjie Wang", "Yuekai Huang", "Qing Wang", "Yihao Huang", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 figures", "summary": "Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nevaluate LLMs' effectiveness, faithfulness and robustness with CoE, including\nits application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs\nshow CoE improves generation accuracy, answer faithfulness, robustness to\nknowledge conflicts, and boosts the performance of existing approaches in three\npractical RAG scenarios."}
{"id": "2412.15529", "pdf": "https://arxiv.org/pdf/2412.15529.pdf", "abs": "https://arxiv.org/abs/2412.15529", "title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation", "authors": ["Qianren Mao", "Yangyifei Luo", "Qili Zhang", "Yashuo Luo", "Zhilong Cao", "Jinlong Zhang", "HanWen Hao", "Zhijun Chen", "Weifeng Jiang", "Junnan Liu", "Xiaolong Wang", "Zhenting Huang", "Zhixing Tan", "Sun Jie", "Bo Li", "Xudong Liu", "Richong Zhang", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points."}
{"id": "2501.00745", "pdf": "https://arxiv.org/pdf/2501.00745.pdf", "abs": "https://arxiv.org/abs/2501.00745", "title": "Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines", "authors": ["Xiyang Hu"], "categories": ["cs.CL", "cs.AI", "cs.GT", "cs.IR", "econ.TH"], "comment": null, "summary": "The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems\nare vulnerable to adversarial attacks, especially ranking manipulation attacks,\nwhere attackers craft webpage content to manipulate the LLM's ranking and\npromote specific content, gaining an unfair advantage over competitors. In this\npaper, we study the dynamics of ranking manipulation attacks. We frame this\nproblem as an Infinitely Repeated Prisoners' Dilemma, where multiple players\nstrategically decide whether to cooperate or attack. We analyze the conditions\nunder which cooperation can be sustained, identifying key factors such as\nattack costs, discount rates, attack success rates, and trigger strategies that\ninfluence player behavior. We identify tipping points in the system dynamics,\ndemonstrating that cooperation is more likely to be sustained when players are\nforward-looking. However, from a defense perspective, we find that simply\nreducing attack success probabilities can, paradoxically, incentivize attacks\nunder certain conditions. Furthermore, defensive measures to cap the upper\nbound of attack success rates may prove futile in some scenarios. These\ninsights highlight the complexity of securing LLM-based systems. Our work\nprovides a theoretical foundation and practical insights for understanding and\nmitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design."}
{"id": "2501.03266", "pdf": "https://arxiv.org/pdf/2501.03266.pdf", "abs": "https://arxiv.org/abs/2501.03266", "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena", "authors": ["Stefan Pasch"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": null, "summary": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. In particular,\nlittle is known about how users respond when models refuse to answer a\nprompt-one of the primary mechanisms used to enforce ethical boundaries in\nLLMs. We address this gap by analyzing nearly 50,000 model comparisons from\nChatbot Arena, a platform where users indicate their preferred LLM response in\npairwise matchups, providing a large-scale setting for studying real-world user\npreferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a\nhand-labeled dataset, we distinguish between refusals due to ethical concerns\nand technical limitations. Our results reveal a substantial refusal penalty:\nethical refusals yield significantly lower win rates than both technical\nrefusals and standard responses, indicating that users are especially\ndissatisfied when models decline a task for ethical reasons. However, this\npenalty is not uniform. Refusals receive more favorable evaluations when the\nunderlying prompt is highly sensitive (e.g., involving illegal content), and\nwhen the refusal is phrased in a detailed and contextually aligned manner.\nThese findings underscore a core tension in LLM design: safety-aligned\nbehaviors may conflict with user expectations, calling for more adaptive\nmoderation strategies that account for context and presentation."}
{"id": "2501.04987", "pdf": "https://arxiv.org/pdf/2501.04987.pdf", "abs": "https://arxiv.org/abs/2501.04987", "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures", "authors": ["Ziwei He", "Jian Yuan", "Haoli Bai", "Jingwen Leng", "Bo Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."}
{"id": "2501.10316", "pdf": "https://arxiv.org/pdf/2501.10316.pdf", "abs": "https://arxiv.org/abs/2501.10316", "title": "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling", "authors": ["Suvodip Dey", "Yi-Jyun Sun", "Gokhan Tur", "Dilek Hakkani-Tur"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well known to hallucinate, producing responses that seem\nplausible but are factually incorrect. On the other hand, users tend to\nover-rely on LLM-based AI agents, accepting AI's suggestion even when it is\nwrong. Adding positive friction, such as explanations or getting user\nconfirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head that functions as a binary classifier to\npredict the relevant slots of the dialogue state mentioned in the conversation.\nWe perform our experiments with multiple backbone LLMs on two established\nbenchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the\nproposed approach not only enables reliable estimation of AI agent errors but\nalso guides the decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy (JGA) of DST output by\nincorporating accountability heads into modern LLMs. Self-correcting the\ndetected errors further increases the JGA from 67.13 to 70.51, achieving\nstate-of-the-art DST performance. Finally, we show that error correction\nthrough user confirmations (friction turn) achieves a similar performance gain,\nhighlighting its potential to reduce user overreliance."}
{"id": "2501.11885", "pdf": "https://arxiv.org/pdf/2501.11885.pdf", "abs": "https://arxiv.org/abs/2501.11885", "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine", "authors": ["Keer Lu", "Zheng Liang", "Zhuoran Zhang", "Da Pan", "Shusen Zhang", "Xin Wu", "Zenan Zhou", "Guosheng Dong", "Bin Cui", "Tengjiao Wang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited remarkable capabilities in\nclinical scenarios. Despite their potential, existing works face challenges\nwhen applying LLMs to medical settings. Strategies relying on training with\nmedical datasets are highly cost-intensive and may suffer from outdated\ntraining data. Leveraging external knowledge bases is a suitable alternative,\nyet it faces obstacles such as limited retrieval precision and poor\neffectiveness in answer extraction. These issues collectively prevent LLMs from\ndemonstrating the expected level of proficiency in mastering medical expertise.\nTo address these challenges, we introduce Med-R^2, a novel LLM physician\nframework that adheres to the Evidence-Based Medicine (EBM) process,\nefficiently integrating retrieval mechanisms as well as the selection and\nreasoning processes of evidence, thereby enhancing the problem-solving\ncapabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM\nphysician. Our comprehensive experiments indicate that Med-R^2 achieves a\n14.74\\% improvement over vanilla RAG methods and even a 3.32\\% enhancement\ncompared to fine-tuning strategies, without incurring additional training\ncosts."}
{"id": "2501.13977", "pdf": "https://arxiv.org/pdf/2501.13977.pdf", "abs": "https://arxiv.org/abs/2501.13977", "title": "Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms", "authors": ["Rajvardhan Oak", "Muhammad Haroon", "Claire Jo", "Magdalena Wojcieszak", "Anshuman Chhabra"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Social media platforms utilize Machine Learning (ML) and Artificial\nIntelligence (AI) powered recommendation algorithms to maximize user\nengagement, which can result in inadvertent exposure to harmful content.\nCurrent moderation efforts, reliant on classifiers trained with extensive\nhuman-annotated data, struggle with scalability and adapting to new forms of\nharm. To address these challenges, we propose a novel re-ranking approach using\nLarge Language Models (LLMs) in zero-shot and few-shot settings. Our method\ndynamically assesses and re-ranks content sequences, effectively mitigating\nharmful content exposure without requiring extensive labeled data. Alongside\ntraditional ranking metrics, we also introduce two new metrics to evaluate the\neffectiveness of re-ranking in reducing exposure to harmful content. Through\nexperiments on three datasets, three models and across three configurations, we\ndemonstrate that our LLM-based approach significantly outperforms existing\nproprietary moderation approaches, offering a scalable and adaptable solution\nfor harm mitigation."}
{"id": "2502.06604", "pdf": "https://arxiv.org/pdf/2502.06604.pdf", "abs": "https://arxiv.org/abs/2502.06604", "title": "Do we really have to filter out random noise in pre-training data for language models?", "authors": ["Jinghan Ru", "Yuxin Xie", "Xianwei Zhuang", "Yuguo Yin", "Zhihui Guo", "Zhiming Liu", "Qianli Ren", "Yuexian Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the Internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation of such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in the loss of next-token prediction (NTP) was significantly lower\nthan the proportion of random noise even when the model was scaled up to 2.7B.\nWe provide a theoretical justification for this phenomenon, which also\nelucidates the success of multilingual models and can be applied to multimodal\nmodels. On the other hand, experiments show that the model's performance in\ndownstream tasks is not based solely on the NTP loss, which means that random\nnoise may result in degraded downstream performance. To address the potential\nadverse effects, we introduce a novel plug-and-play Local Gradient Matching\nloss, which explicitly enhances the denoising capability of the downstream task\nhead by aligning the gradient of normal and perturbed features without\nrequiring knowledge of the model's parameters. Additional experiments on 8\nlanguage and 14 vision benchmarks further validate its effectiveness."}
{"id": "2502.06876", "pdf": "https://arxiv.org/pdf/2502.06876.pdf", "abs": "https://arxiv.org/abs/2502.06876", "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging", "authors": ["Jinluan Yang", "Dingnan Jin", "Anke Tang", "Li Shen", "Didi Zhu", "Zhengyu Chen", "Ziyu Zhao", "Daixin Wang", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI. Existing methods like data mixture strategies\nface limitations, including heavy reliance on expert knowledge and conflicting\noptimization signals. While model merging offers parameter-level\nconflict-resolution strategies through integrating specialized models'\nparameters, its potential for 3H optimization remains underexplored. This paper\nsystematically compares the effectiveness of model merging and data mixture\nmethods in constructing 3H-aligned LLMs for the first time, revealing\npreviously overlooked collaborative and conflict relationships among the 3H\ndimensions and discussing the advantages and drawbacks of data mixture\n(\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in\nmitigating the conflict for balanced 3H optimization. Specially, we propose a\nnovel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular\n\\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and\nsparsity-aware rank selection strategies to address the challenges of\npreference noise accumulation and layer sparsity adaptation inherent in\n3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and\nrobustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model\nmerging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release\nour models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for\nfurther investigations."}
{"id": "2502.07490", "pdf": "https://arxiv.org/pdf/2502.07490.pdf", "abs": "https://arxiv.org/abs/2502.07490", "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More", "authors": ["Xialie Zhuang", "Zhikai Jia", "Jianjin Li", "Zhenyu Zhang", "Li Shen", "Zheng Cao", "Shiwei Liu"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages,7 figures", "summary": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models."}
{"id": "2502.08666", "pdf": "https://arxiv.org/pdf/2502.08666.pdf", "abs": "https://arxiv.org/abs/2502.08666", "title": "Hallucination, Monofacts, and Miscalibration: An Empirical Investigation", "authors": ["Miranda Muqing Miao", "Michael Kearns"], "categories": ["cs.CL", "cs.AI"], "comment": "Code available at https://github.com/mmiao2/Hallucination.git", "summary": "Hallucinated facts in large language models (LLMs) have recently been shown\nto obey a statistical lower bound determined by the monofact rate (related to\nthe classical Good-Turing missing mass estimator) minus model miscalibration\n(Kalai & Vempala, 2024). We present the first empirical investigation of this\nthree-way relationship in classical n-gram models and fine-tuned\nencoder-decoder Transformers. By generating training data from Pareto\ndistributions with varying shape parameters, we systematically control the\nmonofact rates and establish its positive relationship with hallucination. To\nbridge theory and practice, we derive an empirical analog of the hallucination\nbound by replacing the population miscalibration term (Section 2.1) with an\nempirical bin-wise KL divergence and confirm its practical viability. We then\nintroduce selective upweighting -- a simple yet effective technique that\nstrategically repeats as little as 5% of training examples -- to deliberately\ninject miscalibration into the model. This intervention reduces hallucination\nby up to 40%, challenging universal deduplication policies. Our experiments\nreveal a critical trade-off: selective upweighting maintains pre-injection\nlevels of accuracy while substantially reducing hallucination, whereas standard\ntraining gradually improves accuracy but fails to address persistently high\nhallucination, indicating an inherent tension in optimization objectives."}
{"id": "2502.11175", "pdf": "https://arxiv.org/pdf/2502.11175.pdf", "abs": "https://arxiv.org/abs/2502.11175", "title": "Investigating Language Preference of Multilingual RAG Systems", "authors": ["Jeonghyun Park", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language\nmodels by integrating external multilingual information to produce\ncontext-aware responses. However, mRAG systems struggle with retrieving\nrelevant information due to linguistic variations between queries and\ndocuments, generating inconsistent responses when multilingual sources\nconflict. In this work, we systematically investigate language preferences in\nboth retrieval and generation of mRAG through a series of experiments. Our\nanalysis indicates that retrievers tend to prefer high-resource and query\nlanguages, yet this preference does not consistently improve generation\nperformance. Moreover, we observe that generators prefer the query language or\nLatin scripts, leading to inconsistent outputs. To overcome these issues, we\npropose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective\nframework that fuses translated multilingual passages with complementary model\nknowledge. Empirical results demonstrate that DKM-RAG mitigates language\npreference in generation and enhances performance across diverse linguistic\nsettings."}
{"id": "2502.11948", "pdf": "https://arxiv.org/pdf/2502.11948.pdf", "abs": "https://arxiv.org/abs/2502.11948", "title": "Can Your Uncertainty Scores Detect Hallucinated Entity?", "authors": ["Min-Hsuan Yeh", "Max Kamachee", "Seongheon Park", "Yixuan Li"], "categories": ["cs.CL"], "comment": null, "summary": "To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research. HalluEntity:\nhttps://huggingface.co/datasets/samuelyeh/HalluEntity"}
{"id": "2502.14662", "pdf": "https://arxiv.org/pdf/2502.14662.pdf", "abs": "https://arxiv.org/abs/2502.14662", "title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL 2025 and WWW2025@HCRS", "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure."}
{"id": "2502.15208", "pdf": "https://arxiv.org/pdf/2502.15208.pdf", "abs": "https://arxiv.org/abs/2502.15208", "title": "Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing", "authors": ["Zhilin Wang", "Yafu Li", "Jianhao Yan", "Yu Cheng", "Yue Zhang"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "9 pages", "summary": "Dynamical systems theory provides a framework for analyzing iterative\nprocesses and evolution over time. Within such systems, repetitive\ntransformations can lead to stable configurations, known as attractors,\nincluding fixed points and limit cycles. Applying this perspective to large\nlanguage models (LLMs), which iteratively map input text to output text,\nprovides a principled approach to characterizing long-term behaviors.\nSuccessive paraphrasing serves as a compelling testbed for exploring such\ndynamics, as paraphrases re-express the same underlying meaning with linguistic\nvariation. Although LLMs are expected to explore a diverse set of paraphrases\nin the text space, our study reveals that successive paraphrasing converges to\nstable periodic states, such as 2-period attractor cycles, limiting linguistic\ndiversity. This phenomenon is attributed to the self-reinforcing nature of\nLLMs, as they iteratively favour and amplify certain textual forms over others.\nThis pattern persists with increasing generation randomness or alternating\nprompts and LLMs. These findings underscore inherent constraints in LLM\ngenerative capability, while offering a novel dynamical systems perspective for\nstudying their expressive potential."}
{"id": "2503.04807", "pdf": "https://arxiv.org/pdf/2503.04807.pdf", "abs": "https://arxiv.org/abs/2503.04807", "title": "Call for Rigor in Reporting Quality of Instruction Tuning Data", "authors": ["Hyeonseok Moon", "Jaehyung Seo", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the ACL2025-main", "summary": "Instruction tuning is crucial for adapting large language models (LLMs) to\nalign with user intentions. Numerous studies emphasize the significance of the\nquality of instruction tuning (IT) data, revealing a strong correlation between\nIT data quality and the alignment performance of LLMs. In these studies, the\nquality of IT data is typically assessed by evaluating the performance of LLMs\ntrained with that data. However, we identified a prevalent issue in such\npractice: hyperparameters for training models are often selected arbitrarily\nwithout adequate justification. We observed significant variations in\nhyperparameters applied across different studies, even when training the same\nmodel with the same data. In this study, we demonstrate the potential problems\narising from this practice and emphasize the need for careful consideration in\nverifying data quality. Through our experiments on the quality of LIMA data and\na selected set of 1,000 Alpaca data points, we demonstrate that arbitrary\nhyperparameter decisions can make any arbitrary conclusion."}
{"id": "2503.10995", "pdf": "https://arxiv.org/pdf/2503.10995.pdf", "abs": "https://arxiv.org/abs/2503.10995", "title": "TigerLLM -- A Family of Bangla Large Language Models", "authors": ["Nishat Raihan", "Marcos Zampieri"], "categories": ["cs.CL"], "comment": null, "summary": "The development of Large Language Models (LLMs) remains heavily skewed\ntowards English and a few other high-resource languages. This linguistic\ndisparity is particularly evident for Bangla - the 5th most spoken language. A\nfew initiatives attempted to create open-source Bangla LLMs with performance\nstill behind high-resource languages and limited reproducibility. To address\nthis gap, we introduce TigerLLM - a family of Bangla LLMs. Our results\ndemonstrate that these models surpass all open-source alternatives and also\noutperform larger proprietary models like GPT3.5 across standard benchmarks,\nestablishing TigerLLM as the new baseline for future Bangla language modeling."}
{"id": "2503.16525", "pdf": "https://arxiv.org/pdf/2503.16525.pdf", "abs": "https://arxiv.org/abs/2503.16525", "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse", "authors": ["Huan Yang", "Renji Zhang", "Mingzhe Huang", "Weijun Wang", "Yin Tang", "Yuanchun Li", "Yunxin Liu", "Deyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods."}
{"id": "2503.16529", "pdf": "https://arxiv.org/pdf/2503.16529.pdf", "abs": "https://arxiv.org/abs/2503.16529", "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Limin Han", "Jiaojiao Zhao", "Junting Guo", "Zhenhong Long", "Shu Yang", "Meijuan An", "Beibei Huang", "Rongjia Du", "Ning Wang", "Kai Wang", "Shiguo Lian"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "21 pages, 13 figures, 4 tables", "summary": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for the entire DeepSeek-R1 model series. Evaluation results\nindicate that the enhanced models achieve significant improvements in safety\nwhile maintaining reasoning capabilities without notable degradation. We\nopen-source the safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource\nfor future research and optimization of DeepSeek models."}
{"id": "2503.23362", "pdf": "https://arxiv.org/pdf/2503.23362.pdf", "abs": "https://arxiv.org/abs/2503.23362", "title": "Mixture of Routers", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong", "Xi-He Qiu", "Chun-Ming Xia", "Fei Dai"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages,4 figures", "summary": "Supervised fine-tuning (SFT) is a milestone in aligning large language models\nwith human instructions and adapting them to downstream tasks. In particular,\nLow-Rank Adaptation (LoRA) has gained widespread attention due to its parameter\nefficiency. However, its impact on improving the performance of large models\nremains limited. Recent studies suggest that combining LoRA with\nMixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE\nadapts to the diversity and complexity of datasets by dynamically selecting the\nmost suitable experts, thereby improving task accuracy and efficiency. Despite\nimpressive results, recent studies reveal issues in the MoE routing mechanism,\nsuch as incorrect assignments and imbalanced expert allocation. Inspired by the\nprinciples of Redundancy and Fault Tolerance Theory. We innovatively integrate\nthe concept of Mixture of Experts into the routing mechanism and propose an\nefficient fine-tuning method called Mixture of Routers (MoR). It employs\nmultiple sub-routers for joint selection and uses a learnable main router to\ndetermine the weights of the sub-routers. The results show that MoR outperforms\nbaseline models on most tasks, achieving an average performance improvement of\n1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method\nsuitable for a wide range of applications. Our code is available here:\nhttps://anonymous.4open.science/r/MoR-DFC6."}
{"id": "2504.01698", "pdf": "https://arxiv.org/pdf/2504.01698.pdf", "abs": "https://arxiv.org/abs/2504.01698", "title": "Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Jiajun Song", "Lifeng Fan", "Wei Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theory of Mind (ToM), the ability to attribute mental states to others, is\nfundamental for human social intelligence and a critical capability for\nadvanced Artificial Intelligence. Recent advancements in Large Language Models\n(LLMs) have shown promising performance on ToM benchmarks, raising the\nquestion: Do these benchmarks necessitate explicit human-like reasoning\nprocesses, or can models succeed through alternative strategies? We investigate\nthis question empirically by applying Reinforcement Learning (RL) and\nSupervised Fine-Tuning (SFT) to LLMs of varying scales (0.5B to 7B parameters)\nand evaluating them across multiple ToM datasets. Our results reveal a\nscale-dependent impact of RL: while RL significantly improves accuracy and\nfosters high-quality, interpretable, and transferable belief-tracking reasoning\nin larger models (7B), it leads to \"reasoning collapse\" in smaller models\n($\\leq$3B), where high accuracy and generalization ability are achieved via\ndrastically shortened, less meaningful responses. Surprisingly, further SFT\nachieves competitive and generalizable performance across these benchmarks,\noften matching or exceeding RL models in accuracy, despite not being explicitly\ntrained to produce structured reasoning traces. These findings highlight a\ncritical discrepancy between benchmark accuracy and the nature of learned\nreasoning. Our work suggests that current ToM benchmarks may be solvable\nwithout requiring the explicit, human-like simulation of mental states they\nwere designed to probe. LLMs, particularly when scale is limited or training\nsignals focus solely on output correctness, may leverage alternative rules\neffective for benchmark data structures."}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184.pdf", "abs": "https://arxiv.org/abs/2504.09184", "title": "Parameterized Synthetic Text Generation with SimpleStories", "authors": ["Lennart Finke", "Chandan Sreedhara", "Thomas Dooms", "Mat Allen", "Emerald Zhang", "Juan Diego Rodriguez", "Noa Nabeshima", "Thomas Marshall", "Dan Braun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million samples each in English and Japanese. Through\nparameterizing prompts at multiple levels of abstraction, we achieve control\nover story characteristics at scale, inducing syntactic and semantic diversity.\nAblations on a newly trained model suite show improved sample efficiency and\nmodel interpretability compared to the TinyStories dataset. We open-source all\nconstituent parts of model creation, hoping to enable novel ways to study the\nend-to-end training process. As a byproduct, we move the frontier regarding the\nfewest-parameter language model that outputs grammatical natural language."}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704.pdf", "abs": "https://arxiv.org/abs/2504.17704", "title": "Safety in Large Reasoning Models: A Survey", "authors": ["Cheng Wang", "Yue Liu", "Baolong Bi", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang", "Bryan Hooi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."}
{"id": "2505.04588", "pdf": "https://arxiv.org/pdf/2505.04588.pdf", "abs": "https://arxiv.org/abs/2505.04588", "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "authors": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Yan Zhang", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a novel RL\nframework that incentivizes the capabilities of LLMs to use a real search\nengine with simulated searches during training. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both useful and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."}
{"id": "2505.05755", "pdf": "https://arxiv.org/pdf/2505.05755.pdf", "abs": "https://arxiv.org/abs/2505.05755", "title": "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions", "authors": ["Dhruvesh Patel", "Aishwarya Sahoo", "Avinash Amballa", "Tahira Naseem", "Tim G. J. Rudner", "Andrew McCallum"], "categories": ["cs.CL", "cs.LG"], "comment": "Corrected a typo in author names", "summary": "Autoregressive models (ARMs), which predict subsequent tokens one-by-one\n``from left to right,'' have achieved significant success across a wide range\nof sequence generation tasks. However, they struggle to accurately represent\nsequences that require satisfying sophisticated constraints or whose sequential\ndependencies are better addressed by out-of-order generation. Masked Diffusion\nModels (MDMs) address some of these limitations, but the process of unmasking\nmultiple tokens simultaneously in MDMs can introduce incoherences, and MDMs\ncannot handle arbitrary infilling constraints when the number of tokens to be\nfilled in is not known in advance. In this work, we introduce Insertion\nLanguage Models (ILMs), which learn to insert tokens at arbitrary positions in\na sequence -- that is, they select jointly both the position and the vocabulary\nelement to be inserted. By inserting tokens one at a time, ILMs can represent\nstrong dependencies between tokens, and their ability to generate sequences in\narbitrary order allows them to accurately model sequences where token\ndependencies do not follow a left-to-right sequential structure. To train ILMs,\nwe propose a tailored network parameterization and use a simple denoising\nobjective. Our empirical evaluation demonstrates that ILMs outperform both ARMs\nand MDMs on common planning tasks. Furthermore, we show that ILMs outperform\nMDMs and perform on par with ARMs in an unconditional text generation task\nwhile offering greater flexibility than MDMs in arbitrary-length text\ninfilling."}
{"id": "2505.06698", "pdf": "https://arxiv.org/pdf/2505.06698.pdf", "abs": "https://arxiv.org/abs/2505.06698", "title": "From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback", "authors": ["Zongqi Wang", "Tianle Gu", "Chen Gong", "Xin Tian", "Siqi Bao", "Yujiu Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena\nare seeing growing adoption for the evaluation of Large Language Models (LLMs).\nExisting research has primarily focused on approximating human-based model\nrankings using limited data and LLM-as-a-Judge. However, the fundamental\npremise of these studies, which attempts to replicate human rankings, is\nflawed. Specifically, these benchmarks typically offer only overall scores,\nlimiting their utility to leaderboard rankings, rather than providing feedback\nthat can guide model optimization and support model profiling. Therefore, we\nadvocate for an evaluation paradigm shift from approximating human-based model\nrankings to providing feedback with analytical value. To this end, we introduce\n\\textbf{Feedbacker}, an evaluation framework that provides comprehensive and\nfine-grained results, thereby enabling thorough identification of a model's\nspecific strengths and weaknesses. Such feedback not only supports the targeted\noptimization of the model but also enhances the understanding of its behavior.\nFeedbacker comprises three key components: an extensible tree-based query\ntaxonomy builder, an automated query synthesis scheme, and a suite of\nvisualization and analysis tools. Furthermore, we propose a novel\nLLM-as-a-Judge method: PC$^{2}$ (Pre-Comparison-derived Criteria) pointwise\nevaluation. This method derives evaluation criteria by pre-comparing the\ndifferences between several auxiliary responses, achieving the accuracy of\npairwise evaluation while maintaining the time complexity of pointwise\nevaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,\nwe demonstrate the usage of Feedbacker and highlight its effectiveness and\npotential. Our project homepage and dataset are available at\nhttps://liudan193.github.io/Feedbacker."}
{"id": "2505.07233", "pdf": "https://arxiv.org/pdf/2505.07233.pdf", "abs": "https://arxiv.org/abs/2505.07233", "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation", "authors": ["Jiashuo Sun", "Xianrui Zhong", "Sizhe Zhou", "Jiawei Han"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 7 figures, 15 tables", "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker. Since irrelevant documents in RAG systems can\nmislead the generator, the reranker plays a vital role in refining retrieved\ndocuments to enhance generation quality and explainability. However, it is\nchallenging to determine the appropriate number of documents ($k$) that the\nreranker should select: too few may result in missing critical information,\nwhile too many introduce noise and inefficiencies. Although recent studies have\nexplored LLM-based rerankers, they primarily leverage internal model knowledge\nand overlook the rich supervisory signals that LLMs can provide, such as using\nresponse quality as feedback for optimizing reranking decisions. In this paper,\nwe propose DynamicRAG, a novel RAG framework where the reranker dynamically\nadjusts both the order and number of retrieved documents based on the query. We\nmodel the reranker as an agent optimized through reinforcement learning (RL),\nusing rewards derived from LLM output quality. Across seven knowledge-intensive\ndatasets, DynamicRAG demonstrates superior performance, achieving\nstate-of-the-art results among models of same parameter sizes. The model, data\nand code are available at https://github.com/GasolSun36/DynamicRAG."}
{"id": "2505.07313", "pdf": "https://arxiv.org/pdf/2505.07313.pdf", "abs": "https://arxiv.org/abs/2505.07313", "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study", "authors": ["Baixuan Xu", "Chunyang Li", "Weiqi Wang", "Wei Fan", "Tianshi Zheng", "Haochen Shi", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Designing effective collaboration structure for multi-agent LLM systems to\nenhance collective reasoning is crucial yet remains under-explored. In this\npaper, we systematically investigate how collaborative reasoning performance is\naffected by three key design dimensions: (1) Expertise-Domain Alignment, (2)\nCollaboration Paradigm (structured workflow vs. diversity-driven integration),\nand (3) System Scale. Our findings reveal that expertise alignment benefits are\nhighly domain-contingent, proving most effective for contextual reasoning\ntasks. Furthermore, collaboration focused on integrating diverse knowledge\nconsistently outperforms rigid task decomposition. Finally, we empirically\nexplore the impact of scaling the multi-agent system with expertise\nspecialization and study the computational trade off, highlighting the need for\nmore efficient communication protocol design. This work provides concrete\nguidelines for configuring specialized multi-agent system and identifies\ncritical architectural trade-offs and bottlenecks for scalable multi-agent\nreasoning. The code will be made available upon acceptance."}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655.pdf", "abs": "https://arxiv.org/abs/2505.09655", "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO."}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724.pdf", "abs": "https://arxiv.org/abs/2505.09724", "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "authors": ["Gino Carmona-DÃ­az", "William JimÃ©nez-Leal", "MarÃ­a Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo BermÃºdez"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis."}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924.pdf", "abs": "https://arxiv.org/abs/2505.09924", "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted to ACL 2025 (main)", "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\nhttps://github.com/redwyd/SymMark."}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354.pdf", "abs": "https://arxiv.org/abs/2505.10354", "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR."}
{"id": "2403.11083", "pdf": "https://arxiv.org/pdf/2403.11083.pdf", "abs": "https://arxiv.org/abs/2403.11083", "title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning", "authors": ["Xiaohao Xu", "Yunkang Cao", "Huaxin Zhang", "Nong Sang", "Xiaonan Huang"], "categories": ["cs.CV", "cs.CL"], "comment": "Best Student Paper Award at IEEE International Conference on Computer\n  Supported Cooperative Work in Design, 2025", "summary": "Anomaly detection is vital in various industrial scenarios, including the\nidentification of unusual patterns in production lines and the detection of\nmanufacturing defects for quality control. Existing techniques tend to be\nspecialized in individual scenarios and lack generalization capacities. In this\nstudy, our objective is to develop a generic anomaly detection model that can\nbe applied in multiple scenarios. To achieve this, we custom-build generic\nvisual language foundation models that possess extensive knowledge and robust\nreasoning abilities as anomaly detectors and reasoners. Specifically, we\nintroduce a multi-modal prompting strategy that incorporates domain knowledge\nfrom experts as conditions to guide the models. Our approach considers diverse\nprompt types, including task descriptions, class context, normality rules, and\nreference images. In addition, we unify the input representation of\nmulti-modality into a 2D image format, enabling multi-modal anomaly detection\nand reasoning. Our preliminary studies demonstrate that combining visual and\nlanguage prompts as conditions for customizing the models enhances anomaly\ndetection performance. The customized models showcase the ability to detect\nanomalies across different data modalities such as images, point clouds, and\nvideos. Qualitative case studies further highlight the anomaly detection and\nreasoning capabilities, particularly for multi-object scenes and temporal data.\nOur code is publicly available at\nhttps://github.com/Xiaohao-Xu/Customizable-VLM"}
{"id": "2404.02882", "pdf": "https://arxiv.org/pdf/2404.02882.pdf", "abs": "https://arxiv.org/abs/2404.02882", "title": "Linear Attention Sequence Parallelism", "authors": ["Weigao Sun", "Zhen Qin", "Dong Li", "Xuyang Shen", "Yu Qiao", "Yiran Zhong"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by TMLR, 23 pages", "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP."}
{"id": "2405.13522", "pdf": "https://arxiv.org/pdf/2405.13522.pdf", "abs": "https://arxiv.org/abs/2405.13522", "title": "Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective", "authors": ["Zhijian Xu", "Hao Wang", "Qiang Xu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Traditional time series forecasting methods predominantly rely on historical\ndata patterns, neglecting external interventions that significantly shape\nfuture dynamics. Through control-theoretic analysis, we show that the implicit\n\"self-stimulation\" assumption limits the accuracy of these forecasts. To\novercome this limitation, we propose an Intervention-Aware Time Series\nForecasting (IATSF) framework explicitly designed to incorporate external\ninterventions. We particularly emphasize textual interventions due to their\nunique capability to represent qualitative or uncertain influences inadequately\ncaptured by conventional exogenous variables. We propose a leak-free benchmark\ncomposed of temporally synchronized textual intervention data across synthetic\nand real-world scenarios. To rigorously evaluate IATSF, we develop FIATS, a\nlightweight forecasting model that integrates textual interventions through\nChannel-Aware Adaptive Sensitivity Modeling (CASM) and Channel-Aware Parameter\nSharing (CAPS) mechanisms, enabling the model to adjust its sensitivity to\ninterventions and historical data in a channel-specific manner. Extensive\nempirical evaluations confirm that FIATS surpasses state-of-the-art methods,\nhighlighting that forecasting improvements stem explicitly from modeling\nexternal interventions rather than increased model complexity alone."}
{"id": "2406.02844", "pdf": "https://arxiv.org/pdf/2406.02844.pdf", "abs": "https://arxiv.org/abs/2406.02844", "title": "Item-Language Model for Conversational Recommendation", "authors": ["Li Yang", "Anushya Subbiah", "Hardik Patel", "Judith Yue Li", "Yanwei Song", "Reza Mirghaderi", "Vikram Aggarwal", "Qifan Wang"], "categories": ["cs.IR", "cs.CL"], "comment": "15 pages, 3 figures", "summary": "Large-language Models (LLMs) have been extremely successful at tasks like\ncomplex dialogue understanding, reasoning and coding due to their emergent\nabilities. These emergent abilities have been extended with multi-modality to\ninclude image, audio, and video capabilities. Recommender systems, on the other\nhand, have been critical for information seeking and item discovery needs.\nRecently, there have been attempts to apply LLMs for recommendations. One\ndifficulty of current attempts is that the underlying LLM is usually not\ntrained on the recommender system data, which largely contains user interaction\nsignals and is often not publicly available. Another difficulty is user\ninteraction signals often have a different pattern from natural language text,\nand it is currently unclear if the LLM training setup can learn more\nnon-trivial knowledge from interaction signals compared with traditional\nrecommender system methods. Finally, it is difficult to train multiple LLMs for\ndifferent use-cases, and to retain the original language and reasoning\nabilities when learning from recommender system data. To address these three\nlimitations, we propose an Item-Language Model (ILM), which is composed of an\nitem encoder to produce text-aligned item representations that encode user\ninteraction signals, and a frozen LLM that can understand those item\nrepresentations with preserved pretrained knowledge. We conduct extensive\nexperiments which demonstrate both the importance of the language-alignment and\nof user interaction knowledge in the item encoder."}
{"id": "2406.11624", "pdf": "https://arxiv.org/pdf/2406.11624.pdf", "abs": "https://arxiv.org/abs/2406.11624", "title": "Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers", "authors": ["Omer Sahin Tas", "Royden Wagner"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "ICLR 2025 final version. Our implementation is available at\n  https://github.com/kit-mrt/future-motion", "summary": "Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead."}
{"id": "2410.00031", "pdf": "https://arxiv.org/pdf/2410.00031.pdf", "abs": "https://arxiv.org/abs/2410.00031", "title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions", "authors": ["Ryan Y. Lin", "Siddhartha Ojha", "Kevin Cai", "Maxwell F. Chen"], "categories": ["cs.GT", "cs.AI", "cs.CL", "q-fin.CP"], "comment": null, "summary": "Machine-learning technologies are seeing increased deployment in real-world\nmarket scenarios. In this work, we explore the strategic behaviors of large\nlanguage models (LLMs) when deployed as autonomous agents in multi-commodity\nmarkets, specifically within Cournot competition frameworks. We examine whether\nLLMs can independently engage in anti-competitive practices such as collusion\nor, more specifically, market division. Our findings demonstrate that LLMs can\neffectively monopolize specific commodities by dynamically adjusting their\npricing and resource allocation strategies, thereby maximizing profitability\nwithout direct human input or explicit collusion commands. These results pose\nunique challenges and opportunities for businesses looking to integrate AI into\nstrategic roles and for regulatory bodies tasked with maintaining fair and\ncompetitive markets. The study provides a foundation for further exploration\ninto the ramifications of deferring high-stakes decisions to LLM-based agents."}
{"id": "2410.11507", "pdf": "https://arxiv.org/pdf/2410.11507.pdf", "abs": "https://arxiv.org/abs/2410.11507", "title": "TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction", "authors": ["Wanying Wang", "Zeyu Ma", "Pengfei Liu", "Mingang Chen"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed to various vertical\ndomains, automatically evaluating their performance across different domains\nremains a critical challenge. Current evaluation methods often rely on static\nand resource-intensive datasets that are not aligned with real-world\nrequirements and lack cross-domain adaptability. To address these limitations,\nwe revisit the evaluation process and introduce two key concepts:\n\\textbf{Benchmark+}, which extends the traditional question-answer benchmark\ninto a more flexible ``strategy-criterion'' format; and \\textbf{Assessment+},\nwhich enhances the interaction process to facilitate deeper exploration and\ncomprehensive analysis from multiple perspectives. We propose\n\\textbf{\\textsc{TestAgent}}, an agent-based evaluation framework that\nimplements these concepts using retrieval-augmented generation and\nreinforcement learning. \\textsc{TestAgent} enables automatic dynamic benchmark\ngeneration and in-depth assessment across diverse vertical domains. Experiments\non tasks ranging from constructing multiple vertical domain evaluations to\ntransforming static benchmarks into dynamic forms demonstrate the effectiveness\nof \\textsc{TestAgent}. This work provides a novel perspective on automatic\nevaluation methods for domain-specific LLMs, offering a pathway for\ndomain-adaptive dynamic benchmark construction and exploratory assessment."}
{"id": "2410.14609", "pdf": "https://arxiv.org/pdf/2410.14609.pdf", "abs": "https://arxiv.org/abs/2410.14609", "title": "DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search", "authors": ["Simon Lupart", "Mohammad Aliannejadi", "Evangelos Kanoulas"], "categories": ["cs.IR", "cs.CL"], "comment": "11 pages, 6 figures. SIGIR '25 Proceedings of the 48th International\n  ACM SIGIR Conference on Research and Development in Information Retrieval\n  July 13--18, 2025 Padua, Italy", "summary": "Conversational Search (CS) involves retrieving relevant documents from a\ncorpus while considering the conversational context, integrating retrieval with\ncontext modeling. Recent advancements in Large Language Models (LLMs) have\nsignificantly enhanced CS by enabling query rewriting based on conversational\ncontext. However, employing LLMs during inference poses efficiency challenges.\nExisting solutions mitigate this issue by distilling embeddings derived from\nhuman-rewritten queries, focusing primarily on learning the context modeling\ntask. These methods, however, often separate the contrastive retrieval task\nfrom the distillation process, treating it as an independent loss term. To\novercome these limitations, we introduce DiSCo (Distillation of Sparse\nConversational retrieval), a novel approach that unifies retrieval and context\nmodeling through a relaxed distillation objective. Instead of relying\nexclusively on representation learning, our method distills similarity scores\nbetween conversations and documents, providing more freedom in the\nrepresentation space and better leveraging the contrastive nature of document\nrelevance. Extensive experiments on Learned Sparse Retrieval (LSR) across five\nCS datasets demonstrate that DiSCo achieves substantial improvements in both\nin-domain and out-of-domain retrieval tasks, achieving up to a six-point gain\nin recall for out-of-domain datasets over state-of-the-art methods.\nAdditionally, DiSCo employs a multi-teacher distillation strategy, using\nmultiple LLMs as teachers, further enhancing performance and surpassing the\nindividual teachers in in-domain settings. Furthermore, analysis of model\nsparsity reveals that DiSCo allows for more effective control over the sparsity\nof the trained models."}
{"id": "2410.14731", "pdf": "https://arxiv.org/pdf/2410.14731.pdf", "abs": "https://arxiv.org/abs/2410.14731", "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection", "authors": ["Bokai Lin", "Zihao Zeng", "Zipeng Xiao", "Siqi Kou", "Tianqi Hou", "Xiaofeng Gao", "Hao Zhang", "Zhijie Deng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."}
{"id": "2411.02335", "pdf": "https://arxiv.org/pdf/2411.02335.pdf", "abs": "https://arxiv.org/abs/2411.02335", "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity", "authors": ["Yuqi Luo", "Chenyang Song", "Xu Han", "Yingfa Chen", "Chaojun Xiao", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.7"], "comment": "23 pages, 13 figures, 6 tables", "summary": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable."}
{"id": "2411.18711", "pdf": "https://arxiv.org/pdf/2411.18711.pdf", "abs": "https://arxiv.org/abs/2411.18711", "title": "Evaluating Vision-Language Models as Evaluators in Path Planning", "authors": ["Mohamed Aghzal", "Xiang Yue", "Erion Plaku", "Ziyu Yao"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to the 2025 IEEE / CVF Computer Vision and Pattern\n  Recognition Conference (CVPR)", "summary": "Despite their promise to perform complex reasoning, large language models\n(LLMs) have been shown to have limited effectiveness in end-to-end planning.\nThis has inspired an intriguing question: if these models cannot plan well, can\nthey still contribute to the planning framework as a helpful plan evaluator? In\nthis work, we generalize this question to consider LLMs augmented with visual\nunderstanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a\nnovel benchmark evaluating VLMs as plan evaluators in complex path-planning\nscenarios. Succeeding in the benchmark requires a VLM to be able to abstract\ntraits of optimal paths from the scenario description, demonstrate precise\nlow-level perception on each path, and integrate this information to decide the\nbetter path. Our analysis of state-of-the-art VLMs reveals that these models\nface significant challenges on the benchmark. We observe that the VLMs can\nprecisely abstract given scenarios to identify the desired traits and exhibit\nmixed performance in integrating the provided information. Yet, their vision\ncomponent presents a critical bottleneck, with models struggling to perceive\nlow-level details about a path. Our experimental results show that this issue\ncannot be trivially addressed via end-to-end fine-tuning; rather, task-specific\ndiscriminative adaptation of these vision encoders is needed for these VLMs to\nbecome effective path evaluators."}
{"id": "2501.02406", "pdf": "https://arxiv.org/pdf/2501.02406.pdf", "abs": "https://arxiv.org/abs/2501.02406", "title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities", "authors": ["Tara Radvand", "Mojtaba Abdolmaleki", "Mohamed Mostagir", "Ambuj Tewari"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly challenging as text generated by\nLarge Language Models (LLMs) becomes almost indistinguishable from\nhuman-generated content. In addition, many institutions utilize in-house LLMs\nand want to ensure that external, non-sanctioned LLMs do not produce content\nwithin the institution. In this paper, we answer the following question: Given\na piece of text, can we identify whether it was produced by a particular LLM or\nnot? We model LLM-generated text as a sequential stochastic process with\ncomplete dependence on history. We then design zero-shot statistical tests to\n(i) distinguish between text generated by two different known sets of LLMs $A$\n(non-sanctioned) and $B$ (in-house), and (ii) identify whether text was\ngenerated by a known LLM or generated by any unknown model, e.g., a human or\nsome other language generation process. We prove that the type I and type II\nerrors of our test decrease exponentially with the length of the text. For\nthat, we show that if $B$ generates the text, then except with an exponentially\nsmall probability in string length, the log-perplexity of the string under $A$\nconverges to the average cross-entropy of $B$ and $A$. We then present\nexperiments using LLMs with white-box access to support our theoretical results\nand empirically examine the robustness of our results to black-box settings and\nadversarial attacks. In the black-box setting, our method achieves an average\nTPR of 82.5\\% at a fixed FPR of 5\\%. Under adversarial perturbations, our\nminimum TPR is 48.6\\% at the same FPR threshold. Both results outperform all\nnon-commercial baselines. See\nhttps://github.com/TaraRadvand74/llm-text-detection for code, data, and an\nonline demo of the project."}
{"id": "2502.01384", "pdf": "https://arxiv.org/pdf/2502.01384.pdf", "abs": "https://arxiv.org/abs/2502.01384", "title": "Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods", "authors": ["Oussama Zekri", "Nicolas BoullÃ©"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "30 pages, 8 figures, 8 tables", "summary": "Discrete diffusion models have recently gained significant attention due to\ntheir ability to process complex discrete structures for language modeling.\nHowever, fine-tuning these models with policy gradient methods, as is commonly\ndone in Reinforcement Learning from Human Feedback (RLHF), remains a\nchallenging task. We propose an efficient, broadly applicable, and\ntheoretically justified policy gradient algorithm, called Score Entropy Policy\nOptimization (SEPO), for fine-tuning discrete diffusion models over\nnon-differentiable rewards. Our numerical experiments across several discrete\ngenerative tasks demonstrate the scalability and efficiency of our method. Our\ncode is available at https://github.com/ozekri/SEPO."}
{"id": "2502.02315", "pdf": "https://arxiv.org/pdf/2502.02315.pdf", "abs": "https://arxiv.org/abs/2502.02315", "title": "Shuttle Between the Instructions and the Parameters of Large Language Models", "authors": ["Wangtao Sun", "Haotian Xu", "Huanxuan Liao", "Xuanqing Yu", "Zhongtao Jiang", "Shizhu He", "Jun Zhao", "Kang Liu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The interaction with Large Language Models (LLMs) through instructions has\nbeen extensively investigated in the research community. While instructions\nhave been widely used as the guidelines for task solving, this paper further\nnotices that both instructions and parameters are the compression of task data.\nTherefore, they could be strongly correlated and can be learned to predict one\nfrom the other. This paper proposes a novel neural network framework, SHIP\n(\\textbf{Sh}uttle between the \\textbf{I}nstructions and the\n\\textbf{P}arameters), to model and learn the mutual mappings between the\ninstructions and the parameters of LLMs. We verify that SHIP can effectively\nmap one of the instructions/parameters to the other by evaluating it on the\ntasks of instruction deduction and induction. The results show that SHIP\nperforms better than existing baseline methods in terms of deductive\ncapabilities while significantly surpassing them in inductive capabilities.\nMoreover, SHIP can effectively combine the two mapping processes to perform\nexcellent inductive reasoning. The code and data for this paper are released at\nhttps://anonymous.4open.science/r/Shuttle-Between-Instructions-Parameters/."}
{"id": "2502.09933", "pdf": "https://arxiv.org/pdf/2502.09933.pdf", "abs": "https://arxiv.org/abs/2502.09933", "title": "MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?", "authors": ["Kai Yan", "Zhan Ling", "Kang Liu", "Yifan Yang", "Ting-Han Fan", "Lingfeng Shen", "Zhengyin Du", "Jiecao Chen"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "36 pages, 11 figures. The last version adds more experiments and\n  modifies name for better summary of the work", "summary": "The ability to recognize patterns from examples and apply them to new ones is\na primal ability for general intelligence, and is widely studied by psychology\nand AI researchers. Many benchmarks have been proposed to measure such ability\nfor Large Language Models (LLMs); however, they focus on few-shot (usually <10)\nsetting and lack evaluation for aggregating many pieces of information from\nlong contexts. On the other hand, the ever-growing context length of LLMs have\nbrought forth the novel paradigm of many-shot In-Context Learning (ICL), which\naddresses new tasks with hundreds to thousands of examples without expensive\nand inefficient fine-tuning. However, many-shot evaluations often focus on\nclassification, and popular long-context LLM tasks such as Needle-In-A-Haystack\n(NIAH) seldom require complicated intelligence for integrating many pieces of\ninformation. To fix the issues from both worlds, we propose MIR-Bench, the\nfirst many-shot in-context reasoning benchmark for pattern recognition that\nasks LLM to predict output via input-output examples from underlying functions\nwith diverse data format. Based on MIR-Bench, we study many novel problems for\nmany-shot in-context reasoning, and acquired many insightful findings including\nscaling effect, robustness, inductive vs. transductive reasoning, retrieval\nAugmented Generation (RAG), coding for inductive reasoning, cross-domain\ngeneralizability, etc."}
{"id": "2502.19676", "pdf": "https://arxiv.org/pdf/2502.19676.pdf", "abs": "https://arxiv.org/abs/2502.19676", "title": "FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark", "authors": ["Zhangdie Yuan", "Zifeng Ding", "Andreas Vlachos"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Forecasting is an important task in many domains, such as technology and\neconomics. However existing forecasting benchmarks largely lack comprehensive\nconfidence assessment, focus on limited question types, and often consist of\nartificial questions that do not align with real-world human forecasting needs.\nTo address these gaps, we introduce FOReCAst (Future Outcome Reasoning and\nConfidence Assessment), a benchmark that evaluates models' ability to make\npredictions and their confidence in them. FOReCAst spans diverse forecasting\nscenarios involving Boolean questions, timeframe prediction, and quantity\nestimation, enabling a comprehensive evaluation of both prediction accuracy and\nconfidence calibration for real-world applications."}
{"id": "2502.20742", "pdf": "https://arxiv.org/pdf/2502.20742.pdf", "abs": "https://arxiv.org/abs/2502.20742", "title": "Structured Preference Optimization for Vision-Language Long-Horizon Task Planning", "authors": ["Xiwen Liang", "Min Lin", "Weiqi Ruan", "Rongtao Xu", "Yuecheng Liu", "Jiaqi Chen", "Bingqian Lin", "Yuzheng Zhuang", "Xiaodan Liang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "18 pages", "summary": "Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines."}
{"id": "2504.13837", "pdf": "https://arxiv.org/pdf/2504.13837.pdf", "abs": "https://arxiv.org/abs/2504.13837", "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "authors": ["Yang Yue", "Zhiqi Chen", "Rui Lu", "Andrew Zhao", "Zhaokai Wang", "Yang Yue", "Shiji Song", "Gao Huang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "30 pages, 27 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955.pdf", "abs": "https://arxiv.org/abs/2504.13955", "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "authors": ["Suhas BN", "Andrew M. Sherrill", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "22 pages, 6 figures Updated Appendix with example model responses", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191.pdf", "abs": "https://arxiv.org/abs/2504.14191", "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery."}
{"id": "2505.08905", "pdf": "https://arxiv.org/pdf/2505.08905.pdf", "abs": "https://arxiv.org/abs/2505.08905", "title": "Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora", "authors": ["Michael Majurski", "Cynthia Matuszek"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Language Models (LMs) continue to advance, improving response quality and\ncoherence. Given Internet-scale training datasets, LMs have likely encountered\nmuch of what users may ask them to generate in some form during their training.\nA plethora of evaluation benchmarks have been constructed to assess model\nquality, response appropriateness, and reasoning capabilities. However, the\nhuman effort required for benchmark construction is rapidly being outpaced by\nthe size and scope of the models under evaluation. Having humans build a\nbenchmark for every possible domain of interest is impractical. Therefore, we\npropose a methodology for automating the construction of fact-based synthetic\ndata model evaluations grounded in document populations. This work leverages\nthe same LMs to evaluate domain-specific knowledge automatically, using only\ngrounding documents (e.g., a textbook) as input. This synthetic data\nbenchmarking approach corresponds well with human curated questions producing a\nSpearman ranking correlation of 0.97 and a benchmark evaluation Pearson\naccuracy correlation of 0.75. This novel approach supports generating both\nmultiple choice and open-ended synthetic data questions to gain diagnostic\ninsight of LM capability. We apply this methodology to evaluate model\nperformance on two recent arXiv preprints, discovering a surprisingly strong\nperformance from Gemma-3 models on open-ended questions. Code is available at\nhttps://github.com/mmajurski/grounded-synth-lm-benchmark"}
{"id": "2505.09665", "pdf": "https://arxiv.org/pdf/2505.09665.pdf", "abs": "https://arxiv.org/abs/2505.09665", "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling", "authors": ["Sulong Zhou", "Qunying Huang", "Shaoheng Zhou", "Yun Hang", "Xinyue Ye", "Aodong Mei", "Kathryn Phung", "Yuning Ye", "Uma Govindswamy", "Zehan Li"], "categories": ["cs.SI", "cs.CL"], "comment": "Corrected capitalization errors in the section subtitle 3.4, 4.3,\n  step 1 in section 3.3.2, and Supplementary Information. Fix typo with\n  \"Weighting\" for step 4 in section 3.3.2", "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent\nyears. Understanding how affected populations perceive and respond during\nwildfire crises is critical for timely and empathetic disaster response. Social\nmedia platforms offer a crowd-sourced channel to capture evolving public\ndiscourse, providing hyperlocal information and insight into public sentiment.\nThis study analyzes Reddit discourse during the 2025 Los Angeles wildfires,\nspanning from the onset of the disaster to full containment. We collect 385\nposts and 114,879 comments related to the Palisades and Eaton fires. We adopt\ntopic modeling methods to identify the latent topics, enhanced by large\nlanguage models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we\ndevelop a hierarchical framework to categorize latent topics, consisting of two\nmain categories, Situational Awareness (SA) and Crisis Narratives (CN). The\nvolume of SA category closely aligns with real-world fire progressions, peaking\nwithin the first 2-5 days as the fires reach the maximum extent. The most\nfrequent co-occurring category set of public health and safety, loss and\ndamage, and emergency resources expands on a wide range of health-related\nlatent topics, including environmental health, occupational health, and one\nhealth. Grief signals and mental health risks consistently accounted for 60\npercentage and 40 percentage of CN instances, respectively, with the highest\ntotal volume occurring at night. This study contributes the first annotated\nsocial media dataset on the 2025 LA fires, and introduces a scalable\nmulti-layer framework that leverages topic modeling for crisis discourse\nanalysis. By identifying persistent public health concerns, our results can\ninform more empathetic and adaptive strategies for disaster response, public\nhealth communication, and future research in comparable climate-related\ndisaster events."}
{"id": "2505.09921", "pdf": "https://arxiv.org/pdf/2505.09921.pdf", "abs": "https://arxiv.org/abs/2505.09921", "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization", "authors": ["Yidan Wang", "Yanan Cao", "Yubing Ren", "Fang Fang", "Zheng Lin", "Binxing Fang"], "categories": ["cs.CR", "cs.CL"], "comment": "Accepted to ACL 2025 (main)", "summary": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at https://github.com/redwyd/PrivacyJailbreak."}
