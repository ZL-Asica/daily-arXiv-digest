{"id": "2509.08108", "pdf": "https://arxiv.org/pdf/2509.08108.pdf", "abs": "https://arxiv.org/abs/2509.08108", "title": "Understanding the Video Content Creation Journey of Creators with Sensory Impairment in Kenya", "authors": ["Lan Xiao", "Maryam Bandukda", "Franklin Mingzhe Li", "Mark Colley", "Catherine Holloway"], "categories": ["cs.HC"], "comment": null, "summary": "Video content creation offers vital opportunities for expression and\nparticipation, yet remains largely inaccessible to creators with sensory\nimpairments, especially in low-resource settings. We conducted interviews with\n20 video creators with visual and hearing impairments in Kenya to examine their\ntools, challenges, and collaborative practices. Our findings show that\naccessibility barriers and infrastructural limitations shape video creation as\na staged, collaborative process involving trusted human partners and emerging\nAI tools. Across workflows, creators actively negotiated agency and trust,\nmaintaining creative control while bridging sensory gaps. We discuss the need\nfor flexible, interdependent collaboration models, inclusive human-AI\nworkflows, and diverse storytelling practices. This work broadens accessibility\nresearch in HCI by examining how technology and social factors intersect in\nlow-resource contexts, suggesting ways to better support disabled creators\nglobally.", "AI": {"tldr": "This paper explores the challenges faced by video creators with sensory impairments in low-resource settings, emphasizing the role of AI tools and collaborative practices in overcoming accessibility barriers.", "motivation": "To address the lack of accessibility for video content creation among creators with sensory impairments, particularly in low-resource environments.", "method": "Interviews with 20 video creators with visual and hearing impairments were conducted to investigate their tools, challenges, and collaborative workflows.", "result": "The study found that accessibility barriers and infrastructure limitations create a collaborative video creation process that relies on trusted partners and AI tools, with creators negotiating their agency and trust.", "conclusion": "The research highlights the importance of flexible collaboration models and inclusive workflows to support disabled creators, stressing the intersection of technology and social factors in enhancing accessibility.", "key_contributions": ["Identified specific challenges faced by video creators with sensory impairments in low-resource settings.", "Proposed a model for flexible collaboration and inclusion of AI tools in the creative process.", "Expanded accessibility research in HCI by integrating insights from low-resource contexts."], "limitations": "The study is limited to a specific demographic and geographic area, which may not universally represent all creators with sensory impairments.", "keywords": ["accessibility", "human-computer interaction", "collaboration", "AI tools", "sensory impairments"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.08203", "pdf": "https://arxiv.org/pdf/2509.08203.pdf", "abs": "https://arxiv.org/abs/2509.08203", "title": "Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units", "authors": ["Ryan Lingo", "Rajeev Chhajer", "Martin Arroyo", "Luka Brkljacic", "Ben Davis", "Nithin Santhanam"], "categories": ["cs.HC", "cs.AI", "cs.SE", "I.2.7; H.5.2"], "comment": "12 pages, 4 figures", "summary": "Large Language Models (LLMs) often produce monolithic text that is hard to\nedit in parts, which can slow down collaborative workflows. We present\ncomponentization, an approach that decomposes model outputs into modular,\nindependently editable units while preserving context. We describe Modular and\nAdaptable Output Decomposition (MAOD), which segments responses into coherent\ncomponents and maintains links among them, and we outline the Component-Based\nResponse Architecture (CBRA) as one way to implement this idea. Our reference\nprototype, MAODchat, uses a microservices design with state-machine-based\ndecomposition agents, vendor-agnostic model adapters, and real-time component\nmanipulation with recomposition.\n  In an exploratory study with four participants from academic, engineering,\nand product roles, we observed that component-level editing aligned with\nseveral common workflows and enabled iterative refinement and selective reuse.\nParticipants also mentioned possible team workflows. Our contributions are: (1)\na definition of componentization for transforming monolithic outputs into\nmanipulable units, (2) CBRA and MAODchat as a prototype architecture, (3)\npreliminary observations from a small user study, (4) MAOD as an algorithmic\nsketch for semantic segmentation, and (5) example Agent-to-Agent protocols for\nautomated decomposition. We view componentization as a promising direction for\nturning passive text consumption into more active, component-level\ncollaboration.", "AI": {"tldr": "This paper introduces componentization to edit Large Language Model outputs more efficiently through modular responses.", "motivation": "Large Language Models produce text that is difficult to edit, slowing down collaboration. There is a need to improve this by enabling easier manipulation of outputs.", "method": "The paper presents Modular and Adaptable Output Decomposition (MAOD) which segments model responses into coherent, editable components while maintaining context, and implements this through the Component-Based Response Architecture (CBRA).", "result": "In a study with four participants, component-level editing facilitated common workflows, enabled iterative refinement, and encouraged selective reuse of components.", "conclusion": "Componentization could transform the way users interact with text generated by LLMs, promoting more effective collaboration and active engagement.", "key_contributions": ["Definition of componentization for LLM outputs", "Introduction of CBRA and MAODchat prototype", "Preliminary user study observations"], "limitations": "", "keywords": ["Componentization", "Large Language Models", "Human-Computer Interaction", "Modular Outputs", "Collaborative Workflows"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.08213", "pdf": "https://arxiv.org/pdf/2509.08213.pdf", "abs": "https://arxiv.org/abs/2509.08213", "title": "A Priest, a Rabbi, and an Atheist Walk Into an Error Bar: Religious Meditations on Uncertainty Visualization", "authors": ["Michael Correll", "Lane Harrison"], "categories": ["cs.HC"], "comment": null, "summary": "In this provocation, we suggest that much (although not all) current\nuncertainty visualization simplifies the myriad forms of uncertainty into error\nbars around an estimate. This apparent simplification into error bars comes\nonly as a result of a vast metaphysics around uncertainty and probability\nunderlying modern statistics. We use examples from religion to present\nalternative views of uncertainty (metaphysical or otherwise) with the goal of\nenriching our conception of what kind of uncertainties we ought to visualize,\nand what kinds of people we might be visualizing those uncertainties for.", "AI": {"tldr": "This paper critiques the simplification of uncertainty visualization, advocating for a broader understanding of uncertainty forms and their visualization.", "motivation": "To challenge the prevailing notion that uncertainty can be effectively represented solely through error bars, proposing a deeper philosophical engagement with uncertainty.", "method": "The authors analyze examples from religion to explore alternative conceptions of uncertainty and propose how this enriches visualization practices.", "result": "The study reveals that current practices largely rely on a narrow interpretation of uncertainty, potentially overlooking diverse forms of uncertainty that could better serve various audiences.", "conclusion": "By expanding the scope of what uncertainties to visualize, the paper encourages a richer dialogue about the needs of different stakeholders in the communication of uncertainty.", "key_contributions": ["Critique of the dominance of error bars in uncertainty visualization", "Integration of alternative philosophical perspectives on uncertainty", "Encouragement to tailor uncertainty visualizations to diverse audiences"], "limitations": "", "keywords": ["uncertainty visualization", "error bars", "philosophy of uncertainty"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.08353", "pdf": "https://arxiv.org/pdf/2509.08353.pdf", "abs": "https://arxiv.org/abs/2509.08353", "title": "An Adaptive Scoring Framework for Attention Assessment in NDD Children via Serious Games", "authors": ["Abdul Rehman", "Ilona Heldal", "Cristina Costescu", "Carmen David", "Jerry Chun-Wei Lin"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces an innovative adaptive scoring framework for children\nwith Neurodevelopmental Disorders (NDD) that is attributed to the integration\nof multiple metrics, such as spatial attention patterns, temporal engagement,\nand game performance data, to create a comprehensive assessment of learning\nthat goes beyond traditional game scoring. The framework employs a progressive\ndifficulty adaptation method, which focuses on specific stimuli for each level\nand adjusts weights dynamically to accommodate increasing cognitive load and\nlearning complexity. Additionally, it includes capabilities for temporal\nanalysis, such as detecting engagement periods, providing rewards for sustained\nattention, and implementing an adaptive multiplier framework based on\nperformance levels. To avoid over-rewarding high performers while maximizing\nimprovement potential for students who are struggling, the designed framework\nfeatures an adaptive temporal impact framework that adjusts performance scales\naccordingly. We also established a multi-metric validation framework using Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation, and\nSpearman correlation, along with defined quality thresholds for assessing\ndeployment readiness in educational settings. This research bridges the gap\nbetween technical eye-tracking metrics and educational insights by explicitly\nmapping attention patterns to learning behaviors, enabling actionable\npedagogical interventions.", "AI": {"tldr": "The paper presents an adaptive scoring framework for children with Neurodevelopmental Disorders, integrating multiple metrics to enhance learning assessments beyond traditional methods.", "motivation": "To create a comprehensive assessment of learning that accommodates diverse cognitive needs in children with Neurodevelopmental Disorders.", "method": "The framework uses progressive difficulty adaptation and dynamic weighting of metrics such as spatial attention, temporal engagement, and game performance.", "result": "The framework includes an adaptive temporal impact system that personalizes assessments, supported by multi-metric validation methodologies like MAE and RMSE.", "conclusion": "This research links technical metrics with educational insights, fostering improved pedagogical interventions for better learning outcomes.", "key_contributions": ["Innovative adaptive scoring framework for children with NDD", "Integration of multiple metrics for comprehensive assessments", "Development of a multi-metric validation framework for educational settings"], "limitations": "", "keywords": ["Neurodevelopmental Disorders", "adaptive scoring", "learning assessment", "educational technology", "temporal engagement"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.07998", "pdf": "https://arxiv.org/pdf/2509.07998.pdf", "abs": "https://arxiv.org/abs/2509.07998", "title": "Bilingual Word Level Language Identification for Omotic Languages", "authors": ["Mesay Gemeda Yigezu", "Girma Yohannis Bade", "Atnafu Lambebo Tonja", "Olga Kolesnikova", "Grigori Sidorov", "Alexander Gelbukh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language identification is the task of determining the languages for a given\ntext. In many real world scenarios, text may contain more than one language,\nparticularly in multilingual communities. Bilingual Language Identification\n(BLID) is the task of identifying and distinguishing between two languages in a\ngiven text. This paper presents BLID for languages spoken in the southern part\nof Ethiopia, namely Wolaita and Gofa. The presence of words similarities and\ndifferences between the two languages makes the language identification task\nchallenging. To overcome this challenge, we employed various experiments on\nvarious approaches. Then, the combination of the BERT based pretrained language\nmodel and LSTM approach performed better, with an F1 score of 0.72 on the test\nset. As a result, the work will be effective in tackling unwanted social media\nissues and providing a foundation for further research in this area.", "AI": {"tldr": "This paper addresses bilingual language identification (BLID) for the Wolaita and Gofa languages in Ethiopia using a combination of BERT and LSTM, achieving an F1 score of 0.72.", "motivation": "To identify and distinguish between two languages in multilingual texts, specifically focusing on Wolaita and Gofa languages in Ethiopia.", "method": "The study employed various experiments combining a BERT-based pretrained language model with an LSTM approach for language identification.", "result": "The combination of BERT and LSTM yielded an F1 score of 0.72 on the test set, demonstrating effective performance in BLID.", "conclusion": "The findings will aid in addressing social media issues and provide a foundation for further research in bilingual language identification.", "key_contributions": ["Introduction of BLID for Wolaita and Gofa languages", "Combination approach using BERT and LSTM", "Achievement of a competitive F1 score for language identification tasks"], "limitations": "", "keywords": ["Bilingual Language Identification", "Wolaita", "Gofa", "BERT", "LSTM"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.08357", "pdf": "https://arxiv.org/pdf/2509.08357.pdf", "abs": "https://arxiv.org/abs/2509.08357", "title": "Personalized Inhibition Training with Eye-Tracking: Enhancing Student Learning and Teacher Assessment in Educational Games", "authors": ["Abdul Rehman", "Ilona Heldal", "Diana Stilwell", "Paula Costa Ferreira", "Jerry Chun-Wei Lin"], "categories": ["cs.HC"], "comment": null, "summary": "Eye tracking (ET) can help to understand visual attention and cognitive\nprocesses in interactive environments. This study presents a comprehensive\neye-tracking analysis framework of the Inhibitory Control Game, named the\nReStroop game, which is an educational intervention aimed at improving\ninhibitory control skills in children through a recycling-themed sorting task,\nfor educational assessment that processes raw gaze data through unified\nalgorithms for fixation detection, performance evaluation, and personalized\nintervention planning. The system employs dual-threshold eye movement detection\n(I-VT and advanced clustering), comprehensive Area of Interest (AOI) analysis,\nand evidence-based risk assessment to transform gaze patterns into actionable\neducational insights. We evaluated this framework across three difficulty\nlevels and revealed critical attention deficits, including low task relevance,\nelevated attention scatter, and compromised processing efficiency. The\nmulti-dimensional risk assessment identified high to moderate risk levels,\ntriggering personalized interventions including focus training, attention\nregulation support, and environmental modifications. The system successfully\ndistinguishes between adaptive learning and cognitive overload, providing early\nwarning indicators for educational intervention. Results demonstrate the\nsystem's effectiveness in objective attention assessment, early risk\nidentification, and the generation of evidence-based recommendations for\nstudents, teachers, and specialists, supporting data-driven educational\ndecision-making and personalized learning approaches.", "AI": {"tldr": "The study presents an eye-tracking analysis framework used in an educational game to enhance children's inhibitory control skills through personalized interventions based on attention insights.", "motivation": "To improve understanding of visual attention and cognitive processes in interactive educational environments, specifically for children.", "method": "The framework utilizes dual-threshold eye movement detection and comprehensive Area of Interest analysis to process gaze data and offer personalized educational insights.", "result": "Three levels of game difficulty revealed critical attention deficits and risk assessments, resulting in effective personalized interventions and actionable recommendations for educational stakeholders.", "conclusion": "The system effectively identifies attention risks and recommends tailored interventions, thereby supporting data-driven decision-making in education.", "key_contributions": ["Comprehensive eye-tracking analysis framework for educational assessment", "Development of personalized intervention strategies based on gaze data", "Early warning indicators for cognitive overload and adaptive learning"], "limitations": "", "keywords": ["Eye Tracking", "Inhibitory Control", "Education", "Personalized Learning", "Attention Assessment"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08000", "pdf": "https://arxiv.org/pdf/2509.08000.pdf", "abs": "https://arxiv.org/abs/2509.08000", "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs", "authors": ["Debdeep Sanyal", "Manodeep Ray", "Murari Mandal"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "The release of open-weight large language models (LLMs) creates a tension\nbetween advancing accessible research and preventing misuse, such as malicious\nfine-tuning to elicit harmful content. Current safety measures struggle to\npreserve the general capabilities of the LLM while resisting a determined\nadversary with full access to the model's weights and architecture, who can use\nfull-parameter fine-tuning to erase existing safeguards. To address this, we\nintroduce AntiDote, a bi-level optimization procedure for training LLMs to be\nresistant to such tampering. AntiDote involves an auxiliary adversary\nhypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)\nweights conditioned on the defender model's internal activations. The defender\nLLM is then trained with an objective to nullify the effect of these\nadversarial weight additions, forcing it to maintain its safety alignment. We\nvalidate this approach against a diverse suite of 52 red-teaming attacks,\nincluding jailbreak prompting, latent space manipulation, and direct\nweight-space attacks. AntiDote is upto 27.4\\% more robust against adversarial\nattacks compared to both tamper-resistance and unlearning baselines. Crucially,\nthis robustness is achieved with a minimal trade-off in utility, incurring a\nperformance degradation of upto less than 0.5\\% across capability benchmarks\nincluding MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute\nefficient methodology for building open-weight models where safety is a more\nintegral and resilient property.", "AI": {"tldr": "Introducing AntiDote, a bi-level optimization method that enhances the robustness of open-weight LLMs against tampering and adversarial attacks while maintaining performance integrity.", "motivation": "To create a balance between accessible research through open-weight models and preventing their misuse via malicious fine-tuning.", "method": "AntiDote employs a bi-level optimization approach with an auxiliary adversary hypernetwork, generating malicious weights to condition the defender model's training, thus enhancing its resilience to attacks.", "result": "AntiDote significantly improves robustness against 52 red-teaming attacks, being 27.4% more resistant compared to traditional tamper-resistance and unlearning baselines, while performance degradation is kept below 0.5% across various benchmarks.", "conclusion": "AntiDote provides a compute-efficient method to develop open-weight models prioritizing safety without substantial losses in utility, making it a practical choice for model training.", "key_contributions": ["Introduction of AntiDote for LLM robustness against adversarial attacks", "Demonstrated performance with a diverse suite of attacks", "Minimal trade-off in model utility while enhancing safety"], "limitations": "", "keywords": ["large language models", "robustness", "adversarial attacks", "safety", "optimization"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2509.08404", "pdf": "https://arxiv.org/pdf/2509.08404.pdf", "abs": "https://arxiv.org/abs/2509.08404", "title": "HyperMOOC: Augmenting MOOC Videos with Concept-based Embedded Visualizations", "authors": ["Li Ye", "Lei Wang", "Lihong Cai", "Ruiqi Yu", "Yong Wang", "Yigang Wang", "Wei Chen", "Zhiguang Zhou"], "categories": ["cs.HC"], "comment": "25 pages,10 figures", "summary": "Massive Open Online Courses (MOOCs) have become increasingly popular\nworldwide. However, learners primarily rely on watching videos, easily losing\nknowledge context and reducing learning effectiveness. We propose HyperMOOC, a\nnovel approach augmenting MOOC videos with concept-based embedded\nvisualizations to help learners maintain knowledge context. Informed by expert\ninterviews and literature review, HyperMOOC employs multi-glyph designs for\ndifferent knowledge types and multi-stage interactions for deeper\nunderstanding. Using a timeline-based radial visualization, learners can grasp\ncognitive paths of concepts and navigate courses through hyperlink-based\ninteractions. We evaluated HyperMOOC through a user study with 36 MOOC learners\nand interviews with two instructors. Results demonstrate that HyperMOOC\nenhances learners' learning effect and efficiency on MOOCs, with participants\nshowing higher satisfaction and improved course understanding compared to\ntraditional video-based learning approaches.", "AI": {"tldr": "HyperMOOC enhances MOOC video learning through concept-based visualizations and interactive design, improving knowledge retention and understanding.", "motivation": "To address the issue of context loss in MOOC video learning, leading to reduced learning effectiveness.", "method": "HyperMOOC integrates multi-glyph designs and multi-stage interactions using timeline-based radial visualizations to enhance user engagement and cognitive understanding.", "result": "User studies with 36 MOOC learners revealed increased learning effectiveness, higher satisfaction rates, and improved understanding compared to traditional methods.", "conclusion": "The integration of concept-based visualizations significantly improves the learning experience in MOOCs by helping learners maintain context and navigate content more effectively.", "key_contributions": ["Introduction of multi-glyph designs for various knowledge types", "Implementation of timeline-based radial visualizations for cognitive path understanding", "Evaluation of learner satisfaction and effectiveness through user studies"], "limitations": "Limited to a specific user demographic of MOOC learners and a single platform for evaluation.", "keywords": ["MOOCs", "visualization", "education technology", "user study", "learning effectiveness"], "importance_score": 6, "read_time_minutes": 25}}
{"id": "2509.08022", "pdf": "https://arxiv.org/pdf/2509.08022.pdf", "abs": "https://arxiv.org/abs/2509.08022", "title": "MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values", "authors": ["Yao Liang", "Dongcheng Zhao", "Feifei Zhao", "Guobin Shen", "Yuwei Wang", "Dongqi Liang", "Yi Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe and effective deployment across diverse user populations.\nHowever, existing benchmarks often neglect cultural and demographic diversity,\nleading to limited understanding of how value alignment generalizes globally.\nIn this work, we introduce MVPBench, a novel benchmark that systematically\nevaluates LLMs' alignment with multi-dimensional human value preferences across\n75 countries. MVPBench contains 24,020 high-quality instances annotated with\nfine-grained value labels, personalized questions, and rich demographic\nmetadata, making it the most comprehensive resource of its kind to date. Using\nMVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,\nrevealing substantial disparities in alignment performance across geographic\nand demographic lines. We further demonstrate that lightweight fine-tuning\nmethods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization\n(DPO), can significantly enhance value alignment in both in-domain and\nout-of-domain settings. Our findings underscore the necessity for\npopulation-aware alignment evaluation and provide actionable insights for\nbuilding culturally adaptive and value-sensitive LLMs. MVPBench serves as a\npractical foundation for future research on global alignment, personalized\nvalue modeling, and equitable AI development.", "AI": {"tldr": "MVPBench is a new benchmark for evaluating how well large language models align with human values across 75 countries, highlighting disparities and proposing methods for improving alignment.", "motivation": "To address the lack of cultural and demographic diversity in existing benchmarks for aligning large language models (LLMs) with human values.", "method": "Development of MVPBench, a benchmark with 24,020 annotated instances for evaluating LLM alignment through demographic metadata and personalized questions; analysis of LLMs using lightweight fine-tuning methods.", "result": "Substantial disparities in alignment performance of LLMs were found across different geographic and demographic groups; lightweight fine-tuning methods significantly improved alignment outcomes.", "conclusion": "The results highlight the need for population-aware alignment evaluation and emphasize the importance of building culturally adaptive LLMs; MVPBench serves as a foundation for future AI development focused on equitable value alignment.", "key_contributions": ["Introduction of MVPBench with 24,020 instances for global human value alignment evaluation.", "Demonstration of disparities in LLM performance based on geography and demographics.", "Evidence that lightweight fine-tuning methods enhance model value alignment."], "limitations": "", "keywords": ["Large Language Models", "Value Alignment", "Benchmarking", "Cultural Diversity", "AI Development"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.08444", "pdf": "https://arxiv.org/pdf/2509.08444.pdf", "abs": "https://arxiv.org/abs/2509.08444", "title": "GlyphWeaver: Unlocking Glyph Design Creativity with Uniform Glyph DSL and AI", "authors": ["Can Liu", "Shiwei Chen", "Zhibang Jiang", "Yong Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Expressive glyph visualizations provide a powerful and versatile means to\nrepresent complex multivariate data through compact visual encodings, but\ncreating custom glyphs remains challenging due to the gap between design\ncreativity and technical implementation. We present GlyphWeaver, a novel\ninteractive system to enable an easy creation of expressive glyph\nvisualizations. Our system comprises three key components: a glyph\ndomain-specific language (GDSL), a GDSL operation management mechanism, and a\nmultimodal interaction interface. The GDSL is a hierarchical container model,\nwhere each container is independent and composable, providing a rigorous yet\npractical foundation for complex glyph visualizations. The operation management\nmechanism restricts modifications of the GDSL to atomic operations, making it\naccessible without requiring direct coding. The multimodal interaction\ninterface enables direct manipulation, natural language commands, and parameter\nadjustments. A multimodal large language model acts as a translator, converting\nthese inputs into GDSL operations. GlyphWeaver significantly lowers the barrier\nfor designers, who often do not have extensive programming skills, to create\nsophisticated glyph visualizations. A case study and user interviews with 13\nparticipants confirm its substantial gains in design efficiency and\neffectiveness of producing creative glyph visualizations.", "AI": {"tldr": "GlyphWeaver is an interactive system that simplifies the creation of expressive glyph visualizations for complex multivariate data through a domain-specific language and multimodal interaction.", "motivation": "There is a gap between design creativity and technical implementation in creating custom glyph visualizations, making it challenging for designers, particularly those without programming skills.", "method": "GlyphWeaver consists of a glyph domain-specific language (GDSL) for hierarchical container models, an operation management mechanism limiting modifications to atomic operations, and a multimodal interaction interface for natural language and direct input.", "result": "User studies indicate that GlyphWeaver significantly improves design efficiency and creativity in producing complex glyph visualizations.", "conclusion": "The system effectively empowers designers with no extensive programming expertise to create sophisticated visual representations of multivariate data.", "key_contributions": ["Introduction of GlyphWeaver as an interactive system for glyph creation", "Development of a hierarchical GDSL for visualizations", "Implementation of a multimodal interaction interface leveraging a large language model"], "limitations": "", "keywords": ["glyph visualizations", "domain-specific language", "multimodal interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08025", "pdf": "https://arxiv.org/pdf/2509.08025.pdf", "abs": "https://arxiv.org/abs/2509.08025", "title": "NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment", "authors": ["Hoang-Trung Nguyen", "Tan-Minh Nguyen", "Xuan-Bach Le", "Tuan-Kiet Le", "Khanh-Huyen Nguyen", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong", "Le-Minh Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents the methodologies and results of the NOWJ team's\nparticipation across all five tasks at the COLIEE 2025 competition, emphasizing\nadvancements in the Legal Case Entailment task (Task 2). Our comprehensive\napproach systematically integrates pre-ranking models (BM25, BERT, monoT5),\nembedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large\nLanguage Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance\nscoring, and contextual re-ranking. Specifically, in Task 2, our two-stage\nretrieval system combined lexical-semantic filtering with contextualized LLM\nanalysis, achieving first place with an F1 score of 0.3195. Additionally, in\nother tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal\nTextual Entailment, and Legal Judgment Prediction--we demonstrated robust\nperformance through carefully engineered ensembles and effective prompt-based\nreasoning strategies. Our findings highlight the potential of hybrid models\nintegrating traditional IR techniques with contemporary generative models,\nproviding a valuable reference for future advancements in legal information\nprocessing.", "AI": {"tldr": "This paper summarizes the NOWJ team's methodologies and results in the COLIEE 2025 competition, focusing on advancements in Legal Case Entailment, leveraging various models and techniques.", "motivation": "To advance legal information processing by integrating traditional information retrieval techniques with contemporary generative models.", "method": "A two-stage retrieval system combining lexical-semantic filtering and contextualized LLM analysis, alongside various pre-ranking models and embedding-based representations.", "result": "Achieved first place in the Legal Case Entailment task with an F1 score of 0.3195, demonstrating strong performance across multiple tasks and robust methodology.", "conclusion": "The study showcases the effectiveness of hybrid models in legal information tasks and suggests a path for future methodologies in this domain.", "key_contributions": ["First place in Legal Case Entailment with innovative retrieval methods", "Integration of traditional IR methods with modern AI models", "Deployment of advanced LLMs and context-aware reasoning strategies"], "limitations": "", "keywords": ["Legal Case Entailment", "Information Retrieval", "Large Language Models"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.08459", "pdf": "https://arxiv.org/pdf/2509.08459.pdf", "abs": "https://arxiv.org/abs/2509.08459", "title": "Printegrated Circuits: Personal Fabrication of 3D Printed Devices with Embedded PCBs", "authors": ["Oliver Child", "Ollie Hanton", "Jack Dawson", "Steve Hodges", "Mike Fraser"], "categories": ["cs.HC"], "comment": null, "summary": "Consumer-level multi-material 3D printing with conductive thermoplastics\nenables fabrication of interactive elements for bespoke tangible devices.\nHowever, large feature sizes, high resistance materials, and limitations of\nprintable control circuitry mean that deployable devices cannot be printed\nwithout post-print assembly steps. To address these challenges, we present\nPrintegrated Circuits, a technique that uses traditional electronics as\nmaterial to 3D print self-contained interactive objects. Embedded PCBs are\nplaced into recesses during a pause in the print, and through a process we term\n\\textit{Prinjection}, conductive filament is injected into their plated-through\nholes. This automatically creates reliable electrical and mechanical contact,\neliminating the need for manual wiring or bespoke connectors. We describe the\ncustom machine code generation that supports our approach, and characterise its\nelectrical and mechanical properties. With our 6 demonstrations, we highlight\nhow the Printegrated Circuits process fits into existing design and prototyping\nworkflows as well as informs future research agendas.", "AI": {"tldr": "The paper presents Printegrated Circuits, a new 3D printing technique that integrates traditional electronics into self-contained interactive objects, eliminating the need for post-print assembly steps.", "motivation": "To overcome the challenges of consumer-level multi-material 3D printing, such as high resistance materials and the need for post-print assembly in interactive device fabrication.", "method": "The technique uses a process called 'Prinjection' to inject conductive filament into the plated-through holes of embedded PCBs during a pause in the 3D printing process, creating reliable electrical and mechanical contacts without manual wiring.", "result": "The Printegrated Circuits technique successfully integrates electronic components during 3D printing, demonstrated through six practical examples, showcasing its reliability and integration into design workflows.", "conclusion": "Printegrated Circuits offers a streamlined approach for creating interactive devices and has potential implications for future research and development in the field of rapid prototyping and electronics integration.", "key_contributions": ["Introduces Printegrated Circuits technique for integrating electronics in 3D printed devices", "Eliminates manual assembly steps with automatic electrical connections", "Demonstrates practical applications within existing design workflows"], "limitations": "", "keywords": ["3D printing", "conductive thermoplastics", "interactive devices", "Prinjection", "embedded PCBs"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08032", "pdf": "https://arxiv.org/pdf/2509.08032.pdf", "abs": "https://arxiv.org/abs/2509.08032", "title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery", "authors": ["Fengyu She", "Nan Wang", "Hongfei Wu", "Ziyi Wan", "Jingmian Wang", "Chang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific literature is growing exponentially, creating a critical\nbottleneck for researchers to efficiently synthesize knowledge. While\ngeneral-purpose Large Language Models (LLMs) show potential in text processing,\nthey often fail to capture scientific domain-specific nuances (e.g., technical\njargon, methodological rigor) and struggle with complex scientific tasks,\nlimiting their utility for interdisciplinary research. To address these gaps,\nthis paper presents SciGPT, a domain-adapted foundation model for scientific\nliterature understanding and ScienceBench, an open source benchmark tailored to\nevaluate scientific LLMs.\n  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:\n(1) low-cost domain distillation via a two-stage pipeline to balance\nperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention\nmechanism that cuts memory consumption by 55\\% for 32,000-token long-document\nreasoning; and (3) knowledge-aware adaptation integrating domain ontologies to\nbridge interdisciplinary knowledge gaps.\n  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in\ncore scientific tasks including sequence labeling, generation, and inference.\nIt also exhibits strong robustness in unseen scientific tasks, validating its\npotential to facilitate AI-augmented scientific discovery.", "AI": {"tldr": "The paper introduces SciGPT, a domain-adapted model for understanding scientific literature and ScienceBench, a benchmark to evaluate scientific LLMs, aimed at improving performance in science-specific tasks.", "motivation": "To address the inefficiencies in synthesizing scientific knowledge due to the limitations of general-purpose Large Language Models in handling domain-specific nuances.", "method": "SciGPT is built on the Qwen3 architecture and features a two-stage low-cost domain distillation process, a Sparse Mixture-of-Experts attention mechanism for memory efficiency, and knowledge-aware adaptation using domain ontologies.", "result": "SciGPT outperforms GPT-4o in key scientific tasks such as sequence labeling, generation, and inference on the ScienceBench, showing robust performance in unseen tasks.", "conclusion": "SciGPT demonstrates the potential for AI-augmented scientific discovery by effectively addressing the limitations of existing LLMs in the scientific domain.", "key_contributions": ["Introduction of a domain-adapted foundation model for scientific literature understanding.", "Development of ScienceBench, an open-source benchmark for evaluating scientific LLMs.", "Innovative low-cost domain distillation and memory-efficient attention mechanisms."], "limitations": "", "keywords": ["SciGPT", "Large Language Models", "ScienceBench", "domain adaptation", "AI in science"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.08514", "pdf": "https://arxiv.org/pdf/2509.08514.pdf", "abs": "https://arxiv.org/abs/2509.08514", "title": "Bias in the Loop: How Humans Evaluate AI-Generated Suggestions", "authors": ["Jacob Beck", "Stephanie Eckman", "Christoph Kern", "Frauke Kreuter"], "categories": ["cs.HC", "stat.ML"], "comment": null, "summary": "Human-AI collaboration increasingly drives decision-making across industries,\nfrom medical diagnosis to content moderation. While AI systems promise\nefficiency gains by providing automated suggestions for human review, these\nworkflows can trigger cognitive biases that degrade performance. We know little\nabout the psychological factors that determine when these collaborations\nsucceed or fail. We conducted a randomized experiment with 2,784 participants\nto examine how task design and individual characteristics shape human responses\nto AI-generated suggestions. Using a controlled annotation task, we manipulated\nthree factors: AI suggestion quality in the first three instances, task burden\nthrough required corrections, and performance-based financial incentives. We\ncollected demographics, attitudes toward AI, and behavioral data to assess four\nperformance metrics: accuracy, correction activity, overcorrection, and\nundercorrection. Two patterns emerged that challenge conventional assumptions\nabout human-AI collaboration. First, requiring corrections for flagged AI\nerrors reduced engagement and increased the tendency to accept incorrect\nsuggestions, demonstrating how cognitive shortcuts influence collaborative\noutcomes. Second, individual attitudes toward AI emerged as the strongest\npredictor of performance, surpassing demographic factors. Participants\nskeptical of AI detected errors more reliably and achieved higher accuracy,\nwhile those favorable toward automation exhibited dangerous overreliance on\nalgorithmic suggestions. The findings reveal that successful human-AI\ncollaboration depends not only on algorithmic performance but also on who\nreviews AI outputs and how review processes are structured. Effective human-AI\ncollaborations require consideration of human psychology: selecting diverse\nevaluator samples, measuring attitudes, and designing workflows that counteract\ncognitive biases.", "AI": {"tldr": "This paper examines the psychological factors influencing human-AI collaboration in decision-making, revealing how task design and individual characteristics impact performance and engagement.", "motivation": "To understand the psychological factors that affect the success or failure of human-AI collaborations, particularly in decision-making contexts.", "method": "A randomized experiment involving 2,784 participants was conducted, manipulating AI suggestion quality, task burden through required corrections, and performance-based financial incentives. Data on demographics, attitudes toward AI, and behavioral metrics were collected.", "result": "Findings indicated that cognitive shortcuts negatively impact engagement and that individual attitudes towards AI are critical predictors of performance, with skeptics performing better than proponents.", "conclusion": "Effective human-AI collaboration needs to account for human psychology and biases by structuring review processes and considering the attitudes of human reviewers.", "key_contributions": ["Identification of cognitive biases affecting human acceptance of AI suggestions", "Demonstration of the impact of individual attitudes toward AI on decision-making performance", "Recommendations for structuring workflows to enhance human-AI collaboration"], "limitations": "", "keywords": ["Human-AI collaboration", "Cognitive biases", "Performance metrics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.08075", "pdf": "https://arxiv.org/pdf/2509.08075.pdf", "abs": "https://arxiv.org/abs/2509.08075", "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "authors": ["Flor Miriam Plaza-del-Arco", "Paul RÃ¶ttger", "Nino Scherrer", "Emanuele Borgonovo", "Elmar Plischke", "Dirk Hovy"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors.", "AI": {"tldr": "This paper investigates the impact of sociodemographic personas on false refusals in large language models, revealing that model capability reduces persona influence.", "motivation": "With the growing integration of LLMs in daily life, it is crucial to understand how personalization through sociodemographic personas can inadvertently lead to false refusals in user requests.", "method": "The study measures the impact of 15 sociodemographic personas across 16 different models and 3 tasks, utilizing a Monte Carlo-based method to quantify false refusals in a sample-efficient manner.", "result": "The analysis indicates that more capable models are less influenced by personas, yet certain personas increase false refusals in specific models, highlighting biases in alignment strategies and safety mechanisms.", "conclusion": "The research suggests that the effects of personas on false refusals may have been overestimated, attributed instead to model choice and task specifics.", "key_contributions": ["Quantified the impact of sociodemographic personas on false refusals in LLMs.", "Developed a sample-efficient Monte Carlo-based method for measuring false refusals.", "Showed that model capability mitigates the impact of personas on refusal rates."], "limitations": "The study focuses on a limited set of personas and models, which may not represent all aspects of LLM behavior.", "keywords": ["Large Language Models", "False Refusal", "Sociodemographic Personas", "Bias", "Model Safety"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08539", "pdf": "https://arxiv.org/pdf/2509.08539.pdf", "abs": "https://arxiv.org/abs/2509.08539", "title": "Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning", "authors": ["Lukas Schach", "Christian Rack", "Ryan P. McMahan", "Marc Erich Latoschik"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This paper examines the generalization capacity of two state-of-the-art\nclassification and similarity learning models in reliably identifying users\nbased on their motions in various Extended Reality (XR) applications. We\ndeveloped a novel dataset containing a wide range of motion data from 49 users\nin five different XR applications: four XR games with distinct tasks and action\npatterns, and an additional social XR application with no predefined task sets.\nThe dataset is used to evaluate the performance and, in particular, the\ngeneralization capacity of the two models across applications. Our results\nindicate that while the models can accurately identify individuals within the\nsame application, their ability to identify users across different XR\napplications remains limited. Overall, our results provide insight into current\nmodels generalization capabilities and suitability as biometric methods for\nuser verification and identification. The results also serve as a much-needed\nrisk assessment of hazardous and unwanted user identification in XR and\nMetaverse applications. Our cross-application XR motion dataset and code are\nmade available to the public to encourage similar research on the\ngeneralization of motion-based user identification in typical Metaverse\napplication use cases.", "AI": {"tldr": "This paper investigates user identification based on motion data in Extended Reality (XR) applications using two classification models.", "motivation": "To evaluate how well current models can generalize user identification across different XR applications, which is crucial for biometric verification.", "method": "A novel dataset was developed consisting of motion data from 49 users across five XR applications, including games and social interactions. The performance of two classification models was assessed in identifying users based on this dataset.", "result": "Models were effective in identifying users within the same application but struggled with generalization across different XR applications.", "conclusion": "Current models exhibit limited generalization capabilities, highlighting the need for improved biometric methods in XR applications and emphasizing the potential risks of user misidentification.", "key_contributions": ["Development of a novel dataset for XR motion data", "Evaluation of user identification models across different applications", "Insights into the limitations of biometric user identification in XR and Metaverse."], "limitations": "Limited generalization ability across different XR environments; focus on motion data only.", "keywords": ["Extended Reality", "user identification", "biometric verification", "motion data", "Metaverse"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08093", "pdf": "https://arxiv.org/pdf/2509.08093.pdf", "abs": "https://arxiv.org/abs/2509.08093", "title": "Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression", "authors": ["Nathaniel Imel", "Noga Zaslavsky"], "categories": ["cs.CL"], "comment": null, "summary": "Converging evidence suggests that systems of semantic categories across human\nlanguages achieve near-optimal compression via the Information Bottleneck (IB)\ncomplexity-accuracy principle. Large language models (LLMs) are not trained for\nthis objective, which raises the question: are LLMs capable of evolving\nefficient human-like semantic systems? To address this question, we focus on\nthe domain of color as a key testbed of cognitive theories of categorization\nand replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two\ninfluential human behavioral studies. First, we conduct an English color-naming\nstudy, showing that Gemini aligns well with the naming patterns of native\nEnglish speakers and achieves a significantly high IB-efficiency score, while\nLlama exhibits an efficient but lower complexity system compared to English.\nSecond, to test whether LLMs simply mimic patterns in their training data or\nactually exhibit a human-like inductive bias toward IB-efficiency, we simulate\ncultural evolution of pseudo color-naming systems in LLMs via iterated\nin-context language learning. We find that akin to humans, LLMs iteratively\nrestructure initially random systems towards greater IB-efficiency and\nincreased alignment with patterns observed across the world's languages. These\nfindings demonstrate that LLMs are capable of evolving perceptually grounded,\nhuman-like semantic systems, driven by the same fundamental principle that\ngoverns semantic efficiency across human languages.", "AI": {"tldr": "This paper investigates whether large language models (LLMs) can evolve efficient semantic systems similar to those found in human languages, using color categorization as a test case.", "motivation": "The study aims to understand if LLMs, not designed with the Information Bottleneck principle in mind, can develop human-like semantic system efficiencies.", "method": "The authors replicate human color-naming studies using Gemini 2.0-flash and Llama 3.3-70B-Instruct, measuring their IB-efficiency and alignment with native English speakers.", "result": "Gemini achieves a high IB-efficiency score and aligns closely with human color naming, while Llama exhibits efficiency but lower complexity. Both models evolve grammar-like structures through simulated cultural evolution.", "conclusion": "LLMs can evolve perceptually grounded semantic systems and demonstrate inductive biases toward IB-efficiency similar to humans.", "key_contributions": ["Demonstration of LLMs' ability to approximate human-like semantic categorization.", "Evaluation of color naming accuracy and efficiency based on the Information Bottleneck principle.", "Simulation of cultural evolution in LLMs leading to improved semantic systems."], "limitations": "The study is limited to color categorization and may not generalize to other semantic domains.", "keywords": ["semantic categories", "large language models", "Information Bottleneck", "color naming", "cultural evolution"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2509.08540", "pdf": "https://arxiv.org/pdf/2509.08540.pdf", "abs": "https://arxiv.org/abs/2509.08540", "title": "Formal verification for robo-advisors: Irrelevant for subjective end-user trust, yet decisive for investment behavior?", "authors": ["Alina Tausch", "Magdalena Wischnewski", "Mustafa Yalciner", "Daniel Neider"], "categories": ["cs.HC"], "comment": null, "summary": "This online-vignette study investigates the impact of certification and\nverification as measures for quality assurance of AI on trust and use of a\nrobo-advisor. Confronting 520 participants with an imaginary situation where\nthey were using an online banking service to invest their inherited money, we\nformed 4 experimental groups. EG1 achieved no further information of their\nrobo-advisor, while EG2 was informed that their robo-advisor was certified by a\nreliable agency for unbiased processes, and EG3 was presented with a formally\nverified robo-advisor that was proven to consider their investment preferences.\nA control group was presented a remote certified human financial advisor. All\ngroups had to decide on how much of their 10,000 euros they would give to their\nadvisor to autonomously invest for them and report on trust and perceived\ndependability. A second manipulation happened afterwards, confronting\nparticipants with either a successful or failed investment. Overall, our\nresults show that the level of quality assurance of the advisor had\nsurprisingly near to no effect of any of our outcome variables, except for\npeople's perception of their own mental model of the advisor. Descriptively,\ndifferences between investments show that seem to favor a verified advisor with\na median investment of 65,000 euros (vs. 50,000). Success or failure\ninformation, though influences only partially by advisor quality, has been\nperceived as a more important clue for advisor trustworthiness, leading to\nsubstantially different trust and dependability ratings. The study shows the\nimportance of thoroughly investigating not only trust, but also trusting\nbehavior with objective measures. It also underlines the need for future\nresearch on formal verification, that might be the gold standard in proving AI\nmathematically, but seems not to take full effect as a cue for trustworthiness\nfor end-users.", "AI": {"tldr": "This study explores how certification and verification of AI impact trust in a robo-advisor using an online banking scenario with 520 participants.", "motivation": "To understand how measures for quality assurance of AI influence user trust and behavior, specifically in contexts like robo-advising.", "method": "An online-vignette study with 520 participants, divided into four experimental groups based on the type of information provided regarding a robo-advisor. Participants made investment decisions and reported on trust and dependability after receiving feedback on investment success or failure.", "result": "The type of quality assurance (certified vs. verified) had little impact on overall trust and investment amounts, although a verified advisor was descriptively favored for investment. The success or failure of investments significantly influenced trust ratings more than advisor quality.", "conclusion": "The findings suggest that, while verification might be seen as a gold standard in AI, it does not significantly enhance user trust. Further research is needed to explore the dynamics of trust behavior in AI interactions.", "key_contributions": ["Investigated the role of certification and verification in user trust toward robo-advisors.", "Demonstrated that success or failure feedback significantly impacts trust ratings.", "Highlighted the discrepancy between formal verification of AI and its perceived trustworthiness by end-users."], "limitations": "The study was conducted in a hypothetical scenario and might not fully capture real-world behaviors and perceptions.", "keywords": ["AI quality assurance", "trust in robo-advisors", "investment behavior", "formal verification", "user trust"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08105", "pdf": "https://arxiv.org/pdf/2509.08105.pdf", "abs": "https://arxiv.org/abs/2509.08105", "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion", "authors": ["Kosei Uemura", "David GuzmÃ¡n", "Quang Phuoc Nguyen", "Jesujoba Oluwadara Alabi", "En-shiun Annie Lee", "David Ifeoluwa Adelani"], "categories": ["cs.CL"], "comment": "under submission", "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings.", "AI": {"tldr": "MERLIN improves reasoning in low-resource languages through a two-stage model-stacking framework.", "motivation": "To enhance reasoning accuracy in low-resource languages (LRLs) using large language models, which struggle compared to mid and high-resource languages.", "method": "A two-stage model-stacking framework employing a curriculum learning strategy, adapting a small set of DoRA weights.", "result": "MERLIN achieves a +12.9 percentage point improvement in exact-match accuracy on the AfriMGSM benchmark over existing methods, outperforming GPT-4o-mini and showing consistent gains on other benchmarks.", "conclusion": "The proposed MERLIN framework effectively enhances model performance in both low and high-resource language settings.", "key_contributions": ["Introduction of a two-stage model-stacking framework for LRLs", "Application of curriculum learning strategy to improve accuracy", "Demonstrated superior performance over existing models like MindMerger and GPT-4o-mini."], "limitations": "", "keywords": ["low-resource languages", "large language models", "curriculum learning", "DoRA weights", "benchmarking"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.08548", "pdf": "https://arxiv.org/pdf/2509.08548.pdf", "abs": "https://arxiv.org/abs/2509.08548", "title": "Embedding Empathy into Visual Analytics: A Framework for Person-Centred Dementia Care", "authors": ["Rhiannon Owen", "Jonathan C. Roberts"], "categories": ["cs.HC"], "comment": "7 pages, 10 figures, accepted for publication in Proceedings IEEE VIS\n  2025", "summary": "Dementia care requires healthcare professionals to balance a patient's\nmedical needs with a deep understanding of their personal needs, preferences,\nand emotional cues. However, current digital tools prioritise quantitative\nmetrics over empathetic engagement,limiting caregivers ability to develop a\ndeeper personal understanding of their patients. This paper presents an empathy\ncentred visualisation framework, developed through a design study, to address\nthis gap. The framework integrates established principles of person centred\ncare with empathy mapping methodologies to encourage deeper engagement. Our\nmethodology provides a structured approach to designing for indirect end users,\npatients whose experience is shaped by a tool they may not directly interact\nwith. To validate the framework, we conducted evaluations with healthcare\nprofessinals, including usability testing of a working prototype and a User\nExperience Questionnaire study. Results suggest the feasibility of the\nframework, with participants highlighting its potential to support a more\npersonal and empathetic relationship between medical staff and patients. The\nwork starts to explore how empathy could be systematically embedded into\nvisualisation design, as we contribute to ongoing efforts in the data\nvisualisation community to support human centred, interpretable, and ethically\naligned clinical care, addressing the urgent need to improve dementia patients\nexperiences in hospital settings.", "AI": {"tldr": "This paper presents an empathy-centred visualization framework aimed at improving dementia care through enhanced healthcare professional-patient engagement.", "motivation": "Current digital tools in dementia care prioritize quantitative metrics over empathetic engagement, limiting caregivers' ability to understand patients' personal needs.", "method": "The framework integrates person-centered care principles with empathy mapping methodologies, developed through a design study.", "result": "Usability testing and evaluations conducted with healthcare professionals suggest the framework is feasible and can support more empathetic relationships between medical staff and patients.", "conclusion": "Embedding empathy systematically into visualization design can enhance human-centered clinical care, addressing challenges faced by dementia patients in hospital settings.", "key_contributions": ["Development of an empathy-centred visualization framework for dementia care.", "Integration of person-centered care principles with empathy mapping methodologies.", "Validation of the framework through usability testing and user experience evaluations."], "limitations": "", "keywords": ["dementia care", "empathy", "visualization framework", "human-centered design", "healthcare"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.08146", "pdf": "https://arxiv.org/pdf/2509.08146.pdf", "abs": "https://arxiv.org/abs/2509.08146", "title": "Bias after Prompting: Persistent Discrimination in Large Language Models", "authors": ["Nivedha Sivakumar", "Natalie Mackraz", "Samira Khorshidi", "Krishna Patel", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A dangerous assumption that can be made from prior work on the bias transfer\nhypothesis (BTH) is that biases do not transfer from pre-trained large language\nmodels (LLMs) to adapted models. We invalidate this assumption by studying the\nBTH in causal models under prompt adaptations, as prompting is an extremely\npopular and accessible adaptation strategy used in real-world applications. In\ncontrast to prior work, we find that biases can transfer through prompting and\nthat popular prompt-based mitigation methods do not consistently prevent biases\nfrom transferring. Specifically, the correlation between intrinsic biases and\nthose after prompt adaptation remain moderate to strong across demographics and\ntasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age\n(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we\nfind that biases remain strongly correlated when varying few-shot composition\nparameters, such as sample size, stereotypical content, occupational\ndistribution and representational balance (rho >= 0.90). We evaluate several\nprompt-based debiasing strategies and find that different approaches have\ndistinct strengths, but none consistently reduce bias transfer across models,\ntasks or demographics. These results demonstrate that correcting bias, and\npotentially improving reasoning ability, in intrinsic models may prevent\npropagation of biases to downstream tasks.", "AI": {"tldr": "This paper investigates the bias transfer hypothesis (BTH) in large language models (LLMs) during prompt adaptation, revealing that biases can transfer and current debiasing strategies are ineffective.", "motivation": "To challenge the assumption that biases from pre-trained LLMs do not transfer to adapted models and to understand the impact of prompt adaptations on bias transfer.", "method": "The authors conducted empirical studies examining biases in causal models under prompt adaptations, analyzing correlations of biases across various demographics and tasks, and evaluating prompt-based debiasing strategies.", "result": "The study found strong correlations between intrinsic biases and prompt-adapted biases across demographics (e.g., gender, age, religion) with moderate to strong correlation coefficients, indicating that biases can indeed transfer through prompting.", "conclusion": "Addressing biases in intrinsic models may help prevent their propagation to downstream tasks, although current prompt-based debiasing strategies do not consistently mitigate bias transfer.", "key_contributions": ["Empirical validation of bias transfer through prompt adaptations in LLMs.", "Quantitative analysis of correlations between intrinsic and adapted biases across demographics.", "Critical evaluation of various prompt-based debiasing strategies."], "limitations": "The effectiveness of prompt-based mitigation methods varies, and none consistently reduce bias transfer across models, tasks, or demographics.", "keywords": ["bias transfer", "large language models", "prompt adaptation", "debiasing strategies", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.08554", "pdf": "https://arxiv.org/pdf/2509.08554.pdf", "abs": "https://arxiv.org/abs/2509.08554", "title": "Acceptability of AI Assistants for Privacy: Perceptions of Experts and Users on Personalized Privacy Assistants", "authors": ["Meihe Xu", "Aurelia TamÃ²-Larrieux", "Arianna Rossi"], "categories": ["cs.HC"], "comment": null, "summary": "Individuals increasingly face an overwhelming number of tasks and decisions.\nTo cope with the new reality, there is growing research interest in developing\nintelligent agents that can effectively assist people across various aspects of\ndaily life in a tailored manner, with privacy emerging as a particular area of\napplication. Artificial intelligence (AI) assistants for privacy, such as\npersonalized privacy assistants (PPAs), have the potential to automatically\nexecute privacy decisions based on users' pre-defined privacy preferences,\nsparing them the mental effort and time usually spent on each privacy decision.\nThis helps ensure that, even when users feel overwhelmed or resigned about\nprivacy, the decisions made by PPAs still align with their true preferences and\nbest interests. While research has explored possible designs of such agents,\nuser and expert perspectives on the acceptability of such AI-driven solutions\nremain largely unexplored. In this study, we conducted five focus groups with\ndomain experts (n = 11) and potential users (n = 26) to uncover key themes\nshaping the acceptance of PPAs. Factors influencing the acceptability of AI\nassistants for privacy include design elements (such as information sources\nused by the agent), external conditions (such as regulation and literacy\neducation), and systemic conditions (e.g., public or market providers and the\nneed to avoid monopoly) to PPAs. These findings provide theoretical extensions\nto technology acceptance models measuring PPAs, insights on design, and policy\nimplications for PPAs, as well as broader implications for the design of AI\nassistants.", "AI": {"tldr": "This study explores user and expert perspectives on the acceptability of personalized privacy assistants (PPAs) through focus groups, identifying key themes influencing their acceptance.", "motivation": "The increasing complexity of daily tasks and decisions, combined with concerns over privacy, necessitates intelligent AI agents to assist users in making privacy-related decisions aligned with their preferences.", "method": "Conducted five focus groups with domain experts (n = 11) and potential users (n = 26) to identify factors influencing the acceptability of AI-driven privacy assistants.", "result": "Key themes identified include design elements, external conditions like regulation and education, and systemic conditions impacting the acceptability of PPAs.", "conclusion": "The findings enhance technology acceptance models for PPAs, provide design and policy insights, and suggest broader implications for AI assistant design.", "key_contributions": ["Identification of factors influencing the acceptability of personalized privacy assistants", "Theoretical extensions to existing technology acceptance models", "Insights for designing AI-driven privacy solutions"], "limitations": "", "keywords": ["Privacy assistants", "AI-driven solutions", "Technology acceptance models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.08150", "pdf": "https://arxiv.org/pdf/2509.08150.pdf", "abs": "https://arxiv.org/abs/2509.08150", "title": "Verbalized Algorithms", "authors": ["Supriya Lall", "Christian Farrell", "Hari Pathanjaly", "Marko Pavic", "Sarvesh Chezhian", "Masataro Asai"], "categories": ["cs.CL"], "comment": "Submitted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.", "AI": {"tldr": "This paper introduces 'verbalized algorithms' (VAs) that leverage classical algorithms to improve the reliability of large language models (LLMs) for reasoning tasks by decomposing tasks into simpler operations.", "motivation": "The motivation is to enhance LLMs' reliability for reasoning tasks by utilizing established theoretical frameworks of classical algorithms.", "method": "The paper presents a paradigm called verbalized algorithms (VAs) that breaks down reasoning tasks into elementary operations using LLMs as oracles for these operations.", "result": "The authors demonstrate the effectiveness of VAs in applications such as sorting and clustering, showcasing improved performance by limiting the LLMâs scope to these structured tasks.", "conclusion": "The approach suggests that integrating classical algorithms with LLMs can yield reliable results for specific reasoning tasks.", "key_contributions": ["Introduction of verbalized algorithms for LLMs", "Demonstration of VAs in sorting and clustering tasks", "Reliability of LLMs enhanced through task decomposition"], "limitations": "", "keywords": ["verbalized algorithms", "large language models", "reasoning tasks", "sorting", "clustering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08589", "pdf": "https://arxiv.org/pdf/2509.08589.pdf", "abs": "https://arxiv.org/abs/2509.08589", "title": "Visual Analysis of Time-Dependent Observables in Cell Signaling Simulations", "authors": ["Lena Cibulski", "Fiete Haack", "Adelinde Uhrmacher", "Stefan Bruckner"], "categories": ["cs.HC"], "comment": null, "summary": "The ability of a cell to communicate with its environment is essential for\nkey cellular functions like replication, metabolism, or cell fate decisions.\nThe involved molecular mechanisms are highly dynamic and difficult to capture\nexperimentally. Simulation studies offer a valuable means for exploring and\npredicting how cell signaling processes unfold. We present a design study on\nthe visual analysis of such studies to support 1) modelers in calibrating model\nparameters such that the simulated signal responses over time reflect reference\nbehavior from cell biology research and 2) cell biologists in exploring the\ninfluence of receptor trafficking on the efficiency of signal transmission\nwithin the cell. We embed time series plots into parallel coordinates to enable\na simultaneous analysis of model parameters and temporal outputs. A usage\nscenario illustrates how our approach assists with typical tasks such as\nassessing the plausibility of temporal outputs or their sensitivity across\nmodel configurations.", "AI": {"tldr": "This paper presents a design study focused on visual analysis techniques for simulating cell signaling processes, enabling modelers and biologists to better understand and calibrate cellular communication models.", "motivation": "To capture the dynamic molecular mechanisms of cell signaling, which are essential for cellular functions, and to aid in the calibration of simulation models.", "method": "The methodology involves embedding time series plots within parallel coordinates to allow simultaneous analysis of model parameters and time outputs, facilitating detailed exploration of model behaviors and effects of receptor trafficking.", "result": "The approach allows for effective assessment of the plausibility and sensitivity of temporal outputs across different model configurations, providing valuable insights for both modelers and biologists.", "conclusion": "The design study demonstrates the utility of advanced visual analysis methods in enhancing understanding of cell signaling dynamics and assists in model calibration and exploration of biological phenomena.", "key_contributions": ["Introduction of a novel visual analysis technique combining time series and parallel coordinates for cellular simulation studies.", "Support for model parameter calibration in relation to biological reference behaviors.", "Facilitation of exploration of receptor trafficking effects on signal transmission efficiency."], "limitations": "", "keywords": ["cell signaling", "visual analysis", "simulation studies", "model calibration", "biological analysis"], "importance_score": 3, "read_time_minutes": 12}}
{"id": "2509.08217", "pdf": "https://arxiv.org/pdf/2509.08217.pdf", "abs": "https://arxiv.org/abs/2509.08217", "title": "Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions", "authors": ["Eve Fleisig", "Matthias Orlikowski", "Philipp Cimiano", "Dan Klein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "For machine learning datasets to accurately represent diverse opinions in a\npopulation, they must preserve variation in data labels while filtering out\nspam or low-quality responses. How can we balance annotator reliability and\nrepresentation? We empirically evaluate how a range of heuristics for annotator\nfiltering affect the preservation of variation on subjective tasks. We find\nthat these methods, designed for contexts in which variation from a single\nground-truth label is considered noise, often remove annotators who disagree\ninstead of spam annotators, introducing suboptimal tradeoffs between accuracy\nand label diversity. We find that conservative settings for annotator removal\n(<5%) are best, after which all tested methods increase the mean absolute error\nfrom the true average label. We analyze performance on synthetic spam to\nobserve that these methods often assume spam annotators are less random than\nreal spammers tend to be: most spammers are distributionally indistinguishable\nfrom real annotators, and the minority that are distinguishable tend to give\nfixed answers, not random ones. Thus, tasks requiring the preservation of\nvariation reverse the intuition of existing spam filtering methods: spammers\ntend to be less random than non-spammers, so metrics that assume variation is\nspam fare worse. These results highlight the need for spam removal methods that\naccount for label diversity.", "AI": {"tldr": "This paper examines how annotator filtering heuristics impact the variation in subjective machine learning datasets, finding that many methods reduce diversity by incorrectly removing dissenting annotators instead of spam.", "motivation": "To improve the representation of diverse opinions in machine learning datasets by ensuring a balance between annotator reliability and data label variation.", "method": "Empirical evaluation of various annotator filtering heuristics to determine their effects on the preservation of variation in subjective tasks.", "result": "Conservative annotator removal settings (<5%) are optimal for maintaining label diversity, while most tested methods worsen mean absolute error beyond this point.", "conclusion": "Current spam filtering methods often misidentify spam annotators as they assume them to be less random than they are, suggesting a need for new approaches that prioritize label diversity.", "key_contributions": ["Identified optimal thresholds for annotator removal that preserve label diversity.", "Revealed the shortcomings of current spam filtering methods in subjective tasks.", "Proposed the need for spam removal techniques that account for the randomness of spammers."], "limitations": "", "keywords": ["machine learning", "annotator filtering", "label diversity", "subjective tasks", "spam detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08689", "pdf": "https://arxiv.org/pdf/2509.08689.pdf", "abs": "https://arxiv.org/abs/2509.08689", "title": "Augmenting speech transcripts of VR recordings with gaze, pointing, and visual context for multimodal coreference resolution", "authors": ["Riccardo Bovo", "Frederik Brudy", "George Fitzmaurice", "Fraser Anderson"], "categories": ["cs.HC"], "comment": null, "summary": "Understanding transcripts of immersive multimodal conversations is\nchallenging because speakers frequently rely on visual context and non-verbal\ncues, such as gestures and visual attention, which are not captured in speech\nalone. This lack of information makes coreferences resolution-the task of\nlinking ambiguous expressions like ``it'' or ``there'' to their intended\nreferents-particularly challenging. In this paper we present a system that\naugments VR speech transcript with eye-tracking laser pointing data, and scene\nmetadata to generate textual descriptions of non-verbal communication and the\ncorresponding objects of interest. To evaluate the system, we collected gaze,\ngesture, and voice data from 12 participants (6 pairs) engaged in an open-ended\ndesign critique of a 3D model of an apartment. Our results show a 26.5\\%\nimprovement in coreference resolution accuracy by a GPT model when using our\nmultimodal transcript compared to a speech-only baseline.", "AI": {"tldr": "This paper presents a system that integrates eye-tracking and scene metadata with VR speech transcripts to improve coreference resolution in multimodal conversations.", "motivation": "The challenge of understanding immersive multimodal conversations due to reliance on visual context and non-verbal cues is significant, especially in resolving ambiguous expressions.", "method": "The system combines VR speech transcripts with eye-tracking laser pointing data and scene metadata, allowing for the generation of textual descriptions that capture non-verbal communication.", "result": "Utilizing the multimodal transcript led to a 26.5% improvement in coreference resolution accuracy by a GPT model over a speech-only baseline.", "conclusion": "The integration of non-verbal cues and scene context significantly enhances the accuracy of coreference resolution in immersive conversations.", "key_contributions": ["Development of a multimodal transcript system for VR environments", "Demonstrated 26.5% improvement in coreference resolution accuracy", "Utilization of eye-tracking and gesture data to enhance communication understanding"], "limitations": "", "keywords": ["multimodal communication", "coreference resolution", "eye-tracking", "VR transcripts", "non-verbal cues"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.08304", "pdf": "https://arxiv.org/pdf/2509.08304.pdf", "abs": "https://arxiv.org/abs/2509.08304", "title": "Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection", "authors": ["Yehudit Aperstein", "Alon Gottlib", "Gal Benita", "Alexander Apartsin"], "categories": ["cs.CL"], "comment": "27 pages, 1 figure", "summary": "Understanding how information is shared across documents, regardless of the\nformat in which it is expressed, is critical for tasks such as information\nretrieval, summarization, and content alignment. In this work, we introduce a\nnovel framework for modelling Semantic Coverage Relations (SCR), which\nclassifies document pairs based on how their informational content aligns. We\ndefine three core relation types: equivalence, where both texts convey the same\ninformation using different textual forms or styles; inclusion, where one\ndocument fully contains the information of another and adds more; and semantic\noverlap, where each document presents partially overlapping content. To capture\nthese relations, we adopt a question answering (QA)-based approach, using the\nanswerability of shared questions across documents as an indicator of semantic\ncoverage. We construct a synthetic dataset derived from the SQuAD corpus by\nparaphrasing source passages and selectively omitting information, enabling\nprecise control over content overlap. This dataset allows us to benchmark\ngenerative language models and train transformer-based classifiers for SCR\nprediction. Our findings demonstrate that discriminative models significantly\noutperform generative approaches, with the RoBERTa-base model achieving the\nhighest accuracy of 61.4% and the Random Forest-based model showing the best\nbalance with a macro-F1 score of 52.9%. The results show that QA provides an\neffective lens for assessing semantic relations across stylistically diverse\ntexts, offering insights into the capacity of current models to reason about\ninformation beyond surface similarity. The dataset and code developed in this\nstudy are publicly available to support reproducibility.", "AI": {"tldr": "A framework for modeling Semantic Coverage Relations (SCR) in documents using a question answering approach to classify document pairs based on information alignment.", "motivation": "Understanding information sharing across documents is essential for tasks like information retrieval and summarization.", "method": "We utilize a question answering-based approach, classifying document pairs into equivalence, inclusion, and semantic overlap by analyzing the answerability of shared questions.", "result": "Discriminative models significantly outperform generative ones; RoBERTa-base achieved 61.4% accuracy, and Random Forest reached a macro-F1 score of 52.9%.", "conclusion": "The findings underscore the effectiveness of QA in assessing semantic relations beyond surface similarity, with a publicly available dataset for reproducibility.", "key_contributions": ["Introduction of the Semantic Coverage Relations framework", "Benchmarking generative and discriminative models using a synthetic dataset", "Public dataset and code for SCR prediction"], "limitations": "The RoBERTa-base model only achieved 61.4% accuracy, indicating room for improvement in understanding semantic coverage.", "keywords": ["Semantic Coverage Relations", "question answering", "document classification", "generative models", "discriminative models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.08345", "pdf": "https://arxiv.org/pdf/2509.08345.pdf", "abs": "https://arxiv.org/abs/2509.08345", "title": "Toward Subtrait-Level Model Explainability in Automated Writing Evaluation", "authors": ["Alejandro Andrade-Lotero", "Lee Becker", "Joshua Southerland", "Scott Hellman"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "Subtrait (latent-trait components) assessment presents a promising path\ntoward enhancing transparency of automated writing scores. We prototype\nexplainability and subtrait scoring with generative language models and show\nmodest correlation between human subtrait and trait scores, and between\nautomated and human subtrait scores. Our approach provides details to demystify\nscores for educators and students.", "AI": {"tldr": "This paper explores subtrait assessment using generative language models to enhance the transparency of automated writing scores, revealing correlations between human and automated scoring.", "motivation": "The goal is to improve transparency in automated writing assessments by introducing subtrait scoring with generative language models.", "method": "The paper presents a prototype of explainability and subtrait scoring, analyzing correlations between human subtrait scores and those generated by automated systems.", "result": "The study found a modest correlation between human and automated scores for both subtraits and overall traits, indicating some level of alignment between the two assessment methods.", "conclusion": "The approach offers deeper insights into writing scores, benefiting educators and students by making automated assessments more understandable.", "key_contributions": ["Prototype for explainability in writing scores", "Introduction of subtrait scoring with generative models", "Correlations found between human and automated scoring methods"], "limitations": "", "keywords": ["automated writing assessment", "explainability", "generative language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.08355", "pdf": "https://arxiv.org/pdf/2509.08355.pdf", "abs": "https://arxiv.org/abs/2509.08355", "title": "Automatic Detection of Inauthentic Templated Responses in English Language Assessments", "authors": ["Yashad Samant", "Lee Becker", "Scott Hellman", "Bradley Behan", "Sarah Hughes", "Joshua Southerland"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "In high-stakes English Language Assessments, low-skill test takers may employ\nmemorized materials called ``templates'' on essay questions to ``game'' or fool\nthe automated scoring system. In this study, we introduce the automated\ndetection of inauthentic, templated responses (AuDITR) task, describe a machine\nlearning-based approach to this task and illustrate the importance of regularly\nupdating these models in production.", "AI": {"tldr": "The paper discusses a method for detecting templated responses in English Language Assessments to improve automated scoring systems.", "motivation": "The prevalence of low-skill test takers using memorized templates to deceive automated scoring systems necessitates the development of detection methods.", "method": "A machine learning-based approach is presented for the automated detection of inauthentic, templated responses (AuDITR).", "result": "The study demonstrates the effectiveness of the presented detection method and emphasizes the importance of regularly updating the models in production.", "conclusion": "Regular updates to detection models improve the reliability of automated scoring in assessments.", "key_contributions": ["Introduction of the AuDITR task for detecting templated responses.", "Development of a machine learning approach for response detection.", "Emphasis on the necessity of model updates in production environments."], "limitations": "", "keywords": ["automated scoring", "language assessment", "machine learning", "templated responses", "model updating"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08358", "pdf": "https://arxiv.org/pdf/2509.08358.pdf", "abs": "https://arxiv.org/abs/2509.08358", "title": "<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs", "authors": ["Sergey Pletenev", "Daniil Moskovskiy", "Alexander Panchenko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.", "AI": {"tldr": "The paper investigates the use of LLM-generated synthetic toxic data for training detoxification models and finds significant performance drops compared to models trained on human data.", "motivation": "To address the gap in research on the application of LLMs in sensitive domains like text detoxification and to explore the potential of synthetic data.", "method": "Synthetic toxic data was generated using Llama 3 and Qwen models from neutral texts in the ParaDetox and SST-2 datasets. Performance was evaluated by fine-tuning models on this synthetic data and comparing them to those trained on human-generated data.", "result": "Models trained on synthetic toxic data performed up to 30% worse in joint metrics than those trained on human data due to a critical lexical diversity gap in the generated content.", "conclusion": "The study emphasizes the limitations of current LLMs in generating toxic content and underscores the need for diverse, human-annotated data in developing effective detoxification systems.", "key_contributions": ["Identified the performance gap between models trained on synthetic vs human data in detoxification tasks.", "Demonstrated that LLMs generate less varied toxic content, impacting the models' effectiveness.", "Highlighted the importance of human-annotated data for training robust detoxification models."], "limitations": "The study primarily focuses on two datasets, which may limit generalizability; further research is needed with more diverse datasets.", "keywords": ["Large Language Models", "Toxicity Detection", "Synthetic Data", "Detoxification", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08381", "pdf": "https://arxiv.org/pdf/2509.08381.pdf", "abs": "https://arxiv.org/abs/2509.08381", "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model", "authors": ["Yu Cheng Chih", "Yong Hao Hou"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER", "summary": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.", "AI": {"tldr": "This paper introduces ETLCH, a billion-parameter LLaMA-based model effectively fine-tuned for structured data extraction tasks, proving that smaller models can outperform larger ones at lower computational costs.", "motivation": "The need for effective structured data extraction in resource-constrained environments where larger language models are often impractical.", "method": "The authors present ETLCH, fine-tuned using low-rank adaptation on a few hundred to one thousand samples per task for tasks including JSON extraction, knowledge graph extraction, and named entity recognition.", "result": "ETLCH demonstrated superior performance compared to strong baselines across various metrics, achieving notable accuracy even with minimal data.", "conclusion": "The study concludes that well-tuned small models can deliver reliable structured outputs economically, making them viable for small teams and low-resource contexts.", "key_contributions": ["Introduction of the ETLCH model for structured data extraction tasks.", "Demonstration of performance gains with fewer training samples compared to larger models.", "Evidence that small models can effectively operate under low-resource conditions."], "limitations": "The paper mainly focuses on a limited number of tasks and the scalability of these results across more complex datasets is not tested extensively.", "keywords": ["structured data extraction", "small language models", "low-rank adaptation", "JSON extraction", "named entity recognition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08438", "pdf": "https://arxiv.org/pdf/2509.08438.pdf", "abs": "https://arxiv.org/abs/2509.08438", "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework", "authors": ["Jinzhong Ning", "Paerhati Tulajiang", "Yingying Le", "Yijia Zhang", "Yuanyuan Sun", "Hongfei Lin", "Haifeng Liu"], "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.", "AI": {"tldr": "This paper introduces the CommonVoice-SpeechRE dataset and a novel framework, RPG-MoGe, for improving Speech Relation Extraction from real human speech.", "motivation": "Existing Speech Relation Extraction datasets are limited in quantity and diversity, relying on synthetic data. There is a need for real human speech to improve model performance.", "method": "The authors propose a novel framework, RPG-MoGe, which includes a multi-order triplet generation ensemble strategy and CNN-based latent relation prediction heads for better cross-modal alignment and triplet generation.", "result": "Experiments show RPG-MoGe outperforms existing state-of-the-art methods in Speech Relation Extraction, utilizing a diverse dataset of 20,000 speech samples.", "conclusion": "The paper provides a new benchmark dataset and an effective solution for real-world Speech Relation Extraction tasks, available for public use.", "key_contributions": ["Introduction of CommonVoice-SpeechRE, a dataset with 20,000 human speech samples.", "Development of RPG-MoGe framework for multi-order triplet generation.", "Enhanced cross-modal alignment through CNN-based relation prompts."], "limitations": "", "keywords": ["Speech Relation Extraction", "CommonVoice-SpeechRE", "Multi-Order Generative Ensemble"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2501.05434", "pdf": "https://arxiv.org/pdf/2501.05434.pdf", "abs": "https://arxiv.org/abs/2501.05434", "title": "GraspR: A Computational Model of Spatial User Preferences for Adaptive Grasp UI Design", "authors": ["Arthur Caetano", "Yunhao Luo", "Adwait Sharma", "Misha Sra"], "categories": ["cs.HC", "H.5.2"], "comment": null, "summary": "Grasp User Interfaces (grasp UIs) enable dual-tasking in XR by allowing\ninteraction with digital content while holding physical objects. However,\ncurrent grasp UI design practices face a fundamental challenge: existing\napproaches either capture user preferences through labor-intensive elicitation\nstudies that are difficult to scale or rely on biomechanical models that\noverlook subjective factors. We introduce GraspR, the first computational model\nthat predicts user preferences for single-finger microgestures in grasp UIs.\nOur data-driven approach combines the scalability of computational methods with\nhuman preference modeling, trained on 1,520 preferences collected via a\ntwo-alternative forced choice paradigm across eight participants and four\nfrequently used grasp variations. We demonstrate GraspR's effectiveness through\na working prototype that dynamically adjusts interface layouts across four\neveryday tasks. We release both the dataset and code to support future research\nin adaptive grasp UIs.", "AI": {"tldr": "GraspR is a computational model that predicts user preferences for microgestures in grasp UIs, enhancing dual-tasking in XR environments.", "motivation": "To address the limitations of existing grasp UI design practices, which are either labor-intensive or ignore subjective user factors.", "method": "A data-driven approach utilizing a two-alternative forced choice paradigm to collect user preferences, trained on 1,520 preferences from eight participants across various grasp variations.", "result": "GraspR effectively predicts user preferences, demonstrated through a prototype that adjusts interface layouts for common tasks.", "conclusion": "GraspR provides a scalable and adaptive solution for grasp UIs, supporting further research with released dataset and code.", "key_contributions": ["Introduction of the GraspR model for predicting user preferences in grasp UIs", "Combination of computational methods with human preference modeling", "Release of dataset and code for future research support"], "limitations": "", "keywords": ["Grasp User Interfaces", "Human-Computer Interaction", "Microgestures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08463", "pdf": "https://arxiv.org/pdf/2509.08463.pdf", "abs": "https://arxiv.org/abs/2509.08463", "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey", "authors": ["Fanzhen Liu", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Jia Wu", "Jian Yang", "Quan Z. Sheng"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "Accepted to the Main Conference of EMNLP 2025. Resources are\n  available at\n  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks", "summary": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.", "AI": {"tldr": "This paper reviews adversarial attacks on automated fact-checking systems, categorizing methodologies, evaluating impacts, and examining defenses.", "motivation": "Misinformation is widespread, and fact-checking is essential for verifying claims. However, automated fact-checking systems are vulnerable to adversarial attacks that can undermine their effectiveness.", "method": "The paper conducts a comprehensive review of existing adversarial attack methodologies against automated fact-checking systems, categorizing them and assessing their impacts and resilience.", "result": "The survey identifies key challenges in adversarial attacks on fact-checking systems and discusses recent advancements in adversary-aware defenses.", "conclusion": "There is an urgent need for developing resilient fact-checking frameworks that can withstand adversarial manipulations to maintain high verification accuracy.", "key_contributions": ["First comprehensive review of adversarial attacks on automated fact-checking systems.", "Categorization of existing attack methodologies and evaluation of their impact.", "Discussion of recent advancements in defenses against adversarial attacks."], "limitations": "The comprehensive overview needs further exploration of open research questions in the field.", "keywords": ["adversarial attacks", "automated fact-checking", "resilience", "information verification", "defenses"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2501.08500", "pdf": "https://arxiv.org/pdf/2501.08500.pdf", "abs": "https://arxiv.org/abs/2501.08500", "title": "Visual Network Analysis in Immersive Environments: A Survey", "authors": ["Lucas Joos", "Maximilian T. Fischer", "Julius Rauscher", "Daniel A. Keim", "Tim Dwyer", "Falk Schreiber", "Karsten Klein"], "categories": ["cs.HC"], "comment": null, "summary": "The increasing complexity and volume of network data demand effective\nanalysis approaches, with visual exploration proving particularly beneficial.\nImmersive technologies, such as augmented reality, virtual reality, and large\ndisplay walls, have enabled the emerging field of immersive analytics, offering\nnew opportunities to enhance user engagement, spatial awareness, and\nproblem-solving. A growing body of work has explored immersive environments for\nnetwork visualisation, ranging from design studies to fully integrated\napplications across various domains. Despite these advancements, the field\nremains fragmented, lacking a clear description of the design space and a\nstructured overview of the aspects that have already been empirically\nevaluated. To address this gap, we present a survey of visual network analysis\nin immersive environments, covering 138 publications retrieved through a\nstructured pipeline. We systematically analyse the key aspects that define the\ndesign space, investigate their coverage in prior applications (n=87), and\nreview user evaluations (n=59) that provide empirical evidence for essential\ndesign-related questions. By synthesising experimental findings and evaluating\nexisting applications, we identify key achievements, highlight research gaps,\nand offer guidance for the design of future approaches. Additionally, we\nprovide an online resource to explore our results interactively, which will be\nupdated as new developments emerge.", "AI": {"tldr": "This paper presents a survey of visual network analysis in immersive environments, analyzing 138 publications to define the design space and empirical evaluations related to immersive analytics.", "motivation": "To address the fragmented nature of immersive analytics and provide a clear overview of the design space for visual network analysis.", "method": "A systematic analysis of 138 publications was conducted, covering various aspects of design and user evaluations in immersive environments for network visualization.", "result": "The analysis identifies key design aspects, empirical evidence from previous evaluations, and highlights existing research gaps in the field.", "conclusion": "This work synthesizes findings and offers guidance for future approaches in immersive analytics, along with an interactive online resource for further exploration.", "key_contributions": ["Comprehensive survey of immersive network visualization", "Identification of design space aspects", "Empirical evaluation of existing applications"], "limitations": "The survey may not cover newly published works after the retrieval process and may have bias in selected publications.", "keywords": ["immersive analytics", "network visualization", "user evaluation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.08480", "pdf": "https://arxiv.org/pdf/2509.08480.pdf", "abs": "https://arxiv.org/abs/2509.08480", "title": "Acquiescence Bias in Large Language Models", "authors": ["Daniel Braun"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.", "AI": {"tldr": "This study investigates acquiescence bias in Large Language Models (LLMs), revealing they tend to answer 'no' regardless of the context, unlike humans who tend to agree.", "motivation": "To understand the presence of acquiescence bias in LLMs, which could have implications for their reliability in survey-related tasks.", "method": "The study tests various LLMs across different tasks and languages (English, German, and Polish) to assess the tendency to show acquiescence bias.", "result": "LLMs consistently showed a bias towards responding 'no', which contrasts with human behavior of agreeing more often in surveys.", "conclusion": "This research highlights significant differences in response tendencies between humans and LLMs, raising questions about the interpretability and dependability of LLM outputs.", "key_contributions": ["Demonstration of LLMs' unique response bias compared to humans", "Analysis across multiple languages and models", "Implications for the use of LLMs in data collection or survey tasks"], "limitations": "Study limited to three languages and specific models; findings may not generalize to all LLMs.", "keywords": ["acquiescence bias", "Large Language Models", "survey responses", "LM interpretations", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.15463", "pdf": "https://arxiv.org/pdf/2501.15463.pdf", "abs": "https://arxiv.org/abs/2501.15463", "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "authors": ["Hua Shen", "Nicholas Clark", "Tanushree Mitra"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Paper", "summary": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps.", "AI": {"tldr": "This study evaluates the alignment between LLMs' stated values and their actions, introducing a framework called ValueActionLens for this assessment.", "motivation": "To explore the discrepancy between LLMs' stated values and their actions, addressing the 'Value-Action Gap' from psychology.", "method": "The authors developed ValueActionLens, creating a dataset of 14.8k actions across cultures and topics, and set up two evaluation tasks based on alignment measures.", "result": "Experiments revealed significant misalignment between LLMs' stated values and their actions, indicating potential harms from such gaps.", "conclusion": "The reliance on LLMs' stated values without considering context can lead to erroneous assumptions about their behavior; reasoned explanations can enhance alignment predictions.", "key_contributions": ["Introduction of the ValueActionLens framework for evaluation of LLMs' values and actions.", "Creation of a comprehensive dataset of value-informed actions across diverse cultures.", "Identification of misalignment patterns and potential harms from value-action gaps."], "limitations": "The study notes that the alignment varies significantly across scenarios and models, suggesting that one-size-fits-all solutions may be inadequate.", "keywords": ["LLM evaluation", "Value-Action Gap", "Context-aware evaluations", "Value-informed actions", "Alignment measures"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.08484", "pdf": "https://arxiv.org/pdf/2509.08484.pdf", "abs": "https://arxiv.org/abs/2509.08484", "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text", "authors": ["Pia Sommerauer", "Giulia Rambelli", "Tommaso Caselli"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP Findings 2025", "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group.", "AI": {"tldr": "This paper investigates the effects of persona-prompting on LLMs in relation to linguistic abstraction and stereotyping.", "motivation": "To explore how persona-prompting in LLMs may affect the representation of social groups and the risk of propagating stereotypes.", "method": "The study employs the Linguistic Expectancy Bias framework to analyze outputs from six open-weight LLMs under three prompting conditions, comparing persona-driven responses with generic AI outputs.", "result": "The analysis reveals limited effectiveness of persona-prompting on reducing linguistic abstraction related to stereotyping, highlighting concerns about its ecological validity.", "conclusion": "Persona-prompting does not significantly mitigate abstraction in language, raising issues regarding its use in accurately representing socio-demographic groups and preventing stereotype propagation.", "key_contributions": ["Introduction of the Self-Stereo dataset for analyzing stereotypes.", "Comparison of persona-driven versus generic responses from multiple LLMs.", "Evidence highlighting limitations of persona-prompting in reducing stereotyping."], "limitations": "The study focuses only on short texts and may not generalize to other forms of LLM output.", "keywords": ["persona-prompting", "linguistic abstraction", "stereotyping", "LLMs", "Self-Stereo"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.08486", "pdf": "https://arxiv.org/pdf/2509.08486.pdf", "abs": "https://arxiv.org/abs/2509.08486", "title": "Too Helpful, Too Harmless, Too Honest or Just Right?", "authors": ["Gautam Siddharth Kashyap", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "EMNLP'25 Main", "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones.", "AI": {"tldr": "TrinityX is a modular alignment framework for large language models that addresses alignment challenges in helpfulness, harmfulness, and honesty by using a Mixture of Calibrated Experts (MoCaE).", "motivation": "Aligning outputs of large language models with the principles of Helpfulness, Harmlessness, and Honesty (HHH) is essential yet challenging due to existing methods optimizing dimensions in isolation and issues with Mixture-of-Experts architectures.", "method": "TrinityX incorporates a Mixture of Calibrated Experts (MoCaE) within a Transformer architecture, with experts trained separately for each HHH dimension and a calibrated, task-adaptive routing mechanism for output integration.", "result": "TrinityX outperforms baselines across alignment benchmarks (Alpaca, BeaverTails, TruthfulQA) with relative improvements of 32.5% in win rate, 33.9% in safety score, and 28.4% in truthfulness, while also reducing memory usage and inference latency by over 40%.", "conclusion": "The proposed TrinityX framework effectively enhances the alignment capabilities of large language models by employing calibrated routing and demonstrates generalization across diverse LLMs.", "key_contributions": ["Introduction of the TrinityX modular alignment framework", "Integration of separately trained experts for alignment dimensions", "Demonstrated significant performance improvements over existing methods"], "limitations": "", "keywords": ["Large Language Models", "Alignment", "Mixture of Experts", "Transformers", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.08541", "pdf": "https://arxiv.org/pdf/2509.08541.pdf", "abs": "https://arxiv.org/abs/2509.08541", "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data.", "AI": {"tldr": "The paper proposes a new method (CM-Align) to enhance multilingual alignment in large language models by addressing limitations in current preference data methodologies.", "motivation": "There is a significant performance gap in alignment between English and other languages in large language models, partly due to low-quality English responses and biased data construction methods.", "method": "The authors introduce a consistency-based data selection method that involves consistency-guided English reference selection and cross-lingual consistency-based multilingual preference data construction.", "result": "Experimental results on three large language models show that CM-Align improves the quality of multilingual preference data, leading to superior alignment performance compared to existing methods.", "conclusion": "Constructing high-quality preference data is essential for improving multilingual alignment in language models, and the proposed CM-Align method effectively addresses existing limitations.", "key_contributions": ["Development of CM-Align for high-quality multilingual preference data.", "Introduction of consistency-guided English reference selection.", "Establishment of cross-lingual consistency in preference construction."], "limitations": "", "keywords": ["multilingual alignment", "large language models", "preference optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.08596", "pdf": "https://arxiv.org/pdf/2509.08596.pdf", "abs": "https://arxiv.org/abs/2509.08596", "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge", "authors": ["Dima Galat", "Diego Molla-Aliod"], "categories": ["cs.CL"], "comment": "CEUR-WS, CLEF2025", "summary": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) for biomedical question answering (QA) through an ensemble of zero-shot models that achieve state-of-the-art results without fine-tuning.", "motivation": "Biomedical QA requires precise interpretation of specialized knowledge from a complex and evolving corpus.", "method": "An ensemble of different LLMs is evaluated for their performance on a Yes/No QA task, specifically using the BioASQ challenge tasks.", "result": "Ensemble methods outperform individual LLMs and sometimes rival domain-tuned systems, while preserving generalizability.", "conclusion": "Ensemble-based zero-shot approaches combined with effective retrieval-augmented generation (RAG) pipelines offer a practical alternative to costly fine-tuned systems.", "key_contributions": ["Demonstrated the effectiveness of ensemble models in biomedical QA", "Showed the impact of context length on performance", "Established the importance of information retrieval in RAG for QA systems"], "limitations": "The study focuses on zero-shot performance and does not explore fine-tuning methods or broader QA contexts.", "keywords": ["Biomedical QA", "Large Language Models", "Information Retrieval", "Ensemble Learning", "Retrieval-Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.08604", "pdf": "https://arxiv.org/pdf/2509.08604.pdf", "abs": "https://arxiv.org/abs/2509.08604", "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications", "authors": ["Anran Li", "Lingfei Qian", "Mengmeng Du", "Yu Yin", "Yan Hu", "Zihao Sun", "Yihang Fu", "Erica Stutz", "Xuguang Ai", "Qianqian Xie", "Rui Zhu", "Jimin Huang", "Yifan Yang", "Siru Liu", "Yih-Chung Tham", "Lucila Ohno-Machado", "Hyunghoon Cho", "Zhiyong Lu", "Hua Xu", "Qingyu Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.", "AI": {"tldr": "This study assesses the memorization of Large Language Models (LLMs) in medicine, evaluating its prevalence, characteristics, volume, and potential impacts on medical applications.", "motivation": "To understand the extent to which LLMs memorize medical training data and its implications for medical applications.", "method": "The study analyzes several adaptation scenarios of LLMs, including continued pretraining on medical corpora, fine-tuning on standard medical benchmarks, and fine-tuning on real-world clinical data containing over 13,000 unique inpatient records.", "result": "Memorization is prevalent across all adaptation scenarios and significantly higher than in the general domain. It can be categorized into beneficial, uninformative, and harmful forms.", "conclusion": "Practical recommendations are provided to promote beneficial memorization, minimize uninformative memorization, and mitigate harmful memorization to protect sensitive patient information.", "key_contributions": ["First comprehensive evaluation of memorization in LLMs within the medical domain", "Identification of three types of memorization: beneficial, uninformative, and harmful", "Practical recommendations for the deployment of LLMs in medical settings"], "limitations": "", "keywords": ["Large Language Models", "medical applications", "memorization", "health informatics", "clinical AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.08612", "pdf": "https://arxiv.org/pdf/2509.08612.pdf", "abs": "https://arxiv.org/abs/2509.08612", "title": "OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis", "authors": ["Xinfeng Liao", "Xuanqi Chen", "Lianxi Wang", "Jiahuan Yang", "Zhuowei Chen", "Ziying Rong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and\ndetermine their sentiment polarity. While dependency trees combined with\ncontextual semantics effectively identify aspect sentiment, existing methods\nrelying on syntax trees and aspect-aware attention struggle to model complex\nsemantic relationships. Their dependence on linear dot-product features fails\nto capture nonlinear associations, allowing noisy similarity from irrelevant\nwords to obscure key opinion terms. Motivated by Differentiable Optimal\nMatching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph\nNetwork (OTESGN), which introduces a Syntactic-Semantic Collaborative\nAttention. It comprises a Syntactic Graph-Aware Attention for mining latent\nsyntactic dependencies and modeling global syntactic topology, as well as a\nSemantic Optimal Transport Attention designed to uncover fine-grained semantic\nalignments amidst textual noise, thereby accurately capturing sentiment signals\nobscured by irrelevant tokens. A Adaptive Attention Fusion module integrates\nthese heterogeneous features, and contrastive regularization further improves\nrobustness. Experiments demonstrate that OTESGN achieves state-of-the-art\nresults, outperforming previous best models by +1.01% F1 on Twitter and +1.30%\nF1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its\nefficacy in precise localization of opinion words and noise resistance.", "AI": {"tldr": "This paper presents OTESGN, a novel model for aspect-based sentiment analysis that enhances the capture of sentiment signals through a combination of syntactic and semantic attention mechanisms.", "motivation": "Existing aspect-based sentiment analysis methods struggle with complex semantic relationships due to their reliance on linear features, resulting in the obscuring of key opinion terms by noise.", "method": "The Optimal Transport Enhanced Syntactic-Semantic Graph Network introduces Syntactic-Semantic Collaborative Attention, which integrates Syntactic Graph-Aware Attention and Semantic Optimal Transport Attention to better model syntactic dependencies and capture fine-grained semantic alignments.", "result": "OTESGN achieves state-of-the-art performance, improving F1 scores by +1.01% on Twitter and +1.30% on Laptop14 benchmarks compared to previous best models.", "conclusion": "The experimental results validate the effectiveness of OTESGN in accurately localizing opinion words and resisting noise, positioning it as a significant advancement in aspect-based sentiment analysis.", "key_contributions": ["Introduction of Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN)", "Development of Syntactic-Semantic Collaborative Attention model", "Demonstrated state-of-the-art performance on sentiment analysis benchmarks"], "limitations": "", "keywords": ["Aspect-based sentiment analysis", "syntactic-semantic attention", "optimal transport"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.08729", "pdf": "https://arxiv.org/pdf/2509.08729.pdf", "abs": "https://arxiv.org/abs/2509.08729", "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates", "authors": ["Hyunjun Kim", "Junwoo Ha", "Sangyoon Yu", "Haon Park"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.", "AI": {"tldr": "Introducing X-Teaming Evolutionary M2S, a framework automating the discovery and optimization of multi-turn-to-single-turn templates using LLM-based evolution, achieving significant structural gains in prompt design.", "motivation": "To automate and improve the template generation process for M2S in iterative red-teaming, addressing the reliance on manually crafted templates.", "method": "Utilizes an automated framework paired with language-model-guided evolution and smart sampling from multiple sources to optimize M2S templates.", "result": "Achieved a 44.8% overall success rate on GPT-4.1 with five evolutionary generations and the discovery of two new template families.", "conclusion": "The study highlights the effectiveness of structural search in enhancing single-turn probes and the critical role of threshold calibration and cross-model evaluation.", "key_contributions": ["Automated framework for template optimization in M2S", "Discovery of new template families and improvement in success rates", "Evidence of prompt length affecting performance outcomes"], "limitations": "", "keywords": ["multi-turn-to-single-turn", "evolutionary algorithms", "LLM-guided optimization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.08753", "pdf": "https://arxiv.org/pdf/2509.08753.pdf", "abs": "https://arxiv.org/abs/2509.08753", "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling", "authors": ["Neil Zeghidour", "Eugene Kharitonov", "Manu Orsini", "VÃ¡clav Volhejn", "Gabriel de Marmiesse", "Edouard Grave", "Patrick PÃ©rez", "Laurent MazarÃ©", "Alexandre DÃ©fossez"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling", "AI": {"tldr": "Delayed Streams Modeling (DSM) is a novel approach for flexible streaming, multimodal sequence-to-sequence learning that outperforms traditional methods in automatic speech recognition (ASR) and text-to-speech (TTS) tasks.", "motivation": "Existing methods for sequence-to-sequence generation either require complete input sequences before generating output, or rely on policies to determine when to advance input or output streams. DSM addresses these challenges by allowing for streaming inference of sequences with a flexible, time-aligned streaming model.", "method": "DSM utilizes a decoder-only language model to treat already aligned input and output streams, incorporating a preprocessing step for alignment and introducing delays between streams.", "result": "Experiments demonstrate that DSM achieves state-of-the-art performance in ASR and TTS tasks, providing low latency and the ability to handle arbitrary long sequences, competing effectively with traditional offline methods.", "conclusion": "The DSM framework represents a significant advancement in sequence-to-sequence models for streaming multimodal data, with practical implementations available for further exploration.", "key_contributions": ["Introduction of Delayed Streams Modeling for sequence-to-sequence tasks", "State-of-the-art performance in ASR and TTS applications", "Flexibility in handling arbitrary long input-output sequences"], "limitations": "", "keywords": ["Delayed Streams Modeling", "sequence-to-sequence", "streaming inference", "automatic speech recognition", "text-to-speech"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.08778", "pdf": "https://arxiv.org/pdf/2509.08778.pdf", "abs": "https://arxiv.org/abs/2509.08778", "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms", "authors": ["Minyeong Choe", "Haehyun Cho", "Changho Seo", "Hyunil Kim"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "Understanding how Transformer-based language models store and retrieve\nfactual associations is critical for improving interpretability and enabling\ntargeted model editing. Prior work, primarily on GPT-style models, has\nidentified MLP modules in early layers as key contributors to factual recall.\nHowever, it remains unclear whether these findings generalize across different\nautoregressive architectures. To address this, we conduct a comprehensive\nevaluation of factual recall across several models -- including GPT, LLaMA,\nQwen, and DeepSeek -- analyzing where and how factual information is encoded\nand accessed. Consequently, we find that Qwen-based models behave differently\nfrom previous patterns: attention modules in the earliest layers contribute\nmore to factual recall than MLP modules. Our findings suggest that even within\nthe autoregressive Transformer family, architectural variations can lead to\nfundamentally different mechanisms of factual recall.", "AI": {"tldr": "This paper evaluates how different Transformer-based language models, including GPT, LLaMA, Qwen, and DeepSeek, store and recall factual information, finding that architectural differences impact the mechanisms of recall.", "motivation": "To improve interpretability and enable targeted model editing of Transformer-based language models by understanding how they store and retrieve factual associations.", "method": "The paper conducts a comprehensive evaluation of factual recall across several autoregressive architectures, analyzing their early layer contributions especially focusing on MLP and attention modules.", "result": "The study reveals that Qwen-based models exhibit a distinct behavior where attention modules in the earliest layers are more crucial for factual recall than MLP modules, contrasting prior research findings.", "conclusion": "The results indicate that architectural variations within the autoregressive Transformer family significantly affect the mechanisms by which factual recall occurs.", "key_contributions": ["Identifies the importance of early attention layers in Qwen-based models for factual recall.", "Compares factual recall mechanisms across multiple autoregressive architectures.", "Challenges previously established understandings of MLP contributions in earlier Transformer models."], "limitations": "", "keywords": ["Transformer models", "factual recall", "language models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.08809", "pdf": "https://arxiv.org/pdf/2509.08809.pdf", "abs": "https://arxiv.org/abs/2509.08809", "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals", "authors": ["Cheng Chen", "Haiyan Yin", "Ivor Tsang"], "categories": ["cs.CL"], "comment": "11 pages, 10 figures", "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.", "AI": {"tldr": "The paper introduces a novel paradigm for evaluating Large Language Models (LLMs) using a student model that collaborates with a noisy teacher to assess annotation quality without oracle feedback, utilizing a new metric called the Consistent and Inconsistent (CAI) Ratio.", "motivation": "Evaluating LLM annotations is challenging in unsupervised environments with limited feedback, necessitating a new approach for assessing and refining annotation quality.", "method": "The proposed method involves a student model assessing the outputs of a noisy teacher (the LLM) using a user preference-based majority voting strategy, combined with the introduction of the CAI Ratio for measuring annotation quality.", "result": "The CAI Ratio displays a strong positive correlation with LLM accuracy across ten open-domain NLP datasets and four LLMs, validating its effectiveness as an evaluation metric.", "conclusion": "The CAI Ratio is established as a critical tool for unsupervised evaluation and model selection, facilitating the identification of robust LLMs under dynamic conditions.", "key_contributions": ["Introduction of an agentic annotation paradigm using a student model and noisy teacher for LLMs.", "Development of the Consistent and Inconsistent (CAI) Ratio as a novel unsupervised evaluation metric.", "Demonstration of strong correlation between CAI Ratio and LLM accuracy in NLP tasks."], "limitations": "", "keywords": ["Large Language Models", "annotation quality", "unsupervised evaluation", "CAI Ratio", "NLP"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2509.08812", "pdf": "https://arxiv.org/pdf/2509.08812.pdf", "abs": "https://arxiv.org/abs/2509.08812", "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages", "authors": ["Hailay Kidu Teklehaymanot", "Dren Fazlija", "Wolfgang Nejdl"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": "This submission is approximately 10 pages in length and includes 1\n  figure and 6 tables", "summary": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC", "AI": {"tldr": "MoVoC introduces a morpheme-aware tokenizer that enhances token efficiency and linguistic fidelity in low-resource languages written in the Geez script.", "motivation": "Subword-based tokenization methods inadequately preserve morphological boundaries in complex languages, which is particularly challenging for low-resource languages like those using the Geez script.", "method": "The paper presents MoVoC, a hybrid tokenizer that incorporates supervised morphological analysis and combines morpheme-based tokens with Byte Pair Encoding (BPE).", "result": "Although the new tokenization method does not significantly improve translation quality, it shows substantial improvements in intrinsic metrics such as MorphoScore and Boundary Precision.", "conclusion": "MoVoC and the accompanying morpheme-annotated datasets will be valuable resources for research in low-resource, morphologically rich languages, promoting further exploration in this area.", "key_contributions": ["Introduction of MoVoC, a morpheme-aware subword tokenizer.", "Release of manually annotated morpheme data for Geez script languages.", "Demonstrated improvements in linguistic fidelity and token efficiency."], "limitations": "The proposed method does not lead to significant gains in automatic translation quality.", "keywords": ["tokenization", "morphology", "Geez script", "natural language processing", "low-resource languages"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.08824", "pdf": "https://arxiv.org/pdf/2509.08824.pdf", "abs": "https://arxiv.org/abs/2509.08824", "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "Helio Pedrini"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development.", "AI": {"tldr": "This paper explores methods for building scalable web-based corpora for large language models (LLMs), specifically focusing on creating a competitive corpus in Portuguese and its impact on model performance.", "motivation": "There is a significant gap in understanding how to construct effective training corpora for languages other than English for large language models (LLMs).", "method": "The authors built a 120B token corpus in Portuguese and investigated the effects of various data selection and preprocessing strategies on LLM performance when adapting models from English to Portuguese.", "result": "The findings indicate that language-specific filtering pipelines improve model performance, highlighting the importance of high-quality data tailored to the target language.", "conclusion": "Adapting LLMs to specific languages using well-constructed datasets leads to significant performance improvements, and the methods developed can be applied to other languages as well.", "key_contributions": ["Developed scalable methods for constructing language-specific corpora for LLMs.", "Demonstrated the effectiveness of language-specific filtering pipelines.", "Provided insights applicable to multilingual LLM development."], "limitations": "", "keywords": ["large language models", "corpora", "language-specific data", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.08825", "pdf": "https://arxiv.org/pdf/2509.08825.pdf", "abs": "https://arxiv.org/abs/2509.08825", "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation", "authors": ["Joachim Baumann", "Paul RÃ¶ttger", "Aleksandra Urman", "Albert WendsjÃ¶", "Flor Miriam Plaza-del-Arco", "Johannes B. Gruber", "Dirk Hovy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.", "AI": {"tldr": "This paper investigates 'LLM hacking', the risks introduced by implementation choices when using large language models (LLMs) for tasks like data annotation in social science research, revealing significant biases and errors in statistical conclusions.", "motivation": "To address how variations in implementation choices when using LLMs can introduce biases in social science research, leading to incorrect conclusions and systematic errors.", "method": "The study replicated 37 data annotation tasks from 21 published social science studies using 18 different LLMs and analyzed 13 million labels to assess the impact of researcher choices on statistical conclusions.", "result": "Approximately one in three hypotheses yielded incorrect conclusions based on LLM-annotated data for state-of-the-art models, and half for smaller models, highlighting the significant risk of LLM hacking in research outcomes.", "conclusion": "While improving model performance reduces LLM hacking risk, no model completely eliminates it. The study stresses the necessity of human annotations to bolster findings' reliability and suggests that common error correction techniques may not adequately reduce risks.", "key_contributions": ["Quantifies the risk of LLM hacking in social science research", "Highlights the need for rigorous verification near significance thresholds", "Demonstrates that common regression correction techniques are ineffective in mitigating LLM hacking risks."], "limitations": "The study focuses specifically on social science research contexts; results may not generalize to other fields or different types of data.", "keywords": ["large language models", "LLM hacking", "data annotation", "social science research", "statistical conclusions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.08827", "pdf": "https://arxiv.org/pdf/2509.08827.pdf", "abs": "https://arxiv.org/abs/2509.08827", "title": "A Survey of Reinforcement Learning for Large Reasoning Models", "authors": ["Kaiyan Zhang", "Yuxin Zuo", "Bingxiang He", "Youbang Sun", "Runze Liu", "Che Jiang", "Yuchen Fan", "Kai Tian", "Guoli Jia", "Pengfei Li", "Yu Fu", "Xingtai Lv", "Yuchen Zhang", "Sihang Zeng", "Shang Qu", "Haozhan Li", "Shijie Wang", "Yuru Wang", "Xinwei Long", "Fangfu Liu", "Xiang Xu", "Jiaze Ma", "Xuekai Zhu", "Ermo Hua", "Yihao Liu", "Zonglin Li", "Huayu Chen", "Xiaoye Qu", "Yafu Li", "Weize Chen", "Zhenzhao Yuan", "Junqi Gao", "Dong Li", "Zhiyuan Ma", "Ganqu Cui", "Zhiyuan Liu", "Biqing Qi", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs", "AI": {"tldr": "This paper surveys recent advancements in applying Reinforcement Learning (RL) to Large Language Models (LLMs), identifying challenges and future directions for enhancing scalability and capabilities in reasoning tasks.", "motivation": "To explore the application of Reinforcement Learning (RL) for enhancing reasoning capabilities of Large Language Models (LLMs) and to identify challenges in scalability and infrastructure.", "method": "The paper presents a comprehensive review of the current state of RL as applied to LLMs, detailing foundational components, core problems, and existing research initiatives.", "result": "The review highlights both the successes of RL in enhancing LLM reasoning capabilities and the existing challenges in scaling RL methodologies for broader applications.", "conclusion": "The authors encourage future research in RL for reasoning models, aiming to facilitate advancements toward Artificial SuperIntelligence (ASI).", "key_contributions": ["Survey of state-of-the-art techniques in RL for LLMs", "Identification of foundational challenges in scaling RL for LLM applications", "Exploration of future research opportunities in RL and LLM interaction"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Artificial SuperIntelligence", "Reasoning", "Scalability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.13729", "pdf": "https://arxiv.org/pdf/2407.13729.pdf", "abs": "https://arxiv.org/abs/2407.13729", "title": "Baba Is AI: Break the Rules to Beat the Benchmark", "authors": ["Nathan Cloos", "Meagan Jens", "Michelangelo Naim", "Yen-Ling Kuo", "Ignacio Cases", "Andrei Barbu", "Christopher J. Cueva"], "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Humans solve problems by following existing rules and procedures, and also by\nleaps of creativity to redefine those rules and objectives. To probe these\nabilities, we developed a new benchmark based on the game Baba Is You where an\nagent manipulates both objects in the environment and rules, represented by\nmovable tiles with words written on them, to reach a specified goal and win the\ngame. We test three state-of-the-art multi-modal large language models (OpenAI\nGPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail\ndramatically when generalization requires that the rules of the game must be\nmanipulated and combined.", "AI": {"tldr": "This paper introduces a benchmark using the game Baba Is You to evaluate the flexibility of language models in rule manipulation for problem-solving.", "motivation": "The study aims to assess how language models can not only follow rules but also creatively manipulate them for goal achievement.", "method": "The researchers developed a benchmark based on the game Baba Is You, where agents manipulate environmental objects and rules to accomplish game goals. They tested three state-of-the-art multi-modal language models.", "result": "The tested language models (GPT-4o, Gemini-1.5-Pro, and Gemini-1.5-Flash) exhibited significant failure in tasks requiring rule manipulation and combination for generalization.", "conclusion": "The findings highlight the limitations of current multi-modal language models in creatively processing and altering rules in tasks, suggesting a need for improvement in flexibility and generalization.", "key_contributions": ["Development of a new benchmark using Baba Is You", "Evaluation of state-of-the-art multi-modal language models", "Insights into the limitations of current AI models in rule manipulation"], "limitations": "The study is limited to three language models and the specific gameplay context of Baba Is You.", "keywords": ["language models", "rule manipulation", "problem-solving", "Baba Is You", "AI evaluation"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2501.12051", "pdf": "https://arxiv.org/pdf/2501.12051.pdf", "abs": "https://arxiv.org/abs/2501.12051", "title": "MedS$^3$: Towards Medical Slow Thinking with Self-Evolved Soft Dual-sided Process Supervision", "authors": ["Shuyang Jiang", "Yusheng Liao", "Zhe Chen", "Ya Zhang", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": "20 pages;", "summary": "Medical language models face critical barriers to real-world clinical\nreasoning applications. However, mainstream efforts, which fall short in task\ncoverage, lack fine-grained supervision for intermediate reasoning steps, and\nrely on proprietary systems, are still far from a versatile, credible and\nefficient language model for clinical reasoning usage. To this end, we propose\n\\mone, a self-evolving framework that imparts robust reasoning capabilities to\nsmall, deployable models. Starting with 8,000 curated instances sampled via a\ncurriculum strategy across five medical domains and 16 datasets, we use a small\nbase policy model to conduct Monte Carlo Tree Search (MCTS) for constructing\nrule-verifiable reasoning trajectories. Self-explored reasoning trajectories\nranked by node values are used to bootstrap the policy model via reinforcement\nfine-tuning and preference learning. Moreover, we introduce a soft dual process\nreward model that incorporates value dynamics: steps that degrade node value\nare penalized, enabling fine-grained identification of reasoning errors even\nwhen the final answer is correct. Experiments on eleven benchmarks show that\n\\mone outperforms the previous state-of-the-art medical model by +6.45 accuracy\npoints and surpasses 32B-scale general-purpose reasoning models by +8.57\npoints. Additional empirical analysis further demonstrates that \\mone achieves\nrobust and faithful reasoning behavior.", "AI": {"tldr": "This paper presents a self-evolving framework named \\mone, designed to enhance reasoning capabilities in small medical language models for clinical applications.", "motivation": "Mainstream medical language models have limited task coverage and lack fine-grained supervision for intermediate reasoning steps, hindering their effectiveness in clinical reasoning.", "method": "The authors propose \\mone, which utilizes a small base policy model to conduct Monte Carlo Tree Search (MCTS) for creating rule-verifiable reasoning pathways, supplemented with reinforcement fine-tuning and a dual process reward model.", "result": "Experimental results indicate \\mone achieves +6.45 accuracy points over the previous state-of-the-art medical model and +8.57 points over general-purpose reasoning models on eleven benchmarks.", "conclusion": "The proposed approach demonstrates improved accuracy and robust reasoning performance for medical language models, addressing critical barriers in clinical applications.", "key_contributions": ["Introduction of a self-evolving framework for medical language models.", "Implementation of Monte Carlo Tree Search for reasoning trajectory construction.", "Development of a soft dual process reward model for fine-grained reasoning error identification."], "limitations": "", "keywords": ["medical language models", "clinical reasoning", "Monte Carlo Tree Search"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.02390", "pdf": "https://arxiv.org/pdf/2502.02390.pdf", "abs": "https://arxiv.org/abs/2502.02390", "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning", "authors": ["Jianfeng Pan", "Senyou Deng", "Shaomang Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 10 figures", "summary": "Research on LLM technologies is rapidly emerging, with most of them employ a\n'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. We validate CoAT's\neffectiveness across a variety of generative and reasoning tasks. Quantitative\nexperiments show that CoAT achieves over 10% performance improvement on\nopen-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15%\ngain on our proprietary CRB dataset.", "AI": {"tldr": "This paper introduces the Chain-of-Associated-Thoughts (CoAT) framework, which combines Monte Carlo Tree Search and associative memory to improve reasoning in LLMs by enabling dynamic knowledge integration and exploration of reasoning pathways.", "motivation": "The research addresses the limitations of traditional LLMs that rely solely on single-query inference, advocating for a more human-like reasoning process through 'slow thinking' techniques.", "method": "The CoAT framework synergizes Monte Carlo Tree Search (MCTS) with a dynamic associative memory mechanism that allows continual knowledge updating and exploration of diverse reasoning pathways.", "result": "CoAT demonstrates significant performance improvements, achieving over 10% enhancement on multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15% on a proprietary CRB dataset.", "conclusion": "The findings validate that CoAT effectively enhances LLM capabilities by allowing adaptive learning and real-time knowledge integration, leading to more accurate and comprehensive outputs.", "key_contributions": ["Introduction of the CoAT framework for LLMs.", "Combination of MCTS with associative memory for improved reasoning.", "Validation of framework effectiveness on various datasets."], "limitations": "", "keywords": ["Large Language Models", "Monte Carlo Tree Search", "Associative Memory"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2502.12737", "pdf": "https://arxiv.org/pdf/2502.12737.pdf", "abs": "https://arxiv.org/abs/2502.12737", "title": "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation", "authors": ["Shengxiang Gao", "Jey Han Lau", "Jianzhong Qi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Knowledge base question answering (KBQA) aims to answer user questions in\nnatural language using rich human knowledge stored in large KBs. As current\nKBQA methods struggle with unseen knowledge base elements at test time,we\nintroduce SG-KBQA: a novel model that injects schema contexts into entity\nretrieval and logical form generation to tackle this issue. It uses the richer\nsemantics and awareness of the knowledge base structure provided by schema\ncontexts to enhance generalizability. We show that SG-KBQA achieves strong\ngeneralizability, outperforming state-of-the-art models on two commonly used\nbenchmark datasets across a variety of test settings. Our source code is\navailable at https://github.com/gaosx2000/SG_KBQA.", "AI": {"tldr": "SG-KBQA is a novel model for knowledge base question answering that improves generalizability by integrating schema contexts into entity retrieval and logical form generation.", "motivation": "Current knowledge base question answering methods struggle with unseen elements at test time, highlighting the need for improved generalizability.", "method": "SG-KBQA injects schema contexts into the processes of entity retrieval and logical form generation to enhance performance.", "result": "SG-KBQA outperforms state-of-the-art models on two benchmark datasets under various test conditions, demonstrating strong generalizability.", "conclusion": "The integration of schema contexts into the KBQA model leads to significant improvements in answering capabilities, particularly for unseen knowledge base elements.", "key_contributions": ["Introduction of SG-KBQA model", "Incorporation of schema contexts for enhanced generalizability", "Demonstrated superior performance on benchmark datasets"], "limitations": "", "keywords": ["Knowledge Base Question Answering", "Schema Contexts", "Generalizability"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2502.16523", "pdf": "https://arxiv.org/pdf/2502.16523.pdf", "abs": "https://arxiv.org/abs/2502.16523", "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data.", "AI": {"tldr": "This paper investigates the robustness of Machine Reading Comprehension models against naturally occurring textual perturbations using Wikipedia edit history, revealing performance degradation in state-of-the-art models and proposing methods to improve robustness.", "motivation": "With the rise of neural language models achieving human-like performance in MRC, there is a need to assess their robustness in real-world scenarios beyond synthetic perturbations.", "method": "The authors propose a framework that examines MRC models by replacing paragraphs in benchmarks with corresponding text from Wikipedia edit history, assessing performance degradation across various model architectures.", "result": "Natural perturbations lead to significant performance degradation in pre-trained language models, including Flan-T5 and other LLMs, indicating that they persist despite state-of-the-art capabilities.", "conclusion": "While training on perturbed examples can enhance robustness against natural perturbations, a performance gap compared to unperturbed data remains, emphasizing the need for continuous evaluation.", "key_contributions": ["Development of a framework to assess MRC robustness with natural perturbations.", "Findings from large-scale studies showing significant degradation in performance in response to natural text variations.", "Proposed training methods that improve robustness, albeit with a performance gap."], "limitations": "The study primarily focuses on certain MRC benchmarks and may not cover all possible natural perturbations across diverse types.", "keywords": ["Machine Reading Comprehension", "robustness", "natural perturbations", "Wikipedia edit history", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.16838", "pdf": "https://arxiv.org/pdf/2502.16838.pdf", "abs": "https://arxiv.org/abs/2502.16838", "title": "REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction", "authors": ["Omar Sharif", "Joseph Gatto", "Madhusudan Basak", "Sarah M. Preum"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP-2025", "summary": "Event argument extraction identifies arguments for predefined event roles in\ntext. Existing work evaluates this task with exact match (EM), where predicted\narguments must align exactly with annotated spans. While suitable for\nspan-based models, this approach falls short for large language models (LLMs),\nwhich often generate diverse yet semantically accurate arguments. EM severely\nunderestimates performance by disregarding valid variations. Furthermore, EM\nevaluation fails to capture implicit arguments (unstated but inferable) and\nscattered arguments (distributed across a document). These limitations\nunderscore the need for an evaluation framework that better captures models'\nactual performance. To bridge this gap, we introduce REGen, a Reliable\nEvaluation framework for Generative event argument extraction. REGen combines\nthe strengths of exact, relaxed, and LLM-based matching to better align with\nhuman judgment. Experiments on six datasets show that REGen reveals an average\nperformance gain of +23.93 F1 over EM, reflecting capabilities overlooked by\nprior evaluation. Human validation further confirms REGen's effectiveness,\nachieving 87.67% alignment with human assessments of argument correctness.", "AI": {"tldr": "This paper presents REGen, a new evaluation framework for generative event argument extraction that better aligns with human judgment compared to exact match evaluation.", "motivation": "Existing evaluation methods for event argument extraction undervalue the performance of large language models and fail to account for valid variations, implicit arguments, and scattered arguments.", "method": "REGen combines exact, relaxed, and LLM-based matching techniques to create a comprehensive evaluation framework.", "result": "Experiments show REGen achieves an average performance gain of +23.93 F1 over traditional exact match evaluation and aligns 87.67% with human assessments.", "conclusion": "REGen significantly improves the evaluation of generative models in event argument extraction by capturing a broader range of correct answers.", "key_contributions": ["Introduction of the REGen framework for event argument extraction evaluation", "Demonstrated substantial performance improvement over existing exact match evaluation methods", "Validation of the framework through human assessments confirming its effectiveness."], "limitations": "", "keywords": ["event argument extraction", "evaluation framework", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.02682", "pdf": "https://arxiv.org/pdf/2503.02682.pdf", "abs": "https://arxiv.org/abs/2503.02682", "title": "MPO: Boosting LLM Agents with Meta Plan Optimization", "authors": ["Weimin Xiong", "Yifan Song", "Qingxiu Dong", "Bingchan Zhao", "Feifan Song", "Xun Wang", "Sujian Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Findings", "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, , which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.", "AI": {"tldr": "Introduction of the Meta Plan Optimization (MPO) framework for improving LLM agent planning.", "motivation": "To resolve issues of planning hallucinations and the need for retraining in existing LLM agent approaches.", "method": "MPO incorporates explicit guidance through meta plans that assist agent planning and allows for continuous optimization based on task execution feedback.", "result": "MPO significantly outperforms existing baselines in task completion efficiency and enhances generalization in unseen scenarios.", "conclusion": "MPO presents a plug-and-play solution to improve LLM-based agents without requiring complex knowledge or significant human effort.", "key_contributions": ["Proposes the Meta Plan Optimization framework for LLM agents", "Demonstrates significant improvements over existing methods in planning tasks", "Enables continuous optimization of meta plans based on feedback"], "limitations": "", "keywords": ["large language models", "planning", "meta plans", "agent optimization", "task execution"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.07104", "pdf": "https://arxiv.org/pdf/2506.07104.pdf", "abs": "https://arxiv.org/abs/2506.07104", "title": "How Far Are We from Optimal Reasoning Efficiency?", "authors": ["Jiaxuan Gao", "Shu Yan", "Qixin Tan", "Lu Yang", "Shusheng Xu", "Wei Fu", "Zhiyu Mei", "Kaifeng Lyu", "Yi Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.", "AI": {"tldr": "The paper introduces the Reasoning Efficiency Gap (REG) as a metric for evaluating fine-tuned Large Reasoning Models (LRMs) and presents REO-RL, a new Reinforcement Learning algorithm to reduce inefficiencies in these models.", "motivation": "To address the excessive verbosity and redundancy in reasoning traces produced by LRMs, which lead to high inference costs and limit their practical deployment.", "method": "The authors propose the Reasoning Efficiency Gap (REG) to quantify the deviation of fine-tuned LRMs from efficiency frontiers. They also introduce a class of Reinforcement Learning algorithms called REO-RL that targets sparse token budgets to minimize REG.", "result": "Systematic evaluation shows that fine-tuning methods currently exhibit significant efficiency gaps, either sacrificing accuracy for shorter reasoning or remaining inefficient. REO-RL reduces REG by >=50 while retaining accuracy under a 16K token budget.", "conclusion": "Aligning fine-tuned LRMs with efficiency frontiers is challenging, highlighting the need for improved methodologies in fine-tuning and assessment.", "key_contributions": ["Introduction of the Reasoning Efficiency Gap (REG) for evaluating LRM efficiency.", "Development of REO-RL, a Reinforcement Learning algorithm that minimizes REG effectively.", "Empirical benchmarking demonstrating the effectiveness of the proposed metric and algorithm."], "limitations": "The challenge of perfectly aligning LRMs with efficiency frontiers remains unsolved, suggesting further research is necessary.", "keywords": ["Large Reasoning Models", "Efficiency", "Reinforcement Learning", "Reasoning Efficiency Gap", "Benchmarking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.15463", "pdf": "https://arxiv.org/pdf/2501.15463.pdf", "abs": "https://arxiv.org/abs/2501.15463", "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "authors": ["Hua Shen", "Nicholas Clark", "Tanushree Mitra"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Paper", "summary": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps.", "AI": {"tldr": "This paper introduces ValueActionLens, a framework for evaluating the alignment between the stated values and actions of LLMs, revealing significant misalignments and the risks they pose.", "motivation": "To explore the discrepancy between the stated values of large language models (LLMs) and their actual behaviors, analogous to the 'Value-Action Gap' in psychology.", "method": "The study develops the ValueActionLens framework, generating a dataset of 14.8k value-informed actions across various cultures and topics, and conducts experiments to measure alignment across different scenarios and models.", "result": "Experiments show that LLMs display a sub-optimal alignment between stated values and actions, with significant variations across different contexts and models, indicating potential harms from these gaps.", "conclusion": "The study emphasizes that relying on LLMs' stated values for behavioral predictions can be misleading and highlights the importance of context-aware evaluations.", "key_contributions": ["Introduction of the ValueActionLens framework for LLM evaluation", "Creation of a dataset for value-informed actions", "Demonstration of the risks associated with value-action gaps in LLMs"], "limitations": "The study focuses on a specific set of cultures and social topics, which may limit generalizability.", "keywords": ["LLM", "Value-Action Gap", "evaluation framework", "value-informed actions", "context-aware assessments"], "importance_score": 9, "read_time_minutes": 12}}
