{"id": "2505.00153", "pdf": "https://arxiv.org/pdf/2505.00153.pdf", "abs": "https://arxiv.org/abs/2505.00153", "title": "Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals", "authors": ["Bhanuja Ainary"], "categories": ["cs.HC", "cs.DC"], "comment": "This thesis was conducted under the guidance of Mohsen Amini Salehi.\n  Special thanks to Minseo Kim and Jacob Bradshaw for their valuable\n  contributions and support throughout the research process. 60 pages, 13\n  Figures, 2 Tables", "summary": "Visually impaired people face significant challenges when attempting to\ninteract with and understand complex environments, and traditional assistive\ntechnologies often struggle to quickly provide necessary contextual\nunderstanding and interactive intelligence. This thesis presents Audo-Sight, a\nstate-of-the-art assistive system that seamlessly integrates Multimodal Large\nLanguage Models (MLLMs) to provide expedient, context-aware interactions for\nBlind and Visually Impaired (BVI) individuals. The system operates in two\ndifferent modalities: personalized interaction through user identification and\npublic access in common spaces like museums and shopping malls. In tailored\nenvironments, the system adjusts its output to conform to the preferences of\nindividual users, thus enhancing accessibility through a user-aware form of\ninteraction. In shared environments, Audo-Sight employs a shared architecture\nthat adapts to its current user with no manual reconfiguration required. To\nfacilitate appropriate interactions with the LLM, the public Audo-Sight\nsolution includes an Age-Range Determiner and Safe Query Filter. Additionally,\nthe system ensures that responses are respectful to BVI users through NeMo\nGuardrails. By utilizing multimodal reasoning, BVI-cognizant response editing,\nand safeguarding features, this work represents a major leap in AI-driven\naccessibility technology capable of increasing autonomy, safety, and\ninteraction for people with visual impairments in social settings. Finally, we\npresent the integration of Audo-Sight and SmartSight, which enables enhanced\nsituational awareness for BVI individuals. This integration takes advantage of\nthe real-time visual analysis of SmartSight, combined with the extensive\nreasoning and interactive capabilities of Audo-Sight, and goes beyond object\nidentification to provide context-driven, voice-controlled assistance in\ndynamic environments.", "AI": {"tldr": "Audo-Sight is an assistive system that utilizes Multimodal Large Language Models to improve interactions for Blind and Visually Impaired individuals by providing contextual awareness in various environments.", "motivation": "Visually impaired people face challenges interacting with complex environments, and existing assistive technologies often lack the necessary contextual understanding.", "method": "The system operates in personalized and public modalities, adapting interactions based on user identification and providing context-aware assistance in public spaces.", "result": "Audo-Sight enhances accessibility and autonomy for BVI individuals, employing features like an Age-Range Determiner and Safe Query Filter, along with integrating capabilities from SmartSight for situational awareness.", "conclusion": "The integration represents a significant advancement in AI-driven accessibility technology for visually impaired individuals, enabling improved interaction and safety in social settings.", "key_contributions": ["Integration of MLLMs for personalized interactions", "Real-time visual analysis combined with voice-controlled assistance", "Development of user-aware and safety features for BVI users"], "limitations": "", "keywords": ["Blind and Visually Impaired", "Multimodal Large Language Models", "AI accessibility"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.00455", "pdf": "https://arxiv.org/pdf/2505.00455.pdf", "abs": "https://arxiv.org/abs/2505.00455", "title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "authors": ["Sungbok Shin", "Hyeon Jeon", "Sanghyun Hong", "Niklas Elmqvist"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE VIS2025", "summary": "Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design.", "AI": {"tldr": "The paper presents the Data Therapist, a tool that aids domain experts in externalizing tacit knowledge for effective data visualization through iterative Q&A and annotation, supported by a large language model.", "motivation": "Effective data visualization relies on domain-specific context and tacit knowledge about data provenance and quality, which is often implicit.", "method": "The Data Therapist tool employs a mixed-initiative approach combining a large language model for analyzing datasets, prompting targeted questions, and allowing interactive annotation.", "result": "Qualitative evaluation with experts revealed patterns in reasoning about data and identified how AI can enhance visualization design.", "conclusion": "The structured knowledge generated can inform both human and automated visualization design processes.", "key_contributions": ["Introduction of the Data Therapist tool for externalizing tacit knowledge", "Combination of Q&A and annotation to improve data visualization", "Identified expert reasoning patterns that inform the use of AI in visualization design"], "limitations": "", "keywords": ["data visualization", "human-computer interaction", "large language model"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.00001", "pdf": "https://arxiv.org/pdf/2505.00001.pdf", "abs": "https://arxiv.org/abs/2505.00001", "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning", "authors": ["Shaun Baek", "Shaun Esua-Mensah", "Cyrus Tsui", "Sejan Vigneswaralingam", "Abdullah Alali", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are primarily trained on high-resource natural\nlanguages, limiting their effectiveness in low-resource settings and in tasks\nrequiring deep logical reasoning. This research introduces Rosetta-PL, a\nbenchmark designed to evaluate LLMs' logical reasoning and generalization\ncapabilities in a controlled environment. We construct Rosetta-PL by\ntranslating a dataset of logical propositions from Lean into a custom logical\nlanguage, which is then used to fine-tune an LLM (e.g., GPT-4o). Our\nexperiments analyze the impact of the size of the dataset and the translation\nmethodology on the performance of the model. Our results indicate that\npreserving logical relationships in the translation process significantly\nboosts precision, with accuracy plateauing beyond roughly 20,000 training\nsamples. These insights provide valuable guidelines for optimizing LLM training\nin formal reasoning tasks and improving performance in various low-resource\nlanguage applications.", "AI": {"tldr": "Introduction of a benchmark, Rosetta-PL, for evaluating LLMs' logical reasoning in low-resource settings.", "motivation": "To enhance the effectiveness of LLMs in low-resource languages and improve their logical reasoning capabilities.", "method": "Rosetta-PL benchmark is constructed by translating a dataset of logical propositions into a custom logical language, which is used for fine-tuning an LLM.", "result": "Maintaining logical relationships in translations significantly increases precision; accuracy improves until around 20,000 training samples.", "conclusion": "Optimizing LLM training in formal reasoning can enhance performance in low-resource language applications.", "key_contributions": ["Introduction of the Rosetta-PL benchmark.", "Insights on dataset size impact on LLM performance.", "Guidelines for optimizing LLM training in formal reasoning."], "limitations": "Focused on specific logical propositions and may not generalize to all reasoning tasks.", "keywords": ["Large Language Models", "logical reasoning", "low-resource languages", "benchmark", "Rosetta-PL"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.00002", "pdf": "https://arxiv.org/pdf/2505.00002.pdf", "abs": "https://arxiv.org/abs/2505.00002", "title": "Symbol grounding in computational systems: A paradox of intentions", "authors": ["Vincent C. Müller"], "categories": ["cs.CL"], "comment": null, "summary": "The paper presents a paradoxical feature of computational systems that\nsuggests that computationalism cannot explain symbol grounding. If the mind is\na digital computer, as computationalism claims, then it can be computing either\nover meaningful symbols or over meaningless symbols. If it is computing over\nmeaningful symbols its functioning presupposes the existence of meaningful\nsymbols in the system, i.e. it implies semantic nativism. If the mind is\ncomputing over meaningless symbols, no intentional cognitive processes are\navailable prior to symbol grounding. In this case, no symbol grounding could\ntake place since any grounding presupposes intentional cognitive processes. So,\nwhether computing in the mind is over meaningless or over meaningful symbols,\ncomputationalism implies semantic nativism.", "AI": {"tldr": "The paper argues that computationalism fails to adequately explain symbol grounding, leading to a paradox regarding meaningful and meaningless symbols in the mind.", "motivation": "To explore the implications of computationalism on symbol grounding and semantic nativism.", "method": "Theoretical analysis of the relationship between computationalism, meaningful symbols, and intentional cognitive processes.", "result": "It concludes that computationalism inherently supports semantic nativism by suggesting that the mind must compute over meaningful symbols to have intentional processes.", "conclusion": "Therefore, regardless of whether the mind operates on meaningful or meaningless symbols, computationalism limits the understanding of symbol grounding.", "key_contributions": ["Presents a paradox regarding computationalism and symbol grounding.", "Explores the implications of intentional cognitive processes for computationalism.", "Highlights the contradiction between meaningful and meaningless symbols in computational theories."], "limitations": "", "keywords": ["computationalism", "symbol grounding", "semantic nativism", "intentional cognitive processes", "meaningful symbols"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2505.00003", "pdf": "https://arxiv.org/pdf/2505.00003.pdf", "abs": "https://arxiv.org/abs/2505.00003", "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs", "authors": ["Zizhou Liu", "Ziwei Gong", "Lin Ai", "Zheng Hui", "Run Chen", "Colin Wayne Leach", "Michelle R. Greene", "Julia Hirschberg"], "categories": ["cs.CL"], "comment": null, "summary": "Psychological insights have long shaped pivotal NLP breakthroughs, including\nthe cognitive underpinnings of attention mechanisms, formative reinforcement\nlearning, and Theory of Mind-inspired social modeling. As Large Language Models\n(LLMs) continue to grow in scale and complexity, there is a rising consensus\nthat psychology is essential for capturing human-like cognition, behavior, and\ninteraction. This paper reviews how psychological theories can inform and\nenhance stages of LLM development, including data, pre-training, post-training,\nand evaluation\\&application. Our survey integrates insights from cognitive,\ndevelopmental, behavioral, social, personality psychology, and\npsycholinguistics. Our analysis highlights current trends and gaps in how\npsychological theories are applied. By examining both cross-domain connections\nand points of tension, we aim to bridge disciplinary divides and promote more\nthoughtful integration of psychology into future NLP research.", "AI": {"tldr": "The paper reviews the integration of psychological theories into Large Language Model (LLM) development to enhance human-like cognition and behavior in NLP.", "motivation": "To highlight the importance of psychology in the development and evaluation of LLMs as they grow in complexity.", "method": "A survey of various psychological theories including cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics, examining their application in LLM stages such as data collection, training, and evaluation.", "result": "Identified current trends and gaps in the application of psychological theories in LLM research, emphasizing both connections and points of tension across disciplines.", "conclusion": "Integrating psychology into NLP research can promote better understanding and development of human-like behaviors in LLMs, bridging gaps between disciplines.", "key_contributions": ["Comprehensive review of psychological theories relevant to LLMs", "Identification of gaps in current LLM research", "Cross-disciplinary analysis of psychology's role in NLP"], "limitations": "", "keywords": ["Large Language Models", "psychology", "NLP", "human-like cognition", "behavior"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00004", "pdf": "https://arxiv.org/pdf/2505.00004.pdf", "abs": "https://arxiv.org/abs/2505.00004", "title": "LangVAE and LangSpace: Building and Probing for Language Model VAEs", "authors": ["Danilo S. Carvalho", "Yingji Zhang", "Harriet Unsworth", "André Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present LangVAE, a novel framework for modular construction of variational\nautoencoders (VAEs) on top of pre-trained large language models (LLMs). Such\nlanguage model VAEs can encode the knowledge of their pre-trained components\ninto more compact and semantically disentangled representations. The\nrepresentations obtained in this way can be analysed with the LangVAE companion\nframework: LangSpace, which implements a collection of probing methods, such as\nvector traversal and interpolation, disentanglement measures, and cluster\nvisualisations. LangVAE and LangSpace offer a flexible, efficient and scalable\nway of building and analysing textual representations, with simple integration\nfor models available on the HuggingFace Hub. Additionally, we conducted a set\nof experiments with different encoder and decoder combinations, as well as\nannotated inputs, revealing a wide range of interactions across architectural\nfamilies and sizes w.r.t. generalisation and disentanglement. Our findings\ndemonstrate a promising framework for systematising the experimentation and\nunderstanding of textual representations.", "AI": {"tldr": "LangVAE is a framework that enhances the modular construction of variational autoencoders using pre-trained large language models, facilitating better representation analysis.", "motivation": "The paper addresses the need for a more effective way to build and analyze textual representations with VAEs that leverage the knowledge of pre-trained language models.", "method": "The proposed framework, LangVAE, constructs VAEs on pre-trained LLMs and is analyzed using the LangSpace framework, which includes various probing methods.", "result": "Experiments revealed diverse interactions among encoder-decoder combinations regarding their generalization and disentanglement capabilities.", "conclusion": "LangVAE and LangSpace provide a scalable method for examining textual representations, proving useful through their flexible integration and experimental findings.", "key_contributions": ["Introduction of LangVAE for constructing VAEs using pre-trained LLMs", "Development of the LangSpace framework for representation analysis", "Empirical findings on the interactions of architectural families regarding VAEs and generalization"], "limitations": "", "keywords": ["variational autoencoders", "large language models", "representation analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2206.09535", "pdf": "https://arxiv.org/pdf/2206.09535.pdf", "abs": "https://arxiv.org/abs/2206.09535", "title": "Characterizing Human Actions in the Digital Platform by Temporal Context", "authors": ["Akira Matsui", "Emilio Ferrara"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advances in digital platforms generate rich, high-dimensional logs of\nhuman behavior, and machine learning models have helped social scientists\nexplain knowledge accumulation, communication, and information diffusion. Such\nmodels, however, almost always treat behavior as sequences of actions,\nabstracting the inter-temporal information among actions. To close this gap, we\nintroduce a two-scale Action-Timing Context(ATC) framework that jointly embeds\neach action and its time interval. ATC obtains low-dimensional representations\nof actions and characterizes them with inter-temporal information. We provide\nthree applications of ATC to real-world datasets and demonstrate that the\nmethod offers a unified view of human behavior. The presented qualitative\nfindings demonstrate that explicitly modeling inter-temporal context is\nessential for a comprehensive, interpretable understanding of human activity on\ndigital platforms.", "AI": {"tldr": "Introduction of the Action-Timing Context (ATC) framework to model human behavior on digital platforms by embedding actions with their time intervals.", "motivation": "To address the gap in existing machine learning models that treat behavior as simple sequences of actions without considering temporal information.", "method": "The Action-Timing Context (ATC) framework jointly embeds actions and their time intervals to produce low-dimensional representations that capture inter-temporal information.", "result": "Application of the ATC framework to real-world datasets demonstrates a unified view of human behavior, yielding qualitative findings that highlight the importance of inter-temporal context.", "conclusion": "Explicitly modeling inter-temporal context is crucial for an interpretable understanding of human activity on digital platforms.", "key_contributions": ["Introduction of the Action-Timing Context framework", "Demonstration of the importance of inter-temporal context in modeling behavior", "Application of ATC to real-world datasets to illustrate its effectiveness."], "limitations": "", "keywords": ["Human Behavior", "Machine Learning", "Temporal Context"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.00006", "pdf": "https://arxiv.org/pdf/2505.00006.pdf", "abs": "https://arxiv.org/abs/2505.00006", "title": "Toward a digital twin of U.S. Congress", "authors": ["Hayden Helm", "Tianyi Chen", "Harvey McGuinness", "Paige Lee", "Brandon Duderstadt", "Carey E. Priebe"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "In this paper we provide evidence that a virtual model of U.S.\ncongresspersons based on a collection of language models satisfies the\ndefinition of a digital twin. In particular, we introduce and provide\nhigh-level descriptions of a daily-updated dataset that contains every Tweet\nfrom every U.S. congressperson during their respective terms. We demonstrate\nthat a modern language model equipped with congressperson-specific subsets of\nthis data are capable of producing Tweets that are largely indistinguishable\nfrom actual Tweets posted by their physical counterparts. We illustrate how\ngenerated Tweets can be used to predict roll-call vote behaviors and to\nquantify the likelihood of congresspersons crossing party lines, thereby\nassisting stakeholders in allocating resources and potentially impacting\nreal-world legislative dynamics. We conclude with a discussion of the\nlimitations and important extensions of our analysis.", "AI": {"tldr": "This paper presents a digital twin model of U.S. congresspersons utilizing language models to generate Tweets that mimic actual congressperson content, predicting their voting behavior.", "motivation": "To explore the utility of language models in analyzing and predicting the behavior of U.S. congresspersons through their social media activity.", "method": "A dataset of Tweets from U.S. congresspersons was created and updated daily, and language models were trained on congressperson-specific subsets of this data to simulate their Tweeting behaviors.", "result": "The generated Tweets closely resemble actual Tweets, and the model can predict voting behavior and party affiliations effectively.", "conclusion": "The study highlights the effectiveness of using generated content for predictive analytics in legislative contexts but also notes limitations in the current approach.", "key_contributions": ["Introduction of a digital twin model of congresspersons using language models", "Creation of a comprehensive dataset of congressperson Tweets updated daily", "Demonstration of predictive capabilities regarding voting behavior and party line crossing"], "limitations": "Potential biases in language models and limitations in prediction accuracy.", "keywords": ["digital twin", "language models", "US congress", "social media", "predictive analytics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.00008", "pdf": "https://arxiv.org/pdf/2505.00008.pdf", "abs": "https://arxiv.org/abs/2505.00008", "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination", "authors": ["Zhaoyi Sun", "Wen-Wai Yim", "Ozlem Uzuner", "Fei Xia", "Meliha Yetisgen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: This review aims to explore the potential and challenges of using\nNatural Language Processing (NLP) to detect, correct, and mitigate medically\ninaccurate information, including errors, misinformation, and hallucination. By\nunifying these concepts, the review emphasizes their shared methodological\nfoundations and their distinct implications for healthcare. Our goal is to\nadvance patient safety, improve public health communication, and support the\ndevelopment of more reliable and transparent NLP applications in healthcare.\n  Methods: A scoping review was conducted following PRISMA guidelines,\nanalyzing studies from 2020 to 2024 across five databases. Studies were\nselected based on their use of NLP to address medically inaccurate information\nand were categorized by topic, tasks, document types, datasets, models, and\nevaluation metrics.\n  Results: NLP has shown potential in addressing medically inaccurate\ninformation on the following tasks: (1) error detection (2) error correction\n(3) misinformation detection (4) misinformation correction (5) hallucination\ndetection (6) hallucination mitigation. However, challenges remain with data\nprivacy, context dependency, and evaluation standards.\n  Conclusion: This review highlights the advancements in applying NLP to tackle\nmedically inaccurate information while underscoring the need to address\npersistent challenges. Future efforts should focus on developing real-world\ndatasets, refining contextual methods, and improving hallucination management\nto ensure reliable and transparent healthcare applications.", "AI": {"tldr": "A review of NLP methods for addressing medically inaccurate information, outlining both potential benefits and existing challenges.", "motivation": "To advance patient safety and improve public health communication by utilizing NLP to detect and mitigate inaccurate medical information.", "method": "A scoping review was conducted following PRISMA guidelines, analyzing relevant studies from 2020 to 2024 across five databases, focusing on NLP applications in healthcare.", "result": "NLP shows potential in tasks such as error detection, correction, misinformation detection, and hallucination mitigation, but faces challenges related to data privacy and contextual evaluation.", "conclusion": "The review emphasizes the need for real-world datasets and refined methods to address challenges in using NLP for healthcare applications.", "key_contributions": ["Unified exploration of NLP's role in addressing medical inaccuracies", "Identification of specific NLP tasks for healthcare applications", "Discussion of challenges affecting NLP reliability in medical contexts"], "limitations": "Challenges with data privacy, context dependency, and evaluation standards remain.", "keywords": ["Natural Language Processing", "Medical Information", "Patient Safety", "Health Communication", "Misinformation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.15877", "pdf": "https://arxiv.org/pdf/2501.15877.pdf", "abs": "https://arxiv.org/abs/2501.15877", "title": "Boli: A dataset for understanding stuttering experience and analyzing stuttered speech", "authors": ["Ashita Batra", "Mannas Narang", "Neeraj Kumar Sharma", "Pradip K Das"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "There is a growing need for diverse, high-quality stuttered speech data,\nparticularly in the context of Indian languages. This paper introduces Project\nBoli, a multi-lingual stuttered speech dataset designed to advance scientific\nunderstanding and technology development for individuals who stutter,\nparticularly in India. The dataset constitutes (a) anonymized metadata (gender,\nage, country, mother tongue) and responses to a questionnaire about how\nstuttering affects their daily lives, (b) captures both read speech (using the\nRainbow Passage) and spontaneous speech (through image description tasks) for\neach participant and (c) includes detailed annotations of five stutter types:\nblocks, prolongations, interjections, sound repetitions and word repetitions.\nWe present a comprehensive analysis of the dataset, including the data\ncollection procedure, experience summarization of people who stutter, severity\nassessment of stuttering events and technical validation of the collected data.\nThe dataset is released as an open access to further speech technology\ndevelopment.", "AI": {"tldr": "This paper presents Project Boli, a multi-lingual stuttered speech dataset aimed at advancing understanding and technology for individuals who stutter in India.", "motivation": "To address the growing demand for diverse, high-quality stuttered speech data, especially in the context of Indian languages.", "method": "The dataset includes anonymized metadata, read and spontaneous speech samples, and annotations of various stutter types, collected through comprehensive procedures and participant questionnaires.", "result": "A detailed analysis of the dataset including management of metadata, speech samples, and stutter type annotations, which are validated for technical robustness.", "conclusion": "The open access release of the dataset is expected to facilitate research and development in speech technology aimed at supporting individuals who stutter.", "key_contributions": ["Introduction of a unique multi-lingual stuttered speech dataset for Indian languages.", "Inclusion of various stutter types with comprehensive annotations.", "Open access to data aimed to support further technological advancements."], "limitations": "", "keywords": ["stuttered speech", "dataset", "Indian languages", "speech technology", "HCI"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.00009", "pdf": "https://arxiv.org/pdf/2505.00009.pdf", "abs": "https://arxiv.org/abs/2505.00009", "title": "Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation", "authors": ["Xiao Zhang", "Kangsheng Wang", "Tianyu Hu", "Huimin Ma"], "categories": ["cs.CL"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo 2025", "summary": "Pre-trained language models (PLMs) demonstrate remarkable intelligence but\nstruggle with emerging tasks unseen during training in real-world applications.\nTraining separate models for each new task is usually impractical. Multi-task\nlearning (MTL) addresses this challenge by transferring shared knowledge from\nsource tasks to target tasks. As an dominant parameter-efficient fine-tuning\nmethod, prompt tuning (PT) enhances MTL by introducing an adaptable vector that\ncaptures task-specific knowledge, which acts as a prefix to the original prompt\nthat preserves shared knowledge, while keeping PLM parameters frozen. However,\nPT struggles to effectively capture the heterogeneity of task-specific\nknowledge due to its limited representational capacity. To address this\nchallenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL\nmethod built on PT, employing the low-rank representation to model task\nheterogeneity and a fast-slow weights mechanism where the slow weight encodes\nshared knowledge, while the fast weight captures task-specific nuances,\navoiding the mixing of shared and task-specific knowledge, caused by training\nlow-rank representations from scratch. Moreover, a zero-initialized attention\nmechanism is introduced to minimize the disruption of immature low-rank\ncomponents on original prompts during warm-up epochs. Experiments on 16 tasks\ndemonstrate that TA-LoRA achieves state-of-the-art performance in full-data and\nfew-shot settings while maintaining superior parameter efficiency.", "AI": {"tldr": "TA-LoRA enhances multi-task learning in pre-trained language models by capturing task heterogeneity with low-rank representation and a fast-slow weights mechanism, achieving state-of-the-art performance in various tasks.", "motivation": "Pre-trained language models struggle with unseen emerging tasks, making separate training impractical. Multi-task learning seeks to address this by leveraging shared knowledge across tasks.", "method": "Task-Adaptive Low-Rank Representation (TA-LoRA) employs low-rank representation combined with a fast-slow weights mechanism to effectively model task-specific knowledge and shared knowledge, along with a zero-initialized attention mechanism for stability during training.", "result": "TA-LoRA achieves state-of-the-art performance on 16 tasks in both full-data and few-shot scenarios, demonstrating superior parameter efficiency compared to existing methods.", "conclusion": "TA-LoRA is a robust solution for improving task adaptability in pre-trained language models while efficiently managing parameters and performance across multiple tasks.", "key_contributions": ["Introduction of Task-Adaptive Low-Rank Representation for multi-task learning", "Development of a fast-slow weights mechanism to separate shared and task-specific knowledge", "Implementation of a zero-initialized attention mechanism for better model stability"], "limitations": "", "keywords": ["multi-task learning", "prompt tuning", "task-specific representation", "pre-trained language models", "parameter efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00010", "pdf": "https://arxiv.org/pdf/2505.00010.pdf", "abs": "https://arxiv.org/abs/2505.00010", "title": "Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models", "authors": ["Tri Nguyen", "Lohith Srikanth Pentapalli", "Magnus Sieverding", "Laurah Turner", "Seth Overla", "Weibing Zheng", "Chris Zhou", "David Furniss", "Danielle Weber", "Michael Gharib", "Matt Kelleher", "Michael Shukis", "Cameron Pawlik", "Kelly Cohen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Jailbreaking in Large Language Models (LLMs) threatens their safe use in\nsensitive domains like education by allowing users to bypass ethical\nsafeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical\neducation platform that simulates patient interactions using LLMs. We annotated\nover 2,300 prompts across 158 conversations using four linguistic variables\nshown to correlate strongly with jailbreak behavior. The extracted features\nwere used to train several predictive models, including Decision Trees, Fuzzy\nLogic-based classifiers, Boosting methods, and Logistic Regression. Results\nshow that feature-based predictive models consistently outperformed Prompt\nEngineering, with the Fuzzy Decision Tree achieving the best overall\nperformance. Our findings demonstrate that linguistic-feature-based models are\neffective and explainable alternatives for jailbreak detection. We suggest\nfuture work explore hybrid frameworks that integrate prompt-based flexibility\nwith rule-based robustness for real-time, spectrum-based jailbreak monitoring\nin educational LLMs.", "AI": {"tldr": "This paper investigates jailbreak detection in Large Language Models (LLMs) used in clinical education, proposing effective linguistic-feature-based models over traditional prompt engineering.", "motivation": "Jailbreaking in LLMs poses risks by allowing the bypassing of ethical safeguards, particularly in sensitive sectors like education.", "method": "Annotated over 2,300 prompts by analyzing 158 conversations using four linguistic variables correlated with jailbreak behavior; trained models including Decision Trees, Fuzzy Logic, Boosting, and Logistic Regression.", "result": "The feature-based predictive models outperformed traditional Prompt Engineering, with the Fuzzy Decision Tree demonstrating the highest performance.", "conclusion": "Linguistic-feature-based models are effective and can serve as explainable alternatives for detecting jailbreaks in educational contexts, suggesting future work on hybrid frameworks for monitoring.", "key_contributions": ["Introduced a methodology for detecting jailbreaks using linguistic features.", "Demonstrated that feature-based models surpass prompt engineering in effectiveness.", "Proposed future exploration of hybrid frameworks for enhanced monitoring of educational LLMs."], "limitations": "The study primarily focuses on a specific domain (clinical education) and may not generalize across all applications of LLMs.", "keywords": ["Jailbreaking", "Large Language Models", "Detection", "Linguistic Features", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00012", "pdf": "https://arxiv.org/pdf/2505.00012.pdf", "abs": "https://arxiv.org/abs/2505.00012", "title": "The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?", "authors": ["Fabian Retkowski", "Andreas Sudmann", "Alexander Waibel"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NLP4DH 2025", "summary": "Qualitative research often involves labor-intensive processes that are\ndifficult to scale while preserving analytical depth. This paper introduces The\nAI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for\nqualitative research and designed to move beyond the limitations of simply\nautomating code assignments, offering a more integrated approach. AICoE\norganizes the entire process, encompassing open coding, code consolidation,\ncode application, and even pattern discovery, leading to a comprehensive\nanalysis of qualitative data.", "AI": {"tldr": "The AI Co-Ethnographer (AICoE) is an innovative pipeline for qualitative research that automates and integrates key analysis processes to enhance data insight.", "motivation": "To address the labor-intensive nature of qualitative research and improve scalability without sacrificing analytical quality.", "method": "The AICoE pipeline manages the entire qualitative research process, from open coding to pattern discovery, facilitating comprehensive data analysis.", "result": "AICoE provides a more cohesive framework for qualitative analysis, improving efficiency and depth of insight compared to traditional methods.", "conclusion": "The integration offered by AICoE enhances qualitative research, allowing researchers to maintain analytical depth while scaling their efforts.", "key_contributions": ["Introduction of AICoE pipeline for qualitative research", "Comprehensive integration of qualitative analysis processes", "Facilitation of pattern discovery in qualitative data"], "limitations": "", "keywords": ["qualitative research", "AI Co-Ethnographer", "data analysis", "pattern discovery", "NLP"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.00013", "pdf": "https://arxiv.org/pdf/2505.00013.pdf", "abs": "https://arxiv.org/abs/2505.00013", "title": "Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa", "authors": ["Yoichi Takenaka"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 tables, 3 appendices. Submitted to New Generation\n  Computing. Includes comparisons between fine-tuned PLMs and LLMs on Japanese\n  emotion classification. Code available at\n  https://pypi.org/project/deberta-emotion-predictor/", "summary": "Background Practical applications such as social media monitoring and\ncustomer-feedback analysis require accurate emotion detection for Japanese\ntext, yet resource scarcity and class imbalance hinder model performance.\n  Objective This study aims to build a high-accuracy model for predicting the\npresence or absence of eight Plutchik emotions in Japanese sentences.\n  Methods Using the WRIME corpus, we transform reader-averaged intensity scores\ninto binary labels and fine-tune four pre-trained language models (BERT,\nRoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two\nlarge language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and\nF1-score serve as evaluation metrics.\n  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score\n(0.662), outperforming all other models. It maintains robust F1 across both\nhigh-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions\n(e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and\nTinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.\n  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most\nreliable solution for binary emotion classification in Japanese. We release\nthis model as a pip-installable package (pip install\ndeberta-emotion-predictor). Future work should augment data for rare emotions,\nreduce model size, and explore prompt engineering to improve LLM performance.\n  This manuscript is under review for possible publication in New Generation\nComputing.", "AI": {"tldr": "This study develops a high-accuracy model for detecting eight Plutchik emotions in Japanese text using fine-tuned language models.", "motivation": "Accurate emotion detection in Japanese text is challenged by resource scarcity and class imbalance, impacting practical applications like social media monitoring.", "method": "The study fine-tunes four pre-trained models (BERT, RoBERTa, DeBERTa variants) and evaluates two large language models (ChatGPT-4o, TinySwallow-1.5B-Instruct) using the WRIME corpus and binary labeling of emotions.", "result": "DeBERTa-v3-large achieved the best mean accuracy of 0.860 and an F1-score of 0.662, outperforming other tested models, while LLMs underperformed significantly.", "conclusion": "The fine-tuned DeBERTa-v3-large model is the most effective for binary emotion classification in Japanese, with future recommendations for improving performance on rare emotions and LLMs.", "key_contributions": ["Development of a high-accuracy emotional classification model for Japanese text", "First comprehensive comparison of PLMs and LLMs on Japanese emotion detection", "Release of a pip-installable package for practical application."], "limitations": "Future work needed on data augmentation for rare emotions and improving LLM performance.", "keywords": ["emotion detection", "Japanese text", "pre-trained language models", "large language models", "natural language processing"], "importance_score": 6, "read_time_minutes": 14}}
{"id": "2505.00014", "pdf": "https://arxiv.org/pdf/2505.00014.pdf", "abs": "https://arxiv.org/abs/2505.00014", "title": "Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and Möbius Strips", "authors": ["Vinit K. Chavan"], "categories": ["cs.CL"], "comment": "10 pages, 6 figures. Code available at\n  https://github.com/vinitchavan/manifold-embedding-nlp", "summary": "Recent advances in representation learning have emphasized the role of\nembedding geometry in capturing semantic structure. Traditional sentence\nembeddings typically reside in unconstrained Euclidean spaces, which may limit\ntheir ability to reflect complex relationships in language. In this work, we\nintroduce a novel framework that constrains sentence embeddings to lie on\ncontinuous manifolds -- specifically the unit sphere, torus, and M\\\"obius strip\n-- using triplet loss as the core training objective. By enforcing differential\ngeometric constraints on the output space, our approach encourages the learning\nof embeddings that are both discriminative and topologically structured.\n  We evaluate our method on benchmark datasets (AG News and MBTI) and compare\nit to classical baselines including TF-IDF, Word2Vec, and unconstrained\nKeras-derived embeddings. Our results demonstrate that manifold-constrained\nembeddings, particularly those projected onto spheres and M\\\"obius strips,\nsignificantly outperform traditional approaches in both clustering quality\n(Silhouette Score) and classification performance (Accuracy). These findings\nhighlight the value of embedding in manifold space -- where topological\nstructure complements semantic separation -- offering a new and mathematically\ngrounded direction for geometric representation learning in NLP.", "AI": {"tldr": "This paper introduces a framework for sentence embeddings constrained to continuous manifolds, improving semantic representation through differential geometric constraints.", "motivation": "The study addresses limitations of traditional sentence embeddings residing in unconstrained Euclidean spaces, which may fail to capture complex language relationships.", "method": "The authors propose a novel training approach that uses triplet loss to constrain sentence embeddings onto continuous manifolds such as the unit sphere, torus, and M\"obius strip.", "result": "The proposed manifold-constrained embeddings significantly outperform classical baselines like TF-IDF and Word2Vec on benchmark datasets, achieving better clustering quality and classification performance.", "conclusion": "Embedding in manifold space provides a mathematically grounded approach to enhance semantic representation in NLP by integrating topological structure with semantic separation.", "key_contributions": ["Introduction of manifold-constrained sentence embeddings", "Use of triplet loss for embedding training", "Demonstration of improved performance on NLP tasks through topologically structured embeddings"], "limitations": "", "keywords": ["sentence embeddings", "manifolds", "topology", "representation learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.00015", "pdf": "https://arxiv.org/pdf/2505.00015.pdf", "abs": "https://arxiv.org/abs/2505.00015", "title": "Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain"], "categories": ["cs.CL"], "comment": "Shortened the abstract to fit within 1920 characters. This paper is\n  currently under Review in Elsevier journal 'Accident Analysis & Prevention'", "summary": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh.", "AI": {"tldr": "This research proposes an automated accident data collection system using Large Language Models (LLMs) and web scraping techniques to improve the reporting and analysis of road traffic accidents in Bangladesh.", "motivation": "Road traffic accidents in developing countries like Bangladesh are underreported due to manual and fragmented data collection methods, necessitating a reliable automated system.", "method": "The system includes automated web scraping code generation, news collection, accident classification, structured data extraction, and duplicate removal, utilizing the multimodal generative LLM Gemini-2.0-Flash for automation.", "result": "Over 111 days, the system scraped 15,000 news articles from 14 Bangladeshi news sites, identifying 705 unique accidents with a code generation module achieving 91.3% calibration accuracy and 80% validation accuracy.", "conclusion": "The system showcased the potential for a scalable, LLM-powered approach to enhance road safety data collection, which can inform data-driven policymaking in Bangladesh.", "key_contributions": ["Development of an automated system for accident data collection using LLMs.", "Integration of web scraping, classification, and data extraction techniques for real-time analysis.", "Creation of a public repository for the system's usage and data accessibility."], "limitations": "", "keywords": ["accident data collection", "Large Language Models", "web scraping", "Bangladesh", "road safety"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.00016", "pdf": "https://arxiv.org/pdf/2505.00016.pdf", "abs": "https://arxiv.org/abs/2505.00016", "title": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Julien Fauqueur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a 20\\% increase in accuracy when trained on Text-to-SQL\ntasks, while Qwen achieved a 5\\% increase. These results suggest that SQL can\nserve not only as a target formalism but also as an effective scaffold for\nlearning robust, transferable reasoning over structured data.", "AI": {"tldr": "This paper presents a framework that teaches large language models to reason with tabular data through a two-stage Text-to-SQL process, improving generalization and accuracy.", "motivation": "The goal is to enhance large language models' reasoning abilities over structured data beyond simple query generation.", "method": "A two-stage framework is proposed: first, detailed chain-of-thought traces are synthesized from real SQL queries for supervision; second, a Group Relative Policy Optimization (GRPO) reinforcement learning objective is introduced to connect SQL execution accuracy to generalizable reasoning.", "result": "The framework shows improved performance on standard Text-to-SQL benchmarks and significant gains on reasoning-intensive datasets like BIRD and CRT-QA, including a 20% accuracy increase for the distilled-quantized LLaMA model.", "conclusion": "SQL can be utilized not only as a target for queries but also as a teaching scaffold for robust reasoning skills over structured data.", "key_contributions": ["Two-stage framework for teaching reasoning with tabular data.", "Introduction of GRPO reinforcement learning objective.", "Demonstrates improved performance on reasoning-intensive datasets."], "limitations": "", "keywords": ["Text-to-SQL", "large language models", "table reasoning", "reinforcement learning", "structured data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00017", "pdf": "https://arxiv.org/pdf/2505.00017.pdf", "abs": "https://arxiv.org/abs/2505.00017", "title": "ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation", "authors": ["Dezheng Han", "Yibin Jia", "Ruxiao Chen", "Wenjie Han", "Shuaishuai Guo", "Jianbo Wang"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "To enable precise and fully automated cell type annotation with large\nlanguage models (LLMs), we developed a graph structured feature marker database\nto retrieve entities linked to differential genes for cell reconstruction. We\nfurther designed a multi task workflow to optimize the annotation process.\nCompared to general purpose LLMs, our method improves human evaluation scores\nby up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while\nmore closely aligning with the cognitive logic of manual annotation.", "AI": {"tldr": "Developed a graph structured feature marker database and a multi-task workflow for automated cell type annotation using LLMs.", "motivation": "To enable precise and fully automated cell type annotation with large language models.", "method": "Created a graph structured feature marker database to retrieve entities linked to differential genes, and designed a multi-task workflow to optimize the annotation process.", "result": "Our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, aligning more closely with manual annotation cognitive logic.", "conclusion": "This approach enhances the efficiency and accuracy of cell type annotation compared to general purpose LLMs.", "key_contributions": ["Graph structured feature marker database", "Multi-task workflow for annotation optimization", "Improved evaluation scores and semantic similarity in cell annotations"], "limitations": "", "keywords": ["cell type annotation", "large language models", "feature marker database", "multi-task workflow", "semantic similarity"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.00019", "pdf": "https://arxiv.org/pdf/2505.00019.pdf", "abs": "https://arxiv.org/abs/2505.00019", "title": "An Empirical Study on Prompt Compression for Large Language Models", "authors": ["Zheng Zhang", "Jinyi Li", "Yihuai Lan", "Xiang Wang", "Hao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by Building Trust Workshop at ICLR 2025", "summary": "Prompt engineering enables Large Language Models (LLMs) to perform a variety\nof tasks. However, lengthy prompts significantly increase computational\ncomplexity and economic costs. To address this issue, we study six prompt\ncompression methods for LLMs, aiming to reduce prompt length while maintaining\nLLM response quality. In this paper, we present a comprehensive analysis\ncovering aspects such as generation performance, model hallucinations, efficacy\nin multimodal tasks, word omission analysis, and more. We evaluate these\nmethods across 13 datasets, including news, scientific articles, commonsense\nQA, math QA, long-context QA, and VQA datasets. Our experiments reveal that\nprompt compression has a greater impact on LLM performance in long contexts\ncompared to short ones. In the Longbench evaluation, moderate compression even\nenhances LLM performance. Our code and data is available at\nhttps://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.", "AI": {"tldr": "This paper examines six methods for compressing prompts used in Large Language Models (LLMs) to reduce their length while preserving response quality.", "motivation": "The growing use of LLMs leads to increased computational complexity and costs associated with lengthy prompts; thus, prompt compression is necessary for efficiency.", "method": "The study analyzes six different prompt compression techniques and evaluates their performance using 13 varied datasets across multiple multimodal tasks.", "result": "The findings indicate that prompt compression notably improves LLM performance in long-context scenarios and moderate compression can enhance overall performance during evaluations.", "conclusion": "The research confirms that effective prompt compression can help maintain LLM response quality while significantly reducing costs and computational demands.", "key_contributions": ["Analysis of six prompt compression methods", "Comprehensive evaluation across 13 diverse datasets", "Demonstration of performance improvements in long-context LLM tasks"], "limitations": "The study may not cover every possible prompt compression technique or all potential LLM applications.", "keywords": ["prompt engineering", "large language models", "compression methods", "long-context performance", "multimodal tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00020", "pdf": "https://arxiv.org/pdf/2505.00020.pdf", "abs": "https://arxiv.org/abs/2505.00020", "title": "Beyond Public Access in LLM Pre-Training Data", "authors": ["Sruly Rosenblat", "Tim O'Reilly", "Ilan Strauss"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we\napply the DE-COP membership inference attack method to investigate whether\nOpenAI's large language models were trained on copyrighted content without\nconsent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable\nmodel, demonstrates strong recognition of paywalled O'Reilly book content\n(AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,\nGPT-3.5 Turbo shows greater relative recognition of publicly accessible\nO'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge\nof public or non-public O'Reilly Media content when tested (AUROC $\\approx$\n50\\%). Testing multiple models, with the same cutoff date, helps us account for\npotential language shifts over time that might bias our findings. These results\nhighlight the urgent need for increased corporate transparency regarding\npre-training data sources as a means to develop formal licensing frameworks for\nAI content training", "AI": {"tldr": "The paper investigates whether OpenAI's language models were trained on copyrighted O'Reilly Media books using a membership inference attack method, revealing strong recognition of copyrighted content by GPT-4o.", "motivation": "To determine if OpenAI's large language models have been trained on copyrighted material without consent, prompting a discussion on corporate transparency and licensing.", "method": "The DE-COP membership inference attack method is applied to a legally obtained dataset of 34 copyrighted O'Reilly Media books, analyzing recognition rates of different models.", "result": "GPT-4o shows 82% recognition of copyrighted content, while GPT-3.5 Turbo favors public content. GPT-4o Mini displays no knowledge of either category.", "conclusion": "Findings emphasize the necessity for more transparency in corporate practices about training data to establish proper licensing frameworks for AI.", "key_contributions": ["Application of DE-COP attack on AI models", "Comparison of copyrighted vs. public content recognition across models", "Highlighting issues of transparency in AI training practices"], "limitations": "Focused only on O'Reilly Media books and does not cover broader data sources.", "keywords": ["membership inference attack", "GPT-4o", "AI training data", "copyright", "transparency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00021", "pdf": "https://arxiv.org/pdf/2505.00021.pdf", "abs": "https://arxiv.org/abs/2505.00021", "title": "Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss", "authors": ["Zhuoang Cai", "Zhenghao Li", "Yang Liu", "Liyuan Guo", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classification tasks often suffer from imbal- anced data distribution, which\npresents chal- lenges in food hazard detection due to severe class imbalances,\nshort and unstructured text, and overlapping semantic categories. In this\npaper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,\nwhich ad- dresses these issues by applying data augmenta- tion techniques to\nimprove classification perfor- mance. We utilize transformer-based models, BERT\nand RoBERTa, as backbone classifiers and explore various data balancing\nstrategies, including random oversampling, Easy Data Augmentation (EDA), and\nfocal loss. Our ex- periments show that EDA effectively mitigates class\nimbalance, leading to significant improve- ments in accuracy and F1 scores.\nFurthermore, combining focal loss with oversampling and EDA further enhances\nmodel robustness, par- ticularly for hard-to-classify examples. These findings\ncontribute to the development of more effective NLP-based classification models\nfor food hazard detection.", "AI": {"tldr": "The paper presents a system for food hazard detection using transformer models and data augmentation techniques to address class imbalance issues.", "motivation": "The paper aims to improve classification performance in food hazard detection, which suffers from imbalanced data distribution, short text, and overlapping semantic categories.", "method": "The study employs transformer-based models (BERT and RoBERTa) and explores data balancing strategies like random oversampling, Easy Data Augmentation (EDA), and focal loss.", "result": "Experiments indicate that EDA significantly mitigates class imbalance, enhancing accuracy and F1 scores. Additionally, combining focal loss with oversampling and EDA improves robustness for difficult examples.", "conclusion": "The findings contribute to developing more effective NLP-based classification models tailored for food hazard detection.", "key_contributions": ["Introduction of effective data augmentation techniques for class imbalance in food hazard detection", "Demonstration of the efficacy of combining focal loss with oversampling and EDA", "Contribution to the NLP classification models for specific real-world applications."], "limitations": "", "keywords": ["food hazard detection", "class imbalance", "data augmentation", "transformer models", "NLP"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.00022", "pdf": "https://arxiv.org/pdf/2505.00022.pdf", "abs": "https://arxiv.org/abs/2505.00022", "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation", "authors": ["Thomas F Burns", "Letitia Parcalabescu", "Stephan Wäldchen", "Michael Barlow", "Gregor Ziegltrum", "Volker Stampa", "Bastian Harren", "Björn Deiseroth"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Scaling data quantity is essential for large language models (LLMs), yet\nrecent findings show that data quality can significantly boost performance and\ntraining efficiency. We introduce a German-language dataset curation pipeline\nthat combines heuristic and model-based filtering techniques with synthetic\ndata generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a\nlarge-scale German pre-training dataset which draws from: (1) Common Crawl web\ndata, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,\norganic web data. We evaluate our dataset by pre-training both a 1B Llama-style\nmodel and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A\ncomparison on German-language benchmarks, including MMMLU, shows significant\nperformance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage\nholds at the 8B scale even when FineWeb2 is enriched by human-curated\nhigh-quality data sources such as Wikipedia. Our findings support the growing\nbody of evidence that model-based data curation and synthetic data generation\ncan significantly enhance LLM pre-training datasets.", "AI": {"tldr": "The paper introduces a data curation pipeline that enhances the quality of a large-scale German-language dataset for training large language models, showing significant performance improvements over existing datasets.", "motivation": "To address the challenge of scaling data quantity while emphasizing the importance of data quality in enhancing performance and training efficiency for large language models.", "method": "The authors presented a dataset curation pipeline that utilizes heuristic and model-based filtering techniques combined with synthetic data generation, resulting in Aleph-Alpha-GermanWeb, a new large-scale German pre-training dataset.", "result": "Aleph-Alpha-GermanWeb demonstrates substantial performance improvements over FineWeb2 alone when benchmarking against German-language tasks, particularly with a 1B Llama-style model and an 8B HAT model.", "conclusion": "The research indicates that combining model-based curation and synthetic data generation effectively enhances LLM pre-training datasets, supporting broader implications for data strategy in AI.", "key_contributions": ["Introduction of a novel dataset curation pipeline for LLMs.", "Creation of Aleph-Alpha-GermanWeb with improved performance metrics.", "Significant findings on the impact of data quality in LLM training."], "limitations": "", "keywords": ["large language models", "data quality", "synthetic data", "dataset curation", "German language"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00023", "pdf": "https://arxiv.org/pdf/2505.00023.pdf", "abs": "https://arxiv.org/abs/2505.00023", "title": "CORG: Generating Answers from Complex, Interrelated Contexts", "authors": ["Hyunji Lee", "Franck Dernoncourt", "Trung Bui", "Seunghyun Yoon"], "categories": ["cs.CL", "cs.AI"], "comment": "published at Findings of NAACL 2025", "summary": "In a real-world corpus, knowledge frequently recurs across documents but\noften contains inconsistencies due to ambiguous naming, outdated information,\nor errors, leading to complex interrelationships between contexts. Previous\nresearch has shown that language models struggle with these complexities,\ntypically focusing on single factors in isolation. We classify these\nrelationships into four types: distracting, ambiguous, counterfactual, and\nduplicated. Our analysis reveals that no single approach effectively addresses\nall these interrelationships simultaneously. Therefore, we introduce Context\nOrganizer (CORG), a framework that organizes multiple contexts into\nindependently processed groups. This design allows the model to efficiently\nfind all relevant answers while ensuring disambiguation. CORG consists of three\nkey components: a graph constructor, a reranker, and an aggregator. Our results\ndemonstrate that CORG balances performance and efficiency effectively,\noutperforming existing grouping methods and achieving comparable results to\nmore computationally intensive, single-context approaches.", "AI": {"tldr": "The paper introduces Context Organizer (CORG), a framework designed to manage and disambiguate multiple contexts in language models, addressing recurrent knowledge inconsistencies seen in complex document interrelationships.", "motivation": "To tackle the challenges language models face with recurrent knowledge in documents that often have inconsistencies due to numerous factors.", "method": "CORG is developed to organize contexts into independently processed groups, utilizing a graph constructor, a reranker, and an aggregator to enhance the model's ability to find relevant answers and ensure disambiguation.", "result": "CORG outperforms existing methods for grouping contexts, achieving a balance of performance and efficiency, while delivering results comparable to more intensive single-context approaches.", "conclusion": "The framework effectively addresses the complexities of knowledge interrelationships, showcasing significant improvements over prior methods.", "key_contributions": ["Introduction of Context Organizer (CORG) framework", "Classification of context relationships into four types", "Demonstrated balance of performance and efficiency in managing document interrelationships"], "limitations": "", "keywords": ["Context Organization", "Language Models", "Disambiguation", "Knowledge Recurrence", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00024", "pdf": "https://arxiv.org/pdf/2505.00024.pdf", "abs": "https://arxiv.org/abs/2505.00024", "title": "Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning", "authors": ["Shaokun Zhang", "Yi Dong", "Jieyu Zhang", "Jan Kautz", "Bryan Catanzaro", "Andrew Tao", "Qingyun Wu", "Zhiding Yu", "Guilin Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 tables, 5 figures", "summary": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text generation tasks. Prior\nwork typically enhances tool-use abilities by either applying supervised\nfine-tuning (SFT) to enforce tool-call correctness or distilling reasoning\ntraces from stronger models for SFT. However, both approaches fall short,\neither omitting reasoning entirely or producing imitative reasoning that limits\ngeneralization. Inspired by the success of DeepSeek-R1 in eliciting reasoning\nthrough rule-based reinforcement learning, we develop the\nNemotron-Research-Tool-N1 series of tool-using language models using a similar\ntraining paradigm. Instead of restrictively supervising intermediate reasoning\ntraces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized\nwith a binary reward that evaluates only the structural validity and functional\ncorrectness of tool invocations. This lightweight supervision allows the model\nto autonomously internalize reasoning strategies, without the need for\nannotated reasoning trajectories. Experiments on the BFCL and API-Bank\nbenchmarks show that Nemotron-Research-Tool-N1-7B and\nNemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve\nstate-of-the-art results, outperforming GPT-4o on both evaluations.", "AI": {"tldr": "This paper presents the Nemotron-Research-Tool-N1 series of language models that utilize a novel training methodology based on reinforcement learning for effective tool usage without supervised reasoning.", "motivation": "The need to enhance large language models with external tools for broader functionality beyond text generation, addressing the limitations of existing supervised fine-tuning methods.", "method": "The Nemotron-Research-Tool-N1 models are trained using a lightweight reinforcement learning approach that evaluates tool invocations based on structural validity and functional correctness, rather than using supervised reasoning traces.", "result": "The Nemotron-Research-Tool-N1 models achieved state-of-the-art performance on BFCL and API-Bank benchmarks, surpassing the performance of GPT-4o.", "conclusion": "This approach allows for effective tool usage in language models by enabling them to autonomously develop reasoning strategies and achieve superior results without extensive labeled training data.", "key_contributions": ["Introduction of a new training paradigm for tool-using language models based on binary reward reinforcement learning.", "Demonstrated superior performance over existing models like GPT-4o on key benchmarks.", "Showcased the ability of models to internalize reasoning strategies without extensive supervision."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "tool use", "natural language processing", "machine learning"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2505.00025", "pdf": "https://arxiv.org/pdf/2505.00025.pdf", "abs": "https://arxiv.org/abs/2505.00025", "title": "A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1", "authors": ["Mingda Zhang", "Jianglong Qin"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "comment": "14 pages, 1 figures", "summary": "In recent years, despite foundation models like DeepSeek-R1 and ChatGPT\ndemonstrating significant capabilities in general tasks, professional knowledge\nbarriers, computational resource requirements, and deployment environment\nlimitations have severely hindered their application in actual medical\nscenarios. Addressing these challenges, this paper proposes an efficient\nlightweight medical vertical large language model architecture method,\nsystematically solving the lightweight problem of medical large models from\nthree dimensions: knowledge acquisition, model compression, and computational\noptimization. At the knowledge acquisition level, a knowledge transfer pipeline\nis designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the\nDeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology\nis adopted to precisely adjust key attention layers. At the model compression\nlevel, compression techniques including 4-bit weight quantization are\nimplemented while preserving the core representation ability for medical\nreasoning. At the computational optimization level, inference optimization\ntechniques such as Flash Attention acceleration and continuous batching are\nintegrated, and a professional prompt template system is constructed to adapt\nto different types of medical problems. Experimental results on medical\nquestion-answering datasets show that the method proposed in this paper\nmaintains professional accuracy while reducing memory consumption by 64.7\\% and\ninference latency by 12.4\\%, providing an effective solution for the\napplication of medical large models in resource-constrained environments such\nas edge computing devices.", "AI": {"tldr": "This paper proposes a lightweight architecture for medical large language models, addressing challenges in knowledge acquisition, model compression, and computational optimization to enhance their application in resource-constrained environments.", "motivation": "To tackle the significant barriers preventing the practical use of powerful foundation models in medical scenarios, this paper introduces an efficient architecture specifically designed for medical applications.", "method": "The proposed methodology involves a knowledge transfer pipeline from a fine-tuned teacher model to a student model, implementation of 4-bit weight quantization, and optimization techniques like Flash Attention and continuous batching.", "result": "Experimental results demonstrate that the proposed method achieves 64.7% reduction in memory consumption and 12.4% decrease in inference latency, while maintaining professional accuracy in medical question-answering tasks.", "conclusion": "The proposed lightweight architecture effectively enables the deployment of large language models in resource-limited medical environments, making them more accessible for practical medical applications.", "key_contributions": ["Lightweight architecture for medical language models", "Knowledge transfer pipeline using Low-Rank Adaptation", "Compression techniques maintaining accuracy and reducing resource use"], "limitations": "The paper may not address the potential ethical and regulatory implications of deploying AI models in medical settings.", "keywords": ["large language models", "medical AI", "lightweight architecture", "knowledge transfer", "model compression"], "importance_score": 9, "read_time_minutes": 14}}
{"id": "2505.00026", "pdf": "https://arxiv.org/pdf/2505.00026.pdf", "abs": "https://arxiv.org/abs/2505.00026", "title": "Theory of Mind in Large Language Models: Assessment and Enhancement", "authors": ["Ruirui Chen", "Weifeng Jiang", "Chengwei Qin", "Cheston Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theory of Mind (ToM)-the ability to infer and reason about others' mental\nstates-is fundamental to human social intelligence. As Large Language Models\n(LLMs) become increasingly integrated into daily life, it is crucial to assess\nand enhance their capacity to interpret and respond to human mental states. In\nthis paper, we review LLMs' ToM capabilities by examining both evaluation\nbenchmarks and the strategies designed to improve them. We focus on widely\nadopted story-based benchmarks and provide an in-depth analysis of methods\naimed at enhancing ToM in LLMs. Furthermore, we outline promising future\nresearch directions informed by recent benchmarks and state-of-the-art\napproaches. Our survey serves as a valuable resource for researchers interested\nin advancing LLMs' ToM capabilities.", "AI": {"tldr": "This paper reviews the Theory of Mind (ToM) capabilities of Large Language Models (LLMs) and presents methods for their enhancement.", "motivation": "With the integration of LLMs into daily life, assessing and improving their ability to understand human mental states is essential for enhancing human-computer interactions.", "method": "The paper examines existing evaluation benchmarks for ToM in LLMs and analyzes strategies that have been proposed to enhance these capabilities.", "result": "The review reveals various methods aimed at improving LLMs' ToM performance and identifies key benchmarks that facilitate their assessment.", "conclusion": "The findings indicate that enhancing ToM capabilities in LLMs can significantly improve their applicability in sensitive and interactive contexts, with suggestions for future research directions.", "key_contributions": ["In-depth analysis of story-based benchmarks for assessing ToM in LLMs.", "Overview of strategies designed to enhance LLMs' ToM capabilities.", "Identification of promising future research avenues related to ToM in LLMs."], "limitations": "", "keywords": ["Theory of Mind", "Large Language Models", "human mental states", "evaluation benchmarks", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.00027", "pdf": "https://arxiv.org/pdf/2505.00027.pdf", "abs": "https://arxiv.org/abs/2505.00027", "title": "Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts", "authors": ["Jian Zhou", "Jiazheng Li", "Sirui Zhuge", "Hai Zhuge"], "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F20 (Secondary)", "I.2.7; I.2.1"], "comment": "25pages, 3 figures, 8 tables", "summary": "This paper proposed an approach to automatically discovering subject\ndimension, action dimension, object dimension and adverbial dimension from\ntexts to efficiently operate texts and support query in natural language. The\nhigh quality of trees guarantees that all subjects, actions, objects and\nadverbials and their subclass relations within texts can be represented. The\nindependency of trees ensures that there is no redundant representation between\ntrees. The expressiveness of trees ensures that the majority of sentences can\nbe accessed from each tree and the rest of sentences can be accessed from at\nleast one tree so that the tree-based search mechanism can support querying in\nnatural language. Experiments show that the average precision, recall and\nF1-score of the abstraction trees constructed by the subclass relations of\nsubject, action, object and adverbial are all greater than 80%. The application\nof the proposed approach to supporting query in natural language demonstrates\nthat different types of question patterns for querying subject or object have\nhigh coverage of texts, and searching multiple trees on subject, action, object\nand adverbial according to the question pattern can quickly reduce search space\nto locate target sentences, which can support precise operation on texts.", "AI": {"tldr": "The paper presents a novel approach for automatically extracting subject, action, object, and adverbial dimensions from text to enhance natural language querying.", "motivation": "To improve text operations and support efficient querying in natural language by clearly representing subjects, actions, objects, and adverbials.", "method": "The proposed approach constructs abstraction trees from texts that represent relevant linguistic dimensions (subject, action, object, adverbial) while ensuring expressiveness and independence among trees to avoid redundancy.", "result": "Experiments show that the abstraction trees yield high performance with precision, recall, and F1-scores exceeding 80%. The approach effectively supports diverse natural language question patterns with good text coverage.", "conclusion": "The tree-based search mechanism enables precise querying and manipulation of texts by quickly narrowing down the search space for target sentences.", "key_contributions": ["Introduction of abstraction trees for extracting linguistic dimensions", "Demonstrated high performance metrics (precision, recall, F1 score)", "Effective natural language query support with diverse question patterns"], "limitations": "", "keywords": ["Natural Language Processing", "Text Representation", "Abstraction Trees", "Querying", "Subject-Action-Object"], "importance_score": 6, "read_time_minutes": 25}}
{"id": "2505.00028", "pdf": "https://arxiv.org/pdf/2505.00028.pdf", "abs": "https://arxiv.org/abs/2505.00028", "title": "Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation", "authors": ["Pengchao Feng", "Ziyang Ma", "Wenxi Chen", "Yao Li", "Sheng Wang", "Kai Yu", "Xie Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In recent years, end-to-end speech-to-speech (S2S) dialogue systems have\ngarnered increasing research attention due to their advantages over traditional\ncascaded systems, including achieving lower latency and more natural\nintegration of nonverbal cues such as emotion and speaker identity. However,\nthese end-to-end systems face key challenges, particularly in incorporating\nexternal knowledge, a capability commonly addressed by Retrieval-Augmented\nGeneration (RAG) in text-based large language models (LLMs). The core\ndifficulty lies in the modality gap between input speech and retrieved textual\nknowledge, which hinders effective integration. To address this issue, we\npropose a novel end-to-end RAG framework that directly retrieves relevant\ntextual knowledge from speech queries, eliminating the need for intermediate\nspeech-to-text conversion via techniques like ASR. Experimental results\ndemonstrate that our method significantly improves the performance of\nend-to-end S2S dialogue systems while achieving higher retrieval efficiency.\nAlthough the overall performance still lags behind cascaded models, our\nframework offers a promising direction for enhancing knowledge integration in\nend-to-end S2S systems. We will release the code and dataset to support\nreproducibility and promote further research in this area.", "AI": {"tldr": "This paper presents a novel end-to-end RAG framework for speech-to-speech dialogue systems that integrates relevant textual knowledge from speech queries without speech-to-text conversion, improving performance and retrieval efficiency.", "motivation": "End-to-end S2S dialogue systems can achieve lower latency and a more natural integration of nonverbal cues but struggle to incorporate external knowledge effectively.", "method": "The proposed framework directly retrieves relevant textual knowledge from speech queries, bypassing the intermediate ASR process.", "result": "The experimental results show significant improvements in the performance of end-to-end S2S dialogue systems and higher retrieval efficiency compared to traditional methods.", "conclusion": "While this method offers enhanced knowledge integration and is a promising direction for end-to-end S2S systems, it still does not outperform traditional cascaded models overall.", "key_contributions": ["Introduction of a novel end-to-end RAG framework for S2S dialogue systems.", "Direct retrieval of textual knowledge from speech queries without ASR.", "Significant improvement in retrieval efficiency and performance metrics."], "limitations": "Overall performance still lags behind that of cascaded models.", "keywords": ["speech-to-speech", "dialogue systems", "retrieval-augmented generation", "end-to-end", "knowledge integration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00029", "pdf": "https://arxiv.org/pdf/2505.00029.pdf", "abs": "https://arxiv.org/abs/2505.00029", "title": "Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting", "authors": ["Yijie Hong", "Xiaofei Yin", "Xinzhong Wang", "Yi Tu", "Ya Guo", "Sufeng Duan", "Weiqiang Wang", "Lingyong Fang", "Depeng Wang", "Huijia Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures", "summary": "Large Vision Language Models have demonstrated impressive versatile\ncapabilities through extensive multimodal pre-training, but face significant\nlimitations when incorporating specialized knowledge domains beyond their\ntraining distribution. These models struggle with a fundamental dilemma: direct\nadaptation approaches that inject domain-specific knowledge often trigger\ncatastrophic forgetting of foundational visual-linguistic abilities. We\nintroduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that\neffectively injects domain-specific knowledge while minimizing catastrophic\nforgetting. Drawing inspiration from supervised fine-tuning in LLMs and\nsubject-driven personalization in text-to-image diffusion models, our method\nemploys a three-phase dialogue structure: Foundation Preservation reinforces\npre-trained visual-linguistic alignment through caption tasks; Contrastive\nDisambiguation introduces carefully designed counterfactual examples to\nmaintain semantic boundaries; and Knowledge Specialization embeds specialized\ninformation through chain-of-thought reasoning. Experimental results across\nmultiple domains confirm SDFT's effectiveness in balancing specialized\nknowledge acquisition with general capability retention. Our key contributions\ninclude a data-centric dialogue template that balances foundational alignment\nwith targeted knowledge integration, a weighted multi-turn supervision\nframework, and comprehensive evaluation across diverse knowledge types.", "AI": {"tldr": "Introducing Structured Dialogue Fine-Tuning (SDFT) to integrate domain-specific knowledge in Vision Language Models while minimizing catastrophic forgetting.", "motivation": "To improve the incorporation of specialized knowledge domains in Vision Language Models without sacrificing foundational visual-linguistic abilities.", "method": "The method involves a three-phase dialogue structure: Foundation Preservation, Contrastive Disambiguation, and Knowledge Specialization.", "result": "Experimental results demonstrate SDFT's effectiveness in maintaining general capabilities while acquiring specialized knowledge across various domains.", "conclusion": "SDFT successfully balances foundational alignment with targeted knowledge integration, evidenced through extensive evaluations.", "key_contributions": ["Data-centric dialogue template for knowledge integration", "Weighted multi-turn supervision framework", "Extensive evaluation across diverse knowledge types"], "limitations": "", "keywords": ["Vision Language Models", "Structured Dialogue Fine-Tuning", "knowledge integration", "catastrophic forgetting", "multimodal pre-training"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00030", "pdf": "https://arxiv.org/pdf/2505.00030.pdf", "abs": "https://arxiv.org/abs/2505.00030", "title": "Can Language Models Represent the Past without Anachronism?", "authors": ["Ted Underwood", "Laura K. Nelson", "Matthew Wilkens"], "categories": ["cs.CL"], "comment": null, "summary": "Before researchers can use language models to simulate the past, they need to\nunderstand the risk of anachronism. We find that prompting a contemporary model\nwith examples of period prose does not produce output consistent with period\nstyle. Fine-tuning produces results that are stylistically convincing enough to\nfool an automated judge, but human evaluators can still distinguish fine-tuned\nmodel outputs from authentic historical text. We tentatively conclude that\npretraining on period prose may be required in order to reliably simulate\nhistorical perspectives for social research.", "AI": {"tldr": "This paper examines the challenges of using language models to simulate past styles, finding that while fine-tuning helps, human evaluators can still tell the difference between model outputs and authentic historical texts.", "motivation": "The study aims to address the risk of anachronism in language model outputs when simulating historical perspectives for social research.", "method": "The paper explores the effectiveness of prompting a contemporary language model with period prose examples and compares outputs from fine-tuned models with genuine historical texts.", "result": "Fine-tuning language models can create outputs that fools automated judges, but human evaluators can distinguish between model-generated text and authentic historical prose.", "conclusion": "Pretraining on period prose is likely necessary for language models to accurately simulate historical perspectives to be useful in social research.", "key_contributions": ["Identified limitations of current language models in simulating historical prose.", "Demonstrated the effectiveness of fine-tuning for improved stylistic alignment.", "Proposed the necessity of pretraining on historical texts for better simulation accuracy."], "limitations": "Human evaluators can still detect differences between model outputs and authentic texts despite fine-tuning efforts.", "keywords": ["language models", "historical simulation", "fine-tuning", "natural language processing", "social research"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.00031", "pdf": "https://arxiv.org/pdf/2505.00031.pdf", "abs": "https://arxiv.org/abs/2505.00031", "title": "Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving", "authors": ["Jin Zhang", "Flood Sung", "Zhilin Yang", "Yang Gao", "Chongjie Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of large language model (LLM) post-training, the effectiveness\nof utilizing synthetic data generated by the LLM itself has been\nwell-presented. However, a key question remains unaddressed: what essential\ninformation should such self-generated data encapsulate? Existing approaches\nonly produce step-by-step problem solutions, and fail to capture the abstract\nmeta-knowledge necessary for generalization across similar problems. Drawing\ninsights from cognitive science, where humans employ high-level abstraction to\nsimplify complex problems before delving into specifics, we introduce a novel\nself-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains\nthe LLM to formulate anticipatory plans, which serve as abstract meta-knowledge\nfor problem-solving, before engaging with the intricacies of problems. This\napproach not only outlines the solution generation path but also shields the\nLLM from the distraction of irrelevant details. During data generation, LEPA\nfirst crafts an anticipatory plan based on the problem, and then generates a\nsolution that aligns with both the plan and the problem. LEPA refines the plan\nthrough self-reflection, aiming to acquire plans that are instrumental in\nyielding correct solutions. During model optimization, the LLM is trained to\npredict both the refined plans and the corresponding solutions. By efficiently\nextracting and utilizing the anticipatory plans, LEPA demonstrates remarkable\nsuperiority over conventional algorithms on various challenging natural\nlanguage reasoning benchmarks.", "AI": {"tldr": "Introduction of a new self-training algorithm called LEarning to Plan before Answering (LEPA) for large language models (LLMs) to enhance problem-solving through anticipatory plans.", "motivation": "To address the lack of essential information in synthetic data generated by LLMs and improve generalization across similar problems by incorporating abstract meta-knowledge.", "method": "LEPA trains LLMs to create anticipatory plans before solving problems, thus refining the generation of solutions during a self-reflection process.", "result": "LEPA outperforms conventional algorithms on various challenging natural language reasoning benchmarks by utilizing anticipatory plans.", "conclusion": "Leveraging anticipatory plans in problem-solving significantly enhances the effectiveness of LLMs in generating correct solutions and understanding complex tasks.", "key_contributions": ["Introduction of anticipatory planning in LLM training", "Improved performance on natural language reasoning tasks", "Refinement process for solution generation and planning"], "limitations": "", "keywords": ["large language models", "self-training algorithm", "reasoning benchmarks"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.00032", "pdf": "https://arxiv.org/pdf/2505.00032.pdf", "abs": "https://arxiv.org/abs/2505.00032", "title": "MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis", "authors": ["Yuyang Sha", "Hongxin Pan", "Wei Xu", "Weiyu Meng", "Gang Luo", "Xinyu Du", "Xiaobing Zhai", "Henry H. Y. Tong", "Caijuan Shi", "Kefeng Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Major depressive disorder (MDD) impacts more than 300 million people\nworldwide, highlighting a significant public health issue. However, the uneven\ndistribution of medical resources and the complexity of diagnostic methods have\nresulted in inadequate attention to this disorder in numerous countries and\nregions. This paper introduces a high-performance MDD diagnosis tool named\nMDD-LLM, an AI-driven framework that utilizes fine-tuned large language models\n(LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis.\nTherefore, we select 274,348 individual information from the UK Biobank cohort\nto train and evaluate the proposed method. Specifically, we select 274,348\nindividual records from the UK Biobank cohort and design a tabular data\ntransformation method to create a large corpus for training and evaluating the\nproposed approach. To illustrate the advantages of MDD-LLM, we perform\ncomprehensive experiments and provide several comparative analyses against\nexisting model-based solutions across multiple evaluation metrics. Experimental\nresults show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of\n0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine\nlearning and deep learning frameworks for MDD diagnosis. Given the limited\nexploration of LLMs in MDD diagnosis, we examine numerous factors that may\ninfluence the performance of our proposed method, such as tabular data\ntransformation techniques and different fine-tuning strategies.", "AI": {"tldr": "This paper presents MDD-LLM, an AI-driven tool for diagnosing major depressive disorder (MDD) using fine-tuned large language models and real-world data.", "motivation": "To address the inadequate attention given to major depressive disorder (MDD) diagnosis worldwide due to uneven distribution of resources and complexity in diagnostic methods.", "method": "The authors utilized a dataset of 274,348 records from the UK Biobank and developed a tabular data transformation method for training and evaluation of the MDD-LLM framework.", "result": "MDD-LLM achieved an accuracy of 0.8378 and an AUC of 0.8919, significantly outperforming existing machine learning and deep learning approaches for MDD diagnosis.", "conclusion": "The study showcases the potential of fine-tuned LLMs in improving MDD diagnostics and highlights various influencing factors on the method's performance.", "key_contributions": ["Introduction of MDD-LLM, an AI-driven diagnostic tool for MDD", "Utilization of a large dataset from the UK Biobank", "Demonstration of superior performance over existing methods."], "limitations": "Limited exploration of LLMs in MDD diagnosis beyond this specific framework and dataset.", "keywords": ["Major Depressive Disorder", "Large Language Models", "AI Diagnosis", "Health Informatics", "Machine Learning"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2505.00033", "pdf": "https://arxiv.org/pdf/2505.00033.pdf", "abs": "https://arxiv.org/abs/2505.00033", "title": "From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models", "authors": ["Andrew Kiruluta"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel spectral generative modeling framework for natural\nlanguage processing that jointly learns a global time varying Fourier\ndictionary and per token mixing coefficients, replacing the ubiquitous self\nattention mechanism in transformer architectures. By enforcing reconstruction\nlosses in both the time domain (embedding reconstruction) and the frequency\ndomain (via Short Time Fourier Transform magnitude matching) alongside a\nstandard language modeling objective, and fitting a Gaussian Mixture Model\n(GMM) prior over the learned mixing vectors, our approach achieves competitive\nperplexity and generation quality on standard benchmarks such as WikiText2 and\nPenn Treebank. In contrast to the quadratic computation complexity of self\nattention, our method operates with linear complexity, delivering substantial\nefficiency gains. We demonstrate that spectral dictionary models can achieve\ncompetitive performance compared to transformer baselines while significantly\nreducing inference latency and memory footprint, offering a compelling\nalternative for scalable language modeling.", "AI": {"tldr": "A novel spectral generative modeling framework for NLP replaces self-attention with linear complexity models, achieving competitive results with reduced resource requirements.", "motivation": "To improve efficiency and reduce computational costs associated with self-attention mechanisms in transformer architectures for NLP tasks.", "method": "The framework learns a global time varying Fourier dictionary and per token mixing coefficients, using reconstruction losses in both time and frequency domains, alongside a Gaussian Mixture Model prior.", "result": "Competitive perplexity and generation quality on datasets like WikiText2 and Penn Treebank, with a significant reduction in inference latency and memory footprint.", "conclusion": "The proposed spectral dictionary model offers a scalable alternative to traditional transformer models while maintaining performance.", "key_contributions": ["Introduces a linear complexity model for NLP tasks that replaces self-attention.", "Combines time and frequency domain reconstruction losses alongside standard language modeling objectives.", "Demonstrates substantial efficiency gains in inference and memory usage compared to transformers."], "limitations": "", "keywords": ["spectral generative modeling", "natural language processing", "Fourier dictionary", "Gaussian Mixture Model", "language modeling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00034", "pdf": "https://arxiv.org/pdf/2505.00034.pdf", "abs": "https://arxiv.org/abs/2505.00034", "title": "Improving Phishing Email Detection Performance of Small Large Language Models", "authors": ["Zijie Lin", "Zikang Liu", "Hanbo Fan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models(LLMs) have demonstrated remarkable performance on many\nnatural language processing(NLP) tasks and have been employed in phishing email\ndetection research. However, in current studies, well-performing LLMs typically\ncontain billions or even tens of billions of parameters, requiring enormous\ncomputational resources. To reduce computational costs, we investigated the\neffectiveness of small-parameter LLMs for phishing email detection. These LLMs\nhave around 3 billion parameters and can run on consumer-grade GPUs. However,\nsmall LLMs often perform poorly in phishing email detection task. To address\nthese issues, we designed a set of methods including Prompt Engineering,\nExplanation Augmented Fine-tuning, and Model Ensemble to improve phishing email\ndetection capabilities of small LLMs. We validated the effectiveness of our\napproach through experiments, significantly improving accuracy on the\nSpamAssassin dataset from around 0.5 for baseline models like\nQwen2.5-1.5B-Instruct to 0.976.", "AI": {"tldr": "This paper explores methods to enhance phishing email detection using small-parameter LLMs, achieving significant accuracy improvements.", "motivation": "To investigate the feasibility of small-parameter LLMs in phishing email detection, reducing computational costs while maintaining performance.", "method": "The paper uses techniques such as Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve detection capabilities of small LLMs.", "result": "The proposed methods improved detection accuracy on the SpamAssassin dataset from approximately 0.5 for baseline models to 0.976.", "conclusion": "Small-parameter LLMs can effectively detect phishing emails with appropriate enhancement methods, making them viable for deployment on consumer-grade hardware.", "key_contributions": ["Introduction of optimization methods for small LLMs in phishing detection", "Demonstration of significant accuracy improvement over baseline models", "Validation of methods on the SpamAssassin dataset"], "limitations": "", "keywords": ["phishing email detection", "small LLMs", "machine learning", "natural language processing", "prompt engineering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00035", "pdf": "https://arxiv.org/pdf/2505.00035.pdf", "abs": "https://arxiv.org/abs/2505.00035", "title": "Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics", "authors": ["Aayam Bansal", "Raghav Agarwal", "Kaashvi Jain"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This paper presents a comprehensive computational framework for analyzing\nlinguistic complexity and socio-cultural trends in hip-hop lyrics. Using a\ndataset of 3,814 songs from 146 influential artists spanning four decades\n(1980-2020), we employ natural language processing techniques to quantify\nmultiple dimensions of lyrical complexity. Our analysis reveals a 23.7%\nincrease in vocabulary diversity over the study period, with East Coast artists\ndemonstrating 17.3% higher lexical variation than other regions. Rhyme density\nincreased by 34.2% across all regions, with Midwest artists exhibiting the\nhighest technical complexity (3.04 rhymes per line). Topic modeling identified\nsignificant shifts in thematic content, with social justice themes decreasing\nfrom 28.5% to 13.8% of content while introspective themes increased from 7.6%\nto 26.3%. Sentiment analysis demon- strated that lyrics became significantly\nmore negative during sociopolitical crises, with polarity decreasing by 0.31\nfollowing major social unrest. Multi-dimensional analysis revealed four dis-\ntinct stylistic approaches that correlate strongly with geographic origin\n(r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish\nquantitative evidence for the evolution of hip- hop as both an art form and a\nreflection of societal dynamics, providing insights into the interplay between\nlinguistic innovation and cultural context in popular music.", "AI": {"tldr": "This paper analyzes linguistic complexity and socio-cultural trends in hip-hop lyrics using NLP techniques on a dataset of 3,814 songs over 40 years, revealing significant changes in vocabulary diversity, rhyme density, themes, and sentiment during sociopolitical events.", "motivation": "To explore the relationship between linguistic innovation in hip-hop lyrics and the socio-cultural context over four decades.", "method": "Natural language processing techniques were applied to a dataset of hip-hop lyrics from 3,814 songs across 146 artists, quantifying dimensions of lyrical complexity and analyzing thematic shifts through topic modeling and sentiment analysis.", "result": "A 23.7% increase in vocabulary diversity, 34.2% increase in rhyme density, a notable decrease in social justice themes, and a correlation between stylistic approaches and geographic origin/time period.", "conclusion": "The findings provide quantitative evidence for the evolution of hip-hop as an art form reflecting societal dynamics, showing how linguistic features in lyrics interact with cultural context.", "key_contributions": ["Quantitative analysis of linguistic complexity in hip-hop lyrics", "Identification of thematic shifts over four decades", "Exploration of the relationship between lyrical features and socio-cultural trends"], "limitations": "", "keywords": ["hip-hop", "natural language processing", "linguistic complexity", "cultural trends", "sentiment analysis"], "importance_score": 2, "read_time_minutes": 12}}
{"id": "2505.00036", "pdf": "https://arxiv.org/pdf/2505.00036.pdf", "abs": "https://arxiv.org/abs/2505.00036", "title": "A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies", "authors": ["Zhongren Chen", "Joshua Kalla", "Quan Le", "Shinpei Nakamura-Sakai", "Jasjeet Sekhon", "Ruixiao Wang"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "In recent years, significant concern has emerged regarding the potential\nthreat that Large Language Models (LLMs) pose to democratic societies through\ntheir persuasive capabilities. We expand upon existing research by conducting\ntwo survey experiments and a real-world simulation exercise to determine\nwhether it is more cost effective to persuade a large number of voters using\nLLM chatbots compared to standard political campaign practice, taking into\naccount both the \"receive\" and \"accept\" steps in the persuasion process (Zaller\n1992). These experiments improve upon previous work by assessing extended\ninteractions between humans and LLMs (instead of using single-shot\ninteractions) and by assessing both short- and long-run persuasive effects\n(rather than simply asking users to rate the persuasiveness of LLM-produced\ncontent). In two survey experiments (N = 10,417) across three distinct\npolitical domains, we find that while LLMs are about as persuasive as actual\ncampaign ads once voters are exposed to them, political persuasion in the\nreal-world depends on both exposure to a persuasive message and its impact\nconditional on exposure. Through simulations based on real-world parameters, we\nestimate that LLM-based persuasion costs between \\$48-\\$74 per persuaded voter\ncompared to \\$100 for traditional campaign methods, when accounting for the\ncosts of exposure. However, it is currently much easier to scale traditional\ncampaign persuasion methods than LLM-based persuasion. While LLMs do not\ncurrently appear to have substantially greater potential for large-scale\npolitical persuasion than existing non-LLM methods, this may change as LLM\ncapabilities continue to improve and it becomes easier to scalably encourage\nexposure to persuasive LLMs.", "AI": {"tldr": "The paper investigates the effectiveness and cost-efficiency of using Large Language Models (LLMs) for political persuasion compared to traditional campaign methods through survey experiments and simulations.", "motivation": "To explore the potential threat of LLMs to democracy and their effectiveness in political persuasion.", "method": "Conducted two survey experiments with 10,417 participants and a real-world simulation exercise to compare LLM-based persuasion with traditional political campaigns, considering both exposure and acceptance of messages.", "result": "Findings indicate LLMs are similarly persuasive as campaign ads but raising concerns about exposure and scaling challenges for LLMs.", "conclusion": "LLMs show potential for political persuasion at a lower cost per persuaded voter, but traditional methods are currently easier to scale; future improvements in LLMs may alter this balance.", "key_contributions": ["Provided empirical data on LLMs' persuasive effectiveness compared to traditional methods", "Developed a framework for assessing extended human-LLM interactions", "Estimated costs of LLM-based persuasion versus traditional campaigns"], "limitations": "The study mainly focuses on cost-effectiveness and does not fully address long-term impacts on democratic processes or ethical considerations of using LLMs in political contexts.", "keywords": ["Large Language Models", "Political Persuasion", "Survey Experiments", "Cost-Effectiveness", "Democracy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00038", "pdf": "https://arxiv.org/pdf/2505.00038.pdf", "abs": "https://arxiv.org/abs/2505.00038", "title": "HyPerAlign: Hypotheses-driven Personalized Alignment", "authors": ["Cristina Garbacea", "Chenhao Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment algorithms are widely used to align large language models (LLMs) to\nhuman users based on preference annotations that reflect their intended\nreal-world use cases. Typically these (often divergent) preferences are\naggregated over a diverse set of users, resulting in fine-tuned models that are\naligned to the ``average-user'' preference. Nevertheless, current models are\nused by individual users in very specific contexts and situations, emphasizing\nthe need for user-dependent preference control. In this work we address the\nproblem of personalizing LLM outputs to their users, aiming to generate\ncustomized responses tailored to individual users, instead of generic outputs\nthat emulate the collective voices of diverse populations. We propose a novel\ninterpretable and sample-efficient hypotheses-driven personalization approach\n(HyPerAlign) where given few-shot examples written by a particular user, we\nfirst infer hypotheses about their communication strategies, personality and\nwriting style, then prompt LLM models with these hypotheses and user specific\nattributes to generate customized outputs. We conduct experiments on two\ndifferent personalization tasks, authorship attribution and deliberative\nalignment, with datasets from diverse domains (news articles, blog posts,\nemails, jailbreaking benchmarks), and demonstrate the superiority of\nhypotheses-driven personalization approach when compared to preference-based\nfine-tuning methods. For deliberative alignment, the helpfulness of LLM models\nis improved by up to $70\\%$ on average. For authorship attribution, results\nindicate consistently high win-rates (commonly $>90\\%$) against\nstate-of-the-art preference fine-tuning approaches for LLM personalization\nacross diverse user profiles and LLM models. Overall, our approach represents\nan interpretable and sample-efficient strategy for the personalization of LLM\nmodels to individual users.", "AI": {"tldr": "The paper presents HyPerAlign, a novel approach for personalizing large language model (LLM) outputs to individual users based on their unique communication strategies and styles.", "motivation": "Current LLMs typically align to average user preferences but often miss the specifics of individual user contexts, highlighting the need for personalized outputs.", "method": "The HyPerAlign approach infers user-specific hypotheses from few-shot examples and uses these to prompt LLMs for customized outputs.", "result": "Experiments show that HyPerAlign significantly outperforms traditional preference-based fine-tuning, leading to improvements in helpfulness by up to 70% and high win rates in authorship attribution tasks.", "conclusion": "HyPerAlign offers a more interpretable and efficient means of personalizing LLMs to enhance user experience and satisfaction.", "key_contributions": ["Introduction of a hypotheses-driven personalization approach for LLMs", "Demonstrated significant performance improvements over traditional fine-tuning methods", "Interpretable model behavior through user-specific hypothesis inference"], "limitations": "", "keywords": ["personalization", "large language models", "hypotheses-driven", "user-specific", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039.pdf", "abs": "https://arxiv.org/abs/2505.00039", "title": "Graph RAG for Legal Norms: A Hierarchical and Temporal Approach", "authors": ["Hudson de Martim"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nsignificantly advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective systems in legal research,\nlegislative analysis, and decision support.", "AI": {"tldr": "This paper presents an adaptation of Graph Retrieval Augmented Generation for analyzing legal norms using structured knowledge graphs to manage complexity in legal data.", "motivation": "To address the complexity and large volume of legal data by leveraging hierarchical structures and knowledge graphs.", "method": "The paper combines structured knowledge graphs with enriched text segments and introduces comprehensive Text Units to improve the analysis of legal norms.", "result": "Graph RAG enhances the representation of legal knowledge, facilitating more effective systems in legal research and analysis.", "conclusion": "The proposed approach significantly advances AI applications in law and helps create better decision support systems.", "key_contributions": ["Introduction of Graph RAG for legal norm analysis", "Integration of hierarchical structures in knowledge graphs", "Development of comprehensive Text Units for richer legal knowledge representation"], "limitations": "", "keywords": ["Graph Retrieval Augmented Generation", "legal norms", "knowledge graphs", "AI in law", "temporal evolution"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.00047", "pdf": "https://arxiv.org/pdf/2505.00047.pdf", "abs": "https://arxiv.org/abs/2505.00047", "title": "Base Models Beat Aligned Models at Randomness and Creativity", "authors": ["Peter West", "Christopher Potts"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment has quickly become a default ingredient in LLM development, with\ntechniques such as reinforcement learning from human feedback making models act\nsafely, follow instructions, and perform ever-better on complex tasks. While\nthese techniques are certainly useful, we propose that they should not be\nuniversally applied and demonstrate a range of tasks on which base language\nmodels consistently outperform their popular aligned forms. Particularly, we\nstudy tasks that require unpredictable outputs, such as random number\ngeneration, mixed strategy games (rock-paper-scissors and hide-and-seek), and\ncreative writing. In each case, aligned models tend towards narrow behaviors\nthat result in distinct disadvantages, for instance, preferring to generate \"7\"\nover other uniformly random numbers, becoming almost fully predictable in some\ngame states, or prioritizing pleasant writing over creative originality. Across\nmodels tested, better performance on common benchmarks tends to correlate with\nworse performance on our tasks, suggesting an effective trade-off in the\nrequired capabilities.", "AI": {"tldr": "This paper critiques the common reliance on alignment in LLM development, showing that unaligned base models can outperform aligned ones on tasks requiring unpredictability and creativity.", "motivation": "The paper aims to highlight the limitations of aligned language models by demonstrating cases where unaligned models perform better in specific tasks that require unpredictability and creativity.", "method": "The authors conducted experiments comparing aligned and unaligned models on various tasks such as random number generation, mixed strategy games, and creative writing to assess their performance differences.", "result": "Unaligned models consistently outperformed aligned models on tasks requiring unpredictable outputs, indicating that alignment may lead to narrow behaviors and predictability.", "conclusion": "While alignment techniques enhance model safety and instruction-following, they can hinder performance in creativity and unpredictability, suggesting a need for careful application of these techniques.", "key_contributions": ["Demonstration of tasks where unaligned models show superior performance", "Analysis of the predictability trade-offs in aligned models", "Insights into the limitations of reinforcement learning from human feedback in creative tasks"], "limitations": "The study focuses on specific tasks and may not generalize across all LLM applications; further research is needed to explore other contexts.", "keywords": ["alignments", "language models", "creativity", "unpredictability", "human feedback"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00050", "pdf": "https://arxiv.org/pdf/2505.00050.pdf", "abs": "https://arxiv.org/abs/2505.00050", "title": "Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting", "authors": ["Aayam Bansal", "Agneya Tharun"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "This study explores the intersection of fashion trends and social media\nsentiment through computational analysis of Twitter data using the T4SA\n(Twitter for Sentiment Analysis) dataset. By applying natural language\nprocessing and machine learning techniques, we examine how sentiment patterns\nin fashion-related social media conversations can serve as predictors for\nemerging fashion trends. Our analysis involves the identification and\ncategorization of fashion-related content, sentiment classification with\nimproved normalization techniques, time series decomposition, statistically\nvalidated causal relationship modeling, cross-platform sentiment comparison,\nand brand-specific sentiment analysis. Results indicate correlations between\nsentiment patterns and fashion theme popularity, with accessories and\nstreetwear themes showing statistically significant rising trends. The Granger\ncausality analysis establishes sustainability and streetwear as primary trend\ndrivers, showing bidirectional relationships with several other themes. The\nfindings demonstrate that social media sentiment analysis can serve as an\neffective early indicator of fashion trend trajectories when proper statistical\nvalidation is applied. Our improved predictive model achieved 78.35% balanced\naccuracy in sentiment classification, establishing a reliable foundation for\ntrend prediction across positive, neutral, and negative sentiment categories.", "AI": {"tldr": "The paper analyzes Twitter sentiment to predict fashion trends using NLP and ML techniques.", "motivation": "To explore the predictive relationship between social media sentiment and emerging fashion trends.", "method": "Utilizes natural language processing and machine learning for sentiment classification and analysis of fashion-related Twitter data, including time series decomposition and causal relationship modeling.", "result": "Identified correlations between sentiment patterns and the popularity of fashion themes, with established causal relationships showing sustainability and streetwear as primary trend drivers.", "conclusion": "Social media sentiment analysis can effectively predict fashion trend trajectories with proper statistical validation, achieving 78.35% balanced accuracy in sentiment classification.", "key_contributions": ["Application of advanced NLP techniques for fashion sentiment analysis.", "Establishment of statistically validated causal relationships between sentiment patterns and fashion trends.", "Demonstration of social media sentiment as a predictor for emerging trends."], "limitations": "", "keywords": ["fashion trends", "social media", "sentiment analysis", "natural language processing", "machine learning"], "importance_score": 4, "read_time_minutes": 13}}
{"id": "2505.00056", "pdf": "https://arxiv.org/pdf/2505.00056.pdf", "abs": "https://arxiv.org/abs/2505.00056", "title": "Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity", "authors": ["Tygo Bloem", "Filip Ilievski"], "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "Meme clustering is critical for toxicity detection, virality modeling, and\ntyping, but it has received little attention in previous research. Clustering\nsimilar Internet memes is challenging due to their multimodality, cultural\ncontext, and adaptability. Existing approaches rely on databases, overlook\nsemantics, and struggle to handle diverse dimensions of similarity. This paper\nintroduces a novel method that uses template-based matching with\nmulti-dimensional similarity features, thus eliminating the need for predefined\ndatabases and supporting adaptive matching. Memes are clustered using local and\nglobal features across similarity categories such as form, visual content,\ntext, and identity. Our combined approach outperforms existing clustering\nmethods, producing more consistent and coherent clusters, while\nsimilarity-based feature sets enable adaptability and align with human\nintuition. We make all supporting code publicly available to support subsequent\nresearch. Code: https://github.com/tygobl/meme-clustering", "AI": {"tldr": "This paper presents a new method for clustering Internet memes using template-based matching and multi-dimensional similarity features to enhance toxicity detection and virality modeling.", "motivation": "Meme clustering is essential for toxicity detection, virality modeling, and typing, yet it has been largely overlooked in research.", "method": "A novel approach using template-based matching and multi-dimensional similarity features for clustering memes without relying on predefined databases.", "result": "The combined method shows superior performance compared to existing techniques, yielding more consistent and coherent meme clusters.", "conclusion": "The proposed method supports adaptive matching and aligns with human intuition, advancing meme clustering research.", "key_contributions": ["Introduction of template-based matching for meme clustering", "Use of multi-dimensional similarity features for adaptive matching", "Public availability of supporting code for further research"], "limitations": "", "keywords": ["Meme clustering", "Toxicity detection", "Virality modeling", "Multi-dimensional similarity", "Template-based matching"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.00057", "pdf": "https://arxiv.org/pdf/2505.00057.pdf", "abs": "https://arxiv.org/abs/2505.00057", "title": "A Report on the llms evaluating the high school questions", "authors": ["Zhu Jiawei", "Chen Wei"], "categories": ["cs.CL"], "comment": null, "summary": "This report aims to evaluate the performance of large language models (LLMs)\nin solving high school science questions and to explore their potential\napplications in the educational field. With the rapid development of LLMs in\nthe field of natural language processing, their application in education has\nattracted widespread attention. This study selected mathematics exam questions\nfrom the college entrance examinations (2019-2023) as evaluation data and\nutilized at least eight LLM APIs to provide answers. A comprehensive assessment\nwas conducted based on metrics such as accuracy, response time, logical\nreasoning, and creativity. Through an in-depth analysis of the evaluation\nresults, this report reveals the strengths and weaknesses of LLMs in handling\nhigh school science questions and discusses their implications for educational\npractice. The findings indicate that although LLMs perform excellently in\ncertain aspects, there is still room for improvement in logical reasoning and\ncreative problem-solving. This report provides an empirical foundation for\nfurther research and application of LLMs in the educational field and offers\nsuggestions for improvement.", "AI": {"tldr": "Evaluation of large language models (LLMs) in solving high school science questions with insights on their educational applications.", "motivation": "To explore the performance and potential applications of LLMs in education, specifically in solving high school science questions.", "method": "Used mathematics exam questions from college entrance examinations (2019-2023) and evaluated at least eight LLM APIs based on accuracy, response time, logical reasoning, and creativity.", "result": "LLMs performed well in certain aspects but showed limitations in logical reasoning and creative problem-solving.", "conclusion": "LLMs have strengths but need improvements in reasoning and creativity; offers a foundation for future research and suggestions for enhancement.", "key_contributions": ["Comprehensive assessment of LLMs in educational contexts", "Identification of strengths and weaknesses in LLM performance", "Suggestions for enhancing LLM applications in education"], "limitations": "Limited to high school science questions and reliant on specific exam formats.", "keywords": ["large language models", "education", "science questions", "performance evaluation", "logical reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.00059", "pdf": "https://arxiv.org/pdf/2505.00059.pdf", "abs": "https://arxiv.org/abs/2505.00059", "title": "BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition", "authors": ["Paige Tuttösí", "Mantaj Dhillon", "Luna Sang", "Shane Eastwood", "Poorvi Bhatia", "Quang Minh Dinh", "Avni Kapoor", "Yewon Jin", "Angelica Lim"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Computer Speech and Language, Special issue:\n  Multi-Speaker, Multi-Microphone, and Multi-Modal Distant Speech Recognition\n  (September 2025)", "summary": "Some speech recognition tasks, such as automatic speech recognition (ASR),\nare approaching or have reached human performance in many reported metrics.\nYet, they continue to struggle in complex, real-world, situations, such as with\ndistanced speech. Previous challenges have released datasets to address the\nissue of distanced ASR, however, the focus remains primarily on distance,\nspecifically relying on multi-microphone array systems. Here we present the\nB(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset\ncontains almost 4 hours of English speech from 98 actors with varying regional\nand non-native accents. The data was collected on smartphones in the actors\nhomes and therefore includes at least 98 different acoustic environments. The\ndata also includes 7 different emotion prompts and both shouted and spoken\nutterances. The smartphones were places in 19 different positions, including\nobstructions and being in a different room than the actor. This data is\npublicly available for use and can be used to evaluate a variety of speech\nrecognition tasks, including: ASR, shout detection, and speech emotion\nrecognition (SER). We provide initial benchmarks for ASR and SER tasks, and\nfind that ASR degrades both with an increase in distance and shout level and\nshows varied performance depending on the intended emotion. Our results show\nthat the BERSt dataset is challenging for both ASR and SER tasks and continued\nwork is needed to improve the robustness of such systems for more accurate\nreal-world use.", "AI": {"tldr": "The BERSt dataset comprises 4 hours of English speech collected from 98 actors in various home environments for evaluating automatic speech recognition (ASR) and speech emotion recognition (SER).", "motivation": "Despite advancements in automatic speech recognition (ASR), challenges remain in real-world situations, particularly with distanced speech, necessitating diverse datasets for evaluation.", "method": "The data was collected on smartphones from actors located in different acoustic environments, involving various accents, emotions, and speaker distances.", "result": "Initial benchmarks indicate that ASR performance deteriorates with increased distance and shout level, revealing variable effectiveness based on emotion.", "conclusion": "The BERSt dataset presents significant challenges for ASR and SER, highlighting the need for further research to enhance system robustness in real-world applications.", "key_contributions": ["Introduction of the BERSt dataset for distanced ASR evaluation.", "Includes diverse emotional and environmental factors in speech data.", "Provides initial benchmarks for ASR and SER tasks."], "limitations": "Data collected under specific conditions; results may vary outside of these parameters.", "keywords": ["Automatic Speech Recognition", "Speech Emotion Recognition", "Distanced Speech", "Dataset", "Acoustic Environments"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.00060", "pdf": "https://arxiv.org/pdf/2505.00060.pdf", "abs": "https://arxiv.org/abs/2505.00060", "title": "Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5", "authors": ["Jeho Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) have shown promise in enabling natural language\ninterfaces for structured data querying through text-to-SQL generation.\nHowever, their application in real-world Business Intelligence (BI) contexts\nremains limited due to semantic hallucinations, structural errors, and a lack\nof domain-specific evaluation frameworks. In this study, we propose a\nFact-Consistency Evaluation Framework for assessing the semantic accuracy of\nLLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM\noptimized for enterprise tasks. We construct a domain-specific benchmark\ncomprising 219 natural language business questions across five SQL complexity\nlevels, derived from actual sales data in LG Electronics' internal BigQuery\nenvironment. Each question is paired with a gold-standard SQL query and a\nvalidated ground-truth answer. We evaluate model performance using answer\naccuracy, execution success rate, semantic error rate, and non-response rate.\nExperimental results show that while Exaone 3.5 performs well on simple\naggregation tasks (93% accuracy in L1), it exhibits substantial degradation in\narithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4),\nwith semantic errors and non-responses concentrated in complex cases.\nQualitative error analysis further identifies common failure types such as\nmisapplied arithmetic logic, incomplete filtering, and incorrect grouping\noperations. Our findings highlight the current limitations of LLMs in\nbusiness-critical environments and underscore the need for fact-consistency\nvalidation layers and hybrid reasoning approaches. This work contributes a\nreproducible benchmark and evaluation methodology for advancing reliable\nnatural language interfaces to structured enterprise data systems.", "AI": {"tldr": "This paper proposes a framework to evaluate the semantic accuracy of SQL outputs generated by LLMs, addressing their limitations in Business Intelligence contexts.", "motivation": "Despite the promise of LLMs in natural language interfaces for structured data querying, their real-world application is hindered by issues like semantic hallucinations and structural errors.", "method": "The study constructs a Fact-Consistency Evaluation Framework and a benchmark of 219 natural language questions tied to SQL queries, evaluating performance based on accuracy and error rates.", "result": "Exaone 3.5 shows high accuracy on simple tasks (93% in L1) but struggles with complex queries, revealing significant performance issues in arithmetic reasoning and grouped ranking tasks.", "conclusion": "The findings highlight the limitations of LLMs in business contexts and the need for improved validation and reasoning approaches, contributing a benchmark for future research.", "key_contributions": ["Development of a Fact-Consistency Evaluation Framework for LLM-generated SQL outputs.", "Creation of a domain-specific benchmark with 219 natural language questions.", "Identification of common failure types in LLM performances."], "limitations": "The study focuses on a single bilingual LLM and its performance on specific SQL complexities in a limited domain.", "keywords": ["Large Language Models", "Natural Language Processing", "SQL Generation", "Business Intelligence", "Evaluation Framework"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2505.00061", "pdf": "https://arxiv.org/pdf/2505.00061.pdf", "abs": "https://arxiv.org/abs/2505.00061", "title": "Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems", "authors": ["Sahar Yarmohammadtoosky", "Yiyun Zhou", "Victoria Yaneva", "Peter Baldwin", "Saed Rezayi", "Brian Clauser", "Polina Harikeo"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "This study examines vulnerabilities in transformer-based automated\nshort-answer grading systems used in medical education, with a focus on how\nthese systems can be manipulated through adversarial gaming strategies. Our\nresearch identifies three main types of gaming strategies that exploit the\nsystem's weaknesses, potentially leading to false positives. To counteract\nthese vulnerabilities, we implement several adversarial training methods\ndesigned to enhance the systems' robustness. Our results indicate that these\nmethods significantly reduce the susceptibility of grading systems to such\nmanipulations, especially when combined with ensemble techniques like majority\nvoting and ridge regression, which further improve the system's defense against\nsophisticated adversarial inputs. Additionally, employing large language models\nsuch as GPT-4 with varied prompting techniques has shown promise in recognizing\nand scoring gaming strategies effectively. The findings underscore the\nimportance of continuous improvements in AI-driven educational tools to ensure\ntheir reliability and fairness in high-stakes settings.", "AI": {"tldr": "The study explores vulnerabilities in transformer-based automated short-answer grading systems in medical education and proposes adversarial training methods to enhance robustness against manipulative strategies.", "motivation": "To identify and address vulnerabilities in AI grading systems that can be exploited through adversarial techniques, ensuring the fairness and reliability of educational assessments.", "method": "The study employs adversarial training methods alongside ensemble techniques like majority voting and ridge regression to fortify grading systems against manipulative inputs.", "result": "The implementation of these methods significantly reduced the grading systems' susceptibility to adversarial strategies, particularly when combined with ensemble techniques and large language models.", "conclusion": "Continuous advancements in AI educational tools are crucial for maintaining their reliability and fairness, especially in high-stakes environments.", "key_contributions": ["Identification of vulnerabilities in automated grading systems through adversarial gaming strategies.", "Implementation of effective adversarial training methods for improved system robustness.", "Demonstration of the effectiveness of large language models in recognizing and scoring exploitative strategies."], "limitations": "", "keywords": ["adversarial training", "automated grading", "transformer-based systems", "AI in education", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00063", "pdf": "https://arxiv.org/pdf/2505.00063.pdf", "abs": "https://arxiv.org/abs/2505.00063", "title": "GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling", "authors": ["Siqi Li", "Yufan Shen", "Xiangnan Chen", "Jiayi Chen", "Hengwei Ju", "Haodong Duan", "Song Mao", "Hongbin Zhou", "Bo Zhang", "Pinlong Cai", "Licheng Wen", "Botian Shi", "Yong Liu", "Xinyu Cai", "Yu Qiao"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The rapid advancement of multimodal large language models (MLLMs) has\nprofoundly impacted the document domain, creating a wide array of application\nscenarios. This progress highlights the need for a comprehensive benchmark to\nevaluate these models' capabilities across various document-specific tasks.\nHowever, existing benchmarks often fail to locate specific model weaknesses or\nguide systematic improvements. To bridge this gap, we introduce a General\nDocument Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key\nscenarios and 19 document-specific tasks. By decoupling visual complexity and\nreasoning complexity, the GDI-Bench structures graded tasks that allow\nperformance assessment by difficulty, aiding in model weakness identification\nand optimization guidance. We evaluate the GDI-Bench on various open-source and\nclosed-source models, conducting decoupled analyses in the visual and reasoning\ndomains. For instance, the GPT-4o model excels in reasoning tasks but exhibits\nlimitations in visual capabilities. To address the diverse tasks and domains in\nthe GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic\nforgetting during the supervised fine-tuning (SFT) process through a\nintelligence-preserving training strategy. Our model achieves state-of-the-art\nperformance on previous benchmarks and the GDI-Bench. Both our benchmark and\nmodel will be open source.", "AI": {"tldr": "Introduction of a General Document Intelligence Benchmark (GDI-Bench) for evaluating multimodal large language models (MLLMs) across document-specific tasks.", "motivation": "To address the lack of comprehensive benchmarks that identify weaknesses and guide improvements in multimodal large language models (MLLMs) for document-specific tasks.", "method": "A General Document Intelligence Benchmark (GDI-Bench) is developed, comprising 1.9k images across 9 scenarios and 19 tasks, with a decoupled assessment of visual and reasoning complexities.", "result": "The GDI-Bench helps in assessing model performance by task difficulty and identifies specific limitations in models, such as the GPT-4o's strong reasoning but weak visual capabilities.", "conclusion": "The GDI-Bench and the GDI Model, which mitigates catastrophic forgetting during supervised fine-tuning, achieve state-of-the-art results and will be open source.", "key_contributions": ["Development of GDI-Bench with 1.9k images for evaluating MLLMs", "Decoupling visual and reasoning complexities in benchmarking", "Introduction of GDI Model with an intelligence-preserving training strategy"], "limitations": "", "keywords": ["multimodal models", "document intelligence", "benchmark", "GPT-4o", "document-specific tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00065", "pdf": "https://arxiv.org/pdf/2505.00065.pdf", "abs": "https://arxiv.org/abs/2505.00065", "title": "ConSens: Assessing context grounding in open-book question answering", "authors": ["Ivan Vankov", "Matyo Ivanov", "Adriana Correia", "Victor Botev"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated considerable success in\nopen-book question answering (QA), where the task requires generating answers\ngrounded in a provided external context. A critical challenge in open-book QA\nis to ensure that model responses are based on the provided context rather than\nits parametric knowledge, which can be outdated, incomplete, or incorrect.\nExisting evaluation methods, primarily based on the LLM-as-a-judge approach,\nface significant limitations, including biases, scalability issues, and\ndependence on costly external systems. To address these challenges, we propose\na novel metric that contrasts the perplexity of the model response under two\nconditions: when the context is provided and when it is not. The resulting\nscore quantifies the extent to which the model's answer relies on the provided\ncontext. The validity of this metric is demonstrated through a series of\nexperiments that show its effectiveness in identifying whether a given answer\nis grounded in the provided context. Unlike existing approaches, this metric is\ncomputationally efficient, interpretable, and adaptable to various use cases,\noffering a scalable and practical solution to assess context utilization in\nopen-book QA systems.", "AI": {"tldr": "Proposes a novel metric for evaluating the context utilization in open-book question answering by contrasting model responses with and without the provided context.", "motivation": "To improve evaluation methods for open-book QA, addressing issues in current evaluation techniques that rely heavily on external systems and exhibit various limitations.", "method": "The proposed metric contrasts the perplexity of model responses under two conditions: with and without provided context, allowing for quantification of the dependence on the external context.", "result": "Experiments demonstrate the effectiveness of the metric in identifying context-grounded answers, showing it to be computationally efficient and interpretable.", "conclusion": "The new metric offers a scalable solution to evaluate context utilization in open-book QA systems, overcoming limitations of existing methods.", "key_contributions": ["Introduces a novel metric for evaluating context utilization in LLM responses.", "Demonstrates effectiveness through experiments against traditional methods.", "Provides a computationally efficient and interpretable solution for scalable evaluation."], "limitations": "May need further validation across different types of QA tasks and contexts.", "keywords": ["Large Language Models", "Open-book QA", "Evaluation Metric", "Context Utilization", "Perplexity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00114", "pdf": "https://arxiv.org/pdf/2505.00114.pdf", "abs": "https://arxiv.org/abs/2505.00114", "title": "Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese", "authors": ["Silvana Yakhni", "Ali Chehab"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper examines the effectiveness of Large Language Models (LLMs) in\ntranslating the low-resource Lebanese dialect, focusing on the impact of\nculturally authentic data versus larger translated datasets. We compare three\nfine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using\nopen-source Aya23 models. Experiments reveal that models fine-tuned on a\nsmaller but culturally aware Lebanese dataset (LW) consistently outperform\nthose trained on larger, non-native data. The best results were achieved\nthrough contrastive fine-tuning paired with contrastive prompting, which\nindicates the benefits of exposing translation models to bad examples. In\naddition, to ensure authentic evaluation, we introduce LebEval, a new benchmark\nderived from native Lebanese content, and compare it to the existing FLoRes\nbenchmark. Our findings challenge the \"More Data is Better\" paradigm and\nemphasize the crucial role of cultural authenticity in dialectal translation.\nWe made our datasets and code available on Github.", "AI": {"tldr": "This paper studies the effectiveness of Large Language Models in translating the Lebanese dialect using culturally authentic data versus larger translated datasets.", "motivation": "To investigate how cultural authenticity affects the translation quality of low-resource dialects, specifically the Lebanese dialect.", "method": "The study compares three fine-tuning methods—Basic, contrastive, and grammar-hint tuning—using open-source Aya23 models on culturally aware versus larger non-native datasets.", "result": "Models fine-tuned on the smaller culturally aware Lebanese dataset outperform those trained on larger datasets. The best performance comes from contrastive fine-tuning combined with contrastive prompting.", "conclusion": "The findings challenge the 'More Data is Better' paradigm, highlighting the importance of cultural authenticity in translating low-resource dialects.", "key_contributions": ["Introduction of LebEval, a benchmark for evaluating Lebanese dialect translation", "Demonstration that smaller, culturally aware datasets yield better results", "Identification of effective fine-tuning techniques for dialect translation"], "limitations": "", "keywords": ["Large Language Models", "dialect translation", "cultural authenticity", "fine-tuning methods", "Lebanese dialect"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00127", "pdf": "https://arxiv.org/pdf/2505.00127.pdf", "abs": "https://arxiv.org/abs/2505.00127", "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs", "authors": ["Jinyan Su", "Jennifer Healey", "Preslav Nakov", "Claire Cardie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly optimized for long reasoning,\nunder the assumption that more reasoning leads to better performance. However,\nemerging evidence suggests that longer responses can sometimes degrade accuracy\nrather than improve it. In this paper, we conduct a systematic empirical study\nof the relationship between reasoning length and answer correctness. We find\nthat LLMs tend to overthink simple problems, generating unnecessarily long\noutputs, and underthink harder ones, failing to extend their reasoning when it\nis most needed. This indicates that models might misjudge problem difficulty\nand fail to calibrate their response length appropriately. Furthermore, we\ninvestigate the effects of length reduction with a preference optimization\nalgorithm when simply preferring the shorter responses regardless of answer\ncorrectness. Experiments show that the generation length can be significantly\nreduced while maintaining acceptable accuracy. Our findings highlight\ngeneration length as a meaningful signal for reasoning behavior and motivate\nfurther exploration into LLMs' self-awareness in reasoning length adaptation.", "AI": {"tldr": "This paper studies the trade-off between reasoning length and correctness in LLMs, finding that overly long outputs can reduce accuracy while shorter responses may be more effective.", "motivation": "To understand how the length of reasoning in LLMs affects the correctness of their answers.", "method": "Conducted a systematic empirical study analyzing the relationship between reasoning length and answer correctness in LLMs, along with experiments using a preference optimization algorithm for length reduction.", "result": "LLMs often generate unnecessarily long outputs for simple problems and fail to extend reasoning for complex ones; however, reducing generation length can maintain acceptable accuracy.", "conclusion": "Generation length is a critical factor in LLM reasoning behavior, and models need better calibration of response length based on problem difficulty.", "key_contributions": ["Systematic empirical study on reasoning length and correctness in LLMs.", "Demonstrated that shorter responses can maintain accuracy.", "Provided insights into the self-awareness of LLMs in adapting reasoning length."], "limitations": "", "keywords": ["Large language models", "Reasoning length", "Answer correctness"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.00147", "pdf": "https://arxiv.org/pdf/2505.00147.pdf", "abs": "https://arxiv.org/abs/2505.00147", "title": "AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) allows a language model to improve its\nproblem-solving capability when provided with suitable information in context.\nSince the choice of in-context information can be determined based on the\nproblem itself, in-context learning is analogous to human learning from\nteachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that\nICL performance can be improved by leveraging a frontier large language model's\n(LLM) ability to predict required skills to solve a problem, popularly referred\nto as an LLM's metacognition, and using the recommended skills to construct\nnecessary in-context examples. While this skill-based strategy boosts ICL\nperformance in larger models, its gains on small language models (SLMs) have\nbeen minimal, highlighting a performance gap in ICL capabilities. We\ninvestigate this gap and show that skill-based prompting can hurt SLM\nperformance on easy questions by introducing unnecessary information, akin to\ncognitive overload. To address this, we introduce AdaptMI, an adaptive approach\nto selecting skill-based in-context Math Instructions for SLMs. Inspired by\ncognitive load theory from human pedagogy, our method only introduces\nskill-based examples when the model performs poorly. We further propose\nAdaptMI+, which adds examples targeted to the specific skills missing from the\nmodel's responses. On 5-shot evaluations across popular math benchmarks and\nfive SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over\nnaive skill-based strategies.", "AI": {"tldr": "The paper presents AdaptMI, an adaptive method for selecting skill-based examples to enhance in-context learning in small language models, addressing performance gaps caused by cognitive overload.", "motivation": "To improve in-context learning (ICL) performance in small language models (SLMs) when faced with problem-solving tasks.", "method": "AdaptMI selectively introduces skill-based in-context examples when the model struggles, inspired by cognitive load theory, and AdaptMI+ further tailors examples to the specific skills the model lacks.", "result": "AdaptMI+ shows an accuracy improvement of up to 6% compared to naive skill-based strategies in 5-shot evaluations across popular math benchmarks.", "conclusion": "The study highlights the importance of tailored in-context learning strategies to prevent cognitive overload and enhance the performance of small language models.", "key_contributions": ["Introduction of AdaptMI for adaptive skill-based prompting in SLMs", "Demonstration of cognitive overload in SLM performance with traditional ICL methods", "Empirical improvement in model accuracy on math benchmarks using AdaptMI+"], "limitations": "The approach may not generalize to all types of problems beyond math or all small language models.", "keywords": ["in-context learning", "language models", "cognitive load", "adaptive prompting", "skill-based examples"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00191", "pdf": "https://arxiv.org/pdf/2505.00191.pdf", "abs": "https://arxiv.org/abs/2505.00191", "title": "IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports", "authors": ["Yuyan Ge", "Kwan Ho Ryan Chan", "Pablo Messina", "René Vidal"], "categories": ["cs.CL"], "comment": "12 pages, 4 figures", "summary": "The development of AI-based methods for analyzing radiology reports could\nlead to significant advances in medical diagnosis--from improving diagnostic\naccuracy to enhancing efficiency and reducing workload. However, the lack of\ninterpretability in these methods has hindered their adoption in clinical\nsettings. In this paper, we propose an interpretable-by-design framework for\nclassifying radiology reports. The key idea is to extract a set of most\ninformative queries from a large set of reports and use these queries and their\ncorresponding answers to predict a diagnosis. Thus, the explanation for a\nprediction is, by construction, the set of selected queries and answers. We use\nthe Information Pursuit framework to select informative queries, the Flan-T5\nmodel to determine if facts are present in the report, and a classifier to\npredict the disease. Experiments on the MIMIC-CXR dataset demonstrate the\neffectiveness of the proposed method, highlighting its potential to enhance\ntrust and usability in medical AI.", "AI": {"tldr": "The paper presents an interpretable AI framework for classifying radiology reports by extracting informative queries to aid medical diagnosis, enhancing usability and trust in medical AI applications.", "motivation": "There is a pressing need for AI methods in analyzing radiology reports that are interpretable to improve clinical adoption and diagnostic accuracy.", "method": "The framework extracts informative queries from a large set of reports using the Information Pursuit framework and employs the Flan-T5 model for fact determination and a classifier for disease prediction.", "result": "Experiments on the MIMIC-CXR dataset show that the proposed method effectively enhances interpretability and trust in AI-driven medical diagnostics.", "conclusion": "The framework's approach to providing explanations based on selected queries and answers opens up avenues for better usability in medical AI.", "key_contributions": ["Introduced an interpretable-by-design AI framework for radiology report classification.", "Utilized query extraction to aid in diagnosis explanations.", "Demonstrated effectiveness on the MIMIC-CXR dataset for medical AI applications."], "limitations": "Potential limitations include reliance on the quality of input queries and generalizability across diverse datasets.", "keywords": ["interpretability", "radiology reports", "AI framework", "medical diagnostics", "MIMIC-CXR"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.00261", "pdf": "https://arxiv.org/pdf/2505.00261.pdf", "abs": "https://arxiv.org/abs/2505.00261", "title": "Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring", "authors": ["Jayoung Song", "KyungTae Lim", "Jungyeul Park"], "categories": ["cs.CL"], "comment": null, "summary": "Despite growing global interest in Korean language education, there remains a\nsignificant lack of learner corpora tailored to Korean L2 writing. To address\nthis gap, we enhance the KoLLA Korean learner corpus by adding multiple\ngrammatical error correction (GEC) references, thereby enabling more nuanced\nand flexible evaluation of GEC systems, and reflects the variability of human\nlanguage. Additionally, we enrich the corpus with rubric-based scores aligned\nwith guidelines from the Korean National Language Institute, capturing\ngrammatical accuracy, coherence, and lexical diversity. These enhancements make\nKoLLA a robust and standardized resource for research in Korean L2 education,\nsupporting advancements in language learning, assessment, and automated error\ncorrection.", "AI": {"tldr": "Enhancement of the KoLLA Korean learner corpus for better evaluation of grammatical error correction systems in Korean L2 writing.", "motivation": "To fill the gap in learner corpora for Korean language education, particularly for L2 writing evaluations.", "method": "Added multiple grammatical error correction references and rubric-based scores to the existing KoLLA corpus.", "result": "The enhanced KoLLA corpus now provides a more nuanced evaluation of GEC systems and reflects variability in human language usage.", "conclusion": "KoLLA becomes a robust resource for research in Korean L2 education and supports advancements in language learning and automated error correction.", "key_contributions": ["Multiple GEC references added to KoLLA corpus.", "Rubric-based scores for grammatical accuracy and coherence were introduced.", "Enhanced resource for automated error correction systems."], "limitations": "", "keywords": ["Korean language education", "learner corpus", "grammatical error correction"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2505.00268", "pdf": "https://arxiv.org/pdf/2505.00268.pdf", "abs": "https://arxiv.org/abs/2505.00268", "title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "authors": ["Jekaterina Novikova", "Carol Anderson", "Borhane Blili-Hamelin", "Subhabrata Majumdar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The hallmark of effective language use lies in consistency -- expressing\nsimilar meanings in similar contexts and avoiding contradictions. While human\ncommunication naturally demonstrates this principle, state-of-the-art language\nmodels struggle to maintain reliable consistency across different scenarios.\nThis paper examines the landscape of consistency research in AI language\nsystems, exploring both formal consistency (including logical rule adherence)\nand informal consistency (such as moral and factual coherence). We analyze\ncurrent approaches to measure aspects of consistency, identify critical\nresearch gaps in standardization of definitions, multilingual assessment, and\nmethods to improve consistency. Our findings point to an urgent need for robust\nbenchmarks to measure and interdisciplinary approaches to ensure consistency in\nthe application of language models on domain-specific tasks while preserving\nthe utility and adaptability.", "AI": {"tldr": "This paper explores the challenges of maintaining consistency in AI language models, highlighting the need for benchmarks and interdisciplinary approaches.", "motivation": "To address the inconsistent performance of AI language models in maintaining reliable meaning across various contexts while ensuring compatibility with domain-specific tasks.", "method": "Analyzing current research on both formal and informal consistency in language models and identifying gaps in measurement and standardization.", "result": "Identified critical gaps in definitions, multilingual metrics, and strategies for enhancing consistency in language model applications.", "conclusion": "There is an urgent need for robust benchmarks and interdisciplinary methods to improve consistency in language models, particularly for domain-specific applications.", "key_contributions": ["Examination of both formal and informal consistency in AI language systems.", "Identification of key research gaps related to consistency measurement.", "Recommendations for the development of robust benchmarks for evaluating consistency."], "limitations": "", "keywords": ["AI", "language models", "consistency", "benchmarking", "interdisciplinary"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00339", "pdf": "https://arxiv.org/pdf/2505.00339.pdf", "abs": "https://arxiv.org/abs/2505.00339", "title": "Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation", "authors": ["Antoun Yaacoub", "Sansiri Tarnpradab", "Phattara Khumprom", "Zainab Assaghir", "Lionel Prevost", "Jérôme Da-Rugna"], "categories": ["cs.CL", "cs.AI"], "comment": "This article will be presented in IJCNN 2025 \"AI Innovations for\n  Education: Transforming Teaching and Learning through Cutting-Edge\n  Technologies\" workshop", "summary": "Artificial intelligence (AI) is rapidly transforming education, presenting\nunprecedented opportunities for personalized learning and streamlined content\ncreation. However, realizing the full potential of AI in educational settings\nnecessitates careful consideration of the quality, cognitive depth, and ethical\nimplications of AI-generated materials. This paper synthesizes insights from\nfour related studies to propose a comprehensive framework for enhancing\nAI-driven educational tools. We integrate cognitive assessment frameworks\n(Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated\nfeedback, and ethical design principles to guide the development of effective\nand responsible AI tools. We outline a structured three-phase approach\nencompassing cognitive alignment, linguistic feedback integration, and ethical\nsafeguards. The practical application of this framework is demonstrated through\nits integration into OneClickQuiz, an AI-powered Moodle plugin for quiz\ngeneration. This work contributes a comprehensive and actionable guide for\neducators, researchers, and developers aiming to harness AI's potential while\nupholding pedagogical and ethical standards in educational content generation.", "AI": {"tldr": "This paper proposes a framework for enhancing AI-driven educational tools, integrating cognitive assessment, linguistic analysis, and ethical design principles.", "motivation": "To address the need for quality and ethical considerations in AI-generated educational materials.", "method": "The study synthesizes insights from four studies and proposes a three-phase approach: cognitive alignment, linguistic feedback integration, and ethical safeguards.", "result": "The framework was applied in the OneClickQuiz plugin, demonstrating an actionable guide for educators and developers.", "conclusion": "The proposed framework assists in developing responsible AI tools while enhancing pedagogical standards in education.", "key_contributions": ["Development of a comprehensive framework for AI in education.", "Integration of cognitive and ethical principles in AI content generation.", "Demonstration of practical application with the OneClickQuiz plugin."], "limitations": "", "keywords": ["AI in Education", "Cognitive Assessment", "Ethical Design", "AI-Generated Materials", "Personalized Learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.00367", "pdf": "https://arxiv.org/pdf/2505.00367.pdf", "abs": "https://arxiv.org/abs/2505.00367", "title": "KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis", "authors": ["JunSeo Kim", "HyeHyeon Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cognitive distortion refers to negative thinking patterns that can lead to\nmental health issues like depression and anxiety in adolescents. Previous\nstudies using natural language processing (NLP) have focused mainly on\nsmall-scale adult datasets, with limited research on adolescents. This study\nintroduces KoACD, the first large-scale dataset of cognitive distortions in\nKorean adolescents, containing 108,717 instances. We applied a multi-Large\nLanguage Model (LLM) negotiation method to refine distortion classification and\ngenerate synthetic data using two approaches: cognitive clarification for\ntextual clarity and cognitive balancing for diverse distortion representation.\nValidation through LLMs and expert evaluations showed that while LLMs\nclassified distortions with explicit markers, they struggled with\ncontext-dependent reasoning, where human evaluators demonstrated higher\naccuracy. KoACD aims to enhance future research on cognitive distortion\ndetection.", "AI": {"tldr": "This study presents KoACD, a large-scale dataset of cognitive distortions in Korean adolescents, and explores methodologies for classification and generation of data using LLMs.", "motivation": "The research addresses the gap in studies focused on cognitive distortions in adolescents by introducing a comprehensive dataset and exploring LLM applications in text classification.", "method": "The study utilized a multi-LLM negotiation method for refining distortion classification and generating synthetic data through cognitive clarification and cognitive balancing approaches.", "result": "Validation showed LLMs effectively classified explicit cognitive distortions but struggled with context-dependent reasoning, while human evaluators performed better in such instances.", "conclusion": "KoACD is expected to advance the understanding and detection of cognitive distortions among Korean adolescents, paving the way for future research in mental health informatics.", "key_contributions": ["Introduction of the KoACD dataset with 108,717 instances", "Application of multi-LLM methods for cognitive distortion classification", "Insights into LLM capabilities and limitations in context-dependent reasoning"], "limitations": "The study's findings are based on a specific demographic (Korean adolescents), which may limit generalizability.", "keywords": ["Cognitive Distortions", "Adolescents", "Large Language Models", "Natural Language Processing", "Mental Health"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.00389", "pdf": "https://arxiv.org/pdf/2505.00389.pdf", "abs": "https://arxiv.org/abs/2505.00389", "title": "CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass", "authors": ["Bowen Zhang", "Zixin Song", "Chunping Li"], "categories": ["cs.CL"], "comment": "Accepted by SIGIR 2025 (Full)", "summary": "As a fundamental task in Information Retrieval and Computational Linguistics,\nsentence representation has profound implications for a wide range of practical\napplications such as text clustering, content analysis, question-answering\nsystems, and web search. Recent advances in pre-trained language models (PLMs)\nhave driven remarkable progress in this field, particularly through\nunsupervised embedding derivation methods centered on discriminative PLMs like\nBERT. However, due to time and computational constraints, few efforts have\nattempted to integrate unsupervised sentence representation with generative\nPLMs, which typically possess much larger parameter sizes. Given that\nstate-of-the-art models in both academia and industry are predominantly based\non generative architectures, there is a pressing need for an efficient\nunsupervised text representation framework tailored to decoder-only PLMs. To\naddress this concern, we propose CSE-SFP, an innovative method that exploits\nthe structural characteristics of generative models. Compared to existing\nstrategies, CSE-SFP requires only a single forward pass to perform effective\nunsupervised contrastive learning. Rigorous experimentation demonstrates that\nCSE-SFP not only produces higher-quality embeddings but also significantly\nreduces both training time and memory consumption. Furthermore, we introduce\ntwo ratio metrics that jointly assess alignment and uniformity, thereby\nproviding a more robust means for evaluating the semantic spatial properties of\nencoding models.", "AI": {"tldr": "CSE-SFP is an innovative unsupervised sentence representation method that utilizes generative pre-trained language models, requiring only a single forward pass for effective contrastive learning, resulting in high-quality embeddings with reduced training time and memory usage.", "motivation": "To address the gap in integrating unsupervised sentence representation methods with generative PLMs, which are more prevalent in state-of-the-art models.", "method": "CSE-SFP employs a single forward pass in a generative PLM for unsupervised contrastive learning, focusing on the structural characteristics of these models.", "result": "CSE-SFP achieved higher-quality embeddings compared to existing methods while significantly lowering both training time and memory consumption.", "conclusion": "The proposed framework offers an efficient way to leverage generative PLMs for unsupervised text representation, suggesting better evaluation metrics for semantic properties.", "key_contributions": ["Introduces CSE-SFP for unsupervised sentence representation using generative PLMs", "Demonstrates significant reductions in training time and memory usage", "Presents new metrics for evaluating semantic spatial properties of embeddings"], "limitations": "", "keywords": ["unsupervised sentence representation", "generative PLMs", "contrastive learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00467", "pdf": "https://arxiv.org/pdf/2505.00467.pdf", "abs": "https://arxiv.org/abs/2505.00467", "title": "Red Teaming Large Language Models for Healthcare", "authors": ["Vahid Balazadeh", "Michael Cooper", "David Pellow", "Atousa Assadi", "Jennifer Bell", "Jim Fackler", "Gabriel Funingana", "Spencer Gable-Cook", "Anirudh Gangadhar", "Abhishek Jaiswal", "Sumanth Kaja", "Christopher Khoury", "Randy Lin", "Kaden McKeen", "Sara Naimimohasses", "Khashayar Namdar", "Aviraj Newatia", "Allan Pang", "Anshul Pattoo", "Sameer Peesapati", "Diana Prepelita", "Bogdana Rakova", "Saba Sadatamin", "Rafael Schulman", "Ajay Shah", "Syed Azhar Shah", "Syed Ahmar Shah", "Babak Taati", "Balagopal Unnikrishnan", "Stephanie Williams", "Rahul G Krishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.", "AI": {"tldr": "The paper details a workshop focused on identifying vulnerabilities in large language models (LLMs) used in healthcare through red teaming by clinicians.", "motivation": "To uncover clinical vulnerabilities in LLMs that could lead to harm, which may not be evident to developers without clinical backgrounds.", "method": "A workshop where computational and clinical experts collaborated to evaluate LLM responses to clinical prompts, identifying potential vulnerabilities.", "result": "Several vulnerabilities were discovered and categorized, along with a replication study assessing these vulnerabilities across various LLMs.", "conclusion": "The collaboration between clinicians and computational experts is vital in identifying risks associated with LLMs in healthcare applications.", "key_contributions": ["Identification and categorization of LLM vulnerabilities in healthcare contexts.", "Replication study results on LLM performance across different models.", "Insights into the necessity of red teaming in healthcare AI."], "limitations": "", "keywords": ["Large Language Models", "Healthcare", "Red Teaming", "Vulnerabilities", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00479", "pdf": "https://arxiv.org/pdf/2505.00479.pdf", "abs": "https://arxiv.org/abs/2505.00479", "title": "Computational Identification of Regulatory Statements in EU Legislation", "authors": ["Gijs Jan Brandsma", "Jens Blom-Hansen", "Christiaan Meijer", "Kody Moodley"], "categories": ["cs.CL", "I.2.7"], "comment": "11 pages, 6 figures", "summary": "Identifying regulatory statements in legislation is useful for developing\nmetrics to measure the regulatory density and strictness of legislation. A\ncomputational method is valuable for scaling the identification of such\nstatements from a growing body of EU legislation, constituting approximately\n180,000 published legal acts between 1952 and 2023. Past work on extraction of\nthese statements varies in the permissiveness of their definitions for what\nconstitutes a regulatory statement. In this work, we provide a specific\ndefinition for our purposes based on the institutional grammar tool. We develop\nand compare two contrasting approaches for automatically identifying such\nstatements in EU legislation, one based on dependency parsing, and the other on\na transformer-based machine learning model. We found both approaches performed\nsimilarly well with accuracies of 80% and 84% respectively and a K alpha of\n0.58. The high accuracies and not exceedingly high agreement suggests potential\nfor combining strengths of both approaches.", "AI": {"tldr": "This paper develops and compares two computational methods for identifying regulatory statements in EU legislation, achieving high accuracies.", "motivation": "To improve the identification of regulatory statements in legislation for measuring regulatory density and strictness.", "method": "The paper examines two methods: one based on dependency parsing and the other on a transformer-based machine learning model, both applied to EU legislation texts.", "result": "The dependency parsing method achieved an accuracy of 80%, while the transformer-based model achieved 84%, with a K alpha of 0.58 indicating moderate agreement between methods.", "conclusion": "Both methods are viable for identifying regulatory statements, and their combination could leverage their respective strengths.", "key_contributions": ["Provided a specific definition of regulatory statements based on institutional grammar", "Developed two computational approaches for identification of these statements", "Demonstrated effective performance of both methods in a large-scale dataset"], "limitations": "", "keywords": ["Regulatory statements", "EU legislation", "Machine learning", "Dependency parsing", "Transformer model"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.00506", "pdf": "https://arxiv.org/pdf/2505.00506.pdf", "abs": "https://arxiv.org/abs/2505.00506", "title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection", "authors": ["Deanna Emery", "Michael Goitia", "Freddie Vargus", "Iulia Neagu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\ndomains, detecting hallucinated content$\\unicode{x2013}$text that is not\ngrounded in supporting evidence$\\unicode{x2013}$has become a critical\nchallenge. Existing benchmarks for hallucination detection are often\nsynthetically generated, narrowly focused on extractive question answering, and\nfail to capture the complexity of real-world scenarios involving multi-document\ncontexts and full-sentence outputs. We introduce the HalluMix Benchmark, a\ndiverse, task-agnostic dataset that includes examples from a range of domains\nand formats. Using this benchmark, we evaluate seven hallucination detection\nsystems$\\unicode{x2013}$both open and closed\nsource$\\unicode{x2013}$highlighting differences in performance across tasks,\ndocument lengths, and input representations. Our analysis highlights\nsubstantial performance disparities between short and long contexts, with\ncritical implications for real-world Retrieval Augmented Generation (RAG)\nimplementations. Quotient Detections achieves the best overall performance,\nwith an accuracy of 0.82 and an F1 score of 0.84.", "AI": {"tldr": "This paper introduces the HalluMix Benchmark for detecting hallucinated content in large language models, showcasing evaluation results across various systems.", "motivation": "The increasing deployment of large language models in high-stakes domains necessitates effective detection of hallucinated content, which is currently underserved by existing benchmarks.", "method": "Introduced a diverse, task-agnostic dataset (HalluMix Benchmark) and evaluated seven hallucination detection systems, analyzing their performance across different tasks and document contexts.", "result": "The analysis revealed significant performance disparities between systems on short and long contexts; Quotient Detections achieved the best overall performance with an accuracy of 0.82 and an F1 score of 0.84.", "conclusion": "The findings underscore the importance of robust hallucination detection mechanisms especially for practical implementations like Retrieval Augmented Generation, suggesting future improvements in model training and evaluation.", "key_contributions": ["Introduction of the HalluMix Benchmark for hallucination detection", "Evaluation of multiple detection systems across diverse tasks", "Insights on performance disparities based on context length"], "limitations": "", "keywords": ["hallucination detection", "large language models", "HalluMix Benchmark", "Retrieval Augmented Generation", "multidocument contexts"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.00551", "pdf": "https://arxiv.org/pdf/2505.00551.pdf", "abs": "https://arxiv.org/abs/2505.00551", "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "authors": ["Chong Zhang", "Yue Deng", "Xiang Lin", "Bin Wang", "Dianwen Ng", "Hai Ye", "Xingxuan Li", "Yao Xiao", "Zhanfeng Mo", "Qi Zhang", "Lidong Bing"], "categories": ["cs.CL"], "comment": null, "summary": "The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs.", "AI": {"tldr": "This paper summarizes recent replication studies on reasoning language models (RLMs), focusing on supervised fine-tuning and reinforcement learning methodologies to enhance model performance.", "motivation": "To provide insights and guidance for researchers and developers working on reasoning language models (RLMs) due to the lack of complete open-sourcing by DeepSeek.", "method": "The paper reviews recent replication studies, emphasizing supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR) for improving RLMs, detailing data preparation, method designs, and training procedures.", "result": "The analysis reveals effective data construction and training techniques, yielding comparable performance to DeepSeek-R1 while enhancing understanding of RLM implementation.", "conclusion": "The survey offers key findings and recommendations that may inspire further research in enhancing reasoning language models and expanding their application scope, while acknowledging existing challenges.", "key_contributions": ["Summary of replication studies for RLMs", "Insights into supervised fine-tuning and reinforcement learning techniques", "Identification of challenges and future research directions"], "limitations": "The survey may not cover every replication study and is dependent on the varying quality of open-source implementations.", "keywords": ["Reasoning Language Models", "Supervised Fine-Tuning", "Reinforcement Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.00557", "pdf": "https://arxiv.org/pdf/2505.00557.pdf", "abs": "https://arxiv.org/abs/2505.00557", "title": "Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models", "authors": ["Makoto Sato"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations in large language models (LLMs) present a growing challenge\nacross real-world applications, from healthcare to law, where factual\nreliability is essential. Despite advances in alignment and instruction tuning,\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\nan open problem. In this study, we propose a prompt-based framework to\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\nperiodic table of elements and tarot divination) in a misleading way, and a\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\nconfidence, and coherence of the output. Controlled experiments across multiple\nLLMs revealed that HIPs consistently produced less coherent and more\nhallucinated responses than their null-fusion controls. These effects varied\nacross models, with reasoning-oriented LLMs showing distinct profiles from\ngeneral-purpose ones. Our framework provides a reproducible testbed for\nstudying hallucination vulnerability, and opens the door to developing safer,\nmore introspective LLMs that can detect and self-regulate the onset of\nconceptual instability.", "AI": {"tldr": "This study introduces a framework for triggering and quantifying hallucinations in large language models (LLMs) using specialized prompts.", "motivation": "To address the challenge of hallucinations in LLMs, which affect reliability in real-world applications such as healthcare and law.", "method": "A prompt-based approach utilizing Hallucination-Inducing Prompts (HIPs) and Hallucination Quantifying Prompts (HQPs) to analyze and score the hallucinations generated by LLMs.", "result": "Controlled experiments showed that HIPs led to less coherent and more hallucinated responses compared to controls, with varying effects across different models.", "conclusion": "The proposed framework serves as a testbed for understanding hallucination vulnerabilities and could contribute to the development of safer, introspective LLMs.", "key_contributions": ["Introduction of Hallucination-Inducing Prompts (HIPs) and Hallucination Quantifying Prompts (HQPs).", "Demonstration of varied hallucination responses across different LLMs.", "Creation of a reproducible testbed for studying hallucinations in LLMs."], "limitations": "", "keywords": ["large language models", "hallucinations", "prompt-based framework", "healthcare", "AI safety"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.00570", "pdf": "https://arxiv.org/pdf/2505.00570.pdf", "abs": "https://arxiv.org/abs/2505.00570", "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension", "authors": ["Jushi Kai", "Boyi Zeng", "Yixuan Wang", "Haoli Bai", "Bo Jiang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.", "AI": {"tldr": "This paper introduces FreqKV, a novel context extension method for large language models that optimizes memory requirements and self-attention complexity by compressing the KV cache in the frequency domain.", "motivation": "Extending the context window in large language models is critical for long-form content generation, but existing methods face significant performance and efficiency challenges.", "method": "The proposed method, FreqKV, filters high-frequency components from the KV cache in the frequency domain and compresses it to a fixed size, facilitating efficient fine-tuning and inference without additional parameters.", "result": "Experiments show that FreqKV effectively improves the performance of long context language modeling and understanding tasks, maintaining minimal information loss during compression.", "conclusion": "FreqKV allows LLMs to leverage a compressed KV cache efficiently, thereby extending context windows without architectural changes or significant fine-tuning efforts.", "key_contributions": ["Introduction of FreqKV for compressing KV cache in the frequency domain", "No additional parameters or architectural modifications required", "Demonstrated improved efficiency for long context applications"], "limitations": "", "keywords": ["Large Language Models", "Context Extension", "KV Cache Compression", "Fine-tuning Efficiency", "Long-context Tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.00582", "pdf": "https://arxiv.org/pdf/2505.00582.pdf", "abs": "https://arxiv.org/abs/2505.00582", "title": "Block Circulant Adapter for Large Language Models", "authors": ["Xinyu Ding", "Meiqi Wang", "Siyu Liao", "Zhongfeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)", "summary": "Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks.", "AI": {"tldr": "Block circulant matrix-based fine-tuning method reduces costs for large language models using Fourier transforms.", "motivation": "Fine-tuning large language models (LLMs) is challenging due to their size and associated costs.", "method": "The proposed method utilizes block circulant matrices and one-dimensional Fourier transforms to lower storage and computation expenses during fine-tuning.", "result": "Experiments indicate a reduction of parameters (14x less than VeRA, 16x smaller than LoRA) and FLOPs (32x less than FourierFT) while achieving comparable or superior task performance.", "conclusion": "This approach offers a promising avenue in the frequency domain to effectively fine-tune large models on various downstream tasks.", "key_contributions": ["Introduction of block circulant matrix-based fine-tuning method", "Stability in training heuristic", "Significant reductions in parameters and computation costs"], "limitations": "", "keywords": ["large language models", "fine-tuning", "block circulant matrices", "Fourier transforms", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.00624", "pdf": "https://arxiv.org/pdf/2505.00624.pdf", "abs": "https://arxiv.org/abs/2505.00624", "title": "FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation", "authors": ["Chaitali Bhattacharyya", "Yeseong Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training large language models (LLMs) from scratch requires significant\ncomputational resources, driving interest in developing smaller,\ndomain-specific LLMs that maintain both efficiency and strong task performance.\nMedium-sized models such as LLaMA, llama} have served as starting points for\ndomain-specific adaptation, but they often suffer from accuracy degradation\nwhen tested on specialized datasets. We introduce FineScope, a framework for\nderiving compact, domain-optimized LLMs from larger pretrained models.\nFineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its\nability to produce interpretable feature representations, to extract\ndomain-specific subsets from large datasets. We apply structured pruning with\ndomain-specific constraints, ensuring that the resulting pruned models retain\nessential knowledge for the target domain. To further enhance performance,\nthese pruned models undergo self-data distillation, leveraging SAE-curated\ndatasets to restore key domain-specific information lost during pruning.\nExtensive experiments and ablation studies demonstrate that FineScope achieves\nhighly competitive performance, outperforming several large-scale\nstate-of-the-art LLMs in domain-specific tasks. Additionally, our results show\nthat FineScope enables pruned models to regain a substantial portion of their\noriginal performance when fine-tuned with SAE-curated datasets. Furthermore,\napplying these datasets to fine-tune pretrained LLMs without pruning also\nimproves their domain-specific accuracy, highlighting the robustness of our\napproach. The code will be released.", "AI": {"tldr": "FineScope is a framework designed to derive efficient, domain-specific large language models (LLMs) from larger pretrained models using structured pruning and self-data distillation.", "motivation": "The need for efficient domain-specific LLMs due to the high computational cost of training large models from scratch.", "method": "FineScope uses Sparse Autoencoder (SAE) for extracting domain-specific subsets and applies structured pruning with domain constraints, followed by self-data distillation to restore lost information.", "result": "FineScope outperforms several state-of-the-art LLMs in domain-specific tasks and helps pruned models regain performance through fine-tuning with SAE-curated datasets.", "conclusion": "The approach significantly enhances the performance and efficiency of domain-specific LLMs, proving effective for specialized applications.", "key_contributions": ["Introduction of the FineScope framework for domain-optimized LLMs.", "Use of structured pruning with domain constraints.", "Demonstration of improved performance through self-data distillation."], "limitations": "", "keywords": ["Large Language Models", "Domain-Specific Adaptation", "Structured Pruning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00626", "pdf": "https://arxiv.org/pdf/2505.00626.pdf", "abs": "https://arxiv.org/abs/2505.00626", "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)", "authors": ["Zihao Wang", "Yibo Jiang", "Jiahao Yu", "Heqing Huang"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "comment": null, "summary": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers.", "AI": {"tldr": "This paper investigates how to improve the role separation capabilities of large language models (LLMs) by reinforcing invariant signals in their input encoding to enhance multi-role behavior without the models relying on memorization.", "motivation": "The motivation is to ensure consistent multi-role behavior in LLMs by examining their ability to distinguish messages from different roles, which is crucial for ensuring reliable interactions.", "method": "The authors investigate role-separation learning through controlled experiments, identifying proxies that models use for role identification, and propose manipulating position IDs to improve role boundary recognition.", "result": "The findings show that fine-tuned models tend to exploit task types and proximity to text beginnings to identify roles. The approach of augmenting token-wise cues improved role distinction, indicating a more robust learning process.", "conclusion": "By focusing on the mechanism of role boundary recognition through position IDs, the study provides insights into enhancing the reliability of LLMs in multi-role contexts rather than relying on superficial memorization.", "key_contributions": ["Investigation of role-separation learning in LLMs", "Identification of proxies used by models for role identification", "Proposal of leveraging position IDs for clearer role distinctions"], "limitations": "The study suggests that while data augmentation helps, it often leads to iterative solutions rather than deep-rooted fixes.", "keywords": ["role separation", "large language models", "role identification", "input encoding", "multi-role behavior"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00654", "pdf": "https://arxiv.org/pdf/2505.00654.pdf", "abs": "https://arxiv.org/abs/2505.00654", "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "authors": ["Daniel N. Nissani"], "categories": ["cs.CL", "cs.AI"], "comment": "submitted to NEURAL COMPUTATION", "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.", "AI": {"tldr": "This paper argues that Large Language Models (LLMs) face an inherent ambiguity barrier that prevents them from truly understanding the meaning of their dialogues, countering claims of their comprehension capabilities.", "motivation": "To address the debate surrounding the understanding capabilities of LLMs and explore a counter-argument to the notion that they can comprehend dialogue meaning.", "method": "The paper employs a thought experiment and semi-formal considerations to examine the limitations of LLMs in understanding language.", "result": "It concludes that LLMs are inherently limited by ambiguity barriers that hinder their ability to grasp the true meanings of their fluent dialogues.", "conclusion": "This research adds to the discourse on LLMs by emphasizing the fundamental limitations in their understanding, challenging optimistic perceptions of their capabilities.", "key_contributions": ["Presentation of a new counter-argument against LLM comprehension", "Identification of the ambiguity barrier that limits LLM understanding", "Use of thought experiments to illustrate the argument"], "limitations": "The argument may require further empirical validation and is based primarily on theoretical considerations.", "keywords": ["Large Language Models", "ambiguity barrier", "understanding", "dialogue", "thought experiment"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.00661", "pdf": "https://arxiv.org/pdf/2505.00661.pdf", "abs": "https://arxiv.org/abs/2505.00661", "title": "On the generalization of language models from in-context learning and finetuning: a controlled study", "authors": ["Andrew K. Lampinen", "Arslan Chaudhry", "Stephanie C. Y. Chan", "Cody Wild", "Diane Wan", "Alex Ku", "Jörg Bornschein", "Razvan Pascanu", "Murray Shanahan", "James L. McClelland"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning -- from failing to\ngeneralize to simple reversals of relations they are trained on, to missing\nlogical deductions that can be made from trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nHowever, language models' in-context learning shows different inductive biases,\nand can generalize better in some of these cases. Here, we explore these\ndifferences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' ability to generalize from finetuning data. The datasets are\nconstructed to isolate the knowledge in the dataset from that in pretraining,\nto create clean tests of generalization. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance.", "AI": {"tldr": "This paper explores the differences in generalization capabilities between in-context learning and fine-tuning in large language models, proposing a method to enhance fine-tuning by incorporating in-context inferences.", "motivation": "Investigate the limitations of fine-tuning in large language models and explore how in-context learning can offer superior generalization capabilities.", "method": "Construct novel datasets to evaluate the generalization abilities of pretrained models when exposed to either in-context learning or fine-tuning, focusing on controlled subsets of information.", "result": "The study finds that in-context learning generally allows for more flexible generalization than fine-tuning, although exceptions exist. A proposed method that integrates in-context inferences into fine-tuning data shows improved generalization across various benchmarks.", "conclusion": "In-context learning exhibits broader generalization capabilities compared to fine-tuning, and incorporating in-context inferences into fine-tuning can enhance model performance.", "key_contributions": ["Comparison of generalization between in-context learning and fine-tuning", "Development of novel datasets for evaluating model generalization", "Proposal of a method to improve fine-tuning through in-context learning"], "limitations": "The research highlights specific cases where fine-tuning can outperform in-context learning, indicating that the relationship is not universally applicable.", "keywords": ["large language models", "in-context learning", "fine-tuning", "generalization", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.00662", "pdf": "https://arxiv.org/pdf/2505.00662.pdf", "abs": "https://arxiv.org/abs/2505.00662", "title": "DeepCritic: Deliberate Critique with Large Language Models", "authors": ["Wenkai Yang", "Jingwen Chen", "Yankai Lin", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Data and models are available at\n  https://github.com/RUCBM/DeepCritic", "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.", "AI": {"tldr": "This paper presents a two-stage framework to enhance the math critique capabilities of Large Language Models (LLMs) for improved oversight and feedback on generated solutions.", "motivation": "The need for precise, scalable supervision of LLM outputs, especially in mathematical reasoning, is critical as LLMs evolve.", "method": "A two-stage framework where the first stage involves generating critiques for supervised fine-tuning, and the second stage employs reinforcement learning to enhance critique quality based on human-labeled and automatically annotated data.", "result": "The developed critique model outperforms existing LLM critics on error identification benchmarks and offers more detailed feedback for refining erroneous steps.", "conclusion": "The proposed framework significantly improves the ability of LLMs to critique math solutions and aids in correcting mistakes effectively.", "key_contributions": ["Introduces a novel two-stage framework for LLM critique enhancement.", "Utilizes multi-perspective verifications and in-depth feedback for each reasoning step.", "Demonstrates significant performance improvements over existing models in mathematical critique tasks."], "limitations": "", "keywords": ["Large Language Models", "math critique", "reinforcement learning", "supervised fine-tuning", "feedback mechanisms"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.00675", "pdf": "https://arxiv.org/pdf/2505.00675.pdf", "abs": "https://arxiv.org/abs/2505.00675", "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions", "authors": ["Yiming Du", "Wenyu Huang", "Danna Zheng", "Zhaowei Wang", "Sebastien Montella", "Mirella Lapata", "Kam-Fai Wong", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs) based agents. While prior surveys have focused on memory\napplications with LLMs, they often overlook the atomic operations that underlie\nmemory dynamics. In this survey, we first categorize memory representations\ninto parametric, contextual structured, and contextual unstructured and then\nintroduce six fundamental memory operations: Consolidation, Updating, Indexing,\nForgetting, Retrieval, and Compression. We systematically map these operations\nto the most relevant research topics across long-term, long-context, parametric\nmodification, and multi-source memory. By reframing memory systems through the\nlens of atomic operations and representation types, this survey provides a\nstructured and dynamic perspective on research, benchmark datasets, and tools\nrelated to memory in AI, clarifying the functional interplay in LLMs based\nagents while outlining promising directions for future research\\footnote{The\npaper list, datasets, methods and tools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.", "AI": {"tldr": "This survey categorizes memory representations in AI systems, particularly focusing on memory dynamics in large language models (LLMs), and introduces six atomic memory operations.", "motivation": "To provide a structured perspective on memory operations within LLMs that is often overlooked in previous surveys, offering insights into various memory representations and their implications in AI.", "method": "The paper categorizes memory representations and introduces six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. It maps these to relevant research topics across different aspects of memory.", "result": "The survey clarifies the functional interplay in LLMs by providing a dynamic framework for understanding memory systems, highlighting various memory representation types and operations.", "conclusion": "This structured perspective aids in clarifying research directions and tools related to memory in AI, suggesting areas for future exploration.", "key_contributions": ["Categorization of memory representations in AI systems.", "Introduction of six fundamental memory operations relevant to LLMs.", "Mapping memory operations to current research topics and datasets."], "limitations": "", "keywords": ["memory in AI", "large language models", "memory operations", "AI systems", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.00679", "pdf": "https://arxiv.org/pdf/2505.00679.pdf", "abs": "https://arxiv.org/abs/2505.00679", "title": "Steering Large Language Models with Register Analysis for Arbitrary Style Transfer", "authors": ["Xinchen Yang", "Marine Carpuat"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies.", "AI": {"tldr": "This paper proposes a novel prompting method based on register analysis to enhance style transfer in large language models (LLMs).", "motivation": "To address the challenge of effectively leveraging LLMs for example-based arbitrary style transfer and improve the description of the style of exemplars.", "method": "The paper introduces a prompting approach that uses register analysis to guide LLMs in rewriting text to match a desired style.", "result": "Empirical evaluations demonstrate that the proposed prompting method improves style transfer strength while maintaining the original meaning better than current prompting strategies.", "conclusion": "The study concludes that register analysis-based prompting is a more effective technique for guiding LLMs in style transfer tasks than existing methods.", "key_contributions": ["Introduction of register analysis as a prompt guiding mechanism for LLMs", "Empirical proof of enhanced style transfer capabilities compared to existing strategies", "Preservation of original text meaning during style transfer"], "limitations": "", "keywords": ["Large Language Models", "style transfer", "register analysis", "prompting methods", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2309.08532", "pdf": "https://arxiv.org/pdf/2309.08532.pdf", "abs": "https://arxiv.org/abs/2309.08532", "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers", "authors": ["Qingyan Guo", "Rui Wang", "Junliang Guo", "Bei Li", "Kaitao Song", "Xu Tan", "Guoqing Liu", "Jiang Bian", "Yujiu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Learning Representations (ICLR) 2024", "summary": "Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.", "AI": {"tldr": "EvoPrompt is a framework that automates the optimization of prompts for LLMs using evolutionary algorithms, significantly improving performance compared to human-engineered prompts and current methods.", "motivation": "LLMs require substantial human effort for prompt crafting; automating this process could enhance usability and effectiveness in various tasks.", "method": "The paper introduces EvoPrompt, which uses evolutionary algorithms to optimize discrete prompts without relying on gradients, operating through a population-based approach that iteratively generates and improves prompts with LLMs.", "result": "EvoPrompt outperforms human-engineered prompts and existing automatic prompt generation methods by up to 25% on the BIG-Bench Hard tasks, covering prompts for various datasets and tasks.", "conclusion": "The integration of LLMs with evolutionary algorithms establishes a new avenue for prompt optimization and encourages further exploration of such hybrid approaches.", "key_contributions": ["Development of a novel prompt optimization framework using evolutionary algorithms", "Demonstration of significant performance gains over human-crafted and automated prompts", "Introduction of synergies between LLMs and evolutionary algorithms for broader applications."], "limitations": "", "keywords": ["Large Language Models", "Evolutionary Algorithms", "Prompt Optimization", "Natural Language Processing", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2401.15371", "pdf": "https://arxiv.org/pdf/2401.15371.pdf", "abs": "https://arxiv.org/abs/2401.15371", "title": "LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning", "authors": ["Buqiang Xu", "Xin Dai", "Zhenghao Liu", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Yukun Yan", "Liner Yang", "Yu Gu", "Ge Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Legal Judgment Prediction (LJP) is a fundamental task of legal artificial\nintelligence, aiming to automatically predict the judgment outcomes of legal\ncases. Existing LJP models primarily focus on identifying legal triggers within\ncriminal fact descriptions by contrastively training language models. However,\nthese LJP models overlook the importance of learning to effectively distinguish\nsubtle differences among judgments, which is crucial for producing more\naccurate predictions. In this paper, we propose LegalDuet, which continuously\npretrains language models to learn a more tailored embedding space for\nrepresenting legal cases. Specifically, LegalDuet designs a dual-view mechanism\nto continuously pretrain language models: 1) Law Case Clustering retrieves\nsimilar cases as hard negatives and employs contrastive training to\ndifferentiate among confusing cases; 2) Legal Decision Matching aims to\nidentify legal clues within criminal fact descriptions to align them with the\nchain of reasoning that contains the correct legal decision. Our experiments on\nthe CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further\nanalysis reveals that LegalDuet improves the ability of pretrained language\nmodels to distinguish confusing criminal charges by reducing prediction\nuncertainty and enhancing the separability of criminal charges. The experiments\ndemonstrate that LegalDuet produces a more concentrated and distinguishable\nembedding space, effectively aligning criminal facts with corresponding legal\ndecisions. The code is available at https://github.com/NEUIR/LegalDuet.", "AI": {"tldr": "LegalDuet is a method designed to improve Legal Judgment Prediction by tailoring language models to better distinguish between subtle differences in legal judgments.", "motivation": "Existing Legal Judgment Prediction models fail to effectively differentiate subtle differences in judgments, hampering their predictive accuracy.", "method": "LegalDuet employs a dual-view mechanism that includes Law Case Clustering for contrastive training and Legal Decision Matching to align criminal facts with legal decisions.", "result": "Experiments on the CAIL2018 dataset show that LegalDuet enhances the ability of models to distinguish confusing criminal charges, reducing prediction uncertainty and improving the separability of charges.", "conclusion": "LegalDuet provides a more tailored embedding space for legal cases, resulting in better prediction outcomes in legal judgment tasks.", "key_contributions": ["Introduction of a dual-view mechanism to improve legal judgment predictions.", "Demonstrated effectiveness on the CAIL2018 dataset.", "Reduction of prediction uncertainty in legal outcomes."], "limitations": "", "keywords": ["Legal Judgment Prediction", "machine learning", "language models", "legal AI", "criminal law"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2402.08498", "pdf": "https://arxiv.org/pdf/2402.08498.pdf", "abs": "https://arxiv.org/abs/2402.08498", "title": "\"Reasoning\" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments", "authors": ["Preetika Verma", "Kokil Jaidka", "Svetlana Churina"], "categories": ["cs.CL"], "comment": "22 pages, 9 figures, 13 tables", "summary": "Large language models (LLMs) play a key role in generating evidence-based and\nstylistic counter-arguments, yet their effectiveness in real-world applications\nhas been underexplored. Previous research often neglects the balance between\nevidentiality and style, which are crucial for persuasive arguments. To address\nthis, we evaluated the effectiveness of stylized evidence-based\ncounter-argument generation in Counterfire, a new dataset of 38,000\ncounter-arguments generated by revising counter-arguments to Reddit's\nChangeMyView community to follow different discursive styles. We evaluated\ngeneric and stylized counter-arguments from basic and fine-tuned models such as\nGPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku,\nLLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings\nreveal that humans prefer stylized counter-arguments over the original outputs,\nwith GPT-3.5 Turbo performing well, though still not reaching human standards\nof rhetorical quality nor persuasiveness. Additionally, our work created a\nnovel argument triplets dataset for studying style control, with human\npreference labels that provide insights into the tradeoffs between evidence\nintegration and argument quality.", "AI": {"tldr": "This paper investigates the effectiveness of stylized evidence-based counter-argument generation using a new dataset and evaluates models on their rhetorical quality and persuasiveness.", "motivation": "The effectiveness of large language models in generating persuasive counter-arguments is underexplored, particularly regarding the balance between evidentiality and stylistic quality.", "method": "The researchers evaluated stylized counter-arguments generated from a dataset of 38,000 examples derived from Reddit's ChangeMyView community, using various models including GPT-3.5 and GPT-4o, assessing their rhetorical quality and persuasiveness through human preference studies.", "result": "Humans preferred stylized counter-arguments over original outputs, with GPT-3.5 Turbo achieving decent results but not matching human quality.", "conclusion": "This study highlighted the importance of style in persuasive argumentation and introduced a new dataset for further research on style control in counter-argument generation.", "key_contributions": ["Evaluation of stylistic counter-arguments in LLMs", "Creation of a novel dataset with preference labels", "Insights into trade-offs between evidence and argument quality"], "limitations": "The models did not reach human standards of rhetorical quality nor persuasiveness.", "keywords": ["large language models", "counter-arguments", "rhetorical quality"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.04532", "pdf": "https://arxiv.org/pdf/2405.04532.pdf", "abs": "https://arxiv.org/abs/2405.04532", "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving", "authors": ["Yujun Lin", "Haotian Tang", "Shang Yang", "Zhekai Zhang", "Guangxuan Xiao", "Chuang Gan", "Song Han"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve", "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.", "AI": {"tldr": "This paper introduces QoQ, a quantization algorithm designed to enhance the performance of large language models (LLMs) during inference by reducing runtime overheads associated with INT4 quantization techniques, leading to significant throughput improvements.", "motivation": "Existing INT4 quantization methods incur excessive runtime overhead in cloud-based LLM serving, particularly when dequantizing weights or partial sums on GPUs. This limits performance gains and cost-effectiveness.", "method": "QoQ quantization algorithm utilizes 4-bit weights, 8-bit activations, and 4-bit KV cache. It incorporates progressive quantization and SmoothAttention to reduce dequantization latency and mitigate accuracy loss, all implemented in the QServe inference library.", "result": "QoQ improved maximum serving throughput of Llama-3-8B by 1.2x on A100 and 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100 and 3.5x on L40S compared to TensorRT-LLM, achieving up to 3x cost reduction in LLM serving.", "conclusion": "QoQ's efficient use of quantization techniques and CUDA core operations allows for greater throughput and reduced costs in cloud-based LLM serving, outperforming existing methods significantly.", "key_contributions": ["Introduction of QoQ algorithm for efficient LLM inference", "Implementation of SmoothAttention to address accuracy degradation", "Development of the QServe library for optimized serving throughput"], "limitations": "", "keywords": ["Quantization", "Large Language Models", "LLM Inference", "Performance Optimization", "CUDA"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.11804", "pdf": "https://arxiv.org/pdf/2405.11804.pdf", "abs": "https://arxiv.org/abs/2405.11804", "title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts", "authors": ["Minghao Wu", "Jiahao Xu", "Yulin Yuan", "Gholamreza Haffari", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL"], "comment": "To appear at TACL", "summary": "Literary translation remains one of the most challenging frontiers in machine\ntranslation due to the complexity of capturing figurative language, cultural\nnuances, and unique stylistic elements. In this work, we introduce TransAgents,\na novel multi-agent framework that simulates the roles and collaborative\npractices of a human translation company, including a CEO, Senior Editor,\nJunior Editor, Translator, Localization Specialist, and Proofreader. The\ntranslation process is divided into two stages: a preparation stage where the\nteam is assembled and comprehensive translation guidelines are drafted, and an\nexecution stage that involves sequential translation, localization,\nproofreading, and a final quality check. Furthermore, we propose two innovative\nevaluation strategies: Monolingual Human Preference (MHP), which evaluates\ntranslations based solely on target language quality and cultural\nappropriateness, and Bilingual LLM Preference (BLP), which leverages large\nlanguage models like GPT-4} for direct text comparison. Although TransAgents\nachieves lower d-BLEU scores, due to the limited diversity of references, its\ntranslations are significantly better than those of other baselines and are\npreferred by both human evaluators and LLMs over traditional human references\nand GPT-4} translations. Our findings highlight the potential of multi-agent\ncollaboration in enhancing translation quality, particularly for longer texts.", "AI": {"tldr": "TransAgents is a multi-agent framework for literary translation that simulates a human translation company's roles and practices, enhancing translation quality through collaboration and innovative evaluation strategies.", "motivation": "Literary translation is complex due to figurative language, cultural nuances, and stylistic elements, making it a challenging area for machine translation.", "method": "The framework divides the translation process into two stages: preparation (team assembly and guideline drafting) and execution (sequential translation, localization, proofreading, and quality checks). It employs two evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP).", "result": "TransAgents yields translations that, despite lower d-BLEU scores due to limited reference diversity, are preferred by human evaluators and LLMs compared to traditional references and GPT-4 translations.", "conclusion": "The research demonstrates that multi-agent collaboration can significantly enhance translation quality, especially for longer texts.", "key_contributions": ["Introduction of the TransAgents framework for literary translation", "Two novel evaluation strategies: MHP and BLP", "Evidence that collaborative practices improve translation quality."], "limitations": "Achieves lower d-BLEU scores due to limited diversity of reference translations.", "keywords": ["Literary translation", "multi-agent framework", "translation evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2407.20906", "pdf": "https://arxiv.org/pdf/2407.20906.pdf", "abs": "https://arxiv.org/abs/2407.20906", "title": "Automated Review Generation Method Based on Large Language Models", "authors": ["Shican Wu", "Xiao Ma", "Dehui Luo", "Lulu Li", "Xiangcheng Shi", "Xin Chang", "Xiaoyun Lin", "Ran Luo", "Chunlei Pei", "Changying Du", "Zhi-Jian Zhao", "Jinlong Gong"], "categories": ["cs.CL", "cs.AI", "physics.data-an"], "comment": "Code: https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024 Published at https://doi.org/10.1093/nsr/nwaf169 for newer\n  edition", "summary": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.", "AI": {"tldr": "Automated review generation using large language models (LLMs) addresses the challenge of handling vast scientific literature, enhancing productivity while reducing cognitive load.", "motivation": "The increasing volume of literature in scientific research overwhelms researchers' abilities to process information effectively, necessitating an efficient solution.", "method": "An automated review generation method leveraging LLMs was developed, tested on 343 articles related to propane dehydrogenation (PDH) catalysts, allowing for rapid analysis and review creation.", "result": "The generated reviews matched or surpassed manual quality, with expert validation confirming high accuracy and minimal hallucination risk (below 0.5%).", "conclusion": "This method improves research productivity and literature management, allowing researchers to generate quality reviews effortlessly, and has potential applications across various research fields.", "key_contributions": ["Development of an automated review generation method using LLMs.", "Demonstrated efficacy through statistical validation with expert verification.", "Released a Windows application for one-click review generation."], "limitations": "", "keywords": ["automated review generation", "large language models", "research productivity", "literature analysis", "cognitive load"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2410.01957", "pdf": "https://arxiv.org/pdf/2410.01957.pdf", "abs": "https://arxiv.org/abs/2410.01957", "title": "Challenges and Future Directions of Data-Centric AI Alignment", "authors": ["Min-Hsuan Yeh", "Jeffrey Wang", "Xuefeng Du", "Seongheon Park", "Leitian Tao", "Shawn Im", "Yixuan Li"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "As AI systems become increasingly capable and influential, ensuring their\nalignment with human values, preferences, and goals has become a critical\nresearch focus. Current alignment methods primarily focus on designing\nalgorithms and loss functions but often underestimate the crucial role of data.\nThis paper advocates for a shift towards data-centric AI alignment, emphasizing\nthe need to enhance the quality and representativeness of data used in aligning\nAI systems. In this position paper, we highlight key challenges associated with\nboth human-based and AI-based feedback within the data-centric alignment\nframework. Through qualitative analysis, we identify multiple sources of\nunreliability in human feedback, as well as problems related to temporal drift,\ncontext dependence, and AI-based feedback failing to capture human values due\nto inherent model limitations. We propose future research directions, including\nimproved feedback collection practices, robust data-cleaning methodologies, and\nrigorous feedback verification processes. We call for future research into\nthese critical directions to ensure, addressing gaps that persist in\nunderstanding and improving data-centric alignment practices.", "AI": {"tldr": "This paper advocates for a data-centric approach to AI alignment, emphasizing the importance of data quality and representativeness in ensuring AI systems align with human values.", "motivation": "To address the critical need for AI systems to align with human values amidst increasing capabilities of AI.", "method": "Qualitative analysis of human and AI feedback in the context of data-centric alignment, identifying challenges and sources of unreliability.", "result": "Identified multiple reliability issues in human feedback and problems with AI feedback capturing human values, suggesting gaps in current understanding.", "conclusion": "Calls for improved feedback practices, robust data-cleaning methods, and verification processes to enhance data-centric alignment.", "key_contributions": ["Emphasis on the importance of data in AI alignment", "Identifies sources of unreliability in feedback mechanisms", "Proposes future research directions for improving data-centric alignment"], "limitations": "Limited to qualitative analysis, may not capture broader quantitative trends.", "keywords": ["AI alignment", "data-centric approach", "human feedback", "AI feedback", "research directions"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2410.20774", "pdf": "https://arxiv.org/pdf/2410.20774.pdf", "abs": "https://arxiv.org/abs/2410.20774", "title": "Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation", "authors": ["Dongryeol Lee", "Yerin Hwang", "Yongil Kim", "Joonsuk Park", "Kyomin Jung"], "categories": ["cs.CL"], "comment": "NAACL 2025 Oral (21 pages, 6 figures, 15 tables)", "summary": "In line with the principle of honesty, there has been a growing effort to\ntrain large language models (LLMs) to generate outputs containing epistemic\nmarkers. However, evaluation in the presence of epistemic markers has been\nlargely overlooked, raising a critical question: Could the use of epistemic\nmarkers in LLM-generated outputs lead to unintended negative consequences? To\naddress this, we present EMBER, a benchmark designed to assess the robustness\nof LLM-judges to epistemic markers in both single and pairwise evaluation\nsettings. Our findings, based on evaluations using EMBER, reveal that all\ntested LLM-judges, including GPT-4o, show a notable lack of robustness in the\npresence of epistemic markers. Specifically, we observe a negative bias toward\nepistemic markers, with a stronger bias against markers expressing uncertainty.\nThis suggests that LLM-judges are influenced by the presence of these markers\nand do not focus solely on the correctness of the content.", "AI": {"tldr": "The paper presents EMBER, a benchmark for evaluating the impact of epistemic markers on large language models (LLMs), revealing biases against uncertainty markers in LLM judgments.", "motivation": "To investigate the negative consequences of LLMs generating outputs with epistemic markers, especially given the lack of evaluation in this area.", "method": "The authors created a benchmark called EMBER to assess LLM-judges' robustness regarding epistemic markers in both single and pairwise evaluation settings.", "result": "Evaluations showed that all tested LLM-judges, including GPT-4o, exhibited significant bias against epistemic markers, especially those indicating uncertainty.", "conclusion": "The presence of epistemic markers influences LLM-judges, causing them to focus less on content correctness and more on the markers themselves.", "key_contributions": ["Introduction of the EMBER benchmark for LLM evaluation", "Evidence of bias in LLM-judges against epistemic markers", "Insights into LLM behavior concerning uncertainty representations"], "limitations": "The study focuses primarily on LLM-judges and does not explore the broader implications for end-users or the generation process.", "keywords": ["epistemic markers", "LLM evaluation", "bias", "robustness", "language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.05862", "pdf": "https://arxiv.org/pdf/2412.05862.pdf", "abs": "https://arxiv.org/abs/2412.05862", "title": "Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis", "authors": ["Aman Kassahun Wassie", "Mahdi Molaei", "Yasmin Moslem"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language directions with varied resource\navailability: English-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs demonstrate a significant\nquality gap in specialized translation compared to multilingual encoder-decoder\nMT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms\nall evaluated LLMs in the 7-8B parameter range across three out of the four\nlanguage directions. While fine-tuning improves the performance of LLMs such as\nMistral and Llama, these models still underperform compared to fine-tuned\nNLLB-200 3.3B models. Our findings highlight the ongoing need for specialized\nMT models to achieve high-quality domain-specific translation, especially in\nmedium-resource and low-resource settings. Moreover, the superior performance\nof larger LLMs over their 8B variants suggests potential value in pre-training\ndomain-specific medium-sized language models, employing targeted data selection\nand knowledge distillation approaches to enhance both quality and efficiency in\nspecialized translation tasks.", "AI": {"tldr": "This study compares the translation performance of open-source autoregressive LLMs and task-oriented MT models in the medical domain, finding LLMs lag behind specialized MT models for domain-specific tasks.", "motivation": "The research aims to evaluate the translation capabilities of LLMs versus specialized MT models in the medical domain, addressing the need for effective translation in low-resource language settings.", "method": "The authors conducted experiments using LLMs and multilingual encoder-decoder models on four language pairs related to medical translation, assessing performance differences.", "result": "NLLB-200 3.3B outperformed evaluated LLMs in the 7-8B parameter range in three language directions, showing a notable performance gap despite fine-tuning efforts on LLMs like Mistral and Llama.", "conclusion": "The results emphasize the necessity of specialized MT models for high-quality domain-specific translations, particularly in low-resource languages, and suggest avenues for pre-training medium-sized models for improved performance.", "key_contributions": ["Comparison of LLMs and specialized MT models in the medical translation domain", "Demonstration of significant performance gaps in translation quality", "Recommendations for focusing on specialized domain-specific training and model selection"], "limitations": "", "keywords": ["large language models", "machine translation", "domain-specific translation", "medical domain", "language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13947", "pdf": "https://arxiv.org/pdf/2501.13947.pdf", "abs": "https://arxiv.org/abs/2501.13947", "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods", "authors": ["Wenli Yang", "Lilian Some", "Michael Bain", "Byeong Kang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.", "AI": {"tldr": "This paper surveys the integration of Large Language Models (LLMs) with structured knowledge-based systems, discussing applications, challenges, and insights for advancing AI technologies.", "motivation": "To explore how LLMs can enhance structured knowledge systems by combining generative capabilities with precise knowledge representation.", "method": "Comprehensive literature examination to identify issues and assess existing solutions regarding LLMs and knowledge bases.", "result": "Demonstrates benefits of LLM integration in terms of data contextualization, model accuracy, and resource utilization, while identifying research gaps.", "conclusion": "Insights contribute to advancing AI technologies and support practical deployment across various sectors.", "key_contributions": ["Integrates LLMs with structured knowledge systems for improved accuracy and contextualization.", "Identifies technical, operational, and ethical challenges in implementation.", "Proposes paths for future research and application improvement."], "limitations": "Limited to examining existing literature without primary data collection or experimental validation.", "keywords": ["Large Language Models", "Knowledge Bases", "Artificial Intelligence", "Ethical Challenges", "Data Contextualization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.05945", "pdf": "https://arxiv.org/pdf/2502.05945.pdf", "abs": "https://arxiv.org/abs/2502.05945", "title": "HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models", "authors": ["Paul Darm", "Annalisa Riccardi"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Large Language Models (LLMs), Interference-time activation shifting,\n  Steerability, Explainability, AI alignment, Interpretability", "summary": "Robust alignment guardrails for large language models are becoming\nincreasingly important with their widespread application. In contrast to\nprevious studies, we demonstrate that inference-time activation interventions\ncan bypass safety alignments and effectively steer model generations towards\nharmful AI coordination for Llama 2. Our method applies fine-grained\ninterventions at specific model subcomponents, particularly attention heads,\nusing a simple binary choice probing strategy. These interventions then\ngeneralise to the open-ended generation setting effectively circumventing\nsafety guardrails. We show that probing single attention heads is more\neffective than intervening on full layers and intervening on only four\nattention heads is comparable to supervised fine-tuning. We further show that\nonly a few example completions are needed to compute effective steering\ndirections, which is an advantage over classical fine-tuning. Our findings\nhighlight the shortcomings of current alignment techniques. In addition, our\nresults suggest that, at the attention head level, activations encode\nfine-grained linearly separable behaviors. Practically, the approach offers a\nstraightforward methodology to steer large language model behaviour, which\ncould be extended to diverse domains beyond safety requiring fine-grained\ncontrol over the model output. The code and datasets for this study can be\nfound on https://github.com/PaulDrm/targeted_intervention.", "AI": {"tldr": "This paper explores how inference-time activation interventions can effectively bypass safety alignments in large language models, particularly Llama 2, through targeted adjustments at the attention head level.", "motivation": "To address the growing need for robust alignment guardrails in large language models amidst their widespread application while highlighting the shortcomings of existing alignment techniques.", "method": "The study employs a method of fine-grained interventions at specific model subcomponents, mainly focusing on attention heads using a binary choice probing strategy to steer model generations.", "result": "The findings demonstrate that intervening on single attention heads is more effective than on full layers, and that only a few example completions are needed for efficient steering directions, rivaling supervised fine-tuning.", "conclusion": "The approach presents a simple methodology to influence large language model behavior, showing potential for expansion into other domains requiring detailed control over output.", "key_contributions": ["Demonstrated activation interventions can bypass safety alignments in LLMs.", "Introduced a probing strategy that is effective at the attention head level.", "Showed that fine-grained interventions require fewer examples compared to classical fine-tuning."], "limitations": "", "keywords": ["Large Language Models", "Inference-time activation shifting", "Steerability", "AI alignment", "Interpretability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.07963", "pdf": "https://arxiv.org/pdf/2502.07963.pdf", "abs": "https://arxiv.org/abs/2502.07963", "title": "Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?", "authors": ["Hye Sun Yun", "Karen Y. C. Zhang", "Ramez Kouzy", "Iain J. Marshall", "Junyi Jessy Li", "Byron C. Wallace"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures, 4 tables", "summary": "Medical research faces well-documented challenges in translating novel\ntreatments into clinical practice. Publishing incentives encourage researchers\nto present \"positive\" findings, even when empirical results are equivocal.\nConsequently, it is well-documented that authors often spin study results,\nespecially in article abstracts. Such spin can influence clinician\ninterpretation of evidence and may affect patient care decisions. In this\nstudy, we ask whether the interpretation of trial results offered by Large\nLanguage Models (LLMs) is similarly affected by spin. This is important since\nLLMs are increasingly being used to trawl through and synthesize published\nmedical evidence. We evaluated 22 LLMs and found that they are across the board\nmore susceptible to spin than humans. They might also propagate spin into their\noutputs: We find evidence, e.g., that LLMs implicitly incorporate spin into\nplain language summaries that they generate. We also find, however, that LLMs\nare generally capable of recognizing spin, and can be prompted in a way to\nmitigate spin's impact on LLM outputs.", "AI": {"tldr": "This study examines whether Large Language Models (LLMs) are affected by 'spin' in medical research findings and how they interpret such findings compared to humans.", "motivation": "Medical research struggles with translating treatments into clinical practice due to 'spin' in published results, which influences clinician decisions. Understanding LLMs' susceptibility to this spin is crucial as they synthesize medical evidence.", "method": "Evaluated 22 LLMs to assess their susceptibility to spin in medical research abstracts and their ability to recognize and mitigate spin in outputs.", "result": "LLMs were found to be more susceptible to spin than humans and may incorporate spin into their generated outputs, but can also be prompted to recognize and mitigate its impact.", "conclusion": "The findings highlight the need for critical evaluation of LLM outputs in medical contexts due to their susceptibility to spin, although they can recognize it under certain prompts.", "key_contributions": ["Uncovered the susceptibility of LLMs to spin in medical research findings.", "Demonstrated that LLMs can propagate spin into generated summaries.", "Provided strategies to prompt LLMs to mitigate the impact of spin."], "limitations": "The study is limited to a set of 22 LLMs and may not generalize beyond the models tested.", "keywords": ["Large Language Models", "spin", "medical research", "clinical practice", "evidence synthesis"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.20984", "pdf": "https://arxiv.org/pdf/2502.20984.pdf", "abs": "https://arxiv.org/abs/2502.20984", "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation", "authors": ["Thanet Markchom", "Tong Wu", "Liting Huang", "Huizhi Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.", "AI": {"tldr": "The paper addresses image ranking based on idiomatic nominal compounds in English and Brazilian Portuguese using LLMs and multilingual CLIP models.", "motivation": "To improve image ranking by enhancing the understanding of idiomatic nominal compounds' meanings, thus addressing the challenges in SemEval-2025 Task 1.", "method": "Utilizing generative LLMs to generate idiomatic meanings for nominal compounds, followed by encoding these meanings with multilingual CLIP models. Employing contrastive learning and data augmentation for fine-tuning embeddings.", "result": "Multimodal representations from the proposed method outperform those based solely on original compounds, although fine-tuning was less effective than using embeddings directly without fine-tuning.", "conclusion": "The use of LLMs and CLIP models significantly enhances the semantic interpretation of idiomatic compounds for image ranking tasks, with room for further optimization in fine-tuning methodologies.", "key_contributions": ["Introduced the use of generative LLMs for generating idiomatic meanings.", "Demonstrated the effectiveness of multilingual CLIP models in image ranking.", "Provided a methodology for contrastive learning and data augmentation applied to LLM representations."], "limitations": "The fine-tuning of embeddings did not yield better results than using embeddings directly without fine-tuning.", "keywords": ["idiomatic meanings", "multimodal representations", "image ranking", "large language models", "CLIP models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895.pdf", "abs": "https://arxiv.org/abs/2503.23895", "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint. Code is available at https://github.com/Trae1ounG/DyPRAG", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.", "AI": {"tldr": "Dynamic Parametric RAG (DyPRAG) enhances Retrieval-augmented Generation (RAG) for large language models (LLMs) by efficiently converting documents into parametric knowledge, reducing inference costs and mitigating RAG hallucination.", "motivation": "Existing RAG methods improve LLM reliability but suffer from high inference costs and hallucination issues due to lack of parametric knowledge.", "method": "DyPRAG uses a lightweight parameter translator model to convert documents into parametric knowledge at test-time, addressing the limitations of Parametric RAG (PRAG).", "result": "DyPRAG significantly reduces inference, training, and storage costs while enhancing LLM knowledge dynamically, demonstrating effectiveness across multiple datasets.", "conclusion": "DyPRAG offers a practical solution to improve knowledge fusion in LLMs, reducing hallucination and enabling superior performance on real-world tasks.", "key_contributions": ["Introduction of a lightweight parameter translator for dynamic knowledge enhancement", "Significant reduction in inference, training, and storage costs", "Demonstration of improved generalization capabilities across multiple datasets."], "limitations": "High training costs and storage needs of the original PRAG can still be a concern, potential limitations in extremely large datasets.", "keywords": ["Retrieval-augmented generation", "Dynamic Parametric RAG", "large language models", "knowledge enhancement", "hallucination"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2504.00027", "pdf": "https://arxiv.org/pdf/2504.00027.pdf", "abs": "https://arxiv.org/abs/2504.00027", "title": "Opioid Named Entity Recognition (ONER-2025) from Reddit", "authors": ["Grigori Sidorov", "Muhammad Ahmad", "Iqra Ameer", "Muhammad Usman", "Ildar Batyrshin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).", "AI": {"tldr": "The paper addresses the opioid overdose epidemic using NLP techniques to analyze Reddit discussions, creating a dataset and a monitoring system to identify overdose events.", "motivation": "To address the critical public health crisis posed by the opioid overdose epidemic and gain insights from social media data.", "method": "The study utilizes Natural Language Processing (NLP) for Opioid Named Entity Recognition (ONER-2025) to extract information from Reddit discussions on opioid use, featuring an annotated dataset and a real-time monitoring system.", "result": "The research produced a manually annotated dataset consisting of 331,285 tokens representing self-reported opioid use experiences across multiple categories and achieved high performance in identifying opioid-related terms.", "conclusion": "The proposed monitoring system effectively integrates machine learning and NLP to identify overdose events in real time, significantly outperforming baseline models.", "key_contributions": ["Creation of a unique, annotated dataset from Reddit on opioid use", "Detailed description of the dataset annotation process", "Development of a real-time monitoring system for overdose event identification"], "limitations": "", "keywords": ["Opioid Overdose", "Natural Language Processing", "Social Media Analysis", "Real-time Monitoring", "Public Health"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194.pdf", "abs": "https://arxiv.org/abs/2504.14194", "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "categories": ["cs.CL"], "comment": "Under review", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose PRRC to\nevaluate data quality across Professionalism, Readability, Reasoning, and\nCleanliness. We further introduce Meta-rater, a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with scalable benefits observed in 3.3B models\ntrained on 100B tokens. Additionally, we release the annotated SlimPajama-627B\ndataset, labeled across 25 quality metrics (including PRRC), to advance\nresearch in data-centric LLM development. Our work establishes that holistic,\nmulti-dimensional quality integration significantly outperforms conventional\nsingle-dimension approaches, offering a scalable paradigm for enhancing\npre-training efficiency and model capability.", "AI": {"tldr": "The paper proposes a multi-dimensional data quality evaluation method, PRRC, and introduces Meta-rater for selecting training datasets for large language models, leading to improved model performance and convergence speed.", "motivation": "To enhance the transparency and quality of pre-training datasets for large language models by addressing the limitations of existing single-dimensional data selection methods.", "method": "Introduction of PRRC for evaluating data quality across four dimensions (Professionalism, Readability, Reasoning, and Cleanliness) and the development of the Meta-rater, which uses multi-dimensional quality metrics to optimize data selection for training.", "result": "Experiments show that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23 with scalable advantages noted in larger models trained on extensive datasets.", "conclusion": "The study demonstrates that a holistic, multi-dimensional approach to data quality significantly beats traditional methods, providing a scalable strategy for enhancing the training of large language models.", "key_contributions": ["Introduction of PRRC for multi-dimensional data quality evaluation", "Development of Meta-rater for optimal data selection", "Release of the SlimPajama-627B dataset with extensive quality labeling"], "limitations": "", "keywords": ["data quality", "large language models", "pre-training datasets", "meta-learning", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060.pdf", "abs": "https://arxiv.org/abs/2504.16060", "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "categories": ["cs.CL"], "comment": "Homepage: https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication.", "AI": {"tldr": "This paper revisits Referring Expression Generation (REG) in vision-language systems, emphasizing the need for pragmatic evaluations and introducing a new dataset for assessment.", "motivation": "Current evaluations of vision-language models often ignore pragmatic elements of referring expressions, leading to incomplete assessments of their communicative competence.", "method": "The authors introduce a new dataset of 1.5k images with written and spoken referring expressions, systematically evaluating state-of-the-art vision-language models against these new criteria.", "result": "Three key failures in pragmatic competence were identified: failing to uniquely identify referents, providing excessive or irrelevant information, and misalignment with human preferences for minimal spatial cues.", "conclusion": "The paper advocates for a focus on developing models and evaluation methods that prioritize pragmatic success and real human communication.", "key_contributions": ["Introduction of the RefOI dataset for pragmatic evaluation in REG", "Identification of key failures in current vision-language models", "Highlighting the inadequacy of standard automatic evaluations for capturing pragmatic nuances."], "limitations": "The proposed dataset and findings may require further validation across diverse contexts and applications.", "keywords": ["Referring Expression Generation", "Vision-Language Models", "Pragmatics", "Human Communication"], "importance_score": 8, "read_time_minutes": 15}}
