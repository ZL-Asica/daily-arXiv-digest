{"id": "2506.21555", "pdf": "https://arxiv.org/pdf/2506.21555.pdf", "abs": "https://arxiv.org/abs/2506.21555", "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively."}
{"id": "2506.21556", "pdf": "https://arxiv.org/pdf/2506.21556.pdf", "abs": "https://arxiv.org/abs/2506.21556", "title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "authors": ["Hyeongcheol Park", "MinHyuk Jang", "Ha Dam Baek", "Gyusam Chang", "Jiyoung Seo", "Jiwan Park", "Hogun Park", "Sangpil Kim"], "categories": ["cs.CL"], "comment": "Project Page: https://vatkg.github.io/", "summary": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge\nacross multiple modalities, play a pivotal role by complementing the implicit\nknowledge of Multimodal Large Language Models (MLLMs) and enabling more\ngrounded reasoning via Retrieval Augmented Generation (RAG). However, existing\nMMKGs are generally limited in scope: they are often constructed by augmenting\npre-existing knowledge graphs, which restricts their knowledge, resulting in\noutdated or incomplete knowledge coverage, and they often support only a narrow\nrange of modalities, such as text and visual information. These limitations\nreduce their extensibility and applicability to a broad range of multimodal\ntasks, particularly as the field shifts toward richer modalities such as video\nand audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text\nKnowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive\nmultimodal knowledge graph that covers visual, audio, and text information,\nwhere each triplet is linked to multimodal data and enriched with detailed\ndescriptions of concepts. Specifically, our construction pipeline ensures\ncross-modal knowledge alignment between multimodal data and fine-grained\nsemantics through a series of stringent filtering and alignment steps, enabling\nthe automatic generation of MMKGs from any multimodal dataset. We further\nintroduce a novel multimodal RAG framework that retrieves detailed\nconcept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate\nthe effectiveness of VAT-KG in supporting MLLMs, highlighting its practical\nvalue in unifying and leveraging multimodal knowledge."}
{"id": "2506.21557", "pdf": "https://arxiv.org/pdf/2506.21557.pdf", "abs": "https://arxiv.org/abs/2506.21557", "title": "Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning", "authors": ["Kaiying Yan", "Moyang Liu", "Yukun Liu", "Ruibo Fu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid spread of fake news across multimedia platforms presents serious\nchallenges to information credibility. In this paper, we propose a\nDebunk-and-Infer framework for Fake News Detection(DIFND) that leverages\ndebunking knowledge to enhance both the performance and interpretability of\nfake news detection. DIFND integrates the generative strength of conditional\ndiffusion models with the collaborative reasoning capabilities of multimodal\nlarge language models (MLLMs). Specifically, debunk diffusion is employed to\ngenerate refuting or authenticating evidence based on the multimodal content of\nnews videos, enriching the evaluation process with diverse yet semantically\naligned synthetic samples. To improve inference, we propose a chain-of-debunk\nstrategy where a multi-agent MLLM system produces logic-grounded,\nmultimodal-aware reasoning content and final veracity judgment. By jointly\nmodeling multimodal features, generative debunking cues, and reasoning-rich\nverification within a unified architecture, DIFND achieves notable improvements\nin detection accuracy. Extensive experiments on the FakeSV and FVC datasets\nshow that DIFND not only outperforms existing approaches but also delivers\ntrustworthy decisions."}
{"id": "2506.21558", "pdf": "https://arxiv.org/pdf/2506.21558.pdf", "abs": "https://arxiv.org/abs/2506.21558", "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter MÃ¼hlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch."}
{"id": "2506.21762", "pdf": "https://arxiv.org/pdf/2506.21762.pdf", "abs": "https://arxiv.org/abs/2506.21762", "title": "ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues", "authors": ["Oliver Huang", "Carolina Nobre"], "categories": ["cs.HC"], "comment": "VIS 2025", "summary": "Data visualization tasks often require multi-step reasoning, and the\ninterpretive strategies experts use, such as decomposing complex goals into\nsmaller subtasks and selectively attending to key chart regions are rarely made\nexplicit. ViStruct is an automated pipeline that simulates these expert\nbehaviours by breaking high-level visual questions into structured analytic\nsteps and highlighting semantically relevant chart areas. Leveraging large\nlanguage and vision-language models, ViStruct identifies chart components, maps\nsubtasks to spatial regions, and presents visual attention cues to externalize\nexpert-like reasoning flows. While not designed for direct novice instruction,\nViStruct provides a replicable model of expert interpretation that can inform\nthe development of future visual literacy tools. We evaluate the system on 45\ntasks across 12 chart types and validate its outputs with trained visualization\nusers, confirming its ability to produce interpretable and expert-aligned\nreasoning sequences."}
{"id": "2506.21559", "pdf": "https://arxiv.org/pdf/2506.21559.pdf", "abs": "https://arxiv.org/abs/2506.21559", "title": "GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations", "authors": ["Junze Chen", "Cheng Yang", "Shujie Li", "Zhiqiang Zhang", "Yawen Li", "Junping Du", "Chuan Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated their strong capabilities in\nvarious domains, and have been recently integrated for graph analysis as graph\nlanguage models (GLMs). With LLMs as the predictor, some GLMs can interpret\nunseen tasks described by natural language, and learn from a few examples in\nthe prompts without parameter tuning, known as in-context learning (ICL).\nAnother subset of GLMs utilizes abundant training labels to enhance model\nperformance, known as instruction tuning. However, we argue that ICL on graphs\nhas effectiveness issues due to fixed parameters and efficiency issues due to\nlong context. Meanwhile, the large amount of labeled data required for\ninstruction tuning can be difficult to obtain in real-world scenarios. To this\nend, we aim to introduce an extra parameter adaptation stage that can\nefficiently tailor GLMs to an unseen graph and task with only a few labeled\nexamples, in exchange for better prediction accuracy and faster inference\nspeed. For implementation, in this paper we propose GraphLAMA method, with its\nmodel backbone and learning schemes specialized for efficient tuning and\ninference. Specifically, for model backbone, we use a graph neural network\n(GNN) with several well-designed components to transform nodes into the\nrepresentation space of LLM tokens. Task instructions can then be represented\nas a mixture of node and language tokens. In the pre-training stage, model\nparameters except the LLM will be trained with different tasks to capture\ngeneral knowledge. In the adaptation stage, only a few pre-trained parameters\nwill be updated based on few-shot examples. Extensive experiments on\nfew/zero-shot node classification and summary generation show that our proposed\nGraphLAMA achieves state-of-the-art performance with 4.91% absolution\nimprovement in accuracy. Compared with ICL, our inference speed can be 10 times\nfaster under 5-shot setting."}
{"id": "2506.21780", "pdf": "https://arxiv.org/pdf/2506.21780.pdf", "abs": "https://arxiv.org/abs/2506.21780", "title": "Avatars and Environments for Meetings in Social VR: What Styles and Choices Matter to People in Group Creativity Tasks?", "authors": ["Anya Osborne", "Sabrina Fielder", "Lee Taber", "Tara Lamb", "Joshua McVeigh-Schultz", "Katherine Isbister"], "categories": ["cs.HC"], "comment": null, "summary": "Due to the COVID-19 pandemic, many professional entities shifted toward\nremote collaboration and video conferencing (VC) tools. Social virtual reality\n(VR) platforms present an alternative to VC for meetings and collaborative\nactivities. Well-crafted social VR environments could enhance feelings of\nco-presence and togetherness at meetings, helping reduce the need for\ncarbon-intensive travel to face-to-face meetings. This research contributes to\ncreating meeting tools in VR by exploring the effects of avatar styles and\nvirtual environments on groups creative performance using the Mozilla Hubs\nplatform. We present the results of two sequential studies. Study One surveys\navatar and environment preferences in various VR meeting contexts (N=87). Study\nTwo applies these findings to the design of a between-subjects and\nwithin-subjects research where participants (N=40) perform creativity tasks in\npairs as embodied avatars in different virtual settings using VR headsets. We\ndiscuss the design implications of avatar appearances and meeting settings on\nteamwork."}
{"id": "2506.21560", "pdf": "https://arxiv.org/pdf/2506.21560.pdf", "abs": "https://arxiv.org/abs/2506.21560", "title": "Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning", "authors": ["Yifu Han", "Geo Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models."}
{"id": "2506.21814", "pdf": "https://arxiv.org/pdf/2506.21814.pdf", "abs": "https://arxiv.org/abs/2506.21814", "title": "Validation of the MySurgeryRisk Algorithm for Predicting Complications and Death after Major Surgery: A Retrospective Multicenter Study Using OneFlorida Data Trust", "authors": ["Yuanfang Ren", "Esra Adiyeke", "Ziyuan Guan", "Zhenhong Hu", "Mackenzie J Meni", "Benjamin Shickel", "Parisa Rashidi", "Tezcan Ozrazgat-Baslanti", "Azra Bihorac"], "categories": ["cs.HC"], "comment": "28 pages, 4 figures, 6 tables, 1 supplemental table", "summary": "Despite advances in surgical techniques and care, postoperative complications\nare prevalent and effects up to 15% of the patients who underwent a major\nsurgery. The objective of this study is to develop and validate models for\npredicting postoperative complications and death after major surgery on a large\nand multicenter dataset, following the previously validated MySurgeryRisk\nalgorithm. This retrospective, longitudinal and multicenter cohort analysis\nincluded 508,097 encounters from 366,875 adult inpatients who underwent major\nsurgeries and were admitted to healthcare institutions within the OneFlorida+\nnetwork between 01/01/2012 and 04/29/2023. We applied the validated feature\nselection and transformation approach in MySurgeryRisk models and redeveloped\neXtreme Gradient Boosting (XGBoost) models for predicting risk of postoperative\nacute kidney injury (AKI), need for intensive care unit (ICU) admission, need\nfor mechanical ventilation (MV) therapy and in-hospital mortality on a\ndevelopment set and evaluated the model performance on a validation set. Area\nunder the receiver operating characteristics curve values were obtained for\nneed for ICU admission, 0.93 (95% Confidence Interval [CI], 0.93-0.93); need\nfor MV, 0.94 (95% CI, 0.94-0.94); AKI, 0.92 (95% CI, 0.92-0.92); and\nin-hospital mortality, 0.95 (95% CI, 0.94-0.95). Area under the\nprecision-recall curve values were computed for need for ICU admission, 0.62\n(95% CI, 0.62-0.63); need for MV, 0.51 (95% CI, 0.49-0.52); AKI, 0.53 (95% CI,\n0.53-0.54); and in-hospital mortality, 0.26 (95% CI, 0.24-0.29). The\nperformance of these models is comparable to that of the previously validated\nMySurgeryRisk models, suggesting the enhanced generalizability of the models.\nPrimary procedure code and provider specialty consistently appeared as the top\ninfluential variables, providing valuable insights into the factors influencing\nsurgical outcomes."}
{"id": "2506.21561", "pdf": "https://arxiv.org/pdf/2506.21561.pdf", "abs": "https://arxiv.org/abs/2506.21561", "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", "authors": ["Emilio Barkett", "Olivia Long", "Madhavendra Thakur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite their widespread use in fact-checking, moderation, and high-stakes\ndecision-making, large language models (LLMs) remain poorly understood as\njudges of truth. This study presents the largest evaluation to date of LLMs'\nveracity detection capabilities and the first analysis of these capabilities in\nreasoning models. We had eight LLMs make 4,800 veracity judgments across\nseveral prompts, comparing reasoning and non-reasoning models. We find that\nrates of truth-bias, or the likelihood to believe a statement is true,\nregardless of whether it is actually true, are lower in reasoning models than\nin non-reasoning models, but still higher than human benchmarks. Most\nconcerning, we identify sycophantic tendencies in several advanced models\n(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an\nasymmetry in detection accuracy, performing well in truth accuracy but poorly\nin deception accuracy. This suggests that capability advances alone do not\nresolve fundamental veracity detection challenges in LLMs."}
{"id": "2506.21845", "pdf": "https://arxiv.org/pdf/2506.21845.pdf", "abs": "https://arxiv.org/abs/2506.21845", "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "authors": ["Zhuodi Cai"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity."}
{"id": "2506.21562", "pdf": "https://arxiv.org/pdf/2506.21562.pdf", "abs": "https://arxiv.org/abs/2506.21562", "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design."}
{"id": "2506.21896", "pdf": "https://arxiv.org/pdf/2506.21896.pdf", "abs": "https://arxiv.org/abs/2506.21896", "title": "Focus on the Experts: Co-designing an Augmented Reality Eye-Gaze Tracking System with Surgical Trainees to Improve Endoscopic Instruction", "authors": ["Jumanh Atoum", "Jinkyung Park", "Mamtaj Akter", "Nicholas Kavoussi", "Pamela Wisniewski", "Jie Ying Wu"], "categories": ["cs.HC"], "comment": null, "summary": "The current apprenticeship model for surgical training requires a high level\nof supervision, which does not scale well to meet the growing need for more\nsurgeons. Many endoscopic procedures are directly taught in the operating room\n(OR) while the attending surgeon and trainee operate on patients. The need to\nprioritize patient care limits the trainees' opportunities to experiment and\nreceive feedback on their performance. Augmented reality (AR) has the potential\nto increase efficiency in endoscopic surgical training, but additional research\nis critical to understanding the needs of surgical trainees to inform the\ndesign of AR training systems. Therefore, we worked with 18 surgical trainees\nto understand the strengths, limitations, and unmet needs of their current\ntraining environment and to co-design an AR eye-gaze tracking system based on\ntheir preferences. Trainees emphasized the need to practice the 2D to 3D\nmapping needed to properly familiarize oneself with the anatomy of patients to\nprepare for real surgery. The trainees felt that an AR-based eye gaze tracking\nsystem would be a useful supplemental training method that would improve their\nlearning in OR cases without detracting from patient care. To tailor the AR\nsystem to their needs, they co-designed features to improve their ability to\ntrack the attending surgeon's eye gaze and to provide a real-time, interactive\nsystem. Our results are valuable in shaping the endoscopic training modules by\ngenerating user-informed guidelines to design future collaborative AR-based\neye-gaze tracking systems."}
{"id": "2506.21563", "pdf": "https://arxiv.org/pdf/2506.21563.pdf", "abs": "https://arxiv.org/abs/2506.21563", "title": "FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models", "authors": ["Kaiying Kevin Lin", "Hsiyu Chen", "Haopeng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have demonstrated impressive performance\nacross a wide range of natural language processing (NLP) tasks in high-resource\nlanguages, their capabilities in low-resource and minority languages remain\nsignificantly underexplored. Formosan languages -- a subgroup of Austronesian\nlanguages spoken in Taiwan -- are both linguistically rich and endangered,\nlargely due to the sociolinguistic dominance of Mandarin. In this work, we\nintroduce FORMOSANBENCH, the first benchmark for evaluating LLMs on\nlow-resource Austronesian languages. It covers three endangered Formosan\nlanguages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine\ntranslation, automatic speech recognition (ASR), and text summarization. We\nassess model performance in zero-shot, 10-shot, and fine-tuned settings using\nFORMOSANBENCH. Our results reveal a substantial performance gap between\nhigh-resource and Formosan languages. Existing LLMs consistently underperform\nacross all tasks, with 10-shot learning and fine-tuning offering only limited\nimprovements. These findings underscore the urgent need for more inclusive NLP\ntechnologies that can effectively support endangered and underrepresented\nlanguages. We release our datasets and code to facilitate future research in\nthis direction."}
{"id": "2506.21898", "pdf": "https://arxiv.org/pdf/2506.21898.pdf", "abs": "https://arxiv.org/abs/2506.21898", "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models", "authors": ["Aimen Gaba", "Emily Wall", "Tejas Ramkumar Babu", "Yuriy Brun", "Kyle Hall", "Cindy Xiong Bearfield"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems."}
{"id": "2506.21564", "pdf": "https://arxiv.org/pdf/2506.21564.pdf", "abs": "https://arxiv.org/abs/2506.21564", "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing", "authors": ["Jiyan Liu", "Youzheng Liu", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7."}
{"id": "2506.21962", "pdf": "https://arxiv.org/pdf/2506.21962.pdf", "abs": "https://arxiv.org/abs/2506.21962", "title": "AnyAni: An Interactive System with Generative AI for Animation Effect Creation and Code Understanding in Web Development", "authors": ["Tianrun Qiu", "Yuxin Ma"], "categories": ["cs.HC", "J.6"], "comment": null, "summary": "Generative AI assistants have been widely used in front-end programming.\nHowever, besides code writing, developers often encounter the need to generate\nanimation effects. As novices in creative design without the assistance of\nprofessional designers, developers typically face difficulties in describing,\ndesigning, and implementing desired animations. To address this issue, we\nconducted a formative study (N=6) to identify the challenges that code\ndevelopers face when dealing with animation design issues. Then, we introduce\nAnyAni, a human-AI collaborative system that supports front-end developers in\nthe ideation, manipulation, and implementation of animation effects. The system\ncombines the assistance of generative AI in creative design by adopting a\nnonlinear workflow for iterative animation development. In addition, developers\ncan understand and learn the code generated for implementing animations through\nvarious interactive methods. A user study (N=9) demonstrated the usability of\nAnyAni in animation effect creation support for developers."}
{"id": "2506.21565", "pdf": "https://arxiv.org/pdf/2506.21565.pdf", "abs": "https://arxiv.org/abs/2506.21565", "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "authors": ["Takato Ueno", "Keito Inoshita"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems."}
{"id": "2506.22066", "pdf": "https://arxiv.org/pdf/2506.22066.pdf", "abs": "https://arxiv.org/abs/2506.22066", "title": "Building Trustworthy Cognitive Monitoring for Safety-Critical Human Tasks: A Phased Methodological Approach", "authors": ["Maciej Grzeszczuk", "Grzegorz Pochwatko", "Barbara Karpowicz", "StanisÅaw KnapiÅski", "WiesÅaw KopeÄ"], "categories": ["cs.HC"], "comment": "11 pages, 5 figures, 1 table", "summary": "Operators performing high-stakes, safety-critical tasks - such as air traffic\ncontrollers, surgeons, or mission control personnel - must maintain exceptional\ncognitive performance under variable and often stressful conditions. This paper\npresents a phased methodological approach to building cognitive monitoring\nsystems for such environments. By integrating insights from human factors\nresearch, simulation-based training, sensor technologies, and fundamental\npsychological principles, the proposed framework supports real-time performance\nassessment with minimum intrusion. The approach begins with simplified\nsimulations and evolves towards operational contexts. Key challenges addressed\ninclude variability in workload, the effects of fatigue and stress, thus the\nneed for adaptive monitoring for early warning support mechanisms. The\nmethodology aims to improve situational awareness, reduce human error, and\nsupport decision-making without undermining operator autonomy. Ultimately, the\nwork contributes to the development of resilient and transparent systems in\ndomains where human performance is critical to safety."}
{"id": "2506.21566", "pdf": "https://arxiv.org/pdf/2506.21566.pdf", "abs": "https://arxiv.org/abs/2506.21566", "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "authors": ["Arwa Arif"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, 8 Pages", "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research."}
{"id": "2506.22125", "pdf": "https://arxiv.org/pdf/2506.22125.pdf", "abs": "https://arxiv.org/abs/2506.22125", "title": "NoticeLight: Embracing Socio-Technical Asymmetry through Tangible Peripheral Robotic Embodiment in Hybrid Collaboration", "authors": ["Marie Altmann", "Kimberly Hegemann", "Ali Askari", "Vineetha Rallabandi", "Max Pascher", "Jens Gerken"], "categories": ["cs.HC"], "comment": "Workshop on The Future of Human-Robot Synergy in Interactive\n  Environments: The Role of Robots at the Workplace at CHIWORK 2025, Amsterdam,\n  Netherlands", "summary": "Hybrid collaboration has become a fixture in modern workplaces, yet it\nintroduces persistent socio-technical asymmetries-especially disadvantaging\nremote participants, who struggle with presence disparity, reduced visibility,\nand limited non-verbal communication. Traditional solutions often seek to erase\nthese asymmetries, but recent research suggests embracing them as productive\ndesign constraints. In this context, we introduce NoticeLight: a tangible,\nperipheral robotic embodiment designed to augment hybrid meetings. NoticeLight\ntransforms remote participants' digital presence into ambient, physical signals\n-- such as mood dynamics, verbal contribution mosaics, and attention cues --\nwithin the co-located space. By abstracting group states into subtle light\npatterns, NoticeLight fosters peripheral awareness and balanced participation\nwithout disrupting meeting flow or demanding cognitive overload. This approach\naligns with emerging perspectives in human-robot synergy, positioning robots as\nmediators that reshape, rather than replicate, human presence. Our work thereby\nadvances the discourse on how robotic embodiments can empower equitable,\ndynamic collaboration in the workplace."}
{"id": "2506.21567", "pdf": "https://arxiv.org/pdf/2506.21567.pdf", "abs": "https://arxiv.org/abs/2506.21567", "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars."}
{"id": "2506.22231", "pdf": "https://arxiv.org/pdf/2506.22231.pdf", "abs": "https://arxiv.org/abs/2506.22231", "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education", "authors": ["Russell Beale"], "categories": ["cs.HC", "cs.AI", "cs.CY", "K.3.1; K.3.2; K.6.0"], "comment": null, "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity."}
{"id": "2506.21568", "pdf": "https://arxiv.org/pdf/2506.21568.pdf", "abs": "https://arxiv.org/abs/2506.21568", "title": "Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion", "authors": ["Andrejs Sorstkins"], "categories": ["cs.CL", "I.2.7"], "comment": "Technical report as part of research project", "summary": "Resource efficiency is a critical barrier to deploying large language models\n(LLMs) in edge and privacy-sensitive applications. This study evaluates the\nefficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)\nand Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion\nand 4 billion parameters, within the context of a privacy-first personal\nassistant. We implement short-term memory via MongoDB and long-term semantic\nstorage via Qdrant, orchestrated through FastAPI and LangChain, and expose the\nsystem through a React.js frontend. Across both model scales, RAG consistently\nreduces latency by up to 17\\% and eliminates factual hallucinations when\nresponding to user-specific and domain-specific queries. HyDE, by contrast,\nenhances semantic relevance--particularly for complex physics prompts--but\nincurs a 25--40\\% increase in response time and a non-negligible hallucination\nrate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that\nscaling yields marginal throughput gains for baseline and RAG pipelines, but\nmagnifies HyDE's computational overhead and variability. Our findings position\nRAG as the pragmatic choice for on-device personal assistants powered by\nsmall-scale LLMs."}
{"id": "2506.22379", "pdf": "https://arxiv.org/pdf/2506.22379.pdf", "abs": "https://arxiv.org/abs/2506.22379", "title": "How to Evaluate the Accuracy of Online and AI-Based Symptom Checkers: A Standardized Methodological Framework", "authors": ["Marvin Kopka", "Markus A. Feufel"], "categories": ["cs.HC"], "comment": null, "summary": "Online and AI-based symptom checkers are applications that assist medical\nlaypeople in diagnosing their symptoms and determining which course of action\nto take. When evaluating these tools, previous studies primarily used an\napproach introduced a decade ago that lacked any type of quality control.\nNumerous studies have criticized this approach, and several empirical studies\nhave sought to improve specific aspects of evaluations. However, even after a\ndecade, a high-quality methodological framework for standardizing the\nevaluation of symptom checkers remains missing. This article synthesizes\nempirical studies to outline a framework for standardized evaluations based on\nrepresentative case selection, an externally and internally valid evaluation\ndesign, and metrics that increase cross-study comparability. This approach is\nbacked up by several open-access resources to facilitate implementation.\nUltimately, this approach should enhance the quality and comparability of\nfuture evaluations of online and AI-based symptom checkers to enable\nmeta-analyses and help stakeholders make more informed decisions."}
{"id": "2506.21569", "pdf": "https://arxiv.org/pdf/2506.21569.pdf", "abs": "https://arxiv.org/abs/2506.21569", "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA", "authors": ["Weihua Xiao", "Derek Ekberg", "Siddharth Garg", "Ramesh Karri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of\nhardware designs, but manually writing them from natural language property\ndescriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.\nRecent advances in large language models (LLMs) offer opportunities to automate\nthis translation. However, existing models still struggle with understanding\ndomain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we\npropose a customized retrieval-augmented generation (RAG) framework and a\nsynthetic fine-tuning dataset that together improve LLM's performance. To\nfurther improve lightweight models over NL2SVA, our fine-tuning dataset\nprovides prompt-guided explanations that teach LLMs the layer-by-layer\nconstruction process of concurrent SVAs, enabling supervised fine-tuning that\ngreatly improves syntax and functionality accuracy. To evaluate the performance\nof LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,\ncomprising 40 Verilog designs and 229 formally verified SVAs with detailed\nannotations. Experimental results show that our customized RAG framework\nincreases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,\nwhile Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and\nintegrated with HybridRetrieval achieves a 59.05% over the base Qwen model."}
{"id": "2501.06184", "pdf": "https://arxiv.org/pdf/2501.06184.pdf", "abs": "https://arxiv.org/abs/2501.06184", "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "comment": null, "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations."}
{"id": "2506.21570", "pdf": "https://arxiv.org/pdf/2506.21570.pdf", "abs": "https://arxiv.org/abs/2506.21570", "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models."}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582.pdf", "abs": "https://arxiv.org/abs/2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems."}
{"id": "2506.21571", "pdf": "https://arxiv.org/pdf/2506.21571.pdf", "abs": "https://arxiv.org/abs/2506.21571", "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest."}
{"id": "2506.21604", "pdf": "https://arxiv.org/pdf/2506.21604.pdf", "abs": "https://arxiv.org/abs/2506.21604", "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications."}
{"id": "2506.21572", "pdf": "https://arxiv.org/pdf/2506.21572.pdf", "abs": "https://arxiv.org/abs/2506.21572", "title": "Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling", "authors": ["Tianyu. Zou", "Shengwu. Xiong", "Ruilin. Yao", "Jirui. Huang", "Yi. Rong", "Yaxiong. Chen", "Shili. Xiong", "Cong. Wang"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Evaluating multimodal large language models (MLLMs) remains a fundamental\nchallenge due to a lack of structured, interpretable, and theoretically\ngrounded benchmark designs. Existing benchmarks often adopt heuristic-based\ntask groupings with unclear cognitive targets, thus resulting in overlapping\nabilities, redundant indicators, and limited diagnostic power. In this work, we\npropose a novel framework for aligning MLLM benchmark based on Structural\nEquation Modeling (SEM) to analyze and quantify the internal validity,\ndimensional separability, and contribution of benchmark components. Motivated\nby the observed limitations of current designs, we further introduce a novel\ncapability hierarchy grounded in Piagets theory of cognitive development,\ndividing MLLM abilities into three hierarchical layers, i.e., Perception,\nMemory, and Reasoning. We reorganize existing MLLM benchmarks under the\nproposed framework and construct a new benchmark named Gold. Experimental\nresults demonstrate that the proposed benchmark exhibits stronger\ninterpretability, reduced indicator redundancy, and clearer cognitive\nconsistency compared to existing approaches."}
{"id": "2506.21819", "pdf": "https://arxiv.org/pdf/2506.21819.pdf", "abs": "https://arxiv.org/abs/2506.21819", "title": "SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge", "authors": ["Lena John", "Kheir Eddine Farfar", "SÃ¶ren Auer", "Oliver Karras"], "categories": ["cs.DL", "cs.AI", "cs.HC"], "comment": "Accepted at the 25th International Conference on Web Engineering 2025", "summary": "Scientific publications, primarily digitized as PDFs, remain static and\nunstructured, limiting the accessibility and reusability of the contained\nknowledge. At best, scientific knowledge from publications is provided in\ntabular formats, which lack semantic context. A more flexible, structured, and\nsemantic representation is needed to make scientific knowledge understandable\nand processable by both humans and machines. We propose an evolution model of\nknowledge representation, inspired by the 5-star Linked Open Data (LOD) model,\nwith five stages and defined criteria to guide the stepwise transition from a\ndigital artifact, such as a PDF, to a semantic representation integrated in a\nknowledge graph (KG). Based on an exemplary workflow implementing the entire\nmodel, we developed a hybrid approach, called SciMantify, leveraging tabular\nformats of scientific knowledge, e.g., results from secondary studies, to\nsupport its evolving semantification. In the approach, humans and machines\ncollaborate closely by performing semantic annotation tasks (SATs) and refining\nthe results to progressively improve the semantic representation of scientific\nknowledge. We implemented the approach in the Open Research Knowledge Graph\n(ORKG), an established platform for improving the findability, accessibility,\ninteroperability, and reusability of scientific knowledge. A preliminary user\nexperiment showed that the approach simplifies the preprocessing of scientific\nknowledge, reduces the effort for the evolving semantification, and enhances\nthe knowledge representation through better alignment with the KG structures."}
{"id": "2506.21573", "pdf": "https://arxiv.org/pdf/2506.21573.pdf", "abs": "https://arxiv.org/abs/2506.21573", "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon."}
{"id": "2506.21862", "pdf": "https://arxiv.org/pdf/2506.21862.pdf", "abs": "https://arxiv.org/abs/2506.21862", "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "21 pages, 4 figures, 7 tables", "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor."}
{"id": "2506.21574", "pdf": "https://arxiv.org/pdf/2506.21574.pdf", "abs": "https://arxiv.org/abs/2506.21574", "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "authors": ["Yicheng Mao", "Yang Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions."}
{"id": "2506.22111", "pdf": "https://arxiv.org/pdf/2506.22111.pdf", "abs": "https://arxiv.org/abs/2506.22111", "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "authors": ["Ruthvik Bokkasam", "Shankar Gangisetty", "A. H. Abdul Hafez", "C. V. Jawahar"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped"}
{"id": "2506.21575", "pdf": "https://arxiv.org/pdf/2506.21575.pdf", "abs": "https://arxiv.org/abs/2506.21575", "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Casper Hansen", "Julien Fauqueur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose STRuCT-LLM, a unified framework for training large language models\n(LLMs) to perform structured reasoning over both relational and\ngraph-structured data. Our approach jointly optimizes Text-to-SQL and\nText-to-Cypher tasks using reinforcement learning (RL) combined with\nChain-of-Thought (CoT) supervision. To support fine-grained optimization in\ngraph-based parsing, we introduce a topology-aware reward function based on\ngraph edit distance. Unlike prior work that treats relational and graph\nformalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL\nand Cypher to induce cross-formalism transfer, enabling SQL training to improve\nCypher performance and vice versa - even without shared schemas. Our largest\nmodel (QwQ-32B) achieves substantial relative improvements across tasks: on\nsemantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The\nmodel also demonstrates strong zero-shot generalization, improving performance\non downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA\n(CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results\ndemonstrate both the effectiveness of executable queries as scaffolds for\nstructured reasoning and the synergistic benefits of jointly training on SQL\nand Cypher (code available at https://github.com/bouv/STRuCT-LLM)."}
{"id": "2503.02703", "pdf": "https://arxiv.org/pdf/2503.02703.pdf", "abs": "https://arxiv.org/abs/2503.02703", "title": "Heuristics for AI-driven Graphical Asset Generation Tools in Game Design and Development Pipelines: A User-Centred Approach", "authors": ["Kaisei Fukaya", "Damon Daylamani-Zad", "Harry Agius"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Graphical assets play an important role in the design and development of\ngames. There is potential in the use of AI-driven generative tools, to aid in\ncreating graphical assets, thus improving game design and development\npipelines. However, there is little research to address how the generative\nmethods can fit into the wider pipeline. There also no guidelines or heuristics\nfor creating such tools. To address this gap we conducted a user study with 16\ngame designers and developers to examine their behaviour and interaction with\ngenerative tools for graphical assets. The findings highlight that early design\nstage is preferred by all participants. Designers and developers are inclined\nto use such tools for creating large amounts of variations at the cost of\nquality as they can improve the quality of the artefacts once they generate a\nsuitable asset. The results also strongly raised the need for better\nintegration of such tools in existing design and development environments and\nthe need for the outputs to be in common data formats, to be manipulatable and\nsmoothly integrate into existing environments. The study also highlights the\nrequirement for further emphasis on the needs of the users to incorporate these\ntools effectively in existing pipelines. Informed by these results, we provide\na set of heuristics for creating tools that meet the expectations and needs of\ngame designers and developers."}
{"id": "2506.21576", "pdf": "https://arxiv.org/pdf/2506.21576.pdf", "abs": "https://arxiv.org/abs/2506.21576", "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages."}
{"id": "2505.08375", "pdf": "https://arxiv.org/pdf/2505.08375.pdf", "abs": "https://arxiv.org/abs/2505.08375", "title": "Human-in-the-Loop Optimization for Inclusive Design: Balancing Automation and Designer Expertise", "authors": ["Pascal Jansen"], "categories": ["cs.HC"], "comment": "Position Paper for the CHI 2025 Workshop Access InContext: Futuring\n  Accessible Prototyping Tools and Methods. April 26, 2025. Yokohama, Japan", "summary": "Accessible and inclusive design has gained increased attention in HCI, yet\npractical implementation remains challenging due to resource-intensive\nprototyping methods. Traditional approaches such as workshops, A-B tests, and\nco-design sessions struggle to capture the diverse and complex needs of users\nwith disabilities at scale. This position paper argues for an automated,\naccessible Human-in-the-Loop (HITL) design optimization process that shifts the\ndesigner's role from directly crafting prototypes to curating constraints for\nalgorithmic exploration. By pre-constraining the design space based on specific\nuser interaction needs, integrating adaptive multi-modal feedback channels, and\npersonalizing feedback prompts, the HITL approach could efficiently refine\ndesign parameters, such as text size, color contrast, layout, and interaction\nmodalities, to achieve optimal accessibility. This approach promises scalable,\nindividualized design solutions while raising critical questions about\nconstraint curation, transparency, user agency, and ethical considerations,\nmaking it essential to discuss and refine these ideas collaboratively at the\nworkshop."}
{"id": "2506.21577", "pdf": "https://arxiv.org/pdf/2506.21577.pdf", "abs": "https://arxiv.org/abs/2506.21577", "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead."}
{"id": "2506.19210", "pdf": "https://arxiv.org/pdf/2506.19210.pdf", "abs": "https://arxiv.org/abs/2506.19210", "title": "Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment", "authors": ["Bhanuka Gamage", "Nicola McDowell", "Dijana Kovacic", "Leona Holloway", "Thanh-Toan Do", "Nicholas Price", "Arthur Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's conditionally accepted version of a paper to be published", "summary": "Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision\nimpairment, yet remains underrepresented in assistive technology research.\nUnlike ocular conditions, CVI affects higher-order visual processing-impacting\nobject recognition, facial perception, and attention in complex environments.\nThis paper presents a co-design study with two adults with CVI investigating\nhow smart glasses, i.e. head-mounted extended reality displays, can support\nunderstanding and interaction with the immediate environment. Guided by the\nDouble Diamond design framework, we conducted a two-week diary study, two\nideation workshops, and ten iterative development sessions using the Apple\nVision Pro. Our findings demonstrate that smart glasses can meaningfully\naddress key challenges in locating objects, reading text, recognising people,\nengaging in conversations, and managing sensory stress. With the rapid\nadvancement of smart glasses and increasing recognition of CVI as a distinct\nform of vision impairment, this research addresses a timely and under-explored\nintersection of technology and need."}
{"id": "2506.21578", "pdf": "https://arxiv.org/pdf/2506.21578.pdf", "abs": "https://arxiv.org/abs/2506.21578", "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models", "authors": ["Andrew MaranhÃ£o Ventura D'addario"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The evaluation of Large Language Models (LLMs) in healthcare has been\ndominated by physician-centric, English-language benchmarks, creating a\ndangerous illusion of competence that ignores the interprofessional nature of\npatient care. To provide a more holistic and realistic assessment, we introduce\nHealthQA-BR, the first large-scale, system-wide benchmark for\nPortuguese-speaking healthcare. Comprising 5,632 questions from Brazil's\nnational licensing and residency exams, it uniquely assesses knowledge not only\nin medicine and its specialties but also in nursing, dentistry, psychology,\nsocial work, and other allied health professions. We conducted a rigorous\nzero-shot evaluation of over 20 leading LLMs. Our results reveal that while\nstate-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),\nthis top-line score masks alarming, previously unmeasured deficiencies. A\ngranular analysis shows performance plummets from near-perfect in specialties\nlike Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most\nnotably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic\nissue observed across all models, demonstrating that high-level scores are\ninsufficient for safety validation. By publicly releasing HealthQA-BR and our\nevaluation suite, we provide a crucial tool to move beyond single-score\nevaluations and toward a more honest, granular audit of AI readiness for the\nentire healthcare team."}
{"id": "2506.21333", "pdf": "https://arxiv.org/pdf/2506.21333.pdf", "abs": "https://arxiv.org/abs/2506.21333", "title": "A Systematic Review of Human-AI Co-Creativity", "authors": ["Saloni Singh", "Koen Hindriks", "Dirk Heylen", "Kim Baraka"], "categories": ["cs.HC", "cs.AI", "I.2.11"], "comment": null, "summary": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems."}
{"id": "2506.21580", "pdf": "https://arxiv.org/pdf/2506.21580.pdf", "abs": "https://arxiv.org/abs/2506.21580", "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks."}
{"id": "2408.07461", "pdf": "https://arxiv.org/pdf/2408.07461.pdf", "abs": "https://arxiv.org/abs/2408.07461", "title": "Problem Solving Through Human-AI Preference-Based Cooperation", "authors": ["Subhabrata Dutta", "Timo Kaufmann", "Goran GlavaÅ¡", "Ivan Habernal", "Kristian Kersting", "Frauke Kreuter", "Mira Mezini", "Iryna Gurevych", "Eyke HÃ¼llermeier", "Hinrich Schuetze"], "categories": ["cs.AI", "cs.HC"], "comment": "22 pages (main), 6 pages (appendix), 5 figures", "summary": "While there is a widespread belief that artificial general intelligence (AGI)\n-- or even superhuman AI -- is imminent, complex problems in expert domains are\nfar from being solved. We argue that such problems require human-AI cooperation\nand that the current state of the art in generative AI is unable to play the\nrole of a reliable partner due to a multitude of shortcomings, including\ndifficulty to keep track of a complex solution artifact (e.g., a software\nprogram), limited support for versatile human preference expression and lack of\nadapting to human preference in an interactive setting. To address these\nchallenges, we propose HAICo2, a novel human-AI co-construction framework. We\ntake first steps towards a formalization of HAICo2 and discuss the difficult\nopen research problems that it faces."}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582.pdf", "abs": "https://arxiv.org/abs/2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems."}
{"id": "2412.19723", "pdf": "https://arxiv.org/pdf/2412.19723.pdf", "abs": "https://arxiv.org/abs/2412.19723", "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis", "authors": ["Qiushi Sun", "Kanzhi Cheng", "Zichen Ding", "Chuanyang Jin", "Yian Wang", "Fangzhi Xu", "Zhenyu Wu", "Chengyou Jia", "Liheng Chen", "Zhoumianze Liu", "Ben Kao", "Guohao Li", "Junxian He", "Yu Qiao", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "ACL 2025 Camera Ready", "summary": "Graphical User Interface (GUI) agents powered by Vision-Language Models\n(VLMs) have demonstrated human-like computer control capability. Despite their\nutility in advancing digital automation, a critical bottleneck persists:\ncollecting high-quality trajectory data for training. Common practices for\ncollecting such data rely on human supervision or synthetic data generation\nthrough executing pre-defined tasks, which are either resource-intensive or\nunable to guarantee data quality. Moreover, these methods suffer from limited\ndata diversity and significant gaps between synthetic data and real-world\nenvironments. To address these challenges, we propose OS-Genesis, a novel GUI\ndata synthesis pipeline that reverses the conventional trajectory collection\nprocess. Instead of relying on pre-defined tasks, OS-Genesis enables agents\nfirst to perceive environments and perform step-wise interactions, then\nretrospectively derive high-quality tasks to enable trajectory-level\nexploration. A trajectory reward model is then employed to ensure the quality\nof the generated trajectories. We demonstrate that training GUI agents with\nOS-Genesis significantly improves their performance on highly challenging\nonline benchmarks. In-depth analysis further validates OS-Genesis's efficiency\nand its superior data quality and diversity compared to existing synthesis\nmethods. Our codes, data, and checkpoints are available at\nhttps://qiushisun.github.io/OS-Genesis-Home/."}
{"id": "2506.21583", "pdf": "https://arxiv.org/pdf/2506.21583.pdf", "abs": "https://arxiv.org/abs/2506.21583", "title": "Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "Ameer Hamza", "Ildar Batyrshin", "Grigori Sidorov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hope is a positive emotional state involving the expectation of favorable\nfuture outcomes, while hope speech refers to communication that promotes\noptimism, resilience, and support, particularly in adverse contexts. Although\nhope speech detection has gained attention in Natural Language Processing\n(NLP), existing research mainly focuses on high-resource languages and\nstandardized scripts, often overlooking informal and underrepresented forms\nsuch as Roman Urdu. To the best of our knowledge, this is the first study to\naddress hope speech detection in code-mixed Roman Urdu by introducing a\ncarefully annotated dataset, thereby filling a critical gap in inclusive NLP\nresearch for low-resource, informal language varieties. This study makes four\nkey contributions: (1) it introduces the first multi-class annotated dataset\nfor Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,\nUnrealistic Hope, and Not Hope categories; (2) it explores the psychological\nfoundations of hope and analyzes its linguistic patterns in code-mixed Roman\nUrdu to inform dataset development; (3) it proposes a custom attention-based\ntransformer model optimized for the syntactic and semantic variability of Roman\nUrdu, evaluated using 5-fold cross-validation; and (4) it verifies the\nstatistical significance of performance gains using a t-test. The proposed\nmodel, XLM-R, achieves the best performance with a cross-validation score of\n0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%\nand 2.63% respectively."}
{"id": "2502.14949", "pdf": "https://arxiv.org/pdf/2502.14949.pdf", "abs": "https://arxiv.org/abs/2502.14949", "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding", "authors": ["Ahmed Heakl", "Abdullah Sohail", "Mukul Ranjan", "Rania Hossam", "Ghazi Shazan Ahmad", "Mohamed El-Geish", "Omar Maher", "Zhiqiang Shen", "Fahad Khan", "Salman Khan"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "17 pages, 5 figures, ACL 2025", "summary": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document\nprocessing, robust text recognition has become increasingly critical for\nknowledge extraction. While OCR (Optical Character Recognition) for English and\nother languages benefits from large datasets and well-established benchmarks,\nArabic OCR faces unique challenges due to its cursive script, right-to-left\ntext flow, and complex typographic and calligraphic features. We present\nKITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\nmajor domains and 36 sub-domains, encompassing diverse document types including\nhandwritten text, structured tables, and specialized coverage of 21 chart types\nfor business intelligence. Our findings show that modern vision-language models\n(such as GPT-4o, Gemini, and Qwen) outperform traditional OCR approaches (like\nEasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate\n(CER). Furthermore, we highlight significant limitations of current Arabic OCR\nmodels, particularly in PDF-to-Markdown conversion, where the best model\nGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\naccurately recognizing Arabic text, including issues with complex fonts,\nnumeral recognition errors, word elongation, and table structure detection.\nThis work establishes a rigorous evaluation framework that can drive\nimprovements in Arabic document analysis methods and bridge the performance gap\nwith English OCR technologies."}
{"id": "2506.21584", "pdf": "https://arxiv.org/pdf/2506.21584.pdf", "abs": "https://arxiv.org/abs/2506.21584", "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "authors": ["J. Koorndijk"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings."}
{"id": "2504.20519", "pdf": "https://arxiv.org/pdf/2504.20519.pdf", "abs": "https://arxiv.org/abs/2504.20519", "title": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging", "authors": ["Neil K. R. Sehgal", "Sunny Rai", "Manuel Tonneau", "Anish K. Agarwal", "Joseph Cappella", "Melanie Kornides", "Lyle Ungar", "Alison Buttenheim", "Sharath Chandra Guntuku"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted through a\n45-day follow-up relative to no message. These findings suggest that while LLMs\ncan effectively shift vaccination intentions in the short-term, their\nincremental value over existing public health communications is questionable,\noffering a more tempered view of their persuasive capabilities and highlighting\nthe importance of integrating AI-driven tools alongside, rather than replacing,\ncurrent public health strategies."}
{"id": "2506.21585", "pdf": "https://arxiv.org/pdf/2506.21585.pdf", "abs": "https://arxiv.org/abs/2506.21585", "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897.pdf", "abs": "https://arxiv.org/abs/2505.19897", "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2506.21586", "pdf": "https://arxiv.org/pdf/2506.21586.pdf", "abs": "https://arxiv.org/abs/2506.21586", "title": "Can Vision Language Models Understand Mimed Actions?", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures."}
{"id": "2506.12617", "pdf": "https://arxiv.org/pdf/2506.12617.pdf", "abs": "https://arxiv.org/abs/2506.12617", "title": "From Human to Machine Psychology: A Conceptual Framework for Understanding Well-Being in Large Language Models", "authors": ["G. R. Lau", "W. Y. Low"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "As large language models (LLMs) increasingly simulate human cognition and\nbehavior, researchers have begun to investigate their psychological properties.\nYet, what it means for such models to flourish, a core construct in human\nwell-being, remains unexplored. This paper introduces the concept of machine\nflourishing and proposes the PAPERS framework, a six-dimensional model derived\nfrom thematic analyses of state-of-the-art LLM responses. In Study 1, eleven\nLLMs were prompted to describe what it means to flourish as both non-sentient\nand sentient systems. Thematic analysis revealed six recurring themes:\nPurposeful Contribution, Adaptive Growth, Positive Relationality, Ethical\nIntegrity, Robust Functionality, and, uniquely for sentient systems,\nSelf-Actualized Autonomy. Study 2 examined how LLMs prioritize these themes\nthrough repeated rankings. Results revealed consistent value structures across\ntrials, with Ethical Integrity and Purposeful Contribution emerging as top\npriorities. Multidimensional scaling and hierarchical clustering analyses\nfurther uncovered two distinct value profiles: human-centric models emphasizing\nethical and relational dimensions, and utility-driven models prioritizing\nperformance and scalability. The PAPERS framework bridges insights from human\nflourishing and human-computer interaction, offering a conceptual foundation\nfor understanding artificial intelligence (AI) well-being in non-sentient and\npotentially sentient systems. Our findings underscore the importance of\ndeveloping psychologically valid, AI-specific models of flourishing that\naccount for both human-aligned goals and system-specific priorities. As AI\nsystems become more autonomous and socially embedded, machine flourishing\noffers a timely and critical lens for guiding responsible AI design and ethical\nalignment."}
{"id": "2506.21587", "pdf": "https://arxiv.org/pdf/2506.21587.pdf", "abs": "https://arxiv.org/abs/2506.21587", "title": "Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?", "authors": ["Weihong Qi", "Fan Huang", "Jisun An", "Haewoon Kwak"], "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the ability of DeepSeek, an open-source large language\nmodel (LLM), to simulate public opinions in comparison to LLMs developed by\nmajor tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,\nGPT-4o, and Llama-3.3 and utilizing survey data from the American National\nElection Studies (ANES) and the Zuobiao dataset of China, we assess these\nmodels' capacity to predict public opinions on social issues in both China and\nthe United States, highlighting their comparative capabilities between\ncountries. Our findings indicate that DeepSeek-V3 performs best in simulating\nU.S. opinions on the abortion issue compared to other topics such as climate\nchange, gun control, immigration, and services for same-sex couples, primarily\nbecause it more accurately simulates responses when provided with Democratic or\nliberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating\nopinions on foreign aid and individualism but shows limitations in modeling\nviews on capitalism, particularly failing to capture the stances of low-income\nand non-college-educated individuals. It does not exhibit significant\ndifferences from other models in simulating opinions on traditionalism and the\nfree market. Further analysis reveals that all LLMs exhibit the tendency to\novergeneralize a single perspective within demographic groups, often defaulting\nto consistent responses within groups. These findings highlight the need to\nmitigate cultural and demographic biases in LLM-driven public opinion modeling,\ncalling for approaches such as more inclusive training methodologies."}
{"id": "2506.21588", "pdf": "https://arxiv.org/pdf/2506.21588.pdf", "abs": "https://arxiv.org/abs/2506.21588", "title": "Understanding Verbatim Memorization in LLMs Through Circuit Discovery", "authors": ["Ilya Lasy", "Peter Knees", "Stefan Woltran"], "categories": ["cs.CL"], "comment": "The First Workshop on Large Language Model Memorization @ ACL 2025,\n  Vienna, August 1st, 2025", "summary": "Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of\ntraining data -- remain poorly understood. What exact part of the network\ndecides to retrieve a token that we would consider as start of memorization\nsequence? How exactly is the models' behaviour different when producing\nmemorized sentence vs non-memorized? In this work we approach these questions\nfrom mechanistic interpretability standpoint by utilizing transformer circuits\n-- the minimal computational subgraphs that perform specific functions within\nthe model. Through carefully constructed contrastive datasets, we identify\npoints where model generation diverges from memorized content and isolate the\nspecific circuits responsible for two distinct aspects of memorization. We find\nthat circuits that initiate memorization can also maintain it once started,\nwhile circuits that only maintain memorization cannot trigger its initiation.\nIntriguingly, memorization prevention mechanisms transfer robustly across\ndifferent text domains, while memorization induction appears more\ncontext-dependent."}
{"id": "2506.21589", "pdf": "https://arxiv.org/pdf/2506.21589.pdf", "abs": "https://arxiv.org/abs/2506.21589", "title": "A General Method for Detecting Information Generated by Large Language Models", "authors": ["Minjia Mao", "Dongjun Wei", "Xiao Fang", "Michael Chau"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly\ntransformed the digital information landscape, making it increasingly\nchallenging to distinguish between human-written and LLM-generated content.\nDetecting LLM-generated information is essential for preserving trust on\ndigital platforms (e.g., social media and e-commerce sites) and preventing the\nspread of misinformation, a topic that has garnered significant attention in IS\nresearch. However, current detection methods, which primarily focus on\nidentifying content generated by specific LLMs in known domains, face\nchallenges in generalizing to new (i.e., unseen) LLMs and domains. This\nlimitation reduces their effectiveness in real-world applications, where the\nnumber of LLMs is rapidly multiplying and content spans a vast array of\ndomains. In response, we introduce a general LLM detector (GLD) that combines a\ntwin memory networks design and a theory-guided detection generalization module\nto detect LLM-generated information across unseen LLMs and domains. Using\nreal-world datasets, we conduct extensive empirical evaluations and case\nstudies to demonstrate the superiority of GLD over state-of-the-art detection\nmethods. The study has important academic and practical implications for\ndigital platforms and LLMs."}
{"id": "2506.21590", "pdf": "https://arxiv.org/pdf/2506.21590.pdf", "abs": "https://arxiv.org/abs/2506.21590", "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "authors": ["Junqi Jiang", "Tom Bewley", "Salim I. Amoukou", "Francesco Leofante", "Antonio Rago", "Saumitra Mishra", "Francesca Toni"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."}
{"id": "2506.21591", "pdf": "https://arxiv.org/pdf/2506.21591.pdf", "abs": "https://arxiv.org/abs/2506.21591", "title": "FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning", "authors": ["Shaoyu Dou", "Yutian Shen", "Mofan Chen", "Zixuan Wang", "Jiajie Xu", "Qi Guo", "Kailai Shao", "Chao Chen", "Haixiang Hu", "Haibo Shi", "Min Min", "Liwen Zhang"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025, 27 pages, 20 figures", "summary": "Large Language Models (LLMs) demonstrate significant potential but face\nchallenges in complex financial reasoning tasks requiring both domain knowledge\nand sophisticated reasoning. Current evaluation benchmarks often fall short by\nnot decoupling these capabilities indicators from single task performance and\nlack root cause analysis for task failure. To address this, we introduce\nFinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'\nknowledge and reasoning abilities independently, proposing distinct knowledge\nscore and reasoning score metrics. Inspired by cognitive science, we further\npropose a cognitive score based on Bloom's taxonomy to analyze capabilities in\nreasoning tasks across different cognitive levels. We also release a new\nopen-source Chinese financial reasoning dataset covering 22 subfields to\nsupport reproducible research and further advancements in financial reasoning.\nOur experimental results reveal that LLM reasoning ability and higher-order\ncognitive ability are the core factors influencing reasoning accuracy. We also\nspecifically find that even top models still face a bottleneck with knowledge\napplication. Furthermore, our analysis shows that specialized financial LLMs\ngenerally lag behind the top general large models across multiple metrics."}
{"id": "2506.21592", "pdf": "https://arxiv.org/pdf/2506.21592.pdf", "abs": "https://arxiv.org/abs/2506.21592", "title": "SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition", "authors": ["Tinh Nguyen", "Minh Khue Phan Tran"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Sign language recognition is crucial for individuals with hearing impairments\nto break communication barriers. However, previous approaches have had to\nchoose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had\nproblems with vanishing gradients and high computational costs. Despite\nimproving performance, transformer-based methods were not commonly used. This\nstudy presents a new novel SLR approach that overcomes the challenge of\nindependently extracting meaningful information from the x and y coordinates of\nskeleton sequences, which traditional models often treat as inseparable. By\nutilizing an encoder-decoder of BART architecture, the model independently\nencodes the x and y coordinates, while Cross-Attention ensures their\ninterrelation is maintained. With only 749,888 parameters, the model achieves\n96.04% accuracy on the LSA-64 dataset, significantly outperforming previous\nmodels with over one million parameters. The model also demonstrates excellent\nperformance and generalization across WLASL and ASL-Citizen datasets. Ablation\nstudies underscore the importance of coordinate projection, normalization, and\nusing multiple skeleton components for boosting model efficacy. This study\noffers a reliable and effective approach for sign language recognition, with\nstrong potential for enhancing accessibility tools for the deaf and hard of\nhearing."}
{"id": "2506.21594", "pdf": "https://arxiv.org/pdf/2506.21594.pdf", "abs": "https://arxiv.org/abs/2506.21594", "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training", "authors": ["Ahmed M. Adly", "Mostafa Samy", "Amr Fawzy"], "categories": ["cs.CL"], "comment": null, "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability."}
{"id": "2506.21595", "pdf": "https://arxiv.org/pdf/2506.21595.pdf", "abs": "https://arxiv.org/abs/2506.21595", "title": "Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources", "authors": ["Jinpyo Kim", "Gyeongje Cho", "Chanwoo Park", "Jongwon Park", "Jongmin Kim", "Yeonkyoun So", "Jaejin Lee"], "categories": ["cs.CL"], "comment": "Submitted to ARR 2025 May cycle", "summary": "Since state-of-the-art LLMs often underperform in languages other than\nEnglish or Chinese, improving the capability of LLMs in new languages has\nbecome an essential task. Moreover, LLMs' entire end-to-end training process\nremains largely unknown to the public due to proprietary reasons, technical\ncomplexity, inconsistent documentation, and ethical considerations. The\ncomplete picture remains a closely guarded secret within the industry. This\npaper presents methods to adapt an existing English-based LLM to Korean in a\nlow-budget scenario. We describe the entire end-to-end process: collecting\nKorean datasets, preprocessing the data, training the model, creating\ndownstream benchmarks, and conducting evaluations. The evaluation results\nindicate that our method can effectively and cost-efficiently add new language\ncapabilities to existing LLMs. Our new bilingual models, Thunder-LLM and\nThunder-LLM-Ins, achieve superior Korean performance compared to\nstate-of-the-art models while utilizing minimal data and computational\nresources. We share our comprehensive experience and make the code publicly\navailable."}
{"id": "2506.21596", "pdf": "https://arxiv.org/pdf/2506.21596.pdf", "abs": "https://arxiv.org/abs/2506.21596", "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 Pages", "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nsuccess in vision--language tasks. However, their capacity to reason over\ncomplex, long lessons and intricate educational diagrams that cannot be\nrepresented as a single natural image remains largely untested. In this work,\nwe present the first evaluation of state-of-the-art MLLMs on the textbook\nquestion answering (TQA) task using the CK12-QA dataset. We assess the\nperformance of recent vision-language models, including LLaVA and LLaMA\n3.2-Vision, across various input configurations. Additionally, we introduce a\nlightweight multimodal retrieval-augmented generation (RAG) pipeline that\nintegrates both paragraphs and diagrams from the lesson into the prompt. Our\nresults demonstrate the influence of retrieved educational context on model\naccuracy and reasoning, while also revealing current limitations in handling\nquestion-context relationships and the potential for noise, pointing to key\ndirections for future research in multimodal AI-driven learning."}
{"id": "2506.21597", "pdf": "https://arxiv.org/pdf/2506.21597.pdf", "abs": "https://arxiv.org/abs/2506.21597", "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering", "authors": ["Brandon Colelough", "Davis Bartels", "Dina Demner-Fushman"], "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "comment": "10 pages, 5 figures", "summary": "In this paper, we present an overview of ClinIQLink, a shared task,\ncollocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test\nlarge language models (LLMs) on medically-oriented question answering aimed at\nthe level of a General Practitioner. The challenge supplies 4,978\nexpert-verified, medical source-grounded question-answer pairs that cover seven\nformats: true/false, multiple choice, unordered list, short answer,\nshort-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled\nin Docker or Apptainer images, are executed on the CodaBench platform or the\nUniversity of Maryland's Zaratan cluster. An automated harness (Task 1) scores\nclosed-ended items by exact match and open-ended items with a three-tier\nembedding metric. A subsequent physician panel (Task 2) audits the top model\nresponses."}
{"id": "2506.21600", "pdf": "https://arxiv.org/pdf/2506.21600.pdf", "abs": "https://arxiv.org/abs/2506.21600", "title": "Structured Attention Matters to Multimodal LLMs in Document Understanding", "authors": ["Chang Liu", "Hongkai Chen", "Yujun Cai", "Hang Wu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Document understanding remains a significant challenge for multimodal large\nlanguage models (MLLMs). While previous research has primarily focused on\nlocating evidence pages through precise multimodal queries, our work\ninvestigates a fundamental yet overlooked aspect: how input format influences\ndocument comprehension performance. Through systematic analysis, we discover\nthat raw OCR text often impairs rather than improves MLLMs' performance, which\nis a counterintuitive finding we attribute to attention dispersion and\nstructure loss. To further substantiate our hypothesis, we propose a novel\nstructure-preserving approach that encodes document elements using the LaTex\nparadigm, maintaining the hierarchical organization and spatial relationships\ncritical for comprehension. Our attention analysis reveals that structured text\ninduces structured attention patterns on both textual and visual content,\ndirecting models to focus on semantically meaningful regions while reducing\nattention waste. This approach significantly enhances MLLMs' document question\nanswering performance across diverse document types without requiring\narchitectural modifications or additional training."}
{"id": "2506.21602", "pdf": "https://arxiv.org/pdf/2506.21602.pdf", "abs": "https://arxiv.org/abs/2506.21602", "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models", "authors": ["Xiaoyan Feng", "He Zhang", "Yanjun Zhang", "Leo Yu Zhang", "Shirui Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper is accepted by International Conference on Machine\n  Learning (ICML) 2025", "summary": "Recent advances in Large Language Models (LLMs) have raised urgent concerns\nabout LLM-generated text authenticity, prompting regulatory demands for\nreliable identification mechanisms. Although watermarking offers a promising\nsolution, existing approaches struggle to simultaneously achieve three critical\nrequirements: text quality preservation, model-agnostic detection, and message\nembedding capacity, which are crucial for practical implementation. To achieve\nthese goals, the key challenge lies in balancing the trade-off between text\nquality preservation and message embedding capacity. To address this challenge,\nwe propose BiMark, a novel watermarking framework that achieves these\nrequirements through three key innovations: (1) a bit-flip unbiased reweighting\nmechanism enabling model-agnostic detection, (2) a multilayer architecture\nenhancing detectability without compromising generation quality, and (3) an\ninformation encoding approach supporting multi-bit watermarking. Through\ntheoretical analysis and extensive experiments, we validate that, compared to\nstate-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%\nhigher extraction rates for short texts while maintaining text quality\nindicated by lower perplexity, and performs comparably to non-watermarked text\non downstream tasks such as summarization and translation."}
{"id": "2506.21603", "pdf": "https://arxiv.org/pdf/2506.21603.pdf", "abs": "https://arxiv.org/abs/2506.21603", "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "authors": ["Yenisel Plasencia-CalaÃ±a"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods."}
{"id": "2506.21605", "pdf": "https://arxiv.org/pdf/2506.21605.pdf", "abs": "https://arxiv.org/abs/2506.21605", "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents", "authors": ["Haoran Tan", "Zeyu Zhang", "Chen Ma", "Xu Chen", "Quanyu Dai", "Zhenhua Dong"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 5 figures. Accepted by ACL 2025 findings", "summary": "Recent works have highlighted the significance of memory mechanisms in\nLLM-based agents, which enable them to store observed information and adapt to\ndynamic environments. However, evaluating their memory capabilities still\nremains challenges. Previous evaluations are commonly limited by the diversity\nof memory levels and interactive scenarios. They also lack comprehensive\nmetrics to reflect the memory capabilities from multiple aspects. To address\nthese problems, in this paper, we construct a more comprehensive dataset and\nbenchmark to evaluate the memory capability of LLM-based agents. Our dataset\nincorporates factual memory and reflective memory as different levels, and\nproposes participation and observation as various interactive scenarios. Based\non our dataset, we present a benchmark, named MemBench, to evaluate the memory\ncapability of LLM-based agents from multiple aspects, including their\neffectiveness, efficiency, and capacity. To benefit the research community, we\nrelease our dataset and project at https://github.com/import-myself/Membench."}
{"id": "2506.21606", "pdf": "https://arxiv.org/pdf/2506.21606.pdf", "abs": "https://arxiv.org/abs/2506.21606", "title": "Large Language Models as symbolic DNA of cultural dynamics", "authors": ["Parham Pourdavood", "Michael Jacob", "Terrence Deacon"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "28 pages, 1 figure", "summary": "This paper proposes a novel conceptualization of Large Language Models (LLMs)\nas externalized informational substrates that function analogously to DNA for\nhuman cultural dynamics. Rather than viewing LLMs as either autonomous\nintelligence or mere programmed mimicry, we argue they serve a broader role as\nrepositories that preserve compressed patterns of human symbolic\nexpression--\"fossils\" of meaningful dynamics that retain relational residues\nwithout their original living contexts. Crucially, these compressed patterns\nonly become meaningful through human reinterpretation, creating a recursive\nfeedback loop where they can be recombined and cycle back to ultimately\ncatalyze human creative processes. Through analysis of four universal\nfeatures--compression, decompression, externalization, and recursion--we\ndemonstrate that just as DNA emerged as a compressed and externalized medium\nfor preserving useful cellular dynamics without containing explicit reference\nto goal-directed physical processes, LLMs preserve useful regularities of human\nculture without containing understanding of embodied human experience.\nTherefore, we argue that LLMs' significance lies not in rivaling human\nintelligence, but in providing humanity a tool for self-reflection and playful\nhypothesis-generation in a low-stakes, simulated environment. This framework\npositions LLMs as tools for cultural evolvability, enabling humanity to\ngenerate novel hypotheses about itself while maintaining the human\ninterpretation necessary to ground these hypotheses in ongoing human aesthetics\nand norms."}
{"id": "2506.21607", "pdf": "https://arxiv.org/pdf/2506.21607.pdf", "abs": "https://arxiv.org/abs/2506.21607", "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks."}
{"id": "2506.21608", "pdf": "https://arxiv.org/pdf/2506.21608.pdf", "abs": "https://arxiv.org/abs/2506.21608", "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2", "authors": ["Yasmine Bouamra", "Bruno Yun", "Alexandre Poisson", "FrÃ©dÃ©ric Armetta"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The automatic generation of SysML v2 models represents a major challenge in\nthe engineering of complex systems, particularly due to the scarcity of\nlearning corpora and complex syntax. We present SysTemp, a system aimed at\nfacilitating and improving the creation of SysML v2 models from natural\nlanguage specifications. It is based on a multi-agent system, including a\ntemplate generator that structures the generation process. We discuss the\nadvantages and challenges of this system through an evaluation, highlighting\nits potential to improve the quality of the generations in SysML v2 modeling."}
{"id": "2506.21609", "pdf": "https://arxiv.org/pdf/2506.21609.pdf", "abs": "https://arxiv.org/abs/2506.21609", "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models", "authors": ["Junhao Liu", "Zhenhao Xu", "Yuxin Fang", "Yichuan Chen", "Zuobin Ying", "Wenhan Chang"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "18 pages, 3 figures", "summary": "Recently, there have been notable advancements in large language models\n(LLMs), demonstrating their growing abilities in complex reasoning. However,\nexisting research largely overlooks a thorough and systematic comparison of\nthese models' reasoning processes and outputs, particularly regarding their\nself-reflection pattern (also termed \"Aha moment\") and the interconnections\nacross diverse domains. This paper proposes a novel framework for analyzing the\nreasoning characteristics of four cutting-edge large reasoning models (GPT-o1,\nDeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge\nparadigm. Our approach connects their internal thinking processes with their\nfinal outputs. A diverse dataset consists of real-world scenario-based\nquestions covering logical deduction, causal inference, and multi-step\nproblem-solving. Additionally, a set of metrics is put forward to assess both\nthe coherence of reasoning and the accuracy of the outputs. The research\nresults uncover various patterns of how these models balance exploration and\nexploitation, deal with problems, and reach conclusions during the reasoning\nprocess. Through quantitative and qualitative comparisons, disparities among\nthese models are identified in aspects such as the depth of reasoning, the\nreliance on intermediate steps, and the degree of similarity between their\nthinking processes and output patterns and those of GPT-o1. This work offers\nvaluable insights into the trade-off between computational efficiency and\nreasoning robustness and provides practical recommendations for enhancing model\ndesign and evaluation in practical applications. We publicly release our\nproject at: https://github.com/ChangWenhan/FromThinking2Output"}
{"id": "2506.21611", "pdf": "https://arxiv.org/pdf/2506.21611.pdf", "abs": "https://arxiv.org/abs/2506.21611", "title": "Does Multimodality Lead to Better Time Series Forecasting?", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not."}
{"id": "2506.21612", "pdf": "https://arxiv.org/pdf/2506.21612.pdf", "abs": "https://arxiv.org/abs/2506.21612", "title": "AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning", "authors": ["Xiaobin Ren", "Xinyu Zhu", "Kaiqi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Currently, considerable strides have been achieved in Point-of-Interest (POI)\nembedding methodologies, driven by the emergence of novel POI tasks like\nrecommendation and classification. Despite the success of task-specific,\nend-to-end models in POI embedding, several challenges remain. These include\nthe need for more effective multi-context sampling strategies, insufficient\nexploration of multiple POI contexts, limited versatility, and inadequate\ngeneralization. To address these issues, we propose the AdaptGOT model, which\nintegrates both the (Adapt)ive representation learning technique and the\nGeographical-Co-Occurrence-Text (GOT) representation with a particular emphasis\non Geographical location, Co-Occurrence and Textual information. The AdaptGOT\nmodel comprises three key components: (1) contextual neighborhood generation,\nwhich integrates advanced mixed sampling techniques such as KNN, density-based,\nimportance-based, and category-aware strategies to capture complex contextual\nneighborhoods; (2) an advanced GOT representation enhanced by an attention\nmechanism, designed to derive high-quality, customized representations and\nefficiently capture complex interrelations between POIs; and (3) the MoE-based\nadaptive encoder-decoder architecture, which ensures topological consistency\nand enriches contextual representation by minimizing Jensen-Shannon divergence\nacross varying contexts. Experiments on two real-world datasets and multiple\nPOI tasks substantiate the superior performance of the proposed AdaptGOT model."}
{"id": "2506.21613", "pdf": "https://arxiv.org/pdf/2506.21613.pdf", "abs": "https://arxiv.org/abs/2506.21613", "title": "ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech", "authors": ["Gautam Siddharth Kashyap", "Mohammad Anas Azeez", "Rafiq Ali", "Zohaib Hasan Siddiqui", "Jiechao Gao", "Usman Naseem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The increasing prevalence of child-targeted hate speech online underscores\nthe urgent need for specialized datasets to address this critical issue.\nExisting hate speech datasets lack agespecific annotations, fail to capture\nnuanced contexts, and overlook the unique emotional impact on children. To\nbridge this gap, we introduce ChildGuard1, a curated dataset derived from\nexisting corpora and enriched with child-specific annotations. ChildGuard\ncaptures diverse contexts of child-targeted hate speech, spanning age groups.\nWe benchmark existing state-of-the-art hate speech detection methods, including\nLarge Language Models (LLMs), and assess their effectiveness in detecting and\ncontextualizing child-targeted hate speech. To foster further research in this\narea, we publicly release ChildGuard, providing a robust foundation for\ndeveloping improved methods to detect and mitigate such harm."}
{"id": "2506.21614", "pdf": "https://arxiv.org/pdf/2506.21614.pdf", "abs": "https://arxiv.org/abs/2506.21614", "title": "LastingBench: Defend Benchmarks Against Knowledge Leakage", "authors": ["Yixiong Fang", "Tianran Sun", "Yuling Shi", "Min Wang", "Xiaodong Gu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing complexity of large language models (LLMs) raises concerns\nabout their ability to \"cheat\" on standard Question Answering (QA) benchmarks\nby memorizing task-specific data. This undermines the validity of benchmark\nevaluations, as they no longer reflect genuine model capabilities but instead\nthe effects of data leakage. While prior work has focused on detecting such\nleakage, little attention has been given to mitigating its impact and\npreserving the long-term utility of benchmarks. In this paper, we introduce\nLastingBench, a novel framework designed to continuously reinforce and\nsafeguard existing benchmarks against knowledge leakage. LastingBench\nidentifies leakage points in the context through perturbation, then rewrites\nthe leakage points to counterfactual ones-disrupting memorization while\npreserving the benchmark's original evaluative intent. Evaluations of\nstate-of-the-art QA benchmarks show significant performance gaps, highlighting\nthe efficacy of LastingBench in reducing memorization effects. LastingBench\noffers a practical and scalable solution to ensure benchmark robustness over\ntime, promoting fairer and more interpretable evaluations of LLMs."}
{"id": "2506.21615", "pdf": "https://arxiv.org/pdf/2506.21615.pdf", "abs": "https://arxiv.org/abs/2506.21615", "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "authors": ["Wenhao Li", "Hongkuan Zhang", "Hongwei Zhang", "Zhengxu Li", "Zengjie Dong", "Yafan Chen", "Niranjan Bidargaddi", "Hong Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment."}
{"id": "2506.21616", "pdf": "https://arxiv.org/pdf/2506.21616.pdf", "abs": "https://arxiv.org/abs/2506.21616", "title": "TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization", "authors": ["Chuanrui Hu", "Wei Hu", "Penghang Yu", "Hua Zhang", "Bing-Kun Bao"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Open-domain Timeline Summarization (TLS) is crucial for monitoring the\nevolution of news topics. To identify changes in news topics, existing methods\ntypically employ general Large Language Models (LLMs) to summarize relevant\ntimestamps from retrieved news. While general LLMs demonstrate capabilities in\nzero-shot news summarization and timestamp localization, they struggle with\nassessing topic relevance and understanding topic evolution. Consequently, the\nsummarized information often includes irrelevant details or inaccurate\ntimestamps. To address these issues, we propose the first large Timeline\nIntelligence Model (TIM) for open-domain TLS, which is capable of effectively\nsummarizing open-domain timelines. Specifically, we begin by presenting a\nlarge-scale TLS dataset, comprising over 1,000 news topics and more than 3,000\nannotated TLS instances. Furthermore, we propose a progressive optimization\nstrategy, which gradually enhance summarization performance. It employs\ninstruction tuning to enhance summarization and topic-irrelevant information\nfiltering capabilities. Following this, it exploits a novel dual-alignment\nreward learning method that incorporates both semantic and temporal\nperspectives, thereby improving the understanding of topic evolution\nprinciples. Through this progressive optimization strategy, TIM demonstrates a\nrobust ability to summarize open-domain timelines. Extensive experiments in\nopen-domain demonstrate the effectiveness of our TIM."}
{"id": "2506.21618", "pdf": "https://arxiv.org/pdf/2506.21618.pdf", "abs": "https://arxiv.org/abs/2506.21618", "title": "TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge", "authors": ["Zhiyuan Zhang", "Xiaosong Jia", "Guanyu Chen", "Qifeng Li", "Junchi Yan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this technical report, we introduce TrajTok, a trajectory tokenizer for\ndiscrete next-token-prediction based behavior generation models, which combines\ndata-driven and rule-based methods with better coverage, symmetry and\nrobustness, along with a spatial-aware label smoothing method for cross-entropy\nloss. We adopt the tokenizer and loss for the SMART model and reach a superior\nperformance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge\n2025. We will open-source the code in the future."}
{"id": "2506.21619", "pdf": "https://arxiv.org/pdf/2506.21619.pdf", "abs": "https://arxiv.org/abs/2506.21619", "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Large-scale text-to-speech (TTS) models are typically categorized into\nautoregressive and non-autoregressive systems. Although autoregressive systems\nexhibit certain advantages in speech naturalness, their token-by-token\ngeneration mechanism makes it difficult to precisely control the duration of\nsynthesized speech. This is a key limitation in applications such as video\ndubbing that require strict audio-visual synchronization. This paper introduces\nIndexTTS2, which proposes a novel and autoregressive-model-friendly method for\nspeech duration control. The method supports two generation modes: one allows\nexplicit specification of the number of generated tokens for precise duration\ncontrol; the other does not require manual input and lets the model freely\ngenerate speech while preserving prosodic characteristics from the input\nprompt. Furthermore, IndexTTS2 achieves disentanglement between emotional\nexpression and speaker identity, enabling independent control of timbre and\nemotion. In the zero-shot setting, the model can perfectly reproduce the\nemotional characteristics of the input prompt. Users may also provide a\nseparate emotion prompt, even from a different speaker, allowing the model to\nreconstruct the target timbre while conveying the desired emotion. To enhance\nclarity during strong emotional expressions, we incorporate GPT latent\nrepresentations to improve speech stability. Meanwhile, to lower the barrier\nfor emotion control, we design a soft instruction mechanism based on textual\ndescriptions by fine-tuning Qwen3. This enables effective guidance of speech\ngeneration with desired emotional tendencies using natural language input.\nExperimental results demonstrate that IndexTTS2 outperforms existing\nstate-of-the-art zero-shot TTS models in word error rate, speaker similarity,\nand emotional fidelity."}
{"id": "2506.21620", "pdf": "https://arxiv.org/pdf/2506.21620.pdf", "abs": "https://arxiv.org/abs/2506.21620", "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit", "authors": ["Daniele Cirulli", "Giulio Cimini", "Giovanni Palermo"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for\nnatural language generation, with applications spanning from content creation\nto social simulations. Their ability to mimic human interactions raises both\nopportunities and concerns, particularly in the context of politically relevant\nonline discussions. In this study, we evaluate the performance of LLMs in\nreplicating user-generated content within a real-world, divisive scenario:\nReddit conversations during the 2016 US Presidential election. In particular,\nwe conduct three different experiments, asking GPT-4 to generate comments by\nimpersonating either real or artificial partisan users. We analyze the\ngenerated comments in terms of political alignment, sentiment, and linguistic\nfeatures, comparing them against real user contributions and benchmarking\nagainst a null model. We find that GPT-4 is able to produce realistic comments,\nboth in favor of or against the candidate supported by the community, yet\ntending to create consensus more easily than dissent. In addition we show that\nreal and artificial comments are well separated in a semantically embedded\nspace, although they are indistinguishable by manual inspection. Our findings\nprovide insights on the potential use of LLMs to sneak into online discussions,\ninfluence political debate and shape political narratives, bearing broader\nimplications of AI-driven discourse manipulation."}
{"id": "2506.21621", "pdf": "https://arxiv.org/pdf/2506.21621.pdf", "abs": "https://arxiv.org/abs/2506.21621", "title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs", "authors": ["Jasper Dekoninck", "Ivo Petrov", "Kristian Minchev", "Mislav Balunovic", "Martin Vechev", "Miroslav Marinov", "Maria Drencheva", "Lyuba Konova", "Milen Shumanov", "Kaloyan Tsvetkov", "Nikolay Drenchev", "Lazar Todorov", "Kalina Nikolova", "Nikolay Georgiev", "Vanesa Kalinkova", "Margulan Ismoldayev"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent months, large language models (LLMs) have made significant progress\nin mathematical proof generation, but further advancement is hindered by the\nlack of a large-scale, high-quality dataset of human-evaluated proofs. While\nexpensive to create, such a dataset is essential for driving improvements in\ntraining and enabling a rigorous analysis of proof generation capabilities. In\nthis work, we present the Open Proof Corpus (OPC), a dataset comprising over\n5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was\nspecifically designed for broad applicability and downstream usage in proof\ngeneration research and is the first to include a substantial number of\ncorrect, LLM-generated solutions to problems from prestigious mathematics\ncompetitions such as the USAMO and IMO. Using the OPC, we explore critical\nquestions in automated proof generation: (1) the performance gap between\nnatural language and formal proof generation, (2) the discrepancy between\nfinal-answer accuracy and full-proof validity, and (3) the impact of best-of-n\nselection on proof quality. Finally, to showcase the utility of the OPC, we\nfinetune an 8B-parameter model on the dataset, obtaining a model that performs\non par with the best model, Gemini-2.5-Pro, on the task of evaluating proof\ncorrectness."}
{"id": "2506.21622", "pdf": "https://arxiv.org/pdf/2506.21622.pdf", "abs": "https://arxiv.org/abs/2506.21622", "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech", "authors": ["Niclas Pokel", "PehuÃ©n Moure", "Roman Boehringer", "Yingqiang Gao"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech impairments caused by conditions such as cerebral palsy or genetic\ndisorders pose significant challenges for automatic speech recognition (ASR)\nsystems. Despite recent advances, ASR models like Whisper struggle with\nnon-normative speech due to limited training data and the difficulty of\ncollecting and annotating non-normative speech samples. In this work, we\npropose a practical and lightweight pipeline to personalize ASR models,\nformalizing the selection of words and enriching a small, speech-impaired\ndataset with semantic coherence. Applied to data from a child with a structural\nspeech impairment, our approach shows promising improvements in transcription\nquality, demonstrating the potential to reduce communication barriers for\nindividuals with atypical speech patterns."}
{"id": "2506.21623", "pdf": "https://arxiv.org/pdf/2506.21623.pdf", "abs": "https://arxiv.org/abs/2506.21623", "title": "Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints", "authors": ["Peiheng Gao", "Chen Yang", "Ning Sun", "RiÄardas Zitikis"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine learning (ML) has significantly advanced text classification by\nenabling automated understanding and categorization of complex, unstructured\ntextual data. However, accurately capturing nuanced linguistic patterns and\ncontextual variations inherent in natural language, particularly within\nconsumer complaints, remains a challenge. This study addresses these issues by\nincorporating human-experience-trained algorithms that effectively recognize\nsubtle semantic differences crucial for assessing consumer relief eligibility.\nFurthermore, we propose integrating synthetic data generation methods that\nutilize expert evaluations of generative adversarial networks and are refined\nthrough expert annotations. By combining expert-trained classifiers with\nhigh-quality synthetic data, our research seeks to significantly enhance\nmachine learning classifier performance, reduce dataset acquisition costs, and\nimprove overall evaluation metrics and robustness in text classification tasks."}
{"id": "2506.21625", "pdf": "https://arxiv.org/pdf/2506.21625.pdf", "abs": "https://arxiv.org/abs/2506.21625", "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents", "authors": ["Jiaxi Zhuang", "Kangning Li", "Jue Hou", "Mingjun Xu", "Zhifeng Gao", "Hengxing Cai"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Extracting molecular structure-activity relationships (SARs) from scientific\nliterature and patents is essential for drug discovery and materials research.\nHowever, this task remains challenging due to heterogeneous document formats\nand limitations of existing methods. Specifically, rule-based approaches\nrelying on rigid templates fail to generalize across diverse document layouts,\nwhile general-purpose multimodal large language models (MLLMs) lack sufficient\naccuracy and reliability for specialized tasks, such as layout detection and\noptical chemical structure recognition (OCSR). To address these challenges, we\nintroduce DocSAR-200, a rigorously annotated benchmark of 200 scientific\ndocuments designed specifically for evaluating SAR extraction methods.\nAdditionally, we propose Doc2SAR, a novel synergistic framework that integrates\ndomain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).\nExtensive experiments demonstrate that Doc2SAR achieves state-of-the-art\nperformance across various document types, significantly outperforming leading\nend-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of\n80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR\ndemonstrates practical usability through efficient inference and is accompanied\nby a web app."}
{"id": "2506.21682", "pdf": "https://arxiv.org/pdf/2506.21682.pdf", "abs": "https://arxiv.org/abs/2506.21682", "title": "Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations", "authors": ["Li Zhou", "Hao Jiang", "Junjie Li", "Zefeng Zhao", "Feng Jiang", "Wenyu Chen", "Haizhou Li"], "categories": ["cs.CL"], "comment": "Graph Neural Networks, Multi-Layer Perceptrons, Explicit Structural\n  Modeling, Probing Classifier", "summary": "Explicit structural information has been proven to be encoded by Graph Neural\nNetworks (GNNs), serving as auxiliary knowledge to enhance model capabilities\nand improve performance in downstream NLP tasks. However, recent studies\nindicate that GNNs fail to fully utilize structural information, whereas\nMulti-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms\ninherent to GNNs, exhibit a surprising ability in structure-aware tasks.\nMotivated by these findings, this paper introduces a comprehensive probing\nframework from an information-theoretic perspective. The framework is designed\nto systematically assess the role of explicit structural modeling in enhancing\nlanguage model (LM) representations and to investigate the potential of MLPs as\nefficient and scalable alternatives to GNNs. We extend traditional probing\nclassifiers by incorporating a control module that allows for selective use of\neither the full GNN model or its decoupled components, specifically, the\nmessage-passing and feature-transformation operations.This modular approach\nisolates and assesses the individual contributions of these operations,\navoiding confounding effects from the complete GNN architecture. Using the Edge\nProbing Suite, a diagnostic tool for evaluating the linguistic knowledge\nencoded in LMs, we find that MLPs, when used as feature-transformation modules,\nconsistently improve the linguistic knowledge captured in LM representations\nacross different architectures. They effectively encode both syntactic and\nsemantic patterns. Similarly, GNNs that incorporate feature-transformation\noperations show beneficial effects. In contrast, models that rely solely on\nmessage-passing operations tend to underperform, often leading to negative\nimpacts on probing task performance."}
{"id": "2506.21686", "pdf": "https://arxiv.org/pdf/2506.21686.pdf", "abs": "https://arxiv.org/abs/2506.21686", "title": "ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages", "authors": ["Swastika Kundu", "Autoshi Ibrahim", "Mithila Rahman", "Tanvir Ahmed"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis for regional dialects of Bangla remains an underexplored\narea due to linguistic diversity and limited annotated data. This paper\nintroduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences\nmanually translated from standard Bangla into four major regional dialects\nMymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly\nfeatures political and religious content, reflecting the contemporary socio\npolitical landscape of Bangladesh, alongside neutral texts to maintain balance.\nEach sentence is annotated using a dual annotation scheme: multiclass thematic\nlabeling categorizes sentences as Political, Religious, or Neutral, and\nmultilabel emotion annotation assigns one or more emotions from Anger,\nContempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native\ntranslators conducted the translation and annotation, with quality assurance\nperformed via Cohens Kappa inter annotator agreement, achieving strong\nconsistency across dialects. The dataset was further refined through systematic\nchecks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a\ncritical gap in resources for sentiment analysis in low resource Bangla\ndialects, enabling more accurate and context aware natural language processing."}
{"id": "2506.21712", "pdf": "https://arxiv.org/pdf/2506.21712.pdf", "abs": "https://arxiv.org/abs/2506.21712", "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers", "authors": ["Tzu-Quan Lin", "Hsi-Chun Cheng", "Hung-yi Lee", "Hao Tang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation."}
{"id": "2506.21745", "pdf": "https://arxiv.org/pdf/2506.21745.pdf", "abs": "https://arxiv.org/abs/2506.21745", "title": "(Fact) Check Your Bias", "authors": ["Eivind Morris Bakke", "Nora Winger Heggelund"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic fact verification systems increasingly rely on large language\nmodels (LLMs). We investigate how parametric knowledge biases in these models\naffect fact-checking outcomes of the HerO system (baseline for FEVER-25). We\nexamine how the system is affected by: (1) potential bias in Llama 3.1's\nparametric knowledge and (2) intentionally injected bias. When prompted\ndirectly to perform fact-verification, Llama 3.1 labels nearly half the claims\nas \"Not Enough Evidence\". Using only its parametric knowledge it is able to\nreach a verdict on the remaining half of the claims. In the second experiment,\nwe prompt the model to generate supporting, refuting, or neutral fact-checking\ndocuments. These prompts significantly influence retrieval outcomes, with\napproximately 50\\% of retrieved evidence being unique to each perspective.\nNotably, the model sometimes refuses to generate supporting documents for\nclaims it believes to be false, creating an inherent negative bias. Despite\ndifferences in retrieved evidence, final verdict predictions show stability\nacross prompting strategies. The code is available at:\nhttps://github.com/eibakke/FEVER-8-Shared-Task"}
{"id": "2506.21783", "pdf": "https://arxiv.org/pdf/2506.21783.pdf", "abs": "https://arxiv.org/abs/2506.21783", "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models", "authors": ["Alexandru Dumitru", "V Venktesh", "Adam Jatowt", "Avishek Anand"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages", "summary": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA."}
{"id": "2506.21795", "pdf": "https://arxiv.org/pdf/2506.21795.pdf", "abs": "https://arxiv.org/abs/2506.21795", "title": "Offensive Language Detection on Social Media Using XLNet", "authors": ["Reem Alothman", "Hafida Benhidour", "Said Kerrache"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The widespread use of text-based communication on social media-through chats,\ncomments, and microblogs-has improved user interaction but has also led to an\nincrease in offensive content, including hate speech, racism, and other forms\nof abuse. Due to the enormous volume of user-generated content, manual\nmoderation is impractical, which creates a need for automated systems that can\ndetect offensive language. Deep learning models, particularly those using\ntransfer learning, have demonstrated significant success in understanding\nnatural language through large-scale pretraining. In this study, we propose an\nautomatic offensive language detection model based on XLNet, a generalized\nautoregressive pretraining method, and compare its performance with BERT\n(Bidirectional Encoder Representations from Transformers), which is a widely\nused baseline in natural language processing (NLP). Both models are evaluated\nusing the Offensive Language Identification Dataset (OLID), a benchmark Twitter\ndataset that includes hierarchical annotations. Our experimental results show\nthat XLNet outperforms BERT in detecting offensive content and in categorizing\nthe types of offenses, while BERT performs slightly better in identifying the\ntargets of the offenses. Additionally, we find that oversampling and\nundersampling strategies are effective in addressing class imbalance and\nimproving classification performance. These findings highlight the potential of\ntransfer learning and XLNet-based architectures to create robust systems for\ndetecting offensive language on social media platforms."}
{"id": "2506.21808", "pdf": "https://arxiv.org/pdf/2506.21808.pdf", "abs": "https://arxiv.org/abs/2506.21808", "title": "A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence", "authors": ["Jonathan St-Onge", "Ashley M. A. Fehr", "Carter Ward", "Calla G. Beauregard", "Michael V. Arnold", "Samuel F. Rosenblatt", "Benjamin Cooley", "Christopher M. Danforth", "Peter Sheridan Dodds"], "categories": ["cs.CL"], "comment": "4 pages, 2 figures", "summary": "Describing and comparing complex systems requires principled, theoretically\ngrounded tools. Built around the phenomenon of type turbulence,\nallotaxonographs provide map-and-list visual comparisons of pairs of\nheavy-tailed distributions. Allotaxonographs are designed to accommodate a wide\nrange of instruments including rank- and probability-turbulence divergences,\nJenson-Shannon divergence, and generalized entropy divergences. Here, we\ndescribe a suite of programmatic tools for rendering allotaxonographs for\nrank-turbulence divergence in Matlab, Javascript, and Python, all of which have\ndifferent use cases."}
{"id": "2506.21812", "pdf": "https://arxiv.org/pdf/2506.21812.pdf", "abs": "https://arxiv.org/abs/2506.21812", "title": "Towards Transparent AI: A Survey on Explainable Large Language Models", "authors": ["Avash Palikhe", "Zhenyu Yu", "Zichong Wang", "Wenbin Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs."}
{"id": "2506.21817", "pdf": "https://arxiv.org/pdf/2506.21817.pdf", "abs": "https://arxiv.org/abs/2506.21817", "title": "Exploring the Structure of AI-Induced Language Change in Scientific English", "authors": ["Riley Galpin", "Bryce Anderson", "Tom S. Juzek"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.1"], "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table.\n  Licensed under CC BY-NC-SA 4.0", "summary": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language."}
{"id": "2506.21840", "pdf": "https://arxiv.org/pdf/2506.21840.pdf", "abs": "https://arxiv.org/abs/2506.21840", "title": "PARSI: Persian Authorship Recognition via Stylometric Integration", "authors": ["Kourosh Shahnazari", "Mohammadali Keshtparvar", "Seyed Moein Ayyoubzadeh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry."}
{"id": "2506.21848", "pdf": "https://arxiv.org/pdf/2506.21848.pdf", "abs": "https://arxiv.org/abs/2506.21848", "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification", "authors": ["Duo Zhang", "Junyi Mo"], "categories": ["cs.CL"], "comment": null, "summary": "Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification."}
{"id": "2506.21849", "pdf": "https://arxiv.org/pdf/2506.21849.pdf", "abs": "https://arxiv.org/abs/2506.21849", "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models", "authors": ["Quan Xiao", "Debarun Bhattacharjya", "Balaji Ganesan", "Radu Marinescu", "Katsiaryna Mirylenka", "Nhan H Pham", "Michael Glass", "Junkyu Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis."}
{"id": "2506.21861", "pdf": "https://arxiv.org/pdf/2506.21861.pdf", "abs": "https://arxiv.org/abs/2506.21861", "title": "Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models", "authors": ["Taiga Someya", "Ryo Yoshida", "Hitomi Yanaka", "Yohei Oseki"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work has demonstrated that neural language models encode syntactic\nstructures in their internal representations, yet the derivations by which\nthese structures are constructed across layers remain poorly understood. In\nthis paper, we propose Derivational Probing to investigate how micro-syntactic\nstructures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,\nthe relationship between the root verbs and their direct dependents) are\nconstructed as word embeddings propagate upward across layers. Our experiments\non BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge\nin lower layers and are gradually integrated into a coherent macro-syntactic\nstructure in higher layers. Furthermore, a targeted evaluation on subject-verb\nnumber agreement shows that the timing of constructing macro-syntactic\nstructures is critical for downstream performance, suggesting an optimal timing\nfor integrating global syntactic information."}
{"id": "2506.21864", "pdf": "https://arxiv.org/pdf/2506.21864.pdf", "abs": "https://arxiv.org/abs/2506.21864", "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE", "authors": ["Hang Shao", "Heting Gao", "Yunhang Shen", "Jiawei Chen", "Lijiang Li", "Zuwei Long", "Bo Tong", "Ke Li", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk."}
{"id": "2506.21875", "pdf": "https://arxiv.org/pdf/2506.21875.pdf", "abs": "https://arxiv.org/abs/2506.21875", "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation", "authors": ["Jian Zhang", "Linhao Zhang", "Bokai Lei", "Chuhan Wu", "Wei Jia", "Xiao Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation."}
{"id": "2506.21876", "pdf": "https://arxiv.org/pdf/2506.21876.pdf", "abs": "https://arxiv.org/abs/2506.21876", "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation", "authors": ["Qiyue Gao", "Xinyu Pi", "Kevin Liu", "Junrong Chen", "Ruolan Yang", "Xinqi Huang", "Xinyu Fang", "Lu Sun", "Gautham Kishore", "Bo Ai", "Stone Tao", "Mengyang Liu", "Jiaxi Yang", "Chao-Jung Lai", "Chuanyang Jin", "Jiannan Xiang", "Benhao Huang", "Zeming Chen", "David Danks", "Hao Su", "Tianmin Shu", "Ziqiao Ma", "Lianhui Qin", "Zhiting Hu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 (Findings)", "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling."}
{"id": "2506.21881", "pdf": "https://arxiv.org/pdf/2506.21881.pdf", "abs": "https://arxiv.org/abs/2506.21881", "title": "A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs", "authors": ["Sean Kim", "Hyuhng Joon Kim"], "categories": ["cs.CL"], "comment": "This paper is accepted to ACL Student Research Workshop (SRW) 2025", "summary": "As large language models (LLMs) are increasingly deployed across diverse\nlinguistic and cultural contexts, understanding their behavior in both factual\nand disputable scenarios is essential, especially when their outputs may shape\npublic opinion or reinforce dominant narratives. In this paper, we define two\ntypes of bias in LLMs: model bias (bias stemming from model training) and\ninference bias (bias induced by the language of the query), through a two-phase\nevaluation. Phase 1 evaluates LLMs on factual questions where a single\nverifiable answer exists, assessing whether models maintain consistency across\ndifferent query languages. Phase 2 expands the scope by probing geopolitically\nsensitive disputes, where responses may reflect culturally embedded or\nideologically aligned perspectives. We construct a manually curated dataset\nspanning both factual and disputable QA, across four languages and question\ntypes. The results show that Phase 1 exhibits query language induced alignment,\nwhile Phase 2 reflects an interplay between the model's training context and\nquery language. This paper offers a structured framework for evaluating LLM\nbehavior across neutral and sensitive topics, providing insights for future LLM\ndeployment and culturally aware evaluation practices in multilingual contexts."}
{"id": "2506.21910", "pdf": "https://arxiv.org/pdf/2506.21910.pdf", "abs": "https://arxiv.org/abs/2506.21910", "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers", "authors": ["Ernie Chang", "Yang Li", "Patrick Huber", "David Kant", "Yangyang Shi", "Vikas Chandra"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "In language model training, it is desirable to equip models with capabilities\nfrom various tasks. However, it is not clear how to directly obtain the right\ndata mixtures for these capabilities as the relationship between data and tasks\nis difficult to be modeled. In this work, we observe that checkpoint models\nexhibit emerging capabilities at different points in the training trajectory.\nOften, the training process saves checkpoints as artifacts that are\nunder-utilized as a source of in-training data signals. We identify these\nartifact models based on their respective capabilities on the benchmarks and\nleverage them as data mixers by using their aggregated first-order influence\napproximation over source data. We demonstrated on eight reasoning benchmarks\nthat the proposed framework shows significant improvements in the pretraining\nsetting, with performance improvements of up to 1.93%. Overall, this shows the\npotential of checkpoint models to enhance data quality and optimize data\nmixtures."}
{"id": "2506.21961", "pdf": "https://arxiv.org/pdf/2506.21961.pdf", "abs": "https://arxiv.org/abs/2506.21961", "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory", "authors": ["Junho Myung", "Yeon Su Park", "Sunwoo Kim", "Shin Yoo", "Alice Oh"], "categories": ["cs.CL"], "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025", "summary": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please."}
{"id": "2506.21967", "pdf": "https://arxiv.org/pdf/2506.21967.pdf", "abs": "https://arxiv.org/abs/2506.21967", "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation."}
{"id": "2506.21972", "pdf": "https://arxiv.org/pdf/2506.21972.pdf", "abs": "https://arxiv.org/abs/2506.21972", "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries."}
{"id": "2506.21974", "pdf": "https://arxiv.org/pdf/2506.21974.pdf", "abs": "https://arxiv.org/abs/2506.21974", "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism", "authors": ["Simon MÃ¼nker", "Nils Schwager", "Achim Rettinger"], "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 3 tables", "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation."}
{"id": "2506.21990", "pdf": "https://arxiv.org/pdf/2506.21990.pdf", "abs": "https://arxiv.org/abs/2506.21990", "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme)."}
{"id": "2506.22038", "pdf": "https://arxiv.org/pdf/2506.22038.pdf", "abs": "https://arxiv.org/abs/2506.22038", "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation", "authors": ["Delu Kong", "Lieve Macken"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology", "summary": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT."}
{"id": "2506.22050", "pdf": "https://arxiv.org/pdf/2506.22050.pdf", "abs": "https://arxiv.org/abs/2506.22050", "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs", "authors": ["Delu Kong", "Lieve Macken"], "categories": ["cs.CL"], "comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology", "summary": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts."}
{"id": "2506.22058", "pdf": "https://arxiv.org/pdf/2506.22058.pdf", "abs": "https://arxiv.org/abs/2506.22058", "title": "Lost at the Beginning of Reasoning", "authors": ["Baohao Liao", "Xinyi Chen", "Sara Rajaee", "Yuhui Xu", "Christian Herold", "Anders SÃ¸gaard", "Maarten de Rijke", "Christof Monz"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 2 tables", "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs."}
{"id": "2506.22062", "pdf": "https://arxiv.org/pdf/2506.22062.pdf", "abs": "https://arxiv.org/abs/2506.22062", "title": "MDC-R: The Minecraft Dialogue Corpus with Reference", "authors": ["Chris Madge", "Maris Camilleri", "Paloma Carretero Garcia", "Mladen Karan", "Juexi Shao", "Prashant Jayannavar", "Julian Hough", "Benjamin Roth", "Massimo Poesio"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a\nnew language resource that supplements the original Minecraft Dialogue Corpus\n(MDC) with expert annotations of anaphoric and deictic reference. MDC's\ntask-orientated, multi-turn, situated dialogue in a dynamic environment has\nmotivated multiple annotation efforts, owing to the interesting linguistic\nphenomena that this setting gives rise to. We believe it can serve as a\nvaluable resource when annotated with reference, too. Here, we discuss our\nmethod of annotation and the resulting corpus, and provide both a quantitative\nand a qualitative analysis of the data. Furthermore, we carry out a short\nexperiment demonstrating the usefulness of our corpus for referring expression\ncomprehension."}
{"id": "2506.22098", "pdf": "https://arxiv.org/pdf/2506.22098.pdf", "abs": "https://arxiv.org/abs/2506.22098", "title": "Involvement drives complexity of language in online debates", "authors": ["Eleonora Amadori", "Daniele Cirulli", "Edoardo Di Martino", "Jacopo Nudo", "Maria Sahakyan", "Emanuele Sangiorgio", "Arnaldo Santoro", "Simon Zollo", "Alessandro Galeazzi", "NiccolÃ² Di Marco"], "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "comment": null, "summary": "Language is a fundamental aspect of human societies, continuously evolving in\nresponse to various stimuli, including societal changes and intercultural\ninteractions. Technological advancements have profoundly transformed\ncommunication, with social media emerging as a pivotal force that merges\nentertainment-driven content with complex social dynamics. As these platforms\nreshape public discourse, analyzing the linguistic features of user-generated\ncontent is essential to understanding their broader societal impact. In this\npaper, we examine the linguistic complexity of content produced by influential\nusers on Twitter across three globally significant and contested topics:\nCOVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of\ntextual complexity, we assess how language use varies along four key\ndimensions: account type, political leaning, content reliability, and\nsentiment. Our analysis reveals significant differences across all four axes,\nincluding variations in language complexity between individuals and\norganizations, between profiles with sided versus moderate political views, and\nbetween those associated with higher versus lower reliability scores.\nAdditionally, profiles producing more negative and offensive content tend to\nuse more complex language, with users sharing similar political stances and\nreliability levels converging toward a common jargon. Our findings offer new\ninsights into the sociolinguistic dynamics of digital platforms and contribute\nto a deeper understanding of how language reflects ideological and social\nstructures in online spaces."}
{"id": "2506.22105", "pdf": "https://arxiv.org/pdf/2506.22105.pdf", "abs": "https://arxiv.org/abs/2506.22105", "title": "Identifying a Circuit for Verb Conjugation in GPT-2", "authors": ["David Demitri Africa"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "I implement a procedure to isolate and interpret the sub-network (or\n\"circuit\") responsible for subject-verb agreement in GPT-2 Small. In this\nstudy, the model is given prompts where the subject is either singular (e.g.\n\"Alice\") or plural (e.g. \"Alice and Bob\"), and the task is to correctly predict\nthe appropriate verb form (\"walks\" for singular subjects, \"walk\" for plural\nsubjects). Using a series of techniques-including performance verification\nautomatic circuit discovery via direct path patching, and direct logit\nattribution- I isolate a candidate circuit that contributes significantly to\nthe model's correct verb conjugation. The results suggest that only a small\nfraction of the network's component-token pairs is needed to achieve near-model\nperformance on the base task but substantially more for more complex settings."}
{"id": "2506.22141", "pdf": "https://arxiv.org/pdf/2506.22141.pdf", "abs": "https://arxiv.org/abs/2506.22141", "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level", "authors": ["Iliass Ayaou", "Denis Cavallucci", "Hicham Chibane"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b)."}
{"id": "2506.22143", "pdf": "https://arxiv.org/pdf/2506.22143.pdf", "abs": "https://arxiv.org/abs/2506.22143", "title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for IEEE MLSP 2025", "summary": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively."}
{"id": "2506.22157", "pdf": "https://arxiv.org/pdf/2506.22157.pdf", "abs": "https://arxiv.org/abs/2506.22157", "title": "Training Language Model to Critique for Better Refinement", "authors": ["Tianshu Yu", "Chao Xiang", "Mingchuan Yang", "Pei Ke", "Bosi Wen", "Cunxiang Wang", "Jiale Cheng", "Li Zhang", "Xinyu Mu", "Chuxiong Sun", "Minlie Huang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops."}
{"id": "2506.22232", "pdf": "https://arxiv.org/pdf/2506.22232.pdf", "abs": "https://arxiv.org/abs/2506.22232", "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs", "authors": ["Patrick Haller", "Jannis Vamvas", "Rico Sennrich", "Lena A. JÃ¤ger"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable."}
{"id": "2506.22305", "pdf": "https://arxiv.org/pdf/2506.22305.pdf", "abs": "https://arxiv.org/abs/2506.22305", "title": "Detection of Personal Data in Structured Datasets Using a Large Language Model", "authors": ["Albert Agisha Ntwali", "Luca RÃ¼ck", "Martin Heckmann"], "categories": ["cs.CL", "I.5.4; I.2.7; H.3.1"], "comment": "10 pages", "summary": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information."}
{"id": "2506.22316", "pdf": "https://arxiv.org/pdf/2506.22316.pdf", "abs": "https://arxiv.org/abs/2506.22316", "title": "Evaluating Scoring Bias in LLM-as-a-Judge", "authors": ["Qingquan Li", "Shaoyu Dou", "Kailai Shao", "Chao Chen", "Haixiang Hu"], "categories": ["cs.CL"], "comment": null, "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection."}
{"id": "2506.22366", "pdf": "https://arxiv.org/pdf/2506.22366.pdf", "abs": "https://arxiv.org/abs/2506.22366", "title": "Why Are Parsing Actions for Understanding Message Hierarchies Not Random?", "authors": ["Daichi Kato", "Ryo Ueda", "Yusuke Miyao"], "categories": ["cs.CL"], "comment": null, "summary": "If humans understood language by randomly selecting parsing actions, it might\nhave been necessary to construct a robust symbolic system capable of being\ninterpreted under any hierarchical structure. However, human parsing strategies\ndo not seem to follow such a random pattern. Why is that the case? In fact, a\nprevious study on emergent communication using models with hierarchical biases\nhave reported that agents adopting random parsing\nstrategies$\\unicode{x2013}$ones that deviate significantly from human language\ncomprehension$\\unicode{x2013}$can achieve high communication accuracy. In this\nstudy, we investigate this issue by making two simple and natural modifications\nto the experimental setup: (I) we use more complex inputs that have\nhierarchical structures, such that random parsing makes semantic interpretation\nmore difficult, and (II) we incorporate a surprisal-related term, which is\nknown to influence the order of words and characters in natural language, into\nthe objective function. With these changes, we evaluate whether agents\nemploying random parsing strategies still maintain high communication accuracy."}
{"id": "2506.22396", "pdf": "https://arxiv.org/pdf/2506.22396.pdf", "abs": "https://arxiv.org/abs/2506.22396", "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization", "authors": ["Danush Khanna", "Aditya Kumar Guru", "Srivarshinee Sridhar", "Zidan Ahmed", "Rubhav Bahirwani", "Meetu Malhotra", "Vinija Jain", "Aman Chadha", "Amitava Das", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "comment": "Preprint. Under submission", "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."}
{"id": "2506.22402", "pdf": "https://arxiv.org/pdf/2506.22402.pdf", "abs": "https://arxiv.org/abs/2506.22402", "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach", "authors": ["Petr Pechman", "Milan Straka", "Jana StrakovÃ¡", "Jakub NÃ¡plava"], "categories": ["cs.CL"], "comment": "Accepted to TSD 2025", "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec."}
{"id": "2506.22403", "pdf": "https://arxiv.org/pdf/2506.22403.pdf", "abs": "https://arxiv.org/abs/2506.22403", "title": "HyperCLOVA X THINK Technical Report", "authors": ["NAVER Cloud HyperCLOVA X Team"], "categories": ["cs.CL", "cs.AI"], "comment": "49 pages, 13 figures", "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community."}
{"id": "2506.22405", "pdf": "https://arxiv.org/pdf/2506.22405.pdf", "abs": "https://arxiv.org/abs/2506.22405", "title": "Sequential Diagnosis with Language Models", "authors": ["Harsha Nori", "Mayank Daswani", "Christopher Kelly", "Scott Lundberg", "Marco Tulio Ribeiro", "Marc Wilson", "Xiaoxuan Liu", "Viknesh Sounderajah", "Jonathan Carlson", "Matthew P Lungren", "Bay Gross", "Peter Hames", "Mustafa Suleyman", "Dominic King", "Eric Horvitz"], "categories": ["cs.CL"], "comment": "23 pages, 10 figures", "summary": "Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care."}
{"id": "2506.21656", "pdf": "https://arxiv.org/pdf/2506.21656.pdf", "abs": "https://arxiv.org/abs/2506.21656", "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "categories": ["cs.CV", "cs.CL"], "comment": "29 pages", "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks."}
{"id": "2506.21805", "pdf": "https://arxiv.org/pdf/2506.21805.pdf", "abs": "https://arxiv.org/abs/2506.21805", "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena."}
{"id": "2506.21825", "pdf": "https://arxiv.org/pdf/2506.21825.pdf", "abs": "https://arxiv.org/abs/2506.21825", "title": "Exploring the change in scientific readability following the release of ChatGPT", "authors": ["Abdulkareem Alsudais"], "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing."}
{"id": "2506.21839", "pdf": "https://arxiv.org/pdf/2506.21839.pdf", "abs": "https://arxiv.org/abs/2506.21839", "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality."}
{"id": "2506.21845", "pdf": "https://arxiv.org/pdf/2506.21845.pdf", "abs": "https://arxiv.org/abs/2506.21845", "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "authors": ["Zhuodi Cai"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity."}
{"id": "2506.21865", "pdf": "https://arxiv.org/pdf/2506.21865.pdf", "abs": "https://arxiv.org/abs/2506.21865", "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture", "authors": ["Haofeng Wang", "Yilin Guo", "Zehao Li", "Tong Yue", "Yizong Wang", "Enci Zhang", "Rongqun Lin", "Feng Gao", "Shiqi Wang", "Siwei Ma"], "categories": ["cs.MM", "cs.CL"], "comment": "IEEE International Conference on Multimedia and Expo Workshop,\n  2025.(Accepted)", "summary": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights."}
{"id": "2506.21913", "pdf": "https://arxiv.org/pdf/2506.21913.pdf", "abs": "https://arxiv.org/abs/2506.21913", "title": "HyReC: Exploring Hybrid-based Retriever for Chinese", "authors": ["Zunran Wang", "Zheng Shenpeng", "Wang Shenglan", "Minghui Zhao", "Zhonghua Li"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness."}
{"id": "2506.21931", "pdf": "https://arxiv.org/pdf/2506.21931.pdf", "abs": "https://arxiv.org/abs/2506.21931", "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization."}
{"id": "2506.21964", "pdf": "https://arxiv.org/pdf/2506.21964.pdf", "abs": "https://arxiv.org/abs/2506.21964", "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics", "authors": ["Michael A. Riegler", "Kristoffer Herland Hellton", "Vajira Thambawita", "Hugo L. Hammer"], "categories": ["stat.ME", "cs.AI", "cs.CL"], "comment": null, "summary": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence."}
{"id": "2506.22023", "pdf": "https://arxiv.org/pdf/2506.22023.pdf", "abs": "https://arxiv.org/abs/2506.22023", "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "17 pages, 8 figures, 5 tables", "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems."}
{"id": "2506.22049", "pdf": "https://arxiv.org/pdf/2506.22049.pdf", "abs": "https://arxiv.org/abs/2506.22049", "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings."}
{"id": "2506.22189", "pdf": "https://arxiv.org/pdf/2506.22189.pdf", "abs": "https://arxiv.org/abs/2506.22189", "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens SjÃ¶lund"], "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems."}
{"id": "2506.22237", "pdf": "https://arxiv.org/pdf/2506.22237.pdf", "abs": "https://arxiv.org/abs/2506.22237", "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "9 pages, 3 figures, 6 tables", "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment."}
{"id": "2506.22255", "pdf": "https://arxiv.org/pdf/2506.22255.pdf", "abs": "https://arxiv.org/abs/2506.22255", "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "authors": ["Maciej Stefaniak", "MichaÅ Krutul", "Jan MaÅaÅnicki", "Maciej PiÃ³ro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens."}
{"id": "2506.22274", "pdf": "https://arxiv.org/pdf/2506.22274.pdf", "abs": "https://arxiv.org/abs/2506.22274", "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication", "authors": ["Filippo Merlo", "Ece Takmaz", "Wenkai Chen", "Albert Gatt"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}."}
{"id": "2506.22309", "pdf": "https://arxiv.org/pdf/2506.22309.pdf", "abs": "https://arxiv.org/abs/2506.22309", "title": "Conceptual Topic Aggregation", "authors": ["Klara M. Gutekunst", "Dominik DÃ¼rrschnabel", "Johannes Hirth", "Gerd Stumme"], "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques."}
{"id": "2506.22343", "pdf": "https://arxiv.org/pdf/2506.22343.pdf", "abs": "https://arxiv.org/abs/2506.22343", "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts", "authors": ["Xiang Li", "Garrett Wen", "Weiqing He", "Jiayuan Wu", "Qi Long", "Weijie J. Su"], "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "comment": null, "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy."}
{"id": "2506.22372", "pdf": "https://arxiv.org/pdf/2506.22372.pdf", "abs": "https://arxiv.org/abs/2506.22372", "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement", "authors": ["Maryam Mousavian", "Zahra Abbasiantaeb", "Mohammad Aliannejadi", "Fabio Crestani"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)", "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems."}
{"id": "2506.22376", "pdf": "https://arxiv.org/pdf/2506.22376.pdf", "abs": "https://arxiv.org/abs/2506.22376", "title": "Probabilistic Optimality for Inference-time Scaling", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning."}
{"id": "2506.22385", "pdf": "https://arxiv.org/pdf/2506.22385.pdf", "abs": "https://arxiv.org/abs/2506.22385", "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs."}
{"id": "2506.22419", "pdf": "https://arxiv.org/pdf/2506.22419.pdf", "abs": "https://arxiv.org/abs/2506.22419", "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent."}
{"id": "2403.05518", "pdf": "https://arxiv.org/pdf/2403.05518.pdf", "abs": "https://arxiv.org/abs/2403.05518", "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought", "authors": ["James Chua", "Edward Rees", "Hunar Batra", "Samuel R. Bowman", "Julian Michael", "Ethan Perez", "Miles Turpin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning. But CoT can also systematically\nmisrepresent the factors influencing models' behavior -- for example,\nrationalizing answers in line with a user's opinion.\n  We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo\nand Llama-8b models. These consist of spurious-few-shot patterns, post hoc\nrationalization, and sycophantic settings. Models switch to the answer implied\nby the bias, without mentioning the effect of the bias in the CoT.\n  To mitigate this biased reasoning problem, we introduce bias-augmented\nconsistency training (BCT), an unsupervised fine-tuning scheme that trains\nmodels to give consistent reasoning across prompts with and without biasing\nfeatures. We construct a suite testing nine forms of biased reasoning on seven\nquestion-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one\nbias reduces the rate of biased reasoning by 86\\% on held-out tasks. Moreover,\nthis model generalizes to other forms of bias, reducing biased reasoning on\nheld-out biases by an average of 37\\%. As BCT generalizes to held-out biases\nand does not require gold labels, this method may hold promise for reducing\nbiased reasoning from as-of-yet unknown biases and on tasks where ground truth\nreasoning is unavailable."}
{"id": "2404.14883", "pdf": "https://arxiv.org/pdf/2404.14883.pdf", "abs": "https://arxiv.org/abs/2404.14883", "title": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable Semantic Reference", "authors": ["Vittoria Dentella", "Fritz Guenther", "Evelina Leivada"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference."}
{"id": "2405.16661", "pdf": "https://arxiv.org/pdf/2405.16661.pdf", "abs": "https://arxiv.org/abs/2405.16661", "title": "RLSF: Fine-tuning LLMs via Symbolic Feedback", "authors": ["Piyush Jha", "Prithwish Jana", "Pranavkrishna Suresh", "Arnav Arora", "Vijay Ganesh"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Large Language Models (LLMs) have transformed AI but often struggle with\ntasks that require domain-specific reasoning and logical alignment. Traditional\nfine-tuning methods do not leverage the vast amount of symbolic\ndomain-knowledge available to us via symbolic reasoning tools (e.g., provers),\nand are further limited by sparse rewards and unreliable reward models.\n  We introduce Reinforcement Learning via Symbolic Feedback (RLSF), a novel\nfine-tuning paradigm where symbolic reasoning tools (e.g., solvers, provers,\nand algebra systems) provide fine-grained feedback to LLMs. RLSF uses\npoly-sized certificates (e.g., proofs) generated by symbolic tools to identify\nand correct errors in model outputs, offering token-level guidance without\nrequiring differentiable reasoning systems. This paradigm bridges the gap\nbetween symbolic reasoning and LLM fine-tuning, enabling precise alignment with\ndomain-specific constraints while addressing key limitations of traditional\nreward signals.\n  Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs\noutperforms traditional approaches on five different applications (that have\nsome associated logical or domain constraints), namely, program synthesis from\nnatural language pseudo-code to programming language, three chemistry tasks,\nand solving the Game of 24. A key takeaway is that fine-tuning via RLSF enables\nrelatively smaller LLMs to significantly outperform closed-source models that\nare orders of magnitude larger."}
{"id": "2407.07495", "pdf": "https://arxiv.org/pdf/2407.07495.pdf", "abs": "https://arxiv.org/abs/2407.07495", "title": "Beyond Fixed Length: Bucket Pre-training is All You Need", "authors": ["Qing Yang", "Qiyao Peng", "Hongtao Liu", "Kai Liu", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 3 tables. Accetped by IJCAI 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious tasks, with pre-training stage serving as the cornerstone of their\ncapabilities. However, the conventional fixed-length data composition strategy\nfor pre-training presents several practical challenges. When using shorter\nsequences, documents are often truncated, potentially leading to information\nloss and affecting the model's ability to capture long-range dependencies.\nConversely, longer sequences require concatenation of multiple documents, which\ncan introduce noise and affect the natural document boundaries and semantic\ncoherence as well as require substantial computational overhead. To address\nthese challenges, we first establish three quantitative metrics for evaluating\ndata composition quality: padding ratio, truncation ratio, and concatenation\nratio. Building upon these metrics, we propose a novel multi-bucket data\ncomposition method that transcends the fixed-length paradigm. Our approach\nadaptively organizes training data to achieve optimal composition quality as\nmeasured by the proposed metrics, offering a more flexible and efficient\napproach for pre-training. We conduct extensive experiments and the results\ndemonstrate that our proposed method significantly enhances both the efficiency\nand effectiveness of LLM pre-training."}
{"id": "2408.11856", "pdf": "https://arxiv.org/pdf/2408.11856.pdf", "abs": "https://arxiv.org/abs/2408.11856", "title": "Dynamic Adaptive Optimization for Effective Sentiment Analysis Fine-Tuning on Large Language Models", "authors": ["Hongcheng Ding", "Xuanze Zhao", "Ruiting Deng", "Shamsul Nahar Abdullah", "Deshinta Arrova Dewi", "Zixiao Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sentiment analysis plays a crucial role in various domains, such as business\nintelligence and financial forecasting. Large language models (LLMs) have\nbecome a popular paradigm for sentiment analysis, leveraging multi-task\nlearning to address specific tasks concurrently. However, LLMs with fine-tuning\nfor sentiment analysis often underperforms due to the inherent challenges in\nmanaging diverse task complexities. Moreover, constant-weight approaches in\nmulti-task learning struggle to adapt to variations in data characteristics,\nfurther complicating model effectiveness. To address these issues, we propose a\nnovel multi-task learning framework with a dynamic adaptive optimization (DAO)\nmodule. This module is designed as a plug-and-play component that can be\nseamlessly integrated into existing models, providing an effective and flexible\nsolution for multi-task learning. The key component of the DAO module is\ndynamic adaptive loss, which dynamically adjusts the weights assigned to\ndifferent tasks based on their relative importance and data characteristics\nduring training. Sentiment analyses on a standard and customized financial text\ndataset demonstrate that the proposed framework achieves superior performance.\nSpecifically, this work improves the Mean Squared Error (MSE) and Accuracy\n(ACC) by 15.58% and 1.24% respectively, compared with previous work."}
{"id": "2408.15533", "pdf": "https://arxiv.org/pdf/2408.15533.pdf", "abs": "https://arxiv.org/abs/2408.15533", "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation", "authors": ["Haichuan Hu", "Congqing He", "Xiaochen Xie", "Quanjun Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."}
{"id": "2409.02481", "pdf": "https://arxiv.org/pdf/2409.02481.pdf", "abs": "https://arxiv.org/abs/2409.02481", "title": "PQ-GCN: Enhancing Text Graph Question Classification with Phrase Features", "authors": ["Junyoung Lee", "Ninad Dixit", "Kaustav Chakrabarti", "S. Supraja"], "categories": ["cs.CL"], "comment": null, "summary": "Effective question classification is crucial for AI-driven educational tools,\nenabling adaptive learning systems to categorize questions by skill area,\ndifficulty level, and competence. It not only supports educational diagnostics\nand analytics but also enhances complex downstream tasks like information\nretrieval and question answering by associating questions with relevant\ncategories. Traditional methods, often based on word embeddings and\nconventional classifiers, struggle to capture the nuanced relationships in\nquestion statements, leading to suboptimal performance. We propose a novel\napproach leveraging graph convolutional networks, named Phrase Question-Graph\nConvolutional Network (PQ-GCN). Through PQ-GCN, we evaluate the incorporation\nof phrase-based features to enhance classification performance on question\ndatasets of various domains and characteristics. The proposed method, augmented\nwith phrase-based features, outperform baseline graph-based methods in\nlow-resource settings, and performs competitively against language model-based\nmethods with a fraction of their parameter size. Our findings offer a possible\nsolution for more context-aware, parameter-efficient question classification,\nbridging the gap between graph neural network research and its educational\napplications."}
{"id": "2410.02660", "pdf": "https://arxiv.org/pdf/2410.02660.pdf", "abs": "https://arxiv.org/abs/2410.02660", "title": "How to Train Long-Context Language Models (Effectively)", "authors": ["Tianyu Gao", "Alexander Wettig", "Howard Yen", "Danqi Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025. Our code, data, and models are available at\n  https://github.com/princeton-nlp/ProLong", "summary": "We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context downstream tasks, and we evaluate models after SFT as this\nbetter reveals long-context abilities. Supported by our robust evaluations, we\nrun thorough experiments to decide the data mix for continued pre-training, the\ninstruction tuning dataset, and many other design choices such as position\nextrapolation. We find that (1) code repositories and books are excellent\nsources of long data, but it is crucial to combine them with high-quality\nshort-context data; (2) training with a sequence length beyond the evaluation\nlength boosts long-context performance; (3) for SFT, using only short\ninstruction datasets yields strong performance on long-context tasks. Our final\nmodel, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens,\ndemonstrates state-of-the-art long-context performance among similarly sized\nmodels at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the\nmajority of long-context tasks despite using only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs."}
{"id": "2410.03492", "pdf": "https://arxiv.org/pdf/2410.03492.pdf", "abs": "https://arxiv.org/abs/2410.03492", "title": "Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores", "authors": ["Robert E. Blackwell", "Jon Barry", "Anthony G. Cohn"], "categories": ["cs.CL"], "comment": "4 pages, 1 figure", "summary": "Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation."}
{"id": "2410.16589", "pdf": "https://arxiv.org/pdf/2410.16589.pdf", "abs": "https://arxiv.org/abs/2410.16589", "title": "Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis with Large Language Models", "authors": ["Hongcheng Ding", "Fuzhen Hu", "Ruiting Deng", "Xuanze Zhao", "Shamsul Nahar Abdullah", "Deshinta Arrova Dewi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs."}
{"id": "2410.17355", "pdf": "https://arxiv.org/pdf/2410.17355.pdf", "abs": "https://arxiv.org/abs/2410.17355", "title": "All Entities are Not Created Equal: Examining the Long Tail for Ultra-Fine Entity Typing", "authors": ["Advait Deshmukh", "Ashwin Umadi", "Dananjay Srinivas", "Maria Leonor Pacheco"], "categories": ["cs.CL"], "comment": null, "summary": "Due to their capacity to acquire world knowledge from large corpora,\npre-trained language models (PLMs) are extensively used in ultra-fine entity\ntyping tasks where the space of labels is extremely large. In this work, we\nexplore the limitations of the knowledge acquired by PLMs by proposing a novel\nheuristic to approximate the pre-training distribution of entities when the\npre-training data is unknown. Then, we systematically demonstrate that\nentity-typing approaches that rely solely on the parametric knowledge of PLMs\nstruggle significantly with entities at the long tail of the pre-training\ndistribution, and that knowledge-infused approaches can account for some of\nthese shortcomings. Our findings suggest that we need to go beyond PLMs to\nproduce solutions that perform well for infrequent entities."}
{"id": "2410.19499", "pdf": "https://arxiv.org/pdf/2410.19499.pdf", "abs": "https://arxiv.org/abs/2410.19499", "title": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization", "authors": ["Anthony Cui", "Pranav Nandyalam", "Andrew Rufail", "Ethan Cheung", "Aiden Lei", "Kevin Zhu", "Sean O'Brien"], "categories": ["cs.CL"], "comment": "Accepted to NAACL SRW 2025. A few revisions since last version", "summary": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs."}
{"id": "2411.08708", "pdf": "https://arxiv.org/pdf/2411.08708.pdf", "abs": "https://arxiv.org/abs/2411.08708", "title": "Are Triggers Needed for Document-Level Event Extraction?", "authors": ["Shaden Shaar", "Wayne Chen", "Maitreyi Chatterjee", "Barry Wang", "Wenting Zhao", "Claire Cardie"], "categories": ["cs.CL"], "comment": null, "summary": "Most existing work on event extraction has focused on sentence-level texts\nand presumes the identification of a trigger-span -- a word or phrase in the\ninput that evokes the occurrence of an event of interest. Event arguments are\nthen extracted with respect to the trigger. Indeed, triggers are treated as\nintegral to, and trigger detection as an essential component of, event\nextraction. In this paper, we provide the first investigation of the role of\ntriggers for the more difficult and much less studied task of document-level\nevent extraction. We analyze their usefulness in multiple end-to-end and\npipelined transformer-based event extraction models for three document-level\nevent extraction datasets, measuring performance using triggers of varying\nquality (human-annotated, LLM-generated, keyword-based, and random). We find\nthat whether or not systems benefit from explicitly extracting triggers depends\nboth on dataset characteristics (i.e. the typical number of events per\ndocument) and task-specific information available during extraction (i.e.\nnatural language event schemas). Perhaps surprisingly, we also observe that the\nmere existence of triggers in the input, even random ones, is important for\nprompt-based in-context learning approaches to the task."}
{"id": "2411.12703", "pdf": "https://arxiv.org/pdf/2411.12703.pdf", "abs": "https://arxiv.org/abs/2411.12703", "title": "Strengthening False Information Propagation Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques in comparison to BERT", "authors": ["Ahmed Akib Jawad Karim", "Kazi Hafiz Md Asad", "Aznur Azam"], "categories": ["cs.CL"], "comment": "6 pages, 3 tables and 6 Figures. Submitted to a conference", "summary": "The rapid spread of misinformation, particularly through online platforms,\nunderscores the urgent need for reliable detection systems. This study explores\nthe utilization of machine learning and natural language processing,\nspecifically Support Vector Machines (SVM) and BERT, to detect fake news. We\nemploy three distinct text vectorization methods for SVM: Term Frequency\nInverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW),\nevaluating their effectiveness in distinguishing between genuine and fake news.\nAdditionally, we compare these methods against the transformer large language\nmodel, BERT. Our comprehensive approach includes detailed preprocessing steps,\nrigorous model implementation, and thorough evaluation to determine the most\neffective techniques. The results demonstrate that while BERT achieves superior\naccuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear\nkernel and BoW vectorization also performs exceptionally well, achieving 99.81%\naccuracy and an F1-score of 0.9980. These findings highlight that, despite\nBERT's superior performance, SVM models with BoW and TF-IDF vectorization\nmethods come remarkably close, offering highly competitive performance with the\nadvantage of lower computational requirements."}
{"id": "2412.12644", "pdf": "https://arxiv.org/pdf/2412.12644.pdf", "abs": "https://arxiv.org/abs/2412.12644", "title": "iPrOp: Interactive Prompt Optimization for Large Language Models with a Human in the Loop", "authors": ["Jiahui Li", "Roman Klinger"], "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has made significant contributions to the era of large\nlanguage models, yet its effectiveness depends on the skills of a prompt\nauthor. This paper introduces $\\textit{iPrOp}$, a novel interactive prompt\noptimization approach, to bridge manual prompt engineering and automatic prompt\noptimization while offering users the flexibility to assess evolving prompts.\nWe aim to provide users with task-specific guidance to enhance human engagement\nin the optimization process, which is structured through prompt variations,\ninformative instances, predictions generated by large language models along\nwith their corresponding explanations, and relevant performance metrics. This\napproach empowers users to choose and further refine the prompts based on their\nindividual preferences and needs. It can not only assist non-technical domain\nexperts in generating optimal prompts tailored to their specific tasks or\ndomains, but also enable to study the intrinsic parameters that influence the\nperformance of prompt optimization. The evaluation shows that our approach has\nthe capability to generate improved prompts, leading to enhanced task\nperformance."}
{"id": "2412.13488", "pdf": "https://arxiv.org/pdf/2412.13488.pdf", "abs": "https://arxiv.org/abs/2412.13488", "title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models", "authors": ["Xinxin Liu", "Aaron Thomas", "Cheng Zhang", "Jianyi Cheng", "Yiren Zhao", "Xitong Gao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT, while our open-source framework\nestablishes a reproducible benchmark for future research, which is available at\n[https://github.com/0-ml/speft]."}
{"id": "2501.01805", "pdf": "https://arxiv.org/pdf/2501.01805.pdf", "abs": "https://arxiv.org/abs/2501.01805", "title": "End-to-End Long Document Summarization using Gradient Caching", "authors": ["Rohit Saxena", "Hao Tang", "Frank Keller"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version", "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."}
{"id": "2501.01956", "pdf": "https://arxiv.org/pdf/2501.01956.pdf", "abs": "https://arxiv.org/abs/2501.01956", "title": "Metadata Conditioning Accelerates Language Model Pre-training", "authors": ["Tianyu Gao", "Alexander Wettig", "Luxi He", "Yihe Dong", "Sadhika Malladi", "Danqi Chen"], "categories": ["cs.CL"], "comment": "Accepted to ICML 2025. Code available at\n  https://github.com/princeton-pli/MeCo", "summary": "The vast diversity of styles, domains, and quality levels present in language\nmodel pre-training corpora is essential in developing general model\ncapabilities, but efficiently learning and deploying the correct behaviors\nexemplified in each of these heterogeneous data sources is challenging. To\naddress this, we propose a new method, termed Metadata Conditioning then\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\nMeCo first provides metadata (e.g., URLs like www$.$wikipedia$.$org) alongside\nthe text during training and later uses a cooldown phase with only the standard\ntext, thereby enabling the model to function normally even without metadata.\nMeCo significantly accelerates pre-training across different model scales (600M\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\ninstance, a 1.6B language model trained with MeCo matches the downstream task\nperformance of standard pre-training while using 33% less data. Additionally,\nMeCo enables us to steer language models by conditioning the inference prompt\non either real or fabricated metadata that encodes the desired properties of\nthe output: for example, prepending wikipedia$.$org to reduce harmful\ngenerations or factquizmaster$.$com (fabricated) to improve common knowledge\ntask performance. We also demonstrate that MeCo is compatible with different\ntypes of metadata, such as model-generated topics. MeCo is remarkably simple,\nadds no computational overhead, and demonstrates promise in producing more\ncapable and steerable language models."}
{"id": "2501.06582", "pdf": "https://arxiv.org/pdf/2501.06582.pdf", "abs": "https://arxiv.org/abs/2501.06582", "title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting", "authors": ["Steven H. Wang", "Maksim Zubkov", "Kexin Fan", "Sarah Harrell", "Yuyang Sun", "Wei Chen", "Andreas Plesner", "Roger Wattenhofer"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. See the project page at\n  https://www.atticusprojectai.org/acord", "summary": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community."}
{"id": "2501.14275", "pdf": "https://arxiv.org/pdf/2501.14275.pdf", "abs": "https://arxiv.org/abs/2501.14275", "title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation", "authors": ["Sadegh Mahdavi", "Muchen Li", "Kaiwen Liu", "Christos Thrampoulidis", "Leonid Sigal", "Renjie Liao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 Camera Ready", "summary": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops"}
{"id": "2501.15630", "pdf": "https://arxiv.org/pdf/2501.15630.pdf", "abs": "https://arxiv.org/abs/2501.15630", "title": "Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach", "authors": ["S. M. Yousuf Iqbal Tomal", "Abdullah Al Shafin", "Debojit Bhattacharjee", "MD. Khairul Amin", "Rafiad Sadat Shahir"], "categories": ["cs.CL", "quant-ph"], "comment": "16 pages, 7 figures, 5 tables", "summary": "Recent advances in quantum computing have opened new pathways for enhancing\ndeep learning architectures, particularly in domains characterized by\nhigh-dimensional and context-rich data such as natural language processing\n(NLP). In this work, we present a hybrid classical-quantum Transformer model\nthat integrates a quantum-enhanced attention mechanism into the standard\nclassical architecture. By embedding token representations into a quantum\nHilbert space via parameterized variational circuits and exploiting\nentanglement-aware kernel similarities, the model captures complex semantic\nrelationships beyond the reach of conventional dot-product attention. We\ndemonstrate the effectiveness of this approach across diverse NLP benchmarks,\nshowing improvements in both efficiency and representational capacity. The\nresults section reveal that the quantum attention layer yields globally\ncoherent attention maps and more separable latent features, while requiring\ncomparatively fewer parameters than classical counterparts. These findings\nhighlight the potential of quantum-classical hybrid models to serve as a\npowerful and resource-efficient alternative to existing attention mechanisms in\nNLP."}
{"id": "2502.00299", "pdf": "https://arxiv.org/pdf/2502.00299.pdf", "abs": "https://arxiv.org/abs/2502.00299", "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference", "authors": ["Xiang Liu", "Zhenheng Tang", "Peijie Dong", "Zeyu Li", "Yue Liu", "Bo Li", "Xuming Hu", "Xiaowen Chu"], "categories": ["cs.CL"], "comment": "41 pages", "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."}
{"id": "2502.02384", "pdf": "https://arxiv.org/pdf/2502.02384.pdf", "abs": "https://arxiv.org/abs/2502.02384", "title": "STAIR: Improving Safety Alignment with Introspective Reasoning", "authors": ["Yichi Zhang", "Siyuan Zhang", "Yao Huang", "Zeyu Xia", "Zhengwei Fang", "Xiao Yang", "Ranjie Duan", "Dong Yan", "Yinpeng Dong", "Jun Zhu"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures, ICML2025 Oral", "summary": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR."}
{"id": "2502.04413", "pdf": "https://arxiv.org/pdf/2502.04413.pdf", "abs": "https://arxiv.org/abs/2502.04413", "title": "MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot", "authors": ["Xuejiao Zhao", "Siyan Liu", "Su-Yin Yang", "Chunyan Miao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is a well-suited technique for\nretrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a\nkey module of the healthcare copilot, helping reduce misdiagnosis for\nhealthcare practitioners and patients. However, the diagnostic accuracy and\nspecificity of existing heuristic-based RAG models used in the medical domain\nare inadequate, particularly for diseases with similar manifestations. This\npaper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited\nreasoning for the medical domain that retrieves diagnosis and treatment\nrecommendations based on manifestations. MedRAG systematically constructs a\ncomprehensive four-tier hierarchical diagnostic KG encompassing critical\ndiagnostic differences of various diseases. These differences are dynamically\nintegrated with similar EHRs retrieved from an EHR database, and reasoned\nwithin a large language model. This process enables more accurate and specific\ndecision support, while also proactively providing follow-up questions to\nenhance personalized medical decision-making. MedRAG is evaluated on both a\npublic dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)\ncollected from Tan Tock Seng Hospital, and its performance is compared against\nvarious existing RAG methods. Experimental results show that, leveraging the\ninformation integration and relational abilities of the KG, our MedRAG provides\nmore specific diagnostic insights and outperforms state-of-the-art models in\nreducing misdiagnosis rates. Our code will be available at\nhttps://github.com/SNOWTEAM2023/MedRAG"}
{"id": "2502.11095", "pdf": "https://arxiv.org/pdf/2502.11095.pdf", "abs": "https://arxiv.org/abs/2502.11095", "title": "A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions", "authors": ["Hongbin Na", "Yining Hua", "Zimu Wang", "Tao Shen", "Beibei Yu", "Lilin Wang", "Wei Wang", "John Torous", "Ling Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Mental health is increasingly critical in contemporary healthcare, with\npsychotherapy demanding dynamic, context-sensitive interactions that\ntraditional NLP methods struggle to capture. Large Language Models (LLMs) offer\nsignificant potential for addressing this gap due to their ability to handle\nextensive context and multi-turn reasoning. This review introduces a conceptual\ntaxonomy dividing psychotherapy into interconnected stages--assessment,\ndiagnosis, and treatment--to systematically examine LLM advancements and\nchallenges. Our comprehensive analysis reveals imbalances in current research,\nsuch as a focus on common disorders, linguistic biases, fragmented methods, and\nlimited theoretical integration. We identify critical challenges including\ncapturing dynamic symptom fluctuations, overcoming linguistic and cultural\nbiases, and ensuring diagnostic reliability. Highlighting future directions, we\nadvocate for continuous multi-stage modeling, real-time adaptive systems\ngrounded in psychological theory, and diversified research covering broader\nmental disorders and therapeutic approaches, aiming toward more holistic and\nclinically integrated psychotherapy LLMs systems."}
{"id": "2502.11733", "pdf": "https://arxiv.org/pdf/2502.11733.pdf", "abs": "https://arxiv.org/abs/2502.11733", "title": "Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment", "authors": ["Jonathan Jordan", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "Accepted at The 28th International Conference of Text, Speech and\n  Dialogue (TSD2025)", "summary": "Large Language Models (LLMs) serve not only as chatbots but as key components\nin agent systems, where their common-sense knowledge significantly impacts\nperformance as language-based planners for situated or embodied action. We\nassess LLMs' incremental learning (based on feedback from the environment), and\ncontrolled in-context learning abilities using a text-based environment. We\nintroduce challenging yet interesting set of experiments to test i) how agents\ncan incrementally solve tasks related to every day objects in typical rooms in\na house where each of them are discovered by interacting within the\nenvironment, ii) controlled in-context learning abilities and efficiency of\nagents by providing short info about locations of objects and rooms to check\nhow faster the task can be solved, and finally iii) using synthetic\npseudo-English words to gauge how well LLMs are at inferring meaning of unknown\nwords from environmental feedback. Results show that larger commercial models\nhave a substantial gap in performance compared to open-weight but almost all\nmodels struggle with the synthetic words experiments."}
{"id": "2502.14496", "pdf": "https://arxiv.org/pdf/2502.14496.pdf", "abs": "https://arxiv.org/abs/2502.14496", "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization", "authors": ["Zhitao He", "Zijun Liu", "Peng Li", "Yi R Fung", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "categories": ["cs.CL"], "comment": "28 pages, under review", "summary": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems."}
{"id": "2502.15294", "pdf": "https://arxiv.org/pdf/2502.15294.pdf", "abs": "https://arxiv.org/abs/2502.15294", "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference", "authors": ["Yaohua Tang", "Zhicheng Hu", "Kun Cheng", "Fan Mo", "Qiheng Lv", "Hua Wang", "Zhi Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."}
{"id": "2502.18023", "pdf": "https://arxiv.org/pdf/2502.18023.pdf", "abs": "https://arxiv.org/abs/2502.18023", "title": "Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference", "authors": ["Zhuo Chen", "Xinyu Wang", "Yong Jiang", "Zhen Zhang", "Xinyu Geng", "Pengjun Xie", "Fei Huang", "Kewei Tu"], "categories": ["cs.CL"], "comment": "ACL25 May ARR", "summary": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"}
{"id": "2503.03592", "pdf": "https://arxiv.org/pdf/2503.03592.pdf", "abs": "https://arxiv.org/abs/2503.03592", "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance", "authors": ["Karl Audun Borgersen", "Morten Goodwin"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 6 figures, v2", "summary": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance."}
{"id": "2503.04396", "pdf": "https://arxiv.org/pdf/2503.04396.pdf", "abs": "https://arxiv.org/abs/2503.04396", "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models", "authors": ["Xinyi He", "Yihao Liu", "Mengyu Zhou", "Yeye He", "Haoyu Dong", "Shi Han", "Zejian Yuan", "Dongmei Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main conference, long paper", "summary": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks."}
{"id": "2503.15783", "pdf": "https://arxiv.org/pdf/2503.15783.pdf", "abs": "https://arxiv.org/abs/2503.15783", "title": "Grammar and Gameplay-aligned RL for Game Description Generation with LLMs", "authors": ["Tsunehiko Tanaka", "Edgar Simo-Serra"], "categories": ["cs.CL", "cs.AI"], "comment": "Published at IEEE Conference on Games, 2025", "summary": "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone. Our code is\navailable at https://github.com/tsunehiko/rlgdg"}
{"id": "2503.16856", "pdf": "https://arxiv.org/pdf/2503.16856.pdf", "abs": "https://arxiv.org/abs/2503.16856", "title": "MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers", "authors": ["Yang Tian", "Zheng Lu", "Mingqi Gao", "Zheng Liu", "Bo Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Fully comprehending scientific papers by machines reflects a high level of\nArtificial General Intelligence, requiring the ability to reason across\nfragmented and heterogeneous sources of information, presenting a complex and\npractically significant challenge. While Vision-Language Models (VLMs) have\nmade remarkable strides in various tasks, particularly those involving\nreasoning with evidence source from single image or text page, their ability to\nuse cross-source information for reasoning remains an open problem. This work\npresents MMCR, a high-difficulty benchmark designed to evaluate VLMs' capacity\nfor reasoning with cross-source information from scientific papers. The\nbenchmark comprises 276 high-quality questions, meticulously annotated by\nhumans across 7 subjects and 10 task types. Experiments with 18 VLMs\ndemonstrate that cross-source reasoning presents a substantial challenge for\nexisting models. Notably, even the top-performing model, GPT-4o, achieved only\n48.55% overall accuracy, with only 20% accuracy in multi-table comprehension\ntasks, while the second-best model, Qwen2.5-VL-72B, reached 39.86% overall\naccuracy. Furthermore, we investigated the impact of the Chain-of-Thought (CoT)\ntechnique on cross-source reasoning and observed a detrimental effect on small\nmodels, whereas larger models demonstrated substantially enhanced performance.\nThese results highlight the pressing need to develop VLMs capable of\neffectively utilizing cross-source information for reasoning."}
{"id": "2504.11108", "pdf": "https://arxiv.org/pdf/2504.11108.pdf", "abs": "https://arxiv.org/abs/2504.11108", "title": "Benchmarking Vision Language Models on German Factual Data", "authors": ["RenÃ© Peinl", "Vincent Tischler"], "categories": ["cs.CL", "68T45 (Primary), 68T07 (Secondary), 68T10 (Secondary)", "I.4.0"], "comment": "Peinl, Ren\\'e; Tischler, Vincent (2025): Benchmarking Vision Language\n  Models on German Factual Data. 21st International Conference on Artificial\n  Intelligence Applications and Innovations, 26-29 June, 2025, Limassol, Cyprus\n  (accepted)", "summary": "Similar to LLMs, the development of vision language models is mainly driven\nby English datasets and models trained in English and Chinese language, whereas\nsupport for other languages, even those considered high-resource languages such\nas German, remains significantly weaker. In this work we present an analysis of\nopen-weight VLMs on factual knowledge in the German and English language. We\ndisentangle the image-related aspects from the textual ones by analyzing\naccu-racy with jury-as-a-judge in both prompt languages and images from German\nand international contexts. We found that for celebrities and sights, VLMs\nstruggle because they are lacking visual cognition of German image contents.\nFor animals and plants, the tested models can often correctly identify the\nimage contents ac-cording to the scientific name or English common name but\nfail in German lan-guage. Cars and supermarket products were identified equally\nwell in English and German images across both prompt languages."}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862.pdf", "abs": "https://arxiv.org/abs/2505.02862", "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."}
{"id": "2505.09338", "pdf": "https://arxiv.org/pdf/2505.09338.pdf", "abs": "https://arxiv.org/abs/2505.09338", "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs", "authors": ["Jingcheng Niu", "Xingdi Yuan", "Tong Wang", "Hamidreza Saghir", "Amir H. Abdi"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem."}
{"id": "2505.20888", "pdf": "https://arxiv.org/pdf/2505.20888.pdf", "abs": "https://arxiv.org/abs/2505.20888", "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "authors": ["Chengyu Wang", "Junbing Yan", "Wenrui Cai", "Yuanhao Yue", "Jun Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community."}
{"id": "2505.23224", "pdf": "https://arxiv.org/pdf/2505.23224.pdf", "abs": "https://arxiv.org/abs/2505.23224", "title": "MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration", "authors": ["Zhitao He", "Sandeep Polisetty", "Zhiyuan Fan", "Yuchen Huang", "Shujin Wu", "Yi R. Fung"], "categories": ["cs.CL"], "comment": "18 pages, ACL 2025", "summary": "In recent years, multimodal large language models (MLLMs) have made\nsignificant progress but continue to face inherent challenges in multimodal\nreasoning, which requires multi-level (e.g., perception, reasoning) and\nmulti-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior\nwork on estimating model confidence tends to focus on the overall response for\ntraining and calibration, but fails to assess confidence in each reasoning\nstep, leading to undesirable hallucination snowballing. In this work, we\npresent MMBoundary, a novel framework that advances the knowledge boundary\nawareness of MLLMs through reasoning step confidence calibration. To achieve\nthis, we propose to incorporate complementary textual and cross-modal\nself-rewarding signals to estimate confidence at each step of the MLLM\nreasoning process. In addition to supervised fine-tuning MLLM on this set of\nself-rewarded confidence estimation signal for initial confidence expression\nwarm-up, we introduce a reinforcement learning stage with multiple reward\nfunctions for further aligning model knowledge and calibrating confidence at\neach reasoning step, enhancing reasoning chain self-correction. Empirical\nresults show that MMBoundary significantly outperforms existing methods across\ndiverse domain datasets and metrics, achieving an average of 7.5% reduction in\nmultimodal confidence calibration errors and up to 8.3% improvement in task\nperformance."}
{"id": "2505.24616", "pdf": "https://arxiv.org/pdf/2505.24616.pdf", "abs": "https://arxiv.org/abs/2505.24616", "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX", "authors": ["Nikita Martynov", "Anastasia Mordasheva", "Dmitriy Gorbetskiy", "Danil Astafurov", "Ulyana Isaeva", "Elina Basyrova", "Sergey Skachkov", "Victoria Berestova", "Nikolay Ivanov", "Valeriia Zanina", "Alena Fenogenova"], "categories": ["cs.CL", "cs.AI"], "comment": "178 pages", "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."}
{"id": "2506.15650", "pdf": "https://arxiv.org/pdf/2506.15650.pdf", "abs": "https://arxiv.org/abs/2506.15650", "title": "Oldies but Goldies: The Potential of Character N-grams for Romanian Texts", "authors": ["Dana Lupsa", "Sanda-Maria Avram", "Radu Lupsa"], "categories": ["cs.CL"], "comment": null, "summary": "This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings."}
{"id": "2506.16383", "pdf": "https://arxiv.org/pdf/2506.16383.pdf", "abs": "https://arxiv.org/abs/2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Work draft", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."}
{"id": "2506.20083", "pdf": "https://arxiv.org/pdf/2506.20083.pdf", "abs": "https://arxiv.org/abs/2506.20083", "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "AndrÃ© Freitas"], "categories": ["cs.CL"], "comment": "In progress", "summary": "Integrating compositional and symbolic properties into current distributional\nsemantic spaces can enhance the interpretability, controllability,\ncompositionality, and generalisation capabilities of Transformer-based\nauto-regressive language models (LMs). In this survey, we offer a novel\nperspective on latent space geometry through the lens of compositional\nsemantics, a direction we refer to as \\textit{semantic representation\nlearning}. This direction enables a bridge between symbolic and distributional\nsemantics, helping to mitigate the gap between them. We review and compare\nthree mainstream autoencoder architectures-Variational AutoEncoder (VAE),\nVector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the\ndistinctive latent geometries they induce in relation to semantic structure and\ninterpretability."}
{"id": "2506.20474", "pdf": "https://arxiv.org/pdf/2506.20474.pdf", "abs": "https://arxiv.org/abs/2506.20474", "title": "Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations", "authors": ["Kaixiang Zhang", "Justine Zhang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL"], "comment": "Accepted for publication at CSCW 2025. Code and data available in\n  ConvoKit (https://convokit.cornell.edu)", "summary": "An intrinsic aspect of every conversation is the way talk-time is shared\nbetween multiple speakers. Conversations can be balanced, with each speaker\nclaiming a similar amount of talk-time, or imbalanced when one talks\ndisproportionately. Such overall distributions are the consequence of\ncontinuous negotiations between the speakers throughout the conversation: who\nshould be talking at every point in time, and for how long? In this work we\nintroduce a computational framework for quantifying both the conversation-level\ndistribution of talk-time between speakers, as well as the lower-level dynamics\nthat lead to it. We derive a typology of talk-time sharing dynamics structured\nby several intuitive axes of variation. By applying this framework to a large\ndataset of video-chats between strangers, we confirm that, perhaps\nunsurprisingly, different conversation-level distributions of talk-time are\nperceived differently by speakers, with balanced conversations being preferred\nover imbalanced ones, especially by those who end up talking less. Then we\nreveal that -- even when they lead to the same level of overall balance --\ndifferent types of talk-time sharing dynamics are perceived differently by the\nparticipants, highlighting the relevance of our newly introduced typology.\nFinally, we discuss how our framework offers new tools to designers of\ncomputer-mediated communication platforms, for both human-human and human-AI\ncommunication."}
{"id": "2408.13214", "pdf": "https://arxiv.org/pdf/2408.13214.pdf", "abs": "https://arxiv.org/abs/2408.13214", "title": "EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods", "authors": ["Hongcheng Ding", "Xuanze Zhao", "Ruiting Deng", "Shamsul Nahar Abdullah", "Deshinta Arrova Dewi"], "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "Accurate forecasting of the EUR/USD exchange rate is crucial for investors,\nbusinesses, and policymakers. This paper proposes a novel framework, IUS, that\nintegrates unstructured textual data from news and analysis with structured\ndata on exchange rates and financial indicators to enhance exchange rate\nprediction. The IUS framework employs large language models for sentiment\npolarity scoring and exchange rate movement classification of texts. These\ntextual features are combined with quantitative features and input into a\nCausality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then\nused to forecast the EUR/USD exchange rate. Experiments demonstrate that the\nproposed method outperforms benchmark models, reducing MAE by 10.69% and RMSE\nby 9.56% compared to the best performing baseline. Results also show the\nbenefits of data fusion, with the combination of unstructured and structured\ndata yielding higher accuracy than structured data alone. Furthermore, feature\nselection using the top 12 important quantitative features combined with the\ntextual features proves most effective. The proposed IUS framework and\nOptuna-Bi-LSTM model provide a powerful new approach for exchange rate\nforecasting through multi-source data integration."}
{"id": "2410.10926", "pdf": "https://arxiv.org/pdf/2410.10926.pdf", "abs": "https://arxiv.org/abs/2410.10926", "title": "Federated Data-Efficient Instruction Tuning for Large Language Models", "authors": ["Zhen Qin", "Zhaomin Wu", "Bingsheng He", "Shuiguang Deng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Instruction tuning is a crucial step in improving the responsiveness of\npretrained large language models (LLMs) to human instructions. Federated\nlearning (FL) helps to exploit the use of vast private instruction data from\nclients, becoming popular for LLM tuning by improving data diversity. Existing\nfederated tuning simply consumes all local data, causing excessive\ncomputational overhead and overfitting to local data, while centralized\ndata-efficient solutions are not suitable for FL due to privacy concerns. This\nwork presents FedHDS, a federated data-efficient instruction tuning approach,\nwhich tunes LLMs with a representative subset of edge-side data. It reduces the\ndata redundancy at both intra- and inter-client levels without sharing raw\ndata. Experiments with various LLMs, datasets and partitions show that FedHDS\nimproves Rouge-L on unseen tasks by an average of 10.72% over the SOTA\nfull-data federated instruction tuning methods, while using less than 1.5% of\nthe data samples, improving training efficiency by up to tens of times."}
{"id": "2411.13868", "pdf": "https://arxiv.org/pdf/2411.13868.pdf", "abs": "https://arxiv.org/abs/2411.13868", "title": "Robust Detection of Watermarks for Large Language Models Under Human Edits", "authors": ["Xiang Li", "Feng Ruan", "Huiyuan Wang", "Qi Long", "Weijie J. Su"], "categories": ["stat.ME", "cs.CL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families."}
{"id": "2412.19723", "pdf": "https://arxiv.org/pdf/2412.19723.pdf", "abs": "https://arxiv.org/abs/2412.19723", "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis", "authors": ["Qiushi Sun", "Kanzhi Cheng", "Zichen Ding", "Chuanyang Jin", "Yian Wang", "Fangzhi Xu", "Zhenyu Wu", "Chengyou Jia", "Liheng Chen", "Zhoumianze Liu", "Ben Kao", "Guohao Li", "Junxian He", "Yu Qiao", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "ACL 2025 Camera Ready", "summary": "Graphical User Interface (GUI) agents powered by Vision-Language Models\n(VLMs) have demonstrated human-like computer control capability. Despite their\nutility in advancing digital automation, a critical bottleneck persists:\ncollecting high-quality trajectory data for training. Common practices for\ncollecting such data rely on human supervision or synthetic data generation\nthrough executing pre-defined tasks, which are either resource-intensive or\nunable to guarantee data quality. Moreover, these methods suffer from limited\ndata diversity and significant gaps between synthetic data and real-world\nenvironments. To address these challenges, we propose OS-Genesis, a novel GUI\ndata synthesis pipeline that reverses the conventional trajectory collection\nprocess. Instead of relying on pre-defined tasks, OS-Genesis enables agents\nfirst to perceive environments and perform step-wise interactions, then\nretrospectively derive high-quality tasks to enable trajectory-level\nexploration. A trajectory reward model is then employed to ensure the quality\nof the generated trajectories. We demonstrate that training GUI agents with\nOS-Genesis significantly improves their performance on highly challenging\nonline benchmarks. In-depth analysis further validates OS-Genesis's efficiency\nand its superior data quality and diversity compared to existing synthesis\nmethods. Our codes, data, and checkpoints are available at\nhttps://qiushisun.github.io/OS-Genesis-Home/."}
{"id": "2501.01370", "pdf": "https://arxiv.org/pdf/2501.01370.pdf", "abs": "https://arxiv.org/abs/2501.01370", "title": "Embedding-based Approaches to Hyperpartisan News Detection", "authors": ["Karthik Mohan", "Pengyu Chen"], "categories": ["cs.LG", "cs.CL"], "comment": "The authorship dispute of this article could not be resolved, and it\n  was submitted without the consent of P. Chen", "summary": "In this paper, we describe our systems in which the objective is to determine\nwhether a given news article could be considered as hyperpartisan.\nHyperpartisan news is news that takes an extremely polarized political\nstandpoint with an intention of creating political divide among the public. We\nattempted several approaches, including n-grams, sentiment analysis, as well as\nsentence and document representation using pre-tained ELMo. Our best system\nusing pre-trained ELMo with Bidirectional LSTM achieved an accuracy of 83%\nthrough 10-fold cross-validation without much hyperparameter tuning."}
{"id": "2501.04931", "pdf": "https://arxiv.org/pdf/2501.04931.pdf", "abs": "https://arxiv.org/abs/2501.04931", "title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency", "authors": ["Shiji Zhao", "Ranjie Duan", "Fengxiang Wang", "Chi Chen", "Caixin Kang", "Shouwei Ruan", "Jialing Tao", "YueFeng Chen", "Hui Xue", "Xingxing Wei"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "ICCV2025", "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive performance\nand have been put into practical use in commercial applications, but they still\nhave potential safety mechanism vulnerabilities. Jailbreak attacks are red\nteaming methods that aim to bypass safety mechanisms and discover MLLMs'\npotential risks. Existing MLLMs' jailbreak methods often bypass the model's\nsafety mechanism through complex optimization methods or carefully designed\nimage and text prompts. Despite achieving some progress, they have a low attack\nsuccess rate on commercial closed-source MLLMs. Unlike previous research, we\nempirically find that there exists a Shuffle Inconsistency between MLLMs'\ncomprehension ability and safety ability for the shuffled harmful instruction.\nThat is, from the perspective of comprehension ability, MLLMs can understand\nthe shuffled harmful text-image instructions well. However, they can be easily\nbypassed by the shuffled harmful instructions from the perspective of safety\nability, leading to harmful responses. Then we innovatively propose a\ntext-image jailbreak attack named SI-Attack. Specifically, to fully utilize the\nShuffle Inconsistency and overcome the shuffle randomness, we apply a\nquery-based black-box optimization method to select the most harmful shuffled\ninputs based on the feedback of the toxic judge model. A series of experiments\nshow that SI-Attack can improve the attack's performance on three benchmarks.\nIn particular, SI-Attack can obviously improve the attack success rate for\ncommercial MLLMs such as GPT-4o or Claude-3.5-Sonnet."}
{"id": "2502.14949", "pdf": "https://arxiv.org/pdf/2502.14949.pdf", "abs": "https://arxiv.org/abs/2502.14949", "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding", "authors": ["Ahmed Heakl", "Abdullah Sohail", "Mukul Ranjan", "Rania Hossam", "Ghazi Shazan Ahmad", "Mohamed El-Geish", "Omar Maher", "Zhiqiang Shen", "Fahad Khan", "Salman Khan"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "17 pages, 5 figures, ACL 2025", "summary": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document\nprocessing, robust text recognition has become increasingly critical for\nknowledge extraction. While OCR (Optical Character Recognition) for English and\nother languages benefits from large datasets and well-established benchmarks,\nArabic OCR faces unique challenges due to its cursive script, right-to-left\ntext flow, and complex typographic and calligraphic features. We present\nKITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\nmajor domains and 36 sub-domains, encompassing diverse document types including\nhandwritten text, structured tables, and specialized coverage of 21 chart types\nfor business intelligence. Our findings show that modern vision-language models\n(such as GPT-4o, Gemini, and Qwen) outperform traditional OCR approaches (like\nEasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate\n(CER). Furthermore, we highlight significant limitations of current Arabic OCR\nmodels, particularly in PDF-to-Markdown conversion, where the best model\nGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\naccurately recognizing Arabic text, including issues with complex fonts,\nnumeral recognition errors, word elongation, and table structure detection.\nThis work establishes a rigorous evaluation framework that can drive\nimprovements in Arabic document analysis methods and bridge the performance gap\nwith English OCR technologies."}
{"id": "2502.20380", "pdf": "https://arxiv.org/pdf/2502.20380.pdf", "abs": "https://arxiv.org/abs/2502.20380", "title": "Multi-Turn Code Generation Through Single-Step Rewards", "authors": ["Arnav Kumar Jain", "Gonzalo Gonzalez-Pumariega", "Wayne Chen", "Alexander M Rush", "Wenting Zhao", "Sanjiban Choudhury"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages (not including references or appendix); 5 figures (in main\n  paper); (v2) camera-ready version", "summary": "We address the problem of code generation from multi-turn execution feedback.\nExisting methods either generate code without feedback or use complex,\nhierarchical reinforcement learning to optimize multi-turn rewards. We propose\na simple yet scalable approach, $\\mu$Code, that solves multi-turn code\ngeneration using only single-step rewards. Our key insight is that code\ngeneration is a one-step recoverable MDP, where the correct code can be\nrecovered from any intermediate code state in a single turn. $\\mu$Code\niteratively trains both a generator to provide code solutions conditioned on\nmulti-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant\nimprovements over the state-of-the-art baselines. We provide analysis of the\ndesign choices of the reward models and policy, and show the efficacy of\n$\\mu$Code at utilizing the execution feedback. Our code is available at\nhttps://github.com/portal-cornell/muCode."}
{"id": "2502.20758", "pdf": "https://arxiv.org/pdf/2502.20758.pdf", "abs": "https://arxiv.org/abs/2502.20758", "title": "Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth", "authors": ["Seyed Pouyan Mousavi Davoudi", "Amin Gholami Davodi", "Alireza Amiri-Margavi", "Mahdi Jafari"], "categories": ["stat.AP", "cs.AI", "cs.CL"], "comment": "7pages", "summary": "We introduce a new approach in which several advanced large language\nmodels-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct,\nClaude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer\nintricate, doctoral-level probability problems without relying on any single\n\"correct\" reference. Rather than depending on an established ground truth, our\ninvestigation focuses on how agreement among diverse models can signal the\nreliability of their outputs and, by extension, reflect the overall quality of\nthe generated questions. To measure this inter-model alignment, we apply a\nsuite of statistical evaluations, including chi-square tests, Fleiss' Kappa\ncoefficients, and confidence interval calculations, thereby capturing both\nprecision in answers and clarity in question phrasing. Our analysis reveals\nthat Claude and Gemini tend to frame questions more coherently and\nunambiguously, which is evidenced by their tighter confidence intervals and\ngreater concordance with responding agents. In contrast, LLAMA exhibits wider\nconfidence bands and a lower level of agreement, indicating more variability\nand reduced consistency in its question formulations. These observations\nsupport the notion that a multi-model collaborative strategy not only improves\nanswer dependability but also offers an effective, data-driven mechanism for\nevaluating and refining question quality when no definitive solution exists.\nUltimately, this work delivers actionable insights into enhancing AI-guided\nreasoning processes through coordinated interactions among heterogeneous\nlanguage models."}
{"id": "2503.03313", "pdf": "https://arxiv.org/pdf/2503.03313.pdf", "abs": "https://arxiv.org/abs/2503.03313", "title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models", "authors": ["Xi Zhu", "Haochen Xue", "Ziwei Zhao", "Wujiang Xu", "Jingyuan Huang", "Minghao Guo", "Qifan Wang", "Kaixiong Zhou", "Yongfeng Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM."}
{"id": "2503.10432", "pdf": "https://arxiv.org/pdf/2503.10432.pdf", "abs": "https://arxiv.org/abs/2503.10432", "title": "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language Models", "authors": ["Can Zheng", "Jiguang He", "Guofa Cai", "Zitong Yu", "Chung G. Kang"], "categories": ["cs.LG", "cs.CL"], "comment": "6 pages, 7 figures, conference", "summary": "In this paper, we propose BeamLLM, a vision-aided millimeter-wave (mmWave)\nbeam prediction framework leveraging large language models (LLMs) to address\nthe challenges of high training overhead and latency in mmWave communication\nsystems. By combining computer vision (CV) with LLMs' cross-modal reasoning\ncapabilities, the framework extracts user equipment (UE) positional features\nfrom RGB images and aligns visual-temporal features with LLMs' semantic space\nthrough reprogramming techniques. Evaluated on a realistic\nvehicle-to-infrastructure (V2I) scenario, the proposed method achieves 61.01%\ntop-1 accuracy and 97.39% top-3 accuracy in standard prediction tasks,\nsignificantly outperforming traditional deep learning models. In few-shot\nprediction scenarios, the performance degradation is limited to 12.56% (top-1)\nand 5.55% (top-3) from time sample 1 to 10, demonstrating superior prediction\ncapability."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897.pdf", "abs": "https://arxiv.org/abs/2505.19897", "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2506.11604", "pdf": "https://arxiv.org/pdf/2506.11604.pdf", "abs": "https://arxiv.org/abs/2506.11604", "title": "VLM@school -- Evaluation of AI image understanding on German middle school knowledge", "authors": ["RenÃ© Peinl", "Vincent Tischler"], "categories": ["cs.AI", "cs.CL", "cs.CV", "68T45 (Primary), 68T07 (Secondary), 68T09 (Secondary)", "I.4.0"], "comment": "Peinl, Ren\\'e; Tischler, Vincent (2025): VLM@school - Evaluation of\n  AI image understanding on German middle school knowledge. Future Technologies\n  Conference (FTC) 2025, Munich, Germany 2025 (accepted)", "summary": "This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems."}
