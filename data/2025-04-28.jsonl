{"id": "2504.17792", "pdf": "https://arxiv.org/pdf/2504.17792.pdf", "abs": "https://arxiv.org/abs/2504.17792", "title": "My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data", "authors": ["Hauke Sandhaus", "Angel Hsing-Chi Hwang", "Wendy Ju", "Qian Yang"], "categories": ["cs.HC", "cs.AI", "cs.DB", "E.m; H.2.8; J.1"], "comment": "To appear in Proc. ACM Hum.-Comput. Interact., Computer-Supported\n  Cooperative Work & Social Computing (CSCW), 2025", "summary": "Safety-critical data, such as crash and near-crash records, are crucial to\nimproving autonomous vehicle (AV) design and development. Sharing such data\nacross AV companies, academic researchers, regulators, and the public can help\nmake all AVs safer. However, AV companies rarely share safety-critical data\nexternally. This paper aims to pinpoint why AV companies are reluctant to share\nsafety-critical data, with an eye on how these barriers can inform new\napproaches to promote sharing. We interviewed twelve AV company employees who\nactively work with such data in their day-to-day work. Findings suggest two\nkey, previously unknown barriers to data sharing: (1) Datasets inherently embed\nsalient knowledge that is key to improving AV safety and are\nresource-intensive. Therefore, data sharing, even within a company, is fraught\nwith politics. (2) Interviewees believed AV safety knowledge is private\nknowledge that brings competitive edges to their companies, rather than public\nknowledge for social good. We discuss the implications of these findings for\nincentivizing and enabling safety-critical AV data sharing, specifically,\nimplications for new approaches to (1) debating and stratifying public and\nprivate AV safety knowledge, (2) innovating data tools and data sharing\npipelines that enable easier sharing of public AV safety data and knowledge;\n(3) offsetting costs of curating safety-critical data and incentivizing data\nsharing."}
{"id": "2504.17934", "pdf": "https://arxiv.org/pdf/2504.17934.pdf", "abs": "https://arxiv.org/abs/2504.17934", "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation."}
{"id": "2504.17960", "pdf": "https://arxiv.org/pdf/2504.17960.pdf", "abs": "https://arxiv.org/abs/2504.17960", "title": "VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics", "authors": ["Kazi Shahrukh Omar", "Shuaijie Wang", "Ridhuparan Kungumaraju", "Tanvi Bhatt", "Fabio Miranda"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted for publication in the IEEE Transactions on Visualization\n  and Computer Graphics. VIGMA is available at https://github.com/komar41/VIGMA", "summary": "Gait disorders are commonly observed in older adults, who frequently\nexperience various issues related to walking. Additionally, researchers and\nclinicians extensively investigate mobility related to gait in typically and\natypically developing children, athletes, and individuals with orthopedic and\nneurological disorders. Effective gait analysis enables the understanding of\nthe causal mechanisms of mobility and balance control of patients, the\ndevelopment of tailored treatment plans to improve mobility, the reduction of\nfall risk, and the tracking of rehabilitation progress. However, analyzing gait\ndata is a complex task due to the multivariate nature of the data, the large\nvolume of information to be interpreted, and the technical skills required.\nExisting tools for gait analysis are often limited to specific patient groups\n(e.g., cerebral palsy), only handle a specific subset of tasks in the entire\nworkflow, and are not openly accessible. To address these shortcomings, we\nconducted a requirements assessment with gait practitioners (e.g., researchers,\nclinicians) via surveys and identified key components of the workflow,\nincluding (1) data processing and (2) data analysis and visualization. Based on\nthe findings, we designed VIGMA, an open-access visual analytics framework\nintegrated with computational notebooks and a Python library, to meet the\nidentified requirements. Notably, the framework supports analytical\ncapabilities for assessing disease progression and for comparing multiple\npatient groups. We validated the framework through usage scenarios with experts\nspecializing in gait and mobility rehabilitation. VIGMA is available at\nhttps://github.com/komar41/VIGMA."}
{"id": "2504.17964", "pdf": "https://arxiv.org/pdf/2504.17964.pdf", "abs": "https://arxiv.org/abs/2504.17964", "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content", "authors": ["Celia Chen", "Alex Leitch"], "categories": ["cs.HC", "cs.AI"], "comment": "Under review at ACM Web Science Conference 2025's Human-GenAI\n  Interactions Workshop, 4 pages", "summary": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments."}
{"id": "2504.17974", "pdf": "https://arxiv.org/pdf/2504.17974.pdf", "abs": "https://arxiv.org/abs/2504.17974", "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "authors": ["Sabur Butt", "Fazlourrahman Balouchzahi", "Ahmad Imam Amjad", "Maaz Amjad", "Hector G. Ceballos", "Salud Maria Jimenez-Zafra"], "categories": ["cs.CL"], "comment": null, "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages."}
{"id": "2504.17997", "pdf": "https://arxiv.org/pdf/2504.17997.pdf", "abs": "https://arxiv.org/abs/2504.17997", "title": "Chatperone: An LLM-Based Negotiable Scaffolding System for Mediating Adolescent Mobile Interactions", "authors": ["Suwon Yoon", "Seungwon Yang", "Jeongwon Choi", "Wonjeong Park", "Inseok Hwang"], "categories": ["cs.HC"], "comment": "5 pages, Workshop Paper", "summary": "Adolescents' uncontrolled exposure to digital content can negatively impact\ntheir development. Traditional regulatory methods, such as time limits or app\nrestrictions, often take a rigid approach, ignoring adolescents'\ndecision-making abilities. Another issue is the lack of content and services\ntailored for adolescents. To address this, we propose Chatperone, a concept of\na system that provides adaptive scaffolding to support adolescents. Chatperone\nfosters healthy mobile interactions through three key modules: Perception,\nNegotiation, and Moderation. This paper outlines these modules' functionalities\nand discusses considerations for real-world implementation."}
{"id": "2504.17993", "pdf": "https://arxiv.org/pdf/2504.17993.pdf", "abs": "https://arxiv.org/abs/2504.17993", "title": "Improving LLM Personas via Rationalization with Psychological Scaffolds", "authors": ["Brihi Joshi", "Xiang Ren", "Swabha Swayamdipta", "Rik Koncel-Kedziorski", "Tim Paek"], "categories": ["cs.CL"], "comment": null, "summary": "Language models prompted with a user description or persona can predict a\nuser's preferences and opinions, but existing approaches to building personas\n-- based solely on a user's demographic attributes and/or prior judgments --\nfail to capture the underlying reasoning behind said user judgments. We\nintroduce PB&J (Psychology of Behavior and Judgments), a framework that\nimproves LLM personas by incorporating rationales of why a user might make\nspecific judgments. These rationales are LLM-generated, and aim to reason about\na user's behavior on the basis of their experiences, personality traits or\nbeliefs. This is done using psychological scaffolds -- structured frameworks\ngrounded in theories such as the Big 5 Personality Traits and Primal World\nBeliefs -- that help provide structure to the generated rationales. Experiments\non public opinion and movie preference prediction tasks demonstrate that LLM\npersonas augmented with PB&J rationales consistently outperform methods using\nonly a user's demographics and/or judgments. Additionally, LLM personas\nconstructed using scaffolds describing user beliefs perform competitively with\nthose using human-written rationales."}
{"id": "2504.17999", "pdf": "https://arxiv.org/pdf/2504.17999.pdf", "abs": "https://arxiv.org/abs/2504.17999", "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving", "authors": ["Chang Xiao", "Brenda Yang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. Our statistical analysis of\ncomputational savings, combined with crowdsourced user studies, provides\ninsights into the trade-offs between service efficiency and user satisfaction,\ndemonstrating that our method can significantly reduce computational\nconsumption up to 16.8\\%. This context-aware computational resource management\nstrategy presents a practical framework for enhancing system efficiency in\ncloud-based conversational AI interfaces without compromising user experience."}
{"id": "2504.18012", "pdf": "https://arxiv.org/pdf/2504.18012.pdf", "abs": "https://arxiv.org/abs/2504.18012", "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation", "authors": ["Zhuang Yu", "Shiliang Sun", "Jing Zhao", "Tengfei Song", "Hao Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) aims to improve translation quality by\nleveraging auxiliary modalities such as images alongside textual input. While\nrecent advances in large-scale pre-trained language and vision models have\nsignificantly benefited unimodal natural language processing tasks, their\neffectiveness and role in MMT remain underexplored. In this work, we conduct a\nsystematic study on the impact of pre-trained encoders and decoders in\nmultimodal translation models. Specifically, we analyze how different training\nstrategies, from training from scratch to using pre-trained and partially\nfrozen components, affect translation performance under a unified MMT\nframework. Experiments are carried out on the Multi30K and CoMMuTE dataset\nacross English-German and English-French translation tasks. Our results reveal\nthat pre-training plays a crucial yet asymmetrical role in multimodal settings:\npre-trained decoders consistently yield more fluent and accurate outputs, while\npre-trained encoders show varied effects depending on the quality of\nvisual-text alignment. Furthermore, we provide insights into the interplay\nbetween modality fusion and pre-trained components, offering guidance for\nfuture architecture design in multimodal translation systems."}
{"id": "2504.18189", "pdf": "https://arxiv.org/pdf/2504.18189.pdf", "abs": "https://arxiv.org/abs/2504.18189", "title": "ClassComet: Exploring and Designing AI-generated Danmaku in Educational Videos to Enhance Online Learning", "authors": ["Zipeng Ji", "Pengcheng An", "Jian Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "Danmaku, users' live comments synchronized with, and overlaying on videos,\nhas recently shown potential in promoting online video-based learning. However,\nuser-generated danmaku can be scarce-especially in newer or less viewed videos\nand its quality is unpredictable, limiting its educational impact. This paper\nexplores how large multimodal models (LMM) can be leveraged to automatically\ngenerate effective, high-quality danmaku. We first conducted a formative study\nto identify the desirable characteristics of content- and emotion-related\ndanmaku in educational videos. Based on the obtained insights, we developed\nClassComet, an educational video platform with novel LMM-driven techniques for\ngenerating relevant types of danmaku to enhance video-based learning. Through\nuser studies, we examined the quality of generated danmaku and their influence\non learning experiences. The results indicate that our generated danmaku is\ncomparable to human-created ones, and videos with both content- and\nemotion-related danmaku showed significant improvement in viewers' engagement\nand learning outcome."}
{"id": "2504.18041", "pdf": "https://arxiv.org/pdf/2504.18041.pdf", "abs": "https://arxiv.org/abs/2504.18041", "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "authors": ["Bang An", "Shiyue Zhang", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs."}
{"id": "2504.18238", "pdf": "https://arxiv.org/pdf/2504.18238.pdf", "abs": "https://arxiv.org/abs/2504.18238", "title": "SecCityVR: Visualization and Collaborative Exploration of Software Vulnerabilities in Virtual Reality", "authors": ["Dennis Wüppelman", "Enes Yigitbas"], "categories": ["cs.HC"], "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 29th International Conference on Evaluation and Assessment\n  in Software Engineering (EASE 2025)", "summary": "Security vulnerabilities in software systems represent significant risks as\npotential entry points for malicious attacks. Traditional dashboards that\ndisplay the results of static analysis security testing often use 2D or 3D\nvisualizations, which tend to lack the spatial details required to effectively\nreveal issues such as the propagation of vulnerabilities across the codebase or\nthe appearance of concurrent vulnerabilities. Additionally, most reporting\nsolutions only treat the analysis results as an artifact that can be reviewed\nor edited asynchronously by developers, limiting real-time, collaborative\nexploration. To the best of our knowledge, no VR-based approach exists for the\nvisualization and interactive exploration of software security vulnerabilities.\nAddressing these challenges, the virtual reality (VR) environment SecCityVR was\ndeveloped as a proof-of-concept implementation that employs the code city\nmetaphor within VR to visualize software security vulnerabilities as colored\nbuilding floors inside the surrounding virtual city. By integrating the\napplication's call graph, vulnerabilities are contextualized within related\nsoftware components. SecCityVR supports multi-user collaboration and\ninteractive exploration. It provides explanations and mitigations for detected\nissues. A user study comparing SecCityVR with the traditional dashboard\nfind-sec-bugs showed the VR approach provided a favorable experience, with\nhigher usability, lower temporal demand, and significantly lower frustration\ndespite having longer task completion times. This paper and its results\ncontribute to the fields of collaborative and secure software engineering, as\nwell as software visualization. It provides a new application of VR code cities\nto visualize security vulnerabilities, as well as a novel environment for\nsecurity audits using collaborative and immersive technologies."}
{"id": "2504.18053", "pdf": "https://arxiv.org/pdf/2504.18053.pdf", "abs": "https://arxiv.org/abs/2504.18053", "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models", "authors": ["Jianyu Liu", "Hangyu Guo", "Ranjie Duan", "Xingyuan Bu", "Yancheng He", "Shilong Li", "Hui Huang", "Jiaheng Liu", "Yucheng Wang", "Chenchen Jing", "Xingwei Qu", "Xiao Zhang", "Yingshui Tan", "Yanan Wu", "Jihao Gu", "Yangguang Li", "Jianke Zhu"], "categories": ["cs.CL", "cs.CV"], "comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM", "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM."}
{"id": "2504.18410", "pdf": "https://arxiv.org/pdf/2504.18410.pdf", "abs": "https://arxiv.org/abs/2504.18410", "title": "Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection on Parental Verbal Abuse", "authors": ["Jiaying Fu", "Jialin Gu", "Tianyue Gong", "Tiange Zhou"], "categories": ["cs.HC"], "comment": "8 pages, 5 figures, accetped by 30th International Symposium on\n  Electronic Art (ISEA 2025)", "summary": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems."}
{"id": "2504.18058", "pdf": "https://arxiv.org/pdf/2504.18058.pdf", "abs": "https://arxiv.org/abs/2504.18058", "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents", "authors": ["Sijia Cheng", "Wen-Yu Chang", "Yun-Nung Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IWSDS 2025", "summary": "The integration of dialogue agents into the sales domain requires a deep\nunderstanding of how these systems interact with users possessing diverse\npersonas. This study explores the influence of user personas, defined using the\nMyers-Briggs Type Indicator (MBTI), on the interaction quality and performance\nof sales-oriented dialogue agents. Through large-scale testing and analysis, we\nassess the pre-trained agent's effectiveness, adaptability, and personalization\ncapabilities across a wide range of MBTI-defined user types. Our findings\nreveal significant patterns in interaction dynamics, task completion rates, and\ndialogue naturalness, underscoring the future potential for dialogue agents to\nrefine their strategies to better align with varying personality traits. This\nwork not only provides actionable insights for building more adaptive and\nuser-centric conversational systems in the sales domain but also contributes\nbroadly to the field by releasing persona-defined user simulators. These\nsimulators, unconstrained by domain, offer valuable tools for future research\nand demonstrate the potential for scaling personalized dialogue systems across\ndiverse applications."}
{"id": "2504.18496", "pdf": "https://arxiv.org/pdf/2504.18496.pdf", "abs": "https://arxiv.org/abs/2504.18496", "title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review", "authors": ["Raymond Fok", "Joseph Chee Chang", "Marissa Radensky", "Pao Siangliulue", "Jonathan Bragg", "Amy X. Zhang", "Daniel S. Weld"], "categories": ["cs.HC"], "comment": null, "summary": "Comprehensive literature review requires synthesizing vast amounts of\nresearch -- a labor intensive and cognitively demanding process. Most prior\nwork focuses either on helping researchers deeply understand a few papers\n(e.g., for triaging or reading), or retrieving from and visualizing a vast\ncorpus. Deep analysis and synthesis of large paper collections (e.g., to\nproduce a survey paper) is largely conducted manually with little support. We\npresent DimInd, an interactive system that scaffolds literature review across\nlarge paper collections through LLM-generated structured representations.\nDimInd scaffolds literature understanding with multiple levels of compression,\nfrom papers, to faceted literature comparison tables with information extracted\nfrom individual papers, to taxonomies of concepts, to narrative syntheses.\nUsers are guided through these successive information transformations while\nmaintaining provenance to source text. In an evaluation with 23 researchers,\nDimInd supported participants in extracting information and conceptually\norganizing papers with less effort compared to a ChatGPT-assisted baseline\nworkflow."}
{"id": "2504.18070", "pdf": "https://arxiv.org/pdf/2504.18070.pdf", "abs": "https://arxiv.org/abs/2504.18070", "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths", "authors": ["Jingjin Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG", "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning."}
{"id": "2504.17823", "pdf": "https://arxiv.org/pdf/2504.17823.pdf", "abs": "https://arxiv.org/abs/2504.17823", "title": "The Cloud Weaving Model for AI development", "authors": ["Darcy Kim", "Aida Kalender", "Sennay Ghebreab", "Giovanni Sileno"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "presented at alt.CHI 2025, Yokohama", "summary": "While analysing challenges in pilot projects developing AI with marginalized\ncommunities, we found it difficult to express them within commonly used\nparadigms. We therefore constructed an alternative conceptual framework to\nground AI development in the social fabric -- the Cloud Weaving Model --\ninspired (amongst others) by indigenous knowledge, motifs from nature, and\nEastern traditions. This paper introduces and elaborates on the fundamental\nelements of the model (clouds, spiders, threads, spiderwebs, and weather) and\ntheir interpretation in an AI context. The framework is then applied to\ncomprehend patterns observed in co-creation pilots approaching marginalized\ncommunities, highlighting neglected yet relevant dimensions for responsible AI\ndevelopment."}
{"id": "2504.18080", "pdf": "https://arxiv.org/pdf/2504.18080.pdf", "abs": "https://arxiv.org/abs/2504.18080", "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization", "authors": ["Wataru Kawakami", "Keita Suzuki", "Junichiro Iwasawa"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications."}
{"id": "2504.17906", "pdf": "https://arxiv.org/pdf/2504.17906.pdf", "abs": "https://arxiv.org/abs/2504.17906", "title": "\"Shifting Access Control Left\" using Asset and Goal Models", "authors": ["Shamal Faily"], "categories": ["cs.CR", "cs.HC"], "comment": "To appear in the NATO ICMCIS proceedings", "summary": "Access control needs have broad design implications, but access control\nspecifications may be elicited before, during, or after these needs are\ncaptured. Because access control knowledge is distributed, we need to make\nknowledge asymmetries more transparent, and use expertise already available to\nstakeholders. In this paper, we present a tool-supported technique identifying\nknowledge asymmetries around access control based on asset and goal models.\nUsing simple and conventional modelling languages that complement different\ndesign techniques, we provide boundary objects to make access control\ntransparent, thereby making knowledge about access control concerns more\nsymmetric. We illustrate this technique using a case study example considering\nthe suitability of a reusable software component in a new military air system."}
{"id": "2504.18085", "pdf": "https://arxiv.org/pdf/2504.18085.pdf", "abs": "https://arxiv.org/abs/2504.18085", "title": "Random-Set Large Language Models", "authors": ["Muhammad Mubashar", "Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 6 figures", "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating."}
{"id": "2504.17921", "pdf": "https://arxiv.org/pdf/2504.17921.pdf", "abs": "https://arxiv.org/abs/2504.17921", "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "authors": ["Mateo Espinosa Zarlenga", "Gabriele Dominici", "Pietro Barbiero", "Zohreh Shams", "Mateja Jamnik"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "comment": null, "summary": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions."}
{"id": "2504.18104", "pdf": "https://arxiv.org/pdf/2504.18104.pdf", "abs": "https://arxiv.org/abs/2504.18104", "title": "Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation", "authors": ["Yinglong Yu", "Hao Shen", "Zhengyi Lyu", "Qi He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In response to the growing problem of misinformation in the context of\nglobalization and informatization, this paper proposes a classification method\nfor fact-check-worthiness estimation based on prompt tuning. We construct a\nmodel for fact-check-worthiness estimation at the methodological level using\nprompt tuning. By applying designed prompt templates to large language models,\nwe establish in-context learning and leverage prompt tuning technology to\nimprove the accuracy of determining whether claims have fact-check-worthiness,\nparticularly when dealing with limited or unlabeled data. Through extensive\nexperiments on public datasets, we demonstrate that the proposed method\nsurpasses or matches multiple baseline methods in the classification task of\nfact-check-worthiness estimation assessment, including classical pre-trained\nmodels such as BERT, as well as recent popular large models like GPT-3.5 and\nGPT-4. Experiments show that the prompt tuning-based method proposed in this\nstudy exhibits certain advantages in evaluation metrics such as F1 score and\naccuracy, thereby effectively validating its effectiveness and advancement in\nthe task of fact-check-worthiness estimation."}
{"id": "2504.18010", "pdf": "https://arxiv.org/pdf/2504.18010.pdf", "abs": "https://arxiv.org/abs/2504.18010", "title": "Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation", "authors": ["Zilin Huang", "Zihao Sheng", "Zhengyang Wan", "Yansong Qu", "Yuhao Luo", "Boyue Wang", "Pei Li", "Yen-Jung Chen", "Jiancong Chen", "Keke Long", "Jiayi Meng", "Yue Leng", "Sikai Chen"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "15 pages, 7 figures", "summary": "Recent advances in autonomous system simulation platforms have significantly\nenhanced the safe and scalable testing of driving policies. However, existing\nsimulators do not yet fully meet the needs of future transportation research,\nparticularly in modeling socially-aware driving agents and enabling effective\nhuman-AI collaboration. This paper introduces Sky-Drive, a novel distributed\nmulti-agent simulation platform that addresses these limitations through four\nkey innovations: (a) a distributed architecture for synchronized simulation\nacross multiple terminals; (b) a multi-modal human-in-the-loop framework\nintegrating diverse sensors to collect rich behavioral data; (c) a human-AI\ncollaboration mechanism supporting continuous and adaptive knowledge exchange;\nand (d) a digital twin (DT) framework for constructing high-fidelity virtual\nreplicas of real-world transportation environments. Sky-Drive supports diverse\napplications such as autonomous vehicle (AV)-vulnerable road user (VRU)\ninteraction modeling, human-in-the-loop training, socially-aware reinforcement\nlearning, personalized driving policy, and customized scenario generation.\nFuture extensions will incorporate foundation models for context-aware decision\nsupport and hardware-in-the-loop (HIL) testing for real-world validation. By\nbridging scenario generation, data collection, algorithm training, and hardware\nintegration, Sky-Drive has the potential to become a foundational platform for\nthe next generation of socially-aware and human-centered autonomous\ntransportation research. The demo video and code are available\nat:https://sky-lab-uw.github.io/Sky-Drive-website/"}
{"id": "2504.18106", "pdf": "https://arxiv.org/pdf/2504.18106.pdf", "abs": "https://arxiv.org/abs/2504.18106", "title": "Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering", "authors": ["Yinglong Yu", "Zhaopu Yao", "Fang Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies."}
{"id": "2504.18044", "pdf": "https://arxiv.org/pdf/2504.18044.pdf", "abs": "https://arxiv.org/abs/2504.18044", "title": "AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How", "authors": ["Omid Veisi", "Sasan Bahrami", "Roman Englert", "Claudia Müller"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.IT", "math.IT"], "comment": "Accepted for presentation at the ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW) 2025. To appear in Proceedings\n  of the ACM on Human-Computer Interaction (PACM HCI)", "summary": "Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social\nComputing requires the examination of ethical and social norms to ensure safe\nincorporation into human life. We conducted a mixed-method study, including an\nonline survey with 111 participants and an interview study with 38 experts, to\ninvestigate the AI ethics and social norms in ChatGPT as everyday life tools.\nThis study aims to evaluate whether ChatGPT in an empirical context operates\nfollowing ethics and social norms, which is critical for understanding actions\nin industrial and academic research and achieving machine ethics. The findings\nof this study provide initial insights into six important aspects of AI ethics,\nincluding bias, trustworthiness, security, toxicology, social norms, and\nethical data. Significant obstacles related to transparency and bias in\nunsupervised data collection methods are identified as ChatGPT's ethical\nconcerns."}
{"id": "2504.18114", "pdf": "https://arxiv.org/pdf/2504.18114.pdf", "abs": "https://arxiv.org/abs/2504.18114", "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "authors": ["Atharva Kulkarni", "Yuan Zhang", "Joel Ruben Antony Moniz", "Xiou Ge", "Bo-Hsiang Tseng", "Dhivya Piraviperumal", "Swabha Swayamdipta", "Hong Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them."}
{"id": "2504.18191", "pdf": "https://arxiv.org/pdf/2504.18191.pdf", "abs": "https://arxiv.org/abs/2504.18191", "title": "MVVM Revisited: Exploring Design Variants of the Model-View-ViewModel Pattern", "authors": ["Mario Fuksa", "Sandro Speth", "Steffen Becker"], "categories": ["cs.SE", "cs.HC"], "comment": "Conference: 28th International Conference on Enterprise Design,\n  Operations, and Computing (EDOC 2024) 16 pages", "summary": "Many enterprise software systems provide complex Graphical User Interfaces\n(GUIs) that need robust architectural patterns for well-structured software\ndesign. However, popular GUI architectural patterns like Model-View-ViewModel\n(MVVM) often lack detailed implementation guidance, leading GUI developers to\ninappropriately use the pattern without a comprehensive overview of design\nvariants and often-mentioned trade-offs. Therefore, this paper presents an\nextensive review of MVVM design aspects and trade-offs, extending beyond the\nstandard MVVM definition. We conducted a multivocal literature review (MLR),\nincluding white and gray literature, to cover essential knowledge from blogs,\npublished papers, and other unpublished formats like books. Using the standard\nMVVM definition as a baseline, our study identifies (1) 76 additional design\nconstructs grouped into 29 design aspects and (2) 16 additional benefits and 15\nadditional drawbacks. These insights can guide enterprise application\ndevelopers in implementing practical MVVM solutions and enable informed design\ndecisions."}
{"id": "2504.18128", "pdf": "https://arxiv.org/pdf/2504.18128.pdf", "abs": "https://arxiv.org/abs/2504.18128", "title": "Temporal Entailment Pretraining for Clinical Language Models over EHR Data", "authors": ["Tatsunori Tanaka", "Fi Zheng", "Kai Sato", "Zhifeng Li", "Yuanyun Zhang", "Shi Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Clinical language models have achieved strong performance on downstream tasks\nby pretraining on domain specific corpora such as discharge summaries and\nmedical notes. However, most approaches treat the electronic health record as a\nstatic document, neglecting the temporally-evolving and causally entwined\nnature of patient trajectories. In this paper, we introduce a novel temporal\nentailment pretraining objective for language models in the clinical domain.\nOur method formulates EHR segments as temporally ordered sentence pairs and\ntrains the model to determine whether a later state is entailed by,\ncontradictory to, or neutral with respect to an earlier state. Through this\ntemporally structured pretraining task, models learn to perform latent clinical\nreasoning over time, improving their ability to generalize across forecasting\nand diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and\ndemonstrate state of the art results on temporal clinical QA, early warning\nprediction, and disease progression modeling."}
{"id": "2504.18271", "pdf": "https://arxiv.org/pdf/2504.18271.pdf", "abs": "https://arxiv.org/abs/2504.18271", "title": "LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method", "authors": ["Tao Wu", "Kexue Fu", "Qiang Hua", "Xinxin Liu", "Muhammad Ali Imran", "Bo Liu"], "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "comment": "Code are available: https://github.com/TaoWu974/LEAM", "summary": "Antenna modeling is a time-consuming and complex process, decreasing the\nspeed of antenna analysis and design. In this paper, a large language model\n(LLM)- enabled antenna modeling method, called LEAM, is presented to address\nthis challenge. LEAM enables automatic antenna model generation based on\nlanguage descriptions via prompt input, images, descriptions from academic\npapers, patents, and technical reports (either one or multiple). The\neffectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna\ngenerated from a complete user description, a slotted patch antenna generated\nfrom an incomplete user description and the operating frequency, and a monopole\nslotted antenna generated from images and descriptions scanned from the\nliterature. For all the examples, correct antenna models are generated in a few\nminutes. The code can be accessed via https://github.com/TaoWu974/LEAM."}
{"id": "2504.18142", "pdf": "https://arxiv.org/pdf/2504.18142.pdf", "abs": "https://arxiv.org/abs/2504.18142", "title": "EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)", "authors": ["Fida Ullah", "Muhammad Ahmad", "Muhammad Tayyab Zamir", "Muhammad Arif", "Grigori sidorov", "Edgardo Manuel Felipe Riverón", "Alexander Gelbukh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) plays a pivotal role in various Natural\nLanguage Processing (NLP) tasks by identifying and classifying named entities\n(NEs) from unstructured data into predefined categories such as person,\norganization, location, date, and time. While extensive research exists for\nhigh-resource languages and general domains, NER in Urdu particularly within\ndomain-specific contexts like education remains significantly underexplored.\nThis is Due to lack of annotated datasets for educational content which limits\nthe ability of existing models to accurately identify entities such as academic\nroles, course names, and institutional terms, underscoring the urgent need for\ntargeted resources in this domain. To the best of our knowledge, no dataset\nexists in the domain of the Urdu language for this purpose. To achieve this\nobjective this study makes three key contributions. Firstly, we created a\nmanually annotated dataset in the education domain, named EDU-NER-2025, which\ncontains 13 unique most important entities related to education domain. Second,\nwe describe our annotation process and guidelines in detail and discuss the\nchallenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed\nkey linguistic challenges, such as morphological complexity and ambiguity,\nwhich are prevalent in formal Urdu texts."}
{"id": "2504.18310", "pdf": "https://arxiv.org/pdf/2504.18310.pdf", "abs": "https://arxiv.org/abs/2504.18310", "title": "Artificial Intelligence health advice accuracy varies across languages and contexts", "authors": ["Prashant Garg", "Thiemo Fetzer"], "categories": ["econ.GN", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "q-fin.EC"], "comment": "10 pages, 2 figures. All data, code and materials used is freely\n  available in the Zenodo (DOI: 10.5281/zenodo.15281282)", "summary": "Using basic health statements authorized by UK and EU registers and 9,100\njournalist-vetted public-health assertions on topics such as abortion, COVID-19\nand politics from sources ranging from peer-reviewed journals and government\nadvisories to social media and news across the political spectrum, we benchmark\nsix leading large language models from in 21 languages, finding that, despite\nhigh accuracy on English-centric textbook claims, performance falls in multiple\nnon-European languages and fluctuates by topic and source, highlighting the\nurgency of comprehensive multilingual, domain-aware validation before deploying\nAI in global health communication."}
{"id": "2504.18180", "pdf": "https://arxiv.org/pdf/2504.18180.pdf", "abs": "https://arxiv.org/abs/2504.18180", "title": "Aligning Language Models for Icelandic Legal Text Summarization", "authors": ["Þórir Hrafn Harðarson", "Hrafn Loftsson", "Stefán Ólafsson"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at NoDaLiDa 2025", "summary": "The integration of language models in the legal domain holds considerable\npromise for streamlining processes and improving efficiency in managing\nextensive workloads. However, the specialized terminology, nuanced language,\nand formal style of legal texts can present substantial challenges. This study\nexamines whether preference-based training techniques, specifically\nReinforcement Learning from Human Feedback and Direct Preference Optimization,\ncan enhance models' performance in generating Icelandic legal summaries that\nalign with domain-specific language standards and user preferences. We compare\nmodels fine-tuned with preference training to those using conventional\nsupervised learning. Results indicate that preference training improves the\nlegal accuracy of generated summaries over standard fine-tuning but does not\nsignificantly enhance the overall quality of Icelandic language usage.\nDiscrepancies between automated metrics and human evaluations further\nunderscore the importance of qualitative assessment in developing language\nmodels for the legal domain."}
{"id": "2504.18332", "pdf": "https://arxiv.org/pdf/2504.18332.pdf", "abs": "https://arxiv.org/abs/2504.18332", "title": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations", "authors": ["Shuting Zhao", "Linxin Bai", "Liangjing Shao", "Ye Zhang", "Xinrong Chen"], "categories": ["cs.CV", "cs.HC", "68U05"], "comment": "9 pages, 6 figures, conference ICMR 2025", "summary": "The growing applications of AR/VR increase the demand for real-time full-body\npose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint\nsignals from the head and hands, reconstructing a full-body pose remains\nchallenging due to the unconstrained lower body. Recent advancements often rely\non conventional neural networks and generative models to improve performance in\nthis task, such as Transformers and diffusion models. However, these approaches\nstruggle to strike a balance between achieving precise pose reconstruction and\nmaintaining fast inference speed. To overcome these challenges, a lightweight\nand efficient model, SSD-Poser, is designed for robust full-body motion\nestimation from sparse observations. SSD-Poser incorporates a well-designed\nhybrid encoder, State Space Attention Encoders, to adapt the state space\nduality to complex motion poses and enable real-time realistic pose\nreconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate\njitter caused by variable-frequency motion signals, remarkably enhancing the\nmotion smoothness. Comprehensive experiments on the AMASS dataset demonstrate\nthat SSD-Poser achieves exceptional accuracy and computational efficiency,\nshowing outstanding inference efficiency compared to state-of-the-art methods."}
{"id": "2504.18221", "pdf": "https://arxiv.org/pdf/2504.18221.pdf", "abs": "https://arxiv.org/abs/2504.18221", "title": "Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish", "authors": ["Shuxiang Du", "Ana Guerberof Arenas", "Antonio Toral", "Kyo Gerrits", "Josep Marco Borillo"], "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This study examines the variability of Chat-GPT machine translation (MT)\noutputs across six different configurations in four languages,with a focus on\ncreativity in a literary text. We evaluate GPT translations in different text\ngranularity levels, temperature settings and prompting strategies with a\nCreativity Score formula. We found that prompting ChatGPT with a minimal\ninstruction yields the best creative translations, with \"Translate the\nfollowing text into [TG] creatively\" at the temperature of 1.0 outperforming\nother configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,\nChatGPT consistently underperforms compared to human translation (HT)."}
{"id": "2504.18380", "pdf": "https://arxiv.org/pdf/2504.18380.pdf", "abs": "https://arxiv.org/abs/2504.18380", "title": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications", "authors": ["Steven Häsler", "Philipp Ackermann"], "categories": ["cs.SE", "cs.AI", "cs.GR", "cs.HC", "spatial computing, extended reality, knowledge representation,\n  spatial reasoning"], "comment": "11 pages, preprint of ICVARS 2025 paper", "summary": "Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications."}
{"id": "2504.18225", "pdf": "https://arxiv.org/pdf/2504.18225.pdf", "abs": "https://arxiv.org/abs/2504.18225", "title": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family", "authors": ["Pierre-Carl Langlais", "Pavel Chizhov", "Mattia Nee", "Carlos Rosas Hinostroza", "Matthieu Delsart", "Irène Girard", "Othman Hicheur", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI."}
{"id": "2504.18449", "pdf": "https://arxiv.org/pdf/2504.18449.pdf", "abs": "https://arxiv.org/abs/2504.18449", "title": "Automatic Bias Detection in Source Code Review", "authors": ["Yoseph Berhanu Alebachew", "Chris Brown"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Bias is an inherent threat to human decision-making, including in decisions\nmade during software development. Extensive research has demonstrated the\npresence of biases at various stages of the software development life-cycle.\nNotably, code reviews are highly susceptible to prejudice-induced biases, and\nindividuals are often unaware of these biases as they occur. Developing methods\nto automatically detect these biases is crucial for addressing the associated\nchallenges. Recent advancements in visual data analytics have shown promising\nresults in detecting potential biases by analyzing user interaction patterns.\nIn this project, we propose a controlled experiment to extend this approach to\ndetect potentially biased outcomes in code reviews by observing how reviewers\ninteract with the code. We employ the \"spotlight model of attention\", a\ncognitive framework where a reviewer's gaze is tracked to determine their focus\nareas on the review screen. This focus, identified through gaze tracking,\nserves as an indicator of the reviewer's areas of interest or concern. We plan\nto analyze the sequence of gaze focus using advanced sequence modeling\ntechniques, including Markov Models, Recurrent Neural Networks (RNNs), and\nConditional Random Fields (CRF). These techniques will help us identify\npatterns that may suggest biased interactions. We anticipate that the ability\nto automatically detect potentially biased interactions in code reviews will\nsignificantly reduce unnecessary push-backs, enhance operational efficiency,\nand foster greater diversity and inclusion in software development. This\napproach not only helps in identifying biases but also in creating a more\nequitable development environment by mitigating these biases effectively"}
{"id": "2504.18246", "pdf": "https://arxiv.org/pdf/2504.18246.pdf", "abs": "https://arxiv.org/abs/2504.18246", "title": "Efficient Single-Pass Training for Multi-Turn Reasoning", "authors": ["Ritesh Goru", "Shanay Mehta", "Prateek Jain"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets."}
{"id": "2406.10370", "pdf": "https://arxiv.org/pdf/2406.10370.pdf", "abs": "https://arxiv.org/abs/2406.10370", "title": "Papers-to-Posts: Supporting Detailed Long-Document Summarization with an Interactive LLM-Powered Source Outline", "authors": ["Marissa Radensky", "Daniel S. Weld", "Joseph Chee Chang", "Pao Siangliulue", "Jonathan Bragg"], "categories": ["cs.HC"], "comment": "Revised for clearer message", "summary": "Compressing long and technical documents (e.g., >10 pages) into shorter-form\narticles (e.g., <2 pages) is critical for communicating information to\ndifferent audiences, for example, blog posts of scientific research paper or\nlegal briefs of dense court proceedings. While large language models (LLMs) are\npowerful tools for condensing large amounts of text, current interfaces to\nthese models lack support for understanding and controlling what content is\nincluded in a detailed summarizing article. Such capability is especially\nimportant for detail- and technical-oriented domains, in which tactical\nselection and coherent synthesis of key details is critical for effective\ncommunication to the target audience. For this, we present interactive reverse\nsource outlines, a novel mechanism for controllable long-form summarization\nfeaturing outline bullet points with automatic point selections that the user\ncan iteratively adjust to obtain an article with the desired content coverage.\nWe implement this mechanism in Papers-to-Posts, a new LLM-powered system for\nauthoring research-paper blog posts. Through a within-subjects lab study (n=20)\nand a between-subjects deployment study (n=37 blog posts, 26 participants), we\ncompare Papers-to-Posts to a strong baseline tool that provides an\nLLM-generated draft and access to free-form prompting. Under time constraints,\nPapers-to-Posts significantly increases writer satisfaction with blog post\nquality, particularly with respect to content coverage. Furthermore,\nquantitative results showed an increase in editing power (change in text for an\namount of time or writing actions) while using Papers-to-Posts, and qualitative\nresults showed that participants found incorporating key research-paper\ninsights in their blog posts easier while using Papers-to-Posts."}
{"id": "2504.18260", "pdf": "https://arxiv.org/pdf/2504.18260.pdf", "abs": "https://arxiv.org/abs/2504.18260", "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment", "authors": ["Guanqun Bi", "Zhuang Chen", "Zhoufu Liu", "Hongkai Wang", "Xiyao Xiao", "Yuqiang Xie", "Wen Zhang", "Yongkang Huang", "Yuxuan Chen", "Libiao Peng", "Yi Feng", "Minlie Huang"], "categories": ["cs.CL"], "comment": "In progress", "summary": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning."}
{"id": "2409.13900", "pdf": "https://arxiv.org/pdf/2409.13900.pdf", "abs": "https://arxiv.org/abs/2409.13900", "title": "Misty: UI Prototyping Through Interactive Conceptual Blending", "authors": ["Yuwen Lu", "Alan Leung", "Amanda Swearngin", "Jeffrey Nichols", "Titus Barik"], "categories": ["cs.HC"], "comment": null, "summary": "UI prototyping often involves iterating and blending elements from examples\nsuch as screenshots and sketches, but current tools offer limited support for\nincorporating these examples. Inspired by the cognitive process of conceptual\nblending, we introduce a novel UI workflow that allows developers to rapidly\nincorporate diverse aspects from design examples into work-in-progress UIs. We\nprototyped this workflow as Misty. Through an exploratory first-use study with\n14 frontend developers, we assessed Misty's effectiveness and gathered feedback\non this workflow. Our findings suggest that Misty's conceptual blending\nworkflow helps developers kickstart creative explorations, flexibly specify\nintent in different stages of prototyping, and inspires developers through\nserendipitous UI blends. Misty demonstrates the potential for tools that blur\nthe boundaries between developers and designers."}
{"id": "2504.18269", "pdf": "https://arxiv.org/pdf/2504.18269.pdf", "abs": "https://arxiv.org/abs/2504.18269", "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Yusuke Sakai", "Jingun Kwon", "Hidetaka Kamigaito", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "categories": ["cs.CL", "cs.CV"], "comment": "Under review", "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance."}
{"id": "2410.14048", "pdf": "https://arxiv.org/pdf/2410.14048.pdf", "abs": "https://arxiv.org/abs/2410.14048", "title": "Co-Designing with Algorithms: Unpacking the Complex Role of GenAI in Interactive System Design Education", "authors": ["Hauke Sandhaus", "Quiquan Gu", "Maria Teresa Parreira", "Wendy Ju"], "categories": ["cs.HC", "K.3.1; K.3.2"], "comment": "Conditionally accepted to DIS'25", "summary": "Generative Artificial Intelligence (GenAI) is transforming Human-Computer\nInteraction (HCI) education and technology design, yet its impact remains\npoorly understood. This study explores how graduate students in an applied HCI\ncourse used GenAI tools during interactive device design. Despite no\nencouragement, all groups integrated GenAI into their workflows. Through 12\npost-class group interviews, we identified how GenAI co-design behaviors\npresent both benefits, such as enhanced creativity and faster design\niterations, and risks, including shallow learning and reflection. Benefits were\nmost evident during the execution phases, while the discovery and reflection\nphases showed limited gains. A taxonomy of usage patterns revealed that\nstudents' outcomes depended more on how they used GenAI than the specific tasks\nperformed. These findings highlight the need for HCI education to adapt to\nGenAI's role and offer recommendations for curricula to better prepare future\ndesigners for effective creative co-design."}
{"id": "2504.18346", "pdf": "https://arxiv.org/pdf/2504.18346.pdf", "abs": "https://arxiv.org/abs/2504.18346", "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "authors": ["Toghrul Abbasli", "Kentaroh Toyoda", "Yuan Wang", "Leon Witt", "Muhammad Asif Ali", "Yukai Miao", "Dan Li", "Qingsong Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs."}
{"id": "2501.13145", "pdf": "https://arxiv.org/pdf/2501.13145.pdf", "abs": "https://arxiv.org/abs/2501.13145", "title": "The GenUI Study: Exploring the Design of Generative UI Tools to Support UX Practitioners and Beyond", "authors": ["Xiang 'Anthony' Chen", "Tiffany Knearem", "Yang Li"], "categories": ["cs.HC"], "comment": null, "summary": "AI can now generate high-fidelity UI mock-up screens from a high-level\ntextual description, promising to support UX practitioners' work. However, it\nremains unclear how UX practitioners would adopt such Generative UI (GenUI)\nmodels in a way that is integral and beneficial to their work. To answer this\nquestion, we conducted a formative study with 37 UX-related professionals that\nconsisted of four roles: UX designers, UX researchers, software engineers, and\nproduct managers. Using a state-of-the-art GenUI tool, each participant went\nthrough a week-long, individual mini-project exercise with role-specific tasks,\nkeeping a daily journal of their usage and experiences with GenUI, followed by\na semi-structured interview. We report findings on participants' workflow using\nthe GenUI tool, how GenUI can support all and each specific roles, and existing\ngaps between GenUI and users' needs and expectations, which lead to design\nimplications to inform future work on GenUI development."}
{"id": "2504.18373", "pdf": "https://arxiv.org/pdf/2504.18373.pdf", "abs": "https://arxiv.org/abs/2504.18373", "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant", "authors": ["Lei Shen", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/."}
{"id": "2503.24249", "pdf": "https://arxiv.org/pdf/2503.24249.pdf", "abs": "https://arxiv.org/abs/2503.24249", "title": "Control Center Framework for Teleoperation Support of Automated Vehicles on Public Roads", "authors": ["Maria-Magdalena Wolf", "Niklas Krauss", "Arwed Schmidt", "Frank Diermeyer"], "categories": ["cs.HC"], "comment": null, "summary": "Implementing a teleoperation system with its various actors and interactions\nis challenging and requires an overview of the necessary functions. This work\ncollects all tasks that arise in a control center for an automated vehicle\nfleet from literature and assigns them to the two roles Remote Operator and\nFleet Manager. Focusing on the driving-related tasks of the remote operator, a\nprocess is derived that contains the sequence of tasks, associated vehicle\nstates, and transitions between the states. The resulting state diagram shows\nall remote operator actions available to effectively resolve automated vehicle\ndisengagements. Thus, the state diagram can be applied to existing legislation\nor modified based on prohibitions of specific interactions. The developed\ncontrol center framework and included state diagram should serve as a basis for\nimplementing and testing remote support for automated vehicles to be validated\non public roads."}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376.pdf", "abs": "https://arxiv.org/abs/2504.18376", "title": "Pushing the boundary on Natural Language Inference", "authors": ["Pablo Miralles-González", "Javier Huertas-Tato", "Alejandro Martín", "David Camacho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality."}
{"id": "2504.13878", "pdf": "https://arxiv.org/pdf/2504.13878.pdf", "abs": "https://arxiv.org/abs/2504.13878", "title": "Learning by gaming, coding and making with EDUMING: A new approach to utilising atypical digital games for learning", "authors": ["Stefan Pietrusky"], "categories": ["cs.HC", "cs.CY"], "comment": "13 pages, 2 figures", "summary": "Papert's constructionism makes it clear that learning is particularly\neffective when learners create tangible artifacts and share and discuss them in\nsocial contexts. Technological progress in recent decades has created numerous\nopportunities for learners to not only passively consume media, but to actively\nshape it through construction. This article uses the EDUMING concept to present\na new method to simplify the development of digital learning games and thus\nsupport their integration into learning situations. A key difference between\nthe concept and established ideas such as game-based learning, gamification,\nserious games, etc. is that games are not closed and are consumed passively,\nbut can also be actively developed by users individually by modifying the\nsource code with the help of an IDE. As part of an empirical study, the\nusability of the game \"Professor Chip's Learning Quest\" (PCLQ) is recorded, as\nwell as previous experience with digital learning games and the acceptance and\nmotivation to use new technologies. The purpose of this article is to test the\nPCLQ digital learning game, developed according to the EDUMING concept, as part\nof an exploratory study regarding its usability, acceptance and suitability for\nuse in schools. The study is intended as a first empirical approach to\npractical testing of the concept."}
{"id": "2504.18386", "pdf": "https://arxiv.org/pdf/2504.18386.pdf", "abs": "https://arxiv.org/abs/2504.18386", "title": "A UD Treebank for Bohairic Coptic", "authors": ["Amir Zeldes", "Nina Speransky", "Nicholas Wagner", "Caroline T. Schroeder"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advances in digital resources for other Coptic dialects,\nespecially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,\nlate Byzantine Egypt, and the contemporary language of the Coptic Church,\nremains critically under-resourced. This paper presents and evaluates the first\nsyntactically annotated corpus of Bohairic Coptic, sampling data from a range\nof works, including Biblical text, saints' lives and Christian ascetic writing.\nWe also explore some of the main differences we observe compared to the\nexisting UD treebank of Sahidic Coptic, the classical dialect of the language,\nand conduct joint and cross-dialect parsing experiments, revealing the unique\nnature of Bohairic as a related, but distinct variety from the more often\nstudied Sahidic."}
{"id": "2504.13948", "pdf": "https://arxiv.org/pdf/2504.13948.pdf", "abs": "https://arxiv.org/abs/2504.13948", "title": "Using customized GPT to develop prompting proficiency in architectural AI-generated images", "authors": ["Juan David Salazar Rodriguez", "Sam Conrad Joyce", "Julfendi"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This research investigates the use of customized GPT models to enhance\nprompting proficiency among architecture students when generating AI-driven\nimages. Prompt engineering is increasingly essential in architectural education\ndue to the widespread adoption of generative AI tools. This study utilized a\nmixed-methods experimental design involving architecture students divided into\nthree distinct groups: a control group receiving no structured support, a\nsecond group provided with structured prompting guides, and a third group\nsupported by both structured guides and interactive AI personas. Students\nengaged in reverse engineering tasks, first guessing provided image prompts and\nthen generating their own prompts, aiming to boost critical thinking and\nprompting skills. Variables examined included time spent prompting, word count,\nprompt similarity, and concreteness. Quantitative analysis involved correlation\nassessments between these variables and a one-way ANOVA to evaluate differences\nacross groups. While several correlations showed meaningful relationships, not\nall were statistically significant. ANOVA results indicated statistically\nsignificant improvements in word count, similarity, and concreteness,\nespecially in the group supported by AI personas and structured prompting\nguides. Qualitative feedback complemented these findings, revealing enhanced\nconfidence and critical thinking skills in students. These results suggest\ntailored GPT interactions substantially improve students' ability to\ncommunicate architectural concepts clearly and effectively."}
{"id": "2504.18406", "pdf": "https://arxiv.org/pdf/2504.18406.pdf", "abs": "https://arxiv.org/abs/2504.18406", "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "authors": ["Yusen Zhang", "Wenliang Zheng", "Aashrith Madasu", "Peng Shi", "Ryo Kamoi", "Hao Zhou", "Zhuoyang Zou", "Shu Zhao", "Sarkar Snigdha Sarathi Das", "Vipul Gupta", "Xiaoxin Lu", "Nan Zhang", "Ranran Haoran Zhang", "Avitej Iyer", "Renze Lou", "Wenpeng Yin", "Rui Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "High-resolution image (HRI) understanding aims to process images with a large\nnumber of pixels, such as pathological images and agricultural aerial images,\nboth of which can exceed 1 million pixels. Vision Large Language Models (VLMs)\ncan allegedly handle HRIs, however, there is a lack of a comprehensive\nbenchmark for VLMs to evaluate HRI understanding. To address this gap, we\nintroduce HRScene, a novel unified benchmark for HRI understanding with rich\nscenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic\ndatasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$\n26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,\ncovering 25 scenarios, ranging from microscopic to radiology images, street\nviews, long-range pictures, and telescope images. It includes HRIs of\nreal-world objects, scanned documents, and composite multi-image. The two\ndiagnostic evaluation datasets are synthesized by combining the target image\nwith the gold answer and distracting images in different orders, assessing how\nwell models utilize regions in HRI. We conduct extensive experiments involving\n28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show\nthat current VLMs achieve an average accuracy of around 50% on real-world\ntasks, revealing significant gaps in HRI understanding. Results on synthetic\ndatasets reveal that VLMs struggle to effectively utilize HRI regions, showing\nsignificant Regional Divergence and lost-in-middle, shedding light on future\nresearch."}
{"id": "2401.17929", "pdf": "https://arxiv.org/pdf/2401.17929.pdf", "abs": "https://arxiv.org/abs/2401.17929", "title": "Reputation-Driven Adoption and Avoidance of Algorithmic Decision Aids in Credence Goods Markets", "authors": ["Alexander Erlei", "Lukas Meub"], "categories": ["econ.GN", "cs.HC", "q-fin.EC"], "comment": null, "summary": "In credence goods markets such as health care or repair services, consumers\nrely on experts with superior information to adequately diagnose and treat\nthem. Experts, however, are constrained in their diagnostic abilities, which\nhurts market efficiency and consumer welfare. Technological breakthroughs that\nsubstitute or complement expert judgments have the potential to alleviate\nconsumer mistreatment. This article studies how competitive experts adopt novel\ndiagnostic technologies when skills are heterogeneously distributed and\nobfuscated to consumers. We differentiate between novel technologies that\nincrease expert abilities, and algorithmic decision aids that complement expert\njudgments, but do not affect an expert's personal diagnostic precision. When\nconsumers build up beliefs about an expert's type through repeated\ninteractions, we show that high-ability experts may strategically forego the\ndecision aid in order to escape a pooling equilibrium by differentiating\nthemselves from low-ability experts. Without future visits, signaling concerns\ncause all experts to randomize their investment choice, leading to\nunder-utilization from low-ability experts and over-utilization from\nhigh-ability experts. Results from two online experiments support our\nhypotheses. High-ability experts are significantly less likely than low-ability\nexperts to invests into an algorithmic decision aid if reputation building is\npossible. Otherwise, there is no difference, and experts who believe that\nconsumers play a signaling game randomize their investment choice."}
{"id": "2504.18412", "pdf": "https://arxiv.org/pdf/2504.18412.pdf", "abs": "https://arxiv.org/abs/2504.18412", "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers", "authors": ["Jared Moore", "Declan Grabb", "William Agnew", "Kevin Klyman", "Stevie Chancellor", "Desmond C. Ong", "Nick Haber"], "categories": ["cs.CL"], "comment": null, "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy."}
{"id": "2408.04032", "pdf": "https://arxiv.org/pdf/2408.04032.pdf", "abs": "https://arxiv.org/abs/2408.04032", "title": "The Evolution of Information Seeking in Software Development: Understanding the Role and Impact of AI Assistants", "authors": ["Ebtesam Al Haque", "Chris Brown", "Thomas D. LaToza", "Brittany Johnson"], "categories": ["cs.SE", "cs.HC"], "comment": "To appear in FSE Companion '25: 33rd ACM International Conference on\n  the Foundations of Software Engineering (HumanAISE Workshop)", "summary": "About 32% of a software practitioners' day involves seeking and using\ninformation to support task completion. Although the information needs of\nsoftware practitioners have been studied extensively, the impact of AI-assisted\ntools on their needs and information-seeking behaviors remains largely\nunexplored. To addresses this gap, we conducted a mixed-method study to\nunderstand AI-assisted information seeking behavior of practitioners and its\nimpact on their perceived productivity and skill development. We found that\ndevelopers are increasingly using AI tools to support their information\nseeking, citing increased efficiency as a key benefit. Our findings also\namplify caveats that come with effectively using AI tools for information\nseeking, especially for learning and skill development, such as the importance\nof foundational developer knowledge that can guide and inform the information\nprovided by AI tools. Our efforts have implications for the effective\nintegration of AI tools into developer workflows as information retrieval\nsystems and learning aids."}
{"id": "2504.18415", "pdf": "https://arxiv.org/pdf/2504.18415.pdf", "abs": "https://arxiv.org/abs/2504.18415", "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs", "authors": ["Hongyu Wang", "Shuming Ma", "Furu Wei"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference."}
{"id": "2409.12447", "pdf": "https://arxiv.org/pdf/2409.12447.pdf", "abs": "https://arxiv.org/abs/2409.12447", "title": "Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts", "authors": ["Jenny T. Liang", "Melissa Lin", "Nikitha Rao", "Brad A. Myers"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Accepted to FSE'25", "summary": "Generative pre-trained models power intelligent software features used by\nmillions of users controlled by developer-written natural language prompts.\nDespite the impact of prompt-powered software, little is known about its\ndevelopment process and its relationship to programming. In this work, we argue\nthat some prompts are programs and that the development of prompts is a\ndistinct phenomenon in programming known as \"prompt programming\". We develop an\nunderstanding of prompt programming using Straussian grounded theory through\ninterviews with 20 developers engaged in prompt development across a variety of\ncontexts, models, domains, and prompt structures. We contribute 15 observations\nto form a preliminary understanding of current prompt programming practices.\nFor example, rather than building mental models of code, prompt programmers\ndevelop mental models of the foundation model (FM)'s behavior on the prompt by\ninteracting with the FM. While prior research shows that experts have\nwell-formed mental models, we find that prompt programmers who have developed\ndozens of prompts still struggle to develop reliable mental models. Our\nobservations show that prompt programming differs from traditional software\ndevelopment, motivating the creation of prompt programming tools and providing\nimplications for software engineering stakeholders."}
{"id": "2504.18428", "pdf": "https://arxiv.org/pdf/2504.18428.pdf", "abs": "https://arxiv.org/abs/2504.18428", "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts", "authors": ["Yiming Wang", "Pei Zhang", "Jialong Tang", "Haoran Wei", "Baosong Yang", "Rui Wang", "Chenshu Sun", "Feitong Sun", "Jiran Zhang", "Junxuan Wu", "Qiqian Cang", "Yichang Zhang", "Fei Huang", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."}
{"id": "2501.10917", "pdf": "https://arxiv.org/pdf/2501.10917.pdf", "abs": "https://arxiv.org/abs/2501.10917", "title": "Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition", "authors": ["Haoyu Xie", "Haoxuan Li", "Chunyuan Zheng", "Haonan Yuan", "Guorui Liao", "Jun Liao", "Li Liu"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Wearable Human Activity Recognition (WHAR) is a prominent research area\nwithin ubiquitous computing. Multi-sensor synchronous measurement has proven to\nbe more effective for WHAR than using a single sensor. However, existing WHAR\nmethods use shared convolutional kernels for indiscriminate temporal feature\nextraction across each sensor variable, which fails to effectively capture\nspatio-temporal relationships of intra-sensor and inter-sensor variables. We\npropose the DecomposeWHAR model consisting of a decomposition phase and a\nfusion phase to better model the relationships between modality variables. The\ndecomposition creates high-dimensional representations of each intra-sensor\nvariable through the improved Depth Separable Convolution to capture local\ntemporal features while preserving their unique characteristics. The fusion\nphase begins by capturing relationships between intra-sensor variables and\nfusing their features at both the channel and variable levels. Long-range\ntemporal dependencies are modeled using the State Space Model (SSM), and later\ncross-sensor interactions are dynamically captured through a self-attention\nmechanism, highlighting inter-sensor spatial correlations. Our model\ndemonstrates superior performance on three widely used WHAR datasets,\nsignificantly outperforming state-of-the-art models while maintaining\nacceptable computational efficiency."}
{"id": "2504.18458", "pdf": "https://arxiv.org/pdf/2504.18458.pdf", "abs": "https://arxiv.org/abs/2504.18458", "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "authors": ["Wenyi Xiao", "Leilei Gan", "Weilong Dai", "Wanggui He", "Ziwei Huang", "Haoyuan Li", "Fangxun Shu", "Zhelun Yu", "Peng Zhang", "Hao Jiang", "Fei Wu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "16 pages, 5 figures, and 12 tables", "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy."}
{"id": "2502.19546", "pdf": "https://arxiv.org/pdf/2502.19546.pdf", "abs": "https://arxiv.org/abs/2502.19546", "title": "Repurposing the scientific literature with vision-language models", "authors": ["Anton Alyakin", "Jaden Stryker", "Daniel Alexander Alber", "Karl L. Sangwon", "Jin Vivian Lee", "Brandon Duderstadt", "Akshay Save", "David Kurland", "Spencer Frome", "Shrutika Singh", "Jeff Zhang", "Eunice Yang", "Ki Yun Park", "Cordelia Orillac", "Aly A. Valliani", "Sean Neifert", "Albert Liu", "Aneek Patel", "Christopher Livia", "Darryl Lau", "Ilya Laufer", "Peter A. Rozman", "Eveline Teresa Hidalgo", "Howard Riina", "Rui Feng", "Todd Hollon", "Yindalon Aphinyanaphongs", "John G. Golfinos", "Laura Snyder", "Eric Leuthardt", "Douglas Kondziolka", "Eric Karl Oermann"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Leading vision-language models (VLMs) are trained on general Internet\ncontent, overlooking scientific journals' rich, domain-specific knowledge.\nTraining on specialty-specific literature could yield high-performance,\ntask-specific tools, enabling generative AI to match generalist models in\nspecialty publishing, educational, and clinical tasks. We created NeuroPubs, a\nmultimodal dataset of 23,000 Neurosurgery Publications articles (134M words,\n78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready\ngraphical abstracts (70% of 100 abstracts) and board-style questions\nindistinguishable from human-written ones (54% of 89,587 questions). We used\nthese questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,\nrandomized controlled trial, our model demonstrated non-inferiority to then\nstate-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical\nutility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,\np=0.3797). Our pilot study demonstrates how training generative AI models on\nspecialty-specific journal content - without large-scale internet data -\nresults in high-performance academic and clinical tools, enabling\ndomain-tailored AI across diverse fields."}
{"id": "2504.18474", "pdf": "https://arxiv.org/pdf/2504.18474.pdf", "abs": "https://arxiv.org/abs/2504.18474", "title": "Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions", "authors": ["James D. Finch", "Yasasvi Josyula", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "Accepted (B) to TACL 2025", "summary": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development."}
{"id": "2504.14603", "pdf": "https://arxiv.org/pdf/2504.14603.pdf", "abs": "https://arxiv.org/abs/2504.14603", "title": "UFO2: The Desktop AgentOS", "authors": ["Chaoyun Zhang", "He Huang", "Chiming Ni", "Jian Mu", "Si Qin", "Shilin He", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Chao Du", "Liqun Li", "Yu Kang", "Zhao Jiang", "Suzhen Zheng", "Rujia Wang", "Jiaxu Qian", "Minghua Ma", "Jian-Guang Lou", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.AI", "cs.HC", "cs.OS"], "comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/", "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation."}
{"id": "2504.18483", "pdf": "https://arxiv.org/pdf/2504.18483.pdf", "abs": "https://arxiv.org/abs/2504.18483", "title": "Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues", "authors": ["Leandra Fichtel", "Maximilian Spliethöver", "Eyke Hüllermeier", "Patricia Jimenez", "Nils Klowait", "Stefan Kopp", "Axel-Cyrille Ngonga Ngomo", "Amelie Robrecht", "Ingrid Scharlau", "Lutz Terfloth", "Anna-Lisa Vollmer", "Henning Wachsmuth"], "categories": ["cs.CL"], "comment": "Submitted to the SIGDial Conference 2025", "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited."}
{"id": "2504.18535", "pdf": "https://arxiv.org/pdf/2504.18535.pdf", "abs": "https://arxiv.org/abs/2504.18535", "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation", "authors": ["Gwen Yidou Weng", "Benjie Wang", "Guy Van den Broeck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes."}
{"id": "2504.17821", "pdf": "https://arxiv.org/pdf/2504.17821.pdf", "abs": "https://arxiv.org/abs/2504.17821", "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension", "authors": ["Xinyu Chen", "Yunxin Li", "Haoyuan Shi", "Baotian Hu", "Wenhan Luo", "Yaowei Wang", "Min Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics."}
{"id": "2504.17884", "pdf": "https://arxiv.org/pdf/2504.17884.pdf", "abs": "https://arxiv.org/abs/2504.17884", "title": "Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval", "authors": ["Yongkang Li", "Panagiotis Eustratiadis", "Simon Lupart", "Evangelos Kanoulas"], "categories": ["cs.IR", "cs.CL"], "comment": "This paper has been accepted as a full paper at SIGIR 2025 and will\n  be presented orally", "summary": "This paper concerns corpus poisoning attacks in dense information retrieval,\nwhere an adversary attempts to compromise the ranking performance of a search\nalgorithm by injecting a small number of maliciously generated documents into\nthe corpus. Our work addresses two limitations in the current literature.\nFirst, attacks that perform adversarial gradient-based word substitution search\ndo so in the discrete lexical space, while retrieval itself happens in the\ncontinuous embedding space. We thus propose an optimization method that\noperates in the embedding space directly. Specifically, we train a perturbation\nmodel with the objective of maintaining the geometric distance between the\noriginal and adversarial document embeddings, while also maximizing the\ntoken-level dissimilarity between the original and adversarial documents.\nSecond, it is common for related work to have a strong assumption that the\nadversary has prior knowledge about the queries. In this paper, we focus on a\nmore challenging variant of the problem where the adversary assumes no prior\nknowledge about the query distribution (hence, unsupervised). Our core\ncontribution is an adversarial corpus attack that is fast and effective. We\npresent comprehensive experimental results on both in- and out-of-domain\ndatasets, focusing on two related tasks: a top-1 attack and a corpus poisoning\nattack. We consider attacks under both a white-box and a black-box setting.\nNotably, our method can generate successful adversarial examples in under two\nminutes per target document; four times faster compared to the fastest\ngradient-based word substitution methods in the literature with the same\nhardware. Furthermore, our adversarial generation method generates text that is\nmore likely to occur under the distribution of natural text (low perplexity),\nand is therefore more difficult to detect."}
{"id": "2504.17892", "pdf": "https://arxiv.org/pdf/2504.17892.pdf", "abs": "https://arxiv.org/abs/2504.17892", "title": "Token Sequence Compression for Efficient Multimodal Computing", "authors": ["Yasmine Omri", "Parth Shroff", "Thierry Tambe"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems."}
{"id": "2504.17902", "pdf": "https://arxiv.org/pdf/2504.17902.pdf", "abs": "https://arxiv.org/abs/2504.17902", "title": "CAMU: Context Augmentation for Meme Understanding", "authors": ["Girish A. Koushik", "Diptesh Kanojia", "Helen Treharne", "Aditya Joshi"], "categories": ["cs.CV", "cs.CL"], "comment": "Under review at ACM MM 2025", "summary": "Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. We\nintroduce a novel framework, CAMU, which leverages large vision-language models\nto generate more descriptive captions, a caption-scoring neural network to\nemphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's\ntext encoder for an improved multimodal understanding of memes. Experiments on\npublicly available hateful meme datasets show that simple projection layer\nfine-tuning yields modest gains, whereas selectively tuning deeper text encoder\nlayers significantly boosts performance on all evaluation metrics. Moreover,\nour approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful\nMemes dataset, at par with the existing SoTA framework while being much more\nefficient, offering practical advantages in real-world scenarios that rely on\nfixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the\nMultiOFF dataset for offensive meme identification, demonstrating its\ngeneralisability. Additional analyses on benign confounders reveal that robust\nvisual grounding and nuanced text representations are crucial for reliable hate\nand offence detection. We will publicly release CAMU along with the resultant\nmodels for further research.\n  Disclaimer: This paper includes references to potentially disturbing,\nhateful, or offensive content due to the nature of the task."}
{"id": "2504.17934", "pdf": "https://arxiv.org/pdf/2504.17934.pdf", "abs": "https://arxiv.org/abs/2504.17934", "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation."}
{"id": "2504.17950", "pdf": "https://arxiv.org/pdf/2504.17950.pdf", "abs": "https://arxiv.org/abs/2504.17950", "title": "Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning", "authors": ["Isadora White", "Kolby Nottingham", "Ayush Maniar", "Max Robinson", "Hansen Lillemark", "Mehul Maheshwari", "Lianhui Qin", "Prithviraj Ammanabrolu"], "categories": ["cs.MA", "cs.CL"], "comment": "9 pages of main paper with 6 main figures, overall 28 pages", "summary": "Collaboration is ubiquitous and essential in day-to-day life -- from\nexchanging ideas, to delegating tasks, to generating plans together. This work\nstudies how LLMs can adaptively collaborate to perform complex embodied\nreasoning tasks. To this end we introduce MINDcraft, an easily extensible\nplatform built to enable LLM agents to control characters in the open-world\ngame of Minecraft; and MineCollab, a benchmark to test the different dimensions\nof embodied and collaborative reasoning. An experimental study finds that the\nprimary bottleneck in collaborating effectively for current state-of-the-art\nagents is efficient natural language communication, with agent performance\ndropping as much as 15% when they are required to communicate detailed task\ncompletion plans. We conclude that existing LLM agents are ill-optimized for\nmulti-agent collaboration, especially in embodied scenarios, and highlight the\nneed to employ methods beyond in-context and imitation learning. Our website\ncan be found here: https://mindcraft-minecollab.github.io/"}
{"id": "2504.18024", "pdf": "https://arxiv.org/pdf/2504.18024.pdf", "abs": "https://arxiv.org/abs/2504.18024", "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark", "authors": ["Yiwei Zha"], "categories": ["cs.CE", "cs.CL", "cs.IR"], "comment": "For open source github repo, see\n  https://github.com/JonathanZha47/SMARTFinRAG", "summary": "Financial sectors are rapidly adopting language model technologies, yet\nevaluating specialized RAG systems in this domain remains challenging. This\npaper introduces SMARTFinRAG, addressing three critical gaps in financial RAG\nassessment: (1) a fully modular architecture where components can be\ndynamically interchanged during runtime; (2) a document-centric evaluation\nparadigm generating domain-specific QA pairs from newly ingested financial\ndocuments; and (3) an intuitive interface bridging research-implementation\ndivides. Our evaluation quantifies both retrieval efficacy and response\nquality, revealing significant performance variations across configurations.\nThe platform's open-source architecture supports transparent, reproducible\nresearch while addressing practical deployment challenges faced by financial\ninstitutions implementing RAG systems."}
{"id": "2504.18099", "pdf": "https://arxiv.org/pdf/2504.18099.pdf", "abs": "https://arxiv.org/abs/2504.18099", "title": "Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture", "authors": ["Leena G Pillai", "D. Muhammad Noorul Mubarak", "Elizabeth Sherly"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "10 pages with 8 figures. This paper presented in an international\n  Conference", "summary": "Speech production is a complex sequential process which involve the\ncoordination of various articulatory features. Among them tongue being a highly\nversatile active articulator responsible for shaping airflow to produce\ntargeted speech sounds that are intellectual, clear, and distinct. This paper\npresents a novel approach for predicting tongue and lip articulatory features\ninvolved in a given speech acoustics using a stacked Bidirectional Long\nShort-Term Memory (BiLSTM) architecture, combined with a one-dimensional\nConvolutional Neural Network (CNN) for post-processing with fixed weights\ninitialization. The proposed network is trained with two datasets consisting of\nsimultaneously recorded speech and Electromagnetic Articulography (EMA)\ndatasets, each introducing variations in terms of geographical origin,\nlinguistic characteristics, phonetic diversity, and recording equipment. The\nperformance of the model is assessed in Speaker Dependent (SD), Speaker\nIndependent (SI), corpus dependent (CD) and cross corpus (CC) modes.\nExperimental results indicate that the proposed model with fixed weights\napproach outperformed the adaptive weights initialization with in relatively\nminimal number of training epochs. These findings contribute to the development\nof robust and efficient models for articulatory feature prediction, paving the\nway for advancements in speech production research and applications."}
{"id": "2504.18333", "pdf": "https://arxiv.org/pdf/2504.18333.pdf", "abs": "https://arxiv.org/abs/2504.18333", "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections", "authors": ["Narek Maloyan", "Dmitry Namiot"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets"}
{"id": "2504.18425", "pdf": "https://arxiv.org/pdf/2504.18425.pdf", "abs": "https://arxiv.org/abs/2504.18425", "title": "Kimi-Audio Technical Report", "authors": ["KimiTeam", "Ding Ding", "Zeqian Ju", "Yichong Leng", "Songxiang Liu", "Tong Liu", "Zeyu Shang", "Kai Shen", "Wei Song", "Xu Tan", "Heyi Tang", "Zhengtao Wang", "Chu Wei", "Yifei Xin", "Xinran Xu", "Jianwei Yu", "Yutao Zhang", "Xinyu Zhou", "Y. Charles", "Jun Chen", "Yanru Chen", "Yulun Du", "Weiran He", "Zhenxing Hu", "Guokun Lai", "Qingcheng Li", "Yangyang Liu", "Weidong Sun", "Jianzhou Wang", "Yuzhi Wang", "Yuefeng Wu", "Yuxin Wu", "Dongchao Yang", "Hao Yang", "Ying Yang", "Zhilin Yang", "Aoxiong Yin", "Ruibin Yuan", "Yutong Zhang", "Zaida Zhou"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio."}
{"id": "2404.03818", "pdf": "https://arxiv.org/pdf/2404.03818.pdf", "abs": "https://arxiv.org/abs/2404.03818", "title": "PRobELM: Plausibility Ranking Evaluation for Language Models", "authors": ["Zhangdie Yuan", "Eric Chamoun", "Rami Aly", "Chenxi Whitehouse", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language\nModels), a benchmark designed to assess language models' ability to discern\nmore plausible from less plausible scenarios through their parametric\nknowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or\ntruthfulness, and others such as COPA explore plausible scenarios without\nexplicitly incorporating world knowledge, PRobELM seeks to bridge this gap by\nevaluating models' capabilities to prioritise plausible scenarios that leverage\nworld knowledge over less plausible alternatives. This design allows us to\nassess the potential of language models for downstream use cases such as\nliterature-based discovery where the focus is on identifying information that\nis likely but not yet known. Our benchmark is constructed from a dataset\ncurated from Wikidata edit histories, tailored to align the temporal bounds of\nthe training data for the evaluated models. PRobELM facilitates the evaluation\nof language models across multiple prompting types, including statement, text\ncompletion, and question-answering. Experiments with 10 models of various sizes\nand architectures on the relationship between model scales, training recency,\nand plausibility performance, reveal that factual accuracy does not directly\ncorrelate with plausibility performance and that up-to-date training data\nenhances plausibility assessment across different model architectures."}
{"id": "2405.19325", "pdf": "https://arxiv.org/pdf/2405.19325.pdf", "abs": "https://arxiv.org/abs/2405.19325", "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution", "authors": ["Minghan Li", "Xilun Chen", "Ari Holtzman", "Beidi Chen", "Jimmy Lin", "Wen-tau Yih", "Xi Victoria Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often hallucinate and lack the ability to\nprovide attribution for their generations. Semi-parametric LMs, such as kNN-LM,\napproach these limitations by refining the output of an LM for a given prompt\nusing its nearest neighbor matches in a non-parametric data store. However,\nthese models often exhibit slow inference speeds and produce non-fluent texts.\nIn this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a\nnovel semi-parametric language modeling approach that is capable of\nincorporating real-world text spans of arbitrary length into the LM generations\nand providing attribution to their sources. NEST performs token-level retrieval\nat each inference step to compute a semi-parametric mixture distribution and\nidentify promising span continuations in a corpus. It then uses an approximate\nspeculative decoding procedure that accepts a prefix of the retrieved span or\ngenerates a new token. NEST significantly enhances the generation quality and\nattribution rate of the base LM across a variety of knowledge-intensive tasks,\nsurpassing the conventional kNN-LM method and performing competitively with\nin-context retrieval augmentation. In addition, NEST substantially improves the\ngeneration speed, achieving a 1.8x speedup in inference time when applied to\nLlama-2-Chat 70B. Code will be released at\nhttps://github.com/facebookresearch/NEST/tree/main."}
{"id": "2406.10432", "pdf": "https://arxiv.org/pdf/2406.10432.pdf", "abs": "https://arxiv.org/abs/2406.10432", "title": "AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction", "authors": ["Peitao Han", "Lis Kanashiro Pereira", "Fei Cheng", "Wan Jou She", "Eiji Aramaki"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 SRW", "summary": "Existing in-context learning (ICL) methods for relation extraction (RE) often\nprioritize language similarity over structural similarity, which can lead to\noverlooking entity relationships. To address this, we propose an AMR-enhanced\nretrieval-based ICL method for RE. Our model retrieves in-context examples\nbased on semantic structure similarity between task inputs and training\nsamples. Evaluations on four standard English RE datasets show that our model\noutperforms baselines in the unsupervised setting across all datasets. In the\nsupervised setting, it achieves state-of-the-art results on three datasets and\ncompetitive results on the fourth."}
{"id": "2406.10602", "pdf": "https://arxiv.org/pdf/2406.10602.pdf", "abs": "https://arxiv.org/abs/2406.10602", "title": "Multilingual Large Language Models and Curse of Multilinguality", "authors": ["Daniil Gurgurov", "Tanja Bäumel", "Tatiana Anikina"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it."}
{"id": "2408.06276", "pdf": "https://arxiv.org/pdf/2408.06276.pdf", "abs": "https://arxiv.org/abs/2408.06276", "title": "Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation", "authors": ["Jieyong Kim", "Hyunseo Kim", "Hyunjin Cho", "SeongKu Kang", "Buru Chang", "Jinyoung Yeo", "Dongha Lee"], "categories": ["cs.CL"], "comment": "Accepted to SIGIR 2025", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."}
{"id": "2408.16073", "pdf": "https://arxiv.org/pdf/2408.16073.pdf", "abs": "https://arxiv.org/abs/2408.16073", "title": "Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings", "authors": ["Leo Yeykelis", "Kaavya Pichai", "James J. Cummings", "Byron Reeves"], "categories": ["cs.CL", "cs.AI"], "comment": "40 pages, 13 figures, 3 tables", "summary": "This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication and generalization of published research about\nmessage effects in marketing. LLM-powered participants (personas) were tested\nby replicating 133 experimental findings from 14 papers containing 45 recent\nstudies published in the Journal of Marketing. For each study, the measures,\nstimuli, and sampling specifications were used to generate prompts for LLMs to\nact as unique personas. The AI personas, 19,447 in total across all of the\nstudies, generated complete datasets and statistical analyses were then\ncompared with the original human study results. The LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication. The overall\nreplication rate including interaction effects was 68% (90 out of 133).\nFurthermore, a test of how human results generalized to different participant\nsamples, media stimuli, and measures showed that replication results can change\nwhen tests go beyond the parameters of the original human studies. Implications\nare discussed for the replication and generalizability crises in social\nscience, the acceleration of theory building in media and marketing psychology,\nand the practical advantages of rapid message testing for consumer products.\nLimitations of AI replications are addressed with respect to complex\ninteraction effects, biases in AI models, and establishing benchmarks for AI\nmetrics in marketing research."}
{"id": "2409.08813", "pdf": "https://arxiv.org/pdf/2409.08813.pdf", "abs": "https://arxiv.org/abs/2409.08813", "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment", "authors": ["Leitian Tao", "Yixuan Li"], "categories": ["cs.CL"], "comment": "Accepted by ICLR 2025", "summary": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback."}
{"id": "2409.12059", "pdf": "https://arxiv.org/pdf/2409.12059.pdf", "abs": "https://arxiv.org/abs/2409.12059", "title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning", "authors": ["Ningyuan Xi", "Xiaoyu Wang", "Yetao Wu", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Jinxian Qu", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 7 figures", "summary": "Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE."}
{"id": "2410.03727", "pdf": "https://arxiv.org/pdf/2410.03727.pdf", "abs": "https://arxiv.org/abs/2410.03727", "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"", "authors": ["Yifei Ming", "Senthil Purushwalkam", "Shrey Pandit", "Zixuan Ke", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The conference version of this paper is published at ICLR 2025", "summary": "Ensuring faithfulness to context in large language models (LLMs) and\nretrieval-augmented generation (RAG) systems is crucial for reliable deployment\nin real-world applications, as incorrect or unsupported information can erode\nuser trust. Despite advancements on standard benchmarks, faithfulness\nhallucination-where models generate responses misaligned with the provided\ncontext-remains a significant challenge. In this work, we introduce FaithEval,\na novel and comprehensive benchmark tailored to evaluate the faithfulness of\nLLMs in contextual scenarios across three diverse tasks: unanswerable,\ninconsistent, and counterfactual contexts. These tasks simulate real-world\nchallenges where retrieval mechanisms may surface incomplete, contradictory, or\nfabricated information. FaithEval comprises 4.9K high-quality problems in\ntotal, validated through a rigorous four-stage context construction and\nvalidation framework, employing both LLM-based auto-evaluation and human\nvalidation. Our extensive study across a wide range of open-source and\nproprietary models reveals that even state-of-the-art models often struggle to\nremain faithful to the given context, and that larger models do not necessarily\nexhibit improved faithfulness.Project is available at:\nhttps://github.com/SalesforceAIResearch/FaithEval."}
{"id": "2411.07611", "pdf": "https://arxiv.org/pdf/2411.07611.pdf", "abs": "https://arxiv.org/abs/2411.07611", "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models", "authors": ["Shuai Niu", "Jing Ma", "Hongzhan Lin", "Liang Bai", "Zhihua Wang", "Yida Xu", "Yunya Song", "Xian Yang"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "13 pages. 7 figures", "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability."}
{"id": "2412.11704", "pdf": "https://arxiv.org/pdf/2412.11704.pdf", "abs": "https://arxiv.org/abs/2412.11704", "title": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data", "authors": ["Atsuki Yamaguchi", "Terufumi Morishita", "Aline Villavicencio", "Nikolaos Aletras"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV."}
{"id": "2502.01220", "pdf": "https://arxiv.org/pdf/2502.01220.pdf", "abs": "https://arxiv.org/abs/2502.01220", "title": "Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations", "authors": ["Hichem Ammar Khodja", "Frédéric Béchet", "Quentin Brabant", "Alexis Nasr", "Gwénolé Lecorvé"], "categories": ["cs.CL", "cs.LG"], "comment": "preprint v3", "summary": "This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\naccuracy of LMs is analyzed along two dimensions: the distance of the incorrect\ncontext from the validity period and the granularity of the context. To this\nend, a dataset called TimeStress is introduced, enabling the evaluation of 18\ndiverse LMs. Results reveal that the best LM achieves perfect accuracy for only\n6% of the studied facts, with critical errors that humans would not make. This\nwork highlights the limitations of current LMs in temporal representation. We\nprovide all data and code for further research."}
{"id": "2502.12486", "pdf": "https://arxiv.org/pdf/2502.12486.pdf", "abs": "https://arxiv.org/abs/2502.12486", "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning", "authors": ["Xiaoqian Liu", "Ke Wang", "Yongbin Li", "Yuchuan Wu", "Wentao Ma", "Aobo Kong", "Fei Huang", "Jianbin Jiao", "Junge Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 4 figures", "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications. Code and data are available\nat https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO."}
{"id": "2502.15654", "pdf": "https://arxiv.org/pdf/2502.15654.pdf", "abs": "https://arxiv.org/abs/2502.15654", "title": "Machine-generated text detection prevents language model collapse", "authors": ["George Drayson", "Emine Yilmaz", "Vasileios Lampos"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present. We release our code at\nhttps://github.com/GeorgeDrayson/model_collapse."}
{"id": "2503.10894", "pdf": "https://arxiv.org/pdf/2503.10894.pdf", "abs": "https://arxiv.org/abs/2503.10894", "title": "HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks", "authors": ["Jiuding Sun", "Jing Huang", "Sidharth Baskaran", "Karel D'Oosterlinck", "Christopher Potts", "Michael Sklar", "Atticus Geiger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Mechanistic interpretability has made great strides in identifying neural\nnetwork features (e.g., directions in hidden activation space) that mediate\nconcepts(e.g., the birth year of a person) and enable predictable manipulation.\nDistributed alignment search (DAS) leverages supervision from counterfactual\ndata to learn concept features within hidden states, but DAS assumes we can\nafford to conduct a brute force search over potential feature locations. To\naddress this, we present HyperDAS, a transformer-based hypernetwork\narchitecture that (1) automatically locates the token-positions of the residual\nstream that a concept is realized in and (2) constructs features of those\nresidual stream vectors for the concept. In experiments with Llama3-8B,\nHyperDAS achieves state-of-the-art performance on the RAVEL benchmark for\ndisentangling concepts in hidden states. In addition, we review the design\ndecisions we made to mitigate the concern that HyperDAS (like all powerful\ninterpretabilty methods) might inject new information into the target model\nrather than faithfully interpreting it."}
{"id": "2504.01840", "pdf": "https://arxiv.org/pdf/2504.01840.pdf", "abs": "https://arxiv.org/abs/2504.01840", "title": "LRAGE: Legal Retrieval Augmented Generation Evaluation Tool", "authors": ["Minhu Park", "Hongseok Oh", "Eunkyung Choi", "Wonseok Hwang"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE."}
{"id": "2504.02810", "pdf": "https://arxiv.org/pdf/2504.02810.pdf", "abs": "https://arxiv.org/abs/2504.02810", "title": "Generative Evaluation of Complex Reasoning in Large Language Models", "authors": ["Haowei Lin", "Xiangyu Wang", "Ruilin Yan", "Baizhou Huang", "Haotian Ye", "Jianhua Zhu", "Zihao Wang", "James Zou", "Jianzhu Ma", "Yitao Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."}
{"id": "2504.08040", "pdf": "https://arxiv.org/pdf/2504.08040.pdf", "abs": "https://arxiv.org/abs/2504.08040", "title": "Can Reasoning LLMs Enhance Clinical Document Classification?", "authors": ["Akram Mustafa", "Usman Naseem", "Mostafa Rahimi Azghadi"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Clinical document classification is essential for converting unstructured\nmedical texts into standardised ICD-10 diagnoses, yet it faces challenges due\nto complex medical language, privacy constraints, and limited annotated\ndatasets. Large Language Models (LLMs) offer promising improvements in accuracy\nand efficiency for this task. This study evaluates the performance and\nconsistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3\nMini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o\nMini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge\nsummaries using the MIMIC-IV dataset. Using cTAKES to structure clinical\nnarratives, models were assessed across three experimental runs, with majority\nvoting determining final predictions. Results showed that reasoning models\noutperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs\n60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and\nF1 score (76%). However, non-reasoning models demonstrated greater stability\n(91% vs 84% consistency). Performance varied across ICD-10 codes, with\nreasoning models excelling in complex cases but struggling with abstract\ncategories. Findings indicate a trade-off between accuracy and consistency,\nsuggesting that a hybrid approach could optimise clinical coding. Future\nresearch should explore multi-label classification, domain-specific\nfine-tuning, and ensemble methods to enhance model reliability in real-world\napplications."}
{"id": "2504.12285", "pdf": "https://arxiv.org/pdf/2504.12285.pdf", "abs": "https://arxiv.org/abs/2504.12285", "title": "BitNet b1.58 2B4T Technical Report", "authors": ["Shuming Ma", "Hongyu Wang", "Shaohan Huang", "Xingxing Zhang", "Ying Hu", "Ting Song", "Yan Xia", "Furu Wei"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures."}
{"id": "2504.14657", "pdf": "https://arxiv.org/pdf/2504.14657.pdf", "abs": "https://arxiv.org/abs/2504.14657", "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs", "authors": ["Yihan Lin", "Zhirong Bella Yu", "Simon Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025", "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings."}
{"id": "2504.15843", "pdf": "https://arxiv.org/pdf/2504.15843.pdf", "abs": "https://arxiv.org/abs/2504.15843", "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model", "authors": ["Junshu Pan", "Wei Shen", "Shulin Huang", "Qiji Zhou", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005.pdf", "abs": "https://arxiv.org/abs/2504.16005", "title": "CAPO: Cost-Aware Prompt Optimization", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119.pdf", "abs": "https://arxiv.org/abs/2504.17119", "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565.pdf", "abs": "https://arxiv.org/abs/2504.17565", "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671.pdf", "abs": "https://arxiv.org/abs/2504.17671", "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "authors": ["Yuanchang Ye", "Weiyan Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "1711.08058", "pdf": "https://arxiv.org/pdf/1711.08058.pdf", "abs": "https://arxiv.org/abs/1711.08058", "title": "Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio", "authors": ["Ahmad AbdulKader", "Kareem Nassar", "Mohamed El-Geish", "Daniel Galvez", "Chetan Patil"], "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "comment": "Published in the proceedings of NeurIPS 2017 Workshop: Machine\n  Learning on the Phone and other Consumer Devices", "summary": "We propose using cascaded classifiers for a keyword spotting (KWS) task on\nnarrow-band (NB), 8kHz audio acquired in non-IID environments -- a more\nchallenging task than most state-of-the-art KWS systems face. We present a\nmodel that incorporates Deep Neural Networks (DNNs), cascading,\nmultiple-feature representations, and multiple-instance learning. The cascaded\nclassifiers handle the task's class imbalance and reduce power consumption on\ncomputationally-constrained devices via early termination. The KWS system\nachieves a false negative rate of 6% at an hourly false positive rate of 0.75"}
{"id": "2405.15638", "pdf": "https://arxiv.org/pdf/2405.15638.pdf", "abs": "https://arxiv.org/abs/2405.15638", "title": "M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models", "authors": ["Hongyu Wang", "Jiayu Xu", "Senwei Xie", "Ruiping Wang", "Jialin Li", "Zhaojie Xie", "Bin Zhang", "Chuyan Xiong", "Xilin Chen"], "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "Multilingual capability is an essential aspect for large multimodal models,\nsince they are usually deployed across various countries and languages.\nHowever, most existing benchmarks for multilingual multimodal reasoning\nstruggle to differentiate between models of varying performance; even language\nmodels without visual capabilities can easily achieve high scores. This leaves\na comprehensive evaluation of leading multilingual multimodal models largely\nunexplored. In this work, we introduce M4U, a novel and challenging benchmark\nfor assessing the capability of multi-discipline multilingual multimodal\nunderstanding and reasoning. M4U contains 10k samples covering 64 disciplines\nacross 16 subfields in Science, Engineering, and Healthcare in six languages.\nUsing M4U, we conduct extensive evaluations of leading Large Multimodal Models\n(LMMs) and Large Language Models (LLMs) with external tools. The evaluation\nresults demonstrate that the state-of-the-art model, GPT-4o, achieves only\n47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs\nexhibit significant language preferences. Our in-depth analysis indicates that\nleading LMMs, including GPT-4o, struggle to perform reasoning using\nmultilingual information present in both visual and textual context.\nSpecifically, they suffer performance degradation when prompted with\ncross-lingual multimodal questions. Our code and dataset is public available."}
{"id": "2406.02566", "pdf": "https://arxiv.org/pdf/2406.02566.pdf", "abs": "https://arxiv.org/abs/2406.02566", "title": "Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition", "authors": ["Ognjen Kundacina", "Vladimir Vincan", "Dragisa Miskovic"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This paper introduces a novel two-stage active learning (AL) pipeline for\nautomatic speech recognition (ASR), combining unsupervised and supervised AL\nmethods. The first stage utilizes unsupervised AL by using x-vectors clustering\nfor diverse sample selection from unlabeled speech data, thus establishing a\nrobust initial dataset for the subsequent supervised AL. The second stage\nincorporates a supervised AL strategy, with a batch AL method specifically\ndeveloped for ASR, aimed at selecting diverse and informative batches of\nsamples. Here, sample diversity is also achieved using x-vectors clustering,\nwhile the most informative samples are identified using a Bayesian AL method\ntailored for ASR with an adaptation of Monte Carlo dropout to approximate\nBayesian inference. This approach enables precise uncertainty estimation,\nthereby enhancing ASR model training with significantly reduced data\nrequirements. Our method has shown superior performance compared to competing\nmethods on homogeneous, heterogeneous, and OOD test sets, demonstrating that\nstrategic sample selection and innovative Bayesian modeling can substantially\noptimize both labeling effort and data utilization in deep learning-based ASR\napplications."}
{"id": "2407.09709", "pdf": "https://arxiv.org/pdf/2407.09709.pdf", "abs": "https://arxiv.org/abs/2407.09709", "title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling", "authors": ["Lecheng Kong", "Jiarui Feng", "Hao Liu", "Chengsong Huang", "Jiaxin Huang", "Yixin Chen", "Muhan Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Foundation models, such as Large Language Models (LLMs) or Large Vision\nModels (LVMs), have emerged as one of the most powerful tools in the respective\nfields. However, unlike text and image data, graph data do not have a\ndefinitive structure, posing great challenges to developing a Graph Foundation\nModel (GFM). For example, current attempts at designing general graph models\neither transform graph data into a language format for LLM-based prediction or\nstill train a GNN model with LLM as an assistant. The former can handle\nunlimited tasks, while the latter captures graph structure much better -- yet,\nno existing work can achieve both simultaneously. In this paper, we identify\nthree key desirable properties of a GFM: self-supervised pretraining, fluidity\nin tasks, and graph awareness. To account for these properties, we extend the\nconventional language modeling to the graph domain and propose a novel\ngenerative graph language model GOFA to solve the problem. The model\ninterleaves randomly initialized GNN layers into a frozen pre-trained LLM so\nthat the semantic and structural modeling abilities are organically combined.\nGOFA is pre-trained on newly proposed graph-level next-word prediction,\nquestion-answering, and structural tasks to obtain the above GFM properties.\nThe pre-trained model is further fine-tuned on downstream tasks to obtain\ntask-solving ability. The fine-tuned model is evaluated on various downstream\ntasks, demonstrating a strong ability to solve structural and contextual\nproblems in zero-shot scenarios. The code is available at\nhttps://github.com/JiaruiFeng/GOFA."}
{"id": "2408.06621", "pdf": "https://arxiv.org/pdf/2408.06621.pdf", "abs": "https://arxiv.org/abs/2408.06621", "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs", "authors": ["Sungmin Cha", "Sungjun Cho", "Dasol Hwang", "Moontae Lee"], "categories": ["cs.LG", "cs.CL"], "comment": "ICLR 2025 camera-ready version", "summary": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables\nrobust and efficient unlearning for LLMs. First, we introduce Inverted Hinge\nLoss, which suppresses unwanted tokens while maintaining fluency by boosting\nthe probability of the next most likely token. Second, we develop a\ndata-adaptive initialization for LoRA adapters via low-rank approximation\nweighted with relative Fisher information, thereby focusing updates on\nparameters critical for removing targeted knowledge. Experiments on the\nTraining Data Extraction Challenge dataset using GPT-Neo models as well as on\nthe TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our\napproach effectively removes sensitive information while maintaining reasoning\nand generative capabilities with minimal impact. Our implementation can be\nfound in https://github.com/csm9493/efficient-llm-unlearning."}
{"id": "2408.08990", "pdf": "https://arxiv.org/pdf/2408.08990.pdf", "abs": "https://arxiv.org/abs/2408.08990", "title": "Adaptive Uncertainty Quantification for Generative AI", "authors": ["Jungeum Kim", "Sean O'Hagan", "Veronika Rockova"], "categories": ["stat.ME", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "This work is concerned with conformal prediction in contemporary applications\n(including generative AI) where a black-box model has been trained on data that\nare not accessible to the user. Mirroring split-conformal inference, we design\na wrapper around a black-box algorithm which calibrates conformity scores. This\ncalibration is local and proceeds in two stages by first adaptively\npartitioning the predictor space into groups and then calibrating sectionally\ngroup by group. Adaptive partitioning (self-grouping) is achieved by fitting a\nrobust regression tree to the conformity scores on the calibration set. This\nnew tree variant is designed in such a way that adding a single new observation\ndoes not change the tree fit with overwhelmingly large probability. This\nadd-one-in robustness property allows us to conclude a finite sample\ngroup-conditional coverage guarantee, a refinement of the marginal guarantee.\nIn addition, unlike traditional split-conformal inference, adaptive splitting\nand within-group calibration yields adaptive bands which can stretch and shrink\nlocally. We demonstrate benefits of local tightening on several simulated as\nwell as real examples using non-parametric regression. Finally, we consider two\ncontemporary classification applications for obtaining uncertainty\nquantification around GPT-4o predictions. We conformalize skin disease\ndiagnoses based on self-reported symptoms as well as predicted states of U.S.\nlegislators based on summaries of their ideology. We demonstrate substantial\nlocal tightening of the uncertainty sets while attaining similar marginal\ncoverage."}
{"id": "2410.12881", "pdf": "https://arxiv.org/pdf/2410.12881.pdf", "abs": "https://arxiv.org/abs/2410.12881", "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "John Kamalu", "Sanjeev Satheesh", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "categories": ["cs.AI", "cs.CL"], "comment": "31 pages, 5 figures, 14 tables", "summary": "The utility of synthetic data to enhance pretraining data quality and hence\nto improve downstream task accuracy has been widely explored in recent large\nlanguage models (LLMs). Yet, these approaches fall inadequate in complex,\nmulti-hop and mathematical reasoning tasks as the synthetic data typically\nfails to add complementary knowledge to the existing raw corpus. In this work,\nwe propose a novel large-scale and diverse Math Informed syNthetic Dialogue\n(MIND) generation method that improves the mathematical reasoning ability of\nLLMs. Specifically, using MIND, we generate synthetic conversations based on\nOpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments\nwith different conversational settings reveal that incorporating knowledge gaps\nbetween dialog participants is essential for generating high-quality math data.\nWe further identify an effective way to format and integrate synthetic and raw\ndata during pretraining to maximize the gain in mathematical reasoning,\nemphasizing the need to restructure raw data rather than use it as-is. Compared\nto pretraining just on raw data, a model pretrained on MIND-OWM shows\nsignificant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%),\nincluding superior performance in specialized knowledge (MMLU: +4.55%,\nMMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING:\n+2.51%)."}
{"id": "2411.01841", "pdf": "https://arxiv.org/pdf/2411.01841.pdf", "abs": "https://arxiv.org/abs/2411.01841", "title": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification", "authors": ["Shi Dong", "Xiaobei Niu", "Rui Zhong", "Zhifeng Wang", "Mingzhang Zuo"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Accurate annotation of educational resources is crucial for effective\npersonalized learning and resource recommendation in online education. However,\nfine-grained knowledge labels often overlap or share similarities, making it\ndifficult for existing multi-label classification methods to differentiate\nthem. The label distribution imbalance due to sparsity of human annotations\nfurther intensifies these challenges. To address these issues, this paper\nintroduces RR2QC, a novel Retrieval Reranking method to multi-label Question\nClassification by leveraging label semantics and meta-label refinement. First,\nRR2QC improves the pre-training strategy by utilizing semantic relationships\nwithin and across label groups. Second, it introduces a class center learning\ntask to align questions with label semantics during downstream training.\nFinally, this method decomposes labels into meta-labels and uses a meta-label\nclassifier to rerank the retrieved label sequences. In doing so, RR2QC enhances\nthe understanding and prediction capability of long-tail labels by learning\nfrom meta-labels that frequently appear in other labels. Additionally, a\nmathematical LLM is used to generate solutions for questions, extracting latent\ninformation to further refine the model's insights. Experimental results show\nthat RR2QC outperforms existing methods in Precision@K and F1 scores across\nmultiple educational datasets, demonstrating its effectiveness for online\neducation applications. The code and datasets are available at\nhttps://github.com/78Erii/RR2QC."}
{"id": "2502.19546", "pdf": "https://arxiv.org/pdf/2502.19546.pdf", "abs": "https://arxiv.org/abs/2502.19546", "title": "Repurposing the scientific literature with vision-language models", "authors": ["Anton Alyakin", "Jaden Stryker", "Daniel Alexander Alber", "Karl L. Sangwon", "Jin Vivian Lee", "Brandon Duderstadt", "Akshay Save", "David Kurland", "Spencer Frome", "Shrutika Singh", "Jeff Zhang", "Eunice Yang", "Ki Yun Park", "Cordelia Orillac", "Aly A. Valliani", "Sean Neifert", "Albert Liu", "Aneek Patel", "Christopher Livia", "Darryl Lau", "Ilya Laufer", "Peter A. Rozman", "Eveline Teresa Hidalgo", "Howard Riina", "Rui Feng", "Todd Hollon", "Yindalon Aphinyanaphongs", "John G. Golfinos", "Laura Snyder", "Eric Leuthardt", "Douglas Kondziolka", "Eric Karl Oermann"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Leading vision-language models (VLMs) are trained on general Internet\ncontent, overlooking scientific journals' rich, domain-specific knowledge.\nTraining on specialty-specific literature could yield high-performance,\ntask-specific tools, enabling generative AI to match generalist models in\nspecialty publishing, educational, and clinical tasks. We created NeuroPubs, a\nmultimodal dataset of 23,000 Neurosurgery Publications articles (134M words,\n78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready\ngraphical abstracts (70% of 100 abstracts) and board-style questions\nindistinguishable from human-written ones (54% of 89,587 questions). We used\nthese questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,\nrandomized controlled trial, our model demonstrated non-inferiority to then\nstate-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical\nutility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,\np=0.3797). Our pilot study demonstrates how training generative AI models on\nspecialty-specific journal content - without large-scale internet data -\nresults in high-performance academic and clinical tools, enabling\ndomain-tailored AI across diverse fields."}
{"id": "2504.08907", "pdf": "https://arxiv.org/pdf/2504.08907.pdf", "abs": "https://arxiv.org/abs/2504.08907", "title": "Spatial Audio Processing with Large Language Model on Wearable Devices", "authors": ["Ayushi Mishra", "Yang Bai", "Priyadarshan Narayanasamy", "Nakul Garg", "Nirupam Roy"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences."}
{"id": "2504.13146", "pdf": "https://arxiv.org/pdf/2504.13146.pdf", "abs": "https://arxiv.org/abs/2504.13146", "title": "Antidistillation Sampling", "authors": ["Yash Savani", "Asher Trockman", "Zhili Feng", "Avi Schwarzschild", "Alexander Robey", "Marc Finzi", "J. Zico Kolter"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By strategically\nmodifying a model's next-token probability distribution, antidistillation\nsampling poisons reasoning traces, rendering them significantly less effective\nfor distillation while preserving the model's practical utility. For further\ndetails, see https://antidistillation.com."}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365.pdf", "abs": "https://arxiv.org/abs/2504.17365", "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
