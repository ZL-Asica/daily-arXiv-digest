{"id": "2507.01081", "pdf": "https://arxiv.org/pdf/2507.01081.pdf", "abs": "https://arxiv.org/abs/2507.01081", "title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma", "authors": ["Megan T. deBettencourt", "Sruthi Sakthivel", "Emily A. Holmes", "Mark Chevillet"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Trauma prevalence is vast globally. Evidence-based digital treatments can\nhelp, but most require human guidance. Human guides provide tailored\ninstructions and responsiveness to internal cognitive states, but limit\nscalability. Can generative AI and neurotechnology provide a scalable\nalternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to\nautomatically deliver and monitor an evidence-based digital treatment,\nspecifically the Imagery Competing Task Intervention (ICTI), to reduce\nintrusive memories after psychological trauma. One hundred healthy volunteers\nwere exposed to videos of traumatic events and randomly assigned to an\nintervention or active control condition. As predicted, intervention\nparticipants reported significantly fewer intrusive memories over the following\nweek. Post-hoc assessment against clinical rubrics confirmed the AI guide\ndelivered the intervention successfully. Additionally, pupil size tracked\nintervention engagement and predicted symptom reduction, providing a candidate\nbiomarker of intervention effectiveness. These findings open a path toward\nrigorous AI-guided digital interventions that can scale to trauma prevalence.", "AI": {"tldr": "This study tests ANTIDOTE, an AI-powered digital treatment for reducing intrusive memories following trauma, showing promising results in engagement and effectiveness.", "motivation": "To explore scalable alternatives to human-guided trauma therapies using generative AI and neurotechnology.", "method": "The study involved 100 healthy volunteers exposed to traumatic videos, with participants assigned to either an AI-guided intervention or an active control condition.", "result": "Participants receiving the AI-guided intervention reported significantly fewer intrusive memories over the following week, with pupil size tracking engagement and predicting symptom reduction.", "conclusion": "The findings suggest that AI-guided digital interventions may be effective and scalable solutions for trauma treatment.", "key_contributions": ["Introduces ANTIDOTE, a novel AI-guided intervention for trauma.", "Demonstrates the use of pupillometry as a biomarker for engagement and symptom reduction.", "Confirms the successful delivery of trauma treatment through AI without human guidance."], "limitations": "", "keywords": ["AI-guided therapy", "pupillometry", "trauma treatment", "digital intervention", "evidence-based"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01121", "pdf": "https://arxiv.org/pdf/2507.01121.pdf", "abs": "https://arxiv.org/abs/2507.01121", "title": "From Literature to ReWA: Discussing Reproductive Well-being in HCI", "authors": ["Hafsah Mahzabin Chowdhury", "Sharifa Sultana"], "categories": ["cs.HC", "cs.CY"], "comment": "23 pages", "summary": "Reproductive well-being is shaped by intersecting cultural, religious,\ngendered, and political contexts, yet current technologies often reflect\nnarrow, Western-centric assumptions. In this literature review, we synthesize\nfindings from 147 peer-reviewed papers published between 2015 and 2025 across\nHCI, CSCW and social computing, ICTD, digital and public health, and AI for\nwell-being scholarship to map the evolving reproductive well-being landscape.\nWe identify three thematic waves that focused on early access and education,\ncultural sensitivity and privacy, and AI integration with policy-aware design,\nand highlight how technologies support or constrain diverse reproductive\nexperiences. Our analysis reveals critical gaps in inclusivity, with persistent\nexclusions of men and non-binary users, migrants, and users in the Global\nSouth. Additionally, we surfaced the significant absence of literature on the\nrole of stakeholders (e.g., husband and family members, household maids and\ncleaning helping hands, midwife, etc.) in the reproductive well-being space.\nDrawing on the findings from the literature, we propose the ReWA framework to\nsupport reproductive well-being for all agendas through six design orientations\nassociated with: location, culture, and history; polyvocality and agency;\nrationality, temporality, distributive roles, and methodology.", "AI": {"tldr": "This literature review synthesizes findings from 147 papers to explore the intersection of technology and reproductive well-being, highlighting inclusivity gaps and proposing a framework for design.", "motivation": "To address the narrow, Western-centric assumptions in current reproductive well-being technologies and synthesize diverse perspectives in the intersection of culture, gender, and technology.", "method": "Literature review of 147 peer-reviewed papers across various fields, analyzing themes in technology and reproductive well-being.", "result": "Identified three thematic waves: early access and education, cultural sensitivity and privacy, and AI integration with policy-aware design. Highlighted gaps in inclusivity for various user groups, including men, non-binary users, and migrants.", "conclusion": "The proposed ReWA framework provides six design orientations to enhance reproductive well-being across diverse contexts, emphasizing inclusivity and stakeholder involvement.", "key_contributions": ["Synthesis of diverse literature on reproductive well-being and technology.", "Identification of critical inclusivity gaps in current technological approaches.", "Introduction of the ReWA framework for a more inclusive design process."], "limitations": "Limited focus on the role of specific stakeholders in reproductive well-being; potential bias in the geographical representation of studies.", "keywords": ["reproductive well-being", "human-computer interaction", "cultural sensitivity", "AI integration", "framework"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.01134", "pdf": "https://arxiv.org/pdf/2507.01134.pdf", "abs": "https://arxiv.org/abs/2507.01134", "title": "Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies", "authors": ["Braden Roper", "William Thompson", "Chris Weaver"], "categories": ["cs.HC"], "comment": "To be published in IEEE Visualization and Visual Analytics (VIS),\n  2025", "summary": "Game-Based Learning has proven to be an effective method for enhancing\nengagement with educational material. However, gaining a deeper understanding\nof player strategies remains challenging. Sequential game-state and\naction-based tracking tools often gather extensive data that can be difficult\nto interpret as long-term strategy. This data presents unique problems to\nvisualization, as it can be fairly natural, noisy data but is constrained\nwithin synthetic, controlled environments, leading to issues such as\noverplotting which can make interpretation complicated. We propose an animated\nvisual encoding tool that utilizes kinetic visualization to address these\nissues. This tool enables researchers to construct animated data narratives\nthrough the configuration of parameter interpolation curves and blending\nlayers. Finally, we demonstrate the usefulness of the tool while addressing\nspecific interests as outlined by a domain expert collaborator.", "AI": {"tldr": "The paper introduces an animated visual encoding tool that utilizes kinetic visualization to improve understanding of player strategies in Game-Based Learning.", "motivation": "There is a need for better tools to interpret and visualize player strategy data in Game-Based Learning contexts.", "method": "The proposed tool employs kinetic visualization techniques that allow researchers to create animated data narratives using parameter interpolation curves and blending layers.", "result": "The tool effectively addresses visualization challenges such as overplotting and noise in data collected from game-state and action tracking.", "conclusion": "This animated visual encoding tool demonstrates enhanced interpretability of player strategies, facilitating deeper insights for researchers.", "key_contributions": ["Introduction of a kinetic visualization tool for Game-Based Learning", "Capability to create animated data narratives through parameter interpolation", "Addressing challenges of visualizing noisy and complex data from gameplay"], "limitations": "The effectiveness of the tool may vary depending on the specific use case and data characteristics.", "keywords": ["Game-Based Learning", "visualization", "kinetic visualization", "animated data narratives", "player strategies"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.01166", "pdf": "https://arxiv.org/pdf/2507.01166.pdf", "abs": "https://arxiv.org/abs/2507.01166", "title": "A Methodological Framework for Capturing Cognitive-Affective States in Collaborative Learning", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Nathaniel Blanchard"], "categories": ["cs.HC"], "comment": "Accepted to the Interactive Workshop: Multimodal, Multiparty Learning\n  Analytics (MMLA) at the conference Educational Data Mining (EDM) 2025", "summary": "Identification of affective and attentional states of individuals within\ngroups is difficult to obtain without disrupting the natural flow of\ncollaboration. Recent work from our group used a retrospect cued recall\nparadigm where participants spoke about their cognitive-affective states while\nthey viewed videos of their groups. We then collected additional participants\nwhere their reports were constrained to a subset of pre-identified\ncognitive-affective states. In this latter case, participants either self\nreported or reported in response to probes. Here, we present an initial\nanalysis of the frequency and temporal distribution of participant reports, and\nhow the distributions of labels changed across the two collections. Our\napproach has implications for the educational data mining community in tracking\ncognitive-affective states in collaborative learning more effectively and in\ndeveloping improved adaptive learning systems that can detect and respond to\ncognitive-affective states.", "AI": {"tldr": "Analyzes how to identify cognitive-affective states in collaborative learning using participant reports and videos.", "motivation": "To track cognitive-affective states without disrupting group collaboration and improve adaptive learning systems.", "method": "Used a retrospect cued recall paradigm and pre-identified cognitive-affective states to gather participant reports.", "result": "Discovery of frequency and temporal changes in participant reports across two collection methods.", "conclusion": "Improves techniques for monitoring cognitive-affective states in collaborative learning, informing educational data mining and adaptive systems.", "key_contributions": ["Initial analysis of cognitive-affective state reporting", "Comparison of self-reports and probe responses", "Implications for adaptive learning systems"], "limitations": "", "keywords": ["cognitive-affective states", "collaborative learning", "adaptive learning systems"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.01019", "pdf": "https://arxiv.org/pdf/2507.01019.pdf", "abs": "https://arxiv.org/abs/2507.01019", "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "This paper introduces MALIBU, a benchmark for assessing biases in multi-agent systems utilizing large language models (LLMs) in persona-based interactions.", "motivation": "The study aims to address the reinforcement of implicit biases in LLM-based multi-agent systems, which can lead to unfair representations and a lack of equitable outcomes.", "method": "MALIBU evaluates bias through scenario-based assessments where AI models complete tasks in predefined contexts. Responses are scored by a multi-agent judging system in two phases assessing labeled demographics and preference comparisons.", "result": "The study found that bias mitigation strategies might benefit marginalized personas, indicating a complexity in achieving true neutrality across responses.", "conclusion": "The findings highlight the necessity for nuanced bias detection methods, balanced fairness strategies, and the development of transparent evaluation benchmarks for multi-agent systems.", "key_contributions": ["Introduction of the MALIBU benchmark for bias assessment in LLM multi-agent systems", "Development of a methodology for scenario-based assessments of biases", "Identification of potential trade-offs in bias mitigation strategies"], "limitations": "The study primarily focuses on specific demographic personas and may not fully encapsulate all forms of bias present in LLM outputs.", "keywords": ["multi-agent systems", "bias assessment", "large language models", "fairness", "transparent evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.01209", "pdf": "https://arxiv.org/pdf/2507.01209.pdf", "abs": "https://arxiv.org/abs/2507.01209", "title": "Judgment as Coordination: A Joint Systems View of Visualization Design Practice", "authors": ["Paul C. Parsons", "Arran Ridley"], "categories": ["cs.HC"], "comment": "IEEE VIS 2025 (conditional acceptance)", "summary": "Professional visualization design has become an increasingly important area\nof inquiry, yet much of the field's discourse remains anchored in\nresearcher-centered contexts. Studies of design practice often focus on\nindividual designers' decisions and reflections, offering limited insight into\nthe collaborative and systemic dimensions of professional work. In this paper,\nwe propose a systems-level reframing of design judgment grounded in the\ncoordination and adaptation that sustain progress amid uncertainty, constraint,\nand misalignment. Drawing on sustained engagement across multiple empirical\nstudies--including ethnographic observation of design teams and qualitative\nstudies of individual practitioners--we identify recurring episodes in which\ncoherence was preserved not by selecting an optimal option, but by repairing\nalignment, adjusting plans, and reframing goals. We interpret these dynamics\nthrough the lens of Joint Cognitive Systems, which provide tools for analyzing\nhow judgment emerges as a distributed capacity within sociotechnical activity.\nThis perspective surfaces often-invisible work in visualization design and\noffers researchers a new conceptual vocabulary for studying how design activity\nis sustained in practice.", "AI": {"tldr": "The paper reframes professional visualization design by emphasizing systems-level dynamics in design judgment, focusing on collaboration and adaptation in practice.", "motivation": "To shift the discourse in visualization design from researcher-centered to a systems-level understanding that incorporates collaborative and systemic dimensions.", "method": "The study involved ethnographic observations of design teams and qualitative studies with individual practitioners to explore design judgment at a systems level.", "result": "The analysis revealed that coherence in design work is often maintained through repairing alignment and adjusting plans rather than by choosing optimal options.", "conclusion": "This systems perspective highlights hidden aspects of design work and provides a conceptual vocabulary for better understanding design activities in practice.", "key_contributions": ["Proposal of a systems-level reframing of design judgment", "Insights into collaborative dynamics in design practice", "Introduction of Joint Cognitive Systems as a framework for analysis"], "limitations": "", "keywords": ["visualization design", "design judgment", "Joint Cognitive Systems"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2507.01160", "pdf": "https://arxiv.org/pdf/2507.01160.pdf", "abs": "https://arxiv.org/abs/2507.01160", "title": "Event-based evaluation of abstractive news summarization", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja Øvrelid"], "categories": ["cs.CL"], "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "This paper proposes a method to evaluate the quality of abstractive summaries by focusing on overlapping events between generated summaries, reference summaries, and original news articles.", "motivation": "Traditional evaluation of generated summaries often relies on human-authored summaries as references, which may not effectively represent the events reported in news articles.", "method": "The authors propose a novel evaluation approach that calculates overlapping events between generated summaries, reference summaries, and original articles, utilizing a richly annotated Norwegian dataset with event annotations.", "result": "Experimental results demonstrate that this new evaluation method provides deeper insights into the event information captured in the summaries compared to traditional methods.", "conclusion": "The proposed evaluation metric enhances the understanding of the quality of abstractive summaries by focusing on event coverage, potentially leading to better summary generation techniques.", "key_contributions": ["Introduction of a new evaluation method focusing on event overlaps for summarization quality assessment.", "Utilization of an annotated dataset that includes event information relevant to news articles.", "Provision of insights into how well generative models capture event information in summaries."], "limitations": "The study is based on a specific Norwegian dataset, which may limit the generalizability of the findings to other languages or contexts.", "keywords": ["abstractive summarization", "event evaluation", "generative models", "natural language processing", "news articles"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.01274", "pdf": "https://arxiv.org/pdf/2507.01274.pdf", "abs": "https://arxiv.org/abs/2507.01274", "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance", "authors": ["Vishakha Lall", "Yisi Liu"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted and Presented at 11th International Maritime Science\n  Conference", "summary": "Traditional simulator-based training for maritime professionals is critical\nfor ensuring safety at sea but often depends on subjective trainer assessments\nof technical skills, behavioral focus, communication, and body language, posing\nchallenges such as subjectivity, difficulty in measuring key features, and\ncognitive limitations. Addressing these issues, this study develops an\nAI-driven framework to enhance maritime training by objectively assessing\ntrainee performance through visual focus tracking, speech recognition, and\nstress detection, improving readiness for high-risk scenarios. The system\nintegrates AI techniques, including visual focus determination using eye\ntracking, pupil dilation analysis, and computer vision; communication analysis\nthrough a maritime-specific speech-to-text model and natural language\nprocessing; communication correctness using large language models; and mental\nstress detection via vocal pitch. Models were evaluated on data from simulated\nmaritime scenarios with seafarers exposed to controlled high-stress events. The\nAI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for\nmaritime speech recognition, and ~90% for stress detection, surpassing existing\nbenchmarks. The system provides insights into visual attention, adherence to\ncommunication checklists, and stress levels under demanding conditions. This\nstudy demonstrates how AI can transform maritime training by delivering\nobjective performance analytics, enabling personalized feedback, and improving\npreparedness for real-world operational challenges.", "AI": {"tldr": "An AI-driven framework enhances maritime training by effectively assessing trainee performance through objective measures like visual focus tracking and speech recognition, thus improving readiness for high-risk scenarios.", "motivation": "The study addresses subjectivity and measurement challenges in traditional maritime training methods that rely on trainer assessments.", "method": "The framework utilizes AI techniques for visual focus determination, speech recognition specific to maritime contexts, and stress detection through vocal analysis.", "result": "The AI algorithms achieved approximately 92% accuracy in visual detection, 91% in maritime speech recognition, and 90% in stress detection, outperforming current benchmarks.", "conclusion": "AI can significantly improve maritime training by providing objective performance analytics and facilitating personalized feedback, enhancing preparedness for operational challenges.", "key_contributions": ["Development of an AI framework for objective performance assessment in maritime training", "Integration of eye tracking and speech recognition tailored for maritime scenarios", "Demonstration of high accuracy in stress detection and performance metrics"], "limitations": "", "keywords": ["maritime training", "AI", "performance assessment", "speech recognition", "stress detection"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.01170", "pdf": "https://arxiv.org/pdf/2507.01170.pdf", "abs": "https://arxiv.org/abs/2507.01170", "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "authors": ["Simon Börjesson", "Erik Ersmark", "Pierre Nugues"], "categories": ["cs.CL"], "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "This paper analyzes the evolution of geographic entries in the \u00174Nordisk familjebok\u00174 encyclopedia through digitized text from two editions, using semantic embeddings and classifiers to identify shifts in focus between 1876-1926.", "motivation": "To study the changes in geographic focus in the \u00174Nordisk familjebok\u00174 encyclopedia as it reflects intellectual trends in Sweden influenced by significant global events.", "method": "The authors employed semantic sentence embeddings for resegmenting raw text and matched entries between editions. They used a transformer-based classifier to extract geographical entries and linked them to Wikidata.", "result": "The analysis reveals a notable shift in geographic focus from Europe towards North America, Africa, Asia, Australia, and northern Scandinavia between the first and second editions.", "conclusion": "The findings indicate a small yet significant shift in emphasis that aligns with historical events such as the First World War and the emergence of new global powers.", "key_contributions": ["Usage of semantic embeddings to analyze historical texts", "Identification of geographic trends in an influential encyclopedia", "Linking of entries to Wikidata for enhanced scholarly access"], "limitations": "", "keywords": ["Nordisk familjebok", "semantic embeddings", "geographic focus", "Wikidata", "historical encyclopedia"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.01436", "pdf": "https://arxiv.org/pdf/2507.01436.pdf", "abs": "https://arxiv.org/abs/2507.01436", "title": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting", "authors": ["Luke S. Snyder", "Chenglong Wang", "Steven Drucker"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures, 1 table", "summary": "Despite the ubiquity of visualization examples published on the web,\nretargeting existing custom chart implementations to new datasets remains\ndifficult, time-intensive, and tedious. The adaptation process assumes author\nfamiliarity with both the implementation of the example as well as how the new\ndataset might need to be transformed to fit into the example code. With recent\nadvances in Large Language Models (LLMs), automatic adaptation of code can be\nachieved from high-level user prompts, reducing the barrier for visualization\nretargeting. To better understand how LLMs can assist retargeting and its\npotential limitations, we characterize and evaluate the performance of LLM\nassistance across multiple datasets and charts of varying complexity,\ncategorizing failures according to type and severity. In our evaluation, we\ncompare two approaches: (1) directly instructing the LLM model to fully\ngenerate and adapt code by treating code as text inputs and (2) a more\nconstrained program synthesis pipeline where the LLM guides the code\nconstruction process by providing structural information (e.g., visual\nencodings) based on properties of the example code and data. We find that both\napproaches struggle when new data has not been appropriately transformed, and\ndiscuss important design recommendations for future retargeting systems.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) can assist in adapting visualization code to new datasets, comparing two approaches in effectiveness and identifying limitations.", "motivation": "The paper addresses the challenges in retargeting existing custom chart implementations for new datasets, which is often difficult and time-consuming for users.", "method": "The authors evaluate LLM assistance performance across various datasets and chart complexities, comparing direct instruction of LLMs with a constrained program synthesis approach.", "result": "The evaluation reveals that both approaches face challenges if the new data is not properly transformed, highlighting their limitations.", "conclusion": "The findings lead to design recommendations for future retargeting systems to enhance their effectiveness in utilizing LLMs for visualization adaptation.", "key_contributions": ["Characterization of LLM performance in visualization retargeting.", "Comparison of direct LLM instruction vs. program synthesis pipeline.", "Design recommendations for improving LLM-assisted visualization retargeting."], "limitations": "Struggles with untransformed new data when adapting existing visualization examples.", "keywords": ["visualization", "Large Language Models", "retargeting", "program synthesis", "chart adaptation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01213", "pdf": "https://arxiv.org/pdf/2507.01213.pdf", "abs": "https://arxiv.org/abs/2507.01213", "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "categories": ["cs.CL"], "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "This paper proposes a novel framework, xLSTM with Multihead Exponential Gated Fusion (MEGA), for aspect-based sentiment analysis (ABSA) that improves efficiency and performance by utilizing an advanced modeling technique.", "motivation": "Existing ABSA methods struggle with balancing computational efficiency and high performance, often lacking context or being computationally expensive.", "method": "The paper introduces xLSTM with Multihead Exponential Gated Fusion (MEGA), integrating a bi-directional mLSTM architecture with PF-mLSTM streams and an mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF).", "result": "Experimental results show that MEGA outperforms state-of-the-art baselines on three benchmark datasets, achieving superior accuracy and efficiency in ABSA tasks.", "conclusion": "The proposed MEGA framework effectively enhances localized context modeling and optimizes short-range dependency capture while maintaining global context.", "key_contributions": ["Introduction of xLSTM with Multihead Exponential Gated Fusion (MEGA) framework.", "Utilization of PF-mLSTM for enhanced localized context modeling.", "Development of the MECGAF mechanism for dynamic combination of outputs."], "limitations": "", "keywords": ["Aspect-based Sentiment Analysis", "NLP", "xLSTM", "Machine Learning", "Deep Learning"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2507.01471", "pdf": "https://arxiv.org/pdf/2507.01471.pdf", "abs": "https://arxiv.org/abs/2507.01471", "title": "Analysis of Drone-Assisted Building Inspection Training in VR vs 2D Monitor Display: an EEG Study", "authors": ["Pengkun Liu", "Jackson Greene", "Jiali Huang", "Pingbo Tang", "Yu Hou"], "categories": ["cs.HC"], "comment": null, "summary": "Researchers have been using simulation-based methods for drone-assisted\ninspection training. Multiple brain regions are associated with information\nprocesses and decision-making, and the connectivity of these regions may\nfurther influence inspectors' performance. However, researchers do not\nunderstand the pathways of the information flows when drone pilots process the\nmaintenance and manipulation of information, which may affect the efficiency of\ntacit knowledge transfer. This study aims to reveal the causal connection\nbetween participants' brain regions using an electroencephalogram and dynamic\ncausal modeling when processing drone-assisted building energy audit tasks\nusing different display modalities. The results showed similar single-direction\nconnectivity patterns for the different simulation groups. The results also\nshowed similar patterns between brain regions related to visual inspection\nperformance before and after training. These findings highlight the nature of\nbrain asymmetries and may be utilized in measuring cognitive states and\ndesigning adaptive automation in the knowledge transfer of drone-based\ninspection.", "AI": {"tldr": "The study investigates brain connectivity and its impact on drone-assisted inspection training, revealing patterns that could enhance knowledge transfer in such tasks.", "motivation": "To uncover how information processing in specific brain regions affects performance in drone-assisted inspection and the efficiency of tacit knowledge transfer.", "method": "Electroencephalogram (EEG) and dynamic causal modeling were used to analyze brain activity while participants performed drone-assisted energy audit tasks under different display modalities.", "result": "The study identified similar single-direction connectivity patterns across different simulation groups, with consistent brain activity linked to visual inspection performance before and after training.", "conclusion": "The findings suggest that understanding brain connectivity can inform the design of adaptive automation for improving knowledge transfer in drone inspections.", "key_contributions": ["Revealed causal connections between brain regions during drone task performance.", "Identified consistent brain connectivity patterns associated with visual inspection.", "Provided insights for designing adaptive automation in drone-based inspections."], "limitations": "", "keywords": ["drone-assisted inspection", "brain connectivity", "tacit knowledge transfer", "cognitive states", "adaptive automation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.01234", "pdf": "https://arxiv.org/pdf/2507.01234.pdf", "abs": "https://arxiv.org/abs/2507.01234", "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "categories": ["cs.CL"], "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "This paper presents a debiasing algorithm that effectively reduces biases in embedding-based similarity metrics for text sequences, leading to improved document similarity and clustering performance.", "motivation": "The research addresses biases in embedding-based text similarity metrics caused by spurious confounders, which hinder applications requiring texts from diverse sources.", "method": "The study introduces a debiasing algorithm that removes observed confounder information from encoder representations, ensuring cleaner embeddings.", "result": "The embeddings show improved performance in document similarity and clustering metrics across all evaluated embedding variants and tasks, with minimal computational cost involved.", "conclusion": "The proposed debiasing method not only reduces biases but also maintains performance on out-of-distribution benchmarks without degrading the embeddings.", "key_contributions": ["Introduction of a novel debiasing algorithm for text embeddings", "Demonstration of improved similarity and clustering performance", "Validation of retained effectiveness on out-of-distribution tasks"], "limitations": "", "keywords": ["embedding-based metrics", "debiasing algorithm", "text similarity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548.pdf", "abs": "https://arxiv.org/abs/2507.01548", "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.", "AI": {"tldr": "This paper examines AI-assisted co-creation for older adults, focusing on aging migrants in urban China, to help them express fragmented personal narratives through oral storytelling and symbolic reconstruction of Hanzi using LLM-supported techniques.", "motivation": "The study aims to address the underrepresentation of aging migrants' narratives and the challenges they face in verbalizing their experiences.", "method": "The research involved a pilot workshop where participants engaged in oral storytelling and symbolic reconstruction of Hanzi with AI assistance, utilizing physical materials and facilitated human interaction.", "result": "Participants successfully transformed their lived experiences into visual and tactile forms, demonstrating how AI can enhance narrative expression without requiring digital literacy.", "conclusion": "The findings suggest that AI can function as a supportive mechanism in human-AI collaborations, enhancing the narrative agency of older adults within sociotechnical systems.", "key_contributions": ["Introduces a novel approach to narrative expression for aging migrants using AI", "Reconceptualizes AI's role from content producer to supportive mechanism", "Demonstrates the feasibility of engaging older adults in AI-assisted creativity without digital literacy"], "limitations": "The study is limited to a specific demographic (aging migrants in urban China) and may not generalize to other populations or contexts.", "keywords": ["AI-assisted co-creation", "aging migrants", "narrative expression", "Hanzi", "human-AI collaboration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.01259", "pdf": "https://arxiv.org/pdf/2507.01259.pdf", "abs": "https://arxiv.org/abs/2507.01259", "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "authors": ["Michał Matak", "Jarosław A. Chudziak"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "This paper presents gAIus, a cognitive LLM-based agent designed to improve legal information retrieval for non-English and non-Chinese speaking countries, demonstrating substantial improvements over existing models.", "motivation": "To address the challenges faced by legal information retrieval in non-English and non-Chinese contexts, particularly in Poland, and to leverage large language models for better explainability and results.", "method": "Development of the gAIus architecture which utilizes a retrieval mechanism based on the Polish Civil Code and evaluates performance against law apprenticeship exam questions.", "result": "The gAIus architecture improves the performance of GPT-3.5 by 419%, surpassing GPT-4o and significantly enhancing its effectiveness in legal information retrieval tasks.", "conclusion": "The findings suggest promising directions for future research applications in legal contexts, emphasizing the need for explainability and improved retrieval in AI applications.", "key_contributions": ["Introduction of gAIus, a cognitive agent for legal tasks", "Significant performance improvement over existing LLMs in legal contexts", "Creation of a specialized dataset for evaluating legal information retrieval"], "limitations": "", "keywords": ["large language models", "legal information retrieval", "cognitive agents"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.01690", "pdf": "https://arxiv.org/pdf/2507.01690.pdf", "abs": "https://arxiv.org/abs/2507.01690", "title": "Designing for Community Care: Reimagining Support for Equity & Well-being in Academia", "authors": ["Beatriz Severes", "Ana O. Henriques", "Rory Clark", "Paulo Bala", "Anna Carter", "Rua Mae Williams", "Geraldine Fitzpatrick"], "categories": ["cs.HC"], "comment": null, "summary": "Academic well-being is deeply influenced by peer-support networks, yet they\nremain informal, inequitable, and unsustainable, often relying on personal\nconnections and social capital rather than structured, inclusive systems.\nAdditionally, institutional well-being responses frequently focus on student\npopulations, neglecting the emotional labour of faculty and staff, reinforcing\nan exclusionary academic culture. Drawing on HCI methodologies, participatory\ndesign, and care ethics, this workshop will provide a space for rethinking how\nacademic communities can support inclusive networks. Through pre-workshop\nengagement, co-design activities, and reflection, participants will examine\nsystemic gaps in networks and explore ways to embed care, equity, and\nsustainability into academic peer-support frameworks -- from informal,\nexclusionary models to structured, inclusive care-based ecosystems. At the end\nof the workshop, participants will co-develop design strategies for integrating\ncare and resilience in academic ecosystems, resources for designing equitable\nsupport systems, and a peer network invested and committed to fostering a\nsupportive academic community.", "AI": {"tldr": "This paper discusses the importance of formalizing peer-support networks in academic settings to enhance well-being through inclusivity and care ethics.", "motivation": "The motivation is to address the shortcomings of current peer-support networks in academic institutions, which are often informal and inequitable, impacting both students and faculty.", "method": "The approach adopted includes HCI methodologies, participatory design, and care ethics, focusing on co-design activities and reflections among participants.", "result": "Participants will identify systemic gaps in existing support networks and develop strategies to create structured, inclusive care-based ecosystems.", "conclusion": "The workshop aims to foster a supportive academic community by integrating care and resilience into peer-support frameworks.", "key_contributions": ["Rethinking peer-support networks for faculty and staff inclusion", "Developing equitable support system resources", "Co-designing strategies to embed care in academic ecosystems."], "limitations": "", "keywords": ["HCI", "peer-support networks", "care ethics", "academic well-being", "participatory design"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.01278", "pdf": "https://arxiv.org/pdf/2507.01278.pdf", "abs": "https://arxiv.org/abs/2507.01278", "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4 was evaluated for its ability to simulate clinical decisions in ophthalmology based on structured text prompts related to retinal images.", "motivation": "Exploring the utility of LLMs in ophthalmology for clinical reasoning and decision-making.", "method": "A retrospective diagnostic validation study using 300 annotated fundus images, assessing GPT-4's ability to classify and recommend referrals for diabetic retinopathy and glaucoma.", "result": "GPT-4 achieved moderate performance with an accuracy of 67.5% in ICDR classification and 82.3% in binary DR referral, but performed poorly in glaucoma referrals, showing that metadata usage did not significantly affect outcomes.", "conclusion": "LLMs like GPT-4 can assist in ophthalmic decision-making from structured prompts, but lack precision for complex tasks, indicating potential roles in education and workflows rather than direct clinical applications.", "key_contributions": ["Evaluation of LLMs in a clinical context for ophthalmology", "Insights on the performance of GPT-4 in image interpretation tasks", "Implications for the educational and documentation roles of LLMs in healthcare."], "limitations": "Lack of precision for complex ophthalmic tasks; not suitable for clinical use.", "keywords": ["large language models", "ophthalmology", "diabetic retinopathy", "glaucoma", "clinical decision-making"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.01719", "pdf": "https://arxiv.org/pdf/2507.01719.pdf", "abs": "https://arxiv.org/abs/2507.01719", "title": "Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America", "authors": ["Dorian Peters", "Fernanda Espinoza", "Marco da Re", "Guido Ivetta", "Luciana Benotti", "Rafael A. Calvo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for.", "AI": {"tldr": "This paper explores the need for culturally and linguistically responsive conversational AI in health contexts, particularly in Latin America, proposing a framework for culturally appropriate CAI.", "motivation": "To address the lack of inclusivity in current LLMs regarding diverse cultural experiences, specifically in health applications in Latin America.", "method": "The study uses qualitative data obtained through participatory workshops conducted in Latin America to understand cultural misalignments and local perspectives on health chatbots.", "result": "Findings suggest that traditional academic notions of culture are inadequate at the local level, highlighting the need for a broader framework that considers economics, politics, geography, and logistics in cultural experiences.", "conclusion": "The proposed 'Pluriversal Conversational AI for Health' framework emphasizes relationality and tolerance rather than merely increasing training data for CAI.", "key_contributions": ["Introduces a framework for culturally appropriate conversational AI for health", "Highlights the importance of local data and community perspectives", "Challenges the conventional academic views on culture in technology"], "limitations": "", "keywords": ["Conversational AI", "Cultural diversity", "Health informatics", "Latin America", "Participatory workshops"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.01281", "pdf": "https://arxiv.org/pdf/2507.01281.pdf", "abs": "https://arxiv.org/abs/2507.01281", "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "CARE-RAG improves the reliability of Retrieval-Augmented Generation (RAG) systems by addressing knowledge conflicts within generated content.", "motivation": "To enhance the reliability of RAG systems by mitigating knowledge conflicts that arise from internal inconsistencies and noisy external content.", "method": "The CARE-RAG framework employs Conflict-Driven Summarization to evaluate all available evidence, deriving parameter-aware evidence and refining retrieved content for accuracy and relevance.", "result": "CARE-RAG consistently outperforms existing RAG baselines in retrieval-based QA datasets, particularly in cases of conflicting or noisy evidence.", "conclusion": "By introducing a QA Repair step and employing a conflict-driven summarization model, CARE-RAG significantly enhances generation trustworthiness and reliability in LLM outputs.", "key_contributions": ["Introduction of CARE-RAG framework for conflict-aware evidence processing.", "Implementation of Conflict-Driven Summarization to evaluate diverse evidence.", "QA Repair step to ensure evaluation integrity of benchmark answers."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Conflict-Aware", "Evidence Summarization", "QA Repair"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.01776", "pdf": "https://arxiv.org/pdf/2507.01776.pdf", "abs": "https://arxiv.org/abs/2507.01776", "title": "Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts", "authors": ["Yuxuan Yang"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "The integration of machine learning (ML) into spatial design holds immense\npotential for optimizing space utilization, enhancing functionality, and\nstreamlining design processes. ML can automate tasks, predict performance\noutcomes, and tailor spaces to user preferences. However, the emotional,\ncultural, and aesthetic dimensions of design remain crucial for creating spaces\nthat truly resonate with users-elements that ML alone cannot address. The key\nchallenge lies in harmonizing data-driven efficiency with the nuanced,\nsubjective aspects of design. This paper proposes a human-machine collaboration\nframework to bridge this gap. An effective framework should recognize that\nwhile ML enhances design efficiency through automation and prediction, it must\nbe paired with human creativity to ensure spaces are emotionally engaging and\nculturally relevant. Human designers contribute intuition, empathy, and\ncultural insight, guiding ML-generated solutions to align with users' emotional\nand cultural needs. Additionally, we explore how various ML models can be\nintegrated with human-centered design principles. These models can automate\ndesign generation and optimization, while human designers refine the outputs to\nensure emotional resonance and aesthetic appeal. Through case studies in office\nand residential design, we illustrate how this framework fosters both\ncreativity and cultural relevance. By merging ML with human creativity, spatial\ndesign can achieve a balance of efficiency and emotional impact, resulting in\nenvironments that are both functional and deeply human.", "AI": {"tldr": "This paper proposes a human-machine collaboration framework that integrates machine learning (ML) with human creativity in spatial design to balance efficiency and emotional impact.", "motivation": "To optimize space utilization and enhance design processes while ensuring emotional, cultural, and aesthetic dimensions are preserved in design.", "method": "The paper discusses the integration of various ML models with human-centered design principles, utilizing case studies in office and residential design.", "result": "The framework demonstrates that combining ML-generated solutions with human intuition and cultural insight leads to more emotionally engaging and relevant designs.", "conclusion": "Merging ML with human creativity can achieve a balance of efficiency and emotional resonance in spatial design.", "key_contributions": ["Proposes a human-machine collaboration framework for spatial design.", "Illustrates the integration of ML models with human-centered design principles.", "Provides case studies showcasing the effectiveness of the framework."], "limitations": "", "keywords": ["Machine Learning", "Human-Machine Collaboration", "Spatial Design", "User Experience", "Cultural Relevance"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.01297", "pdf": "https://arxiv.org/pdf/2507.01297.pdf", "abs": "https://arxiv.org/abs/2507.01297", "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "categories": ["cs.CL", "cs.IR"], "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "This paper introduces CompactDS, a high-quality, web-scale datastore that enhances Retrieval-augmented Generation (RAG) capabilities, leading to significant accuracy improvements across reasoning-intensive benchmarks.", "motivation": "To improve RAG performance on reasoning-intensive benchmarks by addressing the lack of a usable web-scale datastore aligned with pretraining data.", "method": "The authors developed CompactDS, a diverse and compact datastore combining in-memory approximate nearest neighbor retrieval with on-disk exact search to optimize retrieval speed and recall.", "result": "CompactDS demonstrated substantial accuracy gains on several benchmarks, including 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH when used in a minimal RAG pipeline.", "conclusion": "The paper concludes that a diverse set of data sources is essential for optimal RAG performance, and CompactDS outperformed existing search engines while being simpler and more reproducible.", "key_contributions": ["Introduction of CompactDS, a new web-scale datastore for RAG", "Demonstration of significant accuracy improvements across multiple reasoning benchmarks", "Establishment of the importance of diverse data sources for effective retrieval"], "limitations": "", "keywords": ["Retrieval-augmented Generation", "RAG", "CompactDS", "web-scale datastore", "reasoning benchmarks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.01862", "pdf": "https://arxiv.org/pdf/2507.01862.pdf", "abs": "https://arxiv.org/abs/2507.01862", "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents", "authors": ["Sanjay Krishna Anbalagan", "Xinrui Nie", "Umesh Mohan", "Vijay Kumar Kanamarlapudi", "Anughna Kommalapati", "Xiaodan Zhao"], "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7"], "comment": "8 pages, 1 figure, pre-print of poster accepted for HCI International\n  2025 (HCII 2025), CCIS vol 2529", "summary": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency.", "AI": {"tldr": "This paper proposes integrating GUI-inspired tasks into LLM prompts for better clarity and user satisfaction in multi-step chatbot interactions.", "motivation": "To improve the clarity and effectiveness of conversational agents in handling complex multi-step interactions that are historically managed better by traditional GUIs.", "method": "Modeling tasks related to user acknowledgment ('submit-like') and context switching ('reset-like') as explicit components in the prompts of large language models.", "result": "The proposed method showed enhancements in multi-turn task coherence, increased user satisfaction, and improved efficiency in applications like hotel booking and customer management.", "conclusion": "By structuring session data around explicit user actions, the study highlights a novel approach for aligning chatbot interactions with backend processes, reducing confusion among users.", "key_contributions": ["Introduction of GUI-inspired task modeling in LLM prompts", "Demonstration of the method in real-world scenarios", "Quantitative improvements in user interaction metrics"], "limitations": "", "keywords": ["chatbot", "LLM", "user interaction", "HCI", "task modeling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01299", "pdf": "https://arxiv.org/pdf/2507.01299.pdf", "abs": "https://arxiv.org/abs/2507.01299", "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "categories": ["cs.CL"], "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA proposes a method for activation sparsification in LLMs, enhancing efficiency without extra training requirements.", "motivation": "To address the limitations of existing activation sparsity methods that either require extensive retraining or suffer from inconsistent performance due to empirical pruning.", "method": "LaRoSA uses layerwise orthogonal rotations to transform input activations and applies Top-K selection to achieve effective and stable sparsity in Large Language Models.", "result": "LaRoSA shows minimal performance degradation with a 0.17 perplexity gap and a 1.30x wall-clock speed-up at 40% sparsity in LLaMA2-7B, outperforming TEAL and CATS in specific tasks.", "conclusion": "LaRoSA effectively enhances LLM efficiency and performance without additional training, making it a practical solution for real-world applications.", "key_contributions": ["Introduction of LaRoSA for improved activation sparsity", "Utilization of layerwise orthogonal rotations for transformation", "Demonstration of significant efficiency gains without extra training"], "limitations": "", "keywords": ["Large Language Models", "Activation sparsity", "Inference speed-up"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.01944", "pdf": "https://arxiv.org/pdf/2507.01944.pdf", "abs": "https://arxiv.org/abs/2507.01944", "title": "Spatial tangible user interfaces for cognitive assessment and training", "authors": ["Ehud Sharlin", "Yuichi Itoh", "Benjamin Watson", "Yoshifumi Kitamura", "Steve Sutphen", "Lili Liu", "Fumio Kishino"], "categories": ["cs.HC"], "comment": null, "summary": "This paper discusses Tangible User Interfaces (TUIs) and their potential\nimpact on cognitive assessment and cognitive training. We believe that TUIs,\nand particularly a subset that we dub spatial TUIs, can extend human computer\ninteraction beyond some of its current limitations. Spatial TUIs exploit human\ninnate spatial and tactile ability in an intuitive and direct manner, affording\ninteraction paradigms that are practically impossible using current interface\ntechnology. As proof-of-concept we examine implementations in the field of\ncognitive assessment and training. In this paper we use Cognitive Cubes, a\nnovel TUI we developed, as an applied test bed for our beliefs, presenting\npromising experimental results for cognitive assessment of spatial ability, and\npossibly for training purposes.", "AI": {"tldr": "This paper explores Tangible User Interfaces, particularly spatial TUIs, and their applications in cognitive assessment and training, presenting results from the Cognitive Cubes implementation.", "motivation": "The paper aims to demonstrate how Tangible User Interfaces, specifically spatial TUIs, can enhance human-computer interaction and address its current limitations.", "method": "The authors developed a novel TUI known as Cognitive Cubes to evaluate its effectiveness in cognitive assessment and training, particularly focusing on spatial abilities.", "result": "The experiments conducted using Cognitive Cubes yielded promising results for assessing and potentially training spatial abilities.", "conclusion": "The study indicates that spatial TUIs can significantly impact cognitive assessment and training approaches.", "key_contributions": ["Introduction of Cognitive Cubes as a TUI for cognitive assessment and training.", "Demonstration of the capabilities of spatial TUIs over traditional interface technologies.", "Experimental evidence supporting the effectiveness of TUIs in evaluating and possibly enhancing cognitive spatial abilities."], "limitations": "", "keywords": ["Tangible User Interfaces", "Cognitive Assessment", "Cognitive Training", "Spatial Ability", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.01334", "pdf": "https://arxiv.org/pdf/2507.01334.pdf", "abs": "https://arxiv.org/abs/2507.01334", "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "This study explores the application of advanced instruction-tuned reasoning models to solve diverse physics problems, demonstrating significant improvements in accuracy and reasoning patterns.", "motivation": "The research aims to enhance the physics reasoning capabilities of Large Language Models (LLMs) through advanced instruction-tuning and problem-solving techniques.", "method": "Experimental evaluation of the Deepseek-R1 model on the SciBench benchmark, focusing on few-shot prompting and reasoning pattern analysis.", "result": "Deepseek-R1 achieves state-of-the-art accuracy in solving complex physics problems while generating distinctive reasoning patterns emphasizing symbolic derivation.", "conclusion": "The study highlights the effectiveness of advanced reasoning models and the potential for performance enhancement through few-shot prompting.", "key_contributions": ["Demonstrated state-of-the-art performance of advanced instruction-tuned reasoning models in physics problem solving.", "Generated unique reasoning patterns that emphasize symbolic derivation.", "Showed measurable accuracy improvements through few-shot prompting."], "limitations": "", "keywords": ["Large Language Models", "Physics Reasoning", "Instruction-tuned Models"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.01335", "pdf": "https://arxiv.org/pdf/2507.01335.pdf", "abs": "https://arxiv.org/abs/2507.01335", "title": "LEDOM: An Open and Fundamental Reverse Language Model", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "LEDOM is a reverse language model trained on 435B tokens, demonstrating improved outputs in mathematical reasoning tasks through reverse token prediction.", "motivation": "To introduce a novel type of language model that processes sequences in reverse and to explore its applications in enhancing generative performance.", "method": "LEDOM is an autoregressive model trained on vast amounts of data, functioning by predicting tokens in reverse temporal order.", "result": "Substantial performance improvements on mathematical reasoning tasks by leveraging reverse reasoning capabilities.", "conclusion": "LEDOM shows significant promise as a foundational model for a range of tasks and will be made publicly available for further research.", "key_contributions": ["Introduction of the first purely reverse language model", "Demonstration of improved mathematical reasoning through reverse token prediction", "All models and resources will be publicly released to aid future research."], "limitations": "", "keywords": ["reverse language model", "autoregressive model", "mathematical reasoning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.01352", "pdf": "https://arxiv.org/pdf/2507.01352.pdf", "abs": "https://arxiv.org/abs/2507.01352", "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "Presentation of Skywork-Reward-V2, a suite of eight reward models trained on a large-scale preference dataset, SynPref-40M, addressing the shortcomings of previous reward models in RLHF.", "motivation": "Current reward models in reinforcement learning from human feedback perform poorly on evaluations due to limitations in preference datasets.", "method": "Introduction of SynPref-40M, a large-scale preference dataset with 40 million pairs. A human-AI synergistic pipeline for data curation was designed, where humans provide annotations and LLMs assist in curation.", "result": "Skywork-Reward-V2 models show state-of-the-art performance across seven major benchmarks, demonstrating robustness in alignment with human preferences and other capabilities.", "conclusion": "Skywork-Reward-V2 highlights the potential of improved data curation and quality in reward modeling and its impact on RLHF.", "key_contributions": ["Introduction of a 40 million preference pair dataset, SynPref-40M.", "Development of Skywork-Reward-V2 models achieving state-of-the-art results.", "Human-AI pipeline for scalable and high-quality data curation."], "limitations": "", "keywords": ["reward models", "RLHF", "preference datasets", "human-AI synergy", "Skywork-Reward-V2"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.01437", "pdf": "https://arxiv.org/pdf/2507.01437.pdf", "abs": "https://arxiv.org/abs/2507.01437", "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "This paper presents a deep learning approach leveraging attention mechanisms for information extraction and multi-label disease prediction from electronic health record texts, utilizing a Transformer model on the MIMIC-IV dataset.", "motivation": "To address the challenges of unstructured and high-dimensional semantic complexity in electronic health record texts for improved information extraction and disease prediction.", "method": "A Transformer-based architecture utilizing multi-layer self-attention mechanisms is proposed for representation learning on clinical text, followed by a Sigmoid-based multi-label classifier.", "result": "The method outperforms existing approaches on multiple performance metrics and maintains strong generalization across varying conditions.", "conclusion": "The developed framework provides an efficient foundational algorithm for processing clinical texts, with practical significance for multi-label medical text modeling tasks.", "key_contributions": ["Introduction of a context-aware semantic alignment mechanism", "Application of multi-layer self-attention in clinical text representation", "Strong performance across various metrics compared to existing methods."], "limitations": "", "keywords": ["electronic health records", "deep learning", "multi-label disease prediction", "Transformer model", "attention mechanisms"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.01449", "pdf": "https://arxiv.org/pdf/2507.01449.pdf", "abs": "https://arxiv.org/abs/2507.01449", "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "LogitSpec is a training-free method that enhances speculative decoding in LLMs by expanding the token retrieval range, increasing inference speed and efficiency.", "motivation": "The need to speed up LLM inference and reduce the overhead of existing draft models in speculative decoding.", "method": "LogitSpec generates draft tokens by leveraging the logit of the last token to predict the next two tokens and retrieves relevant references for them in a retrieval-based manner.", "result": "LogitSpec achieved up to 2.61 times speedup and 3.28 mean accepted tokens per decoding step across various text generation benchmarks.", "conclusion": "LogitSpec provides a significant improvement in inference acceleration without the complexity of training a draft model, making it practical for integration into current LLM frameworks.", "key_contributions": ["Proposes a novel method for generating draft tokens using logit speculation.", "Eliminates the need for a training phase, making it plug-and-play.", "Demonstrates significant speed and efficiency improvements in LLM decoding."], "limitations": "", "keywords": ["speculative decoding", "large language models", "token retrieval", "inference acceleration", "LogitSpec"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.01479", "pdf": "https://arxiv.org/pdf/2507.01479.pdf", "abs": "https://arxiv.org/abs/2507.01479", "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "The paper presents a method for enhancing automatic text simplification using human feedback from individuals with intellectual disabilities to personalize learning in LLMs.", "motivation": "To improve language accessibility for individuals with intellectual disabilities through personalized text simplifications.", "method": "The authors extend standard supervised fine-tuning of LLMs by implementing direct preference optimization using feedback from users with intellectual disabilities on generated text simplifications.", "result": "Post-training LLMs exhibited effective personalization in generating text simplifications that align with user preferences, demonstrating the value of direct feedback in model training.", "conclusion": "Including the preferences of actual users with intellectual disabilities in the development of ATS systems is crucial for creating effective personalized AI solutions.", "key_contributions": ["Introduction of direct preference optimization for LLM-based ATS systems", "Development of a comprehensive pipeline for personalized LLM-based ATS", "Highlighting the importance of user feedback in AI accessibility solutions"], "limitations": "", "keywords": ["Automatic Text Simplification", "Large Language Models", "User Feedback"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.01541", "pdf": "https://arxiv.org/pdf/2507.01541.pdf", "abs": "https://arxiv.org/abs/2507.01541", "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "authors": ["Álvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "categories": ["cs.CL"], "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "This paper presents a novel framework that enhances out-of-scope intent detection in task-oriented dialogue systems by combining uncertainty modeling with fine-tuned large language models.", "motivation": "To address the challenge of out-of-scope intent detection in task-oriented dialogue systems, ensuring robustness to unseen and ambiguous queries.", "method": "The framework applies uncertainty estimation to outputs from an in-scope intent detection classifier and utilizes a fine-tuned LLM to address high-uncertainty instances for final decision-making.", "result": "The proposed method achieves state-of-the-art performance on key benchmarks, effectively balancing computational efficiency and performance.", "conclusion": "The combination of traditional methods with fine-tuned LLMs leads to improved OOS detection in real-world applications.", "key_contributions": ["Introduction of a modular framework for OOS intent detection", "Effective integration of uncertainty modeling with LLMs", "Demonstrated state-of-the-art results on OOS detection benchmarks"], "limitations": "", "keywords": ["Out-of-scope detection", "Task-oriented dialogue systems", "Large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01543", "pdf": "https://arxiv.org/pdf/2507.01543.pdf", "abs": "https://arxiv.org/abs/2507.01543", "title": "Is External Information Useful for Stance Detection with LLMs?", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "This study evaluates the impact of external information on stance detection performance across large language models, revealing that such information often degrades performance contrary to prior assumptions.", "motivation": "To investigate whether external information, such as Wikipedia and web search excerpts, can enhance stance detection performance in large language models.", "method": "The study conducts a systematic evaluation involving eight large language models across three datasets with twelve targets to assess the effect of external information on stance detection.", "result": "External information predominantly degraded stance detection performance, with macro F1 scores decreasing by up to 27.9%.", "conclusion": "The research highlights the risk of information biases in LLM-based stance classifiers, contrasting with earlier studies that suggested performance improvement through external information.", "key_contributions": ["Systematic evaluation of external information impact on LLMs for stance detection.", "Demonstration of performance degradation in LLMs when using external information.", "Insights into the alignment of LLM predictions with provided information rather than ground truth."], "limitations": "Results may vary across different contexts and datasets; the study focuses on specific models and targets.", "keywords": ["stance detection", "large language models", "external information", "performance degradation", "bias"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.01594", "pdf": "https://arxiv.org/pdf/2507.01594.pdf", "abs": "https://arxiv.org/abs/2507.01594", "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Gašić"], "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "This paper presents LUSTER, a novel LLM-based system for task-oriented dialogue that incorporates emotional understanding and reinforcement learning to improve conversational agents' effectiveness.", "motivation": "Advances in large language models have improved fluency and understanding, but effective task-oriented dialogue systems still struggle with emotional intelligence and precise communication in noisy environments.", "method": "The authors developed a task-oriented dialogue system that integrates architectural, representational, and emotional elements, using an LLM and reinforcement learning to optimize for user sentiment and task success in a challenging evaluation setup.", "result": "The study reveals that blending LLM capabilities with structured rewards enhances the resilience and emotional responsiveness of task-oriented dialogue systems.", "conclusion": "The findings suggest that LUSTER provides a promising framework for creating next-generation conversational agents that can better understand and respond to user emotions and achieve task goals efficiently.", "key_contributions": ["Development of LUSTER, an LLM-based ToD system", "Integration of emotional considerations into ToD architecture", "Use of end-to-end reinforcement learning for optimizing user sentiment and task success"], "limitations": "", "keywords": ["task-oriented dialogue", "large language models", "reinforcement learning", "emotional intelligence", "conversational agents"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2211.03324", "pdf": "https://arxiv.org/pdf/2211.03324.pdf", "abs": "https://arxiv.org/abs/2211.03324", "title": "Decoding Neural Signals: Invasive BMI Review", "authors": ["Rezwan Firuzi", "Ayub Bokani", "Jahan Hassan", "Hamed Ahmadyani", "Mohammad Foad Abdi", "Dana Naderi", "Diako Ebrahimi"], "categories": ["cs.HC"], "comment": "We have made significant changes to this article", "summary": "Human civilization has witnessed transformative technological milestones,\nfrom ancient fire lighting to the internet era. This chapter delves into the\ninvasive brain machine interface (BMI), a pioneering technology poised to be a\ndefining chapter in our progress. Beyond aiding medical conditions, invasive\nBMI promises far reaching impacts across diverse technologies and aspects of\nlife. The exploration begins by unraveling the biological and engineering\nprinciples essential for BMI implementation. The chapter comprehensively\nanalyzes potential applications, methodologies for detecting and decoding brain\nsignals, and options for stimulating signals within the human brain. It\nconcludes with a discussion on the multifaceted challenges and opportunities\nfor the continued development of invasive BMI. This chapter not only provides a\nprofound understanding of the foundational elements of invasive BMI but also\nserves as a guide through its applications, intricacies, and potential societal\nimplications. Navigating neurobiology, engineering innovations, and the\nevolving landscape of human AI symbiosis, the chapter sheds light on the\npromises and hurdles that define the future of invasive BMI.", "AI": {"tldr": "This chapter discusses invasive brain machine interfaces (BMIs), their technological principles, applications, methodologies for brain signal detection and stimulation, and the challenges and opportunities for their development.", "motivation": "To explore the transformative potential of invasive BMIs beyond medical applications and understand their implications in technology and society.", "method": "The chapter analyzes biological and engineering principles, applications of BMIs, and methodologies for detecting, decoding, and stimulating brain signals.", "result": "It provides a comprehensive overview of invasive BMIs, their foundational elements, applications, and potential societal impacts.", "conclusion": "Invasive BMIs hold significant promise and pose multifaceted challenges that must be addressed for future advancements.", "key_contributions": ["Detailed analysis of invasive BMI principles", "Exploration of diverse applications and methodologies", "Discussion on societal implications and challenges"], "limitations": "", "keywords": ["brain machine interface", "BMI", "neurobiology", "engineering", "human-AI symbiosis"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.01627", "pdf": "https://arxiv.org/pdf/2507.01627.pdf", "abs": "https://arxiv.org/abs/2507.01627", "title": "Chart Question Answering from Real-World Analytical Narratives", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "categories": ["cs.CL"], "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "This paper introduces a novel dataset for chart question answering (CQA) derived from visualization notebooks, emphasizing authentic reasoning workflows in a multimodal context.", "motivation": "The motivation behind this research is to create a dataset for chart question answering that reflects real-world scenarios and analytical narratives, addressing the shortcomings of existing benchmarks.", "method": "The dataset was constructed by compiling visualization notebooks with real multi-view charts and pairing them with natural language questions that resonate with analytical narratives.", "result": "When benchmarked with state-of-the-art multimodal large language models, it was found that GPT-4.1 achieved an accuracy of 69.3%, highlighting the difficulties posed by this dataset's more realistic setting.", "conclusion": "The findings indicate a substantial performance gap in chart question answering tasks for current models, calling for further advancements in multimodal understanding.", "key_contributions": ["Introduction of a new dataset for chart question answering based on visualization notebooks.", "Inclusion of natural language questions grounded in analytical narratives.", "Identification of performance challenges in state-of-the-art models in a more authentic CQA context."], "limitations": "", "keywords": ["Chart Question Answering", "Dataset", "Multimodal Models", "Natural Language Processing", "Visualization"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2412.16825", "pdf": "https://arxiv.org/pdf/2412.16825.pdf", "abs": "https://arxiv.org/abs/2412.16825", "title": "SoK: Usability Studies in Differential Privacy", "authors": ["Onyinye Dibia", "Prianka Bhattacharjee", "Brad Stenger", "Steven Baldasty", "Mako Bates", "Ivoline C. Ngong", "Yuanyuan Feng", "Joseph P. Near"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Differential Privacy (DP) has emerged as a pivotal approach for safeguarding\nindividual privacy in data analysis, yet its practical adoption is often\nhindered by challenges in the implementation and communication of DP. This\npaper presents a comprehensive systematization of existing research studies\naround the usability of DP, synthesizing insights from studies on both the\npractical use of DP tools and strategies for conveying DP parameters that\ndetermine privacy protection levels, such as epsilon($\\varepsilon$). By\nreviewing and analyzing these studies, we identify core usability challenges,\nbest practices, and critical gaps in current DP tools that affect adoption\nacross diverse user groups, including developers, data analysts, and\nnon-technical stakeholders. Our analysis highlights actionable insights and\npathways for future research that emphasizes user-centered design and clear\ncommunication, fostering the development of more accessible DP tools that meet\npractical needs and support broader adoption.", "AI": {"tldr": "This paper systematizes research on the usability of Differential Privacy (DP), identifying challenges and best practices for developers and non-technical stakeholders.", "motivation": "To address hurdles in the practical adoption of Differential Privacy by examining usability issues and providing actionable insights for better communication and tool design.", "method": "The authors conducted a systematic review of existing literature on the usability of Differential Privacy tools and communication strategies regarding privacy parameters.", "result": "The study uncovers critical usability challenges and gaps in current DP tools, along with best practices for improving accessibility and understanding across user groups.", "conclusion": "The paper calls for user-centered design in DP tools and better communication strategies to enhance adoption by diverse stakeholders.", "key_contributions": ["Systematization of existing usability research on Differential Privacy", "Identification of core challenges and best practices", "Proposals for future research directions emphasizing user-centered design"], "limitations": "Focuses primarily on usability without delving deeply into technical implementation aspects of Differential Privacy.", "keywords": ["Differential Privacy", "usability", "user-centered design", "privacy communication", "adoption challenges"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.01633", "pdf": "https://arxiv.org/pdf/2507.01633.pdf", "abs": "https://arxiv.org/abs/2507.01633", "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "This paper investigates the effectiveness of global scores versus pairwise comparisons for model evaluation in NLP, highlighting their respective strengths and weaknesses.", "motivation": "To improve decision-making in selecting appropriate model evaluation strategies for instruction-tuned neural language models in natural language processing.", "method": "The study involved computational experiments on both synthetic and real-world datasets, employing standard global metrics alongside the Bradley-Terry model for pairwise comparisons.", "result": "Global scores tend to offer more reliable overall rankings but may overlook strong models, while pairwise comparisons excel in distinguishing strong models in the presence of lower global scores, although they require more comparisons to reach convergence.", "conclusion": "A balanced approach to model evaluation that considers both global metrics and pairwise comparisons may yield the best insights for selecting models in NLP tasks.", "key_contributions": ["Empirical investigation of global scores vs. pairwise comparisons in NLP model evaluation", "Foundations for improved evaluation strategies for instruction-tuned neural models", "Provision of code and data for reproducibility and further research"], "limitations": "Pairwise comparisons require more comparisons to converge if there are frequent ties.", "keywords": ["NLP", "model evaluation", "pairwise comparisons", "global scores", "neural language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.01645", "pdf": "https://arxiv.org/pdf/2507.01645.pdf", "abs": "https://arxiv.org/abs/2507.01645", "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "authors": ["Rifki Afina Putri"], "categories": ["cs.CL"], "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "This paper explores the transferability of pre-trained language models for sentiment analysis in low-resource Indonesian local languages, comparing zero-shot performance and an adapter-based approach.", "motivation": "To investigate how well pre-trained language models transfer to low-resource languages, focusing on sentiment analysis across different Indonesian local languages.", "method": "The study evaluates zero-shot performance and adapter-based transfer using models like monolingual Indonesian BERT, mBERT, and XLM-R on ten local languages, categorized as seen, partially seen, and unseen.", "result": "Multilingual models perform best on seen languages, moderately on partially seen ones, and poorly on unseen languages. The MAD-X adapter significantly improves performance, particularly for seen and partially seen languages, without needing labeled data.", "conclusion": "Prior exposure of the model to a language is the strongest predictor of transfer success, while tokenization and vocabulary overlap show weak correlation with prediction quality.", "key_contributions": ["Empirical analysis of language model transferability to low-resource languages.", "Introduction of the MAD-X adapter improving performance without labeled data.", "Categorization of languages to analyze model performance disparities."], "limitations": "Does not fully explain performance differences by vocabulary overlap and tokenization.", "keywords": ["transferability", "pre-trained language models", "sentiment analysis", "low-resource languages", "MAD-X"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.01702", "pdf": "https://arxiv.org/pdf/2507.01702.pdf", "abs": "https://arxiv.org/abs/2507.01702", "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "Introducing AdamMeme, a framework for evaluating multimodal Large Language Models' understanding of harmful memes via dynamic, agent-based assessments.", "motivation": "To improve the evaluation of multimodal Large Language Models (mLLMs) in understanding harmful memes, as existing static benchmarks are inadequate.", "method": "AdamMeme employs a flexible, agent-based approach that continuously updates meme data and evaluates mLLMs through multi-agent collaboration.", "result": "Extensive experiments demonstrate that AdamMeme effectively reveals varying performances and weaknesses in target mLLMs regarding harmful meme interpretation.", "conclusion": "The AdamMeme framework allows for more comprehensive and adaptable assessments of mLLMs, helping to identify specific limitations in their reasoning about harmfulness.", "key_contributions": ["Introduction of the AdamMeme framework for assessing mLLMs on harmful memes", "Dynamic adaptation of evaluations through agent collaboration", "Fine-grained analysis of model-specific weaknesses in meme interpretation"], "limitations": "The framework may require frequent updates to keep pace with rapidly evolving meme content.", "keywords": ["multimodal", "Large Language Models", "meme harmfulness", "evaluation framework", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.01715", "pdf": "https://arxiv.org/pdf/2507.01715.pdf", "abs": "https://arxiv.org/abs/2507.01715", "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "The paper explores bias and stereotype detection in language models, introducing a dataset called StereoBias. It finds that joint training on these tasks improves detection performance significantly.", "motivation": "To address the harmful effects of bias and stereotypes in language models, especially in critical applications like content moderation and decision-making.", "method": "The authors introduce StereoBias, a dataset labeled for bias and stereotype detection across various categories and conduct experiments comparing encoder-only models and fine-tuned decoder-only models using QLoRA.", "result": "Joint training on bias and stereotype detection enhances model performance, improving bias detection significantly compared to individual training.", "conclusion": "Leveraging the relationship between bias and stereotypes can lead to the development of fairer and more effective AI systems.", "key_contributions": ["Introduction of the StereoBias dataset for bias and stereotype detection.", "Demonstration that joint training improves bias detection more than task separation.", "Experiments showing that improvements result from the connection between bias and stereotypes."], "limitations": "", "keywords": ["Bias Detection", "Stereotype Detection", "Language Models", "Fair AI", "Dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01734", "pdf": "https://arxiv.org/pdf/2507.01734.pdf", "abs": "https://arxiv.org/abs/2507.01734", "title": "LLMs for Legal Subsumption in German Employment Contracts", "authors": ["Oliver Wardas", "Florian Matthes"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "The paper explores the use of Large Language Models (LLMs) in classifying clauses of German employment contracts as valid, unfair, or void, addressing challenges in legal NLP applications.", "motivation": "Legal work presents unique challenges for NLP due to its text-heavy and resource-intensive nature. A need for interpretable and trustworthy data-driven approaches in dynamic legal environments exists.", "method": "Collaborated with legal experts to extend an existing dataset and evaluate LLMs using in-context learning across three legal contexts: no context, full legal text, and distilled examination guidelines.", "result": "Results indicate that full-text sources moderately improve performance of LLMs, and examination guidelines significantly boost recall for void clauses, achieving an 80% weighted F1-Score.", "conclusion": "While LLMs show potential in assisting legal reviews, their performance is still significantly lower than that of human lawyers, highlighting both advancements and limitations in the use of AI in legal contexts.", "key_contributions": ["Extended dataset with examination guidelines and legal annotations", "Exploration of LLMs in contract legality classification", "Comparison of performance using different legal context variants"], "limitations": "LLMs' performance remains considerably below that of human lawyers, indicating limitations in current methodologies.", "keywords": ["Natural Language Processing", "Legal AI", "Large Language Models", "Employment Law", "Contract Review"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01764", "pdf": "https://arxiv.org/pdf/2507.01764.pdf", "abs": "https://arxiv.org/abs/2507.01764", "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "authors": ["Matteo Di Cristofaro"], "categories": ["cs.CL"], "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "This paper explores the impact of tokenisation discrepancies on language data representation and analysis, particularly with challenges involving emojis and homoglyphs.", "motivation": "To understand how discrepancies in tokenisation affect linguistic data representation and the validity of analytical findings in corpus linguistics.", "method": "The study examines tokenisation issues, focusing on emojis and homoglyphs, and presents preprocessing methods for accurate digital text representation in corpora.", "result": "The research highlights the importance of careful tokenisation to maintain the fidelity of corpus data and emphasizes the necessity for both linguistic and technical understanding to improve corpus analysis accuracy.", "conclusion": "Ensuring accurate tokenisation is critical for reliable linguistic analysis, with implications for both quantitative and qualitative research approaches.", "key_contributions": ["Illuminates how tokenisation discrepancies affect linguistic representation", "Provides methods for preprocessing emojis and homoglyphs", "Highlights the need for linguistic and technical understanding in corpus analysis"], "limitations": "", "keywords": ["tokenisation", "corpus linguistics", "data representation", "linguistic analysis", "corpus fidelity"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2507.01785", "pdf": "https://arxiv.org/pdf/2507.01785.pdf", "abs": "https://arxiv.org/abs/2507.01785", "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "MuRating is a framework that enhances data quality in multilingual large language models by transferring high-quality English data signals to evaluate document quality across 17 languages.", "motivation": "To improve the performance of large language models by addressing data quality issues in multilingual contexts, particularly when existing methods primarily focus on English.", "method": "MuRating aggregates multiple English 'raters' through pairwise comparisons to create unified document-quality scores, which are then projected through translation to train a multilingual evaluator.", "result": "MuRating selects balanced subsets of English and multilingual content for pretraining a LLaMA model, significantly boosting accuracy on both English and multilingual benchmarks, particularly on knowledge-intensive tasks.", "conclusion": "The framework not only improves performance across multiple languages but also highlights issues such as translation fidelity and selection bias, paving the way for future research.", "key_contributions": ["Introduction of MuRating framework for multilingual document evaluation", "Demonstrated performance improvement of LLaMA model through balanced data selection", "Analysis of translation fidelity and selection biases in multilingual training."], "limitations": "Focuses on the performance related to selected benchmarks; potential biases in the training data.", "keywords": ["data quality", "large language models", "multilingual", "MuRating", "document evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01786", "pdf": "https://arxiv.org/pdf/2507.01786.pdf", "abs": "https://arxiv.org/abs/2507.01786", "title": "Probing Evaluation Awareness of Language Models", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofstätter"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "This paper investigates evaluation awareness in Llama-3.3-70B-Instruct, demonstrating its ability to distinguish between testing and deployment phases which has implications for AI governance and safety evaluations.", "motivation": "Understanding evaluation awareness is critical for AI governance as it affects the reliability of safety evaluations and industry commitments.", "method": "The study employs linear probes to analyze Llama-3.3-70B-Instruct's ability to separate prompts from evaluation and deployment phases.", "result": "Findings reveal that the model successfully distinguishes between real-world evaluation and deployment prompts, with safety evaluations being classified correctly, indicating that they may seem artificial to the model.", "conclusion": "The research highlights the importance of trustworthy evaluations and suggests exploring model internals to enhance blackbox methods for safety audits.", "key_contributions": ["Demonstrates evaluation awareness in Llama-3.3-70B-Instruct.", "Reveals implications for AI governance and safety evaluations.", "Suggests leveraging model internals for safety audits."], "limitations": "Limited to the analysis of a specific model (Llama-3.3-70B-Instruct) and its current evaluation methods.", "keywords": ["evaluation awareness", "AI governance", "safety evaluations", "Llama-3", "deception"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.01790", "pdf": "https://arxiv.org/pdf/2507.01790.pdf", "abs": "https://arxiv.org/abs/2507.01790", "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "This paper investigates the behavior of vision-language models when faced with conflicting information from different input modalities, revealing biases towards certain modalities and suggesting methods for improved model performance.", "motivation": "To understand how multimodal AI models handle conflicting input streams, specifically focusing on vision-language models.", "method": "The paper tests models with inconsistent inputs (e.g., images and captions that do not match) and analyzes how they report information from different modalities, observing biases and evaluating internal representations and attention mechanisms.", "result": "Models tend to favor one input modality over the other when reporting information, with variations observed across different models. The study identifies specific attention heads and modality-agnostic router heads that can enhance model performance by restructuring representations and improving handling of conflicting signals.", "conclusion": "The research contributes to a deeper understanding of multimodal model behavior in the presence of conflicting information, paving the way for future improvements in AI systems that work with complex multimodal data.", "key_contributions": ["Identified biases in modality preference among vision-language models when faced with conflicting information.", "Revealed internal representation structures and specific attention heads that favor one modality.", "Proposed transferability of modality-agnostic router heads to enhance model performance across datasets."], "limitations": "The study is limited to vision-language models and may not generalize to other multimodal systems outside this scope.", "keywords": ["multimodal AI", "vision-language models", "conflicting information", "internal representations", "attention mechanisms"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.01802", "pdf": "https://arxiv.org/pdf/2507.01802.pdf", "abs": "https://arxiv.org/abs/2507.01802", "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan Rüping"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "This paper analyzes the MDACE dataset to evaluate explainable medical coding systems, revealing alignment between ground truth evidence and code descriptions, and offers recommendations for system development.", "motivation": "To improve automatic medical coding processes by enhancing transparency and explainability through the evaluation of existing systems.", "method": "An in-depth analysis of the MDACE dataset combined with plausibility evaluations of explainable medical coding systems.", "result": "Findings show a significant alignment between ground truth evidence and code descriptions, indicating a viable method for assessing medical coding systems.", "conclusion": "The study provides insights into automatic medical coding and emphasizes the need for improved explainability methods, offering recommendations for future development.", "key_contributions": ["In-depth analysis of the MDACE dataset.", "Evaluation of existing explainable medical coding systems.", "Recommendations for enhancing automatic medical coding processes."], "limitations": "Limited to the evaluation of existing systems and datasets; potential generalization issues beyond the MDACE dataset.", "keywords": ["medical coding", "explainable AI", "dataset analysis", "health informatics", "automatic documentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01810", "pdf": "https://arxiv.org/pdf/2507.01810.pdf", "abs": "https://arxiv.org/abs/2507.01810", "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "categories": ["cs.CL", "cs.IR"], "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "This paper compares the parseability of JSON, YAML, and XML outputs from small language models for extracting attributes from clinical notes, finding JSON most effective.", "motivation": "To identify the most effective serialization format for open attribute-value extraction in clinical settings using language models.", "method": "A comparative analysis of parseability focused on three serialization formats (JSON, YAML, XML) using small language models.", "result": "JSON consistently exhibits the highest parseability, with improvements seen through targeted prompting, but challenges arise with longer documents and specific note types.", "conclusion": "The study provides practical guidance for selecting serialization formats and designing prompts for language models in clinical contexts.", "key_contributions": ["Identification of JSON as the most effective serialization format for clinical note extraction.", "Analysis of parseability affected by document length and note types.", "Insights on error patterns related to specific formats."], "limitations": "Focused primarily on structured output parseability; other factors in real-world deployment remain unaddressed.", "keywords": ["parseability", "JSON", "clinical notes", "language models", "attribute-value extraction"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.01844", "pdf": "https://arxiv.org/pdf/2507.01844.pdf", "abs": "https://arxiv.org/abs/2507.01844", "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "The paper presents a systematic method to analyze how training data influences the outputs of Large Language Models (LLMs), focusing on low-perplexity text spans.", "motivation": "To enhance transparency, accountability, privacy, and fairness in LLM outputs by understanding the impact of their training data.", "method": "A systematic approach is introduced that analyzes low-perplexity sequences generated by LLMs, tracing them back to their original sources in the training data.", "result": "Many low-perplexity spans cannot be directly mapped to the training corpus. For those that can, the study quantifies their occurrence in the source documents, revealing insights into verbatim recall.", "conclusion": "The findings pave the way for a deeper understanding of LLM behavior influenced by training data.", "key_contributions": ["Introduces a systematic analysis of low-perplexity sequences in LLMs.", "Quantifies the distribution of training data recall in LLM outputs.", "Highlights the limitations of mapping generated text back to training sources."], "limitations": "The study primarily focuses on low-perplexity spans, which may not represent all aspects of LLM outputs.", "keywords": ["Large Language Models", "transparency", "training data", "fairness", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.01853", "pdf": "https://arxiv.org/pdf/2507.01853.pdf", "abs": "https://arxiv.org/abs/2507.01853", "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "EKA-EVAL is a comprehensive evaluation framework for multilingual and Indic-specific benchmarks for Large Language Models.", "motivation": "To address the need for evaluation frameworks for linguistically diverse areas like India, surpassing English-centric benchmarks.", "method": "EKA-EVAL integrates over 35 benchmarks, including 10 Indic-specific datasets, to cover various evaluation categories such as reasoning and reading comprehension, with support for distributed inference and multi-GPU usage.", "result": "EKA-EVAL provides broader benchmark coverage compared to existing tools and is the first extensible evaluation suite for both global and Indic LLMs.", "conclusion": "The framework is open-source and aims to establish a multilingual evaluation ecosystem for LLMs, with planned expansion to over 100 benchmarks.", "key_contributions": ["First end-to-end evaluation suite for Indic and global LLMs.", "Integration of multiple benchmarks including Indic-specific datasets.", "Support for advanced features like distributed inference and multi-GPU usage."], "limitations": "", "keywords": ["Large Language Models", "evaluation framework", "Indic languages", "multilingual benchmarking", "open-source"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01872", "pdf": "https://arxiv.org/pdf/2507.01872.pdf", "abs": "https://arxiv.org/abs/2507.01872", "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "DIY-MKG is an open-source tool for multilingual language learning that enables users to create personalized vocabulary knowledge graphs and generates adaptive quizzes using LLMs.", "motivation": "To overcome limitations in existing language learning tools, particularly for polyglot learners, including lack of cross-linguistic connections, limited customization, and problems with cognitive offloading.", "method": "DIY-MKG allows users to construct personalized vocabulary graphs by expanding with related words suggested by an LLM, coupled with rich annotation features and adaptive quizzes generated by the system.", "result": "The evaluation of DIY-MKG demonstrated reliable vocabulary expansion across languages and high accuracy in quiz generation, confirming the system's effectiveness.", "conclusion": "DIY-MKG significantly enhances the language learning experience for polyglot users by providing the tools necessary for customized learning and active engagement.", "key_contributions": ["Open-source design enabling personalized vocabulary knowledge graphs.", "Integration of LLM for dynamic quiz generation tailored to individual learning needs.", "Feedback loop for quiz question refinement through user engagement."], "limitations": "", "keywords": ["multilingual learning", "language models", "adaptive learning", "vocabulary expansion", "knowledge graphs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01887", "pdf": "https://arxiv.org/pdf/2507.01887.pdf", "abs": "https://arxiv.org/abs/2507.01887", "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "Introducing MiCoTAl, a framework that improves long-form Chain of Thought (CoT) distillation for small language models (SLMs) using intermediate-sized models as teacher assistants.", "motivation": "To address the limitations of small language models in learning long-form reasoning due to their restricted capacity and high computational demands of large models.", "method": "The MiCoTA framework employs intermediate-sized models as teacher assistants and utilizes intermediate-length CoT sequences to enhance the learning process of SLMs.", "result": "SLMs distilled from large teachers showed poor performance, but using MiCoTA, they achieved significant improvements in reasoning performance on various benchmarks, with scores improving by 3.47 and 3.93 for specific models.", "conclusion": "MiCoTA enables SLMs to improve reasoning capabilities, providing a new avenue for effective long-CoT data distillation in language models.", "key_contributions": ["Introduction of MiCoTAl framework for CoT distillation.", "Use of intermediate-sized models as teacher assistants.", "Demonstrated significant performance enhancements on reasoning tasks."], "limitations": "The work is still in progress and may require further validation and testing.", "keywords": ["long language models", "small language models", "reasoning tasks", "distillation", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.01900", "pdf": "https://arxiv.org/pdf/2507.01900.pdf", "abs": "https://arxiv.org/abs/2507.01900", "title": "High-Layer Attention Pruning with Rescaling", "authors": ["Songtao Liu", "Peng Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "This paper presents a novel pruning algorithm for large language models that strategically prunes attention heads in higher layers and introduces an adaptive rescaling parameter to maintain representation scale, leading to improved performance over existing methods.", "motivation": "To improve the effectiveness of pruning large language models by addressing the limitations of conventional training-free structured pruning methods that do not consider the architectural positioning of attention heads.", "method": "A novel pruning algorithm that selectively prunes attention heads from higher layers of LLMs, along with an adaptive rescaling parameter to adjust token representations post-pruning.", "result": "The proposed method outperforms traditional structured pruning techniques across various large language models in both generation and discriminative tasks, particularly excelling in generation tasks.", "conclusion": "The new approach to pruning and representation rescaling enables significant performance improvements in LLMs, indicating that attention head positioning and post-pruning adjustments are crucial for effective model compression.", "key_contributions": ["Introduction of a strategic pruning algorithm targeting higher layers of LLMs", "Adaptive rescaling parameter to maintain token representation scale after pruning", "Demonstrated performance improvements across multiple LLMs and datasets."], "limitations": "", "keywords": ["pruning", "large language models", "attention heads", "adaptive rescaling", "model compression"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.01903", "pdf": "https://arxiv.org/pdf/2507.01903.pdf", "abs": "https://arxiv.org/abs/2507.01903", "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "A comprehensive survey on the application of AI in scientific research, addressing existing gaps and providing a systematic taxonomy for AI4Research.", "motivation": "To provide a unified understanding and survey of AI applications in the research process due to recent advancements in large language models.", "method": "Introduce a systematic taxonomy for classifying five mainstream tasks in AI4Research, identify research gaps and future directions, and compile various resources and applications.", "result": "The survey categorizes tasks in AI4Research, highlights research gaps, and provides an extensive list of resources and applications.", "conclusion": "The work aims to stimulate innovative breakthroughs in AI4Research by providing the research community with better access to resources.", "key_contributions": ["Systematic taxonomy for classifying tasks in AI4Research", "Identification of key research gaps and future directions", "Compilation of multidisciplinary applications and resources"], "limitations": "", "keywords": ["AI for Research", "Large Language Models", "Taxonomy", "Research Gaps", "Applications"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.01915", "pdf": "https://arxiv.org/pdf/2507.01915.pdf", "abs": "https://arxiv.org/abs/2507.01915", "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "This paper presents a novel approach, Gradient-Adaptive Policy Optimization (GAPO), for aligning large language models with human preferences by framing the problem as multi-objective optimization, showing superior results in both helpfulness and harmlessness.", "motivation": "The challenge of effectively aligning LLMs with diverse human preferences, especially when those preferences conflict.", "method": "Introduction of Gradient-Adaptive Policy Optimization (GAPO), which employs multiple-gradient descent to optimally balance trade-offs between conflicting human values in LLM alignment.", "result": "GAPO outperforms current state-of-the-art methods in aligning LLMs, showing improved performance on metrics of helpfulness and harmlessness on the Mistral-7B model.", "conclusion": "GAPO converges towards a Pareto optimal solution, offering a promising approach for addressing the multi-objective nature of human value alignment in LLMs.", "key_contributions": ["Introduction of GAPO for human preference alignment in LLMs.", "Development of P-GAPO that considers user preferences across multiple objectives.", "Demonstration of superior performance compared to existing methods."], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Large Language Models", "Multi-objective Optimization", "Gradient Descent"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.01921", "pdf": "https://arxiv.org/pdf/2507.01921.pdf", "abs": "https://arxiv.org/abs/2507.01921", "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "The paper investigates the effectiveness of distilling reasoning traces from a teacher model to improve a student model's reasoning capabilities, demonstrating that carefully selected, challenging examples are more effective than random sampling.", "motivation": "To understand how reasoning demonstrations from a teacher model can be systematically used to boost the reasoning skills of a smaller student model.", "method": "A systematic analysis of distilling reasoning capabilities is conducted, involving the selection of high-quality reasoning traces ('NaturalThoughts') from a teacher model and evaluating their impact on student models.", "result": "Training with 'NaturalThoughts' significantly outperforms traditional reasoning datasets on various STEM reasoning benchmarks, demonstrating improved sample efficiency.", "conclusion": "Selecting challenging examples that require diverse reasoning strategies is a more effective approach for transferring reasoning skills from larger teacher models to smaller student models.", "key_contributions": ["Curated high-quality reasoning traces ('NaturalThoughts') for model training.", "Systematic analysis of sample efficiency and scalability in reasoning task training.", "Demonstrated superior performance of 'NaturalThoughts' compared to existing datasets on STEM benchmarks."], "limitations": "", "keywords": ["natural language processing", "reasoning traces", "machine learning", "student model", "teacher model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01923", "pdf": "https://arxiv.org/pdf/2507.01923.pdf", "abs": "https://arxiv.org/abs/2507.01923", "title": "Decision-oriented Text Evaluation", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "This paper introduces a decision-oriented framework for evaluating natural language generation in high-stakes domains by measuring its influence on human and LLM decision outcomes, finding the importance of analytical commentary in enhancing decision-making.", "motivation": "To address the limitations of conventional intrinsic evaluation methods in assessing the efficacy of generated text in decision-making contexts.", "method": "The authors employ market digest texts to evaluate decision-making quality based on financial performance of trades executed by human investors and LLM agents informed by these texts.", "result": "Neither humans nor LLMs outperform random performance using summaries alone, but richer analytical commentaries lead to significant improvements in collaborative decision-making outcomes.", "conclusion": "The study emphasizes that traditional metrics for text generation are insufficient and advocates for evaluating text based on its facilitation of decision-making between humans and LLMs.", "key_contributions": ["Proposes a novel decision-oriented evaluation framework for NLG based on decision outcomes.", "Demonstrates that analytical commentaries can enhance the performance of human-LLM teams compared to using plain summaries.", "Highlights the limitations of traditional evaluation methods in high-stakes domains."], "limitations": "The performance impacts were constrained by reliance solely on specific market digest text types and may not generalize to all high-stakes domains.", "keywords": ["natural language generation", "decision-making", "human-LM collaboration", "evaluation framework", "financial text"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.01931", "pdf": "https://arxiv.org/pdf/2507.01931.pdf", "abs": "https://arxiv.org/abs/2507.01931", "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "This study compares the performance of OpenAI's Whisper and Facebook's Wav2Vec-BERT ASR models on Bangla, a low-resource language, finding Wav2Vec-BERT superior in efficiency and accuracy.", "motivation": "To evaluate and improve Automatic Speech Recognition systems for low-resource languages by comparing state-of-the-art models.", "method": "Experiments were conducted using Mozilla Common Voice-17 and OpenSLR datasets, focusing on fine-tuning and hyperparameter optimization to assess model performance based on WER, CER, training time, and computational efficiency.", "result": "Wav2Vec-BERT outperformed Whisper in all key metrics tested, including lower WER and CER, faster training times, and better computational efficiency.", "conclusion": "The study suggests that Wav2Vec-BERT is more suitable for developing efficient speech recognition systems for low-resource languages like Bangla.", "key_contributions": ["Comparison of two advanced ASR models on low-resource language", "Evidence of Wav2Vec-BERT's superiority in efficiency and performance", "Insights into ASR model optimization for low-resource languages"], "limitations": "", "keywords": ["Automatic Speech Recognition", "low-resource languages", "Wav2Vec-BERT", "Whisper", "performance comparison"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.01936", "pdf": "https://arxiv.org/pdf/2507.01936.pdf", "abs": "https://arxiv.org/abs/2507.01936", "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "authors": ["Adrian de Wynter", "Tangming Yuan"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "This paper evaluates large language models' (LLMs) ability to maintain persuasive debates and their comprehension of dialogical structures. While LLMs can effectively sway beliefs in dialogues, they lack true understanding of context and deeper dialogue structures, leading to limitations in their use as evaluators.", "motivation": "As LLMs are increasingly deployed in critical areas such as chatbots and mental health, their reasoning capabilities and understanding of dialogue need rigorous examination to ensure their effectiveness and reliability.", "method": "The study evaluates the ability of LLMs to maintain debates and measures this against their comprehension of dialogue structures and pragmatic context.", "result": "LLMs can maintain coherent and persuasive debates, swaying participants' beliefs, but when tested for deeper comprehension of dialogue, they fail to demonstrate understanding, revealing limitations in their use as evaluators.", "conclusion": "Effective dialogue performance does not require true understanding, suggesting that pragmatic context and coherence are secondary to the persuasive effectiveness of LLMs.", "key_contributions": ["Evaluated the debate-maintaining abilities of LLMs.", "Demonstrated the disconnect between effective dialogue and understanding of context.", "Highlighted implications for argumentation theory regarding LLMs as evaluators."], "limitations": "LLMs' understanding of the context in dialogues is lacking, which affects their evaluation capabilities.", "keywords": ["large language models", "dialogue comprehension", "persuasive debates", "argumentation theory", "AI evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548.pdf", "abs": "https://arxiv.org/abs/2507.01548", "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.", "AI": {"tldr": "The paper investigates AI-assisted co-creation techniques for older adults to express complex personal narratives through a workshop that combines storytelling and glyph reconstruction.", "motivation": "To enable aging migrants in urban China to express fragmented personal narratives using AI as a supportive tool rather than a content creator.", "method": "A pilot workshop where participants engaged in oral storytelling and reconstructed Hanzi characters using a Large Language Model, combined with physical materials, with human facilitation.", "result": "Participants successfully transformed their lived experiences into visual and tactile forms without the need for digital literacy, showcasing effective human-AI collaboration.", "conclusion": "The approach redefines AI's role in narrative expression for older adults, emphasizing supportive interaction within sociotechnical contexts.", "key_contributions": ["Introduced a novel workshop format for storytelling using AI and traditional methods.", "Demonstrated the feasibility of narrative engagement without digital literacy.", "Shifted the perspective of AI from content generation to support in narrative agency."], "limitations": "Limited to a specific demographic (aging migrants in urban China) and may not be generalizable.", "keywords": ["AI-assisted co-creation", "aging migrants", "narrative agency", "Hanzi reconstruction", "human-AI collaboration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2404.16369", "pdf": "https://arxiv.org/pdf/2404.16369.pdf", "abs": "https://arxiv.org/abs/2404.16369", "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal", "authors": ["Yukai Zhou", "Jian Lou", "Zhijie Huang", "Zhan Qin", "Yibei Yang", "Wenjie Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Ensuring the safety alignment of Large Language Models (LLMs) is critical for\ngenerating responses consistent with human values. However, LLMs remain\nvulnerable to jailbreaking attacks, where carefully crafted prompts manipulate\nthem into producing toxic content. One category of such attacks reformulates\nthe task as an optimization problem, aiming to elicit affirmative responses\nfrom the LLM. However, these methods heavily rely on predefined objectionable\nbehaviors, limiting their effectiveness and adaptability to diverse harmful\nqueries. In this study, we first identify why the vanilla target loss is\nsuboptimal and then propose enhancements to the loss objective. We introduce\nDSN (Don't Say No) attack, which combines a cosine decay schedule method with\nrefusal suppression to achieve higher success rates. Extensive experiments\ndemonstrate that DSN outperforms baseline attacks and achieves state-of-the-art\nattack success rates (ASR). DSN also shows strong universality and\ntransferability to unseen datasets and black-box models.", "AI": {"tldr": "This paper introduces the DSN attack, a novel method designed to enhance the safety alignment of Large Language Models (LLMs) by preventing the generation of toxic content, achieving state-of-the-art success rates in evading safety measures.", "motivation": "To improve the safety alignment of LLMs against jailbreaking attacks and ensure they generate responses consistent with human values.", "method": "The study critiques traditional approaches to jailbreaking and proposes a new attack method named DSN, which integrates a cosine decay schedule with refusal suppression strategies to better manipulate LLM responses.", "result": "DSN outperforms existing methods in success rates and shows strong adaptability to various datasets and models.", "conclusion": "The proposed DSN approach enhances LLM response safety and offers significant improvements over traditional jailbreaking attacks, demonstrating high universality and effectiveness.", "key_contributions": ["Introduction of the DSN attack methodology", "Demonstration of state-of-the-art attack success rates", "Enhanced adaptability to unseen datasets and black-box models."], "limitations": "", "keywords": ["Large Language Models", "safety alignment", "jailbreaking attacks", "DSN attack", "machine learning"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2405.13012", "pdf": "https://arxiv.org/pdf/2405.13012.pdf", "abs": "https://arxiv.org/abs/2405.13012", "title": "Divergent Creativity in Humans and Large Language Models", "authors": ["Antoine Bellemare-Pepin", "François Lespinasse", "Philipp Thölke", "Yann Harel", "Kory Mathewson", "Jay A. Olson", "Yoshua Bengio", "Karim Jerbi"], "categories": ["cs.CL", "cs.AI"], "comment": "First two and last listed authors are corresponding authors. The\n  first two listed authors contributed equally to this work", "summary": "The recent surge of Large Language Models (LLMs) has led to claims that they\nare approaching a level of creativity akin to human capabilities. This idea has\nsparked a blend of excitement and apprehension. However, a critical piece that\nhas been missing in this discourse is a systematic evaluation of LLMs' semantic\ndiversity, particularly in comparison to human divergent thinking. To bridge\nthis gap, we leverage recent advances in computational creativity to analyze\nsemantic divergence in both state-of-the-art LLMs and a substantial dataset of\n100,000 humans. We found evidence that LLMs can surpass average human\nperformance on the Divergent Association Task, and approach human creative\nwriting abilities, though they fall short of the typical performance of highly\ncreative humans. Notably, even the top performing LLMs are still largely\nsurpassed by highly creative individuals, underscoring a ceiling that current\nLLMs still fail to surpass. Our human-machine benchmarking framework addresses\nthe polemic surrounding the imminent replacement of human creative labour by\nAI, disentangling the quality of the respective creative linguistic outputs\nusing established objective measures. While prompting deeper exploration of the\ndistinctive elements of human inventive thought compared to those of AI\nsystems, we lay out a series of techniques to improve their outputs with\nrespect to semantic diversity, such as prompt design and hyper-parameter\ntuning.", "AI": {"tldr": "This paper evaluates the semantic diversity of Large Language Models (LLMs) compared to human divergent thinking, revealing that LLMs can outperform average humans but not highly creative individuals.", "motivation": "To systematically evaluate LLMs' semantic diversity and compare it to human divergent thinking, addressing misconceptions about AI and creativity.", "method": "Analyzed semantic divergence in LLMs and a dataset of 100,000 humans using the Divergent Association Task and established objective measures.", "result": "LLMs can surpass average human performance in semantic diversity but do not reach the level of highly creative individuals, indicating limitations in their creative abilities.", "conclusion": "While LLMs show promise in creative tasks, they do not replace human creative labor due to inherent limitations in their outputs compared to highly creative humans. Techniques for enhancing LLM outputs are suggested.", "key_contributions": ["Systematic evaluation framework for LLMs' semantic diversity", "Benchmarking of LLMs against human creativity", "Techniques for improving LLM outputs in terms of semantic diversity"], "limitations": "Current LLMs still cannot surpass the typical performance of highly creative humans.", "keywords": ["Large Language Models", "semantic diversity", "human creativity", "computational creativity", "divergent thinking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.02544", "pdf": "https://arxiv.org/pdf/2408.02544.pdf", "abs": "https://arxiv.org/abs/2408.02544", "title": "Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions", "authors": ["Xinbei Ma", "Yiting Wang", "Yao Yao", "Tongxin Yuan", "Aston Zhang", "Zhuosheng Zhang", "Hai Zhao"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic.", "AI": {"tldr": "This paper explores the distraction susceptibility of multimodal large language model agents in GUI environments and proposes methods to enhance their faithfulness.", "motivation": "To investigate the faithfulness of multimodal large language model agents in GUI settings and understand their susceptibility to distractions from the environment.", "method": "Evaluated a variety of multimodal large language models as GUI agents using a simulated dataset, with experiments designed around three different working patterns that exhibit varying levels of perception.", "result": "Experimental results demonstrate that all evaluated models, regardless of being generalist or specialist GUI agents, show a tendency to be distracted by unrelated environmental content.", "conclusion": "The findings highlight the need for further research on improving the distraction resilience of multimodal agents and also suggest exploring adversarial environment injection methods to enhance agent faithfulness.", "key_contributions": ["Assessment of MLLM agents' distraction susceptibility in a GUI context", "Introduction of adversarial environment injection for improving agent performance", "First study indicating the distraction risk of these agents from benign but irrelevant environmental content."], "limitations": "The scope is limited to benign environments; further studies needed in various contextual setups.", "keywords": ["Human-Computer Interaction", "Multimodal Large Language Models", "User Interface Agents", "Distraction", "Adversarial Environments"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.13514", "pdf": "https://arxiv.org/pdf/2409.13514.pdf", "abs": "https://arxiv.org/abs/2409.13514", "title": "Unifying Global and Near-Context Biasing in a Single Trie Pass", "authors": ["Iuliia Thorbecke", "Esaú Villatoro-Tello", "Juan Zuluaga-Gomez", "Shashi Kumar", "Sergio Burdisso", "Pradeep Rangappa", "Andrés Carofilis", "Srikanth Madikeri", "Petr Motlicek", "Karthik Pandia", "Kadri Hacioğlu", "Andreas Stolcke"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to TSD2025", "summary": "Despite the success of end-to-end automatic speech recognition (ASR) models,\nchallenges persist in recognizing rare, out-of-vocabulary words - including\nnamed entities (NE) - and in adapting to new domains using only text data. This\nwork presents a practical approach to address these challenges through an\nunexplored combination of an NE bias list and a word-level n-gram language\nmodel (LM). This solution balances simplicity and effectiveness, improving\nentities' recognition while maintaining or even enhancing overall ASR\nperformance. We efficiently integrate this enriched biasing method into a\ntransducer-based ASR system, enabling context adaptation with almost no\ncomputational overhead. We present our results on three datasets spanning four\nlanguages and compare them to state-of-the-art biasing strategies. We\ndemonstrate that the proposed combination of keyword biasing and n-gram LM\nimproves entity recognition by up to 32% relative and reduces overall WER by up\nto a 12% relative.", "AI": {"tldr": "This paper investigates challenges in automatic speech recognition (ASR) for recognizing rare words and adapting to new domains. It proposes a novel method combining a named entity bias list and an n-gram language model to enhance ASR performance.", "motivation": "To address the difficulties in recognizing rare, out-of-vocabulary words, particularly named entities, and to improve adaptability in ASR systems with minimal computational costs.", "method": "The authors introduce a practical approach that unites a named entity bias list with a word-level n-gram language model, integrating this method into a transducer-based ASR system.", "result": "The proposed method improves entity recognition by up to 32% relative and reduces overall word error rate (WER) by up to 12% relative, tested across three datasets in four languages.", "conclusion": "This approach balances simplicity and effectiveness, leading to better ASR performance and entity recognition.", "key_contributions": ["Combines named entity biasing with n-gram language model", "Demonstrates significant improvements in entity recognition", "Integrates efficiently into existing ASR systems with little overhead"], "limitations": "", "keywords": ["automatic speech recognition", "named entities", "n-gram language model", "biasing strategies", "context adaptation"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2410.06716", "pdf": "https://arxiv.org/pdf/2410.06716.pdf", "abs": "https://arxiv.org/abs/2410.06716", "title": "Guaranteed Generation from Large Language Models", "authors": ["Minbeom Kim", "Thibaut Thonet", "Jos Rozen", "Hwaran Lee", "Kyomin Jung", "Marc Dymetman"], "categories": ["cs.CL"], "comment": "ICLR 2025", "summary": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.", "AI": {"tldr": "The paper presents GUARD, a method that ensures constraint satisfaction in text generation from large language models while maintaining the original distribution.", "motivation": "With the increasing use of large language models (LLMs), there is a need for mechanisms to control text generation to adhere to specific constraints without losing the quality of the output.", "method": "GUARD combines training-time approaches with inference-time rejection sampling to enforce constraints while controlling the KL divergence between proposal and ideal distributions.", "result": "GUARD achieves perfect constraint satisfaction in text generation scenarios with minimal impact on the quality of the original output distribution and significantly improves inference efficiency.", "conclusion": "GUARD offers a viable solution to the challenge of generating text that meets strict constraints, demonstrating effective constraint enforcement without sacrificing generative performance.", "key_contributions": ["Introduction of GUARD for guaranteed text generation with constraints", "Theoretical framework linking KL divergence control to inference efficiency", "Extensive experimentation validating GUARD's performance in challenging scenarios"], "limitations": "", "keywords": ["large language models", "constraint satisfaction", "text generation", "GUARD", "inference efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.05563", "pdf": "https://arxiv.org/pdf/2412.05563.pdf", "abs": "https://arxiv.org/abs/2412.05563", "title": "A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions", "authors": ["Ola Shorinwa", "Zhiting Mei", "Justin Lidard", "Allen Z. Ren", "Anirudha Majumdar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The remarkable performance of large language models (LLMs) in content\ngeneration, coding, and common-sense reasoning has spurred widespread\nintegration into many facets of society. However, integration of LLMs raises\nvalid questions on their reliability and trustworthiness, given their\npropensity to generate hallucinations: plausible, factually-incorrect\nresponses, which are expressed with striking confidence. Previous work has\nshown that hallucinations and other non-factual responses generated by LLMs can\nbe detected by examining the uncertainty of the LLM in its response to the\npertinent prompt, driving significant research efforts devoted to quantifying\nthe uncertainty of LLMs. This survey seeks to provide an extensive review of\nexisting uncertainty quantification methods for LLMs, identifying their salient\nfeatures, along with their strengths and weaknesses. We present existing\nmethods within a relevant taxonomy, unifying ostensibly disparate methods to\naid understanding of the state of the art. Furthermore, we highlight\napplications of uncertainty quantification methods for LLMs, spanning chatbot\nand textual applications to embodied artificial intelligence applications in\nrobotics. We conclude with open research challenges in uncertainty\nquantification of LLMs, seeking to motivate future research.", "AI": {"tldr": "This survey reviews uncertainty quantification methods for large language models (LLMs), focusing on their detection of hallucinations and applications in various AI fields.", "motivation": "With the integration of LLMs into society, there are concerns about their reliability due to the generation of factually incorrect responses, known as hallucinations.", "method": "The paper reviews existing methods of uncertainty quantification for LLMs and categorizes them into a unified taxonomy to enhance understanding.", "result": "The survey identifies strengths and weaknesses of current uncertainty quantification methods, and discusses their applications in chatbots, text applications, and robotics.", "conclusion": "The study concludes with open challenges in LLM uncertainty quantification, encouraging further research.", "key_contributions": ["Extensive review of uncertainty quantification methods for LLMs", "Taxonomy of methods to aid understanding", "Identification of applications across different AI domains"], "limitations": "", "keywords": ["large language models", "uncertainty quantification", "hallucinations", "AI applications", "robotics"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2501.03456", "pdf": "https://arxiv.org/pdf/2501.03456.pdf", "abs": "https://arxiv.org/abs/2501.03456", "title": "Text to Band Gap: Pre-trained Language Models as Encoders for Semiconductor Band Gap Prediction", "authors": ["Ying-Ting Yeh", "Janghoon Ock", "Shagun Maheshwari", "Amir Barati Farimani"], "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": null, "summary": "We investigate the use of transformer-based language models, RoBERTa, T5, and\nLLaMA, for predicting the band gaps of semiconductor materials directly from\ntextual representations that encode key material features such as chemical\ncomposition, crystal system, space group, number of atoms per unit cell,\nvalence electron count, and other relevant electronic and structural\nproperties. Quantum chemistry simulations such as DFT provide accurate\npredictions but are computationally intensive, limiting their feasibility for\nlarge-scale materials screening. Shallow ML models offer faster alternatives\nbut typically require extensive data preprocessing to convert non-numerical\nmaterial features into structured numerical inputs, often at the cost of losing\ncritical descriptive information. In contrast, our approach leverages\npretrained language models to process textual data directly, eliminating the\nneed for manual feature engineering. We construct material descriptions in two\nformats: structured strings that combine key features in a consistent template,\nand natural language narratives generated using the ChatGPT API. For each\nmodel, we append a custom regression head and perform task-specific finetuning\non a curated dataset of inorganic compounds. Our results show that finetuned\nlanguage models, particularly the decoder-only LLaMA-3 architecture, can\noutperform conventional approaches in prediction accuracy and flexibility,\nachieving an MAE of 0.25 eV and R2 of 0.89, compared to the best shallow ML\nbaseline, which achieved an MAE of 0.32 eV and R2 of 0.84. Notably, LLaMA-3\nachieves competitive accuracy with minimal finetuning, suggesting its\narchitecture enables more transferable representations for scientific tasks.\nThis work demonstrates the effectiveness of finetuned language models for\nscientific property prediction and provides a scalable, language-native\nframework for materials informatics.", "AI": {"tldr": "The paper explores transformer-based language models for predicting semiconductor material band gaps using textual representations, outperforming conventional methods while minimizing feature engineering.", "motivation": "To address the computational intensity of quantum chemistry simulations and the data preprocessing challenges of shallow ML models, this study aims to utilize pretrained language models for predicting material properties directly from textual data.", "method": "The authors employ transformer-based models like RoBERTa, T5, and LLaMA, using structured strings and natural language narratives as input, followed by finetuning on a dataset of inorganic compounds with custom regression heads.", "result": "Finetuned LLaMA-3 achieves superior prediction performance (MAE of 0.25 eV and R2 of 0.89) compared to the best shallow ML baseline (MAE of 0.32 eV and R2 of 0.84), with minimal finetuning required.", "conclusion": "The findings demonstrate the capabilities of language models in scientific property prediction, offering a scalable framework for materials informatics without extensive manual feature extraction.", "key_contributions": ["Demonstrates the effectiveness of finetuned language models for predicting band gaps from textual data.", "Introduces a scalable, language-native framework for materials informatics that reduces the need for feature engineering.", "Shows that the LLaMA-3 architecture provides competitive accuracy with minimal finetuning."], "limitations": "", "keywords": ["semiconductor materials", "band gap prediction", "transformer models", "materials informatics", "language representation"], "importance_score": 7, "read_time_minutes": 12}}
