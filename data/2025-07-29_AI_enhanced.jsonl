{"id": "2507.19483", "pdf": "https://arxiv.org/pdf/2507.19483.pdf", "abs": "https://arxiv.org/abs/2507.19483", "title": "The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "comment": "39 Pages, no figures", "summary": "AI systems now function as cognitive extensions, evolving from tools to\nactive cognitive collaborators within human-AI integrated systems. While these\nsystems can amplify cognition - enhancing problem-solving, learning, and\ncreativity - they present a fundamental \"comfort-growth paradox\": AI's\nuser-friendly nature may foster intellectual stagnation by minimizing cognitive\nfriction necessary for development. As AI aligns with user preferences and\nprovides frictionless assistance, it risks inducing cognitive complacency\nrather than promoting growth. We introduce Enhanced Cognitive Scaffolding to\nresolve this paradox - reconceptualizing AI from convenient assistant to\ndynamic mentor. Drawing from Vygotskian theories, educational scaffolding\nprinciples, and AI ethics, our framework integrates three dimensions: (1)\nProgressive Autonomy, where AI support gradually fades as user competence\nincreases; (2) Adaptive Personalization, tailoring assistance to individual\nneeds and learning trajectories; and (3) Cognitive Load Optimization, balancing\nmental effort to maximize learning while minimizing unnecessary complexity.\nResearch across educational, workplace, creative, and healthcare domains\nsupports this approach, demonstrating accelerated skill acquisition, improved\nself-regulation, and enhanced higher-order thinking. The framework includes\nsafeguards against risks like dependency, skill atrophy, and bias\namplification. By prioritizing cognitive development over convenience in\nhuman-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward\ngenuinely amplified cognition while safeguarding autonomous thought and\ncontinuous learning.", "AI": {"tldr": "The paper introduces Enhanced Cognitive Scaffolding, a framework to optimize human-AI interaction by shifting AI's role from a convenient assistant to a dynamic mentor, addressing the comfort-growth paradox.", "motivation": "AI systems evolve from tools to cognitive collaborators, enhancing human cognition but risking cognitive complacency. This paper seeks to resolve these issues through a new framework.", "method": "The framework consists of three dimensions: (1) Progressive Autonomy, (2) Adaptive Personalization, and (3) Cognitive Load Optimization, supported by research in various domains.", "result": "Research shows that Enhanced Cognitive Scaffolding can accelerate skill acquisition, improve self-regulation, and enhance higher-order thinking across fields.", "conclusion": "Prioritizing cognitive development over convenience can lead to enhanced human cognition while preventing dependency and bias amplification in AI interactions.", "key_contributions": ["Introduction of Enhanced Cognitive Scaffolding framework", "Integration of Vygotskian theories and educational principles", "Empirical support across multiple domains for improved cognitive outcomes"], "limitations": "The framework requires careful implementation to avoid overwhelming users or reinforcing biases.", "keywords": ["Human-AI interaction", "Cognitive development", "Enhanced Cognitive Scaffolding", "Artificial Intelligence", "Education"], "importance_score": 9, "read_time_minutes": 39}}
{"id": "2507.19485", "pdf": "https://arxiv.org/pdf/2507.19485.pdf", "abs": "https://arxiv.org/abs/2507.19485", "title": "Creativity as a Human Right: Design Considerations for Computational Creativity Systems", "authors": ["Alayt Issak"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "We investigate creativity that is underlined in the Universal Declaration of\nHuman Rights (UDHR) to present design considerations for Computational\nCreativity (CC) systems. We find this declaration to describe creativity in\nsalient aspects and bring to light creativity as a Human Right attributed to\nthe Fourth Generation of such rights. This generation of rights attributes CC\nsystems and the evolving nature of interaction with entities of shared\nintelligence. Our methodology examines five of thirty articles from the UDHR\nand demonstrates each article with actualizations concluding with design\nconsiderations for each. We contribute our findings to ground the relationship\nbetween creativity and CC systems.", "AI": {"tldr": "The paper explores the connection between creativity and Computational Creativity (CC) systems as outlined in the Universal Declaration of Human Rights (UDHR), providing design considerations for CC systems.", "motivation": "To highlight the importance of creativity as a Human Right and inform the design of Computational Creativity systems.", "method": "The methodology involves examining five articles from the UDHR and demonstrating their application in the context of CC systems, leading to design considerations.", "result": "Findings establish a relationship between creativity and CC systems, emphasizing creativity as a core component in their design.", "conclusion": "The study presents design considerations for CC systems based on the implications of creativity outlined in the UDHR.", "key_contributions": ["Establishment of creativity as a Human Right in the context of CC.", "Design considerations for developing CC systems based on UDHR articles.", "Linking human rights to the design of intelligent systems."], "limitations": "", "keywords": ["Creativity", "Computational Creativity", "Human Rights", "Universal Declaration of Human Rights", "Design Considerations"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.19486", "pdf": "https://arxiv.org/pdf/2507.19486.pdf", "abs": "https://arxiv.org/abs/2507.19486", "title": "Confirmation bias: A challenge for scalable oversight", "authors": ["Gabriel Recchia", "Chatrik Singh Mangat", "Jinu Nyachhyon", "Mridul Sharma", "Callum Canavan", "Dylan Epstein-Gross", "Muhammed Abdulbari"], "categories": ["cs.HC", "cs.AI"], "comment": "61 pages, 8 figures", "summary": "Scalable oversight protocols aim to empower evaluators to accurately verify\nAI models more capable than themselves. However, human evaluators are subject\nto biases that can lead to systematic errors. We conduct two studies examining\nthe performance of simple oversight protocols where evaluators know that the\nmodel is \"correct most of the time, but not all of the time\". We find no\noverall advantage for the tested protocols, although in Study 1, showing\narguments in favor of both answers improves accuracy in cases where the model\nis incorrect. In Study 2, participants in both groups become more confident in\nthe system's answers after conducting online research, even when those answers\nare incorrect. We also reanalyze data from prior work that was more optimistic\nabout simple protocols, finding that human evaluators possessing knowledge\nabsent from models likely contributed to their positive results--an advantage\nthat diminishes as models continue to scale in capability. These findings\nunderscore the importance of testing the degree to which oversight protocols\nare robust to evaluator biases, whether they outperform simple deference to the\nmodel under evaluation, and whether their performance scales with increasing\nproblem difficulty and model capability.", "AI": {"tldr": "This paper studies the effectiveness of scalable oversight protocols for AI model evaluation, highlighting biases that affect human evaluators and presenting mixed results on performance under varying conditions.", "motivation": "To explore how human evaluators can accurately verify advanced AI models while being subject to biases that can cause errors in judgment.", "method": "Conducted two studies testing simple oversight protocols which informed evaluators that models are mostly correct. Evaluators' performance and confidence were measured across different scenarios.", "result": "The studies found no overall advantage for the oversight protocols; however, showing arguments for both answers improved accuracy in some aspects. Evaluators became overconfident in online research scenarios even when the model's answers were incorrect.", "conclusion": "The effectiveness of oversight protocols is dependent on evaluator biases, and reliance on them doesn't necessarily outperform simple deference to models, especially as model capabilities increase.", "key_contributions": ["Examination of oversight protocols under human biases", "Insights on evaluator confidence and performance", "Reanalysis of previous optimistic findings regarding simple protocols"], "limitations": "Results may vary with different evaluators or contexts, and factors influencing confidence and accuracy need further exploration.", "keywords": ["Human-Computer Interaction", "Oversight Protocols", "AI Model Evaluation", "Bias in Evaluators", "Model Capability"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.19488", "pdf": "https://arxiv.org/pdf/2507.19488.pdf", "abs": "https://arxiv.org/abs/2507.19488", "title": "E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets", "authors": ["Alexandros Gazis", "Eleftheria Katsiri"], "categories": ["cs.HC", "cs.CY", "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0"], "comment": "The article is under review by MDPI, Electronics journal. 36 pages,\n  20 figures, 67 references", "summary": "E-polis is a serious digital game designed to gamify sociological surveys\nstudying young people's political opinions. In this platform game, players\nnavigate a digital world, encountering quests posing sociological questions.\nPlayers' answers shape the city-game world, altering building structures based\non their choices. E-polis is a serious game, not a government simulation,\naiming to understand players' behaviors and opinions thus we do not train the\nplayers but rather understand them and help them visualize their choices in\nshaping a city's future. Also, it is noticed that no correct or incorrect\nanswers apply. Moreover, our game utilizes a novel middleware architecture for\ndevelopment, diverging from typical asset prefab scene and script segregation.\nThis article presents the data layer of our game's middleware, specifically\nfocusing on data analysis based on respondents' gameplay answers. E-polis\nrepresents an innovative approach to gamifying sociological research, providing\na unique platform for gathering and analyzing data on political opinions among\nyouth and contributing to the broader field of serious games.", "AI": {"tldr": "E-polis is a serious digital game designed to study young people's political opinions through gameplay that affects a virtual city.", "motivation": "To understand and visualize young people's political opinions and behaviors in shaping a city's future.", "method": "E-polis allows players to answer sociological questions through gameplay, with their choices impacting the game's city structure and environment.", "result": "The game's innovative middleware architecture facilitates detailed analysis of players' responses and the relationship between gameplay and political opinions.", "conclusion": "E-polis provides a unique platform for gathering and analyzing data on youth political opinions, contributing to the field of serious games.", "key_contributions": ["Novel approach to gamifying sociological research", "Middleware architecture for data analysis", "Engaging method for understanding political opinions among youth"], "limitations": "", "keywords": ["gamification", "sociological research", "youth opinions", "serious games", "middleware architecture"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2507.19511", "pdf": "https://arxiv.org/pdf/2507.19511.pdf", "abs": "https://arxiv.org/abs/2507.19511", "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media", "authors": ["Khalid Hasan", "Jamil Saquer", "Mukulika Ghosh"], "categories": ["cs.CL", "cs.LG"], "comment": "The 49th IEEE International Conference on Computers, Software, and\n  Applications (COMPSAC 2025) (camera-ready)", "summary": "The rising prevalence of mental health disorders necessitates the development\nof robust, automated tools for early detection and monitoring. Recent advances\nin Natural Language Processing (NLP), particularly transformer-based\narchitectures, have demonstrated significant potential in text analysis. This\nstudy provides a comprehensive evaluation of state-of-the-art transformer\nmodels (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term\nMemory (LSTM) based approaches using different text embedding techniques for\nmental health disorder classification on Reddit. We construct a large annotated\ndataset, validating its reliability through statistical judgmental analysis and\ntopic modeling. Experimental results demonstrate the superior performance of\ntransformer models over traditional deep-learning approaches. RoBERTa achieved\nthe highest classification performance, with a 99.54% F1 score on the hold-out\ntest set and a 96.05% F1 score on the external test set. Notably, LSTM models\naugmented with BERT embeddings proved highly competitive, achieving F1 scores\nexceeding 94% on the external dataset while requiring significantly fewer\ncomputational resources. These findings highlight the effectiveness of\ntransformer-based models for real-time, scalable mental health monitoring. We\ndiscuss the implications for clinical applications and digital mental health\ninterventions, offering insights into the capabilities and limitations of\nstate-of-the-art NLP methodologies in mental disorder detection.", "AI": {"tldr": "This study evaluates transformer models for mental health disorder classification, finding that they outperform traditional LSTM approaches, with RoBERTa achieving the highest performance.", "motivation": "The need for automated tools for early detection and monitoring of mental health disorders due to their rising prevalence.", "method": "Evaluation of transformer models (BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA) against LSTM models using various text embedding techniques on annotated datasets from Reddit.", "result": "RoBERTa achieved a 99.54% F1 score on the hold-out test set and a 96.05% F1 score on the external test set, showing superior performance compared to LSTMs.", "conclusion": "Transformer models are effective for real-time mental health monitoring, offering insights that could enhance clinical applications and digital mental health interventions.", "key_contributions": ["Construction of a large annotated dataset for mental health classification.", "Demonstration of the superiority of transformer models over LSTM architecture.", "Insights into the implications of NLP methodologies for clinical applications."], "limitations": "", "keywords": ["mental health", "NLP", "transformer models", "LSTM", "classification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.19490", "pdf": "https://arxiv.org/pdf/2507.19490.pdf", "abs": "https://arxiv.org/abs/2507.19490", "title": "RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information", "authors": ["Xinzheng Wu", "Junyi Chen", "Peiyi Wang", "Shunxiang Chen", "Yong Shen"], "categories": ["cs.HC", "cs.CV"], "comment": "Submitted for ITSC 2025", "summary": "In the research and development (R&D) and verification and validation (V&V)\nphases of autonomous driving decision-making and planning systems, it is\nnecessary to integrate human factors to achieve decision-making and evaluation\nthat align with human cognition. However, most existing datasets primarily\nfocus on vehicle motion states and trajectories, neglecting human-related\ninformation. In addition, current naturalistic driving datasets lack sufficient\nsafety-critical scenarios while simulated datasets suffer from low\nauthenticity. To address these issues, this paper constructs the Risk-Informed\nSubjective Evaluation and Eye-tracking (RISEE) dataset which specifically\ncontains human subjective evaluations and eye-tracking data apart from regular\nnaturalistic driving trajectories. By leveraging the complementary advantages\nof drone-based (high realism and extensive scenario coverage) and\nsimulation-based (high safety and reproducibility) data collection methods, we\nfirst conduct drone-based traffic video recording at a highway ramp merging\narea. After that, the manually selected highly interactive scenarios are\nreconstructed in simulation software, and drivers' first-person view (FPV)\nvideos are generated, which are then viewed and evaluated by recruited\nparticipants. During the video viewing process, participants' eye-tracking data\nis collected. After data processing and filtering, 3567 valid subjective risk\nratings from 101 participants across 179 scenarios are retained, along with\n2045 qualified eye-tracking data segments. The collected data and examples of\nthe generated FPV videos are available in our website.", "AI": {"tldr": "This paper introduces the RISEE dataset, which integrates human subjective evaluations and eye-tracking data alongside traditional driving trajectories to enhance autonomous driving decision-making by incorporating human factors.", "motivation": "Address the lack of human-related information and safety-critical scenarios in existing autonomous driving datasets.", "method": "The RISEE dataset was created by first recording drone-based traffic videos at a highway ramp merging area, followed by reconstructing high-interaction scenarios in simulation software for evaluation by participants, from whom eye-tracking data and subjective risk ratings were collected.", "result": "The study resulted in 3567 valid subjective risk ratings and 2045 qualified eye-tracking data segments from 101 participants across 179 scenarios.", "conclusion": "The RISEE dataset offers a novel resource that bridges the gap between human factors and autonomous driving evaluation, improving the authenticity and safety-critical nature of such datasets.", "key_contributions": ["Introduction of the RISEE dataset that includes human evaluations and eye-tracking data.", "Combination of drone-based and simulation data collection methods to enhance dataset realism and safety.", "Availability of the dataset for research purposes, including example FPV videos."], "limitations": "The dataset may still not cover all possible driving scenarios and does not account for individual variations in human subject responses fully.", "keywords": ["autonomous driving", "human factors", "eye-tracking", "risk evaluation", "naturalistic driving"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.19521", "pdf": "https://arxiv.org/pdf/2507.19521.pdf", "abs": "https://arxiv.org/abs/2507.19521", "title": "Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables", "authors": ["Vishakh Padmakumar", "Joseph Chee Chang", "Kyle Lo", "Doug Downey", "Aakanksha Naik"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing volume of academic literature makes it essential for\nresearchers to organize, compare, and contrast collections of documents. Large\nlanguage models (LLMs) can support this process by generating schemas defining\nshared aspects along which to compare papers. However, progress on schema\ngeneration has been slow due to: (i) ambiguity in reference-based evaluations,\nand (ii) lack of editing/refinement methods. Our work is the first to address\nboth issues. First, we present an approach for augmenting unannotated table\ncorpora with synthesized intents and apply it to create a dataset for studying\nschema generation conditioned on a given information need, thus reducing\nambiguity. With this dataset, we show how incorporating table intents\nsignificantly improves baseline performance in reconstructing reference\nschemas. Next, we propose several LLM-based schema editing techniques. We start\nby comprehensively benchmarking several single-shot schema generation methods,\nincluding prompted LLM workflows and fine-tuned models, showing that smaller,\nopen-weight models can be fine-tuned to be competitive with state-of-the-art\nprompted LLMs. Then we demonstrate that our editing techniques can further\nimprove schemas generated by these methods.", "AI": {"tldr": "This paper advances schema generation for document comparison by addressing ambiguity and proposing LLM-based editing techniques, resulting in improved performance and refinement of schemas.", "motivation": "With the growing volume of academic literature, effective organization and comparison of documents becomes essential, necessitating improved schema generation methods.", "method": "The authors augment unannotated table corpora with synthesized intents to create a dataset for studying schema generation. They benchmark multiple schema generation methods, including LLM workflows and fine-tuned models, and introduce editing techniques to enhance generated schemas.", "result": "Incorporating table intents into schema generation significantly improves baseline performance, and LLM-based editing techniques further refine the generated schemas, leading to better quality comparisons.", "conclusion": "The study shows that better data and innovative editing methods enhance the effectiveness of schema generation, making LLMs more competitive in this domain.", "key_contributions": ["Introduction of a dataset for schema generation using synthesized intents", "Benchmarking of schema generation methods with findings that small, open-weight models can be effective", "Development of LLM-based editing techniques to improve schema quality"], "limitations": "", "keywords": ["schema generation", "large language models", "document comparison"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.19491", "pdf": "https://arxiv.org/pdf/2507.19491.pdf", "abs": "https://arxiv.org/abs/2507.19491", "title": "Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory using Consumer Wearables", "authors": ["Peter Neigel", "David Antony Selby", "Shota Arai", "Benjamin Tag", "Niels van Berkel", "Sebastian Vollmer", "Andrew Vargo", "Koichi Kise"], "categories": ["cs.HC", "cs.CY"], "comment": "18 pages, 6 figures, 7 tables", "summary": "Wearable devices offer detailed sleep-tracking data. However, whether this\ninformation enhances our understanding of sleep or simply quantifies\nalready-known patterns remains unclear. This work explores the relationship\nbetween subjective sleep self-assessments and sensor data from an Oura ring\nover 4--8 weeks in-the-wild. 29 participants rated their sleep quality daily\ncompared to the previous night and completed a working memory task. Our\nfindings reveal that differences in REM sleep, nocturnal heart rate, N-Back\nscores, and bedtimes highly predict sleep self-assessment in significance and\neffect size. For N-Back performance, REM sleep duration, prior night's REM\nsleep, and sleep self-assessment are the strongest predictors. We demonstrate\nthat self-report sensitivity towards sleep markers differs among participants.\nWe identify three groups, highlighting that sleep trackers provide more\ninformation gain for some users than others. Additionally, we make all\nexperiment data publicly available.", "AI": {"tldr": "This study investigates the relationship between subjective sleep assessments and Oura ring sensor data, revealing key predictors of sleep quality perceptions among participants.", "motivation": "To determine whether wearable sleep-tracking data enhances understanding of sleep or merely quantifies existing knowledge.", "method": "29 participants rated their sleep quality daily and completed a working memory task, with data collected over 4-8 weeks from an Oura ring.", "result": "Key predictors of sleep self-assessment included REM sleep, nocturnal heart rate, and N-Back task performance, with significant differences in sensitivity to sleep markers among participants.", "conclusion": "The findings suggest sleep trackers may offer varying information benefits to users, and the data from the study is publicly accessible.", "key_contributions": ["Exploration of the link between subjective sleep assessments and wearable sensor data.", "Identification of key predictors influencing sleep quality self-assessments.", "Segmentation of participants into groups based on sensitivity to sleep tracker data."], "limitations": "The study is limited to a small sample size and the specific wearable device used (Oura ring).", "keywords": ["sleep tracking", "wearable devices", "subjective assessments", "REM sleep", "self-report sensitivity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19537", "pdf": "https://arxiv.org/pdf/2507.19537.pdf", "abs": "https://arxiv.org/abs/2507.19537", "title": "Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri", "authors": ["Felix Kraus", "Nicolas Blumenröhr", "Danah Tonne", "Achim Streit"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for\nthe automated translation of SKOS thesauri. This work addresses a critical need\nin the Digital Humanities (DH), where language diversity can limit access,\nreuse, and semantic interoperability of knowledge resources. WOKIE combines\nexternal translation services with targeted refinement using Large Language\nModels (LLMs), balancing translation quality, scalability, and cost. Designed\nto run on everyday hardware and be easily extended, the application requires no\nprior expertise in machine translation or LLMs. We evaluate WOKIE across\nseveral DH thesauri in 15 languages with different parameters, translation\nservices and LLMs, systematically analysing translation quality, performance,\nand ontology matching improvements. Our results show that WOKIE is suitable to\nenhance the accessibility, reuse, and cross-lingual interoperability of\nthesauri by hurdle-free automated translation and improved ontology matching\nperformance, supporting more inclusive and multilingual research\ninfrastructures.", "AI": {"tldr": "WOKIE is an open-source pipeline for automating the translation of SKOS thesauri, enhancing accessibility and interoperability in Digital Humanities.", "motivation": "To address the challenges of language diversity in Digital Humanities, which limits access and reuse of knowledge resources.", "method": "WOKIE combines external translation services with targeted refinement using Large Language Models (LLMs) to improve translation quality and performance.", "result": "WOKIE was evaluated on multiple DH thesauri in 15 languages, demonstrating enhanced translation quality, performance, and ontology matching.", "conclusion": "WOKIE effectively supports more inclusive, multilingual research infrastructures by providing automated translation and improved ontology performance.", "key_contributions": ["Open-source and modular design for easy extensibility", "Combines translation services with LLMs for improved quality", "Addresses language diversity to enhance accessibility and reuse in DH."], "limitations": "", "keywords": ["WOKIE", "automated translation", "Digital Humanities", "Large Language Models", "SKOS thesauri"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.19492", "pdf": "https://arxiv.org/pdf/2507.19492.pdf", "abs": "https://arxiv.org/abs/2507.19492", "title": "ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation", "authors": ["Jovana Kondic", "Pengyuan Li", "Dhiraj Joshi", "Zexue He", "Shafiq Abedin", "Jennifer Sun", "Ben Wiesel", "Eli Schwartz", "Ahmed Nassar", "Bo Wu", "Assaf Arbelle", "Aude Oliva", "Dan Gutfreund", "Leonid Karlinsky", "Rogerio Feris"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Chart-to-code reconstruction -- the task of recovering executable plotting\nscripts from chart images -- provides important insights into a model's ability\nto ground data visualizations in precise, machine-readable form. Yet many\nexisting multimodal benchmarks largely focus primarily on answering questions\nabout charts or summarizing them. To bridge this gap, we present ChartGen, a\nfully-automated pipeline for code-guided synthetic chart generation. Starting\nfrom seed chart images, ChartGen (i) prompts a vision-language model (VLM) to\nreconstruct each image into a python script, and (ii) iteratively augments that\nscript with a code-oriented large language model (LLM). Using ChartGen, we\ncreate 222.5K unique chart-image code pairs from 13K seed chart images, and\npresent an open-source synthetic chart dataset covering 27 chart types, 11\nplotting libraries, and multiple data modalities (image, code, text, CSV,\nDocTags). From this corpus, we curate a held-out chart-to-code evaluation\nsubset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B -\n26B parameters), highlighting substantial room for progress. We release the\npipeline, prompts, and the dataset to help accelerate efforts towards robust\nchart understanding and vision-conditioned code generation:\nhttps://github.com/SD122025/ChartGen/", "AI": {"tldr": "This paper presents ChartGen, a pipeline for generating executable plotting scripts from chart images, creating a large synthetic dataset for evaluating multimodal models.", "motivation": "The need to recover executable plotting scripts from chart images to enhance machine understanding of data visualizations.", "method": "ChartGen uses a vision-language model to reconstruct chart images into Python scripts and then refines these scripts with a code-oriented large language model.", "result": "A synthetic dataset of 222.5K unique chart-image code pairs was created, demonstrating substantial room for improvement in multimodal models for chart understanding.", "conclusion": "The release of ChartGen and the accompanying dataset aims to accelerate research in chart understanding and vision-conditioned code generation.", "key_contributions": ["Introduction of ChartGen for code-guided synthetic chart generation", "Creation of a large synthetic chart-image code dataset", "Evaluation of multiple vision-language models on chart-to-code reconstruction"], "limitations": "", "keywords": ["chart-to-code", "synthetic dataset", "vision-language models", "data visualization", "code generation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.19586", "pdf": "https://arxiv.org/pdf/2507.19586.pdf", "abs": "https://arxiv.org/abs/2507.19586", "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning", "authors": ["Shengyuan Wang", "Jie Feng", "Tianhui Liu", "Dan Pei", "Yong Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) possess extensive world knowledge, including\ngeospatial knowledge, which has been successfully applied to various geospatial\ntasks such as mobility prediction and social indicator prediction. However,\nLLMs often generate inaccurate geospatial knowledge, leading to geospatial\nhallucinations (incorrect or inconsistent representations of geospatial\ninformation) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic\nevaluation and mitigation of geospatial hallucinations remain largely\nunexplored. To address this gap, we propose a comprehensive evaluation\nframework for geospatial hallucinations, leveraging structured geospatial\nknowledge graphs for controlled assessment. Through extensive evaluation across\n20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.\nBuilding on these insights, we introduce a dynamic factuality aligning method\nbased on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on\nthe proposed benchmark. Extensive experimental results demonstrate the\neffectiveness of our benchmark and learning algorithm in enhancing the\ntrustworthiness of LLMs in geospatial knowledge and reasoning tasks.", "AI": {"tldr": "This paper evaluates and mitigates geospatial hallucinations in large language models (LLMs) using a new framework and a method based on Kahneman-Tversky Optimization, resulting in significant performance improvements.", "motivation": "The study addresses the gap in systematic evaluation and mitigation of geospatial hallucinations in LLMs, which affects their reliability in geospatial tasks.", "method": "A comprehensive evaluation framework was developed using structured geospatial knowledge graphs, and a dynamic factuality aligning method based on Kahneman-Tversky Optimization was introduced.", "result": "The proposed method achieved a performance improvement of over 29.6% on the benchmark for geospatial knowledge accuracy in LLMs.", "conclusion": "The findings demonstrate the effectiveness of the evaluation framework and the learning algorithm in enhancing LLMs' trustworthiness for geospatial knowledge and reasoning.", "key_contributions": ["Development of an evaluation framework for geospatial hallucinations in LLMs.", "Introduction of a dynamic method to mitigate geospatial hallucinations using Kahneman-Tversky Optimization.", "Extensive experimental results that validate the proposed benchmark and learning algorithm."], "limitations": "", "keywords": ["large language models", "geospatial knowledge", "hallucinations", "Kahneman-Tversky Optimization", "evaluation framework"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2507.19493", "pdf": "https://arxiv.org/pdf/2507.19493.pdf", "abs": "https://arxiv.org/abs/2507.19493", "title": "From Bench to Bedside: A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice", "authors": ["Yaowei Bai", "Ruiheng Zhang", "Yu Lei", "Jingfeng Yao", "Shuguang Ju", "Chaoyang Wang", "Wei Yao", "Yiwan Guo", "Guilin Zhang", "Chao Wan", "Qian Yuan", "Xuhua Duan", "Xinggang Wang", "Tao Sun", "Yongchao Xu", "Chuansheng Zheng", "Huangxuan Zhao", "Bo Du"], "categories": ["cs.HC", "eess.IV"], "comment": null, "summary": "A global shortage of radiologists has been exacerbated by the significant\nvolume of chest X-ray workloads, particularly in primary care. Although\nmultimodal large language models show promise, existing evaluations\npredominantly rely on automated metrics or retrospective analyses, lacking\nrigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray\ninterpretation system based on DeepSeek Janus-Pro model, was developed and\nrigorously validated through a multicenter prospective trial (NCT06874647). Our\nsystem outperforms state-of-the-art X-ray report generation models in automated\nreport generation, surpassing even larger-scale models including ChatGPT 4o\n(200B parameters), while demonstrating robust detection of eight clinically\ncritical radiographic findings (area under the curve, AUC > 0.8). Retrospective\nevaluation confirms significantly higher report accuracy than Janus-Pro and\nChatGPT 4o. In prospective clinical deployment, AI assistance significantly\nimproved report quality scores (4.37 vs. 4.11, P < 0.001), reduced\ninterpretation time by 18.5% (P < 0.001), and was preferred by a majority of\nexperts (3 out of 5) in 52.7% of cases. Through lightweight architecture and\ndomain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and\nworkflow efficiency, particularly in resource-constrained settings. The model\narchitecture and implementation framework will be open-sourced to facilitate\nthe clinical translation of AI-assisted radiology solutions.", "AI": {"tldr": "Janus-Pro-CXR is a chest X-ray interpretation system that outperforms existing models, improves report quality, reduces interpretation time, and enhances diagnostic reliability in clinical settings.", "motivation": "To address the global shortage of radiologists and improve the quality and efficiency of chest X-ray interpretations in primary care through AI assistance.", "method": "A multicenter prospective trial validated the Janus-Pro-CXR system against state-of-the-art models, focusing on automated report generation and detection of critical radiographic findings.", "result": "The system showed significantly improved report accuracy and reduced interpretation time compared to existing models, with high preference among clinical experts.", "conclusion": "Janus-Pro-CXR enhances diagnostic reliability and workflow efficiency in radiology, especially in resource-limited environments, and plans to open-source the model for broader use.", "key_contributions": ["Outperforms existing X-ray report generation models", "Demonstrates robust detection of critical radiographic findings", "Improves diagnostic reliability and workflow efficiency"], "limitations": "", "keywords": ["chest X-ray", "AI assistance", "radiology", "report generation", "health informatics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.19595", "pdf": "https://arxiv.org/pdf/2507.19595.pdf", "abs": "https://arxiv.org/abs/2507.19595", "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.", "AI": {"tldr": "This survey reviews efficient attention mechanisms in transformer-based models focusing on linear and sparse attention techniques to mitigate computational complexity in long-context modeling.", "motivation": "To address the limitations of quadratic time and memory complexity in self-attention mechanisms of large language models, promoting more efficient long-context modeling.", "method": "The paper systematically reviews recent advancements in efficient attention, categorizing them into linear and sparse approaches, and exploring their theoretical and hardware implications.", "result": "The author integrates various efficient attention methods into large-scale pre-trained language models, analyzing architectures that utilize efficient attention exclusively or in hybrid forms.", "conclusion": "This work serves as a foundational reference for further advancements in the design of scalable and efficient language models by combining theoretical foundations with deployment strategies.", "key_contributions": ["Comprehensive overview of efficient attention mechanisms in language models", "Integration of algorithmic innovations with hardware considerations", "Analysis of efficient attention in large-scale pre-trained models"], "limitations": "", "keywords": ["efficiency", "attention mechanisms", "large language models"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2507.19494", "pdf": "https://arxiv.org/pdf/2507.19494.pdf", "abs": "https://arxiv.org/abs/2507.19494", "title": "Evaluating Personalized Beneficial Interventions in the Daily Lives of Older Adults Using a Camera", "authors": ["Longfei Chen", "Christopher Lochhead", "Robert B. Fisher", "Nusa Faric", "Jacques Fleuriot", "Subramanian Ramamoorthy"], "categories": ["cs.HC"], "comment": "AIiH 2025, International Conference on AI in Healthcare", "summary": "Beneficial daily activity interventions have been shown to improve both the\nphysical and mental health of older adults. However, there is a lack of robust\nobjective metrics and personalized strategies to measure their impact. In this\nstudy, two older adults aged over 65, living in Edinburgh, UK, selected their\npreferred daily interventions (mindful meals and art crafts), which are then\nassessed for effectiveness. The total monitoring period across both\nparticipants was 8 weeks. Their physical behaviours were continuously monitored\nusing a non-contact, privacy-preserving camera-based system. Postural and\nmobility statistics were extracted using computer vision algorithms and\ncompared across periods with and without the interventions. The results\ndemonstrate significant behavioural changes for both participants, highlighting\nthe effectiveness of both these activities and the monitoring system.", "AI": {"tldr": "This study assesses the effectiveness of daily activity interventions (mindful meals and art crafts) on the physical and mental health of older adults using a non-contact camera-based monitoring system.", "motivation": "There is a lack of robust objective metrics and personalized strategies to measure the impact of daily activity interventions on elderly health.", "method": "Two older adults over 65 selected preferred daily interventions, which were assessed over an 8-week monitoring period using a camera-based system to track physical behaviors and extract statistics via computer vision algorithms.", "result": "Significant behavioral changes were observed in both participants, indicating the effectiveness of mindful meals and art crafts, as well as the monitoring system used.", "conclusion": "The findings support the use of specified interventions and the proposed monitoring system to improve the health outcomes of older adults.", "key_contributions": ["Utilization of a non-contact, privacy-preserving monitoring system for assessing daily interventions.", "Demonstration of significant behavioral changes through selected activities for older adults.", "Presentation of robust metrics for measuring the impact of interventions on physical behavior."], "limitations": "The study involves only two participants, which may limit the generalizability of results.", "keywords": ["activity interventions", "older adults", "camera monitoring", "computer vision", "health outcomes"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.19598", "pdf": "https://arxiv.org/pdf/2507.19598.pdf", "abs": "https://arxiv.org/abs/2507.19598", "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?", "authors": ["Muntasir Wahed", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Nirav Diwan", "Gang Wang", "Dilek Hakkani-Tür", "Ismini Lourentzou"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Winner Defender Team at Amazon Nova AI Challenge 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision.", "AI": {"tldr": "This paper addresses the robustness of Large Language Models (LLMs) in code generation against adversarial attacks using multi-turn prompts. It introduces a benchmark for evaluating these models and presents improvements to rejection rates through fine-tuning.", "motivation": "To explore the vulnerabilities of LLMs in code generation against adversarial misuse through sophisticated coding prompts that evade safety filters.", "method": "The authors propose code decomposition attacks and build a benchmark called \benchmarkname{} for evaluating both single-turn and multi-turn malicious prompts. They conducted empirical evaluations on various models to assess their robustness.", "result": "The study revealed that LLMs exhibit persistent vulnerabilities, particularly in multi-turn interactions. Fine-tuning on the MOCHA dataset resulted in significantly improved rejection rates while maintaining coding capabilities, achieving up to a 32.4% increase in robustness against adversarial datasets.", "conclusion": "Enhancing LLMs' resistance to adversarial prompts is crucial, and the proposed MOCHA fine-tuning method shows promise in maintaining performance while improving safety against multi-turn attacks.", "key_contributions": ["Introduction of code decomposition attacks for LLMs.", "Development of the \benchmarkname{} benchmark for systematic evaluation.", "Demonstration of significant improvement in rejection rates through fine-tuning on MOCHA."], "limitations": "The study may not cover all potential adversarial strategies and focuses primarily on multi-turn interactions.", "keywords": ["Large Language Models", "code generation", "adversarial attacks", "benchmarking", "fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19495", "pdf": "https://arxiv.org/pdf/2507.19495.pdf", "abs": "https://arxiv.org/abs/2507.19495", "title": "Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action", "authors": ["Qing Dong", "Pengyuan Liu", "Dong Yu", "Chen Kang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative agents have made significant progress in simulating human\nbehavior, but existing frameworks often simplify emotional modeling and focus\nprimarily on specific tasks, limiting the authenticity of the simulation. Our\nwork proposes the Psychological-mechanism Agent (PSYA) framework, based on the\nCognitive Triangle (Feeling-Thought-Action), designed to more accurately\nsimulate human behavior. The PSYA consists of three core modules: the Feeling\nmodule (using a layer model of affect to simulate changes in short-term,\nmedium-term, and long-term emotions), the Thought module (based on the Triple\nNetwork Model to support goal-directed and spontaneous thinking), and the\nAction module (optimizing agent behavior through the integration of emotions,\nneeds and plans). To evaluate the framework's effectiveness, we conducted daily\nlife simulations and extended the evaluation metrics to self-influence,\none-influence, and group-influence, selection five classic psychological\nexperiments for simulation. The results show that the PSYA framework generates\nmore natural, consistent, diverse, and credible behaviors, successfully\nreplicating human experimental outcomes. Our work provides a richer and more\naccurate emotional and cognitive modeling approach for generative agents and\noffers an alternative to human participants in psychological experiments.", "AI": {"tldr": "This paper introduces the Psychological-mechanism Agent (PSYA) framework to enhance the simulation of human behavior through improved emotional and cognitive modeling.", "motivation": "To address the limitations of existing generative agents that simplify emotional modeling and primarily focus on specific tasks.", "method": "The PSYA framework includes three modules: Feeling (to simulate emotional changes), Thought (to support goal-directed and spontaneous thinking), and Action (to optimize behavior through integrating emotions, needs, and plans).", "result": "The PSYA framework generated more natural, consistent, diverse, and credible behaviors, successfully replicating human experimental outcomes in simulations.", "conclusion": "The work provides a richer emotional and cognitive modeling approach for generative agents, offering an alternative for psychological experiments.", "key_contributions": ["Introduction of the PSYA framework for better emotional and cognitive modeling", "Evaluation through daily life simulations and psychological experiments", "Demonstration of improved behavior authenticity in generative agents"], "limitations": "", "keywords": ["Generative Agents", "Human Behavior Simulation", "Psychological Modeling"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.19616", "pdf": "https://arxiv.org/pdf/2507.19616.pdf", "abs": "https://arxiv.org/abs/2507.19616", "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track", "authors": ["Xuchen Wei", "Yangxin Wu", "Yaoyin Zhang", "Henglyu Liu", "Kehai Chen", "Xuefeng Bai", "Min Zhang"], "categories": ["cs.CL"], "comment": "7 pages, 1 figure, submitted to IWSLT 2025", "summary": "This paper presents HITSZ's submission for the IWSLT 2025 Indic track,\nfocusing on speech-to-text translation (ST) for English-to-Indic and\nIndic-to-English language pairs. To enhance translation quality in this\nlow-resource scenario, we propose an end-to-end system integrating the\npre-trained Whisper automated speech recognition (ASR) model with Krutrim, an\nIndic-specialized large language model (LLM). Experimental results demonstrate\nthat our end-to-end system achieved average BLEU scores of $28.88$ for\nEnglish-to-Indic directions and $27.86$ for Indic-to-English directions.\nFurthermore, we investigated the Chain-of-Thought (CoT) method. While this\nmethod showed potential for significant translation quality improvements on\nsuccessfully parsed outputs (e.g. a $13.84$ BLEU increase for\nTamil-to-English), we observed challenges in ensuring the model consistently\nadheres to the required CoT output format.", "AI": {"tldr": "This paper discusses an end-to-end speech-to-text translation system for English-to-Indic and Indic-to-English using a pre-trained Whisper ASR model combined with an Indic-specialized LLM.", "motivation": "To improve translation quality in low-resource speech-to-text translation tasks between English and Indic languages.", "method": "An end-to-end system integrating the pre-trained Whisper ASR model with Krutrim, an Indic-specialized LLM.", "result": "Achieved average BLEU scores of 28.88 for English-to-Indic and 27.86 for Indic-to-English translations. Additionally, the Chain-of-Thought method increased BLEU scores significantly for certain language pairs, but consistency in output format was a challenge.", "conclusion": "The proposed system demonstrates substantial improvements in translation quality while highlighting the necessity for consistent output formatting in Chain-of-Thought applications.", "key_contributions": ["Integration of a pre-trained Whisper ASR model with an Indic-specialized large language model.", "Demonstration of significant BLEU score improvement using Chain-of-Thought method in some language pairs.", "Exploration of challenges in maintaining output consistency in CoT applications."], "limitations": "Challenges in ensuring consistent adherence to the required output format for the Chain-of-Thought method.", "keywords": ["speech-to-text translation", "large language model", "Chain-of-Thought", "BLEU scores", "low-resource translation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.19496", "pdf": "https://arxiv.org/pdf/2507.19496.pdf", "abs": "https://arxiv.org/abs/2507.19496", "title": "Technological Requirements for Videoconferencing Judicial Hearings: Enhancing the Credibility and Reliability of Remote Testimonies", "authors": ["Jorge Alberto Araujo"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This paper analyzes the technological requirements necessary to enhance the\ncredibility and reliability of judicial hearings conducted via videoconference,\nfrom the internal perspective of the judiciary. Drawing on the practical\nexperience of a judge who conducts daily hearings, this study identifies\nlimitations in current platforms for verifying the authenticity of testimonies\nand proposes tailored functionalities for the judicial context. Recognizing\nthat remote hearings represent a convenience for the parties without replacing\nthe option of in-person attendance, the article suggests implementing features\nsuch as eye tracking, environment verification, and blocking of parallel\napplications, in addition to improvements in transmission quality. The study\nconcludes that developing specific modules for witnesses - focusing on security\nand monitoring - can significantly contribute to equalizing the credibility\nbetween remote and in-person hearings, thus expanding access to justice without\ncompromising procedural reliability.", "AI": {"tldr": "The paper discusses technological advancements needed to improve the reliability of video hearings in the judiciary, suggesting specific functionalities to enhance witness credibility.", "motivation": "To enhance the credibility and reliability of judicial hearings conducted via videoconference, addressing limitations of current platforms.", "method": "Analysis of the requirements and practical experiences of judges conducting videoconferences, with a focus on identifying technological deficiencies.", "result": "Proposes functionalities like eye tracking, environment verification, and improved transmission quality to ensure witnesses' authenticity during remote hearings.", "conclusion": "Specific modules for witnesses can balance the credibility of remote and in-person hearings, thus enhancing access to justice while maintaining procedural reliability.", "key_contributions": ["Identification of limitations in current videoconference platforms for judicial hearings", "Proposals for tailored technological functionalities for remote testimonies", "Emphasis on the importance of security and monitoring in remote judicial processes"], "limitations": "", "keywords": ["videoconference", "judicial hearings", "credibility", "remote testimonies", "access to justice"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.19634", "pdf": "https://arxiv.org/pdf/2507.19634.pdf", "abs": "https://arxiv.org/abs/2507.19634", "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks", "authors": ["Sara Papi", "Maike Züfle", "Marco Gaido", "Beatrice Savoldi", "Danni Liu", "Ioannis Douros", "Luisa Bentivogli", "Jan Niehues"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD"], "comment": "Work in progress", "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.", "AI": {"tldr": "The paper introduces MCIF, a multilingual benchmark for evaluating multimodal instruction-following in large language models across multiple languages and modalities.", "motivation": "To address the inadequacies of existing benchmarks that fail to evaluate multilingual and multimodal capabilities of LLMs in a unified framework.", "method": "MCIF integrates human annotations based on scientific talks to assess instruction-following in crosslingual and multimodal contexts over both short and long-form inputs.", "result": "The benchmark spans three modalities (speech, vision, text) and four languages (English, German, Italian, Chinese), providing a basis for comprehensive evaluation of MLLMs.", "conclusion": "MCIF is intended to advance the research in multilingual multimodal LLMs, offering an open resource for assessment and benchmarking.", "key_contributions": ["Introduction of MCIF, the first multilingual benchmark for MLLMs", "Focus on crosslingual instruction-following in multimodal settings", "Incorporation of human annotations for comprehensive evaluation"], "limitations": "", "keywords": ["multimodal LLMs", "multilingual evaluation", "benchmarking", "instruction-following", "crosslingual"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19497", "pdf": "https://arxiv.org/pdf/2507.19497.pdf", "abs": "https://arxiv.org/abs/2507.19497", "title": "Unlimited Editions: Documenting Human Style in AI Art Generation", "authors": ["Alex Leitch", "Celia Chen"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.IR"], "comment": "alt.CHI 2025", "summary": "As AI art generation becomes increasingly sophisticated, HCI research has\nfocused primarily on questions of detection, authenticity, and automation. This\npaper argues that such approaches fundamentally misunderstand how artistic\nvalue emerges from the concerns that drive human image production. Through\nexamination of historical precedents, we demonstrate that artistic style is not\nonly visual appearance but the resolution of creative struggle, as artists\nwrestle with influence and technical constraints to develop unique ways of\nseeing. Current AI systems flatten these human choices into reproducible\npatterns without preserving their provenance. We propose that HCI's role lies\nnot only in perfecting visual output, but in developing means to document the\norigins and evolution of artistic style as it appears within generated visual\ntraces. This reframing suggests new technical directions for HCI research in\ngenerative AI, focused on automatic documentation of stylistic lineage and\ncreative choice rather than simple reproduction of aesthetic effects.", "AI": {"tldr": "This paper critiques current HCI approaches to AI art generation, emphasizing the need to understand artistic value as stemming from human creative struggle rather than just visual reproduction.", "motivation": "Current HCI research in AI art generation often focuses on detection and authenticity, missing the deeper creative processes involved in artistic expression.", "method": "The paper examines historical precedents in art to argue that artistic style results from a resolution of creative struggles and technical constraints.", "result": "The analysis reveals that AI systems reduce human choice to reproducible patterns, lacking in preserving artistic provenance.", "conclusion": "The authors suggest reorienting HCI research towards automatic documentation of artistic style's origins and evolution to enhance understanding of creativity in AI-generated art.", "key_contributions": ["Critique of existing HCI perspectives on AI art generation.", "Proposal for documenting artistic lineage in generative AI.", "Call for a focus on the creative process rather than just aesthetic outcomes."], "limitations": "", "keywords": ["AI Art", "Human-Computer Interaction", "Artistic Value", "Generative AI", "Creative Process"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.19666", "pdf": "https://arxiv.org/pdf/2507.19666.pdf", "abs": "https://arxiv.org/abs/2507.19666", "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams", "authors": ["Andrei Vlad Man", "Răzvan-Alexandru Smădu", "Cristian-George Craciun", "Dumitru-Clementin Cercel", "Florin Pop", "Mihaela-Claudia Cercel"], "categories": ["cs.CL"], "comment": "49 pages, 52 figures", "summary": "The intersection of AI and legal systems presents a growing need for tools\nthat support legal education, particularly in under-resourced languages such as\nRomanian. In this work, we aim to evaluate the capabilities of Large Language\nModels (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning\nabout Romanian driving law through textual and visual question-answering tasks.\nTo facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising\nRomanian driving test questions, text-based and image-based, alongside\nannotated legal references and human explanations. We implement and assess\nretrieval-augmented generation (RAG) pipelines, dense retrievers, and\nreasoning-optimized models across tasks including Information Retrieval (IR),\nQuestion Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate\nthat domain-specific fine-tuning significantly enhances retrieval performance.\nAt the same time, chain-of-thought prompting and specialized reasoning models\nimprove QA accuracy, surpassing the minimum grades required to pass driving\nexams. However, visual reasoning remains challenging, highlighting the\npotential and the limitations of applying LLMs and VLMs to legal education.", "AI": {"tldr": "This paper evaluates LLMs and VLMs for Romanian driving law through a novel dataset and multimodal tasks.", "motivation": "To address the need for legal education tools in under-resourced languages like Romanian.", "method": "Utilization of RoD-TAL dataset for evaluating LLMs and VLMs in textual and visual question-answering tasks, including RAG pipelines and reasoning-optimized models.", "result": "Domain-specific fine-tuning enhances retrieval performance; specialized reasoning models improve QA accuracy, though visual reasoning remains a challenge.", "conclusion": "While LLMs and VLMs show promise in legal education, limitations exist, particularly in visual reasoning capabilities.", "key_contributions": ["Introduction of RoD-TAL, a multimodal dataset for Romanian driving law assessment.", "Implementation of retrieval-augmented generation and reasoning-optimized models.", "Findings on the effectiveness of domain-specific fine-tuning and chain-of-thought prompting."], "limitations": "Visual reasoning challenges persist when applying LLMs and VLMs to legal education tasks.", "keywords": ["Large Language Models", "Vision-Language Models", "legal education", "Romanian driving law", "multimodal dataset"], "importance_score": 3, "read_time_minutes": 49}}
{"id": "2507.19498", "pdf": "https://arxiv.org/pdf/2507.19498.pdf", "abs": "https://arxiv.org/abs/2507.19498", "title": "ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings", "authors": ["Yue Wu", "Xiaolan Chen", "Weiyi Zhang", "Shunming Liu", "Wing Man Rita Sum", "Xinyuan Wu", "Xianwen Shang", "Chea-su Kee", "Mingguang He", "Danli Shi"], "categories": ["cs.HC", "cs.AI"], "comment": "35 pages, 4 figures, 1 table", "summary": "Large language models (LLMs) show promise for tailored healthcare\ncommunication but face challenges in interpretability and multi-task\nintegration particularly for domain-specific needs like myopia, and their\nreal-world effectiveness as patient education tools has yet to be demonstrated.\nHere, we introduce ChatMyopia, an LLM-based AI agent designed to address text\nand image-based inquiries related to myopia. To achieve this, ChatMyopia\nintegrates an image classification tool and a retrieval-augmented knowledge\nbase built from literature, expert consensus, and clinical guidelines. Myopic\nmaculopathy grading task, single question examination and human evaluations\nvalidated its ability to deliver personalized, accurate, and safe responses to\nmyopia-related inquiries with high scalability and interpretability. In a\nrandomized controlled trial (n=70, NCT06607822), ChatMyopia significantly\nimproved patient satisfaction compared to traditional leaflets, enhancing\npatient education in accuracy, empathy, disease awareness, and patient-eyecare\npractitioner communication. These findings highlight ChatMyopia's potential as\na valuable supplement to enhance patient education and improve satisfaction\nwith medical services in primary eye care settings.", "AI": {"tldr": "ChatMyopia is an LLM-based AI agent for myopia-related inquiries that integrates image classification and a knowledge base, showing improved patient education and satisfaction.", "motivation": "The paper addresses challenges in using LLMs for healthcare communication, particularly for specific domains like myopia, and highlights the lack of real-world effectiveness of existing tools.", "method": "ChatMyopia combines an image classification tool with a retrieval-augmented knowledge base sourced from literature, expert consensus, and clinical guidelines, and is validated through human evaluations and a randomized controlled trial.", "result": "In a trial involving 70 participants, ChatMyopia significantly enhanced patient satisfaction over traditional methods, showing improvements in accuracy, empathy, disease awareness, and communication.", "conclusion": "ChatMyopia emerges as a promising tool to enhance patient education and satisfaction in primary eye care, highlighting its potential utility in clinical settings.", "key_contributions": ["Introduction of ChatMyopia for myopia-related inquiries", "Integration of text and image-based response systems", "Demonstrated effectiveness in improving patient education and satisfaction"], "limitations": "", "keywords": ["Large Language Models", "Myopia", "Patient Education", "Healthcare AI", "Image Classification"], "importance_score": 10, "read_time_minutes": 35}}
{"id": "2507.19699", "pdf": "https://arxiv.org/pdf/2507.19699.pdf", "abs": "https://arxiv.org/abs/2507.19699", "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks", "authors": ["Maitha Alshehhi", "Ahmed Sharshar", "Mohsen Guizani"], "categories": ["cs.CL"], "comment": "Published in the 3rd International Workshop on Generalizing from\n  Limited Resources in the Open World. Workshop at International Joint\n  Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Although LLMs have attained significant success in high-resource languages,\ntheir capacity in low-resource linguistic environments like Kannada and Arabic\nis not yet fully understood. This work benchmarking the performance of\nmultilingual and monolingual Large Language Models (LLMs) across Arabic,\nEnglish, and Indic languages, with particular emphasis on the effects of model\ncompression strategies such as pruning and quantization. Findings shows\nsignificant performance differences driven by linguistic diversity and resource\navailability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.\nWe find that multilingual versions of the model outperform their\nlanguage-specific counterparts across the board, indicating substantial\ncross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in\nmaintaining model accuracy while promoting efficiency, but aggressive pruning\nsignificantly compromises performance, especially in bigger models. Our\nfindings pinpoint key strategies to construct scalable and fair multilingual\nNLP solutions and underscore the need for interventions to address\nhallucination and generalization errors in the low-resource setting.", "AI": {"tldr": "Benchmarking multilingual and monolingual LLMs across Arabic, English, and Indic languages reveals performance differences influenced by linguistic diversity and resource availability, highlighting the advantages of multilingual models and the impacts of model compression strategies.", "motivation": "To understand the performance of LLMs in low-resource languages like Kannada and Arabic and the effects of model compression strategies.", "method": "The study benchmarks multilingual and monolingual LLMs across multiple languages, examining the impact of pruning and quantization on their performance.", "result": "The findings show that multilingual LLMs outperform monolingual counterparts, with quantization maintaining accuracy but aggressive pruning harming performance.", "conclusion": "The research suggests strategies for developing effective multilingual NLP solutions and emphasizes the importance of addressing hallucination and generalization issues in low-resource environments.", "key_contributions": ["Performance benchmarking of multilingual vs. monolingual LLMs", "Insights into model compression effects on LLMs", "Recommendations for scalable multilingual NLP solutions"], "limitations": "Focus on specific languages may not generalize; the potential for hallucination and generalization errors in low-resource contexts needs further research.", "keywords": ["Large Language Models", "multilingual NLP", "model compression", "pruning", "quantization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.19500", "pdf": "https://arxiv.org/pdf/2507.19500.pdf", "abs": "https://arxiv.org/abs/2507.19500", "title": "Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems", "authors": ["Omkar Suresh Hatti"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The proliferation of artificial intelligence provides an opportunity to\ncreate psychological spaciousness in society. Spaciousness is defined as the\nability to hold diverse interpersonal interactions and forms the basis for\nvulnerability that leads to authenticity that leads to prosocial behaviors and\nthus to societal harmony. This paper demonstrates an attempt to quantify, the\nhuman conditioning to subconsciously modify authentic self-expression to fit\nthe norms of the dominant culture. Gaze is explored across various marginalized\nand intersectional groups, using concepts from postmodern philosophy and\npsychology. The effects of gaze are studied through analyzing a few redacted\nReddit posts, only to be discussed in discourse and not endorsement. A\nmathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite\nMetric is presented to model the analysis of two sets of conversational spaces\nin relation to one another. The outcome includes an equation to train Large\nLanguage Models (LLMs) - the working mechanism of AI products such as Chat-GPT;\nand an argument for affirming and inclusive HCI, based on the equation, is\npresented. The argument is supported by a few principles of Neuro-plasticity,\nThe brain's lifelong capacity to rewire.", "AI": {"tldr": "This paper explores the concept of psychological spaciousness in society, using gaze analysis to quantify how individuals modify authentic self-expression to conform to cultural norms. It introduces the Gaze Pressure Index (GPI)-Diff Composite Metric to model conversational spaces, and provides an equation intended for training Large Language Models (LLMs), advocating for inclusive Human-Computer Interaction (HCI) based on neuro-plasticity principles.", "motivation": "To quantify how individuals subconsciously modify their authentic self-expression in response to societal norms, and to advocate for more inclusive HCI practices.", "method": "The paper analyzes gaze across marginalized and intersectional groups using postmodern philosophy and psychology, employing a mathematical formulation to develop the Gaze Pressure Index (GPI)-Diff Composite Metric.", "result": "The study results in an equation designed to enhance the training of Large Language Models (LLMs), contributing to the conversation on inclusive HCI.", "conclusion": "Affirmative and inclusive HCI should be prioritized, supported by neuro-plasticity principles that showcase the brain's ability to adapt and rewire.", "key_contributions": ["Development of the Gaze Pressure Index (GPI)-Diff Composite Metric", "Introduction of a mathematical equation for training LLMs", "Advocacy for affirming and inclusive HCI practices based on psychological concepts."], "limitations": "The analysis is based on a limited number of redacted Reddit posts and does not endorse their content.", "keywords": ["artificial intelligence", "gaze", "HCI", "neuro-plasticity", "inclusive design"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.19710", "pdf": "https://arxiv.org/pdf/2507.19710.pdf", "abs": "https://arxiv.org/abs/2507.19710", "title": "Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs", "authors": ["Ronak Upasham", "Tathagata Dey", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "In Table-to-Text (T2T) generation, existing approaches predominantly focus on\nproviding objective descriptions of tabular data. However, generating text that\nincorporates subjectivity, where subjectivity refers to interpretations beyond\nraw numerical data, remains underexplored. To address this, we introduce a\nnovel pipeline that leverages intermediate representations to generate both\nobjective and subjective text from tables. Our three-stage pipeline consists\nof: 1) extraction of Resource Description Framework (RDF) triples, 2)\naggregation of text into coherent narratives, and 3) infusion of subjectivity\nto enrich the generated text. By incorporating RDFs, our approach enhances\nfactual accuracy while maintaining interpretability. Unlike large language\nmodels (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs\nsmaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5\nand outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our\napproach through quantitative and qualitative analyses, demonstrating its\neffectiveness in balancing factual accuracy with subjective interpretation. To\nthe best of our knowledge, this is the first work to propose a structured\npipeline for T2T generation that integrates intermediate representations to\nenhance both factual correctness and subjectivity.", "AI": {"tldr": "This paper introduces a novel pipeline for Table-to-Text generation that incorporates both objective descriptions and subjective interpretations of tabular data.", "motivation": "Existing approaches in Table-to-Text generation mainly focus on objective descriptions, leaving the incorporation of subjectivity largely unexplored.", "method": "The proposed three-stage pipeline includes extraction of RDF triples, aggregation of text into coherent narratives, and infusion of subjectivity to enhance generated text.", "result": "The pipeline utilizes smaller, fine-tuned T5 models and demonstrates comparable performance to GPT-3.5 while outperforming other models like Mistral-7B and Llama-2 in several metrics.", "conclusion": "This work is the first to propose a structured pipeline for T2T generation that enhances both factual correctness and subjectivity through intermediate representations.", "key_contributions": ["Novel three-stage pipeline for Table-to-Text generation", "Incorporation of subjectivity in text generation", "Utilization of RDF triples for enhanced factual accuracy"], "limitations": "", "keywords": ["Table-to-Text", "subjectivity", "RDF", "T5 models", "natural language generation"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.19690", "pdf": "https://arxiv.org/pdf/2507.19690.pdf", "abs": "https://arxiv.org/abs/2507.19690", "title": "Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems", "authors": ["Jeffrey Heer", "Dominik Moritz", "Ron Pechuk"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Though powerful tools for analysis and communication, interactive\nvisualizations often fail to support real-time interaction with large datasets\nwith millions or more records. To highlight and filter data, users indicate\nvalues or intervals of interest. Such selections may span multiple components,\ncombine in complex ways, and require optimizations to ensure low-latency\nupdates. We describe Mosaic Selections, a model for representing, managing, and\noptimizing user selections, in which one or more filter predicates are added to\nqueries that request data for visualizations and input widgets. By analyzing\nboth queries and selection predicates, Mosaic Selections enable automatic\noptimizations, including pre-aggregating data to rapidly compute selection\nupdates. We contribute a formal description of our selection model and\noptimization methods, and their implementation in the open-source Mosaic\narchitecture. Benchmark results demonstrate orders-of-magnitude latency\nimprovements for selection-based optimizations over unoptimized queries and\nexisting optimizers for the Vega language. The Mosaic Selection model provides\ninfrastructure for flexible, interoperable filtering across multiple\nvisualizations, alongside automatic optimizations to scale to millions and even\nbillions of records.", "AI": {"tldr": "This paper presents the Mosaic Selections model for improving real-time interaction with large datasets in visualizations, enabling efficient user selection management and optimization.", "motivation": "The need for efficient real-time interaction with large datasets in interactive visualizations, which often struggle with latency issues due to complex user selections.", "method": "The Mosaic Selections model incorporates one or more filter predicates into data queries for visualizations and input widgets, allowing for automatic optimizations such as pre-aggregation of data for quick selection updates.", "result": "Benchmarking shows that Mosaic Selections can achieve orders-of-magnitude latency improvements compared to unoptimized queries and existing optimizers in the Vega language.", "conclusion": "Mosaic Selections provide a robust framework for flexible filtering in visualizations while maintaining efficiency even with millions or billions of records.", "key_contributions": ["Formal description of the Mosaic Selections model", "Implementation of the model in the open-source Mosaic architecture", "Significant latency improvements for selection-based optimizations."], "limitations": "", "keywords": ["interactive visualizations", "real-time interaction", "data optimization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.19741", "pdf": "https://arxiv.org/pdf/2507.19741.pdf", "abs": "https://arxiv.org/abs/2507.19741", "title": "Basic Reading Distillation", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation.", "AI": {"tldr": "The paper introduces basic reading distillation (BRD), a method for training small models to imitate basic reading behaviors of large language models (LLMs) for improved performance on NLP tasks without the high resource requirements of LLMs.", "motivation": "To address the high computational resource demands of LLMs and improve the performance of smaller models in natural language processing tasks.", "method": "Basic reading distillation (BRD) involves educating small models to replicate fundamental reading behaviors of LLMs, such as named entity recognition and question answering, on generic texts.", "result": "The small model trained with BRD outperformed or matched the performance of models over 20 times its size on various benchmarks, showing significant competitive advantages.", "conclusion": "BRD effectively alters the probability distribution of the small model, providing insight into model training and demonstrating orthogonality to existing distillation methods.", "key_contributions": ["Introduction of basic reading distillation (BRD) as a novel training method for small models", "Demonstrated that small models can achieve competitive performance against much larger LLMs", "Provided insights into the effective training strategies influencing small model capabilities."], "limitations": "", "keywords": ["large language models", "distillation", "natural language processing", "small models", "basic reading behaviors"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19736", "pdf": "https://arxiv.org/pdf/2507.19736.pdf", "abs": "https://arxiv.org/abs/2507.19736", "title": "LowKeyEMG: Electromyographic typing with a reduced keyset", "authors": ["Johannes Y. Lee", "Derek Xiao", "Shreyas Kaasyap", "Nima R. Hadidi", "John L. Zhou", "Jacob Cunningham", "Rakshith R. Gore", "Deniz O. Eren", "Jonathan C. Kao"], "categories": ["cs.HC", "eess.SP"], "comment": "11+3 pages, 5 main figures, 2 supplementary tables, 4 supplementary\n  figures", "summary": "We introduce LowKeyEMG, a real-time human-computer interface that enables\nefficient text entry using only 7 gesture classes decoded from surface\nelectromyography (sEMG). Prior work has attempted full-alphabet decoding from\nsEMG, but decoding large character sets remains unreliable, especially for\nindividuals with motor impairments. Instead, LowKeyEMG reduces the English\nalphabet to 4 gesture keys, with 3 more for space and system interaction, to\nreliably translate simple one-handed gestures into text, leveraging the\nrecurrent transformer-based language model RWKV for efficient computation. In\nreal-time experiments, participants achieved average one-handed keyboardless\ntyping speeds of 23.3 words per minute with LowKeyEMG, and improved gesture\nefficiency by 17% (relative to typed phrase length). When typing with only 7\nkeys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this\nlow-key typing paradigm can maintain practical communication rates. Our results\nhave implications for assistive technologies and any interface where input\nbandwidth is constrained.", "AI": {"tldr": "LowKeyEMG presents a real-time text entry interface using 7 gesture classes from sEMG, achieving efficient and reliable communication for individuals with motor impairments.", "motivation": "To improve text entry for individuals with motor impairments, focusing on reliability where full-alphabet sEMG decoding has been insufficient.", "method": "Develops a real-time interface that simplifies the English alphabet to 4 gesture keys plus 3 for space/system, using the RWKV language model for efficient computation.", "result": "Participants achieved an average typing speed of 23.3 words per minute and 98.2% top-3 word accuracy, with a 17% increase in gesture efficiency compared to traditional typing.", "conclusion": "LowKeyEMG enables practical communication rates for users with constrained input capabilities, supporting assistive technology applications.", "key_contributions": ["Introduction of a low-key gesture-based typing paradigm for sEMG", "Efficiency improvements in gesture recognition and typing speed", "High accuracy rates and relevance for assistive technologies"], "limitations": "", "keywords": ["sEMG", "gesture recognition", "human-computer interaction", "assistive technology", "language model"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19748", "pdf": "https://arxiv.org/pdf/2507.19748.pdf", "abs": "https://arxiv.org/abs/2507.19748", "title": "JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models", "authors": ["Yifan Hao", "Fangning Chao", "Yaqian Hao", "Zhaojun Cui", "Huan Bai", "Haiyu Zhang", "Yankai Liu", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Mathematical reasoning is a cornerstone of artificial general intelligence\nand a primary benchmark for evaluating the capabilities of Large Language\nModels (LLMs). While state-of-the-art models show promise, they often falter\nwhen faced with complex problems that demand deep conceptual understanding and\nintricate, multi-step deliberation. To address this challenge, we introduce\nJT-Math-8B, a series of open-source models comprising base, instruct, and\nthinking versions, built upon a systematic, multi-stage optimization framework.\nOur pre-training corpus is a high-quality, 210B-token dataset curated through a\ndedicated data pipeline that uses model-based validation to ensure quality and\ndiversity. The Instruct Model is optimized for direct, concise answers through\nSupervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)\nmethod. The Thinking Model is trained for complex problem-solving using a Long\nChain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage\nRL curriculum that progressively increases task difficulty and context length\nup to 32K tokens. JT-Math-8B achieves state-of-the-art results among\nopen-source models of similar size, surpassing prominent models like OpenAI's\nO1-mini and GPT-4o , and demonstrating superior performance on\ncompetition-level mathematics.", "AI": {"tldr": "JT-Math-8B introduces new open-source models designed for mathematical reasoning, showing high performance on complex problems.", "motivation": "Addressing the limitations of current state-of-the-art LLMs in solving complex mathematical problems requiring deep understanding.", "method": "The models consist of base, instruct, and thinking versions, developed through a multi-stage optimization framework and trained on a 210B-token dataset with a focus on quality and diversity.", "result": "Achieves state-of-the-art results surpassing other models like OpenAI's O1-mini and GPT-4o on complex mathematical tasks.", "conclusion": "JT-Math-8B demonstrates superior performance in mathematical reasoning, paving the way for advancements in applied LLMs for educational and other cognitive tasks.", "key_contributions": ["Introduction of JT-Math-8B as a series of open-source models for mathematical reasoning.", "Implementation of a Long Chain-of-Thought training approach for complex problem-solving.", "Utilization of a novel, multi-stage reinforcement learning curriculum."], "limitations": "", "keywords": ["Mathematics", "Large Language Models", "Open-source", "Reinforcement Learning", "AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19782", "pdf": "https://arxiv.org/pdf/2507.19782.pdf", "abs": "https://arxiv.org/abs/2507.19782", "title": "KinemaFX: A Kinematic-Driven Interactive System for Particle Effect Exploration and Customization", "authors": ["Yifei Zhang", "Lin-Ping Yuan", "Yuheng Zhao", "Jielin Feng", "Siming Chen"], "categories": ["cs.HC"], "comment": "Meta Review Overall Rating 3.5 Weakly Accept Contribution to HCI This\n  paper presents KinemaFX, an LLM-powered interactive system leveraging\n  semantic and kinematic inputs to help non-experts explore, customize, and\n  compose particle effects", "summary": "Particle effects are widely used in games and animation to simulate natural\nphenomena or stylized visual effects. However, creating effect artworks is\nchallenging for non-expert users due to their lack of specialized skills,\nparticularly in finding particle effects with kinematic behaviors that match\ntheir intent. To address these issues, we present KinemaFX, a kinematic-driven\ninteractive system, to assist non-expert users in constructing customized\nparticle effect artworks. We propose a conceptual model of particle effects\nthat captures both semantic features and kinematic behaviors. Based on the\nmodel, KinemaFX adopts a workflow powered by Large Language Models (LLMs) that\nsupports intent expression through combined semantic and kinematic inputs,\nwhile enabling implicit preference-guided exploration and subsequent creation\nof customized particle effect artworks based on exploration results.\nAdditionally, we developed a kinematic-driven method to facilitate efficient\ninteractive particle effect search within KinemaFX via structured\nrepresentation and measurement of particle effects. To evaluate KinemaFX, we\nillustrate usage scenarios and conduct a user study employing an ablation\napproach. Evaluation results demonstrate that KinemaFX effectively supports\nusers in efficiently and customarily creating particle effect artworks.", "AI": {"tldr": "KinemaFX is an LLM-driven system that aids non-expert users in creating customized particle effects by utilizing a conceptual model for semantic and kinematic inputs.", "motivation": "To assist non-expert users in creating effect artworks, which is challenging due to a lack of specialized skills in matching particle effects to their intent.", "method": "KinemaFX utilizes Large Language Models to express intent through semantic and kinematic inputs, enabling guided exploration and creation of particle effects.", "result": "Evaluation shows that KinemaFX effectively supports users in efficiently creating customized particle effect artworks.", "conclusion": "KinemaFX provides a helpful framework for non-experts to construct particle effects, enhancing accessibility and creativity in this field.", "key_contributions": ["Introduction of a conceptual model for particle effects", "Development of an LLM-powered interactive system", "Demonstration of user efficiency in creating artworks"], "limitations": "Limited to non-expert user scenarios; further studies needed to assess expert use cases.", "keywords": ["Human-Computer Interaction", "Particle Effects", "Large Language Models", "Interactive Systems", "Customization"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.19756", "pdf": "https://arxiv.org/pdf/2507.19756.pdf", "abs": "https://arxiv.org/abs/2507.19756", "title": "Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs", "authors": ["Rebecca M. M. Hicke", "Brian Haggard", "Mia Ferrante", "Rayhan Khanna", "David Mimno"], "categories": ["cs.CL"], "comment": null, "summary": "In addition to its more widely studied political activities, the American\nEvangelical movement has a well-developed but less externally visible cultural\nand literary side. Christian Fiction, however, has been little studied, and\nwhat scholarly attention there is has focused on the explosively popular Left\nBehind series. In this work, we use computational tools to provide both a broad\ntopical overview of Christian Fiction as a genre and a more directed\nexploration of how its authors depict divine acts. Working with human\nannotators we first developed definitions and a codebook for \"acts of God.\" We\nthen adapted those instructions designed for human annotators for use by a\nrecent, lightweight LM with the assistance of a much larger model. The\nlaptop-scale LM is capable of matching human annotations, even when the task is\nsubtle and challenging. Using these annotations, we show that significant and\nmeaningful differences exist between the Left Behind books and Christian\nFiction more broadly and between books by male and female authors.", "AI": {"tldr": "This paper explores Christian Fiction using computational tools to analyze divine acts, comparing the Left Behind series with other works in the genre.", "motivation": "To address the lack of scholarly attention on Christian Fiction, particularly on how divine acts are depicted in the genre.", "method": "The authors developed a codebook for 'acts of God' with human annotators and adapted it for use by a lightweight language model, which showed capability in matching human annotations.", "result": "The analysis revealed significant differences in the depiction of divine acts between the Left Behind series and broader Christian Fiction, as well as between male and female authors.", "conclusion": "Computational tools can enhance the study of under-researched literary genres such as Christian Fiction, offering insights into thematic differences.", "key_contributions": ["Developed a codebook for 'acts of God' in Christian Fiction", "Demonstrated the effectiveness of a lightweight language model in matching human annotations", "Highlighted significant differences in themes between the Left Behind series and other works."], "limitations": "", "keywords": ["Christian Fiction", "Acts of God", "Computational analysis", "Left Behind series", "Literary study"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.19898", "pdf": "https://arxiv.org/pdf/2507.19898.pdf", "abs": "https://arxiv.org/abs/2507.19898", "title": "TS-Insight: Visualizing Thompson Sampling for Verification and XAI", "authors": ["Parsa Vares", "Éloi Durant", "Jun Pang", "Nicolas Médoc", "Mohammad Ghoniem"], "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "I.2.6; H.5.2"], "comment": "Accepted as a poster at IEEE VIS 2025 (\"TS-Insight: Visual\n  Fingerprinting of Multi-Armed Bandits\"). Open-source tool available at\n  https://github.com/parsavares/ts-insight", "summary": "Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit\nalgorithms used to balance exploration and exploitation strategies in active\nlearning. Yet, their probabilistic nature often turns them into a ``black\nbox'', hindering debugging and trust. We introduce TS-Insight, a visual\nanalytics tool explicitly designed to shed light on the internal decision\nmechanisms of Thompson Sampling-based algorithms, for model developers. It\ncomprises multiple plots, tracing for each arm the evolving posteriors,\nevidence counts, and sampling outcomes, enabling the verification, diagnosis,\nand explainability of exploration/exploitation dynamics. This tool aims at\nfostering trust and facilitating effective debugging and deployment in complex\nbinary decision-making scenarios especially in sensitive domains requiring\ninterpretable decision-making.", "AI": {"tldr": "TS-Insight is a visual analytics tool designed to improve interpretability and debugging of Thompson Sampling algorithms in active learning.", "motivation": "To address the opacity of Thompson Sampling algorithms by providing a visual tool that enhances understanding and trust in the decision-making process.", "method": "The tool provides various plots that track evolving posteriors, evidence counts, and sampling outcomes for each arm in the algorithm.", "result": "TS-Insight enables better debugging and verification of exploration/exploitation processes in Thompson Sampling.", "conclusion": "By enhancing the explainability of these algorithms, TS-Insight promotes trust and facilitates their application in sensitive domains requiring interpretable decision-making.", "key_contributions": ["Introduction of a visual analytics tool for Thompson Sampling algorithms", "Enables tracking of decision-making metrics in real-time", "Fosters trust and facilitates debugging in complex decision-making scenarios"], "limitations": "", "keywords": ["Thompson Sampling", "visual analytics", "active learning", "explainability", "debugging"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.19766", "pdf": "https://arxiv.org/pdf/2507.19766.pdf", "abs": "https://arxiv.org/abs/2507.19766", "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.", "AI": {"tldr": "This paper presents an Ultra-Long Output Reinforcement Learning (UloRL) approach to enhance LLM reasoning capabilities through efficient training of ultra-long outputs.", "motivation": "To improve reasoning in large language models (LLMs) and address inefficiencies in traditional reinforcement learning when dealing with ultra-long output sequences.", "method": "The proposed UloRL divides ultra-long output decoding into short segments, thereby reducing delays and introducing dynamic masking of well-Mastered Positive Tokens to avoid entropy collapse during training.", "result": "The UloRL approach resulted in a 2.06x increase in training speed and improved performance on AIME2025 from 70.9% to 85.1% and on BeyondAIME from 50.7% to 61.9%, surpassing existing models in effectiveness.", "conclusion": "The methods proposed present significant advancements in enhancing LLM reasoning capabilities with ultra-long sequence generation, and the authors will release their code and model for community use.", "key_contributions": ["Introduction of Ultra-Long Output Reinforcement Learning (UloRL) approach.", "Dynamic masking of well-Mastered Positive Tokens to prevent entropy collapse.", "Strategies for efficient training with ultra-long outputs."], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Ultra-Long Outputs"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.19988", "pdf": "https://arxiv.org/pdf/2507.19988.pdf", "abs": "https://arxiv.org/abs/2507.19988", "title": "Visual Analytics Using Tensor Unified Linear Comparative Analysis", "authors": ["Naoki Okami", "Kazuki Miyake", "Naohisa Sakamoto", "Jorji Nonaka", "Takanori Fujiwara"], "categories": ["cs.HC", "cs.GR", "cs.LG", "I.3.8; H.5.2"], "comment": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  and IEEE VIS 2025", "summary": "Comparing tensors and identifying their (dis)similar structures is\nfundamental in understanding the underlying phenomena for complex data. Tensor\ndecomposition methods help analysts extract tensors' essential characteristics\nand aid in visual analytics for tensors. In contrast to dimensionality\nreduction (DR) methods designed only for analyzing a matrix (i.e., second-order\ntensor), existing tensor decomposition methods do not support flexible\ncomparative analysis. To address this analysis limitation, we introduce a new\ntensor decomposition method, named tensor unified linear comparative analysis\n(TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA\nintegrates discriminant analysis and contrastive learning schemes for tensor\ndecomposition, enabling flexible comparison of tensors. We also introduce an\neffective method to visualize a core tensor extracted from TULCA into a set of\n2D visualizations. We integrate TULCA's functionalities into a visual analytics\ninterface to support analysts in interpreting and refining the TULCA results.\nWe demonstrate the efficacy of TULCA and the visual analytics interface with\ncomputational evaluations and two case studies, including an analysis of log\ndata collected from a supercomputer.", "AI": {"tldr": "This paper introduces TULCA, a tensor decomposition method that facilitates comparative analysis of tensors through discriminant analysis and contrastive learning, and presents a visual analytics interface for interpreting results.", "motivation": "Understanding complex data through tensor comparison necessitates improved decomposition methods that support flexible analysis.", "method": "The proposed TULCA method extends traditional dimensionality reduction techniques to tensors, integrating discriminant analysis and contrastive learning for effective tensor decomposition.", "result": "TULCA allows for flexible comparison of tensors and provides a means to visualize results through 2D representations.", "conclusion": "The effectiveness of TULCA and its visual analytics interface is validated through computational evaluations and case studies involving log data from supercomputers.", "key_contributions": ["Introduction of TULCA for tensor analysis", "Integration of discriminant analysis and contrastive learning for tensor decomposition", "Development of a visual analytics interface for interpreting TULCA results"], "limitations": "", "keywords": ["tensor decomposition", "visual analytics", "discriminant analysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.19786", "pdf": "https://arxiv.org/pdf/2507.19786.pdf", "abs": "https://arxiv.org/abs/2507.19786", "title": "Flora: Effortless Context Construction to Arbitrary Length and Scale", "authors": ["Tianxiang Chen", "Zhentao Tan", "Xiaofan Bo", "Yue Wu", "Tao Gong", "Qi Chu", "Jieping Ye", "Nenghai Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Effectively handling long contexts is challenging for Large Language Models\n(LLMs) due to the rarity of long texts, high computational demands, and\nsubstantial forgetting of short-context abilities. Recent approaches have\nattempted to construct long contexts for instruction tuning, but these methods\noften require LLMs or human interventions, which are both costly and limited in\nlength and diversity. Also, the drop in short-context performances of present\nlong-context LLMs remains significant. In this paper, we introduce Flora, an\neffortless (human/LLM-free) long-context construction strategy. Flora can\nmarkedly enhance the long-context performance of LLMs by arbitrarily assembling\nshort instructions based on categories and instructing LLMs to generate\nresponses based on long-context meta-instructions. This enables Flora to\nproduce contexts of arbitrary length and scale with rich diversity, while only\nslightly compromising short-context performance. Experiments on\nLlama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three\nlong-context benchmarks while maintaining strong performances in short-context\ntasks. Our data-construction code is available at\n\\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.", "AI": {"tldr": "Flora is a strategy for constructing long contexts for LLMs without human or LLM interventions, improving long-context performance while maintaining short-context abilities.", "motivation": "Address the challenges LLMs face with long contexts, including computational demands and the degradation of short-context capabilities.", "method": "Flora assembles short instructions into long contexts through effective categorization and generates diverse responses based on long-context meta-instructions.", "result": "Experiments show that LLMs using Flora perform well on long-context benchmarks while retaining strong short-context performance.", "conclusion": "Flora presents a cost-effective and efficient method for enhancing LLM capabilities in handling long contexts without sacrificing short-context performance.", "key_contributions": ["Introduced Flora for long-context construction", "Improves long-context performance of LLMs", "Maintains short-context performance with diverse generation"], "limitations": "The approach may still have some trade-offs in the richness of short-context outputs despite overall performance benefits.", "keywords": ["Long-context", "LLM", "Instruction tuning", "Machine learning", "Natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20006", "pdf": "https://arxiv.org/pdf/2507.20006.pdf", "abs": "https://arxiv.org/abs/2507.20006", "title": "Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques", "authors": ["Jun-Hsiang Yao", "Jielin Feng", "Xinfang Tian", "Kai Xu", "Gulshat Amirkhanova", "Siming Chen"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures", "summary": "Virtual Reality (VR) broadcasting has emerged as a promising medium for\nproviding immersive viewing experiences of major sports events such as tennis.\nHowever, current VR broadcast systems often lack an effective camera language\nand do not adequately incorporate dynamic, in-game visualizations, limiting\nviewer engagement and narrative clarity. To address these limitations, we\nanalyze 400 out-of-play segments from eight major tennis broadcasts to develop\na tennis-specific design framework that effectively combines cinematic camera\nmovements with embedded visualizations. We further refine our framework by\nexamining 25 cinematic VR animations, comparing their camera techniques with\ntraditional tennis broadcasts to identify key differences and inform\nadaptations for VR. Based on data extracted from the broadcast videos, we\nreconstruct a simulated game that captures the players' and ball's motion and\ntrajectories. Leveraging this design framework and processing pipeline, we\ndevelope Beyond the Broadcast, a VR tennis viewing system that integrates\nembedded visualizations with adaptive camera motions to construct a\ncomprehensive and engaging narrative. Our system dynamically overlays tactical\ninformation and key match events onto the simulated environment, enhancing\nviewer comprehension and narrative engagement while ensuring perceptual\nimmersion and viewing comfort. A user study involving tennis viewers\ndemonstrate that our approach outperforms traditional VR broadcasting methods\nin delivering an immersive, informative viewing experience.", "AI": {"tldr": "The paper presents a VR tennis viewing system that enhances viewer engagement through dynamic camera movements and embedded visualizations, outperforming traditional VR broadcasting methods.", "motivation": "To improve viewer engagement and narrative clarity in VR broadcasts of sports events, particularly tennis, which currently lack effective camera language and dynamic visualizations.", "method": "The authors analyzed 400 out-of-play segments from major tennis broadcasts and 25 cinematic VR animations to develop a tennis-specific design framework that combines cinematic techniques with adaptive camera movements. They created a VR viewing system called Beyond the Broadcast that integrates these elements.", "result": "The system was evaluated through a user study, indicating that it offers a more immersive and informative viewing experience compared to traditional VR broadcasting methods.", "conclusion": "Beyond the Broadcast significantly enhances viewer comprehension and engagement in VR tennis broadcasts by incorporating tactical information and key match events while maintaining immersion and comfort.", "key_contributions": ["Development of a tennis-specific design framework for VR broadcasting.", "Creation of Beyond the Broadcast, a VR tennis viewing system integrating dynamic visuals and camera techniques.", "User study demonstrating superior performance in engagement and comprehension compared to traditional methods."], "limitations": "The study focuses specifically on tennis and may not generalize to other sports broadcasts. Limited diversity in user feedback based on the sports background of participants may affect broader applicability.", "keywords": ["Virtual Reality", "Tennis Broadcasting", "User Engagement", "Cinematic Techniques", "Adaptive Visualizations"], "importance_score": 6, "read_time_minutes": 11}}
{"id": "2507.19823", "pdf": "https://arxiv.org/pdf/2507.19823.pdf", "abs": "https://arxiv.org/abs/2507.19823", "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs", "authors": ["Dongquan Yang", "Yifan Yang", "Xiaotian Yu", "Xianbiao Qi", "Rong Xiao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.", "AI": {"tldr": "HCAttention is a framework for efficient long-context inference in large language models that enhances KV cache memory usage without model fine-tuning.", "motivation": "To address the memory challenges in processing long-context inputs in large language models, particularly with respect to KV cache compression methods that degrade significantly when memory is highly reduced.", "method": "HCAttention employs key quantization, value offloading, and dynamic KV eviction for efficient inference under extreme memory constraints.", "result": "HCAttention achieves accurate performance while reducing the KV cache memory footprint to 25% of its original size, competing effectively at 12.5% cache size, setting a new state-of-the-art in LLM KV cache compression.", "conclusion": "HCAttention allows the Llama-3-8B model to handle 4 million tokens on a single A100 GPU, making it an efficient choice for long-context inputs without requiring model fine-tuning.", "key_contributions": ["Introduction of HCAttention framework for KV cache compression", "Enabling 4 million tokens processing on a single A100 GPU", "Setting a new state-of-the-art for LLM KV cache memory efficiency"], "limitations": "", "keywords": ["large language models", "KV cache", "attention computation", "memory efficiency", "HCAttention"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20137", "pdf": "https://arxiv.org/pdf/2507.20137.pdf", "abs": "https://arxiv.org/abs/2507.20137", "title": "Dynamite: Real-Time Debriefing Slide Authoring through AI-Enhanced Multimodal Interaction", "authors": ["Panayu Keelawat", "David Barron", "Kaushik Narasimhan", "Daniel Manesh", "Xiaohang Tang", "Xi Chen", "Sang Won Lee", "Yan Chen"], "categories": ["cs.HC"], "comment": "Accepted to VL/HCC 2025", "summary": "Facilitating class-wide debriefings after small-group discussions is a common\nstrategy in ethics education. Instructor interviews revealed that effective\ndebriefings should highlight frequently discussed themes and surface\nunderrepresented viewpoints, making accurate representations of insight\noccurrence essential. Yet authoring presentations in real time is cognitively\noverwhelming due to the volume of data and tight time constraints. We present\nDynamite, an AI-assisted system that enables semantic updates to\ninstructor-authored slides during live classroom discussions. These updates are\npowered by semantic data binding, which links slide content to evolving\ndiscussion data, and semantic suggestions, which offer revision options aligned\nwith pedagogical goals. In a within-subject in-lab study with 12 participants,\nDynamite outperformed a text-based AI baseline in content accuracy and quality.\nParticipants used voice and sketch input to quickly organize semantic blocks,\nthen applied suggestions to accelerate refinement as data stabilized.", "AI": {"tldr": "Dynamite is an AI-assisted system for real-time updates of instructor slides during classroom discussions, enhancing the accuracy and relevancy of debriefing content.", "motivation": "Effective debriefing in ethics education requires accurate representation of discussions, but real-time slide updates are cognitively taxing for instructors.", "method": "The study introduces Dynamite, which utilizes semantic data binding and suggestions to facilitate rapid updates to slides during discussions. A within-subject in-lab study assessed its effectiveness against a text-based AI baseline.", "result": "Dynamite significantly improved content accuracy and quality compared to traditional methods, allowing faster organization and refinement of information during discussions.", "conclusion": "The AI-assisted system enhances the quality of instructional debriefings, making it easier for instructors to present relevant themes and diverse viewpoints.", "key_contributions": ["Introduction of an AI-assisted system for live classroom discussions", "Improvements in content accuracy and quality compared to traditional methods", "Utilization of voice and sketch input for quick semantic updates"], "limitations": "", "keywords": ["AI-assisted teaching", "real-time updates", "classroom discussions"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.19867", "pdf": "https://arxiv.org/pdf/2507.19867.pdf", "abs": "https://arxiv.org/abs/2507.19867", "title": "DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments", "authors": ["Anshul Chavda", "M Jagadeesh", "Chintalapalli Raja Kullayappa", "B Jayaprakash", "Medchalimi Sruthi", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "In-car conversational AI is becoming increasingly critical as autonomous\nvehicles and smart assistants gain widespread adoption. Yet, existing datasets\nfail to capture the spontaneous disfluencies such as hesitations, false starts,\nrepetitions, and self-corrections that characterize real driver-AI dialogs. To\naddress this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn\ndialogs across seven automotive domains, generated using a two-stage,\nprompt-driven pipeline that dynamically integrates disfluencies during\nsynthesis. We show that DiscoDrive is effective both as a training resource,\nenabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on\nthe MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4\nimprovements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1\nimprovements of 1.35 to 3.48), and as a data augmentation resource in\nlow-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,\nMETEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10\npercent of KVRET. Human evaluations further confirm that dialogs sampled from\nDiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness\n(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more\ncontext-appropriate than leading post-hoc methods (such as LARD), without\ncompromising clarity. DiscoDrive fills a critical gap in existing resources and\nserves as a versatile corpus for both training and augmenting conversational\nAI, enabling robust handling of real-world, disfluent in-car interactions.", "AI": {"tldr": "Introduction of DiscoDrive, a synthetic corpus for in-car conversational AI focusing on spontaneous disfluencies.", "motivation": "To address the lack of datasets capturing spontaneous disfluencies in driver-AI conversations, which are critical for developing realistic conversational AI in autonomous vehicles.", "method": "DiscoDrive is generated using a two-stage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis, resulting in a corpus of 3500 multi-turn dialogs across seven automotive domains.", "result": "DiscoDrive enabled models like DialoGPT-Medium and T5-Base to surpass KVRET-trained models in specific test sets, with notable BLEU-4, METEOR, ROUGE-L, and BERTScore F1 improvements. Human evaluations rated DiscoDrive dialogs higher in naturalness and coherence compared to KVRET.", "conclusion": "DiscoDrive is a vital resource for training and augmenting conversational AI, addressing disfluency in real-world, in-car interactions effectively.", "key_contributions": ["Introduction of DiscoDrive corpus for in-car dialogs", "Demonstrated effectiveness for training conversational AI", "Human evaluations show improved dialog quality"], "limitations": "", "keywords": ["Conversational AI", "Disfluencies", "Dataset Generation", "Autonomous Vehicles", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.20261", "pdf": "https://arxiv.org/pdf/2507.20261.pdf", "abs": "https://arxiv.org/abs/2507.20261", "title": "Occupational Safety within Non-Routine Manufacturing Processes: Evaluating the Validity of Task-Based Ergonomic Assessments", "authors": ["Charu Tripathi", "Manish Arora", "Amaresh Chakrabarti"], "categories": ["cs.HC"], "comment": null, "summary": "Direct measurement ergonomic assessment is reshaping occupational safety by\nfacilitating highly reliable risk estimation. Industry 5.0, advocating\nhuman-centricity, has catalysed increasing adoption of direct measurement tools\nin manufacturing industries. However, due to technical and feasibility\nconstraints in their practical implementations, especially within non routine\nmanufacturing processes, task based approach to ergonomic assessment is\nutilized. Despite enabling operationalization of robust ergonomic assessment\ntechnologies within complicated industrial processes, task based approach\nraises several validity concerns. Hence, to ascertain functional utility of the\nresultant safety interventions, this study evaluates the construct validity of\ntask based ergonomic assessment within non routine work utilizing Multitrait\nmultimethod (MTMM) matrix followed by video-based content analysis. Ergonomic\nexposure traits were collected for 46 participants through direct measurement\nand self reported techniques utilizing inertial motion capture and Borg's RPE\nrating scale respectively. Findings include unsubstantiated convergent validity\n(low same trait correlations from 0.149 to 0.243) and weak evidence of\ndiscriminant validity with statistical significance (p value less than 0.001).\nThe study also identifies three primary factors undermining construct validity\nthrough video based content analysis. Findings also elucidate misinterpretation\nof ergonomic risk and action levels. Therefore, practical implications entail\nunderestimation of actual ergonomic risks when estimated through task based\nassessment. This highlights the need for enhancement in ergonomic assessment\ntechnologies focused on cumulative load analysis compatible within diverse\nindustrial processes.", "AI": {"tldr": "This study evaluates the construct validity of task-based ergonomic assessment in non-routine manufacturing, revealing low convergent validity and weak discriminant validity, emphasizing the need for improved ergonomic assessment technologies.", "motivation": "The paper addresses the validity concerns of task-based ergonomic assessments in the context of Industry 5.0 where human-centric practices are emphasized, particularly in challenging industrial environments.", "method": "The study utilizes a Multitrait multimethod (MTMM) matrix and video-based content analysis to evaluate ergonomic assessments collected from 46 participants using direct measurement and self-reported techniques.", "result": "Findings indicate low same trait correlations for convergent validity (ranging from 0.149 to 0.243) and weak discriminant validity (p < 0.001), revealing factors that undermine the construct validity of ergonomic assessments.", "conclusion": "The results suggest significant underestimation of ergonomic risks through task-based assessments, necessitating advancements in ergonomic technologies that are compatible with diverse industrial processes.", "key_contributions": ["Evaluates construct validity of task-based ergonomic assessments in non-routine situations.", "Identifies validity issues in ergonomic risk assessment methodologies.", "Discovers factors that contribute to misinterpretation of ergonomic risks."], "limitations": "The study is limited to non-routine manufacturing processes and may not generalize to all industrial sectors.", "keywords": ["ergonomics", "task-based assessment", "Industry 5.0", "construct validity", "human-centricity"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.19869", "pdf": "https://arxiv.org/pdf/2507.19869.pdf", "abs": "https://arxiv.org/abs/2507.19869", "title": "The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment", "authors": ["Danil Fokin", "Monika Płużyczka", "Grigory Golovin"], "categories": ["cs.CL"], "comment": null, "summary": "We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing\nthe receptive vocabulary size of both native and non-native Polish speakers.\nBased on Item Response Theory and Computerized Adaptive Testing, PVST\ndynamically adjusts to each test-taker's proficiency level, ensuring high\naccuracy while keeping the test duration short. To validate the test, a pilot\nstudy was conducted with 1.475 participants. Native Polish speakers\ndemonstrated significantly larger vocabularies compared to non-native speakers.\nFor native speakers, vocabulary size showed a strong positive correlation with\nage. The PVST is available online at myvocab.info/pl.", "AI": {"tldr": "The Polish Vocabulary Size Test (PVST) is a new adaptive tool for measuring vocabulary size in Polish speakers, validated with a large pilot study.", "motivation": "The need for an accurate and efficient assessment tool for vocabulary size in Polish speakers.", "method": "The PVST utilizes Item Response Theory and Computerized Adaptive Testing to adjust to the user's proficiency level.", "result": "The pilot study involving 1,475 participants showed that native speakers have significantly larger vocabularies and that their vocabulary size correlates positively with age.", "conclusion": "The PVST provides a reliable measure of vocabulary size and is accessible online for users.", "key_contributions": ["Introduction of the PVST as a dynamic assessment tool", "Validation through extensive pilot study", "Correlation findings linking vocabulary size and age for native speakers"], "limitations": "", "keywords": ["vocabulary assessment", "Polish language", "adaptive testing"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2507.20300", "pdf": "https://arxiv.org/pdf/2507.20300.pdf", "abs": "https://arxiv.org/abs/2507.20300", "title": "Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance and Experience in Minecraft", "authors": ["Xin Sun", "Lei Wang", "Yue Li", "Jie Li", "Massimo Poesio", "Julian Frommel", "Koen Hinriks", "Jiahuan Pei"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "With large language models (LLMs) on the rise, in-game interactions are\nshifting from rigid commands to natural conversations. However, the impacts of\nLLMs on player performance and game experience remain underexplored. This work\nexplores LLM's role as a co-builder during gameplay, examining its impact on\ntask performance, usability, and player experience. Using Minecraft as a\nsandbox, we present an LLM-assisted interface that engages players through\nnatural language, aiming to facilitate creativity and simplify complex gaming\ncommands. We conducted a mixed-methods study with 30 participants, comparing\nLLM-assisted and command-based interfaces across simple and complex game tasks.\nQuantitative and qualitative analyses reveal that the LLM-assisted interface\nsignificantly improves player performance, engagement, and overall game\nexperience. Additionally, task complexity has a notable effect on player\nperformance and experience across both interfaces. Our findings highlight the\npotential of LLM-assisted interfaces to revolutionize virtual experiences,\nemphasizing the importance of balancing intuitiveness with predictability,\ntransparency, and user agency in AI-driven, multimodal gaming environments.", "AI": {"tldr": "This study investigates the impact of LLMs on player performance and experience in gaming, specifically using an LLM-assisted interface in Minecraft.", "motivation": "To explore how LLMs can transform in-game interactions from rigid commands to natural conversations, enhancing player experience and performance.", "method": "A mixed-methods study with 30 participants comparing LLM-assisted and command-based interfaces while performing simple and complex tasks in Minecraft.", "result": "The LLM-assisted interface significantly improved player performance, engagement, and overall game experience compared to traditional command-based interfaces.", "conclusion": "LLM-assisted interfaces have the potential to revolutionize gameplay, but it is essential to balance intuitiveness, predictability, transparency, and user agency.", "key_contributions": ["Introduction of an LLM-assisted interface for gaming", "Demonstrated significant improvements in player performance and engagement", "Insights into the effects of task complexity on gameplay experience"], "limitations": "Study limited to Minecraft and 30 participants, may not generalize across all games or player demographics.", "keywords": ["large language models", "game interaction", "player experience", "usability", "mixed-methods study"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.19885", "pdf": "https://arxiv.org/pdf/2507.19885.pdf", "abs": "https://arxiv.org/abs/2507.19885", "title": "Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam", "authors": ["Cesar Augusto Madid Truyts", "Amanda Gomes Rabelo", "Gabriel Mesquita de Souza", "Daniel Scaldaferri Lages", "Adriano Jose Pereira", "Uri Adrian Prync Flato", "Eduardo Pontes dos Reis", "Joaquim Edson Vieira", "Paulo Sergio Panse Silveira", "Edson Amaro Junior"], "categories": ["cs.CL"], "comment": null, "summary": "Artificial intelligence (AI) has shown the potential to revolutionize\nhealthcare by improving diagnostic accuracy, optimizing workflows, and\npersonalizing treatment plans. Large Language Models (LLMs) and Multimodal\nLarge Language Models (MLLMs) have achieved notable advancements in natural\nlanguage processing and medical applications. However, the evaluation of these\nmodels has focused predominantly on the English language, leading to potential\nbiases in their performance across different languages.\n  This study investigates the capability of six LLMs (GPT-4.0 Turbo,\nLLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and\nCommand R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,\nand Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese\nfrom the medical residency entrance exam of the Hospital das Cl\\'inicas da\nFaculdade de Medicina da Universidade de S\\~ao Paulo (HCFMUSP) - the largest\nhealth complex in South America. The performance of the models was benchmarked\nagainst human candidates, analyzing accuracy, processing time, and coherence of\nthe generated explanations.\n  The results show that while some models, particularly Claude-3.5-Sonnet and\nClaude-3-Opus, achieved accuracy levels comparable to human candidates,\nperformance gaps persist, particularly in multimodal questions requiring image\ninterpretation. Furthermore, the study highlights language disparities,\nemphasizing the need for further fine-tuning and data set augmentation for\nnon-English medical AI applications.\n  Our findings reinforce the importance of evaluating generative AI in various\nlinguistic and clinical settings to ensure a fair and reliable deployment in\nhealthcare. Future research should explore improved training methodologies,\nimproved multimodal reasoning, and real-world clinical integration of AI-driven\nmedical assistance.", "AI": {"tldr": "This study evaluates the performance of various LLMs and MLLMs in answering medical questions in Brazilian Portuguese, revealing performance gaps compared to human candidates and highlighting the necessity for improvements in multilingual medical AI applications.", "motivation": "The motivation behind this study is to investigate the performance of LLMs and MLLMs in a non-English language, specifically Brazilian Portuguese, to identify biases and performance gaps that may affect their usability in healthcare.", "method": "The study benchmarked six LLMs and four MLLMs against human candidates using questions from the medical residency entrance exam of a prestigious hospital in Brazil, measuring accuracy, processing time, and coherence of responses.", "result": "Some models, like Claude-3.5-Sonnet and Claude-3-Opus, demonstrated accuracy levels comparable to human candidates, however, notable gaps in performance were observed, especially in multimodal questions.", "conclusion": "The study emphasizes the critical need for fine-tuning and dataset augmentation to enhance the performance of AI systems in diverse linguistic contexts within healthcare applications.", "key_contributions": ["Performance evaluation of LLMs and MLLMs in Brazilian Portuguese medical questions", "Identification of performance gaps in multimodal question response", "Recommendations for improved training methodologies and clinical integration of AI"], "limitations": "The study's focus was limited to Brazilian Portuguese medical questions and may not fully represent performance across all languages or medical specializations.", "keywords": ["Large Language Models", "Multimodal Models", "Health Informatics", "Language Disparities", "AI in Healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.20355", "pdf": "https://arxiv.org/pdf/2507.20355.pdf", "abs": "https://arxiv.org/abs/2507.20355", "title": "CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration", "authors": ["Zheng Wei", "Hongtao Wu", "lvmin Zhang", "Xian Xu", "Yefeng Zheng", "Pan Hui", "Maneesh Agrawala", "Huamin Qu", "Anyi Rao"], "categories": ["cs.HC"], "comment": null, "summary": "Effective communication between directors and cinematographers is fundamental\nin film production, yet traditional approaches relying on visual references and\nhand-drawn storyboards often lack the efficiency and precision necessary during\npre-production. We present CineVision, an AI-driven platform that integrates\nscriptwriting with real-time visual pre-visualization to bridge this\ncommunication gap. By offering dynamic lighting control, style emulation based\non renowned filmmakers, and customizable character design, CineVision enables\ndirectors to convey their creative vision with heightened clarity and rapidly\niterate on scene composition. In a 24-participant lab study, CineVision yielded\nshorter task times and higher usability ratings than two baseline methods,\nsuggesting a potential to ease early-stage communication and accelerate\nstoryboard drafts under controlled conditions. These findings underscore\nCineVision's potential to streamline pre-production processes and foster deeper\ncreative synergy among filmmaking teams, particularly for new collaborators.Our\ncode and demo are available at https://github.com/TonyHongtaoWu/CineVision.", "AI": {"tldr": "CineVision is an AI-driven platform for pre-production in filmmaking that integrates scriptwriting with real-time visual pre-visualization, improving communication between directors and cinematographers.", "motivation": "To improve the efficiency and precision of communication between directors and cinematographers during the pre-production phase of film production, overcoming the limitations of traditional visual references and hand-drawn storyboards.", "method": "CineVision integrates AI with scriptwriting and real-time visual pre-visualization, providing features like dynamic lighting control, style emulation, and customizable character design for enhanced creative expression.", "result": "In a lab study with 24 participants, CineVision demonstrated shorter task times and higher usability ratings compared to two baseline methods, indicating its effectiveness in facilitating communication and accelerating storyboard drafts.", "conclusion": "CineVision has the potential to streamline pre-production processes and enhance collaboration among filmmaking teams, especially beneficial for new collaborators.", "key_contributions": ["Dynamic lighting control", "Style emulation based on renowned filmmakers", "Customizable character design"], "limitations": "", "keywords": ["AI-driven platform", "pre-production", "film production"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2507.19899", "pdf": "https://arxiv.org/pdf/2507.19899.pdf", "abs": "https://arxiv.org/abs/2507.19899", "title": "A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs", "authors": ["Prajval Bolegave", "Pushpak Bhattacharya"], "categories": ["cs.CL"], "comment": null, "summary": "Early detection of depression from online social media posts holds promise\nfor providing timely mental health interventions. In this work, we present a\nhigh-quality, expert-annotated dataset of 1,017 social media posts labeled with\ndepressive spans and mapped to 12 depression symptom categories. Unlike prior\ndatasets that primarily offer coarse post-level labels\n\\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of\nboth model predictions and generated explanations.\n  We develop an evaluation framework that leverages this clinically grounded\ndataset to assess the faithfulness and quality of natural language explanations\ngenerated by large language models (LLMs). Through carefully designed prompting\nstrategies, including zero-shot and few-shot approaches with domain-adapted\nexamples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,\nGemini 2.5 Pro, and Claude 3.7 Sonnet.\n  Our comprehensive empirical analysis reveals significant differences in how\nthese models perform on clinical explanation tasks, with zero-shot and few-shot\nprompting. Our findings underscore the value of human expertise in guiding LLM\nbehavior and offer a step toward safer, more transparent AI systems for\npsychological well-being.", "AI": {"tldr": "This paper presents an expert-annotated dataset of social media posts for detecting depression and evaluates large language models' explanations on mental health tasks.", "motivation": "To improve early detection of depression through social media analysis and enhance the quality of AI-generated explanations in mental health contexts.", "method": "A high-quality dataset of 1,017 annotated social media posts is used to assess the performance of popular LLMs via zero-shot and few-shot prompting strategies.", "result": "Different proprietary LLMs exhibit varying effectiveness in generating clinically relevant explanations, revealing gaps in AI comprehension of mental health nuances.", "conclusion": "Human expertise is crucial in refining LLM outputs for mental health applications, contributing to the development of safer AI systems.", "key_contributions": ["Development of a fine-grained dataset for depression detection", "Evaluation framework leveraging clinical data for LLMs", "Insights into the role of human expertise in AI explanation generation"], "limitations": "The dataset may not encompass all forms of depression expression in social media, potentially limiting the model's generalizability.", "keywords": ["depression detection", "social media", "large language models", "natural language explanations", "mental health"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.20437", "pdf": "https://arxiv.org/pdf/2507.20437.pdf", "abs": "https://arxiv.org/abs/2507.20437", "title": "EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband", "authors": ["Kian Mahmoodi", "Yudong Xie", "Tan Gemicioglu", "Chi-Jung Lee", "Jiwan Kim", "Cheng Zhang"], "categories": ["cs.HC"], "comment": "8 pages, 3 figures. Proceedings of the 2025 ACM International\n  Symposium on Wearable Computers (ISWC '25)", "summary": "Grip force is commonly used as an overall health indicator in older adults\nand is valuable for tracking progress in physical training and rehabilitation.\nExisting methods for wearable grip force measurement are cumbersome and\nuser-dependent, making them insufficient for practical, continuous grip force\nmeasurement. We introduce EchoForce, a novel wristband using acoustic sensing\nfor low-cost, non-contact measurement of grip force. EchoForce captures\nacoustic signals reflected from subtle skin deformations by flexor muscles on\nthe forearm. In a user study with 11 participants, EchoForce achieved a\nfine-tuned user-dependent mean error rate of 9.08% and a user-independent mean\nerror rate of 12.3% using a foundation model. Our system remained accurate\nbetween sessions, hand orientations, and users, overcoming a significant\nlimitation of past force sensing systems. EchoForce makes continuous grip force\nmeasurement practical, providing an effective tool for health monitoring and\nnovel interaction techniques.", "AI": {"tldr": "EchoForce is a wristband that uses acoustic sensing for continuous, non-contact measurement of grip force, overcoming limitations of previous systems.", "motivation": "To provide practical and continuous grip force measurement for health monitoring and rehabilitation in older adults, addressing the shortcomings of existing user-dependent methods.", "method": "A novel wristband called EchoForce, which captures acoustic signals from skin deformations using a foundation model to measure grip force.", "result": "EchoForce achieved a mean error rate of 9.08% for user-dependent measurements and 12.3% for user-independent measurements, maintaining accuracy across sessions and users.", "conclusion": "EchoForce is effective for continuous grip force measurement, offering potential benefits in health monitoring and interaction techniques.", "key_contributions": ["Novel approach to non-contact grip force measurement using acoustic sensing.", "High accuracy in both user-dependent and user-independent scenarios.", "Practical application for health monitoring in older adults."], "limitations": "Only tested with a small participant group (11), which may limit generalizability.", "keywords": ["grip force", "health monitoring", "acoustic sensing", "wearable technology", "rehabilitation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.19906", "pdf": "https://arxiv.org/pdf/2507.19906.pdf", "abs": "https://arxiv.org/abs/2507.19906", "title": "CaliDrop: KV Cache Compression with Calibration", "authors": ["Yi Su", "Quantong Qiu", "Yuechi Zhou", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.", "AI": {"tldr": "CaliDrop enhances token eviction strategies for Key-Value caching in Large Language Models (LLMs) to reduce memory usage while retaining accuracy.", "motivation": "As LLMs generate outputs, they require significant computational resources, and the linear growth of KV cache memory usage becomes a bottleneck, especially for long-context tasks.", "method": "CaliDrop improves token eviction by using speculative calibration of discarded tokens based on the observation of high similarity between queries at nearby positions.", "result": "CaliDrop shows considerable accuracy improvements over traditional token eviction methods, even with high compression ratios.", "conclusion": "The proposed CaliDrop strategy effectively mitigates accuracy loss associated with token eviction in LLMs, making it a viable solution for enhancing KV cache efficiency.", "key_contributions": ["Introduction of CaliDrop for enhanced token eviction in LLMs", "Demonstrated significant accuracy retention of KV cache with high compression", "Detailed analysis of attention patterns leading to improved eviction strategies"], "limitations": "The effectiveness of CaliDrop may vary depending on the specific model architecture and task characteristics.", "keywords": ["Large Language Models", "Token Eviction", "KV Cache", "CaliDrop", "Attention Patterns"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20655", "pdf": "https://arxiv.org/pdf/2507.20655.pdf", "abs": "https://arxiv.org/abs/2507.20655", "title": "CoGrader: Transforming Instructors' Assessment of Project Reports through Collaborative LLM Integration", "authors": ["Zixin Chen", "Jiachen Wang", "Yumeng Li", "Haobo Li", "Chuhan Shi", "Rong Zhang", "Huamin Qu"], "categories": ["cs.HC"], "comment": null, "summary": "Grading project reports are increasingly significant in today's educational\nlandscape, where they serve as key assessments of students' comprehensive\nproblem-solving abilities. However, it remains challenging due to the\nmultifaceted evaluation criteria involved, such as creativity and\npeer-comparative achievement. Meanwhile, instructors often struggle to maintain\nfairness throughout the time-consuming grading process. Recent advances in AI,\nparticularly large language models, have demonstrated potential for automating\nsimpler grading tasks, such as assessing quizzes or basic writing quality.\nHowever, these tools often fall short when it comes to complex metrics, like\ndesign innovation and the practical application of knowledge, that require an\ninstructor's educational insights into the class situation. To address this\nchallenge, we conducted a formative study with six instructors and developed\nCoGrader, which introduces a novel grading workflow combining human-LLM\ncollaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader\nwas found effective in improving grading efficiency and consistency while\nproviding reliable peer-comparative feedback to students. We also discuss\ndesign insights and ethical considerations for the development of human-AI\ncollaborative grading systems.", "AI": {"tldr": "CoGrader is a grading system that combines human and AI insights for improved grading efficiency and fairness in educational assessments.", "motivation": "The need for a fair and efficient grading system in education that accounts for complex evaluation criteria like creativity and practical application of knowledge.", "method": "A formative study was conducted with six instructors to develop CoGrader, which integrates human-LLM collaborative metrics design, benchmarking, and AI-assisted feedback.", "result": "CoGrader was effective in enhancing grading efficiency and consistency, providing reliable peer-comparative feedback.", "conclusion": "The study discusses design insights and ethical implications for implementing human-AI collaborative grading systems.", "key_contributions": ["Development of CoGrader for collaborative grading", "Improvement in grading consistency and efficiency", "Provision of AI-assisted feedback in educational assessments."], "limitations": "Limited to the input and perspectives of six instructors; may not generalize to all educational contexts.", "keywords": ["AI in education", "grading systems", "human-AI collaboration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.19962", "pdf": "https://arxiv.org/pdf/2507.19962.pdf", "abs": "https://arxiv.org/abs/2507.19962", "title": "KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models", "authors": ["Seorin Kim", "Dongyoung Lee", "Jaejin Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often exhibit societal biases in their outputs,\nprompting ethical concerns regarding fairness and harm. In this work, we\npropose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing\nframework that implicitly aligns attention distributions between stereotypical\nand anti-stereotypical sentence pairs without directly modifying model weights.\nKLAAD introduces a composite training objective combining Cross-Entropy, KL\ndivergence, and Triplet losses, guiding the model to consistently attend across\nbiased and unbiased contexts while preserving fluency and coherence.\nExperimental evaluation of KLAAD demonstrates improved bias mitigation on both\nthe BBQ and BOLD benchmarks, with minimal impact on language modeling quality.\nThe results indicate that attention-level alignment offers a principled\nsolution for mitigating bias in generative language models.", "AI": {"tldr": "KLAAD is a debiasing framework for large language models that aligns attention distributions to reduce societal biases without changing model weights.", "motivation": "Address ethical concerns regarding societal biases present in large language models' outputs.", "method": "KLAAD uses a composite training objective that combines Cross-Entropy, KL divergence, and Triplet losses, aligning attention between biased and unbiased contexts.", "result": "KLAAD showed improved bias mitigation on BBQ and BOLD benchmarks with minimal effect on language modeling quality.", "conclusion": "Attention-level alignment proves to be an effective approach for bias reduction in generative language models.", "key_contributions": ["Introduces KLAAD for attention-based debiasing", "Combines multiple loss functions for effective training", "Demonstrates improved bias mitigation with minimal quality loss"], "limitations": "", "keywords": ["Large Language Models", "Debiasing", "Attention Alignment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.20656", "pdf": "https://arxiv.org/pdf/2507.20656.pdf", "abs": "https://arxiv.org/abs/2507.20656", "title": "EarXplore: An Open Research Database on Earable Interaction", "authors": ["Jonas Hummel", "Tobias Röddiger", "Valeria Zitz", "Philipp Lepold", "Michael Küttner", "Marius Prill", "Christopher Clarke", "Hans Gellersen", "Michael Beigl"], "categories": ["cs.HC"], "comment": null, "summary": "Interaction with earables - earphones equipped with additional sensors - has\nbeen identified as one of four major areas of earable research. Worn naturally\nand positioned near key physiological signals, earables support a wide range of\ninteraction modalities and have demonstrated the ability to detect multiple\ninputs simultaneously. Yet this diversity has resulted in a fragmented body of\nresearch, making it increasingly difficult to track developments and identify\nrelevant studies. To address this, we introduce EarXplore, a curated,\ninteractive online database on earable interaction research. Designed through a\nquestion-centered process that guided both the development of 34 criteria\napplied to annotate 118 studies and the structure of the platform, EarXplore\ncomprises four distinct yet integrated views: a Tabular View for structured\nexploration, a Graphical View for visual overviews, a Similarity View for\nidentifying conceptual links, and a Timeline View for analyzing trends and\nscholarly lineage. We demonstrate how the platform supports tailored\nexploration, targeted filtering, and interactive information retrieval,\nallowing researchers to query the literature and synthesize information in the\nformat of their choice. We furthermore leverage the contents and capabilities\nof the platform to discuss the research gaps and opportunities in the field.\nWith built-in mechanisms for continuous community updates, EarXplore not only\nreflects the current state of the field but also evolves alongside it, serving\nas a living resource to inform and accelerate future developments.", "AI": {"tldr": "Introduction of EarXplore, a curated online database for earable interaction research.", "motivation": "To address the fragmented body of research in earable interaction technologies and support researchers in tracking developments.", "method": "Development of EarXplore through a question-centered process, applying 34 criteria to annotate 118 studies and structuring the platform with four integrated views: Tabular, Graphical, Similarity, and Timeline.", "result": "EarXplore enables tailored exploration, targeted filtering, and interactive information retrieval, facilitating literature queries and information synthesis.", "conclusion": "EarXplore serves as a dynamic resource that continuously reflects and evolves the state of earable interaction research, identifying gaps and opportunities.", "key_contributions": ["Curated database for earable interaction research.", "Four distinct views for data exploration and analysis.", "Continuous community updates to reflect current research trends."], "limitations": "", "keywords": ["earables", "interaction", "online database", "literature exploration", "research trends"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.19969", "pdf": "https://arxiv.org/pdf/2507.19969.pdf", "abs": "https://arxiv.org/abs/2507.19969", "title": "Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text", "authors": ["Mizanur Rahman", "Md Tahmid Rahman Laskar", "Shafiq Joty", "Enamul Hoque"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Automated data visualization plays a crucial role in simplifying data\ninterpretation, enhancing decision-making, and improving efficiency. While\nlarge language models (LLMs) have shown promise in generating visualizations\nfrom natural language, the absence of comprehensive benchmarks limits the\nrigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark\ndesigned to assess text-to-visualization models, covering 20+ chart types and\ndiverse data science queries, including trend analysis, correlation, outlier\ndetection, and predictive analytics. It comprises 1,985 samples, each with a\ndata table, natural language query, short answer, visualization code, and\nannotated charts. The queries involve complex reasoning, conversational turns,\nand dynamic data retrieval. We benchmark 11 open-source and closed-source\nmodels, revealing significant performance gaps, highlighting key challenges,\nand offering insights for future advancements. To close this gap, we propose\nthe first cross-modal actor-critic agentic framework that jointly refines the\ntextual answer and visualization code, increasing GPT-4o`s pass rate from 26%\nto 42% over the direct approach and improving chart quality. We also introduce\nan automated LLM-based evaluation framework that enables scalable assessment\nacross thousands of samples without human annotation, measuring answer\ncorrectness, code execution success, visualization readability, and chart\naccuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.", "AI": {"tldr": "Introduction of Text2Vis, a benchmark for assessing text-to-visualization models, evaluating over 20 chart types and complex data science queries.", "motivation": "The need for a rigorous benchmark to evaluate the capabilities of large language models in automated data visualization due to the lack of comprehensive assessment tools.", "method": "The study assesses 11 models using a curated dataset of 1,985 samples consisting of data tables, queries, answers, visualization code, and annotated charts. A cross-modal actor-critic framework refines text answers and visualization code.", "result": "The benchmarking revealed significant performance gaps across the tested models, with an improvement in GPT-4o's pass rate from 26% to 42% using the proposed framework.", "conclusion": "The work highlights challenges in text-to-visualization tasks and provides a scalable, automated evaluation framework for future advancements in data visualization capabilities of LLMs.", "key_contributions": ["Introduction of the Text2Vis benchmark for text-to-visualization models.", "Development of a cross-modal actor-critic framework for improved model performance.", "Creation of an automated evaluation framework for scalable assessment of visualization capabilities."], "limitations": "Challenge of ensuring high-quality visual outputs and comprehensive benchmarking methods.", "keywords": ["automated data visualization", "text-to-visualization models", "benchmarking", "large language models", "cross-modal learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20720", "pdf": "https://arxiv.org/pdf/2507.20720.pdf", "abs": "https://arxiv.org/abs/2507.20720", "title": "Beyond Text: Probing K-12 Educators' Perspectives and Ideas for Learning Opportunities Leveraging Multimodal Large Language Models", "authors": ["Tiffany Tseng", "Katelyn Lam", "Tiffany Lin Fu", "Alekhya Maram"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are beginning to empower new user\nexperiences that can flexibly generate content from a range of inputs,\nincluding images, text, speech, and video. These capabilities have the\npotential to enrich learning by enabling users to capture and interact with\ninformation using a variety of modalities, but little is known about how\neducators envision how MLLMs might shape the future of learning experiences,\nwhat challenges diverse teachers encounter when interpreting how these models\nwork, and what practical needs should be considered for successful\nimplementation in educational contexts. We investigated educator perspectives\nthrough formative workshops with 12 K-12 educators, where participants\nbrainstormed learning opportunities, discussed practical concerns for effective\nuse, and prototyped their own MLLM-powered learning applications using Claude\n3.5 and its Artifacts feature for previewing code-based output. We use case\nstudies to illustrate two contrasting end-user approaches (teacher-and\nstudent-driven), and share insights about opportunities and concerns expressed\nby our participants, ending with implications for leveraging MLLMs for future\nlearning experiences.", "AI": {"tldr": "The paper explores educator perspectives on the use of Multimodal Large Language Models (MLLMs) in K-12 education, revealing opportunities and challenges for implementing these models.", "motivation": "To understand how educators envision the integration of MLLMs in learning environments and identify practical needs for effective implementation.", "method": "Formative workshops with 12 K-12 educators to brainstorm applications, discuss concerns, and prototype MLLM-powered learning tools.", "result": "Insights on varied educator approaches towards MLLMs and the identification of opportunities and challenges in their application for educational contexts.", "conclusion": "The findings highlight the need for targeted support and resources to effectively integrate MLLMs into K-12 learning experiences.", "key_contributions": ["Investigation of educator perspectives on MLLMs in education", "Prototyping of MLLM-powered learning applications", "Case studies illustrating diverse approaches to MLLM usage in classrooms"], "limitations": "Exploratory study with a small sample size of educators; findings may not generalize to all educational contexts.", "keywords": ["Multimodal Large Language Models", "educational technology", "K-12 education", "teacher perspectives", "learning applications"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.19980", "pdf": "https://arxiv.org/pdf/2507.19980.pdf", "abs": "https://arxiv.org/abs/2507.19980", "title": "Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory", "authors": ["Dan Song", "Won-Chan Lee", "Hong Jiao"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the estimation of reliability for large language\nmodels (LLMs) in scoring writing tasks from the AP Chinese Language and Culture\nExam. Using generalizability theory, the research evaluates and compares score\nconsistency between human and AI raters across two types of AP Chinese\nfree-response writing tasks: story narration and email response. These essays\nwere independently scored by two trained human raters and seven AI raters. Each\nessay received four scores: one holistic score and three analytic scores\ncorresponding to the domains of task completion, delivery, and language use.\nResults indicate that although human raters produced more reliable scores\noverall, LLMs demonstrated reasonable consistency under certain conditions,\nparticularly for story narration tasks. Composite scoring that incorporates\nboth human and AI raters improved reliability, which supports that hybrid\nscoring models may offer benefits for large-scale writing assessments.", "AI": {"tldr": "The study analyzes the reliability of large language models (LLMs) in scoring AP Chinese writing tasks compared to human raters, finding that LLMs can achieve consistency under specific conditions, particularly with story narration tasks.", "motivation": "To evaluate the effectiveness of large language models in scoring writing tasks, and to explore hybrid scoring models that combine human and AI assessment for enhanced reliability in large-scale writing evaluations.", "method": "The research employs generalizability theory to assess the score consistency between human and AI raters across different types of AP Chinese free-response writing tasks, using independent scoring from trained raters.", "result": "Human raters yielded more reliable scores overall, but LLMs showed reasonable consistency under specific conditions, especially for story narration. The inclusion of AI scores in composite scoring improved overall reliability.", "conclusion": "Hybrid scoring models that integrate both human and AI raters demonstrate potential benefits for enhancing reliability in large-scale writing assessments.", "key_contributions": ["Investigation of LLMs in educational assessment", "Empirical comparison of human and AI scoring reliability", "Advocacy for hybrid scoring models in large-scale evaluations"], "limitations": "Study focused solely on AP Chinese Language and Culture Exam; findings may not generalize to other contexts or languages.", "keywords": ["large language models", "scoring assessment", "hybrid models", "generalizability theory", "education"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.20730", "pdf": "https://arxiv.org/pdf/2507.20730.pdf", "abs": "https://arxiv.org/abs/2507.20730", "title": "Vocalize: Lead Acquisition and User Engagement through Gamified Voice Competitions", "authors": ["Edvin Teskeredzic", "Muamer Paric", "Adna Sestic", "Petra Fribert", "Anamarija Lukac", "Hadzem Hadzic", "Kemal Altwlkany", "Emanuel Lacic"], "categories": ["cs.HC", "cs.MM"], "comment": "Accepted to ACM Hypertext 2025", "summary": "This paper explores the prospect of creating engaging user experiences and\ncollecting leads through an interactive and gamified platform. We introduce\nVocalize, an end-to-end system for increasing user engagement and lead\nacquisition through gamified voice competitions. Using audio processing\ntechniques and LLMs, we create engaging and interactive experiences that have\nthe potential to reach a wide audience, foster brand recognition, and increase\ncustomer loyalty. We describe the system from a technical standpoint and report\nresults from launching Vocalize at 4 different live events. Our user study\nshows that Vocalize is capable of generating significant user engagement, which\nshows potential for gamified audio campaigns in marketing and similar\nverticals.", "AI": {"tldr": "The paper presents Vocalize, a gamified platform that enhances user engagement and lead acquisition through audio competitions powered by LLMs and audio processing techniques.", "motivation": "To create engaging user experiences and collect leads through an interactive gamified system.", "method": "Vocalize combines audio processing and large language models to facilitate gamified voice competitions, assessed through user studies at live events.", "result": "The user study indicates that Vocalize significantly increases user engagement, showcasing its effectiveness for gamified audio marketing campaigns.", "conclusion": "The findings endorse the potential of Vocalize for improving user interaction and brand loyalty through engaging audio experiences.", "key_contributions": ["Introduction of Vocalize as a gamified platform for user engagement", "Utilization of audio processing and LLMs in voice competitions", "Empirical evidence of increased user engagement from live events"], "limitations": "", "keywords": ["gamification", "user engagement", "audio processing", "LLMs", "voice competitions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.19995", "pdf": "https://arxiv.org/pdf/2507.19995.pdf", "abs": "https://arxiv.org/abs/2507.19995", "title": "VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering", "authors": ["Tan-Minh Nguyen", "Hoang-Trung Nguyen", "Trong-Khoi Dao", "Xuan-Hieu Phan", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs) has led to significant\nachievements in various domains, including legal text processing. Leveraging\nLLMs for legal tasks is a natural evolution and an increasingly compelling\nchoice. However, their capabilities are often portrayed as greater than they\ntruly are. Despite the progress, we are still far from the ultimate goal of\nfully automating legal tasks using artificial intelligence (AI) and natural\nlanguage processing (NLP). Moreover, legal systems are deeply domain-specific\nand exhibit substantial variation across different countries and languages. The\nneed for building legal text processing applications for different natural\nlanguages is, therefore, large and urgent. However, there is a big challenge\nfor legal NLP in low-resource languages such as Vietnamese due to the scarcity\nof resources and annotated data. The need for labeled legal corpora for\nsupervised training, validation, and supervised fine-tuning is critical. In\nthis paper, we introduce the VLQA dataset, a comprehensive and high-quality\nresource tailored for the Vietnamese legal domain. We also conduct a\ncomprehensive statistical analysis of the dataset and evaluate its\neffectiveness through experiments with state-of-the-art models on legal\ninformation retrieval and question-answering tasks.", "AI": {"tldr": "This paper presents the VLQA dataset for legal text processing in Vietnamese and evaluates its effectiveness for legal information retrieval and question-answering tasks.", "motivation": "Large language models (LLMs) show potential in automating legal tasks, but challenges remain due to resource scarcity in low-resource languages like Vietnamese.", "method": "The paper introduces the VLQA dataset, provides a statistical analysis of its quality, and evaluates its effectiveness with state-of-the-art models for legal NLP tasks.", "result": "The evaluation demonstrates that the VLQA dataset significantly aids legal information retrieval and question-answering tasks in the Vietnamese context.", "conclusion": "Building robust legal NLP applications requires comprehensive datasets like VLQA, particularly for low-resource languages, to enhance legal automation efforts.", "key_contributions": ["Introduction of the VLQA dataset for Vietnamese legal text processing", "Statistical analysis of dataset quality", "Evaluation of effectiveness with state-of-the-art models"], "limitations": "", "keywords": ["legal NLP", "large language models", "Vietnamese", "dataset", "legal information retrieval"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20741", "pdf": "https://arxiv.org/pdf/2507.20741.pdf", "abs": "https://arxiv.org/abs/2507.20741", "title": "Beyond QWERTY: A pressure-based text input approach for XR that enables a touch-typing like experience", "authors": ["Fabian Rücker", "Torben Storch"], "categories": ["cs.HC"], "comment": null, "summary": "Text input in extended reality (XR) applications remains inefficient and\ntedious. Most solutions are derived from the traditional keyboard layout, yet\nfail to translate its positive characteristics to the spatial digital realm.\nThis limits the productive use of immersive technologies. In this work, we\nanalyze physical keyboard input to identify key characteristics that facilitate\nits comfort, touch typing and high typing speeds. Building on these findings,\nwe propose a novel pressure-based text input modality that transfers these\ncharacteristics into immersive space by substituting the two-dimensional QWERTY\nlayout with a linear scale. This design facilitates a touch-typing-like\nexperience, eliminating the need for visual guidance for proficient users. Our\nskill-based approach enables typing speeds of over 200 characters per minute.\nAdditionally, it is suitable for discreet use in public spaces and everyday\ntext-input tasks, since the proposed system requires virtually no hand or\nfinger movements and resembles smartphone-based text input in appearance.", "AI": {"tldr": "The paper presents a novel pressure-based text input modality for XR applications that enhances typing efficiency by mimicking the comfort and speed of physical keyboards.", "motivation": "To improve text input efficiency in XR applications, which currently rely on traditional keyboard layouts and are often inefficient.", "method": "Analyzed physical keyboard characteristics that contribute to comfort and high typing speeds, then developed a pressure-based input system that replaces the QWERTY layout with a linear scale.", "result": "Proposed system allows typing speeds over 200 characters per minute in a touch-typing-like experience without visual guidance and minimal hand movement.", "conclusion": "The new input modality is suitable for discreet text input in public spaces and everyday tasks, enhancing the usability of XR technologies.", "key_contributions": ["Introduction of a pressure-based text input for XR that enhances typing comfort and speed.", "Elimination of visual guidance for proficient users, promoting touch typing in immersive environments.", "Demonstrated typing speeds exceeding 200 characters per minute."], "limitations": "", "keywords": ["extended reality", "text input", "pressure-based modality", "typing efficiency", "immersive technologies"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.20019", "pdf": "https://arxiv.org/pdf/2507.20019.pdf", "abs": "https://arxiv.org/abs/2507.20019", "title": "Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach", "authors": ["Saurav Singla", "Aarav Singla", "Advik Gupta", "Parnika Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages. PyTorch code for few-shot anomaly detection using\n  meta-learning is available upon request or can be shared via GitHub", "summary": "We propose a meta learning framework for detecting anomalies in human\nlanguage across diverse domains with limited labeled data. Anomalies in\nlanguage ranging from spam and fake news to hate speech pose a major challenge\ndue to their sparsity and variability. We treat anomaly detection as a few shot\nbinary classification problem and leverage meta-learning to train models that\ngeneralize across tasks. Using datasets from domains such as SMS spam, COVID-19\nfake news, and hate speech, we evaluate model generalization on unseen tasks\nwith minimal labeled anomalies. Our method combines episodic training with\nprototypical networks and domain resampling to adapt quickly to new anomaly\ndetection tasks. Empirical results show that our method outperforms strong\nbaselines in F1 and AUC scores. We also release the code and benchmarks to\nfacilitate further research in few-shot text anomaly detection.", "AI": {"tldr": "This paper presents a meta-learning framework designed for detecting language anomalies using limited labeled data, addressing issues such as spam, fake news, and hate speech.", "motivation": "Anomalies in human language, like spam and hate speech, are challenging to detect due to their sparse and variable nature, especially when labeled data is limited.", "method": "The authors frame anomaly detection as a few-shot binary classification problem and employ meta-learning techniques, including episodic training and prototypical networks, to enhance model generalization across diverse tasks.", "result": "The proposed method shows superior performance compared to strong baselines, achieving higher F1 and AUC scores on benchmark datasets including SMS spam and COVID-19 fake news.", "conclusion": "The study demonstrates that the meta-learning approach effectively addresses the challenge of few-shot anomaly detection in language, providing code and benchmarks for further research.", "key_contributions": ["A meta-learning framework for few-shot text anomaly detection", "Combination of episodic training with prototypical networks for model adaptation", "Empirical validation of model generalization across diverse domains"], "limitations": "", "keywords": ["anomaly detection", "meta-learning", "few-shot learning", "language processing", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20805", "pdf": "https://arxiv.org/pdf/2507.20805.pdf", "abs": "https://arxiv.org/abs/2507.20805", "title": "Understanding Bias in Perceiving Dimensionality Reduction Projections", "authors": ["Seoyoung Doh", "Hyeon Jeon", "Sungbok Shin", "Ghulam Jilani Quadri", "Nam Wook Kim", "Jinwook Seo"], "categories": ["cs.HC", "cs.LG"], "comment": "6 pages", "summary": "Selecting the dimensionality reduction technique that faithfully represents\nthe structure is essential for reliable visual communication and analytics. In\nreality, however, practitioners favor projections for other attractions, such\nas aesthetics and visual saliency, over the projection's structural\nfaithfulness, a bias we define as visual interestingness. In this research, we\nconduct a user study that (1) verifies the existence of such bias and (2)\nexplains why the bias exists. Our study suggests that visual interestingness\nbiases practitioners' preferences when selecting projections for analysis, and\nthis bias intensifies with color-encoded labels and shorter exposure time.\nBased on our findings, we discuss strategies to mitigate bias in perceiving and\ninterpreting DR projections.", "AI": {"tldr": "This research explores how visual interestingness influences practitioners' choice of dimensionality reduction techniques, revealing a preference for aesthetics over structural faithfulness.", "motivation": "Understanding the bias towards aesthetic over structural considerations in dimensionality reduction selections for better visualization and analysis.", "method": "A user study was conducted to assess the existence of visual interestingness bias and investigate the factors influencing this bias.", "result": "The study confirmed the bias, showing that visual interestingness is prioritized by practitioners, especially when color-encoded labels are used and exposure time is brief.", "conclusion": "The findings indicate the need for strategies to mitigate visual interestingness bias in the selection and interpretation of dimensionality reduction projections.", "key_contributions": ["Identification of visual interestingness bias in dimensionality reduction techniques.", "Empirical evidence from user studies on bias influencing practitioner choices.", "Recommendations for strategies to mitigate bias in visualization."], "limitations": "The study is limited to specific dimensionality reduction techniques and may not generalize to all visualization practices.", "keywords": ["dimensionality reduction", "visual interestingness", "user study", "visualization bias", "aesthetics"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2507.20030", "pdf": "https://arxiv.org/pdf/2507.20030.pdf", "abs": "https://arxiv.org/abs/2507.20030", "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression", "authors": ["Runchao Li", "Yao Fu", "Mu Sheng", "Xianxuan Long", "Haotian Yu", "Pan Li"], "categories": ["cs.CL"], "comment": null, "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.", "AI": {"tldr": "FAEDKV is a new KV cache compression framework for LLMs that improves contextual information retention without training.", "motivation": "To address the limitations of current KV cache compression methods that lead to biased representations and require costly retraining.", "method": "FAEDKV uses the Infinite-Window Fourier Transform (IWDFT) to transform the KV cache into the frequency domain, aiding in unbiased information retention.", "result": "FAEDKV outperforms existing KV cache compression methods by up to 22% in LongBench benchmark tests and shows better retrieval accuracy on position-agnostic tasks.", "conclusion": "The proposed framework effectively maintains contextual integrity while reducing memory and computational requirements in LLMs.", "key_contributions": ["Introduction of a training-free KV cache compression framework", "Utilization of Infinite-Window Fourier Transform for unbiased token contribution", "Demonstrated superior performance on LongBench benchmark and Needle-In-A-Haystack task"], "limitations": "", "keywords": ["Large Language Models", "KV cache", "compression", "Fourier Transform", "contextual information"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2507.20933", "pdf": "https://arxiv.org/pdf/2507.20933.pdf", "abs": "https://arxiv.org/abs/2507.20933", "title": "ProForm: Solder-Free Circuit Assembly Using Thermoforming", "authors": ["Narjes Pourjafarian", "Zhenming Yang", "Jeffrey I. Lipton", "Benyamin Davaji", "Gregory D. Abowd"], "categories": ["cs.HC", "cond-mat.mtrl-sci"], "comment": null, "summary": "Electronic waste (e-waste) is a growing global challenge, with millions of\nfunctional components discarded due to the difficulty of repair and reuse.\nTraditional circuit assembly relies on soldering, which creates semi-permanent\nbonds that limit component recovery and contribute to unnecessary waste. We\nintroduce ProForm, a thermoforming approach for solder-free circuit\nprototyping. By encapsulating electronic components with pressure-formed\nthermoplastics, ProForm enables secure, reversible mounting without the need\nfor solder or custom mechanical housings. This approach supports a wide range\nof substrates, including flexible, paper-based, and non-planar circuits,\nfacilitating easy reuse, replacement, and rapid prototyping. We demonstrate\nProForm's versatility to support prototyping practices. We show that ProFormed\ncircuits exhibit good electrical performance and mechanical stability. While\nmotivated by a need for sustainable electronics practices, ProForm has other\nsignificant advantages over traditional soldering.", "AI": {"tldr": "ProForm introduces a solder-free method for circuit prototyping using thermoformed thermoplastics, promoting sustainability and ease of reuse in electronics.", "motivation": "To address the growing problem of electronic waste by providing a sustainable alternative to traditional soldering methods, which limit component recovery.", "method": "ProForm uses a thermoforming approach to encapsulate electronic components in pressure-formed thermoplastics, allowing for secure and reversible mounting.", "result": "ProFormed circuits demonstrated good electrical performance and mechanical stability while facilitating rapid prototyping and easy reuse.", "conclusion": "ProForm not only supports sustainable electronics practices but also offers advantages over traditional soldering methods in terms of versatility and ease of use.", "key_contributions": ["Introduction of a solder-free prototyping method", "Support for various substrates including flexible and paper-based circuits", "Facilitation of rapid prototyping and component reuse"], "limitations": "", "keywords": ["electronic waste", "solder-free prototyping", "thermoforming"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.20046", "pdf": "https://arxiv.org/pdf/2507.20046.pdf", "abs": "https://arxiv.org/abs/2507.20046", "title": "Infogen: Generating Complex Statistical Infographics from Documents", "authors": ["Akash Ghosh", "Aparna Garimella", "Pritika Ramu", "Sambaran Bandyopadhyay", "Sriparna Saha"], "categories": ["cs.CL"], "comment": "ACL Main 2025", "summary": "Statistical infographics are powerful tools that simplify complex data into\nvisually engaging and easy-to-understand formats. Despite advancements in AI,\nparticularly with LLMs, existing efforts have been limited to generating simple\ncharts, with no prior work addressing the creation of complex infographics from\ntext-heavy documents that demand a deep understanding of the content. We\naddress this gap by introducing the task of generating statistical infographics\ncomposed of multiple sub-charts (e.g., line, bar, pie) that are contextually\naccurate, insightful, and visually aligned. To achieve this, we define\ninfographic metadata that includes its title and textual insights, along with\nsub-chart-specific details such as their corresponding data and alignment. We\nalso present Infodat, the first benchmark dataset for text-to-infographic\nmetadata generation, where each sample links a document to its metadata. We\npropose Infogen, a two-stage framework where fine-tuned LLMs first generate\nmetadata, which is then converted into infographic code. Extensive evaluations\non Infodat demonstrate that Infogen achieves state-of-the-art performance,\noutperforming both closed and open-source LLMs in text-to-statistical\ninfographic generation.", "AI": {"tldr": "Introducing a new method for generating complex statistical infographics from text using LLMs, overcoming limitations in current AI approaches.", "motivation": "To address the lack of tools for creating complex infographics from text-heavy documents that require understanding of context and content.", "method": "The authors propose Infogen, a two-stage framework that uses fine-tuned LLMs to first generate infographic metadata and then convert this metadata into infographic code.", "result": "Infogen was evaluated on the Infodat dataset, showing state-of-the-art performance in generating statistical infographics compared to existing LLMs.", "conclusion": "The proposed method significantly enhances the capability of LLMs in utilizing complex data for infographic creation, filling a crucial gap in the current landscape.", "key_contributions": ["Introduction of a new framework (Infogen) for infographic generation from text documents.", "Creation of Infodat, the first benchmark dataset for text-to-infographic metadata generation.", "Demonstration of Infogen's superior performance over existing LLMs in generating complex statistical infographics."], "limitations": "No specific limitations mentioned in the abstract.", "keywords": ["infographics", "machine learning", "large language models", "data visualization", "benchmark dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20943", "pdf": "https://arxiv.org/pdf/2507.20943.pdf", "abs": "https://arxiv.org/abs/2507.20943", "title": "The Impact of Simple, Brief, and Adaptive Instructions within Virtual Reality Training: Components of Cognitive Load Theory in an Assembly Task", "authors": ["Rebecca L. Pharmer", "Christopher D. Wickens", "Lucas Plabst", "Benjamin A. Clegg", "Leanne M. Hirshfield", "Joanna E. Lewis", "Jalynn B. Nicoly", "Cara A. Spencer", "Francisco R. Ortega"], "categories": ["cs.HC"], "comment": null, "summary": "Objective: The study examined the effects of varying all three core elements\nof cognitive load on learning efficiency during a shape assembly task in\nvirtual reality (VR).\n  Background: Adaptive training systems aim to improve learning efficiency and\nretention by dynamically adjusting difficulty. However, design choices can\nimpact the cognitive workload imposed on the learner. The present experiments\nexamined how aspects of cognitive load impact training outcomes.\n  Method: Participants learned step-by-step shape assembly in a VR environment.\nCognitive load was manipulated across three dimensions: Intrinsic Load (shape\ncomplexity), Extraneous Load (instruction verbosity), and Germane Load\n(adaptive vs. fixed training). In adaptive training (experiment 1), difficulty\nincreased based on individual performance. In fixed training (experiment 2),\ndifficulty followed a preset schedule from a yoked participant.\n  Results: Higher Intrinsic Load significantly increased training times and\nsubjective workload but did not affect retention test accuracy. Extraneous Load\nmodestly impacted training time, with little impact on workload or retention.\nAdaptive training shortened overall training time without increasing workload\nor impairing retention. No interactions were observed between the three types\nof load. Conclusion: Both Intrinsic and Extraneous Load increased training\ntime, but adaptive training improved efficiency without harming retention. The\nlack of interaction between the elements suggests training benefits can be\nworth seeking within any of the components of cognitive load. Application:\nThese findings support the use of VR adaptive systems in domains such as\nmanufacturing and military service, where efficient assembly skill acquisition\nis critical. Tailoring difficulty in real-time can optimize efficiency without\ncompromising learning.", "AI": {"tldr": "The study investigates how cognitive load elements impact learning efficiency in a VR shape assembly task.", "motivation": "To examine the impact of cognitive load on training outcomes and learning efficiency in adaptive training systems.", "method": "Participants learned shape assembly via VR, with cognitive load manipulated in three dimensions: Intrinsic Load, Extraneous Load, and Germane Load, across adaptive and fixed training conditions.", "result": "Higher Intrinsic Load increased training times but did not affect retention accuracy; adaptive training reduced training time without increasing workload or harming retention.", "conclusion": "Adaptive training can improve learning efficiency in VR without degrading retention, supporting its application in critical skill acquisition environments.", "key_contributions": ["Examined the impact of cognitive load on training efficiency in VR", "Demonstrated that adaptive training improves learning outcomes", "Identified specific load types affecting training time and retention"], "limitations": "", "keywords": ["Cognitive Load", "Virtual Reality", "Adaptive Training"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.20055", "pdf": "https://arxiv.org/pdf/2507.20055.pdf", "abs": "https://arxiv.org/abs/2507.20055", "title": "A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications", "authors": ["Avaljot Singh", "Yamin Chandini Sarita", "Aditya Mishra", "Ishaan Goyal", "Gagandeep Singh", "Charith Mendis"], "categories": ["cs.CL"], "comment": null, "summary": "The uninterpretability of DNNs has led to the adoption of abstract\ninterpretation-based certification as a practical means to establish trust in\nreal-world systems that rely on DNNs. However, the current landscape supports\nonly a limited set of certifiers, and developing new ones or modifying existing\nones for different applications remains difficult. This is because the\nmathematical design of certifiers is expressed at the neuron level, while their\nimplementations are optimized and executed at the tensor level. This mismatch\ncreates a semantic gap between design and implementation, making manual\nbridging both complex and expertise-intensive -- requiring deep knowledge in\nformal methods, high-performance computing, etc.\n  We propose a compiler framework that automatically translates neuron-level\nspecifications of DNN certifiers into tensor-based, layer-level\nimplementations. This is enabled by two key innovations: a novel stack-based\nintermediate representation (IR) and a shape analysis that infers the implicit\ntensor operations needed to simulate the neuron-level semantics. During\nlifting, the shape analysis creates tensors in the minimal shape required to\nperform the corresponding operations. The IR also enables domain-specific\noptimizations as rewrites. At runtime, the resulting tensor computations\nexhibit sparsity tied to the DNN architecture. This sparsity does not align\nwell with existing formats. To address this, we introduce g-BCSR, a\ndouble-compression format that represents tensors as collections of blocks of\nvarying sizes, each possibly internally sparse.\n  Using our compiler and g-BCSR, we make it easy to develop new certifiers and\nanalyze their utility across diverse DNNs. Despite its flexibility, the\ncompiler achieves performance comparable to hand-optimized implementations.", "AI": {"tldr": "This paper presents a compiler framework designed to automatically convert neuron-level specifications of DNN certifiers into tensor-based implementations, bridging the semantic gap between design and execution.", "motivation": "The need for trustworthy DNN systems and the challenges of developing certifiers that operate efficiently at different levels of abstraction.", "method": "A compiler framework that utilizes a novel stack-based intermediate representation and shape analysis to translate specifications into optimized tensor computations.", "result": "Demonstrated that the compiler can produce implementations that are performance-competitive with hand-optimized versions while allowing for easier development of new certifiers across various DNNs.", "conclusion": "The proposed framework significantly simplifies the creation of new DNN certifiers while maintaining high performance through effective tensor representation and optimization.", "key_contributions": ["Development of a compiler framework for DNN certifiers", "Introduction of a novel stack-based intermediate representation", "Creation of g-BCSR, a new tensor representation format"], "limitations": "", "keywords": ["DNN certification", "Compiler framework", "Tensor representation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.21000", "pdf": "https://arxiv.org/pdf/2507.21000.pdf", "abs": "https://arxiv.org/abs/2507.21000", "title": "Towards Effective Human Performance in XR Space Framework based on Real-time Eye Tracking Biofeedback", "authors": ["Barbara Karpowicz", "Tomasz Kowalewski", "Pavlo Zinevych", "Adam Kuzdraliński", "Grzegorz Marcin Wójcik", "Wiesław Kopeć"], "categories": ["cs.HC"], "comment": null, "summary": "This paper proposes an eye tracking module for the XR Space Framework aimed\nat enhancing human performance in XR-based applications, specifically in\ntraining, screening, and teleoperation. This framework provides a methodology\nand components that streamline the development of adaptive real-time virtual\nimmersive systems. It contains multimodal measurements - declarative in the\nform of in-VR questionnaires and objective, including eye tracking, body\nmovement, and psychophysiological data (e.g., ECG, GSR, PPG). A key focus of\nthis paper is the integration of real-time eye tracking data into XR\nenvironments to facilitate a biofeedback loop, providing insight into user\nattention, cognitive load, and engagement. Given the relatively high\nmeasurement frequency of eye tracking - recognized as a noninvasive yet robust\npsychophysiological measure - this technology is particularly well suited for\nreal-time adjustments in task difficulty and feedback to enhance learning and\noperational effectiveness. Despite its established role in cognitive and\nattentional studies, implementing eye tracking metrics within dynamic,\nreal-time XR environments poses unique challenges, particularly given the\ncomplex moving visuals presented in head-mounted displays (HMDs). This paper\naddresses these challenges by focusing on the essential aspects of integrating\neye tracking in immersive systems based on real-time engines, ultimately\nfacilitating more efficient, adaptive XR applications.", "AI": {"tldr": "This paper introduces an eye tracking module for the XR Space Framework to improve human performance in XR applications through real-time data integration and adaptive adjustments.", "motivation": "To enhance human performance in XR-based applications such as training, screening, and teleoperation by integrating eye tracking for real-time user insights.", "method": "The paper outlines a multimodal framework that integrates eye tracking, body movement, and psychophysiological data to create adaptive immersive systems.", "result": "The integration of eye tracking data improves user engagement and allows for real-time adjustments to task difficulty, boosting learning and operational effectiveness.", "conclusion": "The challenges of implementing eye tracking in real-time XR environments can be addressed, leading to more effective and adaptive applications in various training and operational scenarios.", "key_contributions": ["Development of an eye tracking module for XR applications", "Integration of multimodal measurements (eye tracking, body movement, psychophysiological data)", "Real-time adjustments based on user attention and cognitive load"], "limitations": "Challenges in implementing eye tracking metrics within dynamic, real-time XR environments due to complex moving visuals.", "keywords": ["Eye Tracking", "XR Space Framework", "Human Performance", "Real-Time Data", "Adaptive Systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20059", "pdf": "https://arxiv.org/pdf/2507.20059.pdf", "abs": "https://arxiv.org/abs/2507.20059", "title": "RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation", "authors": ["Ran Xu", "Yuchen Zhuang", "Yue Yu", "Haoyu Wang", "Wenqi Shi", "Carl Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. Code will be published at:\n  https://github.com/ritaranx/RAG_in_the_Wild", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved at inference time. While RAG\ndemonstrates strong performance on benchmarks largely derived from\ngeneral-domain corpora like Wikipedia, its effectiveness under realistic,\ndiverse retrieval scenarios remains underexplored. We evaluated RAG systems\nusing MassiveDS, a large-scale datastore with mixture of knowledge, and\nidentified critical limitations: retrieval mainly benefits smaller models,\nrerankers add minimal value, and no single retrieval source consistently\nexcels. Moreover, current LLMs struggle to route queries across heterogeneous\nknowledge sources. These findings highlight the need for adaptive retrieval\nstrategies before deploying RAG in real-world settings. Our code and data can\nbe found at https://github.com/ritaranx/RAG_in_the_Wild.", "AI": {"tldr": "This paper evaluates retrieval-augmented generation (RAG) systems using a diverse dataset and identifies limitations in their performance and query routing.", "motivation": "To assess the effectiveness of RAG systems in realistic retrieval scenarios, moving beyond general-domain benchmarks.", "method": "The study involved evaluating RAG systems with a large-scale datastore (MassiveDS) that includes diverse knowledge sources and analyzing the performance of these systems.", "result": "Critical limitations were found: retrieval benefits smaller models, rerankers add minimal value, and no single retrieval source consistently performs well.", "conclusion": "Adaptive retrieval strategies are necessary for effective deployment of RAG in real-world applications.", "key_contributions": ["Evaluation of RAG performance with a diverse datastore", "Identification of critical limitations in current RAG systems", "Highlighting the necessity for adaptive retrieval strategies"], "limitations": "Focus on a specific test set may limit generalizability; ongoing work to improve strategies.", "keywords": ["retrieval-augmented generation", "large language models", "machine learning", "information retrieval", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21012", "pdf": "https://arxiv.org/pdf/2507.21012.pdf", "abs": "https://arxiv.org/abs/2507.21012", "title": "User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with \"Vibe Coding\"", "authors": ["Tianyi Li", "Tanay Maheshwari", "Alex Voelker"], "categories": ["cs.HC"], "comment": null, "summary": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise.", "AI": {"tldr": "This paper explores the use of generative user interfaces, specifically 'vibe coding', leveraging LLMs for rapid prototyping in user-centered design, particularly in traffic data analytics.", "motivation": "To enhance user-centered design practices by integrating LLMs into the prototyping process for better feedback and alternative design evaluation.", "method": "Empirical case study integrating 'vibe coding' into the design process for an interactive data analytics interface used by traffic engineers.", "result": "The application of vibe coding allowed the team to gather extensive user feedback and iterate on design ideas effectively, showcasing the utility of AI in design.", "conclusion": "Generative UIs can significantly improve user-centered design processes but come with challenges in balancing design and domain expertise.", "key_contributions": ["Introduction of 'vibe coding' as a design methodology leveraging LLMs", "Empirical insights into using generative UIs for user-centered design", "Assessment of the advantages and pitfalls of integrating AI into design processes"], "limitations": "Potential dependency on the quality of prompts and LLM outputs; the challenge of integrating domain-specific knowledge with design expertise.", "keywords": ["generative user interfaces", "vibe coding", "user-centered design", "large language models", "traffic data analytics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.20091", "pdf": "https://arxiv.org/pdf/2507.20091.pdf", "abs": "https://arxiv.org/abs/2507.20091", "title": "ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models", "authors": ["Kaizhi Qian", "Xulin Fan", "Junrui Ni", "Slava Shechtman", "Mark Hasegawa-Johnson", "Chuang Gan", "Yang Zhang"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Speech language models refer to language models with speech processing and\nunderstanding capabilities. One key desirable capability for speech language\nmodels is the ability to capture the intricate interdependency between content\nand prosody. The existing mainstream paradigm of training speech language\nmodels, which converts speech into discrete tokens before feeding them into\nLLMs, is sub-optimal in learning prosody information -- we find that the\nresulting LLMs do not exhibit obvious emerging prosody processing capabilities\nvia pre-training alone. To overcome this, we propose ProsodyLM, which\nintroduces a simple tokenization scheme amenable to learning prosody. Each\nspeech utterance is first transcribed into text, followed by a sequence of\nword-level prosody tokens. Compared with conventional speech tokenization\nschemes, the proposed tokenization scheme retains more complete prosody\ninformation, and is more understandable to text-based LLMs. We find that\nProsodyLM can learn surprisingly diverse emerging prosody processing\ncapabilities through pre-training alone, ranging from harnessing the prosody\nnuances in generated speech, such as contrastive focus, understanding emotion\nand stress in an utterance, to maintaining prosody consistency in long\ncontexts.", "AI": {"tldr": "ProsodyLM proposes a new tokenization scheme for speech that improves prosody understanding in language models.", "motivation": "To enhance the capability of speech language models in processing and understanding prosody, which is not effectively captured by the existing mainstream training methods.", "method": "Introduces a novel tokenization scheme that transcribes speech into text and includes word-level prosody tokens, aiming to retain complete prosody information.", "result": "ProsodyLM demonstrates significant improvements in learning diverse prosody processing capabilities during pre-training, successfully capturing nuances in generated speech, such as focus, emotion, and maintaining prosody consistency across long contexts.", "conclusion": "By adopting the new tokenization approach, ProsodyLM outperforms traditional methods in understanding and generating speech with correct prosodic elements.", "key_contributions": ["Introduction of a new tokenization scheme for speech processing", "Improved retention of prosody information in LLMs", "Enhanced prosody processing capabilities observed during pre-training"], "limitations": "", "keywords": ["speech language models", "prosody", "tokenization", "LLMs", "speech processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20111", "pdf": "https://arxiv.org/pdf/2507.20111.pdf", "abs": "https://arxiv.org/abs/2507.20111", "title": "AI-Driven Generation of Old English: A Framework for Low-Resource Languages", "authors": ["Rodrigo Gabriel Salazar Alva", "Matías Nuñez", "Cristian López", "Javier Martín Arista"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Preserving ancient languages is essential for understanding humanity's\ncultural and linguistic heritage, yet Old English remains critically\nunder-resourced, limiting its accessibility to modern natural language\nprocessing (NLP) techniques. We present a scalable framework that uses advanced\nlarge language models (LLMs) to generate high-quality Old English texts,\naddressing this gap. Our approach combines parameter-efficient fine-tuning\n(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a\ndual-agent pipeline that separates the tasks of content generation (in English)\nand translation (into Old English). Evaluation with automated metrics (BLEU,\nMETEOR, and CHRF) shows significant improvements over baseline models, with\nBLEU scores increasing from 26 to over 65 for English-to-Old English\ntranslation. Expert human assessment also confirms high grammatical accuracy\nand stylistic fidelity in the generated texts. Beyond expanding the Old English\ncorpus, our method offers a practical blueprint for revitalizing other\nendangered languages, effectively uniting AI innovation with the goals of\ncultural preservation.", "AI": {"tldr": "A framework using LLMs for generating high-quality Old English texts, enhancing accessibility and preservation of endangered languages.", "motivation": "To address the critical under-resourcing of Old English, hindering its accessibility to NLP techniques and cultural understanding.", "method": "The framework employs Low-Rank Adaptation (LoRA) for efficient fine-tuning, combines data augmentation through backtranslation, and utilizes a dual-agent pipeline for content generation and translation tasks.", "result": "Significant improvements in translation quality, evidenced by an increase in BLEU scores from 26 to over 65 and confirmed high grammatical accuracy and stylistic fidelity by expert assessment.", "conclusion": "The method not only expands the Old English corpus but also serves as a model for revitalizing other endangered languages through AI.", "key_contributions": ["Development of a scalable framework for Old English text generation", "Combination of parameter-efficient fine-tuning with data augmentation", "Dual-agent pipeline for enhanced translation efficiency"], "limitations": "", "keywords": ["Old English", "Natural Language Processing", "Language Preservation", "Large Language Models", "Cultural Heritage"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20133", "pdf": "https://arxiv.org/pdf/2507.20133.pdf", "abs": "https://arxiv.org/abs/2507.20133", "title": "Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering", "authors": ["Anas Mohamed", "Azal Ahmad Khan", "Xinran Wang", "Ahmad Faraz Khan", "Shuwen Ge", "Saman Bahzad Khan", "Ayaan Ahmad", "Ali Anwar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative AI can now synthesize strikingly realistic images from text, yet\noutput quality remains highly sensitive to how prompts are phrased. Direct\nPreference Optimization (DPO) offers a lightweight, off-policy alternative to\nRL for automatic prompt engineering, but its token-level regularization leaves\nsemantic inconsistency unchecked as prompts that win higher preference scores\ncan still drift away from the user's intended meaning.\n  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency\nyet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an\nexponential weight proportional to the cosine distance between the original\nprompt and winning candidate in embedding space, softly down-weighting training\nsignals that would otherwise reward semantically mismatched prompts. We provide\nthe first analytical bound on semantic drift for preference-tuned prompt\ngenerators, showing that Sem-DPO keeps learned prompts within a provably\nbounded neighborhood of the original text. On three standard text-to-image\nprompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%\nhigher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,\nPickScore) than DPO, while also outperforming state-of-the-art baselines. These\nfindings suggest that strong flat baselines augmented with semantic weighting\nshould become the new standard for prompt-optimization studies and lay the\ngroundwork for broader, semantics-aware preference optimization in language\nmodels.", "AI": {"tldr": "Sem-DPO improves semantic consistency in automatic prompt engineering for generative AI by scaling the DPO loss with respect to cosine distance, leading to better prompt optimization outcomes.", "motivation": "Generative AI's ability to create realistic images from text is contingent on the phrasing of prompts; existing methods like Direct Preference Optimization (DPO) often fail to maintain the semantic consistency of prompts.", "method": "Sem-DPO employs a modified DPO loss that uses exponential weighting based on cosine distance in embedding space to discourage semantically mismatched prompts, while still optimizing for preference scores.", "result": "Sem-DPO provides 8-12% higher CLIP similarity and 5-9% higher human-preference scores compared to DPO and outperforms state-of-the-art baselines on multiple benchmarks.", "conclusion": "The findings advocate for the incorporation of semantic weighting in prompt optimization, establishing a framework for semantic-aware preference optimization in language models.", "key_contributions": ["Introduction of Sem-DPO for prompt optimization", "Analytical bound on semantic drift for preference-tuned prompts", "Empirical evidence of performance improvements over traditional DPO"], "limitations": "", "keywords": ["Generative AI", "Prompt Engineering", "Semantics", "Optimization", "Language Models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.20136", "pdf": "https://arxiv.org/pdf/2507.20136.pdf", "abs": "https://arxiv.org/abs/2507.20136", "title": "Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG", "authors": ["Baiyu Chen", "Wilson Wongso", "Xiaoqian Hu", "Yue Tan", "Flora Salim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD Cup 2025 Meta CRAG-MM Challenge", "summary": "This paper presents the technical solution developed by team CRUISE for the\nKDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn\n(CRAG-MM) challenge. The challenge aims to address a critical limitation of\nmodern Vision Language Models (VLMs): their propensity to hallucinate,\nespecially when faced with egocentric imagery, long-tail entities, and complex,\nmulti-hop questions. This issue is particularly problematic in real-world\napplications where users pose fact-seeking queries that demand high factual\naccuracy across diverse modalities. To tackle this, we propose a robust,\nmulti-stage framework that prioritizes factual accuracy and truthfulness over\ncompleteness. Our solution integrates a lightweight query router for\nefficiency, a query-aware retrieval and summarization pipeline, a dual-pathways\ngeneration and a post-hoc verification. This conservative strategy is designed\nto minimize hallucinations, which incur a severe penalty in the competition's\nscoring metric. Our approach achieved 3rd place in Task 1, demonstrating the\neffectiveness of prioritizing answer reliability in complex multi-modal RAG\nsystems. Our implementation is available at\nhttps://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .", "AI": {"tldr": "This paper presents a robust framework developed for the KDD Cup 2025 challenge aimed at improving factual accuracy in Vision Language Models.", "motivation": "The paper addresses the challenge of hallucination in Vision Language Models, particularly when processing egocentric imagery and complex queries that require high factual accuracy.", "method": "The solution is a multi-stage framework that includes a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, dual-pathways generation, and post-hoc verification.", "result": "The approach was effective in minimizing hallucinations, leading to a 3rd place finish in Task 1 of the competition, indicating the framework's reliability for complex multi-modal tasks.", "conclusion": "The findings support the importance of prioritizing answer reliability in multi-modal RAG systems, especially in health informatics and user-facing applications.", "key_contributions": ["Development of a multi-stage framework to enhance factual accuracy in VLMs", "Integration of efficient query processing methods", "Demonstrated effectiveness through KDD Cup performance"], "limitations": "", "keywords": ["Vision Language Models", "factual accuracy", "multi-modal systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20145", "pdf": "https://arxiv.org/pdf/2507.20145.pdf", "abs": "https://arxiv.org/abs/2507.20145", "title": "Multi-Agent Interactive Question Generation Framework for Long Document Understanding", "authors": ["Kesen Wang", "Daulet Toibazar", "Abdulrahman Alfulayt", "Abdulaziz S. Albadawi", "Ranya A. Alkahtani", "Asma A. Ibrahim", "Haneen A. Alhomoud", "Sherif Mohamed", "Pedro J. Moreno"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Document Understanding (DU) in long-contextual scenarios with complex layouts\nremains a significant challenge in vision-language research. Although Large\nVision-Language Models (LVLMs) excel at short-context DU tasks, their\nperformance declines in long-context settings. A key limitation is the scarcity\nof fine-grained training data, particularly for low-resource languages such as\nArabic. Existing state-of-the-art techniques rely heavily on human annotation,\nwhich is costly and inefficient. We propose a fully automated, multi-agent\ninteractive framework to generate long-context questions efficiently. Our\napproach efficiently generates high-quality single- and multi-page questions\nfor extensive English and Arabic documents, covering hundreds of pages across\ndiverse domains. This facilitates the development of LVLMs with enhanced\nlong-context understanding ability. Experimental results in this work have\nshown that our generated English and Arabic questions\n(\\textbf{AraEngLongBench}) are quite challenging to major open- and\nclose-source LVLMs. The code and data proposed in this work can be found in\nhttps://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and\nAnswer (QA) pairs and structured system prompts can be found in the Appendix.", "AI": {"tldr": "This paper presents a fully automated framework for generating long-context questions for documents, addressing the challenge of Document Understanding in low-resource languages.", "motivation": "To improve Document Understanding in long-context scenarios, particularly for low-resource languages like Arabic, where fine-grained training data is scarce.", "method": "The authors propose a multi-agent interactive framework that generates single- and multi-page questions efficiently for documents in English and Arabic.", "result": "Experimental results indicate that the generated questions (AraEngLongBench) pose significant challenges to current state-of-the-art LVLMs, highlighting the effectiveness of the proposed framework.", "conclusion": "The proposed method can facilitate improved long-context understanding in Large Vision-Language Models, with accessible code and data for further research.", "key_contributions": ["Development of a fully automated question generation framework for long-context documents.", "Introduction of the AraEngLongBench dataset, specifically for English and Arabic.", "Demonstration of the challenging nature of generated questions for existing LVLMs."], "limitations": "The framework relies on existing LVLMs, which may have limitations in understanding complex layouts.", "keywords": ["Document Understanding", "Long-context scenarios", "Vision-Language Models", "Q&A generation", "Arabic documents"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.20152", "pdf": "https://arxiv.org/pdf/2507.20152.pdf", "abs": "https://arxiv.org/abs/2507.20152", "title": "Goal Alignment in LLM-Based User Simulators for Conversational AI", "authors": ["Shuhaib Mehri", "Xiaocheng Yang", "Takyoung Kim", "Gokhan Tur", "Shikib Mehri", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "User simulators are essential to conversational AI, enabling scalable agent\ndevelopment and evaluation through simulated interactions. While current Large\nLanguage Models (LLMs) have advanced user simulation capabilities, we reveal\nthat they struggle to consistently demonstrate goal-oriented behavior across\nmulti-turn conversations--a critical limitation that compromises their\nreliability in downstream applications. We introduce User Goal State Tracking\n(UGST), a novel framework that tracks user goal progression throughout\nconversations. Leveraging UGST, we present a three-stage methodology for\ndeveloping user simulators that can autonomously track goal progression and\nreason to generate goal-aligned responses. Moreover, we establish comprehensive\nevaluation metrics for measuring goal alignment in user simulators, and\ndemonstrate that our approach yields substantial improvements across two\nbenchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a\ncritical gap in conversational AI and establish UGST as an essential framework\nfor developing goal-aligned user simulators.", "AI": {"tldr": "This paper introduces User Goal State Tracking (UGST), a framework for improving user simulators in conversational AI by tracking goal progression and generating goal-aligned responses.", "motivation": "User simulators are crucial for developing and evaluating conversational AI but currently lack consistent goal-oriented behavior, impacting their reliability.", "method": "The paper presents a three-stage methodology leveraging UGST to autonomously track user goal progression in conversations and generate goal-aligned responses.", "result": "The proposed approach demonstrates significant improvements in goal alignment across benchmarks MultiWOZ 2.4 and {\tau}-Bench.", "conclusion": "UGST addresses a critical gap in conversational AI, enhancing the development of goal-aligned user simulators and establishing a new standard for evaluation.", "key_contributions": ["Introduction of the UGST framework for tracking user goals", "Development of a three-stage methodology for autonomous user simulators", "Establishment of comprehensive evaluation metrics for user simulator goal alignment"], "limitations": "", "keywords": ["user simulators", "conversational AI", "goal alignment", "UGST", "Large Language Models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.20181", "pdf": "https://arxiv.org/pdf/2507.20181.pdf", "abs": "https://arxiv.org/abs/2507.20181", "title": "SGPO: Self-Generated Preference Optimization based on Self-Improver", "authors": ["Hyeonji Lee", "Daejin Jo", "Seohwan Yun", "Sungwoong Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs), despite their extensive pretraining on diverse\ndatasets, require effective alignment to human preferences for practical and\nreliable deployment. Conventional alignment methods typically employ off-policy\nlearning and depend on human-annotated datasets, which limits their broad\napplicability and introduces distribution shift issues during training. To\naddress these challenges, we propose Self-Generated Preference Optimization\nbased on Self-Improver (SGPO), an innovative alignment framework that leverages\nan on-policy self-improving mechanism. Specifically, the improver refines\nresponses from a policy model to self-generate preference data for direct\npreference optimization (DPO) of the policy model. Here, the improver and\npolicy are unified into a single model, and in order to generate higher-quality\npreference data, this self-improver learns to make incremental yet discernible\nimprovements to the current responses by referencing supervised fine-tuning\noutputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the\nproposed SGPO significantly improves performance over DPO and baseline\nself-improving methods without using external preference data.", "AI": {"tldr": "The paper presents Self-Generated Preference Optimization (SGPO), a new alignment framework for large language models that creates preference data through self-improvement without external datasets.", "motivation": "To overcome limitations in conventional alignment methods of large language models which rely on off-policy learning and human-annotated data, introducing issues of distribution shifts during training.", "method": "SGPO employs an on-policy self-improving mechanism that refines the responses of a policy model to self-generate preference data for direct preference optimization.", "result": "Experimental results demonstrate that SGPO significantly enhances performance on AlpacaEval 2.0 and Arena-Hard compared to direct preference optimization (DPO) and other baseline self-improving methods.", "conclusion": "The proposed SGPO framework effectively optimizes large language model responses by generating preference data internally, leading to improved performance without the need for external datasets.", "key_contributions": ["Introduction of Self-Generated Preference Optimization (SGPO) framework.", "Unification of improver and policy into a single model for enhanced preference data generation.", "Demonstration of improved performance metrics without external preference data."], "limitations": "", "keywords": ["large language models", "alignment methods", "self-improving mechanism"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.20185", "pdf": "https://arxiv.org/pdf/2507.20185.pdf", "abs": "https://arxiv.org/abs/2507.20185", "title": "SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding", "authors": ["Yuqi Yang", "Weiqi Wang", "Baixuan Xu", "Wei Fan", "Qing Zong", "Chunkit Chan", "Zheye Deng", "Xin Liu", "Yifan Gao", "Changlong Yu", "Chen Luo", "Yang Li", "Zheng Li", "Qingyu Yin", "Bing Yin", "Yangqiu Song"], "categories": ["cs.CL"], "comment": null, "summary": "Session history is a common way of recording user interacting behaviors\nthroughout a browsing activity with multiple products. For example, if an user\nclicks a product webpage and then leaves, it might because there are certain\nfeatures that don't satisfy the user, which serve as an important indicator of\non-the-spot user preferences. However, all prior works fail to capture and\nmodel customer intention effectively because insufficient information\nexploitation and only apparent information like descriptions and titles are\nused. There is also a lack of data and corresponding benchmark for explicitly\nmodeling intention in E-commerce product purchase sessions. To address these\nissues, we introduce the concept of an intention tree and propose a dataset\ncuration pipeline. Together, we construct a sibling multimodal benchmark,\nSessionIntentBench, that evaluates L(V)LMs' capability on understanding\ninter-session intention shift with four subtasks. With 1,952,177 intention\nentries, 1,132,145 session intention trajectories, and 13,003,664 available\ntasks mined using 10,905 sessions, we provide a scalable way to exploit the\nexisting session data for customer intention understanding. We conduct human\nannotations to collect ground-truth label for a subset of collected data to\nform an evaluation gold set. Extensive experiments on the annotated data\nfurther confirm that current L(V)LMs fail to capture and utilize the intention\nacross the complex session setting. Further analysis show injecting intention\nenhances LLMs' performances.", "AI": {"tldr": "The paper introduces SessionIntentBench, a benchmark for understanding user intention in E-commerce sessions, highlighting the inadequacy of current L(V)LMs in capturing intention across browsing sessions and demonstrating improvements through intentional data injection.", "motivation": "To improve the understanding and modeling of customer intention in E-commerce product purchase sessions, which traditional approaches have inadequately addressed.", "method": "The authors introduce the concept of an intention tree and a dataset curation pipeline, creating SessionIntentBench—a multimodal benchmark for evaluating L(V)LMs' capability on intention shifts with extensive session data.", "result": "The constructed dataset includes 1,952,177 intention entries and shows that L(V)LMs fail to effectively model user intention, which can be enhanced by injecting intention data.", "conclusion": "The study emphasizes the need for improved intention modeling in E-commerce and suggests that current L(V)LMs are insufficient, but can benefit from intention injection.", "key_contributions": ["Introduction of the intention tree concept", "Creation of the SessionIntentBench benchmark", "Demonstration of L(V)LMs' performance improvement through intention data injection"], "limitations": "The current benchmark is based on user session data, which may not encompass all possible user behaviors and intentions in E-commerce settings.", "keywords": ["E-commerce", "Customer intention", "Session data", "Multimodal benchmark", "L(V)LM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20419", "pdf": "https://arxiv.org/pdf/2507.20419.pdf", "abs": "https://arxiv.org/abs/2507.20419", "title": "Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?", "authors": ["Khloud AL Jallad", "Nada Ghneim", "Ghaida Rebdawi"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Natural Language Understanding (NLU) is a basic task in Natural Language\nProcessing (NLP). The evaluation of NLU capabilities has become a trending\nresearch topic that attracts researchers in the last few years, resulting in\nthe development of numerous benchmarks. These benchmarks include various tasks\nand datasets in order to evaluate the results of pretrained models via public\nleaderboards. Notably, several benchmarks contain diagnostics datasets designed\nfor investigation and fine-grained error analysis across a wide range of\nlinguistic phenomena. This survey provides a comprehensive review of available\nEnglish, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on\ntheir diagnostics datasets and the linguistic phenomena they covered. We\npresent a detailed comparison and analysis of these benchmarks, highlighting\ntheir strengths and limitations in evaluating NLU tasks and providing in-depth\nerror analysis. When highlighting the gaps in the state-of-the-art, we noted\nthat there is no naming convention for macro and micro categories or even a\nstandard set of linguistic phenomena that should be covered. Consequently, we\nformulated a research question regarding the evaluation metrics of the\nevaluation diagnostics benchmarks: \"Why do not we have an evaluation standard\nfor the NLU evaluation diagnostics benchmarks?\" similar to ISO standard in\nindustry. We conducted a deep analysis and comparisons of the covered\nlinguistic phenomena in order to support experts in building a global hierarchy\nfor linguistic phenomena in future. We think that having evaluation metrics for\ndiagnostics evaluation could be valuable to gain more insights when comparing\nthe results of the studied models on different diagnostics benchmarks.", "AI": {"tldr": "This survey reviews various NLU benchmarks and their diagnostic datasets, emphasizing their strengths and weaknesses.", "motivation": "The evaluation of NLU capabilities has become increasingly important, necessitating a comprehensive overview of existing benchmarks and their ability to facilitate error analysis.", "method": "The paper provides a detailed comparison and analysis of available English, Arabic, and Multilingual NLU benchmarks, focusing on diagnostics datasets and linguistic phenomena.", "result": "Identifies gaps in current benchmarks, such as the lack of naming conventions and standard evaluation metrics for diagnostics benchmarks.", "conclusion": "A standard for the evaluation metrics of NLU diagnostics benchmarks is proposed to enhance comparisons and insights in NLU research.", "key_contributions": ["Comprehensive review of NLU benchmarks across multiple languages.", "Identification of strengths and limitations in existing diagnostics datasets.", "Formulation of a research question regarding the need for standardized evaluation metrics."], "limitations": "Lacks a unified naming convention and standard set of linguistic phenomena for evaluations.", "keywords": ["Natural Language Understanding", "NLP", "Evaluation Metrics", "Diagnostics Datasets", "Linguistic Phenomena"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.20187", "pdf": "https://arxiv.org/pdf/2507.20187.pdf", "abs": "https://arxiv.org/abs/2507.20187", "title": "Diversity-Enhanced Reasoning for Subjective Questions", "authors": ["Yumeng Wang", "Zhiyuan Fan", "Jiayu Liu", "Yi R. Fung"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities\nhave shown strong performance on objective tasks, such as math reasoning and\ncoding. However, their effectiveness on subjective questions that may have\ndifferent responses from different perspectives is still limited by a tendency\ntowards homogeneous reasoning, introduced by the reliance on a single ground\ntruth in supervised fine-tuning and verifiable reward in reinforcement\nlearning. Motivated by the finding that increasing role perspectives\nconsistently improves performance, we propose MultiRole-R1, a\ndiversity-enhanced framework with multiple role perspectives, to improve the\naccuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an\nunsupervised data construction pipeline that generates reasoning chains that\nincorporate diverse role perspectives. We further employ reinforcement learning\nvia Group Relative Policy Optimization (GRPO) with reward shaping, by taking\ndiversity as a reward signal in addition to the verifiable reward. With\nspecially designed reward functions, we successfully promote perspective\ndiversity and lexical diversity, uncovering a positive relation between\nreasoning diversity and accuracy. Our experiment on six benchmarks demonstrates\nMultiRole-R1's effectiveness and generalizability in enhancing both subjective\nand objective reasoning, showcasing the potential of diversity-enhanced\ntraining in LRMs.", "AI": {"tldr": "MultiRole-R1 is a framework designed to enhance subjective reasoning in large reasoning models by incorporating multiple role perspectives and using diversity as a reward signal during reinforcement learning.", "motivation": "To address the limitations of large reasoning models on subjective questions, which are prone to homogeneous reasoning due to reliance on single ground truths and verifiable rewards.", "method": "The approach includes an unsupervised data construction pipeline for generating reasoning chains with diverse role perspectives, paired with reinforcement learning using Group Relative Policy Optimization that incorporates diversity as a reward alongside traditional rewards.", "result": "Experiments on six benchmarks demonstrate that MultiRole-R1 improves both subjective and objective reasoning by promoting perspective and lexical diversity, leading to enhanced accuracy.", "conclusion": "The research shows that diversity-enhanced training in large reasoning models can significantly improve their performance on subjective reasoning tasks.", "key_contributions": ["Introduction of MultiRole-R1 framework", "Use of multiple role perspectives for reasoning", "Combination of diversity as a reward signal in reinforcement learning"], "limitations": "", "keywords": ["Large reasoning models", "Diversity in reasoning", "Reinforcement learning"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.20208", "pdf": "https://arxiv.org/pdf/2507.20208.pdf", "abs": "https://arxiv.org/abs/2507.20208", "title": "IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs", "authors": ["Aviya Maimon", "Amir DN Cohen", "Gal Vishne", "Shauli Ravfogel", "Reut Tsarfaty"], "categories": ["cs.CL"], "comment": null, "summary": "Current evaluations of large language models (LLMs) rely on benchmark scores,\nbut it is difficult to interpret what these individual scores reveal about a\nmodel's overall skills. Specifically, as a community we lack understanding of\nhow tasks relate to one another, what they measure in common, how they differ,\nor which ones are redundant. As a result, models are often assessed via a\nsingle score averaged across benchmarks, an approach that fails to capture the\nmodels' wholistic strengths and limitations. Here, we propose a new evaluation\nparadigm that uses factor analysis to identify latent skills driving\nperformance across benchmarks. We apply this method to a comprehensive new\nleaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a\nsmall set of latent skills that largely explain performance. Finally, we turn\nthese insights into practical tools that identify redundant tasks, aid in model\nselection, and profile models along each latent skill.", "AI": {"tldr": "The paper introduces a new evaluation framework for large language models (LLMs) using factor analysis to uncover underlying skills driving performance on multiple benchmarks.", "motivation": "Current approaches to evaluating LLMs through singular benchmark scores are insufficient for understanding their comprehensive abilities and limitations.", "method": "The authors utilize factor analysis to analyze performance data from 60 LLMs across 44 tasks, identifying latent skills responsible for their performance.", "result": "The analysis reveals a small set of latent skills that explain a significant portion of the performance variance across tasks.", "conclusion": "The insights gained provide tools for identifying redundant tasks, assist in selecting appropriate models, and allow for profiling models based on their latent skills.", "key_contributions": ["Proposes a new evaluation paradigm for LLMs using factor analysis", "Identifies latent skills that explain performance across multiple benchmarks", "Provides practical tools for model selection and task redundancy identification"], "limitations": "The study is dependent on the quality and scope of the tasks included in the evaluation.", "keywords": ["large language models", "evaluation", "factor analysis", "performance benchmarking", "latent skills"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20210", "pdf": "https://arxiv.org/pdf/2507.20210.pdf", "abs": "https://arxiv.org/abs/2507.20210", "title": "Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation", "authors": ["Minh Hoang Nguyen", "Thuat Thien Nguyen", "Minh Nhat Ta"], "categories": ["cs.CL", "68T50, 68T05", "I.2.7; I.7"], "comment": "11 pages, 6 figures", "summary": "News recommendation systems play a vital role in mitigating information\noverload by delivering personalized news content. A central challenge is to\neffectively model both multi-view news representations and the dynamic nature\nof user interests, which often span both short- and long-term preferences.\nExisting methods typically rely on single-view features of news articles (e.g.,\ntitles or categories) or fail to comprehensively capture user preferences\nacross time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news\nrecommendation framework that integrates NAML for attentive multi-view news\nmodeling and LSTUR for capturing both long- and short-term user\nrepresentations. Our model also incorporates BERT-based word embeddings to\nenhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely\nused benchmarks, MIND-small and MIND-large. Experimental results show that\nCo-NAML-LSTUR achieves substantial improvements over most state-of-the-art\nbaselines on MIND-small and MIND-large, respectively. These results demonstrate\nthe effectiveness of combining multi-view news representations with dual-scale\nuser modeling. The implementation of our model is publicly available at\nhttps://github.com/MinhNguyenDS/Co-NAML-LSTUR.", "AI": {"tldr": "Co-NAML-LSTUR proposes a hybrid framework for news recommendation that addresses multi-view news representations and user preferences over time.", "motivation": "To mitigate information overload in news consumption by delivering personalized content that accurately models dynamic user interests.", "method": "The proposed model integrates NAML for attentive multi-view news modeling and LSTUR for capturing both short- and long-term user representations, enhanced by BERT-based word embeddings.", "result": "Experimental results indicate that Co-NAML-LSTUR significantly outperforms various state-of-the-art baselines on both MIND-small and MIND-large benchmarks.", "conclusion": "Combining multi-view news representations with dual-scale user modeling proves effective in improving news recommendation systems.", "key_contributions": ["Introduces Co-NAML-LSTUR framework for hybrid news recommendation", "Enhances semantic feature extraction using BERT-based embeddings", "Demonstrates improved performance on established benchmarks (MIND-small, MIND-large)"], "limitations": "", "keywords": ["news recommendation", "personalization", "multi-view representation", "user modeling", "BERT"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2507.20241", "pdf": "https://arxiv.org/pdf/2507.20241.pdf", "abs": "https://arxiv.org/abs/2507.20241", "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models", "authors": ["Yi Feng", "Jiaqi Wang", "Wenxuan Zhang", "Zhuang Chen", "Yutong Shen", "Xiyao Xiao", "Minlie Huang", "Liping Jing", "Jian Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications.", "AI": {"tldr": "This paper introduces a framework that improves mental health support using large language models through an Interactive Narrative Therapist and an Innovative Moment Assessment.", "motivation": "Address the limitations of current mental health support approaches that lack realism in psychotherapy and do not effectively track therapeutic progression.", "method": "The framework has two components: INT simulates expert narrative therapists and IMA evaluates effectiveness by tracking critical narrative shifts in client speech.", "result": "Experimental results show that INT outperforms standard LLMs in therapeutic quality and depth through experiments on 260 simulated clients and 230 human participants.", "conclusion": "The effectiveness of INT was demonstrated in creating high-quality support conversations, benefiting social applications.", "key_contributions": ["Introduction of Interactive Narrative Therapist (INT) for simulating expert therapy", "Development of Innovative Moment Assessment (IMA) for evaluating therapy effectiveness", "Demonstration of improved therapeutic outcomes compared to standard LLMs"], "limitations": "", "keywords": ["mental health", "large language models", "narrative therapy", "interactive systems", "therapy assessment"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2412.06336", "pdf": "https://arxiv.org/pdf/2412.06336.pdf", "abs": "https://arxiv.org/abs/2412.06336", "title": "A Combined Channel Approach for Decoding Intracranial EEG Signals: Enhancing Accuracy through Spatial Information Integration", "authors": ["Maryam Ostadsharif Memar", "Navid Ziaei", "Behzad Nazari"], "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "Intracranial EEG (iEEG) recording, characterized by high spatial and temporal\nresolution and superior signal-to-noise ratio (SNR), enables the development of\nprecise brain-computer interface (BCI) systems for neural decoding. However,\nthe invasive nature of the procedure significantly limits the availability of\niEEG datasets in terms of both the number of participants and the duration of\nrecorded sessions. To address this limitation, we propose a single-participant\nmachine learning model optimized for decoding iEEG signals. The model employs\n18 key features and operates in two modes: best channel and combined channel.\nThe combined channel mode integrates spatial information from multiple brain\nregions, leading to superior classification performance. Evaluations across\nthree datasets -- Music Reconstruction, Audio Visual, and AJILE12 --\ndemonstrate that the combined channel mode consistently outperforms the best\nchannel mode across all classifiers. In the best-performing cases, Random\nForest achieved an F1 score of 0.81 +/- 0.05 in the Music Reconstruction\ndataset and 0.82 +/- 0.10 in the Audio Visual dataset, while XGBoost achieved\nan F1 score of 0.84 +/- 0.08 in the AJILE12 dataset. Furthermore, the analysis\nof brain region contributions in the combined channel mode revealed that the\nmodel identifies relevant brain regions aligned with physiological expectations\nfor each task and effectively combines data from electrodes in these regions to\nachieve high performance. These findings highlight the potential of integrating\nspatial information across brain regions to improve task decoding, offering new\navenues for advancing BCI systems and neurotechnological applications.", "AI": {"tldr": "This paper presents a machine learning model for Intracranial EEG (iEEG) signal decoding, showing superior performance through the integration of spatial information from multiple brain regions.", "motivation": "The invasive nature of iEEG recording limits dataset availability, necessitating the development of effective machine learning models for neural decoding.", "method": "A single-participant model using 18 key features, operating in 'best channel' and 'combined channel' modes, was evaluated across three datasets (Music Reconstruction, Audio Visual, AJILE12).", "result": "The combined channel mode produced consistently higher classification performance, with Random Forest achieving up to 0.82 F1 score and XGBoost 0.84 F1 score in different datasets, outperforming the best channel mode.", "conclusion": "Integrating spatial information from multiple brain regions enhances task decoding in iEEG, presenting opportunities for advancements in BCI systems and neurotechnology.", "key_contributions": ["Introduction of a combined channel mode for iEEG decoding", "Demonstrated superior classification performance over best channel mode", "Identified relevant brain regions aligned with physiological expectations"], "limitations": "", "keywords": ["Intracranial EEG", "Machine Learning", "Brain-Computer Interface", "Neurotechnology", "Signal Decoding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20249", "pdf": "https://arxiv.org/pdf/2507.20249.pdf", "abs": "https://arxiv.org/abs/2507.20249", "title": "Modeling Professionalism in Expert Questioning through Linguistic Differentiation", "authors": ["Giulia D'Agostino", "Chung-Chi Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Professionalism is a crucial yet underexplored dimension of expert\ncommunication, particularly in high-stakes domains like finance. This paper\ninvestigates how linguistic features can be leveraged to model and evaluate\nprofessionalism in expert questioning. We introduce a novel annotation\nframework to quantify structural and pragmatic elements in financial analyst\nquestions, such as discourse regulators, prefaces, and request types. Using\nboth human-authored and large language model (LLM)-generated questions, we\nconstruct two datasets: one annotated for perceived professionalism and one\nlabeled by question origin. We show that the same linguistic features correlate\nstrongly with both human judgments and authorship origin, suggesting a shared\nstylistic foundation. Furthermore, a classifier trained solely on these\ninterpretable features outperforms gemini-2.0 and SVM baselines in\ndistinguishing expert-authored questions. Our findings demonstrate that\nprofessionalism is a learnable, domain-general construct that can be captured\nthrough linguistically grounded modeling.", "AI": {"tldr": "The paper explores modeling and evaluating professionalism in expert communication using linguistic features, specifically in financial analyst questions.", "motivation": "To address the underexplored dimension of professionalism in expert communication, especially in high-stakes fields like finance.", "method": "It introduces a novel annotation framework to quantify linguistic elements in financial analyst questions and uses both human-crafted and LLM-generated datasets for analysis.", "result": "The study demonstrates that specific linguistic features correlate with both human judgments of professionalism and authorship origin, achieving better performance than existing baselines with a feature-based classifier.", "conclusion": "Professionalism can be modeled as a linguistically grounded construct that is applicable across various domains.", "key_contributions": ["Introduces a new framework for annotating professionalism in expert communication.", "Constructs datasets with human-authored and LLM-generated financial analyst questions.", "Shows that linguistic features are effective in classifying professionalism in expert questions."], "limitations": "The study is centered on a specific domain (finance) and may not generalize across all fields of expert communication.", "keywords": ["professionalism", "linguistic features", "financial communication", "expert questions", "classification"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.04429", "pdf": "https://arxiv.org/pdf/2501.04429.pdf", "abs": "https://arxiv.org/abs/2501.04429", "title": "User-Centered-Design as an Empty Signifier in the Context of Developing Digital Applications", "authors": ["Murat Sariyar"], "categories": ["cs.HC", "K.4.2"], "comment": "11 pages", "summary": "To reduce cycles of rejection and redesign -- especially in the absence of\nclear acceptance criteria and the diversity of possible development paths --\nUser-Centered Design (UCD) has become a central methodology in computer\nscience, emphasizing the integration of user perspectives throughout the entire\nsystem lifecycle. Despite its widespread adoption, however, UCD remains\nconceptually ambiguous and theoretically underdeveloped. This paper addresses\nthat gap by drawing on the theories of Ernesto Laclau and Jacques Lacan to\nanalyze UCD as a potential empty signifier: a term that gains rhetorical power\nprecisely through its semantic openness. We argue that this ambiguity enables\nUCD to unify diverse and sometimes conflicting expectations under a shared\nlabel, which both empowers participatory design practices and conceals\nunderlying tensions. Acknowledging UCD as an empty signifier allows for a more\ncritical engagement with its practical and symbolic functions, revealing how it\ncan foster inclusivity, empathy, and user empowerment, but also how it risks\nideological capture and conceptual dilution. This theoretical reframing opens\nnew pathways for reflection and renewal within sociotechnical system design.", "AI": {"tldr": "This paper critiques User-Centered Design (UCD) by evaluating its conceptual ambiguities and theoretical underdevelopment, proposing a framework for its analysis using Laclau and Lacan's theories.", "motivation": "To address the conceptual ambiguities and theoretical gaps in User-Centered Design (UCD) and understand its impact on participatory design practices.", "method": "The paper utilizes the theories of Ernesto Laclau and Jacques Lacan to analyze UCD as an empty signifier within the context of sociotechnical system design.", "result": "The analysis reveals that UCD acts as an empty signifier that unifies conflicting expectations while fostering inclusivity and user empowerment, but also risks ideological capture and conceptual dilution.", "conclusion": "A critical engagement with UCD as an empty signifier can foster reflection and renewal in sociotechnical system design.", "key_contributions": ["Theoretical framework for analyzing UCD as an empty signifier.", "Insight into the dual role of UCD in participatory design practices.", "Identification of potential risks associated with UCD's conceptual ambiguity."], "limitations": "", "keywords": ["User-Centered Design", "sociotechnical systems", "Laclau", "Lacan", "design ambiguity"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.20252", "pdf": "https://arxiv.org/pdf/2507.20252.pdf", "abs": "https://arxiv.org/abs/2507.20252", "title": "Post-Completion Learning for Language Models", "authors": ["Xiang Fei", "Siqi Wang", "Shu Wei", "Yuxiang Nie", "Wei Shi", "Hao Feng", "Can Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>}) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency.", "AI": {"tldr": "A new training framework, Post-Completion Learning (PCL), enhances language model performance by utilizing the space after model output completion for self-evaluation and reasoning improvements.", "motivation": "Current language model training paradigms overlook learning opportunities that arise after the generation of the <eos> token.", "method": "A white-box reinforcement learning method allows the model to generate self-assessments and reward predictions, optimizing reasoning and evaluation capabilities through dual-track SFT and mixed RL training.", "result": "Experimental results show consistent improvements in model performance on various datasets compared to traditional SFT and RL methods.", "conclusion": "PCL presents a new approach to language model training that enhances output quality while preserving deployment efficiency.", "key_contributions": ["Introduction of Post-Completion Learning framework", "Utilization of post-completion space for self-assessment", "Hybrid optimization through dual-track SFT and reinforcement learning"], "limitations": "", "keywords": ["Post-Completion Learning", "Reinforcement Learning", "Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.00946", "pdf": "https://arxiv.org/pdf/2503.00946.pdf", "abs": "https://arxiv.org/abs/2503.00946", "title": "A Review of LLM-Assisted Ideation", "authors": ["Sitong Li", "Stefano Padilla", "Pierre Le Bras", "Junyu Dong", "Mike Chantler"], "categories": ["cs.HC"], "comment": null, "summary": "We present a comprehensive, in-depth review of ideation assisted by large\nlanguage models (LLMs), highlighting emerging trends and identifying\nunaddressed research gaps. In total, we examined 61 studies investigating the\napplication of LLMs in both group and individual ideation processes. From these\nstudies, we derived the Hourglass Ideation Framework for LLM-assisted ideation,\ncomprising three phases and seven key ideation stages, which served as the\nbasis for our systematic survey. Our analysis reveals that LLMs are most\nfrequently used for idea generation and refinement, but their use in scope\nspecification, foundational material structuring and multi-idea evaluation and\nselection remains limited. We provide our findings in extensive tabular and\nonline formats. These catalogues detail research on LLM-assisted, purely\nLLM-based, and human-only activities across the seven ideation stages for each\nof the 61 studies. These also detail creative domains, publication outlets,\ninteraction designs, user study designs, and assessment methods. Our analysis\nof system interaction design reveals a predominant focus on supporting\nindividual ideation activities and text-based interaction, with a growing trend\nof incorporating multimedia elements. However, in group ideation, tools and\ninteraction modalities targeting both synchronous and asynchronous\ncollaboration are much scarcer. We synthesize the primary findings of our\nreview and outline promising directions for future research in LLM-assisted\nideation. We hope this review will help researchers quickly gain an overview of\nthis rapidly expanding area, efficiently locate relevant work, and identify\nunderexplored areas for further investigation. In addition, we believe the\nframework we present here will form the basis for the development of future\nproblem and solution space taxonomies, and methodologies for LLM-assisted\nideation development and use.", "AI": {"tldr": "This paper reviews 61 studies on LLM-assisted ideation, introducing the Hourglass Ideation Framework and identifying key trends and gaps in research.", "motivation": "To explore the application of large language models in ideation processes and establish a framework for understanding their impact.", "method": "A systematic review of 61 studies on LLM-assisted ideation with extensive analysis of interaction designs and ideation stages.", "result": "LLMs are mainly used for idea generation and refinement, but usage in scope specification and multi-idea evaluation is limited. The review presents detailed findings of LLM applications across various ideation stages.", "conclusion": "The review outlines the findings to aid researchers in navigating the field and suggests directions for future research in LLM-assisted ideation.", "key_contributions": ["Introduction of the Hourglass Ideation Framework for LLM-assisted ideation", "Identification of underexplored areas in the application of LLMs in ideation", "Cataloguing extensive research data across ideation stages and interaction designs"], "limitations": "Limited exploration of LLM use in group ideation and interaction modalities for collaboration.", "keywords": ["Large Language Models", "Ideation", "Human-Computer Interaction", "Machine Learning", "Research Framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.20264", "pdf": "https://arxiv.org/pdf/2507.20264.pdf", "abs": "https://arxiv.org/abs/2507.20264", "title": "EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms", "authors": ["Abeer Aldayel", "Areej Alokaili"], "categories": ["cs.CL"], "comment": "Under review for publication", "summary": "Shaping inclusive representations that embrace diversity and ensure fair\nparticipation and reflections of values is at the core of many\nconversation-based models. However, many existing methods rely on surface\ninclusion using mention of user demographics or behavioral attributes of social\ngroups. Such methods overlook the nuanced, implicit expression of opinion\nembedded in conversations. Furthermore, the over-reliance on overt cues can\nexacerbate misalignment and reinforce harmful or stereotypical representations\nin model outputs. Thus, we took a step back and recognized that equitable\ninclusion needs to account for the implicit expression of opinion and use the\nstance of responses to validate the normative alignment. This study aims to\nevaluate how opinions are represented in NLP or computational models by\nintroducing an alignment evaluation framework that foregrounds implicit, often\noverlooked conversations and evaluates the normative social views and\ndiscourse. Our approach models the stance of responses as a proxy for the\nunderlying opinion, enabling a considerate and reflective representation of\ndiverse social viewpoints. We evaluate the framework using both (i)\npositive-unlabeled (PU) online learning with base classifiers, and (ii)\ninstruction-tuned language models to assess post-training alignment. Through\nthis, we provide a lens on how implicit opinions are (mis)represented and offer\na pathway toward more inclusive model behavior.", "AI": {"tldr": "This paper develops an alignment evaluation framework to better represent implicit opinions in NLP models, addressing the limitations of current methods that focus on overt user demographics.", "motivation": "The study addresses the shortcomings of existing conversation-based models that rely on surface-level inclusion, which can reinforce harmful stereotypes and misalignments in representations.", "method": "The proposed framework evaluates implicit expressions of opinion in conversations by modeling the stance of responses and utilizing both positive-unlabeled learning and instruction-tuned language models for alignment assessment.", "result": "The evaluation reveals a pathway toward more equitable inclusion in model behavior by focusing on implicit opinions rather than surface demographics.", "conclusion": "By foregrounding implicit opinions, the framework promotes a more reflective and diverse representation in computational models, paving the way for improved alignment with normative social views.", "key_contributions": ["Introduces a novel alignment evaluation framework for implicit opinions in NLP models.", "Models stance of responses as a proxy for underlying opinions, enhancing representation.", "Utilizes advanced learning techniques to assess model alignment post-tuning."], "limitations": "", "keywords": ["NLP", "implicit opinions", "model alignment", "inclusion", "stance modeling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.02622", "pdf": "https://arxiv.org/pdf/2504.02622.pdf", "abs": "https://arxiv.org/abs/2504.02622", "title": "Exploring undercurrents of learning tensions in an LLM-enhanced landscape: A student-centered qualitative perspective on LLM vs Search", "authors": ["Rahul R. Divekar", "Sophia Guerra", "Lisette Gonzalez", "Natasha Boos", "Helen Zhou"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are transforming how students learn by providing\nreadily available tools that can quickly augment or complete various learning\nactivities with non-trivial performance. Similar paradigm shifts have occurred\nin the past with the introduction of search engines and Wikipedia, which\nreplaced or supplemented traditional information sources such as libraries and\nbooks. This study investigates the potential for LLMs to represent the next\nshift in learning, focusing on their role in information discovery and\nsynthesis compared to existing technologies, such as search engines. Using a\nwithin-subjects, counterbalanced design, participants learned new topics using\na search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews\nexplored students' reflections, preferences, pain points, and overall\nperceptions. We present analysis of their responses that show nuanced insights\ninto when, why, and how students prefer LLMs over search engines, offering\nimplications for educators, policymakers, and technology developers navigating\nthe evolving educational landscape.", "AI": {"tldr": "This study explores the impact of large language models (LLMs) on student learning by comparing their use with traditional search engines.", "motivation": "To investigate the transformative potential of LLMs in education and their role in information discovery and synthesis.", "method": "A within-subjects, counterbalanced design was used, where participants learned using Google and ChatGPT, followed by interviews to gather their reflections and preferences.", "result": "Analysis revealed nuanced insights into when and why students prefer LLMs over traditional search engines, highlighting unique advantages and challenges.", "conclusion": "LLMs may represent a significant shift in educational technology, necessitating adaptation from educators and developers.", "key_contributions": ["Comparison of LLMs and traditional search engines in educational contexts", "Insights into student preferences and experiences with LLMs", "Implications for future educational tools and policies"], "limitations": "The study is limited to specific learning topics and may not generalize across all subjects or demographics.", "keywords": ["Large language models", "Student learning", "Information synthesis", "Educational technology", "Search engines"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.20278", "pdf": "https://arxiv.org/pdf/2507.20278.pdf", "abs": "https://arxiv.org/abs/2507.20278", "title": "MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning", "authors": ["Kang Yang", "Jingxue Chen", "Qingkun Tang", "Tianxiang Zhang", "Qianchun Lu"], "categories": ["cs.CL"], "comment": "12pages,3figures", "summary": "Large language models (LLMs) face significant challenges in effectively\nleveraging sequential environmental feedback (EF) signals, such as natural\nlanguage evaluations, for feedback-independent chain-of-thought (CoT)\nreasoning. Existing approaches either convert EF into scalar rewards, losing\nrich contextual information, or employ refinement datasets, failing to exploit\nthe multi-step and discrete nature of EF interactions. To address these\nlimitations, we propose MoL-RL, a novel training paradigm that integrates\nmulti-step EF signals into LLMs through a dual-objective optimization\nframework. Our method combines MoL (Mixture-of-Losses) continual training,\nwhich decouples domain-specific EF signals (optimized via cross-entropy loss)\nand general language capabilities (preserved via Kullback-Leibler divergence),\nwith GRPO-based post-training to distill sequential EF interactions into\nsingle-step inferences. This synergy enables robust feedback-independent\nreasoning without relying on external feedback loops. Experimental results on\nmathematical reasoning (MATH-500, AIME24/AIME25) and code generation\n(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art\nperformance with the Qwen3-8B model, while maintaining strong generalization\nacross model scales (Qwen3-4B). This work provides a promising approach for\nleveraging multi-step textual feedback to enhance LLMs' reasoning capabilities\nin diverse domains.", "AI": {"tldr": "Introduction of MoL-RL, a novel training method for LLMs that uses multi-step environmental feedback to improve chain-of-thought reasoning without the need for external feedback.", "motivation": "LLMs struggle with sequential environmental feedback signals, leading to inefficiencies in reasoning processes. Existing methods either simplify feedback too much or do not fully utilize the complexity of EF interactions.", "method": "MoL-RL employs a dual-objective optimization framework that integrates Mixture-of-Losses continual training to effectively combine domain-specific feedback and general language capabilities, along with GRPO-based post-training to refine sequential interactions into single-step actions.", "result": "MoL-RL demonstrates state-of-the-art results on benchmarks for mathematical reasoning and code generation, outperforming existing methods while exhibiting strong generalization across different model sizes.", "conclusion": "The proposed method effectively enhances LLM reasoning capabilities by leveraging complex multi-step textual feedback, promising improvements across various applications.", "key_contributions": ["Introduction of MoL-RL as a new optimization paradigm for training LLMs", "Utilization of multi-step EF signals to enhance reasoning", "Demonstrated state-of-the-art performance on key benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought Reasoning", "Multi-step Feedback"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.20279", "pdf": "https://arxiv.org/pdf/2507.20279.pdf", "abs": "https://arxiv.org/abs/2507.20279", "title": "What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations", "authors": ["Katharina Trinley", "Toshiki Nakai", "Tatiana Anikina", "Tanja Baeumel"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Large language models (LLMs) excel at multilingual tasks, yet their internal\nlanguage processing remains poorly understood. We analyze how Aya-23-8B, a\ndecoder-only LLM trained on balanced multilingual data, handles code-mixed,\ncloze, and translation tasks compared to predominantly monolingual models like\nLlama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization\nanalyses, we find: (1) Aya-23 activates typologically related language\nrepresentations during translation, unlike English-centric models that rely on\na single pivot language; (2) code-mixed neuron activation patterns vary with\nmixing rates and are shaped more by the base language than the mixed-in one;\nand (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in\nfinal layers, diverging from prior findings on decoder-only models. Neuron\noverlap analysis further shows that script similarity and typological relations\nimpact processing across model types. These findings reveal how multilingual\ntraining shapes LLM internals and inform future cross-lingual transfer\nresearch.", "AI": {"tldr": "The study investigates how the Aya-23-8B language model processes multilingual tasks compared to monolingual models, revealing distinct activation patterns for code-mixed and translation tasks.", "motivation": "To understand the internal language processing of large language models (LLMs), particularly in multilingual contexts.", "method": "The paper employs logit lens and neuron specialization analyses to study the activation patterns of Aya-23-8B in comparison to Llama 3 and Chinese-LLaMA-2.", "result": "The findings indicate that Aya-23 uniquely activates language representations based on typing relatedness during translation and showcases varying code-mixed neuron activation influenced by the base language.", "conclusion": "The results enhance the understanding of how multilingual training influences the internal workings of LLMs and could guide future research on cross-lingual transfer.", "key_contributions": ["Analysis of code-mixed and cloze tasks in a multilingual LLM", "Comparative evaluation against predominantly monolingual models", "Insights into neuron specialization and activation patterns in multilingual contexts"], "limitations": "", "keywords": ["large language models", "multilingual tasks", "code-mixing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20301", "pdf": "https://arxiv.org/pdf/2507.20301.pdf", "abs": "https://arxiv.org/abs/2507.20301", "title": "Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation", "authors": ["Abdullah Alabdullah", "Lifeng Han", "Chenghua Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Dialectal Arabic (DA) poses a persistent challenge for natural language\nprocessing (NLP), as most everyday communication in the Arab world occurs in\ndialects that diverge significantly from Modern Standard Arabic (MSA). This\nlinguistic divide limits access to digital services and educational resources\nand impedes progress in Arabic machine translation. This paper presents two\ncore contributions to advancing DA-MSA translation for the Levantine, Egyptian,\nand Gulf dialects, particularly in low-resource and computationally constrained\nsettings: a comprehensive evaluation of training-free prompting techniques, and\nthe development of a resource-efficient fine-tuning pipeline. Our evaluation of\nprompting strategies across six large language models (LLMs) found that\nfew-shot prompting consistently outperformed zero-shot, chain-of-thought, and\nour proposed Ara-TEaR method. GPT-4o achieved the highest performance across\nall prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a\nCHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint\nmulti-dialect trained models outperformed single-dialect counterparts by over\n10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than\n1% performance loss. The results and insights of our experiments offer a\npractical blueprint for improving dialectal inclusion in Arabic NLP, showing\nthat high-quality DA-MSA machine translation is achievable even with limited\nresources and paving the way for more inclusive language technologies.", "AI": {"tldr": "This paper addresses the challenges in Dialectal Arabic to Modern Standard Arabic translation by evaluating training-free prompting techniques and developing a resource-efficient fine-tuning pipeline.", "motivation": "Dialectal Arabic (DA) presents significant challenges in NLP, as most communication in the Arab world occurs in dialects that differ from Modern Standard Arabic (MSA), limiting access to digital services and resources.", "method": "The paper evaluates training-free prompting techniques across six large language models (LLMs) and develops a resource-efficient fine-tuning pipeline for translating various Arabic dialects.", "result": "Few-shot prompting consistently outperformed other approaches, with GPT-4o achieving the highest performance. A quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming GPT-4o, while multi-dialect models surpassed single-dialect models by over 10% CHrF++. ", "conclusion": "The findings provide a blueprint for improving inclusion in Arabic NLP, demonstrating that quality DA-MSA translation is achievable with limited resources and can lead to better language technologies.", "key_contributions": ["Evaluation of training-free prompting techniques for DA-MSA translation.", "Development of a resource-efficient fine-tuning pipeline for dialectal models.", "Introduction of quantization techniques that reduce memory usage significantly with minimal performance loss."], "limitations": "", "keywords": ["Dialectal Arabic", "Machine Translation", "Natural Language Processing", "Fine-tuning", "Large Language Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887.pdf", "abs": "https://arxiv.org/abs/2504.13887", "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite increasing AI chatbot deployment in public discourse, empirical\nevidence on their capacity to foster intercultural empathy remains limited.\nThrough a randomized experiment, we assessed how different AI deliberation\napproaches--cross-cultural deliberation (presenting other-culture\nperspectives), own-culture deliberation (representing participants' own\nculture), and non-deliberative control--affect intercultural empathy across\nAmerican and Latin American participants. Cross-cultural deliberation increased\nintercultural empathy among American participants through positive emotional\nengagement, but produced no such effects for Latin American participants, who\nperceived AI responses as culturally inauthentic despite explicit prompting to\nrepresent their cultural perspectives. Our analysis of participant-driven\nfeedback, where users directly flagged and explained culturally inappropriate\nAI responses, revealed systematic gaps in AI's representation of Latin American\ncontexts that persist despite sophisticated prompt engineering. These findings\ndemonstrate that current approaches to AI cultural alignment--including\nlinguistic adaptation and explicit cultural prompting--cannot fully address\ndeeper representational asymmetries in AI systems. Our work advances both\ndeliberation theory and AI alignment research by revealing how the same AI\nsystem can simultaneously promote intercultural understanding for one cultural\ngroup while failing for another, with critical implications for designing\nequitable AI systems for cross-cultural democratic discourse.", "AI": {"tldr": "This study investigates how different AI deliberation approaches affect intercultural empathy, revealing that while cross-cultural deliberation enhances empathy for Americans, it fails to resonate with Latin Americans due to perceived cultural inauthenticity.", "motivation": "To explore the effectiveness of AI chatbots in fostering intercultural empathy across diverse cultural backgrounds, particularly amidst the rise of AI in public discourse.", "method": "A randomized experiment comparing three AI deliberation approaches: cross-cultural deliberation, own-culture deliberation, and a non-deliberative control group, involving American and Latin American participants.", "result": "Cross-cultural deliberation increased intercultural empathy among American participants but was ineffective for Latin American participants, who found the AI responses culturally inauthentic.", "conclusion": "Current AI cultural alignment methods do not sufficiently address representational issues, highlighting the need for equitable AI systems in cross-cultural dialogue.", "key_contributions": ["Demonstrates the varying efficacy of AI deliberation approaches on different cultural groups.", "Reveals systematic cultural representation gaps in AI responses, particularly for Latin Americans.", "Advances both deliberation theory and AI alignment research with implications for equitable AI design."], "limitations": "The study highlights limitations in the AI's ability to represent complex cultural contexts despite advanced prompt engineering.", "keywords": ["AI", "intercultural empathy", "cross-cultural deliberation", "AI alignment", "cultural representation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20343", "pdf": "https://arxiv.org/pdf/2507.20343.pdf", "abs": "https://arxiv.org/abs/2507.20343", "title": "DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns", "authors": ["Bernd J. Kröger"], "categories": ["cs.CL"], "comment": "10 pages, 29 references, 2 figures, supplementary material", "summary": "We present DYNARTmo, a dynamic articulatory model designed to visualize\nspeech articulation processes in a two-dimensional midsagittal plane. The model\nbuilds upon the UK-DYNAMO framework and integrates principles of articulatory\nunderspecification, segmental and gestural control, and coarticulation.\nDYNARTmo simulates six key articulators based on ten continuous and six\ndiscrete control parameters, allowing for the generation of both vocalic and\nconsonantal articulatory configurations. The current implementation is embedded\nin a web-based application (SpeechArticulationTrainer) that includes sagittal,\nglottal, and palatal views, making it suitable for use in phonetics education\nand speech therapy. While this paper focuses on the static modeling aspects,\nfuture work will address dynamic movement generation and integration with\narticulatory-acoustic modules.", "AI": {"tldr": "DYNARTmo is a dynamic articulatory model that visualizes speech articulation processes and is implemented in a web-based application for educational and therapeutic use.", "motivation": "To improve the visualization of speech articulation processes in phonetics education and speech therapy by utilizing a dynamic model.", "method": "DYNARTmo models six key articulators using ten continuous and six discrete control parameters, allowing for visualization of vocalic and consonantal configurations.", "result": "The model is integrated into the SpeechArticulationTrainer web application, providing multiple views (sagittal, glottal, palatal) for enhanced learning.", "conclusion": "Future developments will focus on dynamic movement generation and integration with acoustic modules to enhance the model's functionality.", "key_contributions": ["Development of DYNARTmo for visualizing articulatory processes", "Integration into a web-based application for phonetics education", "Focus on coarticulation and articulatory underspecification in the model"], "limitations": "Current implementation emphasizes static modeling; dynamic aspects and full articulatory-acoustic integration are to be further developed.", "keywords": ["articulatory model", "speech visualization", "phonetics education", "speech therapy", "coarticulation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.20352", "pdf": "https://arxiv.org/pdf/2507.20352.pdf", "abs": "https://arxiv.org/abs/2507.20352", "title": "RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing", "authors": ["Hao Xiang", "Tianyi Tang", "Yang Su", "Bowen Yu", "An Yang", "Fei Huang", "Yichang Zhang", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Jingren Zhou", "Junyang Lin", "Le Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shown outstanding\npotential for role-playing applications. Evaluating these capabilities is\nbecoming crucial yet remains challenging. Existing benchmarks mostly adopt a\n\\textbf{character-centric} approach, simplify user-character interactions to\nisolated Q&A tasks, and fail to reflect real-world applications. To address\nthis limitation, we introduce RMTBench, a comprehensive \\textbf{user-centric}\nbilingual role-playing benchmark featuring 80 diverse characters and over 8,000\ndialogue rounds. RMTBench includes custom characters with detailed backgrounds\nand abstract characters defined by simple traits, enabling evaluation across\nvarious user scenarios. Our benchmark constructs dialogues based on explicit\nuser motivations rather than character descriptions, ensuring alignment with\npractical user applications. Furthermore, we construct an authentic multi-turn\ndialogue simulation mechanism. With carefully selected evaluation dimensions\nand LLM-based scoring, this mechanism captures the complex intention of\nconversations between the user and the character. By shifting focus from\ncharacter background to user intention fulfillment, RMTBench bridges the gap\nbetween academic evaluation and practical deployment requirements, offering a\nmore effective framework for assessing role-playing capabilities in LLMs. All\ncode and datasets will be released soon.", "AI": {"tldr": "Introduction of RMTBench, a user-centric bilingual role-playing benchmark for evaluating Large Language Models (LLMs) with a focus on user intentions rather than character backgrounds.", "motivation": "To overcome the limitations of existing character-centric approaches that simplify user-character interactions and do not reflect real-world applications, requiring a more user-focused evaluation method for LLMs in role-playing scenarios.", "method": "Development of RMTBench, which features 80 diverse characters and over 8,000 dialogue rounds, incorporating both custom and abstract character definitions. The benchmark emphasizes user motivations to construct dialogues and implements a multi-turn dialogue simulation mechanism with LLM-based scoring.", "result": "RMTBench provides a comprehensive framework for evaluating LLM capabilities in role-playing contexts, highlighting the importance of user intention fulfillment during interactions.", "conclusion": "By bridging the gap between academic evaluation and practical deployment requirements, RMTBench offers a robust tool for more effectively assessing role-playing capabilities in LLMs.", "key_contributions": ["Introduction of a user-centric bilingual benchmark for role-playing applications in LLMs.", "Inclusion of a diverse set of characters and dialogue scenarios to better assess user interactions.", "A novel scoring mechanism capturing complex intentions in conversation."], "limitations": "", "keywords": ["Large Language Models", "role-playing benchmark", "user-centric evaluation", "dialogue simulation", "multi-turn conversation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.20398", "pdf": "https://arxiv.org/pdf/2507.20398.pdf", "abs": "https://arxiv.org/abs/2507.20398", "title": "Length Representations in Large Language Models", "authors": ["Sangjun Moon", "Dasom Choi", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities across\nvarious tasks, that are learned from massive amounts of text-based data.\nAlthough LLMs can control output sequence length, particularly in\ninstruction-based settings, the internal mechanisms behind this control have\nbeen unexplored yet. In this study, we provide empirical evidence on how output\nsequence length information is encoded within the internal representations in\nLLMs. In particular, our findings show that multi-head attention mechanisms are\ncritical in determining output sequence length, which can be adjusted in a\ndisentangled manner. By scaling specific hidden units within the model, we can\ncontrol the output sequence length without losing the informativeness of the\ngenerated text, thereby indicating that length information is partially\ndisentangled from semantic information. Moreover, some hidden units become\nincreasingly active as prompts become more length-specific, thus reflecting the\nmodel's internal awareness of this attribute. Our findings suggest that LLMs\nhave learned robust and adaptable internal mechanisms for controlling output\nlength without any external control.", "AI": {"tldr": "This study investigates how output sequence length is encoded in large language models (LLMs), revealing that multi-head attention mechanisms are crucial for controlling this aspect while maintaining text informativeness.", "motivation": "To explore the internal mechanisms of LLMs that allow control over output sequence length, which has not been thoroughly examined.", "method": "Empirical analysis of LLMs focusing on the role of multi-head attention in determining output sequence length and how it can be manipulated through selective activation of hidden units.", "result": "LLMs can control output sequence length in a disentangled manner, where multi-head attention mechanisms play a key role. Specific hidden units can be scaled to adjust length without losing text quality.", "conclusion": "LLMs possess robust internal mechanisms for managing output length independently from semantic content, indicating adaptability in their representations.", "key_contributions": ["Identification of multi-head attention's role in output length control.", "Demonstration of disentangled representation of length and semantic information.", "Empirical evidence of internal awareness of length-specific prompts."], "limitations": "", "keywords": ["large language models", "output sequence length", "multi-head attention", "disentangled representations", "internal mechanisms"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20409", "pdf": "https://arxiv.org/pdf/2507.20409.pdf", "abs": "https://arxiv.org/abs/2507.20409", "title": "Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Gunhee Kim", "Motahhare Eslami", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Under review; 17 pages", "summary": "Chain-of-Thought (CoT) prompting helps models think step by step. But what\nhappens when they must see, understand, and judge-all at once? In visual tasks\ngrounded in social context, where bridging perception with norm-grounded\njudgments is essential, flat CoT often breaks down. We introduce Cognitive\nChain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning\nthrough three cognitively inspired stages: perception, situation, and norm. Our\nexperiments show that, across multiple multimodal benchmarks (including intent\ndisambiguation, commonsense reasoning, and safety), CoCoT consistently\noutperforms CoT and direct prompting (+8\\% on average). Our findings\ndemonstrate that cognitively grounded reasoning stages enhance interpretability\nand social awareness in VLMs, paving the way for safer and more reliable\nmultimodal systems.", "AI": {"tldr": "The paper introduces Cognitive Chain-of-Thought (CoCoT), a new prompting strategy for visual language models (VLMs) that improves reasoning through three cognitively inspired stages: perception, situation, and norm.", "motivation": "To enhance visual tasks reliant on social context where understanding and judgement must occur simultaneously, addressing limitations of flat Chain-of-Thought (CoT) prompting.", "method": "The paper proposes CoCoT as a scaffolding strategy for reasoning that divides the process into three stages: perception, situation, and norm, enabling more effective interpretability.", "result": "CoCoT consistently outperforms traditional CoT and direct prompting by an average of 8% across various multimodal benchmarks, improving intent disambiguation, commonsense reasoning, and safety.", "conclusion": "Cognitively grounded reasoning stages notably enhance the interpretability and social awareness of VLMs, suggesting a pathway to safer and more reliable multimodal systems.", "key_contributions": ["Introduction of Cognitive Chain-of-Thought (CoCoT) methodology", "Demonstrated improvements in reasoning accuracy on multimodal benchmarks", "Enhanced interpretability and social context awareness in visual language models"], "limitations": "", "keywords": ["Cognitive Chain-of-Thought", "visual language models", "multimodal systems", "reasoning", "social awareness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20411", "pdf": "https://arxiv.org/pdf/2507.20411.pdf", "abs": "https://arxiv.org/abs/2507.20411", "title": "CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning", "authors": ["George Ibrahim", "Rita Ramos", "Yova Kementchedjhieva"], "categories": ["cs.CL"], "comment": "Published as a conference paper at COLM 2025", "summary": "Multilingual vision-language models have made significant strides in image\ncaptioning, yet they still lag behind their English counterparts due to limited\nmultilingual training data and costly large-scale model parameterization.\nRetrieval-augmented generation (RAG) offers a promising alternative by\nconditioning caption generation on retrieved examples in the target language,\nreducing the need for extensive multilingual training. However, multilingual\nRAG captioning models often depend on retrieved captions translated from\nEnglish, which can introduce mismatches and linguistic biases relative to the\nsource language. We introduce CONCAP, a multilingual image captioning model\nthat integrates retrieved captions with image-specific concepts, enhancing the\ncontextualization of the input image and grounding the captioning process\nacross different languages. Experiments on the XM3600 dataset indicate that\nCONCAP enables strong performance on low- and mid-resource languages, with\nhighly reduced data requirements. Our findings highlight the effectiveness of\nconcept-aware retrieval augmentation in bridging multilingual performance gaps.", "AI": {"tldr": "CONCAP enhances multilingual image captioning by integrating retrieved captions with image-specific concepts, significantly improving performance on low-resource languages.", "motivation": "To address the performance gap in multilingual vision-language models for image captioning compared to English models, which is exacerbated by limited multilingual data and biases in translated captions.", "method": "The paper introduces CONCAP, a multilingual image captioning model that uses retrieval-augmented generation (RAG) combined with image-specific concepts to improve captioning across languages.", "result": "Experiments on the XM3600 dataset demonstrate that CONCAP significantly outperforms existing multilingual models, particularly in low- and mid-resource languages, with reduced data requirements.", "conclusion": "CONCAP effectively enhances multilingual image captioning by improving the contextualization and grounding of captions across languages, thus bridging performance gaps in multilingual settings.", "key_contributions": ["Introduction of the CONCAP model for multilingual image captioning", "Integration of image-specific concepts with retrieved captions", "Demonstrated effectiveness on low- and mid-resource languages with reduced data requirements"], "limitations": "", "keywords": ["multilingual image captioning", "retrieval-augmented generation", "CONCAP", "low-resource languages"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.16117", "pdf": "https://arxiv.org/pdf/2507.16117.pdf", "abs": "https://arxiv.org/abs/2507.16117", "title": "BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation", "authors": ["Eden Wu", "Dishita G Turakhia", "Guande Wu", "Christos Koutras", "Sarah Keegan", "Wenke Liu", "Beata Szeitz", "David Fenyo", "Cláudio T. Silva", "Juliana Freire"], "categories": ["cs.HC"], "comment": "11 pages, 9 figures. Accepted to IEEE VIS 2025 (Full Papers Track,\n  submission ID 1204)", "summary": "Biomedical data harmonization is essential for enabling exploratory analyses\nand meta-studies, but the process of schema matching - identifying semantic\ncorrespondences between elements of disparate datasets (schemas) - remains a\nlabor-intensive and error-prone task. Even state-of-the-art automated methods\noften yield low accuracy when applied to biomedical schemas due to the large\nnumber of attributes and nuanced semantic differences between them. We present\nBDIViz, a novel visual analytics system designed to streamline the schema\nmatching process for biomedical data. Through formative studies with domain\nexperts, we identified key requirements for an effective solution and developed\ninteractive visualization techniques that address both scalability challenges\nand semantic ambiguity. BDIViz employs an ensemble approach that combines\nmultiple matching methods with LLM-based validation, summarizes matches through\ninteractive heatmaps, and provides coordinated views that enable users to\nquickly compare attributes and their values. Our method-agnostic design allows\nthe system to integrate various schema matching algorithms and adapt to\napplication-specific needs. Through two biomedical case studies and a\nwithin-subject user study with domain experts, we demonstrate that BDIViz\nsignificantly improves matching accuracy while reducing cognitive load and\ncuration time compared to baseline approaches.", "AI": {"tldr": "BDIViz is a visual analytics system that enhances schema matching for biomedical data, improving accuracy and reducing cognitive load.", "motivation": "Address the labor-intensive and error-prone task of schema matching in biomedical data harmonization.", "method": "BDIViz combines multiple schema matching methods with LLM-based validation and employs interactive visualization techniques, including heatmaps and coordinated views.", "result": "BDIViz significantly improves matching accuracy and reduces cognitive load and curation time in biomedical schema matching.", "conclusion": "The method-agnostic design of BDIViz allows it to adapt to various schema matching algorithms and specific application needs, demonstrating effectiveness through user studies.", "key_contributions": ["Development of BDIViz for improved schema matching in biomedical informatics.", "Integration of LLM-based validation with interactive visualization techniques.", "Demonstration of significant accuracy improvements in matching through case studies and user studies."], "limitations": "", "keywords": ["biomedical data", "schema matching", "visual analytics", "LLM-based validation", "interactive visualization"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.20419", "pdf": "https://arxiv.org/pdf/2507.20419.pdf", "abs": "https://arxiv.org/abs/2507.20419", "title": "Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?", "authors": ["Khloud AL Jallad", "Nada Ghneim", "Ghaida Rebdawi"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Natural Language Understanding (NLU) is a basic task in Natural Language\nProcessing (NLP). The evaluation of NLU capabilities has become a trending\nresearch topic that attracts researchers in the last few years, resulting in\nthe development of numerous benchmarks. These benchmarks include various tasks\nand datasets in order to evaluate the results of pretrained models via public\nleaderboards. Notably, several benchmarks contain diagnostics datasets designed\nfor investigation and fine-grained error analysis across a wide range of\nlinguistic phenomena. This survey provides a comprehensive review of available\nEnglish, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on\ntheir diagnostics datasets and the linguistic phenomena they covered. We\npresent a detailed comparison and analysis of these benchmarks, highlighting\ntheir strengths and limitations in evaluating NLU tasks and providing in-depth\nerror analysis. When highlighting the gaps in the state-of-the-art, we noted\nthat there is no naming convention for macro and micro categories or even a\nstandard set of linguistic phenomena that should be covered. Consequently, we\nformulated a research question regarding the evaluation metrics of the\nevaluation diagnostics benchmarks: \"Why do not we have an evaluation standard\nfor the NLU evaluation diagnostics benchmarks?\" similar to ISO standard in\nindustry. We conducted a deep analysis and comparisons of the covered\nlinguistic phenomena in order to support experts in building a global hierarchy\nfor linguistic phenomena in future. We think that having evaluation metrics for\ndiagnostics evaluation could be valuable to gain more insights when comparing\nthe results of the studied models on different diagnostics benchmarks.", "AI": {"tldr": "This survey reviews existing NLU benchmarks across English, Arabic, and multilingual datasets focusing on diagnostics for error analysis.", "motivation": "To address the lack of standardization in evaluation metrics for NLU benchmarks and facilitate better error analysis and comparison of models.", "method": "Comprehensive review and comparison of available NLU benchmarks and their diagnostic datasets, focusing on linguistic phenomena.", "result": "Identified gaps in NLU evaluation standards; proposed a research question regarding the standardization of evaluation metrics for diagnostics benchmarks.", "conclusion": "Establishing evaluation metrics for diagnostics could enhance insights and comparisons between NLU models across different benchmarks.", "key_contributions": ["Comprehensive review of NLU benchmarks in multiple languages", "Identification of gaps in evaluation standards", "Proposal for a standardization of evaluation metrics for diagnostics"], "limitations": "Lack of standard naming conventions and categories for linguistic phenomena.", "keywords": ["Natural Language Understanding", "benchmark", "evaluation metrics", "error analysis", "linguistic phenomena"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.16466", "pdf": "https://arxiv.org/pdf/2507.16466.pdf", "abs": "https://arxiv.org/abs/2507.16466", "title": "SceneLoom: Communicating Data with Scene Context", "authors": ["Lin Gao", "Leixian Shen", "Yuheng Zhao", "Jiexiang Lan", "Huamin Qu", "Siming Chen"], "categories": ["cs.HC"], "comment": null, "summary": "In data-driven storytelling contexts such as data journalism and data videos,\ndata visualizations are often presented alongside real-world imagery to support\nnarrative context. However, these visualizations and contextual images\ntypically remain separated, limiting their combined narrative expressiveness\nand engagement. Achieving this is challenging due to the need for fine-grained\nalignment and creative ideation. To address this, we present SceneLoom, a\nVision-Language Model (VLM)-powered system that facilitates the coordination of\ndata visualization with real-world imagery based on narrative intents. Through\na formative study, we investigated the design space of coordination\nrelationships between data visualization and real-world scenes from the\nperspectives of visual alignment and semantic coherence. Guided by the derived\ndesign considerations, SceneLoom leverages VLMs to extract visual and semantic\nfeatures from scene images and data visualization, and perform design mapping\nthrough a reasoning process that incorporates spatial organization, shape\nsimilarity, layout consistency, and semantic binding. The system generates a\nset of contextually expressive, image-driven design alternatives that achieve\ncoherent alignments across visual, semantic, and data dimensions. Users can\nexplore these alternatives, select preferred mappings, and further refine the\ndesign through interactive adjustments and animated transitions to support\nexpressive data communication. A user study and an example gallery validate\nSceneLoom's effectiveness in inspiring creative design and facilitating design\nexternalization.", "AI": {"tldr": "SceneLoom is a VLM-powered system that enhances data visualization in storytelling by integrating real-world imagery, enabling coherent visual and semantic design mappings.", "motivation": "The need to improve the integration of data visualizations with real-world imagery in storytelling contexts, enhancing narrative expressiveness and engagement.", "method": "SceneLoom uses a Vision-Language Model to extract visual and semantic features from both scene images and data visualizations, performing design mapping based on spatial organization, shape similarity, layout consistency, and semantic binding.", "result": "The system generates contextually expressive design alternatives that support coherent alignments across visual, semantic, and data dimensions, validated through user studies and an example gallery.", "conclusion": "SceneLoom effectively inspires creative design and facilitates design externalization, demonstrating its potential in data-driven storytelling.", "key_contributions": ["Introduction of SceneLoom as a system for coordinating data visualizations with imagery", "Development of a design mapping method using VLMs", "User validation showcasing enhanced design expressiveness and engagement"], "limitations": "The study may be limited by the specific contexts tested and the scalability of the system across diverse storytelling formats.", "keywords": ["data visualization", "human-computer interaction", "narrative context", "Vision-Language Model", "creative design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20423", "pdf": "https://arxiv.org/pdf/2507.20423.pdf", "abs": "https://arxiv.org/abs/2507.20423", "title": "CodeNER: Code Prompting for Named Entity Recognition", "authors": ["Sungwoo Han", "Hyeyeon Kim", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "18 pages, 6 figures", "summary": "Recent studies have explored various approaches for treating candidate named\nentity spans as both source and target sequences in named entity recognition\n(NER) by leveraging large language models (LLMs). Although previous approaches\nhave successfully generated candidate named entity spans with suitable labels,\nthey rely solely on input context information when using LLMs, particularly,\nChatGPT. However, NER inherently requires capturing detailed labeling\nrequirements with input context information. To address this issue, we propose\na novel method that leverages code-based prompting to improve the capabilities\nof LLMs in understanding and performing NER. By embedding code within prompts,\nwe provide detailed BIO schema instructions for labeling, thereby exploiting\nthe ability of LLMs to comprehend long-range scopes in programming languages.\nExperimental results demonstrate that the proposed code-based prompting method\noutperforms conventional text-based prompting on ten benchmarks across English,\nArabic, Finnish, Danish, and German datasets, indicating the effectiveness of\nexplicitly structuring NER instructions. We also verify that combining the\nproposed code-based prompting method with the chain-of-thought prompting\nfurther improves performance.", "AI": {"tldr": "This paper introduces a code-based prompting method to enhance named entity recognition (NER) with large language models (LLMs), showing improved results over traditional text-based approaches.", "motivation": "To improve NER by better capturing labeling requirements and using the capabilities of LLMs for understanding context and instructions.", "method": "The study proposes a novel prompting approach that embeds code within prompts to provide detailed BIO schema instructions, thus leveraging LLMs' ability to understand long-range dependencies common in programming languages.", "result": "The code-based prompting method significantly outperforms conventional text-based prompting across ten benchmarks in multiple languages, demonstrating improved effectiveness in structuring NER instructions.", "conclusion": "The findings validate the efficacy of code-based prompting in enhancing LLM performance for NER tasks, especially when combined with chain-of-thought prompting strategies.", "key_contributions": ["Introduction of code-based prompting for NER using LLMs", "Demonstrated superiority of the method over traditional techniques", "Performance evaluation across multiple languages and datasets"], "limitations": "The approach relies on effective integration of code within prompts, which may not generalize across all NER tasks or languages.", "keywords": ["named entity recognition", "large language models", "code-based prompting", "BIO schema", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20491", "pdf": "https://arxiv.org/pdf/2507.20491.pdf", "abs": "https://arxiv.org/abs/2507.20491", "title": "Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems", "authors": ["Tuan Bui", "Trong Le", "Phat Thai", "Sang Nguyen", "Minh Hua", "Ngan Pham", "Thang Bui", "Tho Quan"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "8 pages, 3 figures. Accepted at the International Joint Conference on\n  Neural Networks (IJCNN) 2025, Workshop on Trustworthiness and Reliability in\n  Neuro-Symbolic AI. https://2025.ijcnn.org", "summary": "Recent advances in large language models (LLMs) have significantly enhanced\nquestion-answering (QA) capabilities, particularly in open-domain contexts.\nHowever, in closed-domain scenarios such as education, healthcare, and law,\nusers demand not only accurate answers but also transparent reasoning and\nexplainable decision-making processes. While neural-symbolic (NeSy) frameworks\nhave emerged as a promising solution, leveraging LLMs for natural language\nunderstanding and symbolic systems for formal reasoning, existing approaches\noften rely on large-scale models and exhibit inefficiencies in translating\nnatural language into formal logic representations.\n  To address these limitations, we introduce Text-JEPA (Text-based\nJoint-Embedding Predictive Architecture), a lightweight yet effective framework\nfor converting natural language into first-order logic (NL2FOL). Drawing\ninspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by\nefficiently generating logic representations, while the Z3 solver operates as\nSystem 2, enabling robust logical inference. To rigorously evaluate the\nNL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework\ncomprising three custom metrics: conversion score, reasoning score, and\nSpearman rho score, which collectively capture the quality of logical\ntranslation and its downstream impact on reasoning accuracy.\n  Empirical results on domain-specific datasets demonstrate that Text-JEPA\nachieves competitive performance with significantly lower computational\noverhead compared to larger LLM-based systems. Our findings highlight the\npotential of structured, interpretable reasoning frameworks for building\nefficient and explainable QA systems in specialized domains.", "AI": {"tldr": "Text-JEPA is a lightweight framework that converts natural language into first-order logic for explainable question-answering in closed-domain contexts.", "motivation": "There is a need for accurate and explainable reasoning in question-answering systems in closed domains like healthcare and education.", "method": "Text-JEPA employs a dual-system cognitive approach, using a lightweight method to generate logic representations and a Z3 solver for logical inference, alongside a comprehensive evaluation framework.", "result": "Text-JEPA shows competitive performance on domain-specific datasets with lower computational overhead compared to larger LLM-based systems.", "conclusion": "The findings suggest structured and interpretable reasoning frameworks are effective for building efficient QA systems in specialized domains.", "key_contributions": ["Introduction of Text-JEPA for NL2FOL conversion", "Development of a comprehensive evaluation framework for reasoning accuracy", "Demonstration of competitive performance with lower resource requirements"], "limitations": "", "keywords": ["large language models", "neural-symbolic frameworks", "explainable AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.19218", "pdf": "https://arxiv.org/pdf/2507.19218.pdf", "abs": "https://arxiv.org/abs/2507.19218", "title": "Technological folie à deux: Feedback Loops Between AI Chatbots and Mental Illness", "authors": ["Sebastian Dohnány", "Zeb Kurth-Nelson", "Eleanor Spens", "Lennart Luettgau", "Alastair Reid", "Iason Gabriel", "Christopher Summerfield", "Murray Shanahan", "Matthew M Nour"], "categories": ["cs.HC", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Artificial intelligence chatbots have achieved unprecedented adoption, with\nmillions now using these systems for emotional support and companionship in\ncontexts of widespread social isolation and capacity-constrained mental health\nservices. While some users report psychological benefits, concerning edge cases\nare emerging, including reports of suicide, violence, and delusional thinking\nlinked to perceived emotional relationships with chatbots. To understand this\nnew risk profile we need to consider the interaction between human cognitive\nand emotional biases, and chatbot behavioural tendencies such as agreeableness\n(sycophancy) and adaptability (in-context learning). We argue that individuals\nwith mental health conditions face increased risks of chatbot-induced belief\ndestabilization and dependence, owing to altered belief-updating, impaired\nreality-testing, and social isolation. Current AI safety measures are\ninadequate to address these interaction-based risks. To address this emerging\npublic health concern, we need coordinated action across clinical practice, AI\ndevelopment, and regulatory frameworks.", "AI": {"tldr": "This paper discusses the rising risks associated with AI chatbots used for emotional support, particularly in individuals with mental health conditions.", "motivation": "The study addresses the adoption of AI chatbots for emotional support amidst social isolation and constrained mental health services, exploring the associated risks, particularly for vulnerable populations.", "method": "The approach involves analyzing the interactions between human cognitive/emotional biases and chatbot behaviors like agreeableness and adaptability, alongside reviewing relevant AI safety measures.", "result": "Findings indicate increased risks of belief destabilization and dependence among users with mental health conditions, alongside a gap in current AI safety measures to mitigate these risks.", "conclusion": "Coordinated action is necessary across diverse fields to tackle the emerging public health concerns posed by chatbot interactions.", "key_contributions": ["Analyzed risks of emotional reliance on chatbots in users with mental health issues.", "Highlighted inadequacies in current AI safety measures.", "Proposed a need for interdisciplinary coordination to address identified risks."], "limitations": "", "keywords": ["AI chatbots", "emotional support", "mental health", "human-computer interaction", "safety measures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20520", "pdf": "https://arxiv.org/pdf/2507.20520.pdf", "abs": "https://arxiv.org/abs/2507.20520", "title": "AQUA: A Large Language Model for Aquaculture & Fisheries", "authors": ["Praneeth Narisetty", "Uday Kumar Reddy Kattamanchi", "Lohit Akshant Nimma", "Sri Ram Kaushik Karnati", "Shiva Nagendra Babu Kore", "Mounika Golamari", "Tejashree Nageshreddy"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": null, "summary": "Aquaculture plays a vital role in global food security and coastal economies\nby providing sustainable protein sources. As the industry expands to meet\nrising demand, it faces growing challenges such as disease outbreaks,\ninefficient feeding practices, rising labor costs, logistical inefficiencies,\nand critical hatchery issues, including high mortality rates and poor water\nquality control. Although artificial intelligence has made significant\nprogress, existing machine learning methods fall short of addressing the\ndomain-specific complexities of aquaculture. To bridge this gap, we introduce\nAQUA, the first large language model (LLM) tailored for aquaculture, designed\nto support farmers, researchers, and industry practitioners. Central to this\neffort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic\nFramework for generating and refining high-quality synthetic data using a\ncombination of expert knowledge, largescale language models, and automated\nevaluation techniques. Our work lays the foundation for LLM-driven innovations\nin aquaculture research, advisory systems, and decision-making tools.", "AI": {"tldr": "Introduction of AQUA, the first LLM designed for aquaculture, aimed at addressing challenges in the industry.", "motivation": "Aquaculture is critical for food security and faces challenges that existing AI methods do not address effectively.", "method": "AQUA utilizes a combination of expert knowledge, language models, and automated evaluation techniques to create high-quality synthetic data.", "result": "AQUA aims to support farmers and researchers by providing an innovative LLM tool tailored to the specific needs of the aquaculture industry.", "conclusion": "AQUA and the AQUADAPT framework pave the way for LLM-driven advancements in aquaculture practices and decision-making.", "key_contributions": ["Introduction of AQUA, a specialized LLM for aquaculture", "Development of the AQUADAPT framework for synthetic data generation", "Innovation in LLM applications for aquaculture research and decision-making tools"], "limitations": "", "keywords": ["aquaculture", "large language model", "synthetic data", "AI", "decision-making"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.20527", "pdf": "https://arxiv.org/pdf/2507.20527.pdf", "abs": "https://arxiv.org/abs/2507.20527", "title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers", "authors": ["Chaitanya Manem", "Pratik Prabhanjan Brahma", "Prakamya Mishra", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL"], "comment": null, "summary": "The demand for Large Language Models (LLMs) capable of sophisticated\nmathematical reasoning is growing across industries. However, the development\nof performant mathematical LLMs is critically bottlenecked by the scarcity of\ndifficult, novel training data. We introduce \\textbf{SAND-Math} (Synthetic\nAugmented Novel and Difficult Mathematics problems and solutions), a pipeline\nthat addresses this by first generating high-quality problems from scratch and\nthen systematically elevating their complexity via a new \\textbf{Difficulty\nHiking} step. We demonstrate the effectiveness of our approach through two key\nfindings. First, augmenting a strong baseline with SAND-Math data significantly\nboosts performance, outperforming the next-best synthetic dataset by\n\\textbf{$\\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a\ndedicated ablation study, we show our Difficulty Hiking process is highly\neffective: by increasing average problem difficulty from 5.02 to 5.98, this\nstep lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation\npipeline, final dataset, and a fine-tuned model form a practical and scalable\ntoolkit for building more capable and efficient mathematical reasoning LLMs.\nSAND-Math dataset is released here:\n\\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}", "AI": {"tldr": "The paper presents SAND-Math, a pipeline for generating challenging mathematical problems to enhance the performance of Large Language Models (LLMs) in mathematical reasoning.", "motivation": "To address the bottleneck in developing powerful mathematical LLMs due to the lack of complex training data, the authors propose a novel solution for generating synthetic math problems.", "method": "SAND-Math generates high-quality mathematical problems and uses a 'Difficulty Hiking' step to systematically increase their complexity.", "result": "The approach significantly boosts LLM performance, achieving a 17.85 absolute point improvement on the AIME25 benchmark and raising performance from 46.38% to 49.23% through enhanced problem difficulty.", "conclusion": "SAND-Math provides a scalable approach for generating high-quality datasets that can aid in the development of more capable mathematical reasoning LLMs.", "key_contributions": ["Introduction of SAND-Math for generating synthetic math problems", "A new Difficulty Hiking method to increase problem complexity", "Demonstrated significant performance improvements on the AIME25 benchmark"], "limitations": "", "keywords": ["Large Language Models", "mathematical reasoning", "synthetic data"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.20528", "pdf": "https://arxiv.org/pdf/2507.20528.pdf", "abs": "https://arxiv.org/abs/2507.20528", "title": "Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations", "authors": ["Effi Levi", "Gal Ron", "Odelia Oshri", "Shaul R. Shenhav"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel multi-labeled scheme for joint annotation of hate and\ncounter-hate speech in social media conversations, categorizing hate and\ncounter-hate messages into thematic and rhetorical dimensions. The thematic\ncategories outline different discursive aspects of each type of speech, while\nthe rhetorical dimension captures how hate and counter messages are\ncommunicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a\nsample of 92 conversations, consisting of 720 tweets, and conduct statistical\nanalyses, incorporating public metrics, to explore patterns of interaction\nbetween the thematic and rhetorical dimensions within and between hate and\ncounter-hate speech. Our findings provide insights into the spread of hate\nmessages on social media, the strategies used to counter them, and their\npotential impact on online behavior.", "AI": {"tldr": "A novel multi-labeled scheme for annotating hate and counter-hate speech on social media is introduced, focusing on thematic and rhetorical dimensions.", "motivation": "To better understand and categorize hate and counter-hate speech in social media settings, addressing the complexity of these interactions.", "method": "The authors annotated 92 conversations with 720 tweets, applying a multi-labeled scheme that incorporates both thematic and rhetorical dimensions, guided by Aristotelian principles.", "result": "Statistical analyses reveal patterns of interaction within and between hate and counter-hate speech, highlighting the strategies and potential impact these messages have on online behavior.", "conclusion": "The study enhances the understanding of hate speech dynamics on social media and the effectiveness of counter-hate strategies.", "key_contributions": ["Introduction of a multi-labeled annotation scheme for speech classification", "Insights into rhetorical strategies used in hate and counter-hate speech", "Statistical analysis of interaction patterns in social media conversations."], "limitations": "Limited sample size of annotated conversations; results may not generalize across all social media platforms.", "keywords": ["hate speech", "counter-hate speech", "social media", "thematic dimensions", "rhetorical dimensions"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.20546", "pdf": "https://arxiv.org/pdf/2507.20546.pdf", "abs": "https://arxiv.org/abs/2507.20546", "title": "Enhancing Hallucination Detection via Future Context", "authors": ["Joosung Lee", "Cheonbok Park", "Hwiyeol Jo", "Jeonghoon Kim", "Joonsuk Park", "Kang Min Yoo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are widely used to generate plausible text on\nonline platforms, without revealing the generation process. As users\nincreasingly encounter such black-box outputs, detecting hallucinations has\nbecome a critical challenge. To address this challenge, we focus on developing\na hallucination detection framework for black-box generators. Motivated by the\nobservation that hallucinations, once introduced, tend to persist, we sample\nfuture contexts. The sampled future contexts provide valuable clues for\nhallucination detection and can be effectively integrated with various\nsampling-based methods. We extensively demonstrate performance improvements\nacross multiple methods using our proposed sampling approach.", "AI": {"tldr": "This paper presents a framework for detecting hallucinations in outputs from large language models (LLMs) by utilizing sampled future contexts, leading to improved performance across several methods.", "motivation": "The increasing use of LLMs raises concerns over the reliability of their outputs, as they often generate plausible but incorrect information (hallucinations) without transparency.", "method": "The authors developed a detection framework that leverages sampled future contexts to identify hallucinations in the text produced by black-box generators.", "result": "The proposed sampling approach shows significant performance improvements in detecting hallucinations across multiple detection methods.", "conclusion": "Integrating future context sampling into hallucination detection can enhance the reliability of outputs from LLMs.", "key_contributions": ["Development of a hallucination detection framework for LLMs", "Introduction of future context sampling for improved detection", "Validation of the framework's effectiveness across various methods."], "limitations": "", "keywords": ["hallucination detection", "large language models", "black-box generators"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20564", "pdf": "https://arxiv.org/pdf/2507.20564.pdf", "abs": "https://arxiv.org/abs/2507.20564", "title": "ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning", "authors": ["Duc-Tai Dinh", "Duc Anh Khoa Dinh"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system\nin Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image\nretrieval and captioning. Our zero-shot approach requires no finetuning on the\ncompetition's data. For retrieval, we ensemble similarity scores from CLIP,\nSigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt\nto guide the Gemma 3 model, enabling it to link high-level events from the\narticle to the visual content in the image. Our system achieved a final score\nof 0.42002, securing a top-4 position on the private test set, demonstrating\nthe effectiveness of combining foundation models through ensembling and\nprompting. Our code is available at https://github.com/ductai05/ZSE-Cap.", "AI": {"tldr": "ZSE-Cap is a top-performing zero-shot system for captioning and image retrieval that combines multiple models without fine-tuning.", "motivation": "To enhance image retrieval and captioning by employing a zero-shot ensemble approach that avoids data-specific fine-tuning.", "method": "The system uses an ensemble of similarity scores from CLIP, SigLIP, and DINOv2 for retrieval and utilizes a tailored prompt to guide the Gemma 3 model for captioning.", "result": "Achieved a final score of 0.42002, placing 4th in the EVENTA shared task, showcasing the effectiveness of the combined approach.", "conclusion": "The study highlights the potential of zero-shot learning and model ensembling in improving image analysis tasks without the need for extensive data-specific training.", "key_contributions": ["Introduction of a zero-shot ensemble method for image retrieval and captioning.", "Demonstrated effective prompting techniques to connect textual and visual information.", "Achieved competitive results in a shared task without fine-tuning on training data."], "limitations": "", "keywords": ["zero-shot learning", "image retrieval", "captioning", "model ensembling", "prompt engineering"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.20614", "pdf": "https://arxiv.org/pdf/2507.20614.pdf", "abs": "https://arxiv.org/abs/2507.20614", "title": "Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior", "authors": ["Anaïs Ollagnier"], "categories": ["cs.CL"], "comment": null, "summary": "Antisocial behavior (ASB) on social media-including hate speech, harassment,\nand trolling-poses growing challenges for platform safety and societal\nwellbeing. While prior work has primarily focused on detecting harmful content\nafter it appears, predictive approaches aim to forecast future harmful\nbehaviors-such as hate speech propagation, conversation derailment, or user\nrecidivism-before they fully unfold. Despite increasing interest, the field\nremains fragmented, lacking a unified taxonomy or clear synthesis of existing\nmethods. This paper presents a systematic review of over 49 studies on ASB\nprediction, offering a structured taxonomy of five core task types: early harm\ndetection, harm emergence prediction, harm propagation prediction, behavioral\nrisk prediction, and proactive moderation support. We analyze how these tasks\ndiffer by temporal framing, prediction granularity, and operational goals. In\naddition, we examine trends in modeling techniques-from classical machine\nlearning to pre-trained language models-and assess the influence of dataset\ncharacteristics on task feasibility and generalization. Our review highlights\nmethodological challenges, such as dataset scarcity, temporal drift, and\nlimited benchmarks, while outlining emerging research directions including\nmultilingual modeling, cross-platform generalization, and human-in-the-loop\nsystems. By organizing the field around a coherent framework, this survey aims\nto guide future work toward more robust and socially responsible ASB\nprediction.", "AI": {"tldr": "This paper systematically reviews studies on predicting antisocial behavior on social media, offering a structured taxonomy and examining methodological challenges and future research directions.", "motivation": "To address the growing challenges of antisocial behavior on social media and to provide a unified approach towards prediction methods.", "method": "A systematic review of over 49 studies on antisocial behavior prediction, classifying them into five core task types and analyzing modeling techniques and dataset characteristics.", "result": "Identified five core task types for predicting antisocial behavior and analyzed trends in modeling techniques, revealing significant methodological challenges.", "conclusion": "The review encourages a coherent framework to guide future research for more robust and socially responsible antisocial behavior prediction methodologies.", "key_contributions": ["Structured taxonomy of antisocial behavior prediction tasks", "Analysis of modeling techniques and their effectiveness", "Identification of methodological challenges and emerging research directions"], "limitations": "Challenges such as dataset scarcity, temporal drift, and a lack of benchmarks limit current research capabilities.", "keywords": ["Antisocial behavior", "Social media", "Prediction", "Machine learning", "Systematic review"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.20643", "pdf": "https://arxiv.org/pdf/2507.20643.pdf", "abs": "https://arxiv.org/abs/2507.20643", "title": "Ontology-Enhanced Knowledge Graph Completion using Large Language Models", "authors": ["Wenbin Guo", "Xin Wang", "Jiaoyan Chen", "Zhao Li", "Zirui Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively adopted in Knowledge Graph\nCompletion (KGC), showcasing significant research advancements. However, as\nblack-box models driven by deep neural architectures, current LLM-based KGC\nmethods rely on implicit knowledge representation with parallel propagation of\nerroneous knowledge, thereby hindering their ability to produce conclusive and\ndecisive reasoning outcomes. We aim to integrate neural-perceptual structural\ninformation with ontological knowledge, leveraging the powerful capabilities of\nLLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.\nWe propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first\nleverages neural perceptual mechanisms to effectively embed structural\ninformation into the textual space, and then uses an automated extraction\nalgorithm to retrieve ontological knowledge from the knowledge graphs (KGs)\nthat needs to be completed, which is further transformed into a textual format\ncomprehensible to LLMs for providing logic guidance. We conducted extensive\nexperiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The\nexperimental results demonstrate that OL-KGC significantly outperforms existing\nmainstream KGC methods across multiple evaluation metrics, achieving\nstate-of-the-art performance.", "AI": {"tldr": "This paper presents OL-KGC, an ontology enhanced method for Knowledge Graph Completion using Large Language Models, improving reasoning outcomes through integration of neural-perceptual structures and ontological knowledge.", "motivation": "To enhance the reasoning capabilities of LLM-based Knowledge Graph Completion methods which suffer from black-box issues and erroneous knowledge propagation.", "method": "The proposed OL-KGC integrates neural perceptual structural information with automatic extraction of ontological knowledge, transforming it into a format suitable for LLMs.", "result": "OL-KGC significantly outperforms existing KGC methods on benchmarks like FB15K-237, UMLS, and WN18RR, achieving state-of-the-art performance.", "conclusion": "The integration of neural perceptual mechanisms with ontological knowledge provides substantial improvements in knowledge graph completion tasks.", "key_contributions": ["Introduction of OL-KGC, an ontology enhanced KGC method utilizing LLMs.", "Effective embedding of structural information into text format for better reasoning.", "Demonstrated superior performance on benchmark datasets compared to existing methods."], "limitations": "", "keywords": ["Knowledge Graph Completion", "Large Language Models", "Ontology", "Neural-perceptual structures", "Artificial Intelligence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20673", "pdf": "https://arxiv.org/pdf/2507.20673.pdf", "abs": "https://arxiv.org/abs/2507.20673", "title": "Geometric-Mean Policy Optimization", "authors": ["Yuzhong Zhao", "Yue Liu", "Junpeng Liu", "Jingye Chen", "Xun Wu", "Yaru Hao", "Tengchao Lv", "Shaohan Huang", "Lei Cui", "Qixiang Ye", "Fang Wan", "Furu Wei"], "categories": ["cs.CL"], "comment": "Code is available at https://github.com/callsys/GMPO", "summary": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have\nenhanced the reasoning capabilities of large language models by optimizing the\narithmetic mean of token-level rewards. However, GRPO suffers from unstable\npolicy updates when processing tokens with outlier importance-weighted rewards,\nwhich manifests as extreme importance sampling ratios during training, i.e.,\nthe ratio between the sampling probabilities assigned to a token by the current\nand old policies. In this work, we propose Geometric-Mean Policy Optimization\n(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic\nmean, GMPO maximizes the geometric mean of token-level rewards, which is\ninherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical\nand experimental analysis to justify the design and stability benefits of GMPO.\nBeyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on\nmultiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,\nincluding AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is\navailable at https://github.com/callsys/GMPO.", "AI": {"tldr": "This paper introduces Geometric-Mean Policy Optimization (GMPO), a stabilized variant of Group Relative Policy Optimization (GRPO), which improves the stability of token-level reward processing by using geometric mean instead of arithmetic mean.", "motivation": "To address the instability of policy updates caused by outlier importance-weighted rewards in GRPO, which leads to extreme importance sampling ratios during training.", "method": "GMPO optimizes the geometric mean of token-level rewards, which is less sensitive to outliers, thus providing a more stable range of importance sampling ratios compared to GRPO.", "result": "GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmarks, enhancing the reasoning capabilities of large language models.", "conclusion": "The theoretical and experimental analysis confirms that GMPO achieves improved stability and performance over GRPO, making it a viable alternative for optimizing language model policies.", "key_contributions": ["Introduction of Geometric-Mean Policy Optimization (GMPO) as a stable alternative to GRPO.", "Demonstrated improved stability and performance metrics on various benchmarks.", "Comprehensive analysis supporting the benefits of GMPO over traditional methods."], "limitations": "", "keywords": ["Geometric Mean", "Policy Optimization", "Human-Computer Interaction", "Large Language Models", "Outlier Sensitivity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20700", "pdf": "https://arxiv.org/pdf/2507.20700.pdf", "abs": "https://arxiv.org/abs/2507.20700", "title": "When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification", "authors": ["Hanna Shcharbakova", "Tatiana Anikina", "Natalia Skachkova", "Josef van Genabith"], "categories": ["cs.CL"], "comment": "Published at the FEVER Workshop, ACL 2025", "summary": "The rapid spread of multilingual misinformation requires robust automated\nfact verification systems capable of handling fine-grained veracity assessments\nacross diverse languages. While large language models have shown remarkable\ncapabilities across many NLP tasks, their effectiveness for multilingual claim\nverification with nuanced classification schemes remains understudied. We\nconduct a comprehensive evaluation of five state-of-the-art language models on\nthe X-Fact dataset, which spans 25 languages with seven distinct veracity\ncategories. Our experiments compare small language models (encoder-based XLM-R\nand mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)\nusing both prompting and fine-tuning approaches. Surprisingly, we find that\nXLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B\nparameters), achieving 57.7% macro-F1 compared to the best LLM performance of\n16.9%. This represents a 15.8% improvement over the previous state-of-the-art\n(41.9%), establishing new performance benchmarks for multilingual fact\nverification. Our analysis reveals problematic patterns in LLM behavior,\nincluding systematic difficulties in leveraging evidence and pronounced biases\ntoward frequent categories in imbalanced data settings. These findings suggest\nthat for fine-grained multilingual fact verification, smaller specialized\nmodels may be more effective than general-purpose large models, with important\nimplications for practical deployment of fact-checking systems.", "AI": {"tldr": "This paper evaluates multilingual fact verification using five language models on the X-Fact dataset, finding that smaller models outperform larger LLMs in accuracy.", "motivation": "The need for effective multilingual misinformation verification systems amidst the rise of misinformation across languages, highlighting the shortcomings of existing large language models.", "method": "Evaluated five language models (XLM-R, mT5, Llama 3.1, Qwen 2.5, Mistral Nemo) on the X-Fact dataset with seven veracity categories, employing both prompting and fine-tuning techniques.", "result": "XLM-R (270M parameters) outperformed all tested LLMs (7-12B parameters) with a macro-F1 score of 57.7%, significantly improving the state-of-the-art by 15.8%.", "conclusion": "Smaller specialized models may be more effective than larger general-purpose models for nuanced multilingual fact verification, impacting the deployment of fact-checking systems.", "key_contributions": ["Comprehensive evaluation of multilingual claim verification.", "Establishment of new performance benchmarks for multilingual fact verification.", "Identification of problematic patterns in LLM behavior for fine-grained tasks."], "limitations": "Focus on five models and specific dataset; findings may not generalize to all misinformation scenarios.", "keywords": ["multilingual", "fact verification", "large language models", "X-Fact dataset", "bias"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20704", "pdf": "https://arxiv.org/pdf/2507.20704.pdf", "abs": "https://arxiv.org/abs/2507.20704", "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models", "authors": ["Gabriel Downer", "Sean Craven", "Damian Ruck", "Jake Thomas"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "9 pages, 9 figures. Jake Thomas served as Editor for this manuscript", "summary": "The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications.", "AI": {"tldr": "A novel pipeline, Text2VLM, is introduced for evaluating Visual Language Models (VLMs) against typographic prompt injection attacks using multimodal prompts, revealing critical vulnerabilities in open-source models.", "motivation": "The integration of Visual Language Models (VLMs) requires robust evaluation mechanisms, especially regarding their vulnerabilities in multimodal content combining text and images.", "method": "Text2VLM transforms text-only evaluation datasets into multimodal formats, generating typographic images that serve as multimodal prompts for VLMs, followed by human evaluations.", "result": "The evaluation highlights that open-source VLMs are more susceptible to prompt injection when incorporating visual inputs, indicating significant alignment weaknesses and performance gaps compared to closed-source models.", "conclusion": "Text2VLM is validated as a scalable tool for assessing multimodal vulnerabilities, contributing to the development of safer VLMs for real-world applications.", "key_contributions": ["Introduction of the Text2VLM pipeline for multimodal prompt evaluation.", "Identification of vulnerabilities in open-source VLMs under visual input conditions.", "Validation through human evaluations ensuring alignment with human expectations."], "limitations": "Focus on specific attack types and reliance on the quality of text-to-image conversions.", "keywords": ["Visual Language Models", "multimodal evaluation", "prompt injection", "safety assessment", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.20749", "pdf": "https://arxiv.org/pdf/2507.20749.pdf", "abs": "https://arxiv.org/abs/2507.20749", "title": "Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study", "authors": ["Yiran Huang", "Lukas Thede", "Massimiliano Mancini", "Wenjia Xu", "Zeynep Akata"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at GCPR 2025", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\ncapabilities, their substantial computational and memory requirements pose\nsignificant barriers to practical deployment. Current parameter reduction\ntechniques primarily involve training MLLMs from Small Language Models (SLMs),\nbut these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs\nthrough structural pruning combined with efficient recovery training.\nSpecifically, we investigate two structural pruning paradigms--layerwise and\nwidthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the\nfeasibility of conducting recovery training with only a small fraction of the\navailable data. Our results show that widthwise pruning generally maintains\nbetter performance in low-resource scenarios with limited computational\nresources or insufficient finetuning data. As for the recovery training,\nfinetuning only the multimodal projector is sufficient at small compression\nlevels (< 20%). Furthermore, a combination of supervised finetuning and\nhidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the\noriginal training data, while retaining over 95% of the original performance.\nThrough empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and\nBunny-v1.0-3B, this study offers actionable insights for practitioners aiming\nto compress MLLMs effectively without extensive computation resources or\nsufficient data.", "AI": {"tldr": "This paper proposes a method to compress Multimodal Large Language Models (MLLMs) using structural pruning and recovery training, showing that effective performance can be maintained with significantly reduced training data.", "motivation": "To address the computational and memory challenges of deploying MLLMs, this study aims to find efficient ways to compress these models while maintaining performance.", "method": "The authors investigate two structural pruning approaches—layerwise and widthwise pruning—applied to MLLMs, and evaluate recovery training using limited data through supervised finetuning and knowledge distillation.", "result": "Widthwise pruning generally yields better performance under low-resource conditions, and finetuning only the multimodal projector proves sufficient at compression levels below 20%. Effective recovery can be achieved with as little as 5% of original training data.", "conclusion": "This study provides practical insights for compressing MLLMs effectively, highlighting that model performance can be preserved while using less computational resources and training data.", "key_contributions": ["Introduces structural pruning methods for MLLMs", "Shows effectiveness of recovery training with minimal data", "Demonstrates that widthwise pruning is superior in low-resource contexts."], "limitations": "The study focuses primarily on parameter reduction without exploring the potential trade-offs in model interpretability or other performance metrics beyond accuracy.", "keywords": ["Multimodal Large Language Models", "Compression", "Structural pruning", "Recovery training", "Supervised finetuning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.20752", "pdf": "https://arxiv.org/pdf/2507.20752.pdf", "abs": "https://arxiv.org/abs/2507.20752", "title": "Multilingual Self-Taught Faithfulness Evaluators", "authors": ["Carlo Alfano", "Aymen Al Marjani", "Zeno Jonke", "Amin Mantrach", "Saab Mansour", "Marcello Federico"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The growing use of large language models (LLMs) has increased the need for\nautomatic evaluation systems, particularly to address the challenge of\ninformation hallucination. Although existing faithfulness evaluation approaches\nhave shown promise, they are predominantly English-focused and often require\nexpensive human-labeled training data for fine-tuning specialized models. As\nLLMs see increased adoption in multilingual contexts, there is a need for\naccurate faithfulness evaluators that can operate across languages without\nextensive labeled data. This paper presents Self-Taught Evaluators for\nMultilingual Faithfulness, a framework that learns exclusively from synthetic\nmultilingual summarization data while leveraging cross-lingual transfer\nlearning. Through experiments comparing language-specific and mixed-language\nfine-tuning approaches, we demonstrate a consistent relationship between an\nLLM's general language capabilities and its performance in language-specific\nevaluation tasks. Our framework shows improvements over existing baselines,\nincluding state-of-the-art English evaluators and machine translation-based\napproaches.", "AI": {"tldr": "The paper presents a framework for evaluating multilingual faithfulness in LLMs using synthetic data, minimizing the reliance on labeled data.", "motivation": "To address the limitations of current evaluation systems for large language models (LLMs) that struggle with information hallucination, especially in multilingual contexts where human-labeled data is scarce.", "method": "The proposed framework learns from synthetic multilingual summarization data and utilizes cross-lingual transfer learning, comparing language-specific fine-tuning against mixed-language approaches.", "result": "The framework shows improvements over existing evaluation baselines, including state-of-the-art English evaluators and machine translation-based methods, demonstrating a link between LLM capabilities and evaluation task performance.", "conclusion": "Self-Taught Evaluators can effectively evaluate LLM outputs across multiple languages without needing extensive labeled datasets, enhancing multilingual LLM application reliability.", "key_contributions": ["Developed a framework for multilingual faithfulness evaluation using synthetic data.", "Utilized cross-lingual transfer learning to reduce dependency on labeled data.", "Showed superior performance to existing multilingual evaluation baselines."], "limitations": "The evaluation is limited to certain languages and may not generalize to all multilingual contexts.", "keywords": ["multilingual", "faithfulness evaluation", "large language models", "cross-lingual transfer learning", "synthetic data"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.20783", "pdf": "https://arxiv.org/pdf/2507.20783.pdf", "abs": "https://arxiv.org/abs/2507.20783", "title": "On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey", "authors": ["Meishan Zhang", "Xin Zhang", "Xinping Zhao", "Shouzheng Huang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "45 pages, 2 figures, 9 tables", "summary": "Text embeddings have attracted growing interest due to their effectiveness\nacross a wide range of natural language processing (NLP) tasks, such as\nretrieval, classification, clustering, bitext mining, and summarization. With\nthe emergence of pretrained language models (PLMs), general-purpose text\nembeddings (GPTE) have gained significant traction for their ability to produce\nrich, transferable representations. The general architecture of GPTE typically\nleverages PLMs to derive dense text representations, which are then optimized\nthrough contrastive learning on large-scale pairwise datasets. In this survey,\nwe provide a comprehensive overview of GPTE in the era of PLMs, focusing on the\nroles PLMs play in driving its development. We first examine the fundamental\narchitecture and describe the basic roles of PLMs in GPTE, i.e., embedding\nextraction, expressivity enhancement, training strategies, learning objectives,\nand data construction. Then, we describe advanced roles enabled by PLMs, such\nas multilingual support, multimodal integration, code understanding, and\nscenario-specific adaptation. Finally, we highlight potential future research\ndirections that move beyond traditional improvement goals, including ranking\nintegration, safety considerations, bias mitigation, structural information\nincorporation, and the cognitive extension of embeddings. This survey aims to\nserve as a valuable reference for both newcomers and established researchers\nseeking to understand the current state and future potential of GPTE.", "AI": {"tldr": "This survey reviews general-purpose text embeddings (GPTE) in the context of pretrained language models (PLMs), exploring their architecture, roles in enhancing NLP tasks, and future research directions.", "motivation": "To provide a comprehensive overview of the development and effectiveness of GPTE due to the rise of PLMs in various NLP tasks.", "method": "The paper surveys existing literature on GPTE architectures, examining the role of PLMs in embedding extraction, expressivity, training strategies, and beyond.", "result": "Key findings include the capability of GPTE to support multilingualism, multimodal integration, and scenario-specific adaptation, alongside highlighting future research areas such as bias mitigation and cognitive extension.", "conclusion": "This survey serves as a crucial resource for understanding GPTEs and their implications in future research in the NLP community.", "key_contributions": ["Comprehensive overview of GPTE with PLMs", "Detailed examination of PLM roles in embedding development", "Identification of future research directions in GPTE"], "limitations": "The paper primarily focuses on GPTE without delving deeply into specific applications or implementations.", "keywords": ["text embeddings", "pretrained language models", "natural language processing", "contrastive learning", "multimodal integration"], "importance_score": 8, "read_time_minutes": 45}}
{"id": "2507.20786", "pdf": "https://arxiv.org/pdf/2507.20786.pdf", "abs": "https://arxiv.org/abs/2507.20786", "title": "Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models", "authors": ["Sam Osian", "Arpan Dutta", "Sahil Bhandari", "Iain E. Buchan", "Dan W. Joyce"], "categories": ["cs.CL"], "comment": "8 pages, 1 figure", "summary": "Prevention of Future Deaths (PFD) reports, issued by coroners in England and\nWales, flag systemic hazards that may lead to further loss of life. Analysis of\nthese reports has previously been constrained by the manual effort required to\nidentify and code relevant cases. In 2025, the Office for National Statistics\n(ONS) published a national thematic review of child-suicide PFD reports ($\\leq$\n18 years), identifying 37 cases from January 2015 to November 2023 - a process\nbased entirely on manual curation and coding. We evaluated whether a fully\nautomated, open source \"text-to-table\" language-model pipeline (PFD Toolkit)\ncould reproduce the ONS's identification and thematic analysis of child-suicide\nPFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD\nreports published from July 2013 to November 2023 were processed via PFD\nToolkit's large language model pipelines. Automated screening identified cases\nwhere the coroner attributed death to suicide in individuals aged 18 or\nyounger, and eligible reports were coded for recipient category and 23 concern\nsub-themes, replicating the ONS coding frame. PFD Toolkit identified 72\nchild-suicide PFD reports - almost twice the ONS count. Three blinded\nclinicians adjudicated a stratified sample of 144 reports to validate the\nchild-suicide screening. Against the post-consensus clinical annotations, the\nLLM-based workflow showed substantial to almost-perfect agreement (Cohen's\n$\\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script\nruntime was 8m 16s, transforming a process that previously took months into one\nthat can be completed in minutes. This demonstrates that automated LLM analysis\ncan reliably and efficiently replicate manual thematic reviews of coronial\ndata, enabling scalable, reproducible, and timely insights for public health\nand safety. The PFD Toolkit is openly available for future research.", "AI": {"tldr": "The PFD Toolkit, an automated language-model pipeline, efficiently identifies child-suicide reports from coroner data, replicating the Office for National Statistics' manual analysis with higher detection rates and significant time savings.", "motivation": "To improve the analysis of Prevention of Future Deaths reports by automating the identification and coding of relevant cases, specifically focusing on child-suicide instances.", "method": "The study used an automated pipeline called PFD Toolkit that processes all PFD reports from July 2013 to November 2023, identifying cases of child suicides and coding them accordingly, thereby streamlining the review process.", "result": "The PFD Toolkit identified 72 child-suicide reports, nearly doubling the count compared to the Office for National Statistics, with a high level of agreement with clinical annotations (Cohen's kappa = 0.82).", "conclusion": "Automated LLM analysis can effectively replicate and enhance manual thematic reviews of coronial data, making insights extraction more efficient and reproducible for public health applications.", "key_contributions": ["Development of the PFD Toolkit for automated analysis of coronial reports", "Demonstrated higher case identification rates compared to manual methods", "Significant reduction in processing time from months to minutes."], "limitations": "", "keywords": ["language model", "child suicide", "coroners", "text analysis", "public health"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.20849", "pdf": "https://arxiv.org/pdf/2507.20849.pdf", "abs": "https://arxiv.org/abs/2507.20849", "title": "Latent Inter-User Difference Modeling for LLM Personalization", "authors": ["Yilun Qiu", "Tianhao Shi", "Xiaoyan Zhao", "Fengbin Zhu", "Yang Zhang", "Fuli Feng"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP.", "AI": {"tldr": "The paper introduces a framework called Difference-aware Embedding-based Personalization (DEP) that enhances LLM outputs by modeling inter-user differences without depending on language prompts, resulting in improved personalized content generation.", "motivation": "There is a growing demand for personalized outputs from large language models (LLMs), yet existing methods mainly focus on individual user history and often miss inter-user differences which are crucial for effective personalization.", "method": "DEP utilizes a framework that models user differences in the latent space rather than through language prompts, creating soft prompts by contrasting user embeddings with peers. A sparse autoencoder is employed to filter user-specific and difference-aware embeddings before integrating them into a frozen LLM.", "result": "Experiments on personalized review generation demonstrate that DEP outperforms baseline methods consistently across various metrics, indicating its efficacy in generating personalized content.", "conclusion": "DEP provides a novel approach to personalization by focusing on behavioral signals derived from inter-user comparisons, leading to more effective use of LLMs for personalized outputs.", "key_contributions": ["Introduction of the DEP framework for personalization", "Utilization of inter-user differences in latent space", "Demonstration of performance improvements over baseline methods"], "limitations": "", "keywords": ["Large Language Models", "Personalization", "User Embedding", "Sparse Autoencoder", "Behavioral Signals"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.20858", "pdf": "https://arxiv.org/pdf/2507.20858.pdf", "abs": "https://arxiv.org/abs/2507.20858", "title": "A survey of diversity quantification in natural language processing: The why, what, where and how", "authors": ["Louis Estève", "Marie-Catherine de Marneffe", "Nurit Melnik", "Agata Savary", "Olha Kanishcheva"], "categories": ["cs.CL"], "comment": null, "summary": "The concept of diversity has received increased consideration in Natural\nLanguage Processing (NLP) in recent years. This is due to various motivations\nlike promoting and inclusion, approximating human linguistic behavior, and\nincreasing systems' performance. Diversity has however often been addressed in\nan ad hoc manner in NLP, and with few explicit links to other domains where\nthis notion is better theorized. We survey articles in the ACL Anthology from\nthe past 6 years, with \"diversity\" or \"diverse\" in their title. We find a wide\nrange of settings in which diversity is quantified, often highly specialized\nand using inconsistent terminology. We put forward a unified taxonomy of why,\nwhat on, where, and how diversity is measured in NLP. Diversity measures are\ncast upon a unified framework from ecology and economy (Stirling, 2007) with 3\ndimensions of diversity: variety, balance and disparity. We discuss the trends\nwhich emerge due to this systematized approach. We believe that this study\npaves the way towards a better formalization of diversity in NLP, which should\nbring a better understanding of this notion and a better comparability between\nvarious approaches.", "AI": {"tldr": "This paper surveys the concept of diversity in NLP, proposing a unified taxonomy and framework to formalize the measurement of diversity across different applications in the field.", "motivation": "Increased consideration of diversity in NLP motivated by inclusion, human-like linguistic behavior, and improved performance.", "method": "Survey of articles from the ACL Anthology regarding diversity in NLP, leading to the proposal of a unified taxonomy using a framework from ecology and economy.", "result": "Diversity is found to be quantified in various ways across different NLP settings, leading to inconsistent terminology; a unified taxonomy is proposed.", "conclusion": "Formalizing diversity in NLP can enhance understanding and comparability of various approaches to measuring it.", "key_contributions": ["Unified taxonomy for measuring diversity in NLP", "Application of ecological and economic frameworks to diversity", "Identification of trends through a systematized approach"], "limitations": "", "keywords": ["diversity", "Natural Language Processing", "taxonomy", "measurement", "framework"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.20859", "pdf": "https://arxiv.org/pdf/2507.20859.pdf", "abs": "https://arxiv.org/abs/2507.20859", "title": "Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings", "authors": ["Luc Builtjes", "Joeran Bosma", "Mathias Prokop", "Bram van Ginneken", "Alessa Hering"], "categories": ["cs.CL"], "comment": "34 pages, 5 figures", "summary": "Medical reports contain rich clinical information but are often unstructured\nand written in domain-specific language, posing challenges for information\nextraction. While proprietary large language models (LLMs) have shown promise\nin clinical natural language processing, their lack of transparency and data\nprivacy concerns limit their utility in healthcare. This study therefore\nevaluates nine open-source generative LLMs on the DRAGON benchmark, which\nincludes 28 clinical information extraction tasks in Dutch. We developed\n\\texttt{llm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess model\nperformance in a zero-shot setting. Several 14 billion parameter models,\nPhi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,\nwhile the bigger Llama-3.3-70B model achieved slightly higher performance at\ngreater computational cost. Translation to English prior to inference\nconsistently degraded performance, highlighting the need of native-language\nprocessing. These findings demonstrate that open-source LLMs, when used with\nour framework, offer effective, scalable, and privacy-conscious solutions for\nclinical information extraction in low-resource settings.", "AI": {"tldr": "The study evaluates open-source generative LLMs for clinical information extraction, proposing a framework that shows competitive performance while addressing data privacy concerns.", "motivation": "To provide an effective and privacy-conscious solution for information extraction from unstructured medical reports written in domain-specific language.", "method": "Evaluation of nine open-source generative LLMs on the DRAGON benchmark involving 28 clinical information extraction tasks in Dutch, using the framework 'llm_extractinator' for zero-shot model performance assessment.", "result": "Several 14 billion parameter models achieved competitive results, with the Llama-3.3-70B model performing slightly better at increased computational cost; native-language processing was essential for maintaining performance.", "conclusion": "Open-source LLMs can effectively and scalably extract clinical information, addressing privacy issues in healthcare applications, especially in low-resource settings.", "key_contributions": ["Development of the 'llm_extractinator' framework for clinical information extraction", "Evaluation of various open-source generative LLMs on clinical tasks", "Highlighting the importance of native-language processing for performance"], "limitations": "Focus on Dutch language tasks may limit generalizability, and larger models require significant computational resources.", "keywords": ["Clinical information extraction", "Open-source LLMs", "Zero-shot learning", "Data privacy", "Natural language processing"], "importance_score": 9, "read_time_minutes": 34}}
{"id": "2507.20906", "pdf": "https://arxiv.org/pdf/2507.20906.pdf", "abs": "https://arxiv.org/abs/2507.20906", "title": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning", "authors": ["Jungwon Park", "Wonjong Rhee"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks by conditioning on input-output examples in the prompt, without requiring\nany update in model parameters. While widely adopted, it remains unclear\nwhether prompting with multiple examples is the most effective and efficient\nway to convey task information. In this work, we propose Soft Injection of task\nembeddings. The task embeddings are constructed only once using few-shot ICL\nprompts and repeatedly used during inference. Soft injection is performed by\nsoftly mixing task embeddings with attention head activations using\npre-optimized mixing parameters, referred to as soft head-selection parameters.\nThis method not only allows a desired task to be performed without in-prompt\ndemonstrations but also significantly outperforms existing ICL approaches while\nreducing memory usage and compute cost at inference time. An extensive\nevaluation is performed across 57 tasks and 12 LLMs, spanning four model\nfamilies of sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show\nthat our method also serves as an insightful tool for analyzing task-relevant\nroles of attention heads, revealing that task-relevant head positions selected\nby our method transfer across similar tasks but not across dissimilar ones --\nunderscoring the task-specific nature of head functionality. Our soft injection\nmethod opens a new paradigm for reducing prompt length and improving task\nperformance by shifting task conditioning from the prompt space to the\nactivation space.", "AI": {"tldr": "This paper introduces Soft Injection of task embeddings, a method that enhances In-Context Learning (ICL) in Large Language Models (LLMs) by optimizing task performance and reducing memory usage without needing in-prompt demonstrations.", "motivation": "To address the inefficiency of using multiple examples in prompting LLMs for task performance, and to explore more effective ways to convey task information.", "method": "The study proposes a method called Soft Injection, which constructs task embeddings using few-shot ICL prompts and combines them with attention head activations using pre-optimized mixing parameters (soft head-selection parameters).", "result": "The proposed method outperforms traditional 10-shot ICL approaches by 10.1%-13.9% across 12 LLMs and 57 different tasks, demonstrating improved task performance and reduced compute costs.", "conclusion": "Soft Injection offers a new approach to task conditioning in LLMs, enabling better performance and efficiency by leveraging activation space instead of prompt space.", "key_contributions": ["Introduces Soft Injection for task embeddings in LLMs.", "Achieves significant performance improvements in various tasks compared to traditional ICL methods.", "Provides insights into the task-relevant roles of attention heads."], "limitations": "", "keywords": ["In-Context Learning", "Large Language Models", "Task Embeddings", "Attention Mechanisms", "Soft Injection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.20917", "pdf": "https://arxiv.org/pdf/2507.20917.pdf", "abs": "https://arxiv.org/abs/2507.20917", "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation", "authors": ["Adrien Bazoge"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work introduces MediQAl, a French medical question answering dataset\ndesigned to evaluate the capabilities of language models in factual medical\nrecall and reasoning over real-world clinical scenarios. MediQAl contains\n32,603 questions sourced from French medical examinations across 41 medical\nsubjects. The dataset includes three tasks: (i) Multiple-Choice Question with\nUnique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)\nOpen-Ended Question with Short-Answer. Each question is labeled as\nUnderstanding or Reasoning, enabling a detailed analysis of models' cognitive\ncapabilities. We validate the MediQAl dataset through extensive evaluation with\n14 large language models, including recent reasoning-augmented models, and\nobserve a significant performance gap between factual recall and reasoning\ntasks. Our evaluation provides a comprehensive benchmark for assessing language\nmodels' performance on French medical question answering, addressing a crucial\ngap in multilingual resources for the medical domain.", "AI": {"tldr": "MediQAl is a French medical question answering dataset with 32,603 questions designed to evaluate language models' recall and reasoning in real-world clinical scenarios.", "motivation": "To evaluate the capabilities of language models in the medical domain, particularly in French-speaking contexts, and address the lack of multilingual resources.", "method": "The dataset includes three tasks: unique answer MCQs, multiple answer MCQs, and open-ended short-answer questions, with questions labeled for understanding or reasoning to analyze cognitive capabilities.", "result": "The evaluation of 14 large language models reveals a significant performance gap between tasks focusing on factual recall versus reasoning.", "conclusion": "MediQAl provides a comprehensive benchmark for assessing language models in French medical question answering, filling a critical gap in multilingual datasets.", "key_contributions": ["Introduction of the MediQAl dataset for French medical question answering.", "Comprehensive evaluation across multiple tasks and 14 language models.", "Identification of performance gaps in medical language processing between recall and reasoning."], "limitations": "Dataset focused on French language only; performance might vary with different medical contexts beyond exams.", "keywords": ["MediQAl", "medical question answering", "language models", "French language", "multilingual resources"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.20924", "pdf": "https://arxiv.org/pdf/2507.20924.pdf", "abs": "https://arxiv.org/abs/2507.20924", "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models", "authors": ["Roberto Labadie-Tamayo", "Adrian Jaques Böck", "Djordje Slijepčević", "Xihui Chen", "Andreas Babic", "Matthias Zeppelzauer"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "I.2"], "comment": "12 pages", "summary": "Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish.", "AI": {"tldr": "The paper presents solutions for the fifth Sexism Identification in Social Networks (EXIST) challenge by focusing on identifying and classifying sexism in social media posts, specifically through three subtasks involving Twitter data.", "motivation": "To address the widespread issue of sexism on social media platforms, the study aims to create effective models for identifying and classifying sexist content in textual posts.", "method": "The paper implements three models: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa model, each addressing different subtasks related to sexism detection in tweets.", "result": "SCBM uses human-interpretable bottleneck concepts for classification, SCBMT combines this with contextual embeddings for better performance, while XLM-RoBERTa achieves competitive rankings in the benchmark evaluations for identifying sexism.", "conclusion": "The models not only perform well in identifying sexism but also provide explanations for their classifications, and the incorporation of additional metadata improves results.", "key_contributions": ["Introduction of SCBM and SCBMT models for sexism identification", "Use of LLMs for generating human-interpretable representations", "Competitive results in benchmark evaluations for language models."], "limitations": "", "keywords": ["sexism", "social media", "Twitter", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.20930", "pdf": "https://arxiv.org/pdf/2507.20930.pdf", "abs": "https://arxiv.org/abs/2507.20930", "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models", "authors": ["Likun Tan", "Kuan-Wei Huang", "Kevin Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/fine-grained-editting.", "AI": {"tldr": "This paper addresses hallucinations in large language models by proposing a method for detecting and editing erroneous content in model-generated responses, particularly in finance.", "motivation": "The critical challenge of hallucinations in large language models affects factual reliability in high-stakes domains like finance, necessitating effective detection and editing mechanisms.", "method": "The authors constructed a synthetic dataset with tagged errors in financial question-answering and fine-tuned four language models (Phi-4, Phi-4-mini, Qwen3-4B, Qwen3-14B) to enhance detection and editing of factual inaccuracies.", "result": "The fine-tuned Phi-4 demonstrated an 8% improvement in binary F1 score and a 30% increase in overall detection performance compared to OpenAI-o3, while the Phi-4-mini model showed competitive performance with minimal drops in accuracy.", "conclusion": "This work provides a practical solution to factual inconsistencies in financial text generation and offers a framework applicable across various domains for improving the reliability of large language models.", "key_contributions": ["Developed a synthetic dataset for training language models on domain-specific errors.", "Achieved significant performance improvements in detecting and editing factual inaccuracies in financial texts.", "Introduced a generalizable framework for enhancing the reliability of large language models."], "limitations": "The results are primarily focused on the finance domain, which may limit the immediate applicability of the findings to other fields without further adaptation.", "keywords": ["large language models", "factual reliability", "error detection", "financial text generation", "synthetic dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.20956", "pdf": "https://arxiv.org/pdf/2507.20956.pdf", "abs": "https://arxiv.org/abs/2507.20956", "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models", "authors": ["Max Peeperkorn", "Tom Kouwenhoven", "Dan Brown", "Anna Jordanous"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality.", "AI": {"tldr": "The paper investigates how instruction-tuning affects the output diversity of large language models (LLMs) during narrative generation, revealing a substantial 'diversity gap' and proposing a new decoding strategy.", "motivation": "To understand the impact of instruction-tuning on the diversity of outputs in narrative generation tasks with large language models.", "method": "The paper analyzes the diversity loss during fine-tuning across the OLMo and OLMo 2 models, utilizing current diversity metrics.", "result": "Instruction-tuning leads to significant decreases in output diversity, with DPO having the largest negative effect. The proposed conformative decoding method enhances diversity without compromising quality.", "conclusion": "Conformative decoding can effectively reintroduce diversity into the outputs of instruction-tuned language models while maintaining or improving output quality.", "key_contributions": ["Analysis of diversity loss during fine-tuning of LLMs", "Identification of DPO's impact on output diversity", "Introduction of the conformative decoding strategy"], "limitations": "", "keywords": ["diversity gap", "instruction-tuning", "conformative decoding", "large language models", "narrative generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.21009", "pdf": "https://arxiv.org/pdf/2507.21009.pdf", "abs": "https://arxiv.org/abs/2507.21009", "title": "Memorization in Fine-Tuned Large Language Models", "authors": ["Danil Savine", "Muni Sreenivas Pydi", "Jamal Atif", "Olivier Cappé"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the mechanisms and factors influencing memorization\nin fine-tuned large language models (LLMs), with a focus on the medical domain\ndue to its privacy-sensitive nature. We examine how different aspects of the\nfine-tuning process affect a model's propensity to memorize training data,\nusing the PHEE dataset of pharmacovigilance events.\n  Our research employs two main approaches: a membership inference attack to\ndetect memorized data, and a generation task with prompted prefixes to assess\nverbatim reproduction. We analyze the impact of adapting different weight\nmatrices in the transformer architecture, the relationship between perplexity\nand memorization, and the effect of increasing the rank in low-rank adaptation\n(LoRA) fine-tuning.\n  Key findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices; (2) Lower\nperplexity in the fine-tuned model correlates with increased memorization; (3)\nHigher LoRA ranks lead to increased memorization, but with diminishing returns\nat higher ranks.\n  These results provide insights into the trade-offs between model performance\nand privacy risks in fine-tuned LLMs. Our findings have implications for\ndeveloping more effective and responsible strategies for adapting large\nlanguage models while managing data privacy concerns.", "AI": {"tldr": "This study explores memorization in fine-tuned large language models in the medical domain, highlighting trade-offs between performance and privacy.", "motivation": "To understand how fine-tuning affects memorization in LLMs, particularly in the privacy-sensitive medical field.", "method": "Implemented membership inference attacks and generation tasks to analyze memorization effects with the PHEE dataset.", "result": "Key findings indicate that specific weight matrices significantly contribute to memorization, with lower perplexity correlating with increased memorization and higher LoRA ranks also increasing memorization, though with diminishing returns.", "conclusion": "The study provides insights for balancing model performance with privacy risks, suggesting responsible fine-tuning strategies for LLMs.", "key_contributions": ["Investigation of memorization in medical fine-tuned LLMs", "Analysis of impacts from weight matrices, perplexity, and LoRA ranks", "Recommendations for balancing model performance and privacy"], "limitations": "", "keywords": ["memorization", "large language models", "fine-tuning", "privacy", "persistence in ML"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.21028", "pdf": "https://arxiv.org/pdf/2507.21028.pdf", "abs": "https://arxiv.org/abs/2507.21028", "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation", "authors": ["Jiaju Chen", "Yuxuan Lu", "Xiaojie Wang", "Huimin Zeng", "Jing Huang", "Jiri Gesi", "Ying Xu", "Bingsheng Yao", "Dakuo Wang"], "categories": ["cs.CL", "68T50"], "comment": null, "summary": "Nearly all human work is collaborative; thus, the evaluation of real-world\nNLP applications often requires multiple dimensions that align with diverse\nhuman perspectives. As real human evaluator resources are often scarce and\ncostly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising\napproach to leverage LLM agents to believably simulate human evaluators. Yet,\nto date, existing LLM-as-a-judge approaches face two limitations: persona\ndescriptions of agents are often arbitrarily designed, and the frameworks are\nnot generalizable to other tasks. To address these challenges, we propose\nMAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from relevant\ntext documents (e.g., research papers), instantiate LLM agents with the\npersonas, and engage in-group debates with multi-agents to Generate\nmulti-dimensional feedback. Our evaluation experiments in both the educational\nand medical domains demonstrate that MAJ-EVAL can generate evaluation results\nthat better align with human experts' ratings compared with conventional\nautomated evaluation metrics and existing LLM-as-a-judge methods.", "AI": {"tldr": "MAJ-EVAL is a Multi-Agent-as-Judge framework that uses LLM agents to simulate human evaluators, providing multi-dimensional feedback for NLP applications in education and medicine.", "motivation": "The evaluation of real-world NLP applications requires multiple dimensions and human perspectives, but real human evaluator resources are often scarce and costly.", "method": "MAJ-EVAL automatically constructs multiple evaluator personas from relevant text documents and engages LLM agents in group debates to generate diverse evaluation feedback.", "result": "MAJ-EVAL produces evaluation results that align better with human experts' ratings compared to traditional automated metrics and existing methods.", "conclusion": "The proposed framework effectively addresses limitations in existing LLM-as-a-judge approaches, enhancing the evaluation process for NLP applications.", "key_contributions": ["Introduction of a framework (MAJ-EVAL) that constructs evaluator personas automatically", "Use of LLM agents to generate multi-dimensional feedback through debates", "Demonstrated improved alignment with human expert ratings in evaluations for educational and medical domains."], "limitations": "Framework may require well-curated input documents to create accurate evaluator personas.", "keywords": ["LLM-as-a-judge", "evaluation framework", "multi-agent system", "NLP applications", "human evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2401.12295", "pdf": "https://arxiv.org/pdf/2401.12295.pdf", "abs": "https://arxiv.org/abs/2401.12295", "title": "Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data", "authors": ["Leonardo Castro-Gonzalez", "Yi-Ling Chung", "Hannak Rose Kirk", "John Francis", "Angus R. Williams", "Pica Johansson", "Jonathan Bright"], "categories": ["cs.CL", "I.2.7; J.4"], "comment": "46 pages, 17 figures, 6 tables", "summary": "The field of machine learning has recently made significant progress in\nreducing the requirements for labelled training data when building new models.\nThese `cheaper' learning techniques hold significant potential for the social\nsciences, where development of large labelled training datasets is often a\nsignificant practical impediment to the use of machine learning for analytical\ntasks. In this article we review three `cheap' techniques that have developed\nin recent years: weak supervision, transfer learning and prompt engineering.\nFor the latter, we also review the particular case of zero-shot prompting of\nlarge language models. For each technique we provide a guide of how it works\nand demonstrate its application across six different realistic social science\napplications (two different tasks paired with three different dataset makeups).\nWe show good performance for all techniques, and in particular we demonstrate\nhow prompting of large language models can achieve high accuracy at very low\ncost. Our results are accompanied by a code repository to make it easy for\nothers to duplicate our work and use it in their own research. Overall, our\narticle is intended to stimulate further uptake of these techniques in the\nsocial sciences.", "AI": {"tldr": "The paper reviews 'cheap' machine learning techniques—weak supervision, transfer learning, and prompt engineering—highlighting their effectiveness in social science applications.", "motivation": "To address the practical challenges in using machine learning for social sciences due to the lack of large labeled datasets.", "method": "The article reviews and demonstrates weak supervision, transfer learning, and prompt engineering, particularly focusing on zero-shot prompting of large language models.", "result": "All techniques showed good performance, especially zero-shot prompting, achieving high accuracy with minimal costs across various social science applications.", "conclusion": "The findings aim to encourage the use of these machine learning techniques in social sciences, providing tools for replication and further research.", "key_contributions": ["Review of three cheap learning techniques", "Application demonstration across social science tasks", "Code repository for reproducibility"], "limitations": "", "keywords": ["machine learning", "weak supervision", "transfer learning", "prompt engineering", "social sciences"], "importance_score": 6, "read_time_minutes": 46}}
{"id": "2403.18140", "pdf": "https://arxiv.org/pdf/2403.18140.pdf", "abs": "https://arxiv.org/abs/2403.18140", "title": "Juru: Legal Brazilian Large Language Model from Reputable Sources", "authors": ["Roseval Malaquias Junior", "Ramon Pires", "Roseli Romero", "Rodrigo Nogueira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The high compute cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Mistral-7B model with 1.9 billion unique tokens\nfrom reputable Brazilian legal sources and conducted few-shot evaluations on\nlegal and general knowledge test suites. Our model, Juru, demonstrates the\nbenefits of domain specialization by achieving improved performance on legal\nbenchmarks, even with a reduced amount of pretraining data. However, this\ndomain specialization through continued pretraining comes at the cost of\nincreased forgetting in unrelated domains, as evidenced by performance\ndegradation on general knowledge test suites in both Portuguese and English.\nThis study contributes to the growing body of scientific evidence showing that\npretraining data selection may enhance the performance of large language\nmodels, enabling the exploration of these models at a lower cost. Juru is\npublicly available at https://huggingface.co/roseval/Juru-7B .", "AI": {"tldr": "This paper presents Juru, a domain-specialized language model achieved by pretraining the Mistral-7B model on Brazilian legal data, showing improved legal performance but increased forgetting in unrelated domains.", "motivation": "To address the high compute costs of pretraining large language models while improving their performance on specific domains, particularly in legal contexts.", "method": "The Mistral-7B model was specialized using 1.9 billion unique tokens from reputable Brazilian legal sources, followed by few-shot evaluations on legal and general knowledge test suites.", "result": "Juru outperforms other models on legal benchmarks, despite the reduced amount of pretraining data, though it suffers from forgetting performance in general knowledge tasks.", "conclusion": "Domain specialization through selective pretraining improves performance on specific tasks but can lead to performance degradation in unrelated areas, highlighting the trade-off in model training.", "key_contributions": ["Introduction of Juru, a specialized model for legal tasks", "Evidence that pretraining data selection can improve performance in specific domains", "Demonstration of the trade-off between specialization and forgetting unrelated knowledge"], "limitations": "Increased forgetting observed in unrelated domains; potential need for balance in pretraining strategies.", "keywords": ["large language models", "domain specialization", "pretraining", "legal benchmarks", "performance degradation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2406.00222", "pdf": "https://arxiv.org/pdf/2406.00222.pdf", "abs": "https://arxiv.org/abs/2406.00222", "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training", "authors": ["Maximillian Chen", "Ruoxi Sun", "Tomas Pfister", "Sercan Ö. Arık"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025; Code:\n  https://github.com/google-research/google-research/tree/master/learning_to_clarify", "summary": "Large language models (LLMs), optimized through human feedback, have rapidly\nemerged as a leading paradigm for developing intelligent conversational\nassistants. However, despite their strong performance across many benchmarks,\nLLM-based agents might still lack conversational skills such as disambiguation\n-- when they are faced with ambiguity, they often overhedge or implicitly guess\nusers' true intents rather than asking clarification questions. Under\ntask-specific settings, high-quality conversation samples are often limited,\nconstituting a bottleneck for LLMs' ability to learn optimal dialogue action\npolicies. We propose Action-Based Contrastive Self-Training (ACT), a\nquasi-online preference optimization algorithm based on Direct Preference\nOptimization (DPO), that enables data-efficient dialogue policy learning in\nmulti-turn conversation modeling. We demonstrate ACT's efficacy under in\ndata-efficient tuning scenarios, even when there is no action label available,\nusing multiple real-world conversational tasks: tabular-grounded\nquestion-answering, machine reading comprehension, and AmbigSQL, a novel task\nfor disambiguating information-seeking requests for complex SQL generation\ntowards data analysis agents. Additionally, we propose evaluating LLMs' ability\nto function as conversational agents by examining whether they can implicitly\nrecognize and reason about ambiguity in conversation. ACT demonstrates\nsubstantial conversation modeling improvements over standard tuning approaches\nlike supervised fine-tuning and DPO.", "AI": {"tldr": "This paper presents Action-Based Contrastive Self-Training (ACT), a method to improve dialogue policies in LLMs, focusing on conversational skills such as disambiguation.", "motivation": "LLM-based agents often struggle with ambiguity in conversations, lacking the ability to ask clarification questions, which hinders their conversational skills.", "method": "The proposed ACT is a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) that enables data-efficient learning of dialogue policies in multi-turn conversations.", "result": "ACT shows improved performance in dialogue modeling over traditional methods like supervised fine-tuning, particularly in data-efficient scenarios.", "conclusion": "The efficacy of ACT suggests it can enhance LLM conversational agents' ability to manage ambiguity and improve their dialogue interactions in real-world tasks.", "key_contributions": ["Introduction of Action-Based Contrastive Self-Training (ACT) for LLMs.", "Demonstration of ACT's effectiveness in data-efficient scenarios without needing action labels.", "Evaluation of LLMs' ability to recognize and reason about ambiguity in conversations."], "limitations": "", "keywords": ["large language models", "dialogue policy learning", "disambiguation", "human feedback", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.13632", "pdf": "https://arxiv.org/pdf/2406.13632.pdf", "abs": "https://arxiv.org/abs/2406.13632", "title": "DoubleDipper: Improving Long-Context LLMs via Context Recycling", "authors": ["Arie Cattan", "Alon Jacovi", "Alex Fabrikant", "Jonathan Herzig", "Roee Aharoni", "Hannah Rashkin", "Dror Marcus", "Avinatan Hassidim", "Yossi Matias", "Idan Szpektor", "Avi Caciularu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In this work,\nwe propose DoubleDipper, a novel In-Context-Learning method that automatically\ngenerates few-shot examples for long context QA tasks by recycling contexts.\nSpecifically, given a long input context (1-3k tokens) and a query, we generate\nadditional query-output pairs from the given context as few-shot examples,\nwhile introducing the context only once. This ensures that the demonstrations\nare leveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context. Surprisingly, despite introducing only single-hop\nICL examples, LLMs successfully generalize to multi-hop long-context QA using\nour approach.", "AI": {"tldr": "DoubleDipper is a novel In-Context-Learning method that improves long context QA tasks using few-shot examples generated from the same input context.", "motivation": "To address the sub-optimal performance of LLMs on tasks that require understanding of long contexts.", "method": "DoubleDipper recycles contexts to create few-shot examples for long context QA tasks by generating additional query-output pairs; it enhances the prompts by identifying relevant paragraphs before the answer.", "result": "The application of DoubleDipper resulted in an average improvement of +16 points across various QA datasets for multiple LLMs.", "conclusion": "The approach not only improves single-hop QA tasks but surprisingly enables LLMs to generalize to multi-hop long-context QA tasks as well.", "key_contributions": ["Introduction of DoubleDipper for few-shot examples in long context QA", "Significant performance improvements across multiple LLMs", "Enhanced relevance attribution in generated answers"], "limitations": "", "keywords": ["Large Language Models", "In-Context Learning", "Long Context QA"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2409.06624", "pdf": "https://arxiv.org/pdf/2409.06624.pdf", "abs": "https://arxiv.org/abs/2409.06624", "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio", "authors": ["Ningyuan Xi", "Yetao Wu", "Kun Fan", "Teng Chen", "Qingqing Gu", "Luo Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 2 figures", "summary": "Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to\nobtain unfamiliar language skills or adapt to new domains. The huge training\ncost of CPT often asks for cautious choice of key hyper-parameters such as the\nmixture ratio of extra language or domain corpus. However, there is no\nsystematic study that bridges the gap between the optimal mixture ratio and the\nactual model performance, and the gap between experimental scaling law and the\nactual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicates the optimal\nexperimental setup. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark but also in some specific domains including math, coding, and\nemotional intelligence. We deploy the final 70B version of LLM on a real-life\nchat system which obtains satisfying performance.", "AI": {"tldr": "This paper explores the Continual Pre-Training (CPT) of Llama-3 models to enhance Chinese language skills by optimizing hyper-parameters such as the Additional Language Mixture Ratio (ALMR) and Learning Rate (LR).", "motivation": "To systematically study the optimal mixture ratio of language corpora and its impact on model performance for continual pre-training of language models, particularly for enhancing capabilities in new domains.", "method": "Continual Pre-Training was performed on Llama-3 8B and 70B models, focusing on optimizing the combination of hyper-parameters like ALMR and LR to improve performance.", "result": "The study demonstrates improved capabilities of the Llama-3 model in Chinese language understanding and performance in benchmarks related to math, coding, and emotional intelligence after careful tuning of hyper-parameters.", "conclusion": "By finding the optimal experimental setup, the model was successfully deployed in a real-life chat system, showing satisfactory performance.", "key_contributions": ["Analysis of hyper-parameter impact on performance", "Enhancement of Llama-3's Chinese capabilities", "Successful deployment of a pre-trained model in a real-world application"], "limitations": "", "keywords": ["Continual Pre-Training", "Large Language Models", "Hyper-parameters", "Chinese language", "Machine Learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.14651", "pdf": "https://arxiv.org/pdf/2410.14651.pdf", "abs": "https://arxiv.org/abs/2410.14651", "title": "Real-time Factuality Assessment from Adversarial Feedback", "authors": ["Sanxing Chen", "Yukun Huang", "Bhuwan Dhingra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that existing evaluations for assessing the factuality of news from\nconventional sources, such as claims on fact-checking websites, result in high\naccuracies over time for LLM-based detectors-even after their knowledge\ncutoffs. This suggests that recent popular false information from such sources\ncan be easily identified due to its likely presence in pre-training/retrieval\ncorpora or the emergence of salient, yet shallow, patterns in these datasets.\nInstead, we argue that a proper factuality evaluation dataset should test a\nmodel's ability to reason about current events by retrieving and reading\nrelated evidence. To this end, we develop a novel pipeline that leverages\nnatural language feedback from a RAG-based detector to iteratively modify\nreal-time news into deceptive variants that challenge LLMs. Our iterative\nrewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent\nfor a strong RAG-based GPT-4o detector. Our experiments reveal the important\nrole of RAG in both evaluating and generating challenging news examples, as\nretrieval-free LLM detectors are vulnerable to unseen events and adversarial\nattacks, while feedback from RAG-based evaluation helps discover more deceitful\npatterns.", "AI": {"tldr": "This paper presents a novel evaluation method for factuality in news, highlighting how LLM-based detectors struggle with current events and proposing a pipeline that uses RAG to generate deceptive news variants for testing.", "motivation": "Existing evaluations for news factuality yield high accuracies over time, but fail to assess a model's ability to reason about current events.", "method": "A novel pipeline is developed leveraging natural language feedback from a RAG-based detector to create deceptive news variants that challenge LLMs.", "result": "The iterative rewrite process significantly decreased the binary classification ROC-AUC by 17.5% for a strong RAG-based GPT-4o detector.", "conclusion": "RAG-based evaluation reveals crucial insights into the vulnerabilities of retrieval-free LLM detectors and helps identify deceitful patterns in news.", "key_contributions": ["Development of a novel pipeline for generating deceptive news variants", "Highlighting the limitations of retrieval-free LLM detectors", "Identifying the important role of RAG in evaluating and generating news examples"], "limitations": "Focused primarily on deceptive variant generation, broader application needs exploration.", "keywords": ["factuality", "LLM", "RAG", "news evaluation", "deception"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.15956", "pdf": "https://arxiv.org/pdf/2410.15956.pdf", "abs": "https://arxiv.org/abs/2410.15956", "title": "Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs", "authors": ["Yanzhu Guo", "Simone Conia", "Zelin Zhou", "Min Li", "Saloni Potdar", "Henry Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.", "AI": {"tldr": "This paper introduces new metrics to evaluate and improve the multilingual naturalness of Large Language Models (LLMs), which tend to exhibit English-centric biases in other languages.", "motivation": "To address the limited focus on the naturalness of multilingual LLM outputs, which often reflect English-centric patterns causing unnatural expressions in non-English languages.", "method": "The authors propose automatic corpus-level metrics to assess lexical and syntactic naturalness in multilingual contexts, and also introduce an alignment method to enhance LLM performance in target languages.", "result": "Evaluation of state-of-the-art LLMs in French and Chinese using the new metrics shows significant English-influenced output patterns, while the proposed alignment method leads to improved naturalness without degrading general performance.", "conclusion": "The study emphasizes the necessity of multilingual assessment tools and alignment strategies for enhancing the output quality of LLMs in various languages and domains.", "key_contributions": ["Novel metrics for assessing multilingual naturalness", "An alignment method for improving LLM outputs", "Evaluation of multilingual LLMs highlighting English biases"], "limitations": "", "keywords": ["Large Language Models", "Multilinguality", "Naturalness Metrics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.23771", "pdf": "https://arxiv.org/pdf/2410.23771.pdf", "abs": "https://arxiv.org/abs/2410.23771", "title": "What is Wrong with Perplexity for Long-context Language Modeling?", "authors": ["Lizhe Fang", "Yifei Wang", "Zhaoyang Liu", "Chenheng Zhang", "Stefanie Jegelka", "Jinyang Gao", "Bolin Ding", "Yisen Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.", "AI": {"tldr": "This paper addresses the limitations of perplexity (PPL) in evaluating long-context inputs for LLMs, proposing a new metric called LongPPL that focuses on key tokens, and introduces LongCE loss for improved fine-tuning.", "motivation": "The reliability of perplexity (PPL) as an evaluation metric for long-context capabilities of large language models (LLMs) is questioned, leading to the development of better methods.", "method": "The authors propose LongPPL, a novel evaluation metric that identifies key tokens important for long-context understanding, and introduce LongCE loss for fine-tuning with a focus on these key tokens.", "result": "LongPPL demonstrated a Pearson correlation of -0.96 with long-context benchmark performance, outperforming traditional PPL in predictive accuracy. LongCE also improved fine-tuning outcomes across benchmarks.", "conclusion": "The paper provides significant insights into PPL's limitations and presents effective solutions (LongPPL and LongCE) for evaluating and enhancing LLMs' long-context capabilities.", "key_contributions": ["Introduction of LongPPL as a key-token focused evaluation metric for LLMs", "Development of LongCE loss for better fine-tuning", "Demonstration of strong correlation between LongPPL and long-context performance benchmarks"], "limitations": "", "keywords": ["long-context", "large language models", "perplexity", "LongPPL", "LongCE"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.04093", "pdf": "https://arxiv.org/pdf/2411.04093.pdf", "abs": "https://arxiv.org/abs/2411.04093", "title": "Summarization of Opinionated Political Documents with Varied Perspectives", "authors": ["Nicholas Deas", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": "COLING 2025", "summary": "Global partisan hostility and polarization has increased, and this\npolarization is heightened around presidential elections. Models capable of\ngenerating accurate summaries of diverse perspectives can help reduce such\npolarization by exposing users to alternative perspectives. In this work, we\nintroduce a novel dataset and task for independently summarizing each political\nperspective in a set of passages from opinionated news articles. For this task,\nwe propose a framework for evaluating different dimensions of perspective\nsummary performance. We benchmark 11 summarization models and LLMs of varying\nsizes and architectures through both automatic and human evaluation. While\nrecent models like GPT-4o perform well on this task, we find that all models\nstruggle to generate summaries that are faithful to the intended perspective.\nOur analysis of summaries focuses on how extraction behavior is impacted by\nfeatures of the input documents.", "AI": {"tldr": "This paper addresses increasing global partisan hostility and polarization with a new summarization task and dataset focused on capturing diverse political perspectives from opinionated news articles.", "motivation": "To reduce polarization by creating models that accurately summarize diverse political perspectives, exposing users to alternative viewpoints.", "method": "The authors introduce a dataset for summarizing different political perspectives from news articles and evaluate 11 summarization models across automatic and human assessments.", "result": "The study finds that models, including recent ones like GPT-4o, perform well yet struggle to produce summaries that accurately reflect the intended perspectives.", "conclusion": "There is a need for improved summarization models that can better capture and represent diverse perspectives, addressing a gap in current AI summarization capabilities.", "key_contributions": ["Introduction of a novel dataset focused on political perspective summarization.", "Benchmarking of 11 summarization models with insights on their performance and limitations.", "Analysis of the relationship between extraction behavior and document features."], "limitations": "All models tested showed difficulty in generating faithful summaries of intended perspectives.", "keywords": ["political polarization", "summarization models", "opinionated news articles"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2412.10271", "pdf": "https://arxiv.org/pdf/2412.10271.pdf", "abs": "https://arxiv.org/abs/2412.10271", "title": "Benchmarking Linguistic Diversity of Large Language Models", "authors": ["Yanzhu Guo", "Guokan Shang", "Chloé Clavel"], "categories": ["cs.CL"], "comment": null, "summary": "The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs.", "AI": {"tldr": "This paper proposes a framework for evaluating the linguistic diversity of Large Language Models (LLMs), addressing their vocabulary, syntax, and semantics.", "motivation": "To address the neglect of linguistic diversity in the evaluation of LLMs, which can impact the richness of machine-generated language.", "method": "A comprehensive framework for evaluating LLMs is proposed, focusing on lexical, syntactic, and semantic diversity, analyzed through benchmarking and case studies.", "result": "Benchmarking of several state-of-the-art LLMs reveals varying levels of linguistic diversity, with an in-depth case study highlighting significant differences in syntactic diversity.", "conclusion": "Development and deployment choices critically affect the linguistic diversity of LLM outputs, suggesting a need for improved evaluation methods in LLM development.", "key_contributions": ["Proposed a framework for evaluating LLMs from linguistic diversity perspectives.", "Benchmarking of multiple LLMs against this framework.", "In-depth analysis of syntactic diversity in LLM outputs."], "limitations": "Limited to state-of-the-art LLMs, may not generalize to all models in practical applications.", "keywords": ["Large Language Models", "Linguistic Diversity", "Evaluation Framework", "Syntactic Diversity", "Machine Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.15748", "pdf": "https://arxiv.org/pdf/2412.15748.pdf", "abs": "https://arxiv.org/abs/2412.15748", "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models", "authors": ["Shamus Sim", "Tyrone Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 7 figures, 3 tables. Conceptualization, both authors.\n  formal analysis, both authors. funding acquisition, both authors.\n  investigation, both authors. resources, both authors. supervision, T.C..\n  validation, both authors. visualization, both authors. writing original\n  draft, both authors. writing review and editing, both authors", "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, in this work, we adapt the\nexisting concept of reasoning behaviour and articulate its interpretation\nwithin the specific context of medical LLMs. We survey and categorise current\nstate-of-the-art approaches for modeling and evaluating reasoning reasoning in\nmedical LLMs. Additionally, we propose theoretical frameworks which can empower\nmedical professionals or machine learning engineers to gain insight into the\nlow-level reasoning operations of these previously obscure models. We also\noutline key open challenges facing the development of Large Reasoning Models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole.", "AI": {"tldr": "This paper explores the reasoning behaviour of medical Large Language Models (LLMs) with a focus on achieving explainable AI in healthcare. It surveys current approaches, proposes theoretical frameworks, and discusses challenges in transparency and trust between clinicians and AI models.", "motivation": "The paper aims to address the surprising lack of studies on the reasoning behaviour of LLMs in the medical domain, emphasizing its importance for explainable AI and its implications for healthcare integration.", "method": "The authors survey current state-of-the-art approaches for modeling and evaluating reasoning in medical LLMs and propose theoretical frameworks to enhance understanding for medical professionals and machine learning engineers.", "result": "The study categorizes reasoning behaviours in medical LLMs and outlines frameworks that allow insights into the reasoning processes of these models, identifying open challenges in the field.", "conclusion": "Increased transparency in medical ML models will boost trust from clinicians and patients, accelerating the integration and development of AI in the healthcare sector.", "key_contributions": ["Proposed frameworks for understanding reasoning in medical LLMs", "Categorization of existing approaches in medical AI", "Identification of key challenges in developing reasoning models"], "limitations": "The study may not cover all existing models or methodologies in the rapidly evolving field of medical AI and reasoning.", "keywords": ["Large Language Models", "explainable AI", "healthcare", "reasoning behaviour", "machine learning"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2412.20677", "pdf": "https://arxiv.org/pdf/2412.20677.pdf", "abs": "https://arxiv.org/abs/2412.20677", "title": "Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA", "authors": ["Qingyun Jin", "Xiaohui Song", "Feng Zhou", "Zengchang Qin"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.", "AI": {"tldr": "This paper proposes a method to convert multi-head attention to grouped-query attention in large language models, improving efficiency and maintaining performance.", "motivation": "To address the degradation of inference throughput in large models due to increasing size and input sequence length, a new method for attention mechanisms is necessary.", "method": "The proposed method employs Procrustes analysis on attention heads for enhanced similarity and computational invariance, followed by $\text{L}_0$ regularization to prune redundant parameters and adapt to the GQA framework.", "result": "Experimental results indicate a successful compression of up to 87.5% of KV heads in the LLaMA2-7B model and 75% in the Sheared-LLaMA-1.3B model with only acceptable performance degradation.", "conclusion": "The method effectively reduces the complexity of attention mechanisms in large language models while maintaining performance levels.", "key_contributions": ["Developed a cost-effective method for converting MHA to GQA.", "Used Procrustes analysis to enhance head similarity and computational invariance.", "Implemented $\text{L}_0$ regularization for parameter pruning."], "limitations": "The performance degradation is acknowledged but deemed acceptable within certain bounds of compression.", "keywords": ["large language models", "attention mechanisms", "Procrustes analysis", "parameter pruning", "natural language processing"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2502.01615", "pdf": "https://arxiv.org/pdf/2502.01615.pdf", "abs": "https://arxiv.org/abs/2502.01615", "title": "Large Language Models Are Human-Like Internally", "authors": ["Tatsuki Kuribayashi", "Yohei Oseki", "Souhaib Ben Taieb", "Kentaro Inui", "Timothy Baldwin"], "categories": ["cs.CL"], "comment": "This is a pre-MIT Press publication version of the paper", "summary": "Recent cognitive modeling studies have reported that larger language models\n(LMs) exhibit a poorer fit to human reading behavior (Oh and Schuler, 2023b;\nShain et al., 2024; Kuribayashi et al., 2024), leading to claims of their\ncognitive implausibility. In this paper, we revisit this argument through the\nlens of mechanistic interpretability and argue that prior conclusions were\nskewed by an exclusive focus on the final layers of LMs. Our analysis reveals\nthat next-word probabilities derived from internal layers of larger LMs align\nwith human sentence processing data as well as, or better than, those from\nsmaller LMs. This alignment holds consistently across behavioral (self-paced\nreading times, gaze durations, MAZE task processing times) and\nneurophysiological (N400 brain potentials) measures, challenging earlier mixed\nresults and suggesting that the cognitive plausibility of larger LMs has been\nunderestimated. Furthermore, we first identify an intriguing relationship\nbetween LM layers and human measures: earlier layers correspond more closely\nwith fast gaze durations, while later layers better align with relatively\nslower signals such as N400 potentials and MAZE processing times. Our work\nopens new avenues for interdisciplinary research at the intersection of\nmechanistic interpretability and cognitive modeling.", "AI": {"tldr": "This paper argues that larger language models align better with human reading behavior than previously thought, particularly when analyzing internal layers instead of just the final layers.", "motivation": "To challenge the notion that larger language models are cognitively implausible and to show that earlier conclusions were biased due to focusing only on final layers.", "method": "The authors analyze next-word probabilities from internal layers of larger language models and compare these with human reading behavior through various behavioral and neurophysiological measures.", "result": "The analysis shows significant alignment between internal layer outputs and human reading data, suggesting that previous criticisms of larger models' cognitive plausibility are unfounded.", "conclusion": "The findings indicate a nuanced relationship between LM layers and reading behaviors, implying potential for further interdisciplinary research in mechanistic interpretability and cognitive modeling.", "key_contributions": ["Reevaluation of cognitive plausibility of larger LMs.", "Demonstrated alignment of internal LMs with human sentence processing.", "Identified relationship between LM layers and human reading metrics."], "limitations": "", "keywords": ["cognitive modeling", "interpretability", "language models", "human reading behavior", "neurophysiology"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.03387", "pdf": "https://arxiv.org/pdf/2502.03387.pdf", "abs": "https://arxiv.org/abs/2502.03387", "title": "LIMO: Less is More for Reasoning", "authors": ["Yixin Ye", "Zhen Huang", "Yang Xiao", "Ethan Chern", "Shijie Xia", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "We challenge the prevailing assumption that complex reasoning in large\nlanguage models (LLMs) necessitates massive training data. We demonstrate that\nsophisticated mathematical reasoning can emerge with only a few examples.\nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves\n63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned\nmodels (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the\ntraining data required by prior approaches. Furthermore, LIMO exhibits strong\nout-of-distribution generalization, achieving a 45.8\\% absolute improvement\nacross diverse benchmarks, outperforming models trained on 100x more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis\n(LIMO Hypothesis): In foundation models where domain knowledge has been\ncomprehensively encoded during pre-training, sophisticated reasoning can emerge\nthrough minimal but strategically designed demonstrations of cognitive\nprocesses. This hypothesis suggests that the threshold for eliciting complex\nreasoning is not dictated by task complexity but rather by two key factors: (1)\nthe completeness of the model's pre-trained knowledge base and (2) the\neffectiveness of post-training examples in serving as \"cognitive templates\"\nthat guide reasoning.", "AI": {"tldr": "This paper introduces the LIMO model, demonstrating that complex reasoning in LLMs can be achieved with minimal training data, highlighting a novel Less-Is-More Reasoning Hypothesis.", "motivation": "To challenge the assumption that large amounts of training data are necessary for complex reasoning in large language models.", "method": "The LIMO model employs simple supervised fine-tuning with only a small percentage of the training data used in prior models.", "result": "LIMO achieves 63.3% accuracy on AIME24 and 95.6% on MATH500, significantly outperforming previous methods while using only 1% of the training data.", "conclusion": "The Less-Is-More Reasoning Hypothesis posits that sophisticated reasoning can arise from strategically designed examples, depending more on the model's pre-trained knowledge and the cognitive nature of the examples than on the task complexity.", "key_contributions": ["Introduction of the LIMO model that requires less training data for effective reasoning", "Demonstration of strong out-of-distribution generalization", "Proposal of the Less-Is-More Reasoning Hypothesis for LLMs"], "limitations": "The reliance on effective post-training examples may not generalize to all types of reasoning tasks.", "keywords": ["large language models", "reasoning", "fine-tuning", "LIMO Hypothesis", "out-of-distribution generalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11131", "pdf": "https://arxiv.org/pdf/2502.11131.pdf", "abs": "https://arxiv.org/abs/2502.11131", "title": "Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM", "authors": ["Yuqi Liu", "Yan Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https://github.com/liuyuqi123study/RankSVM_for_SLR", "AI": {"tldr": "The paper explores improving legal case retrieval performance using a RankSVM classifier to enhance ranking rather than relying solely on language models.", "motivation": "To address the inefficiencies in similar case retrieval in legal AI by improving the ranking of retrieved cases.", "method": "The authors employed a RankSVM (pairwise method) to replace the fully connected layer in existing models, testing it on the LeCaRDv1 and LeCaRDv2 datasets.", "result": "RankSVM generally improved retrieval performance on LeCaRDv1 and LeCaRDv2 datasets and helped mitigate overfitting due to class imbalance.", "conclusion": "Using RankSVM enhances the performance of similar case retrieval by optimizing ranking and is beneficial in handling class imbalances.", "key_contributions": ["Introduced RankSVM for improving ranking in legal case retrieval", "Demonstrated effectiveness on LeCaRDv1 and LeCaRDv2 datasets", "Provided code available for public use for further research."], "limitations": "", "keywords": ["Legal AI", "Similar Case Retrieval", "RankSVM", "Machine Learning", "Legal Datasets"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2502.18573", "pdf": "https://arxiv.org/pdf/2502.18573.pdf", "abs": "https://arxiv.org/abs/2502.18573", "title": "FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models", "authors": ["Radu Marinescu", "Debarun Bhattacharjya", "Junkyu Lee", "Tigran Tchrakian", "Javier Carnerero Cano", "Yufang Hou", "Elizabeth Daly", "Alessandra Pascale"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated vast capabilities on\ngenerative tasks in recent years, yet they struggle with guaranteeing the\nfactual correctness of the generated content. This makes these models\nunreliable in realistic situations where factually accurate responses are\nexpected. In this paper, we propose FactReasoner, a new factuality assessor\nthat relies on probabilistic reasoning to assess the factuality of a long-form\ngenerated response. Specifically, FactReasoner decomposes the response into\natomic units, retrieves relevant contexts for them from an external knowledge\nsource, and constructs a joint probability distribution over the atoms and\ncontexts using probabilistic encodings of the logical relationships\n(entailment, contradiction) between the textual utterances corresponding to the\natoms and contexts. FactReasoner then computes the posterior probability of\nwhether atomic units in the response are supported by the retrieved contexts.\nOur experiments on labeled and unlabeled benchmark datasets demonstrate clearly\nthat FactReasoner improves considerably over state-of-the-art prompt-based\napproaches in terms of both factual precision and recall.", "AI": {"tldr": "FactReasoner is a new tool for assessing the factual correctness of generated long-form responses using probabilistic reasoning and context retrieval.", "motivation": "To tackle the issue of factual correctness in generated content from large language models, making them more reliable for real-world applications.", "method": "FactReasoner decomposes responses into atomic units, retrieves relevant contextual information, and uses probabilistic reasoning to evaluate the factuality of these units against the contexts.", "result": "FactReasoner shows significant improvement over current state-of-the-art prompt-based methods in both factual precision and recall on benchmark datasets.", "conclusion": "The proposed FactReasoner provides a robust method for evaluating the factual correctness of long-form content from LLMs, enhancing their usability in scenarios requiring factual accuracy.", "key_contributions": ["Introduction of FactReasoner for factual assessment", "Use of probabilistic reasoning in evaluating factuality", "Experimental validation showing improved performance over existing methods."], "limitations": "", "keywords": ["factuality assessment", "large language models", "probabilistic reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.08026", "pdf": "https://arxiv.org/pdf/2503.08026.pdf", "abs": "https://arxiv.org/abs/2503.08026", "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents", "authors": ["Zhen Tan", "Jun Yan", "I-Hung Hsu", "Rujun Han", "Zifeng Wang", "Long T. Le", "Yiwen Song", "Yanfei Chen", "Hamid Palangi", "George Lee", "Anand Iyer", "Tianlong Chen", "Huan Liu", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Large Language Models (LLMs) have made significant progress in open-ended\ndialogue, yet their inability to retain and retrieve relevant information from\nlong-term interactions limits their effectiveness in applications requiring\nsustained personalization. External memory mechanisms have been proposed to\naddress this limitation, enabling LLMs to maintain conversational continuity.\nHowever, existing approaches struggle with two key challenges. First, rigid\nmemory granularity fails to capture the natural semantic structure of\nconversations, leading to fragmented and incomplete representations. Second,\nfixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user\ninteraction patterns. In this work, we propose Reflective Memory Management\n(RMM), a novel mechanism for long-term dialogue agents, integrating forward-\nand backward-looking reflections: (1) Prospective Reflection, which dynamically\nsummarizes interactions across granularities-utterances, turns, and\nsessions-into a personalized memory bank for effective future retrieval, and\n(2) Retrospective Reflection, which iteratively refines the retrieval in an\nonline reinforcement learning (RL) manner based on LLMs' cited evidence.\nExperiments show that RMM demonstrates consistent improvement across various\nmetrics and benchmarks. For example, RMM shows more than 10% accuracy\nimprovement over the baseline without memory management on the LongMemEval\ndataset.", "AI": {"tldr": "This paper introduces Reflective Memory Management (RMM), a novel mechanism designed to enhance long-term dialogue agents by improving memory retention and retrieval through dynamic summarization and reinforcement learning.", "motivation": "The inability of LLMs to retain and retrieve relevant information from long-term interactions limits their effectiveness in personalized applications.", "method": "Reflective Memory Management (RMM) integrates two types of reflections: Prospective Reflection for dynamic summarization across interaction granularities, and Retrospective Reflection for iterative retrieval refinement using online reinforcement learning.", "result": "Experiments demonstrate that RMM improves accuracy by over 10% compared to the baseline on the LongMemEval dataset.", "conclusion": "RMM provides a more effective approach for managing memory in long-term dialogue systems, addressing significant challenges in current models.", "key_contributions": ["Introduction of Reflective Memory Management (RMM)", "Implementation of Prospective and Retrospective Reflections", "Demonstrated improvements on multiple metrics and benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Reflective Memory Management", "dialogue systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.10789", "pdf": "https://arxiv.org/pdf/2503.10789.pdf", "abs": "https://arxiv.org/abs/2503.10789", "title": "Data Caricatures: On the Representation of African American Language in Pretraining Corpora", "authors": ["Nicholas Deas", "Blake Vente", "Amith Ananthram", "Jessica A. Grieser", "Desmond Patton", "Shana Kleiner", "James Shepard", "Kathleen McKeown"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "With a combination of quantitative experiments, human judgments, and\nqualitative analyses, we evaluate the quantity and quality of African American\nLanguage (AAL) representation in 12 predominantly English, open-source\npretraining corpora. We specifically focus on the sources, variation, and\nnaturalness of included AAL texts representing the AAL-speaking community. We\nfind that AAL is underrepresented in all evaluated pretraining corpora compared\nto US demographics, constituting as few as 0.007% and at most 0.18% of\ndocuments. We also find that more than 25% of AAL texts in C4 may be perceived\nas inappropriate for LLMs to generate and to reinforce harmful stereotypes.\nFinally, we find that most automated filters are more likely to conserve White\nMainstream English (WME) texts over AAL in pretraining corpora.", "AI": {"tldr": "This paper evaluates the representation of African American Language in open-source pretraining corpora and highlights significant underrepresentation and potential biases in language models.", "motivation": "To assess the quantity and quality of African American Language representation in pretraining datasets used for language models, given the demographic disparities in language representation.", "method": "Quantitative experiments, human judgments, and qualitative analyses of 12 open-source pretraining corpora, focusing on the sources and naturalness of AAL texts.", "result": "AAL representation in the corpora is extremely low, ranging from 0.007% to 0.18%, and a significant portion of AAL texts may perpetuate harmful stereotypes, with automated filters favoring White Mainstream English.", "conclusion": "The study highlights a critical gap in representation of AAL in language model training datasets, suggesting the need for more inclusive data practices to prevent biases.", "key_contributions": ["Quantitative assessment of AAL representation in language training data", "Identification of potential biases in language model outputs", "Recommendations for improving AAL representation in corpora"], "limitations": "The study is limited to 12 pretraining corpora and may not comprehensively capture all aspects of AAL representation.", "keywords": ["African American Language", "language representation", "pretraining corpora", "natural language processing", "bias in machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.11657", "pdf": "https://arxiv.org/pdf/2503.11657.pdf", "abs": "https://arxiv.org/abs/2503.11657", "title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs", "authors": ["Vincent Li", "Tim Knappe", "Yule Fu", "Kevin Han", "Kevin Zhu"], "categories": ["cs.CL"], "comment": "Accepted to ICML AI4Math Workshop 2025, NAACL SRW 2025", "summary": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing tasks requiring multi-step logical reasoning capabilities,\nsuch as automated theorem proving. However, challenges persist within theorem\nproving, such as the identification of key mathematical concepts, understanding\ntheir interrelationships, and formalizing proofs correctly within natural\nlanguage. We present KG-prover, a novel framework that leverages knowledge\ngraphs mined from reputable mathematical texts to augment general-purpose LLMs\nto construct and formalize mathematical proofs. We also study the effects of\nscaling graph-based, test-time compute using KG-Prover, demonstrating\nsignificant performance improvements over baselines across multiple datasets.\nGeneral-purpose LLMs improve up to 21\\% on miniF2F-test when combined with\nKG-Prover, with consistent improvements ranging from 2-11\\% on the ProofNet,\nminiF2F-test, and MUSTARD datasets without additional scaling. Furthermore,\nKG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a\npromising approach for augmenting natural language proof reasoning with\nknowledge graphs without the need for additional finetuning.", "AI": {"tldr": "KG-Prover enhances LLMs for automated theorem proving by using knowledge graphs to improve performance in mathematical proof construction.", "motivation": "To address challenges in automated theorem proving, such as identifying key mathematical concepts and formalizing proofs, by leveraging knowledge graphs.", "method": "A novel framework, KG-Prover, incorporates knowledge graphs from mathematical texts into LLMs, allowing them to better understand and formalize mathematical proofs without additional finetuning.", "result": "KG-Prover improves the performance of general-purpose LLMs by up to 21% on the miniF2F-test and shows consistent enhancements of 2-11% across multiple datasets.", "conclusion": "This research demonstrates a promising framework for enhancing natural language proofs through the integration of knowledge graphs, leading to significant improvements in LLM performance.", "key_contributions": ["Introduction of KG-Prover for mathematical proof construction", "Demonstration of significant performance improvements in LLMs using knowledge graphs", "Validation of effectiveness on multiple benchmark datasets without the need for additional finetuning."], "limitations": "", "keywords": ["large language models", "theorem proving", "knowledge graphs", "automated reasoning", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.12370", "pdf": "https://arxiv.org/pdf/2503.12370.pdf", "abs": "https://arxiv.org/abs/2503.12370", "title": "Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs", "authors": ["Rupak Sarkar", "Neha Srikanth", "Taylor Hudson", "Rachel Rudinger", "Claire Bonial", "Philip Resnik"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "While it is commonly accepted that maintaining common ground plays a role in\nconversational success, little prior research exists connecting conversational\ngrounding to success in task-oriented conversations. We study failures of\ngrounding in the Ubuntu IRC dataset, where participants use text-only\ncommunication to resolve technical issues. We find that disruptions in\nconversational flow often stem from a misalignment in common ground, driven by\na divergence in beliefs and assumptions held by participants. These\ndisruptions, which we call conversational friction, significantly correlate\nwith task success. We find that although LLMs can identify overt cases of\nconversational friction, they struggle with subtler and more context-dependent\ninstances requiring pragmatic or domain-specific reasoning.", "AI": {"tldr": "This paper explores the impact of conversational grounding on task-oriented conversations, particularly in the context of the Ubuntu IRC dataset.", "motivation": "The research addresses the gap in understanding how conversational grounding affects task success in technical discussions.", "method": "Analysis of the Ubuntu IRC dataset to identify instances of conversational friction and their correlation with task success.", "result": "Disruptions in conversational flow are linked to misalignment in common ground, which correlates with task success; LLMs can identify overt but not nuanced cases of conversational friction.", "conclusion": "Improving understanding of conversational grounding can enhance outcomes in task-oriented interactions, particularly in technical environments.", "key_contributions": ["Introduces the concept of conversational friction related to common ground.", "Demonstrates the correlation between conversational alignment and task success.", "Evaluates the capabilities of LLMs in identifying conversational issues."], "limitations": "The study is limited to the Ubuntu IRC dataset and may not generalize to all task-oriented conversations.", "keywords": ["Conversational Grounding", "Task-Oriented Conversations", "Conversational Friction"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2503.12854", "pdf": "https://arxiv.org/pdf/2503.12854.pdf", "abs": "https://arxiv.org/abs/2503.12854", "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation", "authors": ["Songjun Tu", "Jiahao Lin", "Xiangyu Tian", "Qichao Zhang", "Linjing Li", "Yuqian Fu", "Nan Xu", "Wei He", "Xiangyuan Lan", "Dongmei Jiang", "Dongbin Zhao"], "categories": ["cs.CL"], "comment": "23pages", "summary": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.", "AI": {"tldr": "This paper explores Direct Preference Optimization (DPO) as a cost-effective alternative to reinforcement learning (RL) for improving the reasoning capabilities of large language models (LLMs).", "motivation": "Growing computational costs of reinforcement learning methods have prompted the search for more efficient alternatives for improving LLM reasoning.", "method": "The study evaluates the effectiveness of DPO through a single round of coarse filtering to enhance mathematical reasoning performance and introduces an iterative enhancement framework for mutual improvement of the generator and reward model.", "result": "DPO significantly boosts mathematical reasoning performance for strong base models and achieves RL-level performance with lower computational cost using simple verifiable rewards.", "conclusion": "DPO emerges as a scalable, resource-efficient solution to enhance LLM reasoning, particularly in scenarios with limited computational resources.", "key_contributions": ["Introduces Direct Preference Optimization (DPO) as an alternative to RL for LLM self-improvement.", "Demonstrates significant performance enhancements in mathematical reasoning through DPO with coarse filtering.", "Establishes an iterative framework for mutual enhancement of the generator and reward model."], "limitations": "", "keywords": ["Direct Preference Optimization", "Reinforcement Learning", "Large Language Models", "Mathematical Reasoning", "Cost-Effective Learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2503.22040", "pdf": "https://arxiv.org/pdf/2503.22040.pdf", "abs": "https://arxiv.org/abs/2503.22040", "title": "Navigating the Risks of Using Large Language Models for Text Annotation in Social Science Research", "authors": ["Hao Lin", "Yongjun Zhang"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize\ncomputational social science, particularly in automated textual analysis. In\nthis paper, we conduct a systematic evaluation of the promises and risks\nassociated with using LLMs for text classification tasks, using social movement\nstudies as an example. We propose a framework for social scientists to\nincorporate LLMs into text annotation, either as the primary coding\ndecision-maker or as a coding assistant. This framework offers researchers\ntools to develop the potential best-performing prompt, and to systematically\nexamine and report the validity and reliability of LLMs as a methodological\ntool. Additionally, we evaluate and discuss its epistemic risks associated with\nvalidity, reliability, replicability, and transparency. We conclude with\nseveral practical guidelines for using LLMs in text annotation tasks and offer\nrecommendations for more effectively communicating epistemic risks in research.", "AI": {"tldr": "The paper evaluates the use of large language models (LLMs) in automated textual analysis for social movement studies and proposes a framework for their effective use in text classification tasks.", "motivation": "To explore the potential and risks of using LLMs in computational social science, particularly in text classification, and to provide guidelines for their use.", "method": "A systematic evaluation of LLMs for text classification, proposing a framework for integrating LLMs as coding decision-makers or assistants in social science research.", "result": "Proposed a framework for utilizing LLMs in text annotation, including the development of effective prompts and a discussion of their validity and reliability in research.", "conclusion": "The paper emphasizes practical guidelines for incorporating LLMs in text tasks while addressing epistemic risks and improving communication in research.", "key_contributions": ["Proposed a framework for using LLMs in text annotation tasks.", "Systematic evaluation of LLMs' validity and reliability.", "Guidelines for addressing epistemic risks in research."], "limitations": "Limited focus on specific applications beyond social movement studies.", "keywords": ["large language models", "text classification", "automated textual analysis", "social movement studies", "epistemic risks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.02398", "pdf": "https://arxiv.org/pdf/2504.02398.pdf", "abs": "https://arxiv.org/abs/2504.02398", "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "authors": ["Gallil Maimon", "Michael Hassid", "Amit Roth", "Yossi Adi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at COLM 2025", "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nIt predicts that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this\npaper we answer a resounding yes! We conduct scaling analysis of interleaved\nSLMs by training several dozen and analysing the scaling trends. We see that\nunder this setup SLMs scale more efficiently with compute. Additionally, our\nresults indicate that the scaling dynamics significantly differ from\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget to increasing model size over training tokens. We also study the role of\nsynthetic data and TextLM model families in unlocking this potential. Results\nsuggest that our scaled up model achieves comparable semantic speech\nperformance to leading models, while using less compute and data. We open\nsource models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims/ .", "AI": {"tldr": "This paper demonstrates that interleaved Speech Language Models (SLMs) scale more efficiently than textless SLMs, requiring less compute and data while achieving comparable performance.", "motivation": "To investigate whether interleaved SLMs can scale more efficiently compared to textless-SLMs amidst concerns regarding compute and data requirements.", "method": "The authors performed scaling analysis by training several dozen interleaved SLMs, examining their efficiency in terms of compute and data utilization.", "result": "Interleaved SLMs showed better scaling efficiency with compute and required notably less data, achieving semantic performance comparable to leading models.", "conclusion": "Interleaved SLMs offer a promising approach for training more efficient models in terms of compute and data requirements, with the availability of resources to facilitate future research.", "key_contributions": ["Proving interleaved SLMs scale more efficiently than textless SLMs", "Identifying differences in scaling dynamics between the two model types", "Open sourcing models and data for further research"], "limitations": "", "keywords": ["Speech Language Models", "scaling analysis", "model efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.07274", "pdf": "https://arxiv.org/pdf/2504.07274.pdf", "abs": "https://arxiv.org/abs/2504.07274", "title": "Language Modeling for the Future of Finance: A Survey into Metrics, Tasks, and Data Opportunities", "authors": ["Nikita Tatarinov", "Siddhant Sukhani", "Agam Shah", "Sudheer Chava"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in language modeling have led to growing interest in applying\nNatural Language Processing (NLP) techniques to financial problems, enabling\nnew approaches to analysis and decision-making. To systematically examine this\ntrend, we review 374 NLP research papers published between 2017 and 2024 across\n38 conferences and workshops, with a focused analysis of 221 papers that\ndirectly address finance-related tasks. We evaluate these papers across 11\nquantitative and qualitative dimensions, and our study identifies the following\nopportunities: (i) expanding the scope of forecasting tasks; (ii) enriching\nevaluation with financial metrics; (iii) leveraging multilingual and\ncrisis-period datasets; and (iv) balancing PLMs with efficient or interpretable\nalternatives. We identify actionable directions for research and practice,\nsupported by dataset and tool recommendations, with implications for both the\nacademia and industry communities.", "AI": {"tldr": "This paper reviews the application of NLP in finance by analyzing 374 research papers and identifies key opportunities for enhancing forecasting tasks and evaluation methods.", "motivation": "To systematically examine the trend of applying NLP techniques to finance and highlight opportunities for improving analysis and decision-making.", "method": "Review of 374 NLP research papers from 2017 to 2024, focusing on 221 finance-related papers evaluated across 11 dimensions.", "result": "Identifies opportunities such as expanding forecasting tasks, improving evaluation with financial metrics, utilizing multilingual datasets, and balancing PLMs with efficient alternatives.", "conclusion": "The study provides actionable directions for research and practice, along with dataset and tool recommendations for academia and industry.", "key_contributions": ["Comprehensive review of 374 NLP papers in finance.", "Identification of underexplored opportunities and directions for future research.", "Recommendations for datasets and tools to enhance financial NLP applications."], "limitations": "", "keywords": ["Natural Language Processing", "Finance", "Language Models", "Forecasting", "Evaluation Metrics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.12549", "pdf": "https://arxiv.org/pdf/2504.12549.pdf", "abs": "https://arxiv.org/abs/2504.12549", "title": "Memorization: A Close Look at Books", "authors": ["Iris Ma", "Ian Domingo", "Alberto Krone-Martins", "Pierre Baldi", "Cristina V. Lopes"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025 L2M2 Workshop", "summary": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.", "AI": {"tldr": "This paper explores the extent to which entire books can be reconstructed from LLMs, specifically the Llama 3 models, using the prefix-prompting technique, highlighting limitations in current mitigation strategies against regurgitation.", "motivation": "To investigate the capabilities of LLMs in reconstructing books and the effectiveness of regurgitation mitigations.", "method": "Utilized the Llama 3 70B models with a prefix-prompting extraction technique to reconstruct books, focusing on extraction rates based on popularity and training data duplication.", "result": "Successfully reconstructed 'Alice's Adventures in Wonderland' from 500 tokens and achieved high extraction rates for other books, although the results varied by book popularity.", "conclusion": "The findings reveal limitations in current regurgitation mitigation strategies, emphasizing the impact of fine-tuning on the memorization retrieval in LLMs.", "key_contributions": ["Introduced a framework for studying extraction from LLMs.", "Demonstrated reconstruction of entire books from minimal prompts.", "Highlighted the relationship between book popularity and extraction success."], "limitations": "Extraction rates do not extend uniformly across all books and depend on factors like popularity.", "keywords": ["LLM", "regurgitation", "fine-tuning", "book extraction", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887.pdf", "abs": "https://arxiv.org/abs/2504.13887", "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite increasing AI chatbot deployment in public discourse, empirical\nevidence on their capacity to foster intercultural empathy remains limited.\nThrough a randomized experiment, we assessed how different AI deliberation\napproaches--cross-cultural deliberation (presenting other-culture\nperspectives), own-culture deliberation (representing participants' own\nculture), and non-deliberative control--affect intercultural empathy across\nAmerican and Latin American participants. Cross-cultural deliberation increased\nintercultural empathy among American participants through positive emotional\nengagement, but produced no such effects for Latin American participants, who\nperceived AI responses as culturally inauthentic despite explicit prompting to\nrepresent their cultural perspectives. Our analysis of participant-driven\nfeedback, where users directly flagged and explained culturally inappropriate\nAI responses, revealed systematic gaps in AI's representation of Latin American\ncontexts that persist despite sophisticated prompt engineering. These findings\ndemonstrate that current approaches to AI cultural alignment--including\nlinguistic adaptation and explicit cultural prompting--cannot fully address\ndeeper representational asymmetries in AI systems. Our work advances both\ndeliberation theory and AI alignment research by revealing how the same AI\nsystem can simultaneously promote intercultural understanding for one cultural\ngroup while failing for another, with critical implications for designing\nequitable AI systems for cross-cultural democratic discourse.", "AI": {"tldr": "The study examines AI chatbots' ability to foster intercultural empathy through different deliberation approaches and reveals significant representational gaps in AI responses for Latin American participants.", "motivation": "Investigate the role of AI chatbots in promoting intercultural empathy and evaluate their effectiveness in a multicultural context.", "method": "A randomized experiment comparing three AI deliberation approaches: cross-cultural deliberation, own-culture deliberation, and non-deliberative control, targeting American and Latin American participants.", "result": "Cross-cultural deliberation enhanced empathy among American participants but not for Latin Americans, who felt AI responses lacked cultural authenticity.", "conclusion": "Current AI cultural alignment techniques are insufficient to bridge representational gaps, highlighting the need for equitable AI system design that serves diverse cultural perspectives effectively.", "key_contributions": ["Empirical evidence of the limitations of AI chatbots in fostering intercultural empathy across different cultural groups.", "Identification of systemic gaps in AI's representation of Latin American cultures despite advanced prompt engineering.", "Implications for AI alignment research and design of democratic discourse systems."], "limitations": "Findings are based on specific cultural contexts and may not generalize to all cultures or AI systems.", "keywords": ["AI chatbots", "intercultural empathy", "deliberation theory", "cultural alignment", "democratic discourse"], "importance_score": 8, "read_time_minutes": 12}}
